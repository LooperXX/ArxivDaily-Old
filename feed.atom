<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://looperxx.github.io/ArxivDaily/index.html</id>
    <title>ArxivDaily</title>
    <updated>2021-06-29T00:42:51.090Z</updated>
    <generator>osmosfeed 1.11.0</generator>
    <link rel="alternate" href="https://looperxx.github.io/ArxivDaily/index.html"/>
    <link rel="self" href="https://looperxx.github.io/ArxivDaily/feed.atom"/>
    <entry>
        <title type="html"><![CDATA[Deep Network Approximation for Smooth Functions. (arXiv:2001.03040v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.03040</id>
        <link href="http://arxiv.org/abs/2001.03040"/>
        <updated>2021-06-28T01:57:57.979Z</updated>
        <summary type="html"><![CDATA[This paper establishes the optimal approximation error characterization of
deep ReLU networks for smooth functions in terms of both width and depth
simultaneously. To that end, we first prove that multivariate polynomials can
be approximated by deep ReLU networks of width $\mathcal{O}(N)$ and depth
$\mathcal{O}(L)$ with an approximation error $\mathcal{O}(N^{-L})$. Through
local Taylor expansions and their deep ReLU network approximations, we show
that deep ReLU networks of width $\mathcal{O}(N\ln N)$ and depth
$\mathcal{O}(L\ln L)$ can approximate $f\in C^s([0,1]^d)$ with a nearly optimal
approximation error $\mathcal{O}(\|f\|_{C^s([0,1]^d)}N^{-2s/d}L^{-2s/d})$. Our
estimate is non-asymptotic in the sense that it is valid for arbitrary width
and depth specified by $N\in\mathbb{N}^+$ and $L\in\mathbb{N}^+$, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jianfeng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zuowei Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haizhao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shijun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bringing Light Into the Dark: A Large-scale Evaluation of Knowledge Graph Embedding Models Under a Unified Framework. (arXiv:2006.13365v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.13365</id>
        <link href="http://arxiv.org/abs/2006.13365"/>
        <updated>2021-06-28T01:57:57.973Z</updated>
        <summary type="html"><![CDATA[The heterogeneity in recently published knowledge graph embedding models'
implementations, training, and evaluation has made fair and thorough
comparisons difficult. In order to assess the reproducibility of previously
published results, we re-implemented and evaluated 21 interaction models in the
PyKEEN software package. Here, we outline which results could be reproduced
with their reported hyper-parameters, which could only be reproduced with
alternate hyper-parameters, and which could not be reproduced at all as well as
provide insight as to why this might be the case.

We then performed a large-scale benchmarking on four datasets with several
thousands of experiments and 24,804 GPU hours of computation time. We present
insights gained as to best practices, best configurations for each model, and
where improvements could be made over previously published best configurations.
Our results highlight that the combination of model architecture, training
approach, loss function, and the explicit modeling of inverse relations is
crucial for a model's performances, and not only determined by the model
architecture. We provide evidence that several architectures can obtain results
competitive to the state-of-the-art when configured carefully. We have made all
code, experimental configurations, results, and analyses that lead to our
interpretations available at https://github.com/pykeen/pykeen and
https://github.com/pykeen/benchmarking]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1"&gt;Mehdi Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berrendorf_M/0/1/0/all/0/1"&gt;Max Berrendorf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoyt_C/0/1/0/all/0/1"&gt;Charles Tapley Hoyt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vermue_L/0/1/0/all/0/1"&gt;Laurent Vermue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galkin_M/0/1/0/all/0/1"&gt;Mikhail Galkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharifzadeh_S/0/1/0/all/0/1"&gt;Sahand Sharifzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fischer_A/0/1/0/all/0/1"&gt;Asja Fischer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1"&gt;Volker Tresp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1"&gt;Jens Lehmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Task-Driven Out-of-Distribution Detection with Statistical Guarantees for Robot Learning. (arXiv:2106.13703v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.13703</id>
        <link href="http://arxiv.org/abs/2106.13703"/>
        <updated>2021-06-28T01:57:57.965Z</updated>
        <summary type="html"><![CDATA[Our goal is to perform out-of-distribution (OOD) detection, i.e., to detect
when a robot is operating in environments that are drawn from a different
distribution than the environments used to train the robot. We leverage
Probably Approximately Correct (PAC)-Bayes theory in order to train a policy
with a guaranteed bound on performance on the training distribution. Our key
idea for OOD detection then relies on the following intuition: violation of the
performance bound on test environments provides evidence that the robot is
operating OOD. We formalize this via statistical techniques based on p-values
and concentration inequalities. The resulting approach (i) provides guaranteed
confidence bounds on OOD detection, and (ii) is task-driven and sensitive only
to changes that impact the robot's performance. We demonstrate our approach on
a simulated example of grasping objects with unfamiliar poses or shapes. We
also present both simulation and hardware experiments for a drone performing
vision-based obstacle avoidance in unfamiliar environments (including wind
disturbances and different obstacle densities). Our examples demonstrate that
we can perform task-driven OOD detection within just a handful of trials.
Comparisons with baselines also demonstrate the advantages of our approach in
terms of providing statistical guarantees and being insensitive to
task-irrelevant distribution shifts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farid_A/0/1/0/all/0/1"&gt;Alec Farid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veer_S/0/1/0/all/0/1"&gt;Sushant Veer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majumdar_A/0/1/0/all/0/1"&gt;Anirudha Majumdar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tighter Analysis of Alternating Stochastic Gradient Method for Stochastic Nested Problems. (arXiv:2106.13781v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.13781</id>
        <link href="http://arxiv.org/abs/2106.13781"/>
        <updated>2021-06-28T01:57:57.960Z</updated>
        <summary type="html"><![CDATA[Stochastic nested optimization, including stochastic compositional, min-max
and bilevel optimization, is gaining popularity in many machine learning
applications. While the three problems share the nested structure, existing
works often treat them separately, and thus develop problem-specific algorithms
and their analyses. Among various exciting developments, simple SGD-type
updates (potentially on multiple variables) are still prevalent in solving this
class of nested problems, but they are believed to have slower convergence rate
compared to that of the non-nested problems. This paper unifies several
SGD-type updates for stochastic nested problems into a single SGD approach that
we term ALternating Stochastic gradient dEscenT (ALSET) method. By leveraging
the hidden smoothness of the problem, this paper presents a tighter analysis of
ALSET for stochastic nested problems. Under the new analysis, to achieve an
$\epsilon$-stationary point of the nested problem, it requires ${\cal
O}(\epsilon^{-2})$ samples. Under certain regularity conditions, applying our
results to stochastic compositional, min-max and reinforcement learning
problems either improves or matches the best-known sample complexity in the
respective cases. Our results explain why simple SGD-type algorithms in
stochastic nested problems all work very well in practice without the need for
further modifications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yuejiao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yin_W/0/1/0/all/0/1"&gt;Wotao Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boolean learning under noise-perturbations in hardware neural networks. (arXiv:2003.12319v2 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.12319</id>
        <link href="http://arxiv.org/abs/2003.12319"/>
        <updated>2021-06-28T01:57:57.953Z</updated>
        <summary type="html"><![CDATA[A high efficiency hardware integration of neural networks benefits from
realizing nonlinearity, network connectivity and learning fully in a physical
substrate. Multiple systems have recently implemented some or all of these
operations, yet the focus was placed on addressing technological challenges.
Fundamental questions regarding learning in hardware neural networks remain
largely unexplored. Noise in particular is unavoidable in such architectures,
and here we investigate its interaction with a learning algorithm using an
opto-electronic recurrent neural network. We find that noise strongly modifies
the system's path during convergence, and surprisingly fully decorrelates the
final readout weight matrices. This highlights the importance of understanding
architecture, noise and learning algorithm as interacting players, and
therefore identifies the need for mathematical tools for noisy, analogue system
optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Andreoli_L/0/1/0/all/0/1"&gt;Louis Andreoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Porte_X/0/1/0/all/0/1"&gt;Xavier Porte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chretien_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phane Chr&amp;#xe9;tien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacquot_M/0/1/0/all/0/1"&gt;Maxime Jacquot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Larger_L/0/1/0/all/0/1"&gt;Laurent Larger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brunner_D/0/1/0/all/0/1"&gt;Daniel Brunner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessing Generalization of SGD via Disagreement. (arXiv:2106.13799v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13799</id>
        <link href="http://arxiv.org/abs/2106.13799"/>
        <updated>2021-06-28T01:57:57.932Z</updated>
        <summary type="html"><![CDATA[We empirically show that the test error of deep networks can be estimated by
simply training the same architecture on the same training set but with a
different run of Stochastic Gradient Descent (SGD), and measuring the
disagreement rate between the two networks on unlabeled test data. This builds
on -- and is a stronger version of -- the observation in Nakkiran & Bansal '20,
which requires the second run to be on an altogether fresh training set. We
further theoretically show that this peculiar phenomenon arises from the
\emph{well-calibrated} nature of \emph{ensembles} of SGD-trained models. This
finding not only provides a simple empirical measure to directly predict the
test error using unlabeled test data, but also establishes a new conceptual
connection between generalization and calibration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yiding Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagarajan_V/0/1/0/all/0/1"&gt;Vaishnavh Nagarajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baek_C/0/1/0/all/0/1"&gt;Christina Baek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1"&gt;J. Zico Kolter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning for Mean Field Games, with Applications to Economics. (arXiv:2106.13755v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.13755</id>
        <link href="http://arxiv.org/abs/2106.13755"/>
        <updated>2021-06-28T01:57:57.924Z</updated>
        <summary type="html"><![CDATA[Mean field games (MFG) and mean field control problems (MFC) are frameworks
to study Nash equilibria or social optima in games with a continuum of agents.
These problems can be used to approximate competitive or cooperative games with
a large finite number of agents and have found a broad range of applications,
in particular in economics. In recent years, the question of learning in MFG
and MFC has garnered interest, both as a way to compute solutions and as a way
to model how large populations of learners converge to an equilibrium. Of
particular interest is the setting where the agents do not know the model,
which leads to the development of reinforcement learning (RL) methods. After
reviewing the literature on this topic, we present a two timescale approach
with RL for MFG and MFC, which relies on a unified Q-learning algorithm. The
main novelty of this method is to simultaneously update an action-value
function and a distribution but with different rates, in a model-free fashion.
Depending on the ratio of the two learning rates, the algorithm learns either
the MFG or the MFC solution. To illustrate this method, we apply it to a mean
field problem of accumulated consumption in finite horizon with HARA utility
function, and to a trader's optimal liquidation problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Angiuli_A/0/1/0/all/0/1"&gt;Andrea Angiuli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Fouque_J/0/1/0/all/0/1"&gt;Jean-Pierre Fouque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Lauriere_M/0/1/0/all/0/1"&gt;Mathieu Lauriere&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessing the Lockdown Effects on Air Quality during COVID-19 Era. (arXiv:2106.13750v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13750</id>
        <link href="http://arxiv.org/abs/2106.13750"/>
        <updated>2021-06-28T01:57:57.919Z</updated>
        <summary type="html"><![CDATA[In this work we investigate the short-term variations in air quality
emissions, attributed to the prevention measures, applied in different cities,
to mitigate the COVID-19 spread. In particular, we emphasize on the
concentration effects regarding specific pollutant gases, such as carbon
monoxide (CO), ozone (O3), nitrogen dioxide (NO2) and sulphur dioxide (SO2).
The assessment of the impact of lockdown on air quality focused on four
European Cities (Athens, Gladsaxe, Lodz and Rome). Available data on pollutant
factors were obtained using global satellite observations. The level of the
employed prevention measures is employed using the Oxford COVID-19 Government
Response Tracker. The second part of the analysis employed a variety of machine
learning tools, utilized for estimating the concentration of each pollutant,
two days ahead. The results showed that a weak to moderate correlation exists
between the corresponding measures and the pollutant factors and that it is
possible to create models which can predict the behaviour of the pollutant
gases under daily human activities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kavouras_I/0/1/0/all/0/1"&gt;Ioannis Kavouras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Protopapadakis_E/0/1/0/all/0/1"&gt;Eftychios Protopapadakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaselimia_M/0/1/0/all/0/1"&gt;Maria Kaselimia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sardis_E/0/1/0/all/0/1"&gt;Emmanuel Sardis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doulamis_N/0/1/0/all/0/1"&gt;Nikolaos Doulamis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Mixture Density Networks. (arXiv:2012.03085v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03085</id>
        <link href="http://arxiv.org/abs/2012.03085"/>
        <updated>2021-06-28T01:57:57.911Z</updated>
        <summary type="html"><![CDATA[We introduce the Graph Mixture Density Networks, a new family of machine
learning models that can fit multimodal output distributions conditioned on
graphs of arbitrary topology. By combining ideas from mixture models and graph
representation learning, we address a broader class of challenging conditional
density estimation problems that rely on structured data. In this respect, we
evaluate our method on a new benchmark application that leverages random graphs
for stochastic epidemic simulations. We show a significant improvement in the
likelihood of epidemic outcomes when taking into account both multimodality and
structure. The empirical analysis is complemented by two real-world regression
tasks showing the effectiveness of our approach in modeling the output
prediction uncertainty. Graph Mixture Density Networks open appealing research
opportunities in the study of structure-dependent phenomena that exhibit
non-trivial conditional output distributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Errica_F/0/1/0/all/0/1"&gt;Federico Errica&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1"&gt;Davide Bacciu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Micheli_A/0/1/0/all/0/1"&gt;Alessio Micheli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interval and fuzzy physics-informed neural networks for uncertain fields. (arXiv:2106.13727v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2106.13727</id>
        <link href="http://arxiv.org/abs/2106.13727"/>
        <updated>2021-06-28T01:57:57.905Z</updated>
        <summary type="html"><![CDATA[Temporally and spatially dependent uncertain parameters are regularly
encountered in engineering applications. Commonly these uncertainties are
accounted for using random fields and processes which require knowledge about
the appearing probability distributions functions which is not readily
available. In these cases non-probabilistic approaches such as interval
analysis and fuzzy set theory are helpful uncertainty measures. Partial
differential equations involving fuzzy and interval fields are traditionally
solved using the finite element method where the input fields are sampled using
some basis function expansion methods. This approach however is problematic, as
it is reliant on knowledge about the spatial correlation fields. In this work
we utilize physics-informed neural networks (PINNs) to solve interval and fuzzy
partial differential equations. The resulting network structures termed
interval physics-informed neural networks (iPINNs) and fuzzy physics-informed
neural networks (fPINNs) show promising results for obtaining bounded solutions
of equations involving spatially uncertain parameter fields. In contrast to
finite element approaches, no correlation length specification of the input
fields as well as no averaging via Monte-Carlo simulations are necessary. In
fact, information about the input interval fields is obtained directly as a
byproduct of the presented solution scheme. Furthermore, all major advantages
of PINNs are retained, i.e. meshfree nature of the scheme, and ease of inverse
problem set-up.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Fuhg_J/0/1/0/all/0/1"&gt;Jan Niklas Fuhg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Fau_A/0/1/0/all/0/1"&gt;Am&amp;#xe9;lie Fau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Bouklas_N/0/1/0/all/0/1"&gt;Nikolaos Bouklas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-player Multi-armed Bandits with Collision-Dependent Reward Distributions. (arXiv:2106.13669v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.13669</id>
        <link href="http://arxiv.org/abs/2106.13669"/>
        <updated>2021-06-28T01:57:57.891Z</updated>
        <summary type="html"><![CDATA[We study a new stochastic multi-player multi-armed bandits (MP-MAB) problem,
where the reward distribution changes if a collision occurs on the arm.
Existing literature always assumes a zero reward for involved players if
collision happens, but for applications such as cognitive radio, the more
realistic scenario is that collision reduces the mean reward but not
necessarily to zero. We focus on the more practical no-sensing setting where
players do not perceive collisions directly, and propose the Error-Correction
Collision Communication (EC3) algorithm that models implicit communication as a
reliable communication over noisy channel problem, for which random coding
error exponent is used to establish the optimal regret that no communication
protocol can beat. Finally, optimizing the tradeoff between code length and
decoding error rate leads to a regret that approaches the centralized MP-MAB
regret, which represents a natural lower bound. Experiments with practical
error-correction codes on both synthetic and real-world datasets demonstrate
the superiority of EC3. In particular, the results show that the choice of
coding schemes has a profound impact on the regret performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1"&gt;Chengshuai Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Cong Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DAEMON: Dataset-Agnostic Explainable Malware Classification Using Multi-Stage Feature Mining. (arXiv:2008.01855v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.01855</id>
        <link href="http://arxiv.org/abs/2008.01855"/>
        <updated>2021-06-28T01:57:57.885Z</updated>
        <summary type="html"><![CDATA[Numerous metamorphic and polymorphic malicious variants are generated
automatically on a daily basis by mutation engines that transform the code of a
malicious program while retaining its functionality, in order to evade
signature-based detection. These automatic processes have greatly increased the
number of malware variants, deeming their fully-manual analysis impossible.
Malware classification is the task of determining to which family a new
malicious variant belongs. Variants of the same malware family show similar
behavioral patterns. Thus, classifying newly discovered malicious programs and
applications helps assess the risks they pose. Moreover, malware classification
facilitates determining which of the newly discovered variants should undergo
manual analysis by a security expert, in order to determine whether they belong
to a new family (e.g., one whose members exploit a zero-day vulnerability) or
are simply the result of a concept drift within a known malicious family. This
motivated intense research in recent years on devising high-accuracy automatic
tools for malware classification. In this work, we present DAEMON - a novel
dataset-agnostic malware classifier. A key property of DAEMON is that the type
of features it uses and the manner in which they are mined facilitate
understanding the distinctive behavior of malware families, making its
classification decisions explainable. We've optimized DAEMON using a
large-scale dataset of x86 binaries, belonging to a mix of several malware
families targeting computers running Windows. We then re-trained it and applied
it, without any algorithmic change, feature re-engineering or parameter tuning,
to two other large-scale datasets of malicious Android applications consisting
of numerous malware families. DAEMON obtained highly accurate classification
results on all datasets, establishing that it is also platform-agnostic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Korine_R/0/1/0/all/0/1"&gt;Ron Korine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hendler_D/0/1/0/all/0/1"&gt;Danny Hendler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safe Learning-based Observers for Unknown Nonlinear Systems using Bayesian Optimization. (arXiv:2005.05888v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.05888</id>
        <link href="http://arxiv.org/abs/2005.05888"/>
        <updated>2021-06-28T01:57:57.879Z</updated>
        <summary type="html"><![CDATA[Data generated from dynamical systems with unknown dynamics enable the
learning of state observers that are: robust to modeling error, computationally
tractable to design, and capable of operating with guaranteed performance. In
this paper, a modular design methodology is formulated, that consists of three
design phases: (i) an initial robust observer design that enables one to learn
the dynamics without allowing the state estimation error to diverge (hence,
safe); (ii) a learning phase wherein the unmodeled components are estimated
using Bayesian optimization and Gaussian processes; and, (iii) a re-design
phase that leverages the learned dynamics to improve convergence rate of the
state estimation error. The potential of our proposed learning-based observer
is demonstrated on a benchmark nonlinear system. Additionally, certificates of
guaranteed estimation performance are provided.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chakrabarty_A/0/1/0/all/0/1"&gt;Ankush Chakrabarty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Benosman_M/0/1/0/all/0/1"&gt;Mouhacine Benosman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single Image Texture Translation for Data Augmentation. (arXiv:2106.13804v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13804</id>
        <link href="http://arxiv.org/abs/2106.13804"/>
        <updated>2021-06-28T01:57:57.872Z</updated>
        <summary type="html"><![CDATA[Recent advances in image synthesis enables one to translate images by
learning the mapping between a source domain and a target domain. Existing
methods tend to learn the distributions by training a model on a variety of
datasets, with results evaluated largely in a subjective manner. Relatively few
works in this area, however, study the potential use of semantic image
translation methods for image recognition tasks. In this paper, we explore the
use of Single Image Texture Translation (SITT) for data augmentation. We first
propose a lightweight model for translating texture to images based on a single
input of source texture, allowing for fast training and testing. Based on SITT,
we then explore the use of augmented data in long-tailed and few-shot image
classification tasks. We find the proposed method is capable of translating
input data into a target domain, leading to consistent improved image
recognition performance. Finally, we examine how SITT and related image
translation methods can provide a basis for a data-efficient, augmentation
engineering approach to model training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Boyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1"&gt;Yin Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tsung-Yi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1"&gt;Serge Belongie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VEGN: Variant Effect Prediction with Graph Neural Networks. (arXiv:2106.13642v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13642</id>
        <link href="http://arxiv.org/abs/2106.13642"/>
        <updated>2021-06-28T01:57:57.866Z</updated>
        <summary type="html"><![CDATA[Genetic mutations can cause disease by disrupting normal gene function.
Identifying the disease-causing mutations from millions of genetic variants
within an individual patient is a challenging problem. Computational methods
which can prioritize disease-causing mutations have, therefore, enormous
applications. It is well-known that genes function through a complex regulatory
network. However, existing variant effect prediction models only consider a
variant in isolation. In contrast, we propose VEGN, which models variant effect
prediction using a graph neural network (GNN) that operates on a heterogeneous
graph with genes and variants. The graph is created by assigning variants to
genes and connecting genes with an gene-gene interaction network. In this
context, we explore an approach where a gene-gene graph is given and another
where VEGN learns the gene-gene graph and therefore operates both on given and
learnt edges. The graph neural network is trained to aggregate information
between genes, and between genes and variants. Variants can exchange
information via the genes they connect to. This approach improves the
performance of existing state-of-the-art models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1"&gt;Jun Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lawrence_C/0/1/0/all/0/1"&gt;Carolin Lawrence&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1"&gt;Mathias Niepert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ranger21: a synergistic deep learning optimizer. (arXiv:2106.13731v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13731</id>
        <link href="http://arxiv.org/abs/2106.13731"/>
        <updated>2021-06-28T01:57:57.858Z</updated>
        <summary type="html"><![CDATA[As optimizers are critical to the performances of neural networks, every year
a large number of papers innovating on the subject are published. However,
while most of these publications provide incremental improvements to existing
algorithms, they tend to be presented as new optimizers rather than composable
algorithms. Thus, many worthwhile improvements are rarely seen out of their
initial publication. Taking advantage of this untapped potential, we introduce
Ranger21, a new optimizer which combines AdamW with eight components, carefully
selected after reviewing and testing ideas from the literature. We found that
the resulting optimizer provides significantly improved validation accuracy and
training speed, smoother training curves, and is even able to train a ResNet50
on ImageNet2012 without Batch Normalization layers. A problem on which AdamW
stays systematically stuck in a bad initial state.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wright_L/0/1/0/all/0/1"&gt;Less Wright&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demeure_N/0/1/0/all/0/1"&gt;Nestor Demeure&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transient Stability Analysis with Physics-Informed Neural Networks. (arXiv:2106.13638v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13638</id>
        <link href="http://arxiv.org/abs/2106.13638"/>
        <updated>2021-06-28T01:57:57.842Z</updated>
        <summary type="html"><![CDATA[Solving the ordinary differential equations that govern the power system is
an indispensable part in transient stability analysis. However, the
traditionally applied methods either carry a significant computational burden,
require model simplifications, or use overly conservative surrogate models.
Neural networks can circumvent these limitations but are faced with high
demands on the used datasets. Furthermore, they are agnostic to the underlying
governing equations. Physics-informed neural network tackle this problem and we
explore their advantages and challenges in this paper. We illustrate the
findings on the Kundur two-area system and highlight possible pathways forward
in developing this method further.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stiasny_J/0/1/0/all/0/1"&gt;Jochen Stiasny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misyris_G/0/1/0/all/0/1"&gt;Georgios S. Misyris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatzivasileiadis_S/0/1/0/all/0/1"&gt;Spyros Chatzivasileiadis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Neural Networks: Essentials. (arXiv:2106.13594v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13594</id>
        <link href="http://arxiv.org/abs/2106.13594"/>
        <updated>2021-06-28T01:57:57.835Z</updated>
        <summary type="html"><![CDATA[Bayesian neural networks utilize probabilistic layers that capture
uncertainty over weights and activations, and are trained using Bayesian
inference. Since these probabilistic layers are designed to be drop-in
replacement of their deterministic counter parts, Bayesian neural networks
provide a direct and natural way to extend conventional deep neural networks to
support probabilistic deep learning. However, it is nontrivial to understand,
design and train Bayesian neural networks due to their complexities. We discuss
the essentials of Bayesian neural networks including duality (deep neural
networks, probabilistic models), approximate Bayesian inference, Bayesian
priors, Bayesian posteriors, and deep variational learning. We use TensorFlow
Probability APIs and code examples for illustration. The main problem with
Bayesian neural networks is that the architecture of deep neural networks makes
it quite redundant, and costly, to account for uncertainty for a large number
of successive layers. Hybrid Bayesian neural networks, which use few
probabilistic layers judicially positioned in the networks, provide a practical
solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1"&gt;Daniel T. Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conjugate Energy-Based Models. (arXiv:2106.13798v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13798</id>
        <link href="http://arxiv.org/abs/2106.13798"/>
        <updated>2021-06-28T01:57:57.828Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose conjugate energy-based models (CEBMs), a new class
of energy-based models that define a joint density over data and latent
variables. The joint density of a CEBM decomposes into an intractable
distribution over data and a tractable posterior over latent variables. CEBMs
have similar use cases as variational autoencoders, in the sense that they
learn an unsupervised mapping from data to latent variables. However, these
models omit a generator network, which allows them to learn more flexible
notions of similarity between data points. Our experiments demonstrate that
conjugate EBMs achieve competitive results in terms of image modelling,
predictive power of latent space, and out-of-domain detection on a variety of
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Esmaeili_B/0/1/0/all/0/1"&gt;Babak Esmaeili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wick_M/0/1/0/all/0/1"&gt;Michael Wick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tristan_J/0/1/0/all/0/1"&gt;Jean-Baptiste Tristan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meent_J/0/1/0/all/0/1"&gt;Jan-Willem van de Meent&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Connecting Sphere Manifolds Hierarchically for Regularization. (arXiv:2106.13549v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13549</id>
        <link href="http://arxiv.org/abs/2106.13549"/>
        <updated>2021-06-28T01:57:57.814Z</updated>
        <summary type="html"><![CDATA[This paper considers classification problems with hierarchically organized
classes. We force the classifier (hyperplane) of each class to belong to a
sphere manifold, whose center is the classifier of its super-class. Then,
individual sphere manifolds are connected based on their hierarchical
relations. Our technique replaces the last layer of a neural network by
combining a spherical fully-connected layer with a hierarchical layer. This
regularization is shown to improve the performance of widely used deep neural
network architectures (ResNet and DenseNet) on publicly available datasets
(CIFAR100, CUB200, Stanford dogs, Stanford cars, and Tiny-ImageNet).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Scieur_D/0/1/0/all/0/1"&gt;Damien Scieur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Youngsung Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Clipping for Federated Learning: Convergence and Client-Level Differential Privacy. (arXiv:2106.13673v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13673</id>
        <link href="http://arxiv.org/abs/2106.13673"/>
        <updated>2021-06-28T01:57:57.807Z</updated>
        <summary type="html"><![CDATA[Providing privacy protection has been one of the primary motivations of
Federated Learning (FL). Recently, there has been a line of work on
incorporating the formal privacy notion of differential privacy with FL. To
guarantee the client-level differential privacy in FL algorithms, the clients'
transmitted model updates have to be clipped before adding privacy noise. Such
clipping operation is substantially different from its counterpart of gradient
clipping in the centralized differentially private SGD and has not been
well-understood. In this paper, we first empirically demonstrate that the
clipped FedAvg can perform surprisingly well even with substantial data
heterogeneity when training neural networks, which is partly because the
clients' updates become similar for several popular deep architectures. Based
on this key observation, we provide the convergence analysis of a differential
private (DP) FedAvg algorithm and highlight the relationship between clipping
bias and the distribution of the clients' updates. To the best of our
knowledge, this is the first work that rigorously investigates theoretical and
empirical issues regarding the clipping operation in FL algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xinwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiangyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1"&gt;Mingyi Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhiwei Steven Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1"&gt;Jinfeng Yi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Residual Echo Suppression with A Tunable Tradeoff Between Signal Distortion and Echo Suppression. (arXiv:2106.13531v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.13531</id>
        <link href="http://arxiv.org/abs/2106.13531"/>
        <updated>2021-06-28T01:57:57.800Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a residual echo suppression method using a UNet
neural network that directly maps the outputs of a linear acoustic echo
canceler to the desired signal in the spectral domain. This system embeds a
design parameter that allows a tunable tradeoff between the desired-signal
distortion and residual echo suppression in double-talk scenarios. The system
employs 136 thousand parameters, and requires 1.6 Giga floating-point
operations per second and 10 Mega-bytes of memory. The implementation satisfies
both the timing requirements of the AEC challenge and the computational and
memory limitations of on-device applications. Experiments are conducted with
161~h of data from the AEC challenge database and from real independent
recordings. We demonstrate the performance of the proposed system in real-life
conditions and compare it with two competing methods regarding echo suppression
and desired-signal distortion, generalization to various environments, and
robustness to high echo levels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ivry_A/0/1/0/all/0/1"&gt;Amir Ivry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1"&gt;Israel Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berdugo_B/0/1/0/all/0/1"&gt;Baruch Berdugo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privileged Zero-Shot AutoML. (arXiv:2106.13743v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13743</id>
        <link href="http://arxiv.org/abs/2106.13743"/>
        <updated>2021-06-28T01:57:57.792Z</updated>
        <summary type="html"><![CDATA[This work improves the quality of automated machine learning (AutoML) systems
by using dataset and function descriptions while significantly decreasing
computation time from minutes to milliseconds by using a zero-shot approach.
Given a new dataset and a well-defined machine learning task, humans begin by
reading a description of the dataset and documentation for the algorithms to be
used. This work is the first to use these textual descriptions, which we call
privileged information, for AutoML. We use a pre-trained Transformer model to
process the privileged text and demonstrate that using this information
improves AutoML performance. Thus, our approach leverages the progress of
unsupervised representation learning in natural language processing to provide
a significant boost to AutoML. We demonstrate that using only textual
descriptions of the data and functions achieves reasonable classification
performance, and adding textual descriptions to data meta-features improves
classification across tabular datasets. To achieve zero-shot AutoML we train a
graph neural network with these description embeddings and the data
meta-features. Each node represents a training dataset, which we use to predict
the best machine learning pipeline for a new test dataset in a zero-shot
fashion. Our zero-shot approach rapidly predicts a high-quality pipeline for a
supervised learning task and dataset. In contrast, most AutoML systems require
tens or hundreds of pipeline evaluations. We show that zero-shot AutoML reduces
running and prediction times from minutes to milliseconds, consistently across
datasets. By speeding up AutoML by orders of magnitude this work demonstrates
real-time AutoML.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1"&gt;Nikhil Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kates_B/0/1/0/all/0/1"&gt;Brandon Kates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mentch_J/0/1/0/all/0/1"&gt;Jeff Mentch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kharkar_A/0/1/0/all/0/1"&gt;Anant Kharkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Udell_M/0/1/0/all/0/1"&gt;Madeleine Udell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1"&gt;Iddo Drori&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data efficiency in graph networks through equivariance. (arXiv:2106.13786v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13786</id>
        <link href="http://arxiv.org/abs/2106.13786"/>
        <updated>2021-06-28T01:57:57.784Z</updated>
        <summary type="html"><![CDATA[We introduce a novel architecture for graph networks which is equivariant to
any transformation in the coordinate embeddings that preserves the distance
between neighbouring nodes. In particular, it is equivariant to the Euclidean
and conformal orthogonal groups in $n$-dimensions. Thanks to its equivariance
properties, the proposed model is extremely more data efficient with respect to
classical graph architectures and also intrinsically equipped with a better
inductive bias. We show that, learning on a minimal amount of data, the
architecture we propose can perfectly generalise to unseen data in a synthetic
problem, while much more training data are required from a standard model to
reach comparable performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farina_F/0/1/0/all/0/1"&gt;Francesco Farina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slade_E/0/1/0/all/0/1"&gt;Emma Slade&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrent Coupled Topic Modeling over Sequential Documents. (arXiv:2106.13732v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.13732</id>
        <link href="http://arxiv.org/abs/2106.13732"/>
        <updated>2021-06-28T01:57:57.771Z</updated>
        <summary type="html"><![CDATA[The abundant sequential documents such as online archival, social media and
news feeds are streamingly updated, where each chunk of documents is
incorporated with smoothly evolving yet dependent topics. Such digital texts
have attracted extensive research on dynamic topic modeling to infer hidden
evolving topics and their temporal dependencies. However, most of the existing
approaches focus on single-topic-thread evolution and ignore the fact that a
current topic may be coupled with multiple relevant prior topics. In addition,
these approaches also incur the intractable inference problem when inferring
latent parameters, resulting in a high computational cost and performance
degradation. In this work, we assume that a current topic evolves from all
prior topics with corresponding coupling weights, forming the
multi-topic-thread evolution. Our method models the dependencies between
evolving topics and thoroughly encodes their complex multi-couplings across
time steps. To conquer the intractable inference challenge, a new solution with
a set of novel data augmentation techniques is proposed, which successfully
discomposes the multi-couplings between evolving topics. A fully conjugate
model is thus obtained to guarantee the effectiveness and efficiency of the
inference technique. A novel Gibbs sampler with a backward-forward filter
algorithm efficiently learns latent timeevolving parameters in a closed-form.
In addition, the latent Indian Buffet Process (IBP) compound distribution is
exploited to automatically infer the overall topic number and customize the
sparse topic proportions for each sequential document without bias. The
proposed method is evaluated on both synthetic and real-world datasets against
the competitive baselines, demonstrating its superiority over the baselines in
terms of the low per-word perplexity, high coherent topics, and better document
time prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jinjin Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1"&gt;Longbing Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1"&gt;Zhiguo Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Domain Active Learning: A Comparative Study. (arXiv:2106.13516v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13516</id>
        <link href="http://arxiv.org/abs/2106.13516"/>
        <updated>2021-06-28T01:57:57.760Z</updated>
        <summary type="html"><![CDATA[Building classifiers on multiple domains is a practical problem in the real
life. Instead of building classifiers one by one, multi-domain learning (MDL)
simultaneously builds classifiers on multiple domains. MDL utilizes the
information shared among the domains to improve the performance. As a
supervised learning problem, the labeling effort is still high in MDL problems.
Usually, this high labeling cost issue could be relieved by using active
learning. Thus, it is natural to utilize active learning to reduce the labeling
effort in MDL, and we refer this setting as multi-domain active learning
(MDAL). However, there are only few works which are built on this setting. And
when the researches have to face this problem, there is no off-the-shelf
solutions. Under this circumstance, combining the current multi-domain learning
models and single-domain active learning strategies might be a preliminary
solution for MDAL problem. To find out the potential of this preliminary
solution, a comparative study over 5 models and 4 selection strategies is made
in this paper. To the best of our knowledge, this is the first work provides
the formal definition of MDAL. Besides, this is the first comparative work for
MDAL problem. From the results, the Multinomial Adversarial Networks (MAN)
model with a simple best vs second best (BvSB) uncertainty strategy shows its
superiority in most cases. We take this combination as our off-the-shelf
recommendation for the MDAL problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1"&gt;Rui He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1"&gt;Ke Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Machine Learning and Data Mining to Leverage Community Knowledge for the Engineering of Stable Metal-Organic Frameworks. (arXiv:2106.13327v1 [cond-mat.mtrl-sci])]]></title>
        <id>http://arxiv.org/abs/2106.13327</id>
        <link href="http://arxiv.org/abs/2106.13327"/>
        <updated>2021-06-28T01:57:57.470Z</updated>
        <summary type="html"><![CDATA[Although the tailored metal active sites and porous architectures of MOFs
hold great promise for engineering challenges ranging from gas separations to
catalysis, a lack of understanding of how to improve their stability limits
their use in practice. To overcome this limitation, we extract thousands of
published reports of the key aspects of MOF stability necessary for their
practical application: the ability to withstand high temperatures without
degrading and the capacity to be activated by removal of solvent molecules.
From nearly 4,000 manuscripts, we use natural language processing and automated
image analysis to obtain over 2,000 solvent-removal stability measures and
3,000 thermal degradation temperatures. We analyze the relationships between
stability properties and the chemical and geometric structures in this set to
identify limits of prior heuristics derived from smaller sets of MOFs. By
training predictive machine learning (ML, i.e., Gaussian process and artificial
neural network) models to encode the structure-property relationships with
graph- and pore-structure-based representations, we are able to make
predictions of stability orders of magnitude faster than conventional
physics-based modeling or experiment. Interpretation of important features in
ML models provides insights that we use to identify strategies to engineer
increased stability into typically unstable 3d-containing MOFs that are
frequently targeted for catalytic applications. We expect our approach to
accelerate the time to discovery of stable, practical MOF materials for a wide
range of applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Nandy_A/0/1/0/all/0/1"&gt;Aditya Nandy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Duan_C/0/1/0/all/0/1"&gt;Chenru Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Kulik_H/0/1/0/all/0/1"&gt;Heather J. Kulik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Recurrent Neural Network for Multistep Time Series Forecasting. (arXiv:2104.12311v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12311</id>
        <link href="http://arxiv.org/abs/2104.12311"/>
        <updated>2021-06-28T01:57:57.449Z</updated>
        <summary type="html"><![CDATA[Time series forecasting based on deep architectures has been gaining
popularity in recent years due to their ability to model complex non-linear
temporal dynamics. The recurrent neural network is one such model capable of
handling variable-length input and output. In this paper, we leverage recent
advances in deep generative models and the concept of state space models to
propose a stochastic adaptation of the recurrent neural network for
multistep-ahead time series forecasting, which is trained with stochastic
gradient variational Bayes. In our model design, the transition function of the
recurrent neural network, which determines the evolution of the hidden states,
is stochastic rather than deterministic as in a regular recurrent neural
network; this is achieved by incorporating a latent random variable into the
transition process which captures the stochasticity of the temporal dynamics.
Our model preserves the architectural workings of a recurrent neural network
for which all relevant information is encapsulated in its hidden states, and
this flexibility allows our model to be easily integrated into any deep
architecture for sequential modelling. We test our model on a wide range of
datasets from finance to healthcare; results show that the stochastic recurrent
neural network consistently outperforms its deterministic counterpart.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1"&gt;Zexuan Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barucca_P/0/1/0/all/0/1"&gt;Paolo Barucca&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-grained Geolocation Prediction of Tweets with Human Machine Collaboration. (arXiv:2106.13411v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13411</id>
        <link href="http://arxiv.org/abs/2106.13411"/>
        <updated>2021-06-28T01:57:57.431Z</updated>
        <summary type="html"><![CDATA[Twitter is a useful resource to analyze peoples' opinions on various topics.
Often these topics are correlated or associated with locations from where these
Tweet posts are made. For example, restaurant owners may need to know where
their target customers eat with respect to the sentiment of the posts made
related to food, policy planners may need to analyze citizens' opinion on
relevant issues such as crime, safety, congestion, etc. with respect to
specific parts of the city, or county or state. As promising as this is, less
than $1\%$ of the crawled Tweet posts come with geolocation tags. That makes
accurate prediction of Tweet posts for the non geo-tagged tweets very critical
to analyze data in various domains. In this research, we utilized millions of
Twitter posts and end-users domain expertise to build a set of deep neural
network models using natural language processing (NLP) techniques, that
predicts the geolocation of non geo-tagged Tweet posts at various level of
granularities such as neighborhood, zipcode, and longitude with latitudes. With
multiple neural architecture experiments, and a collaborative human-machine
workflow design, our ongoing work on geolocation detection shows promising
results that empower end-users to correlate relationship between variables of
choice with the location information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dutt_F/0/1/0/all/0/1"&gt;Florina Dutt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1"&gt;Subhajit Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FastICARL: Fast Incremental Classifier and Representation Learning with Efficient Budget Allocation in Audio Sensing Applications. (arXiv:2106.07268v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07268</id>
        <link href="http://arxiv.org/abs/2106.07268"/>
        <updated>2021-06-28T01:57:57.407Z</updated>
        <summary type="html"><![CDATA[Various incremental learning (IL) approaches have been proposed to help deep
learning models learn new tasks/classes continuously without forgetting what
was learned previously (i.e., avoid catastrophic forgetting). With the growing
number of deployed audio sensing applications that need to dynamically
incorporate new tasks and changing input distribution from users, the ability
of IL on-device becomes essential for both efficiency and user privacy.

However, prior works suffer from high computational costs and storage demands
which hinders the deployment of IL on-device. In this work, to overcome these
limitations, we develop an end-to-end and on-device IL framework, FastICARL,
that incorporates an exemplar-based IL and quantization in the context of
audio-based applications. We first employ k-nearest-neighbor to reduce the
latency of IL. Then, we jointly utilize a quantization technique to decrease
the storage requirements of IL. We implement FastICARL on two types of mobile
devices and demonstrate that FastICARL remarkably decreases the IL time up to
78-92% and the storage requirements by 2-4 times without sacrificing its
performance. FastICARL enables complete on-device IL, ensuring user privacy as
the user data does not need to leave the device.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1"&gt;Young D. Kwon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chauhan_J/0/1/0/all/0/1"&gt;Jagmohan Chauhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mascolo_C/0/1/0/all/0/1"&gt;Cecilia Mascolo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Linear Temporal Properties from Noisy Data: A MaxSAT Approach. (arXiv:2104.15083v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.15083</id>
        <link href="http://arxiv.org/abs/2104.15083"/>
        <updated>2021-06-28T01:57:57.400Z</updated>
        <summary type="html"><![CDATA[We address the problem of inferring descriptions of system behavior using
Linear Temporal Logic (LTL) from a finite set of positive and negative
examples. Most of the existing approaches for solving such a task rely on
predefined templates for guiding the structure of the inferred formula. The
approaches that can infer arbitrary LTL formulas, on the other hand, are not
robust to noise in the data. To alleviate such limitations, we devise two
algorithms for inferring concise LTL formulas even in the presence of noise.
Our first algorithm infers minimal LTL formulas by reducing the inference
problem to a problem in maximum satisfiability and then using off-the-shelf
MaxSAT solvers to find a solution. To the best of our knowledge, we are the
first to incorporate the usage of MaxSAT solvers for inferring formulas in LTL.
Our second learning algorithm relies on the first algorithm to derive a
decision tree over LTL formulas based on a decision tree learning algorithm. We
have implemented both our algorithms and verified that our algorithms are
efficient in extracting concise LTL descriptions even in the presence of noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gaglione_J/0/1/0/all/0/1"&gt;Jean-Rapha&amp;#xeb;l Gaglione&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neider_D/0/1/0/all/0/1"&gt;Daniel Neider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1"&gt;Rajarshi Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1"&gt;Ufuk Topcu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhe Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tensor-based framework for training flexible neural networks. (arXiv:2106.13542v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13542</id>
        <link href="http://arxiv.org/abs/2106.13542"/>
        <updated>2021-06-28T01:57:57.385Z</updated>
        <summary type="html"><![CDATA[Activation functions (AFs) are an important part of the design of neural
networks (NNs), and their choice plays a predominant role in the performance of
a NN. In this work, we are particularly interested in the estimation of
flexible activation functions using tensor-based solutions, where the AFs are
expressed as a weighted sum of predefined basis functions. To do so, we propose
a new learning algorithm which solves a constrained coupled matrix-tensor
factorization (CMTF) problem. This technique fuses the first and zeroth order
information of the NN, where the first-order information is contained in a
Jacobian tensor, following a constrained canonical polyadic decomposition
(CPD). The proposed algorithm can handle different decomposition bases. The
goal of this method is to compress large pretrained NN models, by replacing
subnetworks, {\em i.e.,} one or multiple layers of the original network, by a
new flexible layer. The approach is applied to a pretrained convolutional
neural network (CNN) used for character classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zniyed_Y/0/1/0/all/0/1"&gt;Yassine Zniyed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usevich_K/0/1/0/all/0/1"&gt;Konstantin Usevich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miron_S/0/1/0/all/0/1"&gt;Sebastian Miron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brie_D/0/1/0/all/0/1"&gt;David Brie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sketching Datasets for Large-Scale Learning (long version). (arXiv:2008.01839v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.01839</id>
        <link href="http://arxiv.org/abs/2008.01839"/>
        <updated>2021-06-28T01:57:57.360Z</updated>
        <summary type="html"><![CDATA[This article considers "compressive learning," an approach to large-scale
machine learning where datasets are massively compressed before learning (e.g.,
clustering, classification, or regression) is performed. In particular, a
"sketch" is first constructed by computing carefully chosen nonlinear random
features (e.g., random Fourier features) and averaging them over the whole
dataset. Parameters are then learned from the sketch, without access to the
original dataset. This article surveys the current state-of-the-art in
compressive learning, including the main concepts and algorithms, their
connections with established signal-processing methods, existing theoretical
guarantees -- on both information preservation and privacy preservation, and
important open problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Gribonval_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Gribonval&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chatalic_A/0/1/0/all/0/1"&gt;Antoine Chatalic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Keriven_N/0/1/0/all/0/1"&gt;Nicolas Keriven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schellekens_V/0/1/0/all/0/1"&gt;Vincent Schellekens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jacques_L/0/1/0/all/0/1"&gt;Laurent Jacques&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schniter_P/0/1/0/all/0/1"&gt;Philip Schniter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transfer Learning in Bandits with Latent Continuity. (arXiv:2102.02472v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02472</id>
        <link href="http://arxiv.org/abs/2102.02472"/>
        <updated>2021-06-28T01:57:57.354Z</updated>
        <summary type="html"><![CDATA[Structured stochastic multi-armed bandits provide accelerated regret rates
over the standard unstructured bandit problems. Most structured bandits,
however, assume the knowledge of the structural parameter such as Lipschitz
continuity, which is often not available. To cope with the latent structural
parameter, we consider a transfer learning setting in which an agent must learn
to transfer the structural information from the prior tasks to the next task,
which is inspired by practical problems such as rate adaptation in wireless
link. We propose a novel framework to provably and accurately estimate the
Lipschitz constant based on previous tasks and fully exploit it for the new
task at hand. We analyze the efficiency of the proposed framework in two folds:
(i) the sample complexity of our estimator matches with the
information-theoretic fundamental limit; and (ii) our regret bound on the new
task is close to that of the oracle algorithm with the full knowledge of the
Lipschitz constant under mild assumptions. Our analysis reveals a set of useful
insights on transfer learning for latent Lipschitzconstants such as the
fundamental challenge a learner faces. Our numerical evaluations confirm our
theoretical findings and show the superiority of the proposed framework
compared to baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1"&gt;Hyejin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1"&gt;Seiyun Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jun_K/0/1/0/all/0/1"&gt;Kwang-Sung Jun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ok_J/0/1/0/all/0/1"&gt;Jungseul Ok&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dealing with Expert Bias in Collective Decision-Making. (arXiv:2106.13539v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.13539</id>
        <link href="http://arxiv.org/abs/2106.13539"/>
        <updated>2021-06-28T01:57:57.348Z</updated>
        <summary type="html"><![CDATA[Quite some real-world problems can be formulated as decision-making problems
wherein one must repeatedly make an appropriate choice from a set of
alternatives. Expert judgements, whether human or artificial, can help in
taking correct decisions, especially when exploration of alternative solutions
is costly. As expert opinions might deviate, the problem of finding the right
alternative can be approached as a collective decision making problem (CDM).
Current state-of-the-art approaches to solve CDM are limited by the quality of
the best expert in the group, and perform poorly if experts are not qualified
or if they are overly biased, thus potentially derailing the decision-making
process. In this paper, we propose a new algorithmic approach based on
contextual multi-armed bandit problems (CMAB) to identify and counteract such
biased expertises. We explore homogeneous, heterogeneous and polarised expert
groups and show that this approach is able to effectively exploit the
collective expertise, irrespective of whether the provided advice is directly
conducive to good performance, outperforming state-of-the-art methods,
especially when the quality of the provided expertise degrades. Our novel
CMAB-inspired approach achieves a higher final performance and does so while
converging more rapidly than previous adaptive algorithms, especially when
heterogeneous expertise is readily available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abels_A/0/1/0/all/0/1"&gt;Axel Abels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lenaerts_T/0/1/0/all/0/1"&gt;Tom Lenaerts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trianni_V/0/1/0/all/0/1"&gt;Vito Trianni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nowe_A/0/1/0/all/0/1"&gt;Ann Now&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Audio Attacks and Defenses against AED Systems -- A Practical Study. (arXiv:2106.07428v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07428</id>
        <link href="http://arxiv.org/abs/2106.07428"/>
        <updated>2021-06-28T01:57:57.340Z</updated>
        <summary type="html"><![CDATA[Audio Event Detection (AED) Systems capture audio from the environment and
employ some deep learning algorithms for detecting the presence of a specific
sound of interest. In this paper, we evaluate deep learning-based AED systems
against evasion attacks through adversarial examples. We run multiple security
critical AED tasks, implemented as CNNs classifiers, and then generate audio
adversarial examples using two different types of noise, namely background and
white noise, that can be used by the adversary to evade detection. We also
examine the robustness of existing third-party AED capable devices, such as
Nest devices manufactured by Google, which run their own black-box deep
learning models.

We show that an adversary can focus on audio adversarial inputs to cause AED
systems to misclassify, similarly to what has been previously done by works
focusing on adversarial examples from the image domain. We then, seek to
improve classifiers' robustness through countermeasures to the attacks. We
employ adversarial training and a custom denoising technique. We show that
these countermeasures, when applied to audio input, can be successful, either
in isolation or in combination, generating relevant increases of nearly fifty
percent in the performance of the classifiers when these are under attack.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Santos_R/0/1/0/all/0/1"&gt;Rodrigo dos Santos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nilizadeh_S/0/1/0/all/0/1"&gt;Shirin Nilizadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A mechanistic-based data-driven approach to accelerate structural topology optimization through finite element convolutional neural network (FE-CNN). (arXiv:2106.13652v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13652</id>
        <link href="http://arxiv.org/abs/2106.13652"/>
        <updated>2021-06-28T01:57:57.334Z</updated>
        <summary type="html"><![CDATA[In this paper, a mechanistic data-driven approach is proposed to accelerate
structural topology optimization, employing an in-house developed finite
element convolutional neural network (FE-CNN). Our approach can be divided into
two stages: offline training, and online optimization. During offline training,
a mapping function is built between high and low resolution representations of
a given design domain. The mapping is expressed by a FE-CNN, which targets a
common objective function value (e.g., structural compliance) across design
domains of differing resolutions. During online optimization, an arbitrary
design domain of high resolution is reduced to low resolution through the
trained mapping function. The original high-resolution domain is thus designed
by computations performed on only the low-resolution version, followed by an
inverse mapping back to the high-resolution domain. Numerical examples
demonstrate that this approach can accelerate optimization by up to an order of
magnitude in computational time. Our proposed approach therefore shows great
potential to overcome the curse-of-dimensionality incurred by density-based
structural topology optimization. The limitation of our present approach is
also discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yue_T/0/1/0/all/0/1"&gt;Tianle Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1"&gt;Zongliang Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elkhodary_K/0/1/0/all/0/1"&gt;Khalil I. Elkhodary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1"&gt;Shan Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xu Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A proximal-proximal majorization-minimization algorithm for nonconvex tuning-free robust regression problems. (arXiv:2106.13683v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.13683</id>
        <link href="http://arxiv.org/abs/2106.13683"/>
        <updated>2021-06-28T01:57:57.317Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce a proximal-proximal majorization-minimization
(PPMM) algorithm for nonconvex tuning-free robust regression problems. The
basic idea is to apply the proximal majorization-minimization algorithm to
solve the nonconvex problem with the inner subproblems solved by a sparse
semismooth Newton (SSN) method based proximal point algorithm (PPA). We must
emphasize that the main difficulty in the design of the algorithm lies in how
to overcome the singular difficulty of the inner subproblem. Furthermore, we
also prove that the PPMM algorithm converges to a d-stationary point. Due to
the Kurdyka-Lojasiewicz (KL) property of the problem, we present the
convergence rate of the PPMM algorithm. Numerical experiments demonstrate that
our proposed algorithm outperforms the existing state-of-the-art algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Tang_P/0/1/0/all/0/1"&gt;Peipei Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengjing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jiang_B/0/1/0/all/0/1"&gt;Bo Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Graph Augmentation to Improve Graph Contrastive Learning. (arXiv:2106.05819v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05819</id>
        <link href="http://arxiv.org/abs/2106.05819"/>
        <updated>2021-06-28T01:57:57.310Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning of graph neural networks (GNN) is in great need
because of the widespread label scarcity issue in real-world graph/network
data. Graph contrastive learning (GCL), by training GNNs to maximize the
correspondence between the representations of the same graph in its different
augmented forms, may yield robust and transferable GNNs even without using
labels. However, GNNs trained by traditional GCL often risk capturing redundant
graph features and thus may be brittle and provide sub-par performance in
downstream tasks. Here, we propose a novel principle, termed adversarial-GCL
(AD-GCL), which enables GNNs to avoid capturing redundant information during
the training by optimizing adversarial graph augmentation strategies used in
GCL. We pair AD-GCL with theoretical explanations and design a practical
instantiation based on trainable edge-dropping graph augmentation. We
experimentally validate AD-GCL by comparing with the state-of-the-art GCL
methods and achieve performance gains of up-to $14\%$ in unsupervised, $6\%$ in
transfer, and $3\%$ in semi-supervised learning settings overall with 18
different benchmark datasets for the tasks of molecule property regression and
classification, and social network classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suresh_S/0/1/0/all/0/1"&gt;Susheel Suresh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_C/0/1/0/all/0/1"&gt;Cong Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neville_J/0/1/0/all/0/1"&gt;Jennifer Neville&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Learning with Multifidelity Modeling for Efficient Rare Event Simulation. (arXiv:2106.13790v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.13790</id>
        <link href="http://arxiv.org/abs/2106.13790"/>
        <updated>2021-06-28T01:57:57.302Z</updated>
        <summary type="html"><![CDATA[While multifidelity modeling provides a cost-effective way to conduct
uncertainty quantification with computationally expensive models, much greater
efficiency can be achieved by adaptively deciding the number of required
high-fidelity (HF) simulations, depending on the type and complexity of the
problem and the desired accuracy in the results. We propose a framework for
active learning with multifidelity modeling emphasizing the efficient
estimation of rare events. Our framework works by fusing a low-fidelity (LF)
prediction with an HF-inferred correction, filtering the corrected LF
prediction to decide whether to call the high-fidelity model, and for enhanced
subsequent accuracy, adapting the correction for the LF prediction after every
HF model call. The framework does not make any assumptions as to the LF model
type or its correlations with the HF model. In addition, for improved
robustness when estimating smaller failure probabilities, we propose using
dynamic active learning functions that decide when to call the HF model. We
demonstrate our framework using several academic case studies and two finite
element (FE) model case studies: estimating Navier-Stokes velocities using the
Stokes approximation and estimating stresses in a transversely isotropic model
subjected to displacements via a coarsely meshed isotropic model. Across these
case studies, not only did the proposed framework estimate the failure
probabilities accurately, but compared with either Monte Carlo or a standard
variance reduction method, it also required only a small fraction of the calls
to the HF model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Dhulipala_S/0/1/0/all/0/1"&gt;S. L. N. Dhulipala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shields_M/0/1/0/all/0/1"&gt;M. D. Shields&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Spencer_B/0/1/0/all/0/1"&gt;B. W. Spencer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bolisetti_C/0/1/0/all/0/1"&gt;C. Bolisetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Slaughter_A/0/1/0/all/0/1"&gt;A. E. Slaughter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Laboure_V/0/1/0/all/0/1"&gt;V. M. Laboure&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chakroborty_P/0/1/0/all/0/1"&gt;P. Chakroborty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Self-Learning Framework for Bladder Cancer Grading Using Histopathological Images. (arXiv:2106.13559v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.13559</id>
        <link href="http://arxiv.org/abs/2106.13559"/>
        <updated>2021-06-28T01:57:57.294Z</updated>
        <summary type="html"><![CDATA[Recently, bladder cancer has been significantly increased in terms of
incidence and mortality. Currently, two subtypes are known based on tumour
growth: non-muscle invasive (NMIBC) and muscle-invasive bladder cancer (MIBC).
In this work, we focus on the MIBC subtype because it is of the worst prognosis
and can spread to adjacent organs. We present a self-learning framework to
grade bladder cancer from histological images stained via immunohistochemical
techniques. Specifically, we propose a novel Deep Convolutional Embedded
Attention Clustering (DCEAC) which allows classifying histological patches into
different severity levels of the disease, according to the patterns established
in the literature. The proposed DCEAC model follows a two-step fully
unsupervised learning methodology to discern between non-tumour, mild and
infiltrative patterns from high-resolution samples of 512x512 pixels. Our
system outperforms previous clustering-based methods by including a
convolutional attention module, which allows refining the features of the
latent space before the classification stage. The proposed network exceeds
state-of-the-art approaches by 2-3% across different metrics, achieving a final
average accuracy of 0.9034 in a multi-class scenario. Furthermore, the reported
class activation maps evidence that our model is able to learn by itself the
same patterns that clinicians consider relevant, without incurring prior
annotation steps. This fact supposes a breakthrough in muscle-invasive bladder
cancer grading which bridges the gap with respect to train the model on
labelled data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Garcia_G/0/1/0/all/0/1"&gt;Gabriel Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Esteve_A/0/1/0/all/0/1"&gt;Anna Esteve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Colomer_A/0/1/0/all/0/1"&gt;Adri&amp;#xe1;n Colomer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ramos_D/0/1/0/all/0/1"&gt;David Ramos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1"&gt;Valery Naranjo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Inertial Block Majorization Minimization Framework for Nonsmooth Nonconvex Optimization. (arXiv:2010.12133v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12133</id>
        <link href="http://arxiv.org/abs/2010.12133"/>
        <updated>2021-06-28T01:57:57.286Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce TITAN, a novel inerTIal block majorizaTion
minimizAtioN framework for non-smooth non-convex optimization problems. To the
best of our knowledge, TITAN is the first framework of block-coordinate update
method that relies on the majorization-minimization framework while embedding
inertial force to each step of the block updates. The inertial force is
obtained via an extrapolation operator that subsumes heavy-ball and
Nesterov-type accelerations for block proximal gradient methods as special
cases. By choosing various surrogate functions, such as proximal, Lipschitz
gradient, Bregman, quadratic, and composite surrogate functions, and by varying
the extrapolation operator, TITAN produces a rich set of inertial
block-coordinate update methods. We study sub-sequential convergence as well as
global convergence for the generated sequence of TITAN. We illustrate the
effectiveness of TITAN on two important machine learning problems, namely
sparse non-negative matrix factorization and matrix completion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Hien_L/0/1/0/all/0/1"&gt;Le Thi Khanh Hien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Phan_D/0/1/0/all/0/1"&gt;Duy Nhat Phan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gillis_N/0/1/0/all/0/1"&gt;Nicolas Gillis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Counterfactual Explanations in Tree Ensembles. (arXiv:2106.06631v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06631</id>
        <link href="http://arxiv.org/abs/2106.06631"/>
        <updated>2021-06-28T01:57:57.258Z</updated>
        <summary type="html"><![CDATA[Counterfactual explanations are usually generated through heuristics that are
sensitive to the search's initial conditions. The absence of guarantees of
performance and robustness hinders trustworthiness. In this paper, we take a
disciplined approach towards counterfactual explanations for tree ensembles. We
advocate for a model-based search aiming at "optimal" explanations and propose
efficient mixed-integer programming approaches. We show that isolation forests
can be modeled within our framework to focus the search on plausible
explanations with a low outlier score. We provide comprehensive coverage of
additional constraints that model important objectives, heterogeneous data
types, structural constraints on the feature space, along with resource and
actionability restrictions. Our experimental analyses demonstrate that the
proposed search approach requires a computational effort that is orders of
magnitude smaller than previous mathematical programming algorithms. It scales
up to large data sets and tree ensembles, where it provides, within seconds,
systematic explanations grounded on well-defined models solved to optimality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parmentier_A/0/1/0/all/0/1"&gt;Axel Parmentier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vidal_T/0/1/0/all/0/1"&gt;Thibaut Vidal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CADDA: Class-wise Automatic Differentiable Data Augmentation for EEG Signals. (arXiv:2106.13695v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13695</id>
        <link href="http://arxiv.org/abs/2106.13695"/>
        <updated>2021-06-28T01:57:57.252Z</updated>
        <summary type="html"><![CDATA[Data augmentation is a key element of deep learning pipelines, as it informs
the network during training about transformations of the input data that keep
the label unchanged. Manually finding adequate augmentation methods and
parameters for a given pipeline is however rapidly cumbersome. In particular,
while intuition can guide this decision for images, the design and choice of
augmentation policies remains unclear for more complex types of data, such as
neuroscience signals. Moreover, label independent strategies might not be
suitable for such structured data and class-dependent augmentations might be
necessary. This idea has been surprisingly unexplored in the literature, while
it is quite intuitive: changing the color of a car image does not change the
object class to be predicted, but doing the same to the picture of an orange
does. This paper aims to increase the generalization power added through
class-wise data augmentation. Yet, as seeking transformations depending on the
class largely increases the complexity of the task, using gradient-free
optimization techniques as done by most existing automatic approaches becomes
intractable for real-world datasets. For this reason we propose to use
differentiable data augmentation amenable to gradient-based learning. EEG
signals are a perfect example of data for which good augmentation policies are
mostly unknown. In this work, we demonstrate the relevance of our approach on
the clinically relevant sleep staging classification task, for which we also
propose differentiable transformations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rommel_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric Rommel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreau_T/0/1/0/all/0/1"&gt;Thomas Moreau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gramfort_A/0/1/0/all/0/1"&gt;Alexandre Gramfort&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Self-Attentive Gated RNNs for Real-Time Speaker Separation. (arXiv:2106.13493v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.13493</id>
        <link href="http://arxiv.org/abs/2106.13493"/>
        <updated>2021-06-28T01:57:57.243Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have recently shown great success in the task of blind
source separation, both under monaural and binaural settings. Although these
methods were shown to produce high-quality separations, they were mainly
applied under offline settings, in which the model has access to the full input
signal while separating the signal. In this study, we convert a non-causal
state-of-the-art separation model into a causal and real-time model and
evaluate its performance under both online and offline settings. We compare the
performance of the proposed model to several baseline methods under anechoic,
noisy, and noisy-reverberant recording conditions while exploring both monaural
and binaural inputs and outputs. Our findings shed light on the relative
difference between causal and non-causal models when performing separation. Our
stateful implementation for online separation leads to a minor drop in
performance compared to the offline model; 0.8dB for monaural inputs and 0.3dB
for binaural inputs while reaching a real-time factor of 0.65. Samples can be
found under the following link:
https://kwanum.github.io/sagrnnc-stream-results/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kabeli_O/0/1/0/all/0/1"&gt;Ori Kabeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Adi_Y/0/1/0/all/0/1"&gt;Yossi Adi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zhenyu Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_B/0/1/0/all/0/1"&gt;Buye Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Anurag Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustness and Generalization to Nearest Categories. (arXiv:2011.08485v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08485</id>
        <link href="http://arxiv.org/abs/2011.08485"/>
        <updated>2021-06-28T01:57:57.235Z</updated>
        <summary type="html"><![CDATA[Adversarial robustness has emerged as a desirable property for neural
networks. Prior work shows that robust networks perform well in some
out-of-distribution generalization tasks, such as transfer learning and outlier
detection. We uncover a different kind of out-of-distribution generalization
property of such networks, and find that they also do well in a task that we
call nearest category generalization (NCG) - given an out-of-distribution
input, they tend to predict the same label as that of the closest training
example. We empirically show that this happens even when the
out-of-distribution inputs lie outside the robustness radius of the training
data, which suggests that these networks may generalize better along unseen
directions on the natural image manifold than arbitrary unseen directions. We
examine how performance changes when we change the robustness regions during
training. We then design experiments to investigate the connection between
out-of-distribution detection and nearest category generalization. Taken
together, our work provides evidence that robust neural networks may resemble
nearest neighbor classifiers in their behavior on out-of-distribution data. The
code is available at
https://github.com/yangarbiter/nearest-category-generalization]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yao-Yuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rashtchian_C/0/1/0/all/0/1"&gt;Cyrus Rashtchian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_K/0/1/0/all/0/1"&gt;Kamalika Chaudhuri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised embedding of trajectories captures the latent structure of mobility. (arXiv:2012.02785v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02785</id>
        <link href="http://arxiv.org/abs/2012.02785"/>
        <updated>2021-06-28T01:57:57.225Z</updated>
        <summary type="html"><![CDATA[Human mobility drives major societal phenomena including epidemics,
economies, and innovation. Historically, mobility was constrained by geographic
distance, however, in the globalizing world, language, culture, and history are
increasingly important. We propose using the neural embedding model word2vec
for studying mobility and capturing its complexity. Word2ec is shown to be
mathematically equivalent to the gravity model of mobility, and using three
human trajectory datasets, we demonstrate that it encodes nuanced relationships
between locations into a vector-space, providing a measure of effective
distance that outperforms baselines. Focusing on the case of scientific
mobility, we show that embeddings uncover cultural, linguistic, and
hierarchical relationships at multiple levels of granularity. Connecting neural
embeddings to the gravity model opens up new avenues for the study of mobility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Murray_D/0/1/0/all/0/1"&gt;Dakota Murray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1"&gt;Jisung Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kojaku_S/0/1/0/all/0/1"&gt;Sadamori Kojaku&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Costas_R/0/1/0/all/0/1"&gt;Rodrigo Costas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_W/0/1/0/all/0/1"&gt;Woo-Sung Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milojevic_S/0/1/0/all/0/1"&gt;Sta&amp;#x161;a Milojevi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahn_Y/0/1/0/all/0/1"&gt;Yong-Yeol Ahn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-stationary Online Learning with Memory and Non-stochastic Control. (arXiv:2102.03758v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03758</id>
        <link href="http://arxiv.org/abs/2102.03758"/>
        <updated>2021-06-28T01:57:57.207Z</updated>
        <summary type="html"><![CDATA[We study the problem of Online Convex Optimization (OCO) with memory, which
allows loss functions to depend on past decisions and thus captures temporal
effects of learning problems. In this paper, we introduce dynamic policy regret
as the performance measure to design algorithms robust to non-stationary
environments, which competes algorithms' decisions with a sequence of changing
comparators. We propose a novel algorithm for OCO with memory that provably
enjoys an optimal dynamic policy regret. The key technical challenge is how to
control the switching cost, the cumulative movements of player's decisions,
which is neatly addressed by a novel decomposition of dynamic policy regret and
an appropriate meta-expert structure. Furthermore, we apply the results to the
problem of online non-stochastic control, i.e., controlling a linear dynamical
system with adversarial disturbance and convex loss functions. We derive a
novel gradient-based controller with dynamic policy regret guarantees, which is
the first controller competitive to a sequence of changing policies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Peng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu-Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhi-Hua Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Competitive Memory: A Neural System for Online Task-Free Lifelong Learning. (arXiv:2106.13300v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13300</id>
        <link href="http://arxiv.org/abs/2106.13300"/>
        <updated>2021-06-28T01:57:57.199Z</updated>
        <summary type="html"><![CDATA[In this article, we propose a novel form of unsupervised learning, continual
competitive memory (CCM), as well as a computational framework to unify related
neural models that operate under the principles of competition. The resulting
neural system is shown to offer an effective approach for combating
catastrophic forgetting in online continual classification problems. We
demonstrate that the proposed CCM system not only outperforms other competitive
learning neural models but also yields performance that is competitive with
several modern, state-of-the-art lifelong learning approaches on benchmarks
such as Split MNIST and Split NotMNIST. CCM yields a promising path forward for
acquiring representations that are robust to interference from data streams,
especially when the task is unknown to the model and must be inferred without
external guidance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ororbia_A/0/1/0/all/0/1"&gt;Alexander G. Ororbia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Sparse Expert Models and Beyond. (arXiv:2105.15082v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15082</id>
        <link href="http://arxiv.org/abs/2105.15082"/>
        <updated>2021-06-28T01:57:57.192Z</updated>
        <summary type="html"><![CDATA[Mixture-of-Experts (MoE) models can achieve promising results with outrageous
large amount of parameters but constant computation cost, and thus it has
become a trend in model scaling. Still it is a mystery how MoE layers bring
quality gains by leveraging the parameters with sparse activation. In this
work, we investigate several key factors in sparse expert models. We observe
that load imbalance may not be a significant problem affecting model quality,
contrary to the perspectives of recent studies, while the number of sparsely
activated experts $k$ and expert capacity $C$ in top-$k$ routing can
significantly make a difference in this context. Furthermore, we take a step
forward to propose a simple method called expert prototyping that splits
experts into different prototypes and applies $k$ top-$1$ routing. This
strategy improves the model quality but maintains constant computational costs,
and our further exploration on extremely large-scale models reflects that it is
more effective in training larger models. We push the model scale to over $1$
trillion parameters and implement it on solely $480$ NVIDIA V100-32GB GPUs, in
comparison with the recent SOTAs on $2048$ TPU cores. The proposed giant model
achieves substantial speedup in convergence over the same-size baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1"&gt;An Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Junyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1"&gt;Rui Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Le Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xianyan Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1"&gt;Ang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiamang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Di Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1"&gt;Lin Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shape registration in the time of transformers. (arXiv:2106.13679v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13679</id>
        <link href="http://arxiv.org/abs/2106.13679"/>
        <updated>2021-06-28T01:57:57.183Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a transformer-based procedure for the efficient
registration of non-rigid 3D point clouds. The proposed approach is data-driven
and adopts for the first time the transformer architecture in the registration
task. Our method is general and applies to different settings. Given a fixed
template with some desired properties (e.g. skinning weights or other animation
cues), we can register raw acquired data to it, thereby transferring all the
template properties to the input geometry. Alternatively, given a pair of
shapes, our method can register the first onto the second (or vice-versa),
obtaining a high-quality dense correspondence between the two. In both
contexts, the quality of our results enables us to target real applications
such as texture transfer and shape interpolation. Furthermore, we also show
that including an estimation of the underlying density of the surface eases the
learning process. By exploiting the potential of this architecture, we can
train our model requiring only a sparse set of ground truth correspondences
($10\sim20\%$ of the total points). The proposed model and the analysis that we
perform pave the way for future exploration of transformer-based architectures
for registration and matching applications. Qualitative and quantitative
evaluations demonstrate that our pipeline outperforms state-of-the-art methods
for deformable and unordered 3D data registration on different datasets and
scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trappolini_G/0/1/0/all/0/1"&gt;Giovanni Trappolini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cosmo_L/0/1/0/all/0/1"&gt;Luca Cosmo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moschella_L/0/1/0/all/0/1"&gt;Luca Moschella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marin_R/0/1/0/all/0/1"&gt;Riccardo Marin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1"&gt;Emanuele Rodol&amp;#xe0;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Session-aware Linear Item-Item Models for Session-based Recommendation. (arXiv:2103.16104v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16104</id>
        <link href="http://arxiv.org/abs/2103.16104"/>
        <updated>2021-06-28T01:57:57.173Z</updated>
        <summary type="html"><![CDATA[Session-based recommendation aims at predicting the next item given a
sequence of previous items consumed in the session, e.g., on e-commerce or
multimedia streaming services. Specifically, session data exhibits some unique
characteristics, i.e., session consistency and sequential dependency over items
within the session, repeated item consumption, and session timeliness. In this
paper, we propose simple-yet-effective linear models for considering the
holistic aspects of the sessions. The comprehensive nature of our models helps
improve the quality of session-based recommendation. More importantly, it
provides a generalized framework for reflecting different perspectives of
session data. Furthermore, since our models can be solved by closed-form
solutions, they are highly scalable. Experimental results demonstrate that the
proposed linear models show competitive or state-of-the-art performance in
various metrics on several real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1"&gt;Minjin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_j/0/1/0/all/0/1"&gt;jinhong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Joonseok Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shim_H/0/1/0/all/0/1"&gt;Hyunjung Shim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jongwuk Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prior Image-Constrained Reconstruction using Style-Based Generative Models. (arXiv:2102.12525v2 [eess.IV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2102.12525</id>
        <link href="http://arxiv.org/abs/2102.12525"/>
        <updated>2021-06-28T01:57:57.154Z</updated>
        <summary type="html"><![CDATA[Obtaining a useful estimate of an object from highly incomplete imaging
measurements remains a holy grail of imaging science. Deep learning methods
have shown promise in learning object priors or constraints to improve the
conditioning of an ill-posed imaging inverse problem. In this study, a
framework for estimating an object of interest that is semantically related to
a known prior image, is proposed. An optimization problem is formulated in the
disentangled latent space of a style-based generative model, and semantically
meaningful constraints are imposed using the disentangled latent representation
of the prior image. Stable recovery from incomplete measurements with the help
of a prior image is theoretically analyzed. Numerical experiments demonstrating
the superior performance of our approach as compared to related methods are
presented.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kelkar_V/0/1/0/all/0/1"&gt;Varun A. Kelkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Anastasio_M/0/1/0/all/0/1"&gt;Mark A. Anastasio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantitative approximation results for complex-valued neural networks. (arXiv:2102.13092v2 [math.FA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13092</id>
        <link href="http://arxiv.org/abs/2102.13092"/>
        <updated>2021-06-28T01:57:57.147Z</updated>
        <summary type="html"><![CDATA[Until recently, applications of neural networks in machine learning have
almost exclusively relied on real-valued networks. It was recently observed,
however, that complex-valued neural networks (CVNNs) exhibit superior
performance in applications in which the input is naturally complex-valued,
such as MRI fingerprinting. While the mathematical theory of real-valued
networks has, by now, reached some level of maturity, this is far from true for
complex-valued networks. In this paper, we analyze the expressivity of
complex-valued networks by providing explicit quantitative error bounds for
approximating $C^n$ functions on compact subsets of $\mathbb{C}^d$ by
complex-valued neural networks that employ the modReLU activation function,
given by $\sigma(z) = \mathrm{ReLU}(|z| - 1) \, \mathrm{sgn} (z)$, which is one
of the most popular complex activation functions used in practice. We show that
the derived approximation rates are optimal (up to log factors) in the class of
modReLU networks with weights of moderate growth.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Caragea_A/0/1/0/all/0/1"&gt;A. Caragea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Lee_D/0/1/0/all/0/1"&gt;D.G. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Maly_J/0/1/0/all/0/1"&gt;J. Maly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Pfander_G/0/1/0/all/0/1"&gt;G. Pfander&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Voigtlaender_F/0/1/0/all/0/1"&gt;F. Voigtlaender&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast quantum state reconstruction via accelerated non-convex programming. (arXiv:2104.07006v3 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07006</id>
        <link href="http://arxiv.org/abs/2104.07006"/>
        <updated>2021-06-28T01:57:57.141Z</updated>
        <summary type="html"><![CDATA[We propose a new quantum state reconstruction method that combines ideas from
compressed sensing, non-convex optimization, and acceleration methods. The
algorithm, called Momentum-Inspired Factored Gradient Descent (\texttt{MiFGD}),
extends the applicability of quantum tomography for larger systems. Despite
being a non-convex method, \texttt{MiFGD} converges \emph{provably} to the true
density matrix at a linear rate, in the absence of experimental and statistical
noise, and under common assumptions. With this manuscript, we present the
method, prove its convergence property and provide Frobenius norm bound
guarantees with respect to the true density matrix. From a practical point of
view, we benchmark the algorithm performance with respect to other existing
methods, in both synthetic and real experiments performed on an IBM's quantum
processing unit. We find that the proposed algorithm performs orders of
magnitude faster than state of the art approaches, with the same or better
accuracy. In both synthetic and real experiments, we observed accurate and
robust reconstruction, despite experimental and statistical noise in the
tomographic data. Finally, we provide a ready-to-use code for state tomography
of multi-qubit systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kim_J/0/1/0/all/0/1"&gt;Junhyung Lyle Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kollias_G/0/1/0/all/0/1"&gt;George Kollias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kalev_A/0/1/0/all/0/1"&gt;Amir Kalev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Wei_K/0/1/0/all/0/1"&gt;Ken X. Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kyrillidis_A/0/1/0/all/0/1"&gt;Anastasios Kyrillidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Masksembles for Uncertainty Estimation. (arXiv:2012.08334v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08334</id>
        <link href="http://arxiv.org/abs/2012.08334"/>
        <updated>2021-06-28T01:57:57.134Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have amply demonstrated their prowess but estimating the
reliability of their predictions remains challenging. Deep Ensembles are widely
considered as being one of the best methods for generating uncertainty
estimates but are very expensive to train and evaluate. MC-Dropout is another
popular alternative, which is less expensive, but also less reliable. Our
central intuition is that there is a continuous spectrum of ensemble-like
models of which MC-Dropout and Deep Ensembles are extreme examples. The first
uses an effectively infinite number of highly correlated models while the
second relies on a finite number of independent models.

To combine the benefits of both, we introduce Masksembles. Instead of
randomly dropping parts of the network as in MC-dropout, Masksemble relies on a
fixed number of binary masks, which are parameterized in a way that allows to
change correlations between individual models. Namely, by controlling the
overlap between the masks and their density one can choose the optimal
configuration for the task at hand. This leads to a simple and easy to
implement method with performance on par with Ensembles at a fraction of the
cost. We experimentally validate Masksembles on two widely used datasets,
CIFAR10 and ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Durasov_N/0/1/0/all/0/1"&gt;Nikita Durasov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1"&gt;Timur Bagautdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baque_P/0/1/0/all/0/1"&gt;Pierre Baque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1"&gt;Pascal Fua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fed-NILM: A Federated Learning-based Non-Intrusive Load Monitoring Method for Privacy-Protection. (arXiv:2105.11085v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11085</id>
        <link href="http://arxiv.org/abs/2105.11085"/>
        <updated>2021-06-28T01:57:57.126Z</updated>
        <summary type="html"><![CDATA[Non-intrusive load monitoring (NILM) is essential for understanding
customer's power consumption patterns and may find wide applications like
carbon emission reduction and energy conservation. The training of NILM models
requires massive load data containing different types of appliances. However,
inadequate load data and the risk of power consumer privacy breaches may be
encountered by local data owners during the NILM model training. To prevent
such potential risks, a novel NILM method named Fed-NILM which is based on
Federated Learning (FL) is proposed in this paper. In Fed-NILM, local model
parameters instead of local load data are shared among multiple data owners.
The global model is obtained by weighted averaging the parameters. Experiments
based on two measured load datasets are conducted to explore the generalization
ability of Fed-NILM. Besides, a comparison of Fed-NILM with locally-trained
NILMs and the centrally-trained NILM is conducted. The experimental results
show that Fed-NILM has superior performance in scalability and convergence.
Fed-NILM outperforms locally-trained NILMs operated by local data owners and
approximates the centrally-trained NILM which is trained on the entire load
dataset without privacy protection. The proposed Fed-NILM significantly
improves the co-modeling capabilities of local data owners while protecting
power consumers' privacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haijin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1"&gt;Caomingzhe Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Junhua Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guolong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1"&gt;Fushuan Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning a Probabilistic Relaxation of Discrete Variables for Soft Detection with Low Complexity: CMDNet. (arXiv:2102.12756v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12756</id>
        <link href="http://arxiv.org/abs/2102.12756"/>
        <updated>2021-06-28T01:57:57.120Z</updated>
        <summary type="html"><![CDATA[Following the great success of Machine Learning (ML), especially Deep Neural
Networks (DNNs), in many research domains in 2010s, several ML-based approaches
were proposed for detection in large inverse linear problems, e.g., massive
MIMO systems. The main motivation behind is that the complexity of Maximum
A-Posteriori (MAP) detection grows exponentially with system dimensions.
Instead of using DNNs, essentially being a black-box, we take a slightly
different approach and introduce a probabilistic Continuous relaxation of
disCrete variables to MAP detection. Enabling close approximation and
continuous optimization, we derive an iterative detection algorithm: Concrete
MAP Detection (CMD). Furthermore, extending CMD by the idea of deep unfolding
into CMDNet, we allow for (online) optimization of a small number of parameters
to different working points while limiting complexity. In contrast to recent
DNN-based approaches, we select the optimization criterion and output of CMDNet
based on information theory and are thus able to learn approximate
probabilities of the individual optimal detector. This is crucial for soft
decoding in today's communication systems. Numerical simulation results in MIMO
systems reveal CMDNet to feature a promising accuracy complexity trade-off
compared to State of the Art. Notably, we demonstrate CMDNet's soft outputs to
be reliable for decoders.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Beck_E/0/1/0/all/0/1"&gt;Edgar Beck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bockelmann_C/0/1/0/all/0/1"&gt;Carsten Bockelmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dekorsy_A/0/1/0/all/0/1"&gt;Armin Dekorsy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From inexact optimization to learning via gradient concentration. (arXiv:2106.05397v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05397</id>
        <link href="http://arxiv.org/abs/2106.05397"/>
        <updated>2021-06-28T01:57:57.100Z</updated>
        <summary type="html"><![CDATA[Optimization was recently shown to control the inductive bias in a learning
process, a property referred to as implicit, or iterative regularization. The
estimator obtained iteratively minimizing the training error can generalise
well with no need of further penalties or constraints. In this paper, we
investigate this phenomenon in the context of linear models with smooth loss
functions. In particular, we investigate and propose a proof technique
combining ideas from inexact optimization and probability theory, specifically
gradient concentration. The proof is easy to follow and allows to obtain sharp
learning bounds. More generally, it highlights a way to develop optimization
results into learning guarantees.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Stankewitz_B/0/1/0/all/0/1"&gt;Bernhard Stankewitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mucke_N/0/1/0/all/0/1"&gt;Nicole M&amp;#xfc;cke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rosasco_L/0/1/0/all/0/1"&gt;Lorenzo Rosasco&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness in the Eyes of the Data: Certifying Machine-Learning Models. (arXiv:2009.01534v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.01534</id>
        <link href="http://arxiv.org/abs/2009.01534"/>
        <updated>2021-06-28T01:57:57.083Z</updated>
        <summary type="html"><![CDATA[We present a framework that allows to certify the fairness degree of a model
based on an interactive and privacy-preserving test. The framework verifies any
trained model, regardless of its training process and architecture. Thus, it
allows us to evaluate any deep learning model on multiple fairness definitions
empirically. We tackle two scenarios, where either the test data is privately
available only to the tester or is publicly known in advance, even to the model
creator. We investigate the soundness of the proposed approach using
theoretical analysis and present statistical guarantees for the interactive
test. Finally, we provide a cryptographic technique to automate fairness
testing and certified inference with only black-box access to the model at hand
while hiding the participants' sensitive data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Segal_S/0/1/0/all/0/1"&gt;Shahar Segal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1"&gt;Yossi Adi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pinkas_B/0/1/0/all/0/1"&gt;Benny Pinkas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baum_C/0/1/0/all/0/1"&gt;Carsten Baum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganesh_C/0/1/0/all/0/1"&gt;Chaya Ganesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keshet_J/0/1/0/all/0/1"&gt;Joseph Keshet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HyperNP: Interactive Visual Exploration of Multidimensional Projection Hyperparameters. (arXiv:2106.13777v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13777</id>
        <link href="http://arxiv.org/abs/2106.13777"/>
        <updated>2021-06-28T01:57:57.070Z</updated>
        <summary type="html"><![CDATA[Projection algorithms such as t-SNE or UMAP are useful for the visualization
of high dimensional data, but depend on hyperparameters which must be tuned
carefully. Unfortunately, iteratively recomputing projections to find the
optimal hyperparameter value is computationally intensive and unintuitive due
to the stochastic nature of these methods. In this paper we propose HyperNP, a
scalable method that allows for real-time interactive hyperparameter
exploration of projection methods by training neural network approximations.
HyperNP can be trained on a fraction of the total data instances and
hyperparameter configurations and can compute projections for new data and
hyperparameters at interactive speeds. HyperNP is compact in size and fast to
compute, thus allowing it to be embedded in lightweight visualization systems
such as web browsers. We evaluate the performance of the HyperNP across three
datasets in terms of performance and speed. The results suggest that HyperNP is
accurate, scalable, interactive, and appropriate for use in real-world
settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Appleby_G/0/1/0/all/0/1"&gt;Gabriel Appleby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Espadoto_M/0/1/0/all/0/1"&gt;Mateus Espadoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1"&gt;Rui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goree_S/0/1/0/all/0/1"&gt;Samuel Goree&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Telea_A/0/1/0/all/0/1"&gt;Alexandru Telea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_E/0/1/0/all/0/1"&gt;Erik W Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_R/0/1/0/all/0/1"&gt;Remco Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized Multimodal ELBO. (arXiv:2105.02470v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02470</id>
        <link href="http://arxiv.org/abs/2105.02470"/>
        <updated>2021-06-28T01:57:57.063Z</updated>
        <summary type="html"><![CDATA[Multiple data types naturally co-occur when describing real-world phenomena
and learning from them is a long-standing goal in machine learning research.
However, existing self-supervised generative models approximating an ELBO are
not able to fulfill all desired requirements of multimodal models: their
posterior approximation functions lead to a trade-off between the semantic
coherence and the ability to learn the joint data distribution. We propose a
new, generalized ELBO formulation for multimodal data that overcomes these
limitations. The new objective encompasses two previous methods as special
cases and combines their benefits without compromises. In extensive
experiments, we demonstrate the advantage of the proposed method compared to
state-of-the-art models in self-supervised, generative learning tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sutter_T/0/1/0/all/0/1"&gt;Thomas M. Sutter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daunhawer_I/0/1/0/all/0/1"&gt;Imant Daunhawer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogt_J/0/1/0/all/0/1"&gt;Julia E. Vogt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NSL: Hybrid Interpretable Learning From Noisy Raw Data. (arXiv:2012.05023v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05023</id>
        <link href="http://arxiv.org/abs/2012.05023"/>
        <updated>2021-06-28T01:57:57.025Z</updated>
        <summary type="html"><![CDATA[Inductive Logic Programming (ILP) systems learn generalised, interpretable
rules in a data-efficient manner utilising existing background knowledge.
However, current ILP systems require training examples to be specified in a
structured logical format. Neural networks learn from unstructured data,
although their learned models may be difficult to interpret and are vulnerable
to data perturbations at run-time. This paper introduces a hybrid
neural-symbolic learning framework, called NSL, that learns interpretable rules
from labelled unstructured data. NSL combines pre-trained neural networks for
feature extraction with FastLAS, a state-of-the-art ILP system for rule
learning under the answer set semantics. Features extracted by the neural
components define the structured context of labelled examples and the
confidence of the neural predictions determines the level of noise of the
examples. Using the scoring function of FastLAS, NSL searches for short,
interpretable rules that generalise over such noisy examples. We evaluate our
framework on propositional and first-order classification tasks using the MNIST
dataset as raw data. Specifically, we demonstrate that NSL is able to learn
robust rules from perturbed MNIST data and achieve comparable or superior
accuracy when compared to neural network and random forest baselines whilst
being more general and interpretable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cunnington_D/0/1/0/all/0/1"&gt;Daniel Cunnington&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russo_A/0/1/0/all/0/1"&gt;Alessandra Russo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1"&gt;Mark Law&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lobo_J/0/1/0/all/0/1"&gt;Jorge Lobo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaplan_L/0/1/0/all/0/1"&gt;Lance Kaplan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the Origin of Information-Seeking Exploration in Probabilistic Objectives for Control. (arXiv:2103.06859v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06859</id>
        <link href="http://arxiv.org/abs/2103.06859"/>
        <updated>2021-06-28T01:57:57.018Z</updated>
        <summary type="html"><![CDATA[The exploration-exploitation trade-off is central to the description of
adaptive behaviour in fields ranging from machine learning, to biology, to
economics. While many approaches have been taken, one approach to solving this
trade-off has been to equip or propose that agents possess an intrinsic
'exploratory drive' which is often implemented in terms of maximizing the
agents information gain about the world -- an approach which has been widely
studied in machine learning and cognitive science. In this paper we
mathematically investigate the nature and meaning of such approaches and
demonstrate that this combination of utility maximizing and information-seeking
behaviour arises from the minimization of an entirely difference class of
objectives we call divergence objectives. We propose a dichotomy in the
objective functions underlying adaptive behaviour between \emph{evidence}
objectives, which correspond to well-known reward or utility maximizing
objectives in the literature, and \emph{divergence} objectives which instead
seek to minimize the divergence between the agent's expected and desired
futures, and argue that this new class of divergence objectives could form the
mathematical foundation for a much richer understanding of the exploratory
components of adaptive and intelligent action, beyond simply greedy utility
maximization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Millidge_B/0/1/0/all/0/1"&gt;Beren Millidge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tschantz_A/0/1/0/all/0/1"&gt;Alexander Tschantz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seth_A/0/1/0/all/0/1"&gt;Anil Seth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buckley_C/0/1/0/all/0/1"&gt;Christopher Buckley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction of Hereditary Cancers Using Neural Networks. (arXiv:2106.13682v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.13682</id>
        <link href="http://arxiv.org/abs/2106.13682"/>
        <updated>2021-06-28T01:57:56.949Z</updated>
        <summary type="html"><![CDATA[Family history is a major risk factor for many types of cancer. Mendelian
risk prediction models translate family histories into cancer risk predictions
based on knowledge of cancer susceptibility genes. These models are widely used
in clinical practice to help identify high-risk individuals. Mendelian models
leverage the entire family history, but they rely on many assumptions about
cancer susceptibility genes that are either unrealistic or challenging to
validate due to low mutation prevalence. Training more flexible models, such as
neural networks, on large databases of pedigrees can potentially lead to
accuracy gains. In this paper, we develop a framework to apply neural networks
to family history data and investigate their ability to learn inherited
susceptibility to cancer. While there is an extensive literature on neural
networks and their state-of-the-art performance in many tasks, there is little
work applying them to family history data. We propose adaptations of
fully-connected neural networks and convolutional neural networks to pedigrees.
In data simulated under Mendelian inheritance, we demonstrate that our proposed
neural network models are able to achieve nearly optimal prediction
performance. Moreover, when the observed family history includes misreported
cancer diagnoses, neural networks are able to outperform the Mendelian BRCAPRO
model embedding the correct inheritance laws. Using a large dataset of over
200,000 family histories, the Risk Service cohort, we train prediction models
for future risk of breast cancer. We validate the models using data from the
Cancer Genetics Network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Guan_Z/0/1/0/all/0/1"&gt;Zoe Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Parmigiani_G/0/1/0/all/0/1"&gt;Giovanni Parmigiani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Braun_D/0/1/0/all/0/1"&gt;Danielle Braun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Trippa_L/0/1/0/all/0/1"&gt;Lorenzo Trippa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joslim: Joint Widths and Weights Optimization for Slimmable Neural Networks. (arXiv:2007.11752v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.11752</id>
        <link href="http://arxiv.org/abs/2007.11752"/>
        <updated>2021-06-28T01:57:56.934Z</updated>
        <summary type="html"><![CDATA[Slimmable neural networks provide a flexible trade-off front between
prediction error and computational requirement (such as the number of
floating-point operations or FLOPs) with the same storage requirement as a
single model. They are useful for reducing maintenance overhead for deploying
models to devices with different memory constraints and are useful for
optimizing the efficiency of a system with many CNNs. However, existing
slimmable network approaches either do not optimize layer-wise widths or
optimize the shared-weights and layer-wise widths independently, thereby
leaving significant room for improvement by joint width and weight
optimization. In this work, we propose a general framework to enable joint
optimization for both width configurations and weights of slimmable networks.
Our framework subsumes conventional and NAS-based slimmable methods as special
cases and provides flexibility to improve over existing methods. From a
practical standpoint, we propose Joslim, an algorithm that jointly optimizes
both the widths and weights for slimmable nets, which outperforms existing
methods for optimizing slimmable networks across various networks, datasets,
and objectives. Quantitatively, improvements up to 1.7% and 8% in top-1
accuracy on the ImageNet dataset can be attained for MobileNetV2 considering
FLOPs and memory footprint, respectively. Our results highlight the potential
of optimizing the channel counts for different layers jointly with the weights
for slimmable networks. Code available at https://github.com/cmu-enyac/Joslim.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1"&gt;Ting-Wu Chin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morcos_A/0/1/0/all/0/1"&gt;Ari S. Morcos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marculescu_D/0/1/0/all/0/1"&gt;Diana Marculescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reliable Graph Neural Network Explanations Through Adversarial Training. (arXiv:2106.13427v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13427</id>
        <link href="http://arxiv.org/abs/2106.13427"/>
        <updated>2021-06-28T01:57:56.911Z</updated>
        <summary type="html"><![CDATA[Graph neural network (GNN) explanations have largely been facilitated through
post-hoc introspection. While this has been deemed successful, many post-hoc
explanation methods have been shown to fail in capturing a model's learned
representation. Due to this problem, it is worthwhile to consider how one might
train a model so that it is more amenable to post-hoc analysis. Given the
success of adversarial training in the computer vision domain to train models
with more reliable representations, we propose a similar training paradigm for
GNNs and analyze the respective impact on a model's explanations. In instances
without ground truth labels, we also determine how well an explanation method
is utilizing a model's learned representation through a new metric and
demonstrate adversarial training can help better extract domain-relevant
insights in chemistry.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Loveland_D/0/1/0/all/0/1"&gt;Donald Loveland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shusen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1"&gt;Bhavya Kailkhura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hiszpanski_A/0/1/0/all/0/1"&gt;Anna Hiszpanski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1"&gt;Yong Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subgraph Federated Learning with Missing Neighbor Generation. (arXiv:2106.13430v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13430</id>
        <link href="http://arxiv.org/abs/2106.13430"/>
        <updated>2021-06-28T01:57:56.893Z</updated>
        <summary type="html"><![CDATA[Graphs have been widely used in data mining and machine learning due to their
unique representation of real-world objects and their interactions. As graphs
are getting bigger and bigger nowadays, it is common to see their subgraphs
separately collected and stored in multiple local systems. Therefore, it is
natural to consider the subgraph federated learning setting, where each local
system holding a small subgraph that may be biased from the distribution of the
whole graph. Hence, the subgraph federated learning aims to collaboratively
train a powerful and generalizable graph mining model without directly sharing
their graph data. In this work, towards the novel yet realistic setting of
subgraph federated learning, we propose two major techniques: (1) FedSage,
which trains a GraphSage model based on FedAvg to integrate node features, link
structures, and task labels on multiple local subgraphs; (2) FedSage+, which
trains a missing neighbor generator along FedSage to deal with missing links
across local subgraphs. Empirical results on four real-world graph datasets
with synthesized subgraph federated learning settings demonstrate the
effectiveness and efficiency of our proposed techniques. At the same time,
consistent theoretical implications are made towards their generalization
ability on the global graphs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Ke Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Carl Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoxiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lichao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yiu_S/0/1/0/all/0/1"&gt;Siu Ming Yiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Building Intelligent Autonomous Navigation Agents. (arXiv:2106.13415v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13415</id>
        <link href="http://arxiv.org/abs/2106.13415"/>
        <updated>2021-06-28T01:57:56.885Z</updated>
        <summary type="html"><![CDATA[Breakthroughs in machine learning in the last decade have led to `digital
intelligence', i.e. machine learning models capable of learning from vast
amounts of labeled data to perform several digital tasks such as speech
recognition, face recognition, machine translation and so on. The goal of this
thesis is to make progress towards designing algorithms capable of `physical
intelligence', i.e. building intelligent autonomous navigation agents capable
of learning to perform complex navigation tasks in the physical world involving
visual perception, natural language understanding, reasoning, planning, and
sequential decision making. Despite several advances in classical navigation
methods in the last few decades, current navigation agents struggle at
long-term semantic navigation tasks. In the first part of the thesis, we
discuss our work on short-term navigation using end-to-end reinforcement
learning to tackle challenges such as obstacle avoidance, semantic perception,
language grounding, and reasoning. In the second part, we present a new class
of navigation methods based on modular learning and structured explicit map
representations, which leverage the strengths of both classical and end-to-end
learning methods, to tackle long-term navigation tasks. We show that these
methods are able to effectively tackle challenges such as localization,
mapping, long-term planning, exploration and learning semantic priors. These
modular learning methods are capable of long-term spatial and semantic
understanding and achieve state-of-the-art results on various navigation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chaplot_D/0/1/0/all/0/1"&gt;Devendra Singh Chaplot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Universal Approximation of Residual Flows in Maximum Mean Discrepancy. (arXiv:2103.05793v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05793</id>
        <link href="http://arxiv.org/abs/2103.05793"/>
        <updated>2021-06-28T01:57:56.876Z</updated>
        <summary type="html"><![CDATA[Normalizing flows are a class of flexible deep generative models that offer
easy likelihood computation. Despite their empirical success, there is little
theoretical understanding of their expressiveness. In this work, we study
residual flows, a class of normalizing flows composed of Lipschitz residual
blocks. We prove residual flows are universal approximators in maximum mean
discrepancy. We provide upper bounds on the number of residual blocks to
achieve approximation under different assumptions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1"&gt;Zhifeng Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_K/0/1/0/all/0/1"&gt;Kamalika Chaudhuri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coded-InvNet for Resilient Prediction Serving Systems. (arXiv:2106.06445v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06445</id>
        <link href="http://arxiv.org/abs/2106.06445"/>
        <updated>2021-06-28T01:57:56.870Z</updated>
        <summary type="html"><![CDATA[Inspired by a new coded computation algorithm for invertible functions, we
propose Coded-InvNet a new approach to design resilient prediction serving
systems that can gracefully handle stragglers or node failures. Coded-InvNet
leverages recent findings in the deep learning literature such as invertible
neural networks, Manifold Mixup, and domain translation algorithms, identifying
interesting research directions that span across machine learning and systems.
Our experimental results show that Coded-InvNet can outperform existing
approaches, especially when the compute resource overhead is as low as 10%. For
instance, without knowing which of the ten workers is going to fail, our
algorithm can design a backup task so that it can correctly recover the missing
prediction result with an accuracy of 85.9%, significantly outperforming the
previous SOTA by 32.5%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dinh_T/0/1/0/all/0/1"&gt;Tuan Dinh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kangwook Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Combination of Linear and Spectral Estimators for Generalized Linear Models. (arXiv:2008.03326v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.03326</id>
        <link href="http://arxiv.org/abs/2008.03326"/>
        <updated>2021-06-28T01:57:56.851Z</updated>
        <summary type="html"><![CDATA[We study the problem of recovering an unknown signal $\boldsymbol x$ given
measurements obtained from a generalized linear model with a Gaussian sensing
matrix. Two popular solutions are based on a linear estimator $\hat{\boldsymbol
x}^{\rm L}$ and a spectral estimator $\hat{\boldsymbol x}^{\rm s}$. The former
is a data-dependent linear combination of the columns of the measurement
matrix, and its analysis is quite simple. The latter is the principal
eigenvector of a data-dependent matrix, and a recent line of work has studied
its performance. In this paper, we show how to optimally combine
$\hat{\boldsymbol x}^{\rm L}$ and $\hat{\boldsymbol x}^{\rm s}$. At the heart
of our analysis is the exact characterization of the joint empirical
distribution of $(\boldsymbol x, \hat{\boldsymbol x}^{\rm L}, \hat{\boldsymbol
x}^{\rm s})$ in the high-dimensional limit. This allows us to compute the
Bayes-optimal combination of $\hat{\boldsymbol x}^{\rm L}$ and
$\hat{\boldsymbol x}^{\rm s}$, given the limiting distribution of the signal
$\boldsymbol x$. When the distribution of the signal is Gaussian, then the
Bayes-optimal combination has the form $\theta\hat{\boldsymbol x}^{\rm
L}+\hat{\boldsymbol x}^{\rm s}$ and we derive the optimal combination
coefficient. In order to establish the limiting distribution of $(\boldsymbol
x, \hat{\boldsymbol x}^{\rm L}, \hat{\boldsymbol x}^{\rm s})$, we design and
analyze an Approximate Message Passing (AMP) algorithm whose iterates give
$\hat{\boldsymbol x}^{\rm L}$ and approach $\hat{\boldsymbol x}^{\rm s}$.
Numerical simulations demonstrate the improvement of the proposed combination
with respect to the two methods considered separately.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Mondelli_M/0/1/0/all/0/1"&gt;Marco Mondelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Thrampoulidis_C/0/1/0/all/0/1"&gt;Christos Thrampoulidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Venkataramanan_R/0/1/0/all/0/1"&gt;Ramji Venkataramanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical Study of Graph-Based Approaches for Semi-Supervised Time Series Classification. (arXiv:2104.08153v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08153</id>
        <link href="http://arxiv.org/abs/2104.08153"/>
        <updated>2021-06-28T01:57:56.841Z</updated>
        <summary type="html"><![CDATA[Time series data play an important role in many applications and their
analysis reveals crucial information for understanding the underlying
processes. Among the many time series learning tasks of great importance, we
here focus on semi-supervised learning based on a graph representation of the
data. Two main aspects are involved in this task. A suitable distance measure
to evaluate the similarities between time series, and a learning method to make
predictions based on these distances. However, the relationship between the two
aspects has never been studied systematically in the context of graph-based
learning. We describe four different distance measures, including (Soft) DTW
and MPDist, a distance measure based on the Matrix Profile, as well as four
successful semi-supervised learning methods, including the graph Allen--Cahn
method and a Graph Convolutional Neural Network. We then compare the
performance of the algorithms on binary classification data sets. In our
findings we compare the chosen graph-based methods using all distance measures
and observe that the results vary strongly with respect to the accuracy. As
predicted by the ``no free lunch'' theorem, no clear best combination to employ
in all cases is found. Our study provides a reproducible framework for future
work in the direction of semi-supervised learning for time series with a focus
on graph representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alfke_D/0/1/0/all/0/1"&gt;Dominik Alfke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gondos_M/0/1/0/all/0/1"&gt;Miriam Gondos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peroche_L/0/1/0/all/0/1"&gt;Lucile Peroche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoll_M/0/1/0/all/0/1"&gt;Martin Stoll&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Matrix Factorization with Grouping Effect. (arXiv:2106.13681v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13681</id>
        <link href="http://arxiv.org/abs/2106.13681"/>
        <updated>2021-06-28T01:57:56.814Z</updated>
        <summary type="html"><![CDATA[Although many techniques have been applied to matrix factorization (MF), they
may not fully exploit the feature structure. In this paper, we incorporate the
grouping effect into MF and propose a novel method called Robust Matrix
Factorization with Grouping effect (GRMF). The grouping effect is a
generalization of the sparsity effect, which conducts denoising by clustering
similar values around multiple centers instead of just around 0. Compared with
existing algorithms, the proposed GRMF can automatically learn the grouping
structure and sparsity in MF without prior knowledge, by introducing a
naturally adjustable non-convex regularization to achieve simultaneous sparsity
and grouping effect. Specifically, GRMF uses an efficient alternating
minimization framework to perform MF, in which the original non-convex problem
is first converted into a convex problem through Difference-of-Convex (DC)
programming, and then solved by Alternating Direction Method of Multipliers
(ADMM). In addition, GRMF can be easily extended to the Non-negative Matrix
Factorization (NMF) settings. Extensive experiments have been conducted using
real-world data sets with outliers and contaminated noise, where the
experimental results show that GRMF has promoted performance and robustness,
compared to five benchmark algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haiyan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Luwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Haoyi Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1"&gt;Dejing Dou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepLoc: A Ubiquitous Accurate and Low-Overhead Outdoor Cellular Localization System. (arXiv:2106.13632v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13632</id>
        <link href="http://arxiv.org/abs/2106.13632"/>
        <updated>2021-06-28T01:57:56.808Z</updated>
        <summary type="html"><![CDATA[Recent years have witnessed fast growth in outdoor location-based services.
While GPS is considered a ubiquitous localization system, it is not supported
by low-end phones, requires direct line of sight to the satellites, and can
drain the phone battery quickly.

In this paper, we propose DeepLoc: a deep learning-based outdoor localization
system that obtains GPS-like localization accuracy without its limitations. In
particular, DeepLoc leverages the ubiquitous cellular signals received from the
different cell towers heard by the mobile device as hints to localize it. To do
that, crowd-sensed geo-tagged received signal strength information coming from
different cell towers is used to train a deep model that is used to infer the
user's position. As part of DeepLoc design, we introduce modules to address a
number of practical challenges including scaling the data collection to large
areas, handling the inherent noise in the cellular signal and geo-tagged data,
as well as providing enough data that is required for deep learning models with
low-overhead.

We implemented DeepLoc on different Android devices. Evaluation results in
realistic urban and rural environments show that DeepLoc can achieve a median
localization accuracy within 18.8m in urban areas and within 15.7m in rural
areas. This accuracy outperforms the state-of-the-art cellular-based systems by
more than 470% and comes with 330% savings in power compared to the GPS. This
highlights the promise of DeepLoc as a ubiquitous accurate and low-overhead
localization system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shokry_A/0/1/0/all/0/1"&gt;Ahmed Shokry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torki_M/0/1/0/all/0/1"&gt;Marwan Torki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Youssef_M/0/1/0/all/0/1"&gt;Moustafa Youssef&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Perception-Action-Communication Loops with Convolutional and Graph Neural Networks. (arXiv:2106.13358v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.13358</id>
        <link href="http://arxiv.org/abs/2106.13358"/>
        <updated>2021-06-28T01:57:56.797Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a perception-action-communication loop design using
Vision-based Graph Aggregation and Inference (VGAI). This multi-agent
decentralized learning-to-control framework maps raw visual observations to
agent actions, aided by local communication among neighboring agents. Our
framework is implemented by a cascade of a convolutional and a graph neural
network (CNN / GNN), addressing agent-level visual perception and feature
learning, as well as swarm-level communication, local information aggregation
and agent action inference, respectively. By jointly training the CNN and GNN,
image features and communication messages are learned in conjunction to better
address the specific task. We use imitation learning to train the VGAI
controller in an offline phase, relying on a centralized expert controller.
This results in a learned VGAI controller that can be deployed in a distributed
manner for online execution. Additionally, the controller exhibits good scaling
properties, with training in smaller teams and application in larger teams.
Through a multi-agent flocking application, we demonstrate that VGAI yields
performance comparable to or better than other decentralized controllers, using
only the visual input modality and without accessing precise location or motion
state information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1"&gt;Ting-Kuei Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gama_F/0/1/0/all/0/1"&gt;Fernando Gama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianlong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1"&gt;Wenqing Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1"&gt;Alejandro Ribeiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadler_B/0/1/0/all/0/1"&gt;Brian M. Sadler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifying malicious accounts in Blockchains using Domain Names and associated temporal properties. (arXiv:2106.13420v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.13420</id>
        <link href="http://arxiv.org/abs/2106.13420"/>
        <updated>2021-06-28T01:57:56.777Z</updated>
        <summary type="html"><![CDATA[The rise in the adoption of blockchain technology has led to increased
illegal activities by cyber-criminals costing billions of dollars. Many machine
learning algorithms are applied to detect such illegal behavior. These
algorithms are often trained on the transaction behavior and, in some cases,
trained on the vulnerabilities that exist in the system. In our approach, we
study the feasibility of using metadata such as Domain Name (DN) associated
with the account in the blockchain and identify whether an account should be
tagged malicious or not. Here, we leverage the temporal aspects attached to the
DNs. Our results identify 144930 DNs that show malicious behavior, and out of
these, 54114 DNs show persistent malicious behavior over time. Nonetheless,
none of these identified malicious DNs were reported in new officially tagged
malicious blockchain DNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sachan_R/0/1/0/all/0/1"&gt;Rohit Kumar Sachan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_R/0/1/0/all/0/1"&gt;Rachit Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1"&gt;Sandeep Kumar Shukla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Branch Prediction as a Reinforcement Learning Problem: Why, How and Case Studies. (arXiv:2106.13429v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13429</id>
        <link href="http://arxiv.org/abs/2106.13429"/>
        <updated>2021-06-28T01:57:56.767Z</updated>
        <summary type="html"><![CDATA[Recent years have seen stagnating improvements to branch predictor (BP)
efficacy and a dearth of fresh ideas in branch predictor design, calling for
fresh thinking in this area. This paper argues that looking at BP from the
viewpoint of Reinforcement Learning (RL) facilitates systematic reasoning
about, and exploration of, BP designs. We describe how to apply the RL
formulation to branch predictors, show that existing predictors can be
succinctly expressed in this formulation, and study two RL-based variants of
conventional BPs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zouzias_A/0/1/0/all/0/1"&gt;Anastasios Zouzias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalaitzidis_K/0/1/0/all/0/1"&gt;Kleovoulos Kalaitzidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grot_B/0/1/0/all/0/1"&gt;Boris Grot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decomposed Mutual Information Estimation for Contrastive Representation Learning. (arXiv:2106.13401v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13401</id>
        <link href="http://arxiv.org/abs/2106.13401"/>
        <updated>2021-06-28T01:57:56.759Z</updated>
        <summary type="html"><![CDATA[Recent contrastive representation learning methods rely on estimating mutual
information (MI) between multiple views of an underlying context. E.g., we can
derive multiple views of a given image by applying data augmentation, or we can
split a sequence into views comprising the past and future of some step in the
sequence. Contrastive lower bounds on MI are easy to optimize, but have a
strong underestimation bias when estimating large amounts of MI. We propose
decomposing the full MI estimation problem into a sum of smaller estimation
problems by splitting one of the views into progressively more informed
subviews and by applying the chain rule on MI between the decomposed views.
This expression contains a sum of unconditional and conditional MI terms, each
measuring modest chunks of the total MI, which facilitates approximation via
contrastive bounds. To maximize the sum, we formulate a contrastive lower bound
on the conditional MI which can be approximated efficiently. We refer to our
general approach as Decomposed Estimation of Mutual Information (DEMI). We show
that DEMI can capture a larger amount of MI than standard non-decomposed
contrastive bounds in a synthetic setting, and learns better representations in
a vision domain and for dialogue generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1"&gt;Alessandro Sordoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dziri_N/0/1/0/all/0/1"&gt;Nouha Dziri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schulz_H/0/1/0/all/0/1"&gt;Hannes Schulz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gordon_G/0/1/0/all/0/1"&gt;Geoff Gordon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bachman_P/0/1/0/all/0/1"&gt;Phil Bachman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tachet_R/0/1/0/all/0/1"&gt;Remi Tachet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BlockGNN: Towards Efficient GNN Acceleration Using Block-Circulant Weight Matrices. (arXiv:2104.06214v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06214</id>
        <link href="http://arxiv.org/abs/2104.06214"/>
        <updated>2021-06-28T01:57:56.745Z</updated>
        <summary type="html"><![CDATA[In recent years, Graph Neural Networks (GNNs) appear to be state-of-the-art
algorithms for analyzing non-euclidean graph data. By applying deep-learning to
extract high-level representations from graph structures, GNNs achieve
extraordinary accuracy and great generalization ability in various tasks.
However, with the ever-increasing graph sizes, more and more complicated GNN
layers, and higher feature dimensions, the computational complexity of GNNs
grows exponentially. How to inference GNNs in real time has become a
challenging problem, especially for some resource-limited edge-computing
platforms.

To tackle this challenge, we propose BlockGNN, a software-hardware co-design
approach to realize efficient GNN acceleration. At the algorithm level, we
propose to leverage block-circulant weight matrices to greatly reduce the
complexity of various GNN models. At the hardware design level, we propose a
pipelined CirCore architecture, which supports efficient block-circulant
matrices computation. Basing on CirCore, we present a novel BlockGNN
accelerator to compute various GNNs with low latency. Moreover, to determine
the optimal configurations for diverse deployed tasks, we also introduce a
performance and resource model that helps choose the optimal hardware
parameters automatically. Comprehensive experiments on the ZC706 FPGA platform
demonstrate that on various GNN tasks, BlockGNN achieves up to $8.3\times$
speedup compared to the baseline HyGCN architecture and $111.9\times$ energy
reduction compared to the Intel Xeon CPU platform.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhe Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1"&gt;Bizhao Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhe Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1"&gt;Yijin Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1"&gt;Guangyu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1"&gt;Guojie Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Deep-Learning-Based Voice Activity Detectors and Room Impulse Response Models in Reverberant Environments. (arXiv:2106.13511v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.13511</id>
        <link href="http://arxiv.org/abs/2106.13511"/>
        <updated>2021-06-28T01:57:56.726Z</updated>
        <summary type="html"><![CDATA[State-of-the-art deep-learning-based voice activity detectors (VADs) are
often trained with anechoic data. However, real acoustic environments are
generally reverberant, which causes the performance to significantly
deteriorate. To mitigate this mismatch between training data and real data, we
simulate an augmented training set that contains nearly five million
utterances. This extension comprises of anechoic utterances and their
reverberant modifications, generated by convolutions of the anechoic utterances
with a variety of room impulse responses (RIRs). We consider five different
models to generate RIRs, and five different VADs that are trained with the
augmented training set. We test all trained systems in three different real
reverberant environments. Experimental results show $20\%$ increase on average
in accuracy, precision and recall for all detectors and response models,
compared to anechoic training. Furthermore, one of the RIR models consistently
yields better performance than the other models, for all the tested VADs.
Additionally, one of the VADs consistently outperformed the other VADs in all
experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ivry_A/0/1/0/all/0/1"&gt;Amir Ivry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1"&gt;Israel Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berdugo_B/0/1/0/all/0/1"&gt;Baruch Berdugo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for High-Impedance Fault Detection: Convolutional Autoencoders. (arXiv:2106.13276v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13276</id>
        <link href="http://arxiv.org/abs/2106.13276"/>
        <updated>2021-06-28T01:57:56.679Z</updated>
        <summary type="html"><![CDATA[High-impedance faults (HIF) are difficult to detect because of their low
current amplitude and highly diverse characteristics. In recent years, machine
learning (ML) has been gaining popularity in HIF detection because ML
techniques learn patterns from data and successfully detect HIFs. However, as
these methods are based on supervised learning, they fail to reliably detect
any scenario, fault or non-fault, not present in the training data.
Consequently, this paper takes advantage of unsupervised learning and proposes
a convolutional autoencoder framework for HIF detection (CAE-HIFD). Contrary to
the conventional autoencoders that learn from normal behavior, the
convolutional autoencoder (CAE) in CAE-HIFD learns only from the HIF signals
eliminating the need for presence of diverse non-HIF scenarios in the CAE
training. CAE distinguishes HIFs from non-HIF operating conditions by employing
cross-correlation. To discriminate HIFs from transient disturbances such as
capacitor or load switching, CAE-HIFD uses kurtosis, a statistical measure of
the probability distribution shape. The performance evaluation studies
conducted using the IEEE 13-node test feeder indicate that the CAE-HIFD
reliably detects HIFs, outperforms the state-of-the-art HIF detection
techniques, and is robust against noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rai_K/0/1/0/all/0/1"&gt;Khushwant Rai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hojatpanah_F/0/1/0/all/0/1"&gt;Farnam Hojatpanah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ajaei_F/0/1/0/all/0/1"&gt;Firouz Badrkhani Ajaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grolinger_K/0/1/0/all/0/1"&gt;Katarina Grolinger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-based Design of Inferential Sensors for Petrochemical Industry. (arXiv:2106.13503v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13503</id>
        <link href="http://arxiv.org/abs/2106.13503"/>
        <updated>2021-06-28T01:57:56.673Z</updated>
        <summary type="html"><![CDATA[Inferential (or soft) sensors are used in industry to infer the values of
imprecisely and rarely measured (or completely unmeasured) variables from
variables measured online (e.g., pressures, temperatures). The main challenge,
akin to classical model overfitting, in designing an effective inferential
sensor is the selection of a correct structure of the sensor. The sensor
structure is represented by the number of inputs to the sensor, which
correspond to the variables measured online and their (simple) combinations.
This work is focused on the design of inferential sensors for product
composition of an industrial distillation column in two oil refinery units, a
Fluid Catalytic Cracking unit and a Vacuum Gasoil Hydrogenation unit. As the
first design step, we use several well-known data pre-treatment (gross error
detection) methods and compare the ability of these approaches to indicate
systematic errors and outliers in the available industrial data. We then study
effectiveness of various methods for design of the inferential sensors taking
into account the complexity and accuracy of the resulting model. The
effectiveness analysis indicates that the improvements achieved over the
current inferential sensors are up to 19 %.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mojto_M/0/1/0/all/0/1"&gt;Martin Mojto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lubusky_K/0/1/0/all/0/1"&gt;Karol &amp;#x13d;ubu&amp;#x161;k&amp;#xfd;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fikar_M/0/1/0/all/0/1"&gt;Miroslav Fikar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paulen_R/0/1/0/all/0/1"&gt;Radoslav Paulen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single and Union Non-parallel Support Vector Machine Frameworks. (arXiv:1910.09734v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.09734</id>
        <link href="http://arxiv.org/abs/1910.09734"/>
        <updated>2021-06-28T01:57:56.661Z</updated>
        <summary type="html"><![CDATA[Considering the classification problem, we summarize the nonparallel support
vector machines with the nonparallel hyperplanes to two types of frameworks.
The first type constructs the hyperplanes separately. It solves a series of
small optimization problems to obtain a series of hyperplanes, but is hard to
measure the loss of each sample. The other type constructs all the hyperplanes
simultaneously, and it solves one big optimization problem with the ascertained
loss of each sample. We give the characteristics of each framework and compare
them carefully. In addition, based on the second framework, we construct a
max-min distance-based nonparallel support vector machine for multiclass
classification problem, called NSVM. It constructs hyperplanes with large
distance margin by solving an optimization problem. Experimental results on
benchmark data sets show the advantages of our NSVM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chun-Na Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yuan-Hai Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huajun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yu-Ting Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Ling-Wei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiu_N/0/1/0/all/0/1"&gt;Naihua Xiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_N/0/1/0/all/0/1"&gt;Nai-Yang Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Source-Criticism Debiasing Method for GloVe Embeddings. (arXiv:2106.13382v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13382</id>
        <link href="http://arxiv.org/abs/2106.13382"/>
        <updated>2021-06-28T01:57:56.648Z</updated>
        <summary type="html"><![CDATA[It is well-documented that word embeddings trained on large public corpora
consistently exhibit known human social biases. Although many methods for
debiasing exist, almost all fixate on completely eliminating biased information
from the embeddings and often diminish training set size in the process. In
this paper, we present a simple yet effective method for debiasing GloVe word
embeddings (Pennington et al., 2014) which works by incorporating explicit
information about training set bias rather than removing biased data outright.
Our method runs quickly and efficiently with the help of a fast bias gradient
approximation method from Brunet et al. (2019). As our approach is akin to the
notion of 'source criticism' in the humanities, we term our method
Source-Critical GloVe (SC-GloVe). We show that SC-GloVe reduces the effect size
on Word Embedding Association Test (WEAT) sets without sacrificing training
data or TOP-1 performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McGovern_H/0/1/0/all/0/1"&gt;Hope McGovern&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonlinear Acoustic Echo Cancellation with Deep Learning. (arXiv:2106.13754v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.13754</id>
        <link href="http://arxiv.org/abs/2106.13754"/>
        <updated>2021-06-28T01:57:56.620Z</updated>
        <summary type="html"><![CDATA[We propose a nonlinear acoustic echo cancellation system, which aims to model
the echo path from the far-end signal to the near-end microphone in two parts.
Inspired by the physical behavior of modern hands-free devices, we first
introduce a novel neural network architecture that is specifically designed to
model the nonlinear distortions these devices induce between receiving and
playing the far-end signal. To account for variations between devices, we
construct this network with trainable memory length and nonlinear activation
functions that are not parameterized in advance, but are rather optimized
during the training stage using the training data. Second, the network is
succeeded by a standard adaptive linear filter that constantly tracks the echo
path between the loudspeaker output and the microphone. During training, the
network and filter are jointly optimized to learn the network parameters. This
system requires 17 thousand parameters that consume 500 Million floating-point
operations per second and 40 Kilo-bytes of memory. It also satisfies hands-free
communication timing requirements on a standard neural processor, which renders
it adequate for embedding on hands-free communication devices. Using 280 hours
of real and synthetic data, experiments show advantageous performance compared
to competing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ivry_A/0/1/0/all/0/1"&gt;Amir Ivry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1"&gt;Israel Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berdugo_B/0/1/0/all/0/1"&gt;Baruch Berdugo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Primordial non-Gaussianity from the Completed SDSS-IV extended Baryon Oscillation Spectroscopic Survey I: Catalogue Preparation and Systematic Mitigation. (arXiv:2106.13724v1 [astro-ph.CO])]]></title>
        <id>http://arxiv.org/abs/2106.13724</id>
        <link href="http://arxiv.org/abs/2106.13724"/>
        <updated>2021-06-28T01:57:56.611Z</updated>
        <summary type="html"><![CDATA[We investigate the large-scale clustering of the final spectroscopic sample
of quasars from the recently completed extended Baryon Oscillation
Spectroscopic Survey (eBOSS). The sample contains $343708$ objects in the
redshift range $0.8<z<2.2$ and $72667$ objects with redshifts $2.2<z<3.5$,
covering an effective area of $4699~{\rm deg}^{2}$. We develop a neural
network-based approach to mitigate spurious fluctuations in the density field
caused by spatial variations in the quality of the imaging data used to select
targets for follow-up spectroscopy. Simulations are used with the same angular
and radial distributions as the real data to estimate covariance matrices,
perform error analyses, and assess residual systematic uncertainties. We
measure the mean density contrast and cross-correlations of the eBOSS quasars
against maps of potential sources of imaging systematics to address algorithm
effectiveness, finding that the neural network-based approach outperforms
standard linear regression. Stellar density is one of the most important
sources of spurious fluctuations, and a new template constructed using data
from the Gaia spacecraft provides the best match to the observed quasar
clustering. The end-product from this work is a new value-added quasar
catalogue with the improved weights to correct for nonlinear imaging systematic
effects, which will be made public. Our quasar catalogue is used to measure the
local-type primordial non-Gaussianity in our companion paper, Mueller et al. in
preparation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Rezaie_M/0/1/0/all/0/1"&gt;Mehdi Rezaie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Ross_A/0/1/0/all/0/1"&gt;Ashley J. Ross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Seo_H/0/1/0/all/0/1"&gt;Hee-Jong Seo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Mueller_E/0/1/0/all/0/1"&gt;Eva-Maria Mueller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Percival_W/0/1/0/all/0/1"&gt;Will J. Percival&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Merz_G/0/1/0/all/0/1"&gt;Grant Merz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Katebi_R/0/1/0/all/0/1"&gt;Reza Katebi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Bunescu_R/0/1/0/all/0/1"&gt;Razvan C. Bunescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Bautista_J/0/1/0/all/0/1"&gt;Julian Bautista&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Brownstein_J/0/1/0/all/0/1"&gt;Joel R. Brownstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Burtin_E/0/1/0/all/0/1"&gt;Etienne Burtin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Dawson_K/0/1/0/all/0/1"&gt;Kyle Dawson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Gil_Marin_H/0/1/0/all/0/1"&gt;H&amp;#xe9;ctor Gil-Mar&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Hou_J/0/1/0/all/0/1"&gt;Jiamin Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Lyke_E/0/1/0/all/0/1"&gt;Eleanor B. Lyke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Macorra_A/0/1/0/all/0/1"&gt;Axel de la Macorra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Rossi_G/0/1/0/all/0/1"&gt;Graziano Rossi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Schneider_D/0/1/0/all/0/1"&gt;Donald P. Schneider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Zarrouk_P/0/1/0/all/0/1"&gt;Pauline Zarrouk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Zhao_G/0/1/0/all/0/1"&gt;Gong-Bo Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proxy Convexity: A Unified Framework for the Analysis of Neural Networks Trained by Gradient Descent. (arXiv:2106.13792v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13792</id>
        <link href="http://arxiv.org/abs/2106.13792"/>
        <updated>2021-06-28T01:57:56.601Z</updated>
        <summary type="html"><![CDATA[Although the optimization objectives for learning neural networks are highly
non-convex, gradient-based methods have been wildly successful at learning
neural networks in practice. This juxtaposition has led to a number of recent
studies on provable guarantees for neural networks trained by gradient descent.
Unfortunately, the techniques in these works are often highly specific to the
problem studied in each setting, relying on different assumptions on the
distribution, optimization parameters, and network architectures, making it
difficult to generalize across different settings. In this work, we propose a
unified non-convex optimization framework for the analysis of neural network
training. We introduce the notions of proxy convexity and proxy
Polyak-Lojasiewicz (PL) inequalities, which are satisfied if the original
objective function induces a proxy objective function that is implicitly
minimized when using gradient methods. We show that stochastic gradient descent
(SGD) on objectives satisfying proxy convexity or the proxy PL inequality leads
to efficient guarantees for proxy objective functions. We further show that
many existing guarantees for neural networks trained by gradient descent can be
unified through proxy convexity and proxy PL inequalities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Frei_S/0/1/0/all/0/1"&gt;Spencer Frei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1"&gt;Quanquan Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Voice Activity Detection for Transient Noisy Environment Based on Diffusion Nets. (arXiv:2106.13763v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.13763</id>
        <link href="http://arxiv.org/abs/2106.13763"/>
        <updated>2021-06-28T01:57:56.595Z</updated>
        <summary type="html"><![CDATA[We address voice activity detection in acoustic environments of transients
and stationary noises, which often occur in real life scenarios. We exploit
unique spatial patterns of speech and non-speech audio frames by independently
learning their underlying geometric structure. This process is done through a
deep encoder-decoder based neural network architecture. This structure involves
an encoder that maps spectral features with temporal information to their
low-dimensional representations, which are generated by applying the diffusion
maps method. The encoder feeds a decoder that maps the embedded data back into
the high-dimensional space. A deep neural network, which is trained to separate
speech from non-speech frames, is obtained by concatenating the decoder to the
encoder, resembling the known Diffusion nets architecture. Experimental results
show enhanced performance compared to competing voice activity detection
methods. The improvement is achieved in both accuracy, robustness and
generalization ability. Our model performs in a real-time manner and can be
integrated into audio-based communication systems. We also present a batch
algorithm which obtains an even higher accuracy for off-line applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ivry_A/0/1/0/all/0/1"&gt;Amir Ivry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berdugo_B/0/1/0/all/0/1"&gt;Baruch Berdugo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1"&gt;Israel Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Private Adaptive Gradient Methods for Convex Optimization. (arXiv:2106.13756v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13756</id>
        <link href="http://arxiv.org/abs/2106.13756"/>
        <updated>2021-06-28T01:57:56.585Z</updated>
        <summary type="html"><![CDATA[We study adaptive methods for differentially private convex optimization,
proposing and analyzing differentially private variants of a Stochastic
Gradient Descent (SGD) algorithm with adaptive stepsizes, as well as the
AdaGrad algorithm. We provide upper bounds on the regret of both algorithms and
show that the bounds are (worst-case) optimal. As a consequence of our
development, we show that our private versions of AdaGrad outperform adaptive
SGD, which in turn outperforms traditional SGD in scenarios with non-isotropic
gradients where (non-private) Adagrad provably outperforms SGD. The major
challenge is that the isotropic noise typically added for privacy dominates the
signal in gradient geometry for high-dimensional problems; approaches to this
that effectively optimize over lower-dimensional subspaces simply ignore the
actual problems that varying gradient geometries introduce. In contrast, we
study non-isotropic clipping and noise addition, developing a principled
theoretical approach; the consequent procedures also enjoy significantly
stronger empirical performance than prior approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Asi_H/0/1/0/all/0/1"&gt;Hilal Asi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duchi_J/0/1/0/all/0/1"&gt;John Duchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fallah_A/0/1/0/all/0/1"&gt;Alireza Fallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javidbakht_O/0/1/0/all/0/1"&gt;Omid Javidbakht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talwar_K/0/1/0/all/0/1"&gt;Kunal Talwar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal Graph Signal Decomposition. (arXiv:2106.13517v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13517</id>
        <link href="http://arxiv.org/abs/2106.13517"/>
        <updated>2021-06-28T01:57:56.574Z</updated>
        <summary type="html"><![CDATA[Temporal graph signals are multivariate time series with individual
components associated with nodes of a fixed graph structure. Data of this kind
arises in many domains including activity of social network users, sensor
network readings over time, and time course gene expression within the
interaction network of a model organism. Traditional matrix decomposition
methods applied to such data fall short of exploiting structural regularities
encoded in the underlying graph and also in the temporal patterns of the
signal. How can we take into account such structure to obtain a succinct and
interpretable representation of temporal graph signals?

We propose a general, dictionary-based framework for temporal graph signal
decomposition (TGSD). The key idea is to learn a low-rank, joint encoding of
the data via a combination of graph and time dictionaries. We propose a highly
scalable decomposition algorithm for both complete and incomplete data, and
demonstrate its advantage for matrix decomposition, imputation of missing
values, temporal interpolation, clustering, period estimation, and rank
estimation in synthetic and real-world data ranging from traffic patterns to
social media activity. Our framework achieves 28% reduction in RMSE compared to
baselines for temporal interpolation when as many as 75% of the observations
are missing. It scales best among baselines taking under 20 seconds on 3.5
million data points and produces the most parsimonious models. To the best of
our knowledge, TGSD is the first framework to jointly model graph signals by
temporal and graph dictionaries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McNeil_M/0/1/0/all/0/1"&gt;Maxwell McNeil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bogdanov_P/0/1/0/all/0/1"&gt;Petko Bogdanov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Jitter: Random Jittering Loss Function. (arXiv:2106.13749v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13749</id>
        <link href="http://arxiv.org/abs/2106.13749"/>
        <updated>2021-06-28T01:57:56.567Z</updated>
        <summary type="html"><![CDATA[Regularization plays a vital role in machine learning optimization. One novel
regularization method called flooding makes the training loss fluctuate around
the flooding level. It intends to make the model continue to random walk until
it comes to a flat loss landscape to enhance generalization. However, the
hyper-parameter flooding level of the flooding method fails to be selected
properly and uniformly. We propose a novel method called Jitter to improve it.
Jitter is essentially a kind of random loss function. Before training, we
randomly sample the Jitter Point from a specific probability distribution. The
flooding level should be replaced by Jitter point to obtain a new target
function and train the model accordingly. As Jitter point acting as a random
factor, we actually add some randomness to the loss function, which is
consistent with the fact that there exists innumerable random behaviors in the
learning process of the machine learning model and is supposed to make the
model more robust. In addition, Jitter performs random walk randomly which
divides the loss curve into small intervals and then flipping them over,
ideally making the loss curve much flatter and enhancing generalization
ability. Moreover, Jitter can be a domain-, task-, and model-independent
regularization method and train the model effectively after the training error
reduces to zero. Our experimental results show that Jitter method can improve
model performance more significantly than the previous flooding method and make
the test loss curve descend twice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhicheng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1"&gt;Chenglei Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1"&gt;Sidan Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fostering Diversity in Spatial Evolutionary Generative Adversarial Networks. (arXiv:2106.13590v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13590</id>
        <link href="http://arxiv.org/abs/2106.13590"/>
        <updated>2021-06-28T01:57:56.471Z</updated>
        <summary type="html"><![CDATA[Generative adversary networks (GANs) suffer from training pathologies such as
instability and mode collapse, which mainly arise from a lack of diversity in
their adversarial interactions. Co-evolutionary GAN (CoE-GAN) training
algorithms have shown to be resilient to these pathologies. This article
introduces Mustangs, a spatially distributed CoE-GAN, which fosters diversity
by using different loss functions during the training. Experimental analysis on
MNIST and CelebA demonstrated that Mustangs trains statistically more accurate
generators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Toutouh_J/0/1/0/all/0/1"&gt;Jamal Toutouh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hemberg_E/0/1/0/all/0/1"&gt;Erik Hemberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OReilly_U/0/1/0/all/0/1"&gt;Una-May O&amp;#x27;Reilly&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[InteL-VAEs: Adding Inductive Biases to Variational Auto-Encoders via Intermediary Latents. (arXiv:2106.13746v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.13746</id>
        <link href="http://arxiv.org/abs/2106.13746"/>
        <updated>2021-06-28T01:57:56.452Z</updated>
        <summary type="html"><![CDATA[We introduce a simple and effective method for learning VAEs with
controllable inductive biases by using an intermediary set of latent variables.
This allows us to overcome the limitations of the standard Gaussian prior
assumption. In particular, it allows us to impose desired properties like
sparsity or clustering on learned representations, and incorporate prior
information into the learned model. Our approach, which we refer to as the
Intermediary Latent Space VAE (InteL-VAE), is based around controlling the
stochasticity of the encoding process with the intermediary latent variables,
before deterministically mapping them forward to our target latent
representation, from which reconstruction is performed. This allows us to
maintain all the advantages of the traditional VAE framework, while
incorporating desired prior information, inductive biases, and even topological
information through the latent mapping. We show that this, in turn, allows
InteL-VAEs to learn both better generative models and representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Miao_N/0/1/0/all/0/1"&gt;Ning Miao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mathieu_E/0/1/0/all/0/1"&gt;Emile Mathieu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Siddharth_N/0/1/0/all/0/1"&gt;N. Siddharth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Teh_Y/0/1/0/all/0/1"&gt;Yee Whye Teh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rainforth_T/0/1/0/all/0/1"&gt;Tom Rainforth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Re-parameterizing VAEs for stability. (arXiv:2106.13739v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13739</id>
        <link href="http://arxiv.org/abs/2106.13739"/>
        <updated>2021-06-28T01:57:56.430Z</updated>
        <summary type="html"><![CDATA[We propose a theoretical approach towards the training numerical stability of
Variational AutoEncoders (VAE). Our work is motivated by recent studies
empowering VAEs to reach state of the art generative results on complex image
datasets. These very deep VAE architectures, as well as VAEs using more complex
output distributions, highlight a tendency to haphazardly produce high training
gradients as well as NaN losses. The empirical fixes proposed to train them
despite their limitations are neither fully theoretically grounded nor
generally sufficient in practice. Building on this, we localize the source of
the problem at the interface between the model's neural networks and their
output probabilistic distributions. We explain a common source of instability
stemming from an incautious formulation of the encoded Normal distribution's
variance, and apply the same approach on other, less obvious sources. We show
that by implementing small changes to the way we parameterize the Normal
distributions on which they rely, VAEs can securely be trained.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dehaene_D/0/1/0/all/0/1"&gt;David Dehaene&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brossard_R/0/1/0/all/0/1"&gt;R&amp;#xe9;my Brossard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic annotation for computational pathology: Multidisciplinary experience and best practice recommendations. (arXiv:2106.13689v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.13689</id>
        <link href="http://arxiv.org/abs/2106.13689"/>
        <updated>2021-06-28T01:57:56.412Z</updated>
        <summary type="html"><![CDATA[Recent advances in whole slide imaging (WSI) technology have led to the
development of a myriad of computer vision and artificial intelligence (AI)
based diagnostic, prognostic, and predictive algorithms. Computational
Pathology (CPath) offers an integrated solution to utilize information embedded
in pathology WSIs beyond what we obtain through visual assessment. For
automated analysis of WSIs and validation of machine learning (ML) models,
annotations at the slide, tissue and cellular levels are required. The
annotation of important visual constructs in pathology images is an important
component of CPath projects. Improper annotations can result in algorithms
which are hard to interpret and can potentially produce inaccurate and
inconsistent results. Despite the crucial role of annotations in CPath
projects, there are no well-defined guidelines or best practices on how
annotations should be carried out. In this paper, we address this shortcoming
by presenting the experience and best practices acquired during the execution
of a large-scale annotation exercise involving a multidisciplinary team of
pathologists, ML experts and researchers as part of the Pathology image data
Lake for Analytics, Knowledge and Education (PathLAKE) consortium. We present a
real-world case study along with examples of different types of annotations,
diagnostic algorithm, annotation data dictionary and annotation constructs. The
analyses reported in this work highlight best practice recommendations that can
be used as annotation guidelines over the lifecycle of a CPath project.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wahab_N/0/1/0/all/0/1"&gt;Noorul Wahab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Miligy_I/0/1/0/all/0/1"&gt;Islam M Miligy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dodd_K/0/1/0/all/0/1"&gt;Katherine Dodd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sahota_H/0/1/0/all/0/1"&gt;Harvir Sahota&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Toss_M/0/1/0/all/0/1"&gt;Michael Toss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wenqi Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jahanifar_M/0/1/0/all/0/1"&gt;Mostafa Jahanifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bilal_M/0/1/0/all/0/1"&gt;Mohsin Bilal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Graham_S/0/1/0/all/0/1"&gt;Simon Graham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Park_Y/0/1/0/all/0/1"&gt;Young Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hadjigeorghiou_G/0/1/0/all/0/1"&gt;Giorgos Hadjigeorghiou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bhalerao_A/0/1/0/all/0/1"&gt;Abhir Bhalerao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lashen_A/0/1/0/all/0/1"&gt;Ayat Lashen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ibrahim_A/0/1/0/all/0/1"&gt;Asmaa Ibrahim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Katayama_A/0/1/0/all/0/1"&gt;Ayaka Katayama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ebili_H/0/1/0/all/0/1"&gt;Henry O Ebili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Parkin_M/0/1/0/all/0/1"&gt;Matthew Parkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sorell_T/0/1/0/all/0/1"&gt;Tom Sorell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Raza_S/0/1/0/all/0/1"&gt;Shan E Ahmed Raza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hero_E/0/1/0/all/0/1"&gt;Emily Hero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Eldaly_H/0/1/0/all/0/1"&gt;Hesham Eldaly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsang_Y/0/1/0/all/0/1"&gt;Yee Wah Tsang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gopalakrishnan_K/0/1/0/all/0/1"&gt;Kishore Gopalakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Snead_D/0/1/0/all/0/1"&gt;David Snead&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rakha_E/0/1/0/all/0/1"&gt;Emad Rakha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rajpoot_N/0/1/0/all/0/1"&gt;Nasir Rajpoot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Minhas_F/0/1/0/all/0/1"&gt;Fayyaz Minhas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Goal Reinforcement Learning environments for simulated Franka Emika Panda robot. (arXiv:2106.13687v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13687</id>
        <link href="http://arxiv.org/abs/2106.13687"/>
        <updated>2021-06-28T01:57:56.281Z</updated>
        <summary type="html"><![CDATA[This technical report presents panda-gym, a set Reinforcement Learning (RL)
environments for the Franka Emika Panda robot integrated with OpenAI Gym. Five
tasks are included: reach, push, slide, pick & place and stack. They all follow
a Multi-Goal RL framework, allowing to use goal-oriented RL algorithms. To
foster open-research, we chose to use the open-source physics engine PyBullet.
The implementation chosen for this package allows to define very easily new
tasks or new robots. This report also presents a baseline of results obtained
with state-of-the-art model-free off-policy algorithms. panda-gym is
open-source at https://github.com/qgallouedec/panda-gym.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gallouedec_Q/0/1/0/all/0/1"&gt;Quentin Gallou&amp;#xe9;dec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cazin_N/0/1/0/all/0/1"&gt;Nicolas Cazin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dellandrea_E/0/1/0/all/0/1"&gt;Emmanuel Dellandr&amp;#xe9;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liming Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effects of boundary conditions in fully convolutional networks for learning spatio-temporal dynamics. (arXiv:2106.11160v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11160</id>
        <link href="http://arxiv.org/abs/2106.11160"/>
        <updated>2021-06-28T01:57:56.274Z</updated>
        <summary type="html"><![CDATA[Accurate modeling of boundary conditions is crucial in computational physics.
The ever increasing use of neural networks as surrogates for physics-related
problems calls for an improved understanding of boundary condition treatment,
and its influence on the network accuracy. In this paper, several strategies to
impose boundary conditions (namely padding, improved spatial context, and
explicit encoding of physical boundaries) are investigated in the context of
fully convolutional networks applied to recurrent tasks. These strategies are
evaluated on two spatio-temporal evolving problems modeled by partial
differential equations: the 2D propagation of acoustic waves (hyperbolic PDE)
and the heat equation (parabolic PDE). Results reveal a high sensitivity of
both accuracy and stability on the boundary implementation in such recurrent
tasks. It is then demonstrated that the choice of the optimal padding strategy
is directly linked to the data semantics. Furthermore, the inclusion of
additional input spatial context or explicit physics-based rules allows a
better handling of boundaries in particular for large number of recurrences,
resulting in more robust and stable neural networks, while facilitating the
design and versatility of such networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alguacil_A/0/1/0/all/0/1"&gt;Antonio Alguacil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pinto_G/0/1/0/all/0/1"&gt;Gon&amp;#xe7;alves Pinto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauerheim_M/0/1/0/all/0/1"&gt;Michael Bauerheim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacob_M/0/1/0/all/0/1"&gt;Marc C. Jacob&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreau_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phane Moreau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Circumpapillary OCT-Focused Hybrid Learning for Glaucoma Grading Using Tailored Prototypical Neural Networks. (arXiv:2106.13551v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.13551</id>
        <link href="http://arxiv.org/abs/2106.13551"/>
        <updated>2021-06-28T01:57:56.261Z</updated>
        <summary type="html"><![CDATA[Glaucoma is one of the leading causes of blindness worldwide and Optical
Coherence Tomography (OCT) is the quintessential imaging technique for its
detection. Unlike most of the state-of-the-art studies focused on glaucoma
detection, in this paper, we propose, for the first time, a novel framework for
glaucoma grading using raw circumpapillary B-scans. In particular, we set out a
new OCT-based hybrid network which combines hand-driven and deep learning
algorithms. An OCT-specific descriptor is proposed to extract hand-crafted
features related to the retinal nerve fibre layer (RNFL). In parallel, an
innovative CNN is developed using skip-connections to include tailored residual
and attention modules to refine the automatic features of the latent space. The
proposed architecture is used as a backbone to conduct a novel few-shot
learning based on static and dynamic prototypical networks. The k-shot paradigm
is redefined giving rise to a supervised end-to-end system which provides
substantial improvements discriminating between healthy, early and advanced
glaucoma samples. The training and evaluation processes of the dynamic
prototypical network are addressed from two fused databases acquired via
Heidelberg Spectralis system. Validation and testing results reach a
categorical accuracy of 0.9459 and 0.8788 for glaucoma grading, respectively.
Besides, the high performance reported by the proposed model for glaucoma
detection deserves a special mention. The findings from the class activation
maps are directly in line with the clinicians' opinion since the heatmaps
pointed out the RNFL as the most relevant structure for glaucoma diagnosis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Garcia_G/0/1/0/all/0/1"&gt;Gabriel Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Amor_R/0/1/0/all/0/1"&gt;Roc&amp;#xed;o del Amor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Colomer_A/0/1/0/all/0/1"&gt;Adri&amp;#xe1;n Colomer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Verdu_Monedero_R/0/1/0/all/0/1"&gt;Rafael Verd&amp;#xfa;-Monedero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Morales_Sanchez_J/0/1/0/all/0/1"&gt;Juan Morales-S&amp;#xe1;nchez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1"&gt;Valery Naranjo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Realistic molecule optimization on a learned graph manifold. (arXiv:2106.13318v1 [physics.chem-ph])]]></title>
        <id>http://arxiv.org/abs/2106.13318</id>
        <link href="http://arxiv.org/abs/2106.13318"/>
        <updated>2021-06-28T01:57:56.255Z</updated>
        <summary type="html"><![CDATA[Deep learning based molecular graph generation and optimization has recently
been attracting attention due to its great potential for de novo drug design.
On the one hand, recent models are able to efficiently learn a given graph
distribution, and many approaches have proven very effective to produce a
molecule that maximizes a given score. On the other hand, it was shown by
previous studies that generated optimized molecules are often unrealistic, even
with the inclusion of mechanics to enforce similarity to a dataset of real drug
molecules. In this work we use a hybrid approach, where the dataset
distribution is learned using an autoregressive model while the score
optimization is done using the Metropolis algorithm, biased toward the learned
distribution. We show that the resulting method, that we call learned realism
sampling (LRS), produces empirically more realistic molecules and outperforms
all recent baselines in the task of molecule optimization with similarity
constraints.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Brossard_R/0/1/0/all/0/1"&gt;R&amp;#xe9;my Brossard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Frigo_O/0/1/0/all/0/1"&gt;Oriel Frigo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Dehaene_D/0/1/0/all/0/1"&gt;David Dehaene&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-training Converts Weak Learners to Strong Learners in Mixture Models. (arXiv:2106.13805v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13805</id>
        <link href="http://arxiv.org/abs/2106.13805"/>
        <updated>2021-06-28T01:57:56.249Z</updated>
        <summary type="html"><![CDATA[We consider a binary classification problem when the data comes from a
mixture of two isotropic distributions satisfying concentration and
anti-concentration properties enjoyed by log-concave distributions among
others. We show that there exists a universal constant $C_{\mathrm{err}}>0$
such that if a pseudolabeler $\boldsymbol{\beta}_{\mathrm{pl}}$ can achieve
classification error at most $C_{\mathrm{err}}$, then for any $\varepsilon>0$,
an iterative self-training algorithm initialized at $\boldsymbol{\beta}_0 :=
\boldsymbol{\beta}_{\mathrm{pl}}$ using pseudolabels $\hat y =
\mathrm{sgn}(\langle \boldsymbol{\beta}_t, \mathbf{x}\rangle)$ and using at
most $\tilde O(d/\varepsilon^2)$ unlabeled examples suffices to learn the
Bayes-optimal classifier up to $\varepsilon$ error, where $d$ is the ambient
dimension. That is, self-training converts weak learners to strong learners
using only unlabeled examples. We additionally show that by running gradient
descent on the logistic loss one can obtain a pseudolabeler
$\boldsymbol{\beta}_{\mathrm{pl}}$ with classification error $C_{\mathrm{err}}$
using only $O(d)$ labeled examples (i.e., independent of $\varepsilon$).
Together our results imply that mixture models can be learned to within
$\varepsilon$ of the Bayes-optimal accuracy using at most $O(d)$ labeled
examples and $\tilde O(d/\varepsilon^2)$ unlabeled examples by way of a
semi-supervised self-training algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Frei_S/0/1/0/all/0/1"&gt;Spencer Frei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1"&gt;Difan Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zixiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1"&gt;Quanquan Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multitask Learning for Citation Purpose Classification. (arXiv:2106.13275v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13275</id>
        <link href="http://arxiv.org/abs/2106.13275"/>
        <updated>2021-06-28T01:57:56.242Z</updated>
        <summary type="html"><![CDATA[We present our entry into the 2021 3C Shared Task Citation Context
Classification based on Purpose competition. The goal of the competition is to
classify a citation in a scientific article based on its purpose. This task is
important because it could potentially lead to more comprehensive ways of
summarizing the purpose and uses of scientific articles, but it is also
difficult, mainly due to the limited amount of available training data in which
the purposes of each citation have been hand-labeled, along with the
subjectivity of these labels. Our entry in the competition is a multi-task
model that combines multiple modules designed to handle the problem from
different perspectives, including hand-generated linguistic features, TF-IDF
features, and an LSTM-with-attention model. We also provide an ablation study
and feature analysis whose insights could lead to future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oesterling_A/0/1/0/all/0/1"&gt;Alex Oesterling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosal_A/0/1/0/all/0/1"&gt;Angikar Ghosal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Haoyang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_R/0/1/0/all/0/1"&gt;Rui Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baig_Y/0/1/0/all/0/1"&gt;Yasa Baig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Semenova_L/0/1/0/all/0/1"&gt;Lesia Semenova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudin_C/0/1/0/all/0/1"&gt;Cynthia Rudin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Interpretable Criminal Charge Prediction and Algorithmic Bias. (arXiv:2106.13456v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13456</id>
        <link href="http://arxiv.org/abs/2106.13456"/>
        <updated>2021-06-28T01:57:56.217Z</updated>
        <summary type="html"><![CDATA[While predictive policing has become increasingly common in assisting with
decisions in the criminal justice system, the use of these results is still
controversial. Some software based on deep learning lacks accuracy (e.g., in
F-1), and many decision processes are not transparent causing doubt about
decision bias, such as perceived racial, age, and gender disparities. This
paper addresses bias issues with post-hoc explanations to provide a trustable
prediction of whether a person will receive future criminal charges given one's
previous criminal records by learning temporal behavior patterns over twenty
years. Bi-LSTM relieves the vanishing gradient problem, and attentional
mechanisms allows learning and interpretation of feature importance. Our
approach shows consistent and reliable prediction precision and recall on a
real-life dataset. Our analysis of the importance of each input feature shows
the critical causal impact on decision-making, suggesting that criminal
histories are statistically significant factors, while identifiers, such as
race, gender, and age, are not. Finally, our algorithm indicates that a suspect
tends to gradually rather than suddenly increase crime severity level over
time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1"&gt;Abdul Rafae Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jia Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varsanyi_P/0/1/0/all/0/1"&gt;Peter Varsanyi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pabreja_R/0/1/0/all/0/1"&gt;Rachit Pabreja&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Masksembles for Uncertainty Estimation. (arXiv:2012.08334v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08334</id>
        <link href="http://arxiv.org/abs/2012.08334"/>
        <updated>2021-06-28T01:57:56.209Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have amply demonstrated their prowess but estimating the
reliability of their predictions remains challenging. Deep Ensembles are widely
considered as being one of the best methods for generating uncertainty
estimates but are very expensive to train and evaluate. MC-Dropout is another
popular alternative, which is less expensive, but also less reliable. Our
central intuition is that there is a continuous spectrum of ensemble-like
models of which MC-Dropout and Deep Ensembles are extreme examples. The first
uses an effectively infinite number of highly correlated models while the
second relies on a finite number of independent models.

To combine the benefits of both, we introduce Masksembles. Instead of
randomly dropping parts of the network as in MC-dropout, Masksemble relies on a
fixed number of binary masks, which are parameterized in a way that allows to
change correlations between individual models. Namely, by controlling the
overlap between the masks and their density one can choose the optimal
configuration for the task at hand. This leads to a simple and easy to
implement method with performance on par with Ensembles at a fraction of the
cost. We experimentally validate Masksembles on two widely used datasets,
CIFAR10 and ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Durasov_N/0/1/0/all/0/1"&gt;Nikita Durasov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1"&gt;Timur Bagautdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baque_P/0/1/0/all/0/1"&gt;Pierre Baque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1"&gt;Pascal Fua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimising simulations for diphoton production at hadron colliders using amplitude neural networks. (arXiv:2106.09474v2 [hep-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09474</id>
        <link href="http://arxiv.org/abs/2106.09474"/>
        <updated>2021-06-28T01:57:56.202Z</updated>
        <summary type="html"><![CDATA[Machine learning technology has the potential to dramatically optimise event
generation and simulations. We continue to investigate the use of neural
networks to approximate matrix elements for high-multiplicity scattering
processes. We focus on the case of loop-induced diphoton production through
gluon fusion and develop a realistic simulation method that can be applied to
hadron collider observables. Neural networks are trained using the one-loop
amplitudes implemented in the NJet C++ library and interfaced to the Sherpa
Monte Carlo event generator where we perform a detailed study for $2\to3$ and
$2\to4$ scattering problems. We also consider how the trained networks perform
when varying the kinematic cuts effecting the phase space and the reliability
of the neural network simulations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-ph/1/au:+Aylett_Bullock_J/0/1/0/all/0/1"&gt;Joseph Aylett-Bullock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Badger_S/0/1/0/all/0/1"&gt;Simon Badger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Moodie_R/0/1/0/all/0/1"&gt;Ryan Moodie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the (Un-)Avoidability of Adversarial Examples. (arXiv:2106.13326v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13326</id>
        <link href="http://arxiv.org/abs/2106.13326"/>
        <updated>2021-06-28T01:57:56.195Z</updated>
        <summary type="html"><![CDATA[The phenomenon of adversarial examples in deep learning models has caused
substantial concern over their reliability. While many deep neural networks
have shown impressive performance in terms of predictive accuracy, it has been
shown that in many instances an imperceptible perturbation can falsely flip the
network's prediction. Most research has then focused on developing defenses
against adversarial attacks or learning under a worst-case adversarial loss. In
this work, we take a step back and aim to provide a framework for determining
whether a model's label change under small perturbation is justified (and when
it is not). We carefully argue that adversarial robustness should be defined as
a locally adaptive measure complying with the underlying distribution. We then
suggest a definition for an adaptive robust loss, derive an empirical version
of it, and develop a resulting data-augmentation framework. We prove that our
adaptive data-augmentation maintains consistency of 1-nearest neighbor
classification under deterministic labels and provide illustrative empirical
evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1"&gt;Sadia Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Urner_R/0/1/0/all/0/1"&gt;Ruth Urner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MDP Playground: A Design and Debug Testbed for Reinforcement Learning. (arXiv:1909.07750v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.07750</id>
        <link href="http://arxiv.org/abs/1909.07750"/>
        <updated>2021-06-28T01:57:56.178Z</updated>
        <summary type="html"><![CDATA[We present \emph{MDP Playground}, an efficient testbed for Reinforcement
Learning (RL) agents with \textit{orthogonal} dimensions that can be controlled
independently to challenge agents in different ways and obtain varying degrees
of hardness in generated environments. We consider and allow control over a
wide variety of dimensions, including \textit{delayed rewards},
\textit{rewardable sequences}, \textit{density of rewards},
\textit{stochasticity}, \textit{image representations}, \textit{irrelevant
features}, \textit{time unit}, \textit{action range} and more. We define a
parameterised collection of fast-to-run toy environments in \textit{OpenAI Gym}
by varying these dimensions and propose to use these for the initial design and
development of agents. We also provide wrappers that inject these dimensions
into complex environments from \textit{Atari} and \textit{Mujoco} to allow for
evaluating agent robustness. We further provide various example use-cases and
instructions on how to use \textit{MDP Playground} to design and debug agents.
We believe that \textit{MDP Playground} is a valuable testbed for researchers
designing new, adaptive and intelligent RL agents and those wanting to unit
test their agents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rajan_R/0/1/0/all/0/1"&gt;Raghu Rajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_J/0/1/0/all/0/1"&gt;Jessica Lizeth Borja Diaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guttikonda_S/0/1/0/all/0/1"&gt;Suresh Guttikonda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferreira_F/0/1/0/all/0/1"&gt;Fabio Ferreira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biedenkapp_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Biedenkapp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hartz_J/0/1/0/all/0/1"&gt;Jan Ole von Hartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1"&gt;Frank Hutter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TTS-Portuguese Corpus: a corpus for speech synthesis in Brazilian Portuguese. (arXiv:2005.05144v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.05144</id>
        <link href="http://arxiv.org/abs/2005.05144"/>
        <updated>2021-06-28T01:57:56.172Z</updated>
        <summary type="html"><![CDATA[Speech provides a natural way for human-computer interaction. In particular,
speech synthesis systems are popular in different applications, such as
personal assistants, GPS applications, screen readers and accessibility tools.
However, not all languages are on the same level when in terms of resources and
systems for speech synthesis. This work consists of creating publicly available
resources for Brazilian Portuguese in the form of a novel dataset along with
deep learning models for end-to-end speech synthesis. Such dataset has 10.5
hours from a single speaker, from which a Tacotron 2 model with the RTISI-LA
vocoder presented the best performance, achieving a 4.03 MOS value. The
obtained results are comparable to related works covering English language and
the state-of-the-art in Portuguese.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Casanova_E/0/1/0/all/0/1"&gt;Edresson Casanova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Junior_A/0/1/0/all/0/1"&gt;Arnaldo Candido Junior&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shulby_C/0/1/0/all/0/1"&gt;Christopher Shulby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oliveira_F/0/1/0/all/0/1"&gt;Frederico Santos de Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Teixeira_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Paulo Teixeira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ponti_M/0/1/0/all/0/1"&gt;Moacir Antonelli Ponti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Aluisio_S/0/1/0/all/0/1"&gt;Sandra Maria Aluisio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You are AllSet: A Multiset Function Framework for Hypergraph Neural Networks. (arXiv:2106.13264v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13264</id>
        <link href="http://arxiv.org/abs/2106.13264"/>
        <updated>2021-06-28T01:57:56.165Z</updated>
        <summary type="html"><![CDATA[Hypergraphs are used to model higher-order interactions amongst agents and
there exist many practically relevant instances of hypergraph datasets. To
enable efficient processing of hypergraph-structured data, several hypergraph
neural network platforms have been proposed for learning hypergraph properties
and structure, with a special focus on node classification. However, almost all
existing methods use heuristic propagation rules and offer suboptimal
performance on many datasets. We propose AllSet, a new hypergraph neural
network paradigm that represents a highly general framework for (hyper)graph
neural networks and for the first time implements hypergraph neural network
layers as compositions of two multiset functions that can be efficiently
learned for each task and each dataset. Furthermore, AllSet draws on new
connections between hypergraph neural networks and recent advances in deep
learning of multiset functions. In particular, the proposed architecture
utilizes Deep Sets and Set Transformer architectures that allow for significant
modeling flexibility and offer high expressive power. To evaluate the
performance of AllSet, we conduct the most extensive experiments to date
involving ten known benchmarking datasets and three newly curated datasets that
represent significant challenges for hypergraph node classification. The
results demonstrate that AllSet has the unique ability to consistently either
match or outperform all other hypergraph neural networks across the tested
datasets. Our implementation and dataset will be released upon acceptance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chien_E/0/1/0/all/0/1"&gt;Eli Chien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1"&gt;Chao Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jianhao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milenkovic_O/0/1/0/all/0/1"&gt;Olgica Milenkovic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Graph Classification over Non-IID Graphs. (arXiv:2106.13423v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13423</id>
        <link href="http://arxiv.org/abs/2106.13423"/>
        <updated>2021-06-28T01:57:56.158Z</updated>
        <summary type="html"><![CDATA[Federated learning has emerged as an important paradigm for training machine
learning models in different domains. For graph-level tasks such as graph
classification, graphs can also be regarded as a special type of data samples,
which can be collected and stored in separate local systems. Similar to other
domains, multiple local systems, each holding a small set of graphs, may
benefit from collaboratively training a powerful graph mining model, such as
the popular graph neural networks (GNNs). To provide more motivation towards
such endeavors, we analyze real-world graphs from different domains to confirm
that they indeed share certain graph properties that are statistically
significant compared with random graphs. However, we also find that different
sets of graphs, even from the same domain or same dataset, are non-IID
regarding both graph structures and node features. To handle this, we propose a
graph clustering federated learning (GCFL) framework that dynamically finds
clusters of local systems based on the gradients of GNNs, and theoretically
justify that such clusters can reduce the structure and feature heterogeneity
among graphs owned by the local systems. Moreover, we observe the gradients of
GNNs to be rather fluctuating in GCFL which impedes high-quality clustering,
and design a gradient sequence-based clustering mechanism based on dynamic time
warping (GCFL+). Extensive experimental results and in-depth analysis
demonstrate the effectiveness of our proposed frameworks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1"&gt;Han Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jing Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1"&gt;Li Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Carl Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Littlestone Classes are Privately Online Learnable. (arXiv:2106.13513v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13513</id>
        <link href="http://arxiv.org/abs/2106.13513"/>
        <updated>2021-06-28T01:57:56.152Z</updated>
        <summary type="html"><![CDATA[We consider the problem of online classification under a privacy constraint.
In this setting a learner observes sequentially a stream of labelled examples
$(x_t, y_t)$, for $1 \leq t \leq T$, and returns at each iteration $t$ a
hypothesis $h_t$ which is used to predict the label of each new example $x_t$.
The learner's performance is measured by her regret against a known hypothesis
class $\mathcal{H}$. We require that the algorithm satisfies the following
privacy constraint: the sequence $h_1, \ldots, h_T$ of hypotheses output by the
algorithm needs to be an $(\epsilon, \delta)$-differentially private function
of the whole input sequence $(x_1, y_1), \ldots, (x_T, y_T)$. We provide the
first non-trivial regret bound for the realizable setting. Specifically, we
show that if the class $\mathcal{H}$ has constant Littlestone dimension then,
given an oblivious sequence of labelled examples, there is a private learner
that makes in expectation at most $O(\log T)$ mistakes -- comparable to the
optimal mistake bound in the non-private case, up to a logarithmic factor.
Moreover, for general values of the Littlestone dimension $d$, the same mistake
bound holds but with a doubly-exponential in $d$ factor. A recent line of work
has demonstrated a strong connection between classes that are online learnable
and those that are differentially-private learnable. Our results strengthen
this connection and show that an online learning algorithm can in fact be
directly privatized (in the realizable setting). We also discuss an adaptive
setting and provide a sublinear regret bound of $O(\sqrt{T})$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Golowich_N/0/1/0/all/0/1"&gt;Noah Golowich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Livni_R/0/1/0/all/0/1"&gt;Roi Livni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A hybrid model-based and learning-based approach for classification using limited number of training samples. (arXiv:2106.13436v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13436</id>
        <link href="http://arxiv.org/abs/2106.13436"/>
        <updated>2021-06-28T01:57:56.131Z</updated>
        <summary type="html"><![CDATA[The fundamental task of classification given a limited number of training
data samples is considered for physical systems with known parametric
statistical models. The standalone learning-based and statistical model-based
classifiers face major challenges towards the fulfillment of the classification
task using a small training set. Specifically, classifiers that solely rely on
the physics-based statistical models usually suffer from their inability to
properly tune the underlying unobservable parameters, which leads to a
mismatched representation of the system's behaviors. Learning-based
classifiers, on the other hand, typically rely on a large number of training
data from the underlying physical process, which might not be feasible in most
practical scenarios. In this paper, a hybrid classification method -- termed
HyPhyLearn -- is proposed that exploits both the physics-based statistical
models and the learning-based classifiers. The proposed solution is based on
the conjecture that HyPhyLearn would alleviate the challenges associated with
the individual approaches of learning-based and statistical model-based
classifiers by fusing their respective strengths. The proposed hybrid approach
first estimates the unobservable model parameters using the available
(suboptimal) statistical estimation procedures, and subsequently use the
physics-based statistical models to generate synthetic data. Then, the training
data samples are incorporated with the synthetic data in a learning-based
classifier that is based on domain-adversarial training of neural networks.
Specifically, in order to address the mismatch problem, the classifier learns a
mapping from the training data and the synthetic data to a common feature
space. Simultaneously, the classifier is trained to find discriminative
features within this space in order to fulfill the classification task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nooraiepour_A/0/1/0/all/0/1"&gt;Alireza Nooraiepour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bajwa_W/0/1/0/all/0/1"&gt;Waheed U. Bajwa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mandayam_N/0/1/0/all/0/1"&gt;Narayan B. Mandayam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction of geophysical properties of rocks on rare well data and attributes of seismic waves by machine learning methods on the example of the Achimov formation. (arXiv:2106.13274v1 [physics.geo-ph])]]></title>
        <id>http://arxiv.org/abs/2106.13274</id>
        <link href="http://arxiv.org/abs/2106.13274"/>
        <updated>2021-06-28T01:57:56.124Z</updated>
        <summary type="html"><![CDATA[Purpose of this research is to forecast the development of sand bodies in
productive sediments based on well log data and seismic attributes. The object
of the study is the productive intervals of Achimov sedimentary complex in the
part of oil field located in Western Siberia. The research shows a
technological stack of machine learning algorithms, methods for enriching the
source data with synthetic ones and algorithms for creating new features. The
result was the model of regression relationship between the values of natural
radioactivity of rocks and seismic wave field attributes with an acceptable
prediction quality. Acceptable quality of the forecast is confirmed both by
model cross validation, and by the data obtained following the results of new
well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Ivlev_D/0/1/0/all/0/1"&gt;Dmitry Ivlev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vulnerability and Transaction behavior based detection of Malicious Smart Contracts. (arXiv:2106.13422v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.13422</id>
        <link href="http://arxiv.org/abs/2106.13422"/>
        <updated>2021-06-28T01:57:56.117Z</updated>
        <summary type="html"><![CDATA[Smart Contracts (SCs) in Ethereum can automate tasks and provide different
functionalities to a user. Such automation is enabled by the `Turing-complete'
nature of the programming language (Solidity) in which SCs are written. This
also opens up different vulnerabilities and bugs in SCs that malicious actors
exploit to carry out malicious or illegal activities on the cryptocurrency
platform. In this work, we study the correlation between malicious activities
and the vulnerabilities present in SCs and find that some malicious activities
are correlated with certain types of vulnerabilities. We then develop and study
the feasibility of a scoring mechanism that corresponds to the severity of the
vulnerabilities present in SCs to determine if it is a relevant feature to
identify suspicious SCs. We analyze the utility of severity score towards
detection of suspicious SCs using unsupervised machine learning (ML) algorithms
across different temporal granularities and identify behavioral changes. In our
experiments with on-chain SCs, we were able to find a total of 1094 benign SCs
across different granularities which behave similar to malicious SCs, with the
inclusion of the smart contract vulnerability scores in the feature set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_R/0/1/0/all/0/1"&gt;Rachit Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thapliyal_T/0/1/0/all/0/1"&gt;Tanmay Thapliyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1"&gt;Sandeep Kumar Shukla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing. (arXiv:2007.15779v5 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.15779</id>
        <link href="http://arxiv.org/abs/2007.15779"/>
        <updated>2021-06-28T01:57:56.105Z</updated>
        <summary type="html"><![CDATA[Pretraining large neural language models, such as BERT, has led to impressive
gains on many natural language processing (NLP) tasks. However, most
pretraining efforts focus on general domain corpora, such as newswire and Web.
A prevailing assumption is that even domain-specific pretraining can benefit by
starting from general-domain language models. In this paper, we challenge this
assumption by showing that for domains with abundant unlabeled text, such as
biomedicine, pretraining language models from scratch results in substantial
gains over continual pretraining of general-domain language models. To
facilitate this investigation, we compile a comprehensive biomedical NLP
benchmark from publicly-available datasets. Our experiments show that
domain-specific pretraining serves as a solid foundation for a wide range of
biomedical NLP tasks, leading to new state-of-the-art results across the board.
Further, in conducting a thorough evaluation of modeling choices, both for
pretraining and task-specific fine-tuning, we discover that some common
practices are unnecessary with BERT models, such as using complex tagging
schemes in named entity recognition (NER). To help accelerate research in
biomedical NLP, we have released our state-of-the-art pretrained and
task-specific models for the community, and created a leaderboard featuring our
BLURB benchmark (short for Biomedical Language Understanding & Reasoning
Benchmark) at https://aka.ms/BLURB.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yu Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tinn_R/0/1/0/all/0/1"&gt;Robert Tinn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Hao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lucas_M/0/1/0/all/0/1"&gt;Michael Lucas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1"&gt;Naoto Usuyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaodong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1"&gt;Tristan Naumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1"&gt;Hoifung Poon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Robot Deep Reinforcement Learning for Mobile Navigation. (arXiv:2106.13280v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.13280</id>
        <link href="http://arxiv.org/abs/2106.13280"/>
        <updated>2021-06-28T01:57:56.092Z</updated>
        <summary type="html"><![CDATA[Deep reinforcement learning algorithms require large and diverse datasets in
order to learn successful policies for perception-based mobile navigation.
However, gathering such datasets with a single robot can be prohibitively
expensive. Collecting data with multiple different robotic platforms with
possibly different dynamics is a more scalable approach to large-scale data
collection. But how can deep reinforcement learning algorithms leverage such
heterogeneous datasets? In this work, we propose a deep reinforcement learning
algorithm with hierarchically integrated models (HInt). At training time, HInt
learns separate perception and dynamics models, and at test time, HInt
integrates the two models in a hierarchical manner and plans actions with the
integrated model. This method of planning with hierarchically integrated models
allows the algorithm to train on datasets gathered by a variety of different
platforms, while respecting the physical capabilities of the deployment robot
at test time. Our mobile navigation experiments show that HInt outperforms
conventional hierarchical policies and single-source approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_K/0/1/0/all/0/1"&gt;Katie Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kahn_G/0/1/0/all/0/1"&gt;Gregory Kahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vision Transformer Architecture Search. (arXiv:2106.13700v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13700</id>
        <link href="http://arxiv.org/abs/2106.13700"/>
        <updated>2021-06-28T01:57:56.072Z</updated>
        <summary type="html"><![CDATA[Recently, transformers have shown great superiority in solving computer
vision tasks by modeling images as a sequence of manually-split patches with
self-attention mechanism. However, current architectures of vision transformers
(ViTs) are simply inherited from natural language processing (NLP) tasks and
have not been sufficiently investigated and optimized. In this paper, we make a
further step by examining the intrinsic structure of transformers for vision
tasks and propose an architecture search method, dubbed ViTAS, to search for
the optimal architecture with similar hardware budgets. Concretely, we design a
new effective yet efficient weight sharing paradigm for ViTs, such that
architectures with different token embedding, sequence size, number of heads,
width, and depth can be derived from a single super-transformer. Moreover, to
cater for the variance of distinct architectures, we introduce \textit{private}
class token and self-attention maps in the super-transformer. In addition, to
adapt the searching for different budgets, we propose to search the sampling
probability of identity operation. Experimental results show that our ViTAS
attains excellent results compared to existing pure transformer architectures.
For example, with $1.3$G FLOPs budget, our searched architecture achieves
$74.7\%$ top-$1$ accuracy on ImageNet and is $2.5\%$ superior than the current
baseline ViT architecture. Code is available at
\url{https://github.com/xiusu/ViTAS}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1"&gt;Xiu Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1"&gt;Shan You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiyang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1"&gt;Mingkai Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1"&gt;Chen Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Changshui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-guided Machine Learning for Remotely Sensed In-Season Crop Growth Estimation. (arXiv:2106.13323v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13323</id>
        <link href="http://arxiv.org/abs/2106.13323"/>
        <updated>2021-06-28T01:57:56.059Z</updated>
        <summary type="html"><![CDATA[Advanced machine learning techniques have been used in remote sensing (RS)
applications such as crop mapping and yield prediction, but remain
under-utilized for tracking crop progress. In this study, we demonstrate the
use of agronomic knowledge of crop growth drivers in a Long Short-Term
Memory-based, Domain-guided neural network (DgNN) for in-season crop progress
estimation. The DgNN uses a branched structure and attention to separate
independent crop growth drivers and capture their varying importance throughout
the growing season. The DgNN is implemented for corn, using RS data in Iowa for
the period 2003-2019, with USDA crop progress reports used as ground truth.
State-wide DgNN performance shows significant improvement over sequential and
dense-only NN structures, and a widely-used Hidden Markov Model method. The
DgNN had a 3.5% higher Nash-Sutfliffe efficiency over all growth stages and 33%
more weeks with highest cosine similarity than the other NNs during test years.
The DgNN and Sequential NN were more robust during periods of abnormal crop
progress, though estimating the Silking-Grainfill transition was difficult for
all methods. Finally, Uniform Manifold Approximation and Projection
visualizations of layer activations showed how LSTM-based NNs separate crop
growth time-series differently from a dense-only structure. Results from this
study exhibit both the viability of NNs in crop growth stage estimation (CGSE)
and the benefits of using domain knowledge. The DgNN methodology presented here
can be extended to provide near-real time CGSE of other crops.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Worrall_G/0/1/0/all/0/1"&gt;George Worrall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rangarajan_A/0/1/0/all/0/1"&gt;Anand Rangarajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Judge_J/0/1/0/all/0/1"&gt;Jasmeet Judge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joslim: Joint Widths and Weights Optimization for Slimmable Neural Networks. (arXiv:2007.11752v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.11752</id>
        <link href="http://arxiv.org/abs/2007.11752"/>
        <updated>2021-06-28T01:57:56.052Z</updated>
        <summary type="html"><![CDATA[Slimmable neural networks provide a flexible trade-off front between
prediction error and computational requirement (such as the number of
floating-point operations or FLOPs) with the same storage requirement as a
single model. They are useful for reducing maintenance overhead for deploying
models to devices with different memory constraints and are useful for
optimizing the efficiency of a system with many CNNs. However, existing
slimmable network approaches either do not optimize layer-wise widths or
optimize the shared-weights and layer-wise widths independently, thereby
leaving significant room for improvement by joint width and weight
optimization. In this work, we propose a general framework to enable joint
optimization for both width configurations and weights of slimmable networks.
Our framework subsumes conventional and NAS-based slimmable methods as special
cases and provides flexibility to improve over existing methods. From a
practical standpoint, we propose Joslim, an algorithm that jointly optimizes
both the widths and weights for slimmable nets, which outperforms existing
methods for optimizing slimmable networks across various networks, datasets,
and objectives. Quantitatively, improvements up to 1.7% and 8% in top-1
accuracy on the ImageNet dataset can be attained for MobileNetV2 considering
FLOPs and memory footprint, respectively. Our results highlight the potential
of optimizing the channel counts for different layers jointly with the weights
for slimmable networks. Code available at https://github.com/cmu-enyac/Joslim.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1"&gt;Ting-Wu Chin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morcos_A/0/1/0/all/0/1"&gt;Ari S. Morcos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marculescu_D/0/1/0/all/0/1"&gt;Diana Marculescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A variational autoencoder approach for choice set generation and implicit perception of alternatives in choice modeling. (arXiv:2106.13319v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.13319</id>
        <link href="http://arxiv.org/abs/2106.13319"/>
        <updated>2021-06-28T01:57:56.042Z</updated>
        <summary type="html"><![CDATA[This paper derives the generalized extreme value (GEV) model with implicit
availability/perception (IAP) of alternatives and proposes a variational
autoencoder (VAE) approach for choice set generation and implicit perception of
alternatives. Specifically, the cross-nested logit (CNL) model with IAP is
derived as an example of IAP-GEV models. The VAE approach is adapted to model
the choice set generation process, in which the likelihood of perceiving chosen
alternatives in the choice set is maximized. The VAE approach for route choice
set generation is exemplified using a real dataset. IAP- CNL model estimated
has the best performance in terms of goodness-of-fit and prediction
performance, compared to multinomial logit models and conventional choice set
generation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_R/0/1/0/all/0/1"&gt;Rui Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bekhor_S/0/1/0/all/0/1"&gt;Shlomo Bekhor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chebyshev-Cantelli PAC-Bayes-Bennett Inequality for the Weighted Majority Vote. (arXiv:2106.13624v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13624</id>
        <link href="http://arxiv.org/abs/2106.13624"/>
        <updated>2021-06-28T01:57:56.035Z</updated>
        <summary type="html"><![CDATA[We present a new second-order oracle bound for the expected risk of a
weighted majority vote. The bound is based on a novel parametric form of the
Chebyshev-Cantelli inequality (a.k.a.\ one-sided Chebyshev's), which is
amenable to efficient minimization. The new form resolves the optimization
challenge faced by prior oracle bounds based on the Chebyshev-Cantelli
inequality, the C-bounds [Germain et al., 2015], and, at the same time, it
improves on the oracle bound based on second order Markov's inequality
introduced by Masegosa et al. [2020]. We also derive the PAC-Bayes-Bennett
inequality, which we use for empirical estimation of the oracle bound. The
PAC-Bayes-Bennett inequality improves on the PAC-Bayes-Bernstein inequality by
Seldin et al. [2012]. We provide an empirical evaluation demonstrating that the
new bounds can improve on the work by Masegosa et al. [2020]. Both the
parametric form of the Chebyshev-Cantelli inequality and the PAC-Bayes-Bennett
inequality may be of independent interest for the study of concentration of
measure in other domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yi-Shan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masegosa_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9;s R. Masegosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lorenzen_S/0/1/0/all/0/1"&gt;Stephan S. Lorenzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Igel_C/0/1/0/all/0/1"&gt;Christian Igel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seldin_Y/0/1/0/all/0/1"&gt;Yevgeny Seldin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Phoneme-aware and Channel-wise Attentive Learning for Text DependentSpeaker Verification. (arXiv:2106.13514v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.13514</id>
        <link href="http://arxiv.org/abs/2106.13514"/>
        <updated>2021-06-28T01:57:56.011Z</updated>
        <summary type="html"><![CDATA[This paper proposes a multi-task learning network with phoneme-aware and
channel-wise attentive learning strategies for text-dependent Speaker
Verification (SV). In the proposed structure, the frame-level multi-task
learning along with the segment-level adversarial learning is adopted for
speaker embedding extraction. The phoneme-aware attentive pooling is exploited
on frame-level features in the main network for speaker classifier, with the
corresponding posterior probability for the phoneme distribution in the
auxiliary subnet. Further, the introduction of Squeeze and Excitation
(SE-block) performs dynamic channel-wise feature recalibration, which improves
the representational ability. The proposed method exploits speaker
idiosyncrasies associated with pass-phrases, and is further improved by the
phoneme-aware attentive pooling and SE-block from temporal and channel-wise
aspects, respectively. The experiments conducted on RSR2015 Part 1 database
confirm that the proposed system achieves outstanding results for textdependent
SV.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_Q/0/1/0/all/0/1"&gt;Qingyang Hong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Promises and Pitfalls of Black-Box Concept Learning Models. (arXiv:2106.13314v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13314</id>
        <link href="http://arxiv.org/abs/2106.13314"/>
        <updated>2021-06-28T01:57:56.005Z</updated>
        <summary type="html"><![CDATA[Machine learning models that incorporate concept learning as an intermediate
step in their decision making process can match the performance of black-box
predictive models while retaining the ability to explain outcomes in human
understandable terms. However, we demonstrate that the concept representations
learned by these models encode information beyond the pre-defined concepts, and
that natural mitigation strategies do not fully work, rendering the
interpretation of the downstream prediction misleading. We describe the
mechanism underlying the information leakage and suggest recourse for
mitigating its effects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahinpei_A/0/1/0/all/0/1"&gt;Anita Mahinpei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1"&gt;Justin Clark&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lage_I/0/1/0/all/0/1"&gt;Isaac Lage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1"&gt;Finale Doshi-Velez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1"&gt;Weiwei Pan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpreting Depression From Question-wise Long-term Video Recording of SDS Evaluation. (arXiv:2106.13393v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13393</id>
        <link href="http://arxiv.org/abs/2106.13393"/>
        <updated>2021-06-28T01:57:55.995Z</updated>
        <summary type="html"><![CDATA[Self-Rating Depression Scale (SDS) questionnaire has frequently been used for
efficient depression preliminary screening. However, the uncontrollable
self-administered measure can be easily affected by insouciantly or deceptively
answering, and producing the different results with the clinician-administered
Hamilton Depression Rating Scale (HDRS) and the final diagnosis. Clinically,
facial expression (FE) and actions play a vital role in clinician-administered
evaluation, while FE and action are underexplored for self-administered
evaluations. In this work, we collect a novel dataset of 200 subjects to
evidence the validity of self-rating questionnaires with their corresponding
question-wise video recording. To automatically interpret depression from the
SDS evaluation and the paired video, we propose an end-to-end hierarchical
framework for the long-term variable-length video, which is also conditioned on
the questionnaire results and the answering time. Specifically, we resort to a
hierarchical model which utilizes a 3D CNN for local temporal pattern
exploration and a redundancy-aware self-attention (RAS) scheme for
question-wise global feature aggregation. Targeting for the redundant long-term
FE video processing, our RAS is able to effectively exploit the correlations of
each video clip within a question set to emphasize the discriminative
information and eliminate the redundancy based on feature pair-wise affinity.
Then, the question-wise video feature is concatenated with the questionnaire
scores for final depression detection. Our thorough evaluations also show the
validity of fusing SDS evaluation and its video recording, and the superiority
of our framework to the conventional state-of-the-art temporal modeling
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1"&gt;Wanqing Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1"&gt;Lizhong Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jihong Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Hui Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CausalCity: Complex Simulations with Agency for Causal Discovery and Reasoning. (arXiv:2106.13364v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.13364</id>
        <link href="http://arxiv.org/abs/2106.13364"/>
        <updated>2021-06-28T01:57:55.987Z</updated>
        <summary type="html"><![CDATA[The ability to perform causal and counterfactual reasoning are central
properties of human intelligence. Decision-making systems that can perform
these types of reasoning have the potential to be more generalizable and
interpretable. Simulations have helped advance the state-of-the-art in this
domain, by providing the ability to systematically vary parameters (e.g.,
confounders) and generate examples of the outcomes in the case of
counterfactual scenarios. However, simulating complex temporal causal events in
multi-agent scenarios, such as those that exist in driving and vehicle
navigation, is challenging. To help address this, we present a high-fidelity
simulation environment that is designed for developing algorithms for causal
discovery and counterfactual reasoning in the safety-critical context. A core
component of our work is to introduce \textit{agency}, such that it is simple
to define and create complex scenarios using high-level definitions. The
vehicles then operate with agency to complete these objectives, meaning
low-level behaviors need only be controlled if necessary. We perform
experiments with three state-of-the-art methods to create baselines and
highlight the affordances of this environment. Finally, we highlight challenges
and opportunities for future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1"&gt;Daniel McDuff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yale Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jiyoung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1"&gt;Vibhav Vineet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vemprala_S/0/1/0/all/0/1"&gt;Sai Vemprala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gyde_N/0/1/0/all/0/1"&gt;Nicholas Gyde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salman_H/0/1/0/all/0/1"&gt;Hadi Salman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shuang Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1"&gt;Kwanghoon Sohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kapoor_A/0/1/0/all/0/1"&gt;Ashish Kapoor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Binary Matrix Factorisation and Completion via Integer Programming. (arXiv:2106.13434v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.13434</id>
        <link href="http://arxiv.org/abs/2106.13434"/>
        <updated>2021-06-28T01:57:55.979Z</updated>
        <summary type="html"><![CDATA[Binary matrix factorisation is an essential tool for identifying discrete
patterns in binary data. In this paper we consider the rank-k binary matrix
factorisation problem (k-BMF) under Boolean arithmetic: we are given an n x m
binary matrix X with possibly missing entries and need to find two binary
matrices A and B of dimension n x k and k x m respectively, which minimise the
distance between X and the Boolean product of A and B in the squared Frobenius
distance. We present a compact and two exponential size integer programs (IPs)
for k-BMF and show that the compact IP has a weak LP relaxation, while the
exponential size LPs have a stronger equivalent LP relaxation. We introduce a
new objective function, which differs from the traditional squared Frobenius
objective in attributing a weight to zero entries of the input matrix that is
proportional to the number of times the zero is erroneously covered in a rank-k
factorisation. For one of the exponential size IPs we describe a computational
approach based on column generation. Experimental results on synthetic and real
word datasets suggest that our integer programming approach is competitive
against available methods for k-BMF and provides accurate low-error
factorisations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Kovacs_R/0/1/0/all/0/1"&gt;Reka A. Kovacs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gunluk_O/0/1/0/all/0/1"&gt;Oktay Gunluk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hauser_R/0/1/0/all/0/1"&gt;Raphael A. Hauser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What will it take to generate fairness-preserving explanations?. (arXiv:2106.13346v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13346</id>
        <link href="http://arxiv.org/abs/2106.13346"/>
        <updated>2021-06-28T01:57:55.961Z</updated>
        <summary type="html"><![CDATA[In situations where explanations of black-box models may be useful, the
fairness of the black-box is also often a relevant concern. However, the link
between the fairness of the black-box model and the behavior of explanations
for the black-box is unclear. We focus on explanations applied to tabular
datasets, suggesting that explanations do not necessarily preserve the fairness
properties of the black-box algorithm. In other words, explanation algorithms
can ignore or obscure critical relevant properties, creating incorrect or
misleading explanations. More broadly, we propose future research directions
for evaluating and generating explanations such that they are informative and
relevant from a fairness perspective.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1"&gt;Jessica Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1"&gt;Sohini Upadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1"&gt;Stephen H. Bach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1"&gt;Himabindu Lakkaraju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multivariate Medians for Image and Shape Analysis. (arXiv:1911.00143v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.00143</id>
        <link href="http://arxiv.org/abs/1911.00143"/>
        <updated>2021-06-28T01:57:55.954Z</updated>
        <summary type="html"><![CDATA[Having been studied since long by statisticians, multivariate median concepts
found their way into the image processing literature in the course of the last
decades, being used to construct robust and efficient denoising filters for
multivariate images such as colour images but also matrix-valued images. Based
on the similarities between image and geometric data as results of the sampling
of continuous physical quantities, it can be expected that the understanding of
multivariate median filters for images provides a starting point for the
development of shape processing techniques. This paper presents an overview of
multivariate median concepts relevant for image and shape processing. It
focusses on their mathematical principles and discusses important properties
especially in the context of image processing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Welk_M/0/1/0/all/0/1"&gt;Martin Welk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized One-Class Learning Using Pairs of Complementary Classifiers. (arXiv:2106.13272v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13272</id>
        <link href="http://arxiv.org/abs/2106.13272"/>
        <updated>2021-06-28T01:57:55.949Z</updated>
        <summary type="html"><![CDATA[One-class learning is the classic problem of fitting a model to the data for
which annotations are available only for a single class. In this paper, we
explore novel objectives for one-class learning, which we collectively refer to
as Generalized One-class Discriminative Subspaces (GODS). Our key idea is to
learn a pair of complementary classifiers to flexibly bound the one-class data
distribution, where the data belongs to the positive half-space of one of the
classifiers in the complementary pair and to the negative half-space of the
other. To avoid redundancy while allowing non-linearity in the classifier
decision surfaces, we propose to design each classifier as an orthonormal frame
and seek to learn these frames via jointly optimizing for two conflicting
objectives, namely: i) to minimize the distance between the two frames, and ii)
to maximize the margin between the frames and the data. The learned orthonormal
frames will thus characterize a piecewise linear decision surface that allows
for efficient inference, while our objectives seek to bound the data within a
minimal volume that maximizes the decision margin, thereby robustly capturing
the data distribution. We explore several variants of our formulation under
different constraints on the constituent classifiers, including kernelized
feature maps. We demonstrate the empirical benefits of our approach via
experiments on data from several applications in computer vision, such as
anomaly detection in video sequences, human poses, and human activities. We
also explore the generality and effectiveness of GODS for non-vision tasks via
experiments on several UCI datasets, demonstrating state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cherian_A/0/1/0/all/0/1"&gt;Anoop Cherian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jue Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models. (arXiv:2106.13353v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13353</id>
        <link href="http://arxiv.org/abs/2106.13353"/>
        <updated>2021-06-28T01:57:55.943Z</updated>
        <summary type="html"><![CDATA[Prompting language models (LMs) with training examples and task descriptions
has been seen as critical to recent successes in few-shot learning. In this
work, we show that finetuning LMs in the few-shot setting can considerably
reduce the need for prompt engineering. In fact, one can use null prompts,
prompts that contain neither task-specific templates nor training examples, and
achieve competitive accuracy to manually-tuned prompts across a wide range of
tasks. While finetuning LMs does introduce new parameters for each downstream
task, we show that this memory overhead can be substantially reduced:
finetuning only the bias terms can achieve comparable or better accuracy than
standard finetuning while only updating 0.1% of the parameters. All in all, we
recommend finetuning LMs for few-shot learning as it is more accurate, robust
to different prompts, and can be made nearly as efficient as using frozen LMs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Logan_R/0/1/0/all/0/1"&gt;Robert L. Logan IV&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balazevic_I/0/1/0/all/0/1"&gt;Ivana Bala&amp;#x17e;evi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1"&gt;Eric Wallace&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petroni_F/0/1/0/all/0/1"&gt;Fabio Petroni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sameer Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1"&gt;Sebastian Riedel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-based multi-parameter mapping. (arXiv:2102.01604v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01604</id>
        <link href="http://arxiv.org/abs/2102.01604"/>
        <updated>2021-06-28T01:57:55.936Z</updated>
        <summary type="html"><![CDATA[Quantitative MR imaging is increasingly favoured for its richer information
content and standardised measures. However, computing quantitative parameter
maps, such as those encoding longitudinal relaxation rate (R1), apparent
transverse relaxation rate (R2*) or magnetisation-transfer saturation (MTsat),
involves inverting a highly non-linear function. Many methods for deriving
parameter maps assume perfect measurements and do not consider how noise is
propagated through the estimation procedure, resulting in needlessly noisy
maps. Instead, we propose a probabilistic generative (forward) model of the
entire dataset, which is formulated and inverted to jointly recover (log)
parameter maps with a well-defined probabilistic interpretation (e.g., maximum
likelihood or maximum a posteriori). The second order optimisation we propose
for model fitting achieves rapid and stable convergence thanks to a novel
approximate Hessian. We demonstrate the utility of our flexible framework in
the context of recovering more accurate maps from data acquired using the
popular multi-parameter mapping protocol. We also show how to incorporate a
joint total variation prior to further decrease the noise in the maps, noting
that the probabilistic formulation allows the uncertainty on the recovered
parameter maps to be estimated. Our implementation uses a PyTorch backend and
benefits from GPU acceleration. It is available at
https://github.com/balbasty/nitorch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balbastre_Y/0/1/0/all/0/1"&gt;Yael Balbastre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brudfors_M/0/1/0/all/0/1"&gt;Mikael Brudfors&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azzarito_M/0/1/0/all/0/1"&gt;Michela Azzarito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lambert_C/0/1/0/all/0/1"&gt;Christian Lambert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Callaghan_M/0/1/0/all/0/1"&gt;Martina F. Callaghan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ashburner_J/0/1/0/all/0/1"&gt;John Ashburner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disease Progression Modeling Workbench 360. (arXiv:2106.13265v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13265</id>
        <link href="http://arxiv.org/abs/2106.13265"/>
        <updated>2021-06-28T01:57:55.919Z</updated>
        <summary type="html"><![CDATA[In this work we introduce Disease Progression Modeling workbench 360 (DPM360)
opensource clinical informatics framework for collaborative research and
delivery of healthcare AI. DPM360, when fully developed, will manage the entire
modeling life cycle, from data analysis (e.g., cohort identification) to
machine learning algorithm development and prototyping. DPM360 augments the
advantages of data model standardization and tooling (OMOP-CDM, Athena, ATLAS)
provided by the widely-adopted OHDSI initiative with a powerful machine
learning training framework, and a mechanism for rapid prototyping through
automatic deployment of models as containerized services to a cloud
environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suryanarayanan_P/0/1/0/all/0/1"&gt;Parthasarathy Suryanarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1"&gt;Prithwish Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madan_P/0/1/0/all/0/1"&gt;Piyush Madan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bore_K/0/1/0/all/0/1"&gt;Kibichii Bore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ogallo_W/0/1/0/all/0/1"&gt;William Ogallo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1"&gt;Rachita Chandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghalwash_M/0/1/0/all/0/1"&gt;Mohamed Ghalwash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buleje_I/0/1/0/all/0/1"&gt;Italo Buleje&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Remy_S/0/1/0/all/0/1"&gt;Sekou Remy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahatma_S/0/1/0/all/0/1"&gt;Shilpa Mahatma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meyer_P/0/1/0/all/0/1"&gt;Pablo Meyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jianying Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hate Speech Detection in Clubhouse. (arXiv:2106.13238v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13238</id>
        <link href="http://arxiv.org/abs/2106.13238"/>
        <updated>2021-06-28T01:57:55.907Z</updated>
        <summary type="html"><![CDATA[With high prevalence of offensive language against the minorities in social
media, counter hate speech generation is considered as an automatic way to
tackle this challenge. The counter hate speeches are supposed to appear as a
third voice to educate people and keep the social red lines bold without
limiting the freedom of speech principles. The counter hate speech generation
is based on the optimistic assumption that, any attempt to intervene the hate
speeches in social media can play a positive role in this context. Beyond that,
previous works ignored to investigate the sequence of comments before and after
counter speech. To the best of our knowledge, no attempt has been made to
measure the counter hate speech impact from statistical point of view. In this
paper, we take the first step in this direction by measuring the counter hate
speech impact on the next comments in terms of Google Perspective Scores.
Furthermore, our experiments show that, counter hate speech can cause negative
impacts, a phenomena which is called aggression in social media.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mansourifar_H/0/1/0/all/0/1"&gt;Hadi Mansourifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alsagheer_D/0/1/0/all/0/1"&gt;Dana Alsagheer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fathi_R/0/1/0/all/0/1"&gt;Reza Fathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1"&gt;Weidong Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_L/0/1/0/all/0/1"&gt;Lan Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yan Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometric learning of the conformational dynamics of molecules using dynamic graph neural networks. (arXiv:2106.13277v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13277</id>
        <link href="http://arxiv.org/abs/2106.13277"/>
        <updated>2021-06-28T01:57:55.901Z</updated>
        <summary type="html"><![CDATA[We apply a temporal edge prediction model for weighted dynamic graphs to
predict time-dependent changes in molecular structure. Each molecule is
represented as a complete graph in which each atom is a vertex and all vertex
pairs are connected by an edge weighted by the Euclidean distance between atom
pairs. We ingest a sequence of complete molecular graphs into a dynamic graph
neural network (GNN) to predict the graph at the next time step. Our dynamic
GNN predicts atom-to-atom distances with a mean absolute error of 0.017 \r{A},
which is considered ``chemically accurate'' for molecular simulations. We also
explored the transferability of a trained network to new molecular systems and
found that finetuning with less than 10% of the total trajectory provides a
mean absolute error of the same order of magnitude as that when training from
scratch on the full molecular trajectory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ashby_M/0/1/0/all/0/1"&gt;Michael Hunter Ashby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bilbrey_J/0/1/0/all/0/1"&gt;Jenna A. Bilbrey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Gradual Argumentation Frameworks using Genetic Algorithms. (arXiv:2106.13585v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13585</id>
        <link href="http://arxiv.org/abs/2106.13585"/>
        <updated>2021-06-28T01:57:55.884Z</updated>
        <summary type="html"><![CDATA[Gradual argumentation frameworks represent arguments and their relationships
in a weighted graph. Their graphical structure and intuitive semantics makes
them a potentially interesting tool for interpretable machine learning. It has
been noted recently that their mechanics are closely related to neural
networks, which allows learning their weights from data by standard deep
learning frameworks. As a first proof of concept, we propose a genetic
algorithm to simultaneously learn the structure of argumentative classification
models. To obtain a well interpretable model, the fitness function balances
sparseness and accuracy of the classifier. We discuss our algorithm and present
first experimental results on standard benchmarks from the UCI machine learning
repository. Our prototype learns argumentative classification models that are
comparable to decision trees in terms of learning performance and
interpretability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Spieler_J/0/1/0/all/0/1"&gt;Jonathan Spieler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potyka_N/0/1/0/all/0/1"&gt;Nico Potyka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Staab_S/0/1/0/all/0/1"&gt;Steffen Staab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerated Computation of a High Dimensional Kolmogorov-Smirnov Distance. (arXiv:2106.13706v1 [stat.CO])]]></title>
        <id>http://arxiv.org/abs/2106.13706</id>
        <link href="http://arxiv.org/abs/2106.13706"/>
        <updated>2021-06-28T01:57:55.839Z</updated>
        <summary type="html"><![CDATA[Statistical testing is widespread and critical for a variety of scientific
disciplines. The advent of machine learning and the increase of computing power
has increased the interest in the analysis and statistical testing of
multidimensional data. We extend the powerful Kolmogorov-Smirnov two sample
test to a high dimensional form in a similar manner to Fasano (Fasano, 1987).
We call our result the d-dimensional Kolmogorov-Smirnov test (ddKS) and provide
three novel contributions therewith: we develop an analytical equation for the
significance of a given ddKS score, we provide an algorithm for computation of
ddKS on modern computing hardware that is of constant time complexity for small
sample sizes and dimensions, and we provide two approximate calculations of
ddKS: one that reduces the time complexity to linear at larger sample sizes,
and another that reduces the time complexity to linear with increasing
dimension. We perform power analysis of ddKS and its approximations on a corpus
of datasets and compare to other common high dimensional two sample tests and
distances: Hotelling's T^2 test and Kullback-Leibler divergence. Our ddKS test
performs well for all datasets, dimensions, and sizes tested, whereas the other
tests and distances fail to reject the null hypothesis on at least one dataset.
We therefore conclude that ddKS is a powerful multidimensional two sample test
for general use, and can be calculated in a fast and efficient manner using our
parallel or approximate methods. Open source implementations of all methods
described in this work are located at https://github.com/pnnl/ddks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hagen_A/0/1/0/all/0/1"&gt;Alex Hagen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jackson_S/0/1/0/all/0/1"&gt;Shane Jackson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kahn_J/0/1/0/all/0/1"&gt;James Kahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Strube_J/0/1/0/all/0/1"&gt;Jan Strube&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Haide_I/0/1/0/all/0/1"&gt;Isabel Haide&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pazdernik_K/0/1/0/all/0/1"&gt;Karl Pazdernik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hainje_C/0/1/0/all/0/1"&gt;Connor Hainje&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Limitations of machine learning for building energy prediction: ASHRAE Great Energy Predictor III Kaggle competition error analysis. (arXiv:2106.13475v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13475</id>
        <link href="http://arxiv.org/abs/2106.13475"/>
        <updated>2021-06-28T01:57:55.796Z</updated>
        <summary type="html"><![CDATA[Machine learning for building energy prediction has exploded in popularity in
recent years, yet understanding its limitations and potential for improvement
are lacking. The ASHRAE Great Energy Predictor III (GEPIII) Kaggle competition
was the largest building energy meter machine learning competition ever held
with 4,370 participants who submitted 39,403 predictions. The test data set
included two years of hourly electricity, hot water, chilled water, and steam
readings from 2,380 meters in 1,448 buildings at 16 locations. This paper
analyzes the various sources and types of residual model error from an
aggregation of the competition's top 50 solutions. This analysis reveals the
limitations for machine learning using the standard model inputs of historical
meter, weather, and basic building metadata. The types of error are classified
according to the amount of time errors occur in each instance, abrupt versus
gradual behavior, the magnitude of error, and whether the error existed on
single buildings or several buildings at once from a single location. The
results show machine learning models have errors within a range of
acceptability on 79.1% of the test data. Lower magnitude model errors occur in
16.1% of the test data. These discrepancies can likely be addressed through
additional training data sources or innovations in machine learning. Higher
magnitude errors occur in 4.8% of the test data and are unlikely to be
accurately predicted regardless of innovation. There is a diversity of error
behavior depending on the energy meter type (electricity prediction models have
unacceptable error in under 10% of test data, while hot water is over 60%) and
building use type (public service less than 14%, while technology/science is
just over 46%).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miller_C/0/1/0/all/0/1"&gt;Clayton Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Picchetti_B/0/1/0/all/0/1"&gt;Bianca Picchetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Chun Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pantelic_J/0/1/0/all/0/1"&gt;Jovan Pantelic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Inference in High-Dimensional Time-Serieswith the Orthogonal Stochastic Linear Mixing Model. (arXiv:2106.13379v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13379</id>
        <link href="http://arxiv.org/abs/2106.13379"/>
        <updated>2021-06-28T01:57:55.788Z</updated>
        <summary type="html"><![CDATA[Many modern time-series datasets contain large numbers of output response
variables sampled for prolonged periods of time. For example, in neuroscience,
the activities of 100s-1000's of neurons are recorded during behaviors and in
response to sensory stimuli. Multi-output Gaussian process models leverage the
nonparametric nature of Gaussian processes to capture structure across multiple
outputs. However, this class of models typically assumes that the correlations
between the output response variables are invariant in the input space.
Stochastic linear mixing models (SLMM) assume the mixture coefficients depend
on input, making them more flexible and effective to capture complex output
dependence. However, currently, the inference for SLMMs is intractable for
large datasets, making them inapplicable to several modern time-series
problems. In this paper, we propose a new regression framework, the orthogonal
stochastic linear mixing model (OSLMM) that introduces an orthogonal constraint
amongst the mixing coefficients. This constraint reduces the computational
burden of inference while retaining the capability to handle complex output
dependence. We provide Markov chain Monte Carlo inference procedures for both
SLMM and OSLMM and demonstrate superior model scalability and reduced
prediction error of OSLMM compared with state-of-the-art methods on several
real-world applications. In neurophysiology recordings, we use the inferred
latent functions for compact visualization of population responses to auditory
stimuli, and demonstrate superior results compared to a competing method
(GPFA). Together, these results demonstrate that OSLMM will be useful for the
analysis of diverse, large-scale time-series datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_R/0/1/0/all/0/1"&gt;Rui Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouchard_K/0/1/0/all/0/1"&gt;Kristofer Bouchard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multifidelity Modeling for Physics-Informed Neural Networks (PINNs). (arXiv:2106.13361v1 [physics.comp-ph])]]></title>
        <id>http://arxiv.org/abs/2106.13361</id>
        <link href="http://arxiv.org/abs/2106.13361"/>
        <updated>2021-06-28T01:57:55.781Z</updated>
        <summary type="html"><![CDATA[Multifidelity simulation methodologies are often used in an attempt to
judiciously combine low-fidelity and high-fidelity simulation results in an
accuracy-increasing, cost-saving way. Candidates for this approach are
simulation methodologies for which there are fidelity differences connected
with significant computational cost differences. Physics-informed Neural
Networks (PINNs) are candidates for these types of approaches due to the
significant difference in training times required when different fidelities
(expressed in terms of architecture width and depth as well as optimization
criteria) are employed. In this paper, we propose a particular multifidelity
approach applied to PINNs that exploits low-rank structure. We demonstrate that
width, depth, and optimization criteria can be used as parameters related to
model fidelity, and show numerical justification of cost differences in
training due to fidelity parameter choices. We test our multifidelity scheme on
various canonical forward PDE models that have been presented in the emerging
PINNs literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Penwarden_M/0/1/0/all/0/1"&gt;Michael Penwarden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zhe_S/0/1/0/all/0/1"&gt;Shandian Zhe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Narayan_A/0/1/0/all/0/1"&gt;Akil Narayan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kirby_R/0/1/0/all/0/1"&gt;Robert M. Kirby&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Medical Image Analysis on Left Atrial LGE MRI for Atrial Fibrillation Studies: A Review. (arXiv:2106.09862v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09862</id>
        <link href="http://arxiv.org/abs/2106.09862"/>
        <updated>2021-06-28T01:57:55.764Z</updated>
        <summary type="html"><![CDATA[Late gadolinium enhancement magnetic resonance imaging (LGE MRI) is commonly
used to visualize and quantify left atrial (LA) scars. The position and extent
of scars provide important information of the pathophysiology and progression
of atrial fibrillation (AF). Hence, LA scar segmentation and quantification
from LGE MRI can be useful in computer-assisted diagnosis and treatment
stratification of AF patients. Since manual delineation can be time-consuming
and subject to intra- and inter-expert variability, automating this computing
is highly desired, which nevertheless is still challenging and
under-researched.

This paper aims to provide a systematic review on computing methods for LA
cavity, wall, scar and ablation gap segmentation and quantification from LGE
MRI, and the related literature for AF studies. Specifically, we first
summarize AF-related imaging techniques, particularly LGE MRI. Then, we review
the methodologies of the four computing tasks in detail, and summarize the
validation strategies applied in each task. Finally, the possible future
developments are outlined, with a brief survey on the potential clinical
applications of the aforementioned methods. The review shows that the research
into this topic is still in early stages. Although several methods have been
proposed, especially for LA segmentation, there is still large scope for
further algorithmic developments due to performance issues related to the high
variability of enhancement appearance and differences in image acquisition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zimmer_V/0/1/0/all/0/1"&gt;Veronika A. Zimmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schnabel_J/0/1/0/all/0/1"&gt;Julia A. Schnabel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1"&gt;Xiahai Zhuang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Noisy Client Learning. (arXiv:2106.13239v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13239</id>
        <link href="http://arxiv.org/abs/2106.13239"/>
        <updated>2021-06-28T01:57:55.744Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) collaboratively aggregates a shared global model
depending on multiple local clients, while keeping the training data
decentralized in order to preserve data privacy. However, standard FL methods
ignore the noisy client issue, which may harm the overall performance of the
aggregated model. In this paper, we first analyze the noisy client statement,
and then model noisy clients with different noise distributions (e.g.,
Bernoulli and truncated Gaussian distributions). To learn with noisy clients,
we propose a simple yet effective FL framework, named Federated Noisy Client
Learning (Fed-NCL), which is a plug-and-play algorithm and contains two main
components: a data quality measurement (DQM) to dynamically quantify the data
quality of each participating client, and a noise robust aggregation (NRA) to
adaptively aggregate the local models of each client by jointly considering the
amount of local training data and the data quality of each client. Our Fed-NCL
can be easily applied in any standard FL workflow to handle the noisy client
issue. Experimental results on various datasets demonstrate that our algorithm
boosts the performances of different state-of-the-art systems with noisy
clients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Li Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1"&gt;Huazhu Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Cheng-Zhong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics perception in sloshing scenes with guaranteed thermodynamic consistency. (arXiv:2106.13301v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13301</id>
        <link href="http://arxiv.org/abs/2106.13301"/>
        <updated>2021-06-28T01:57:55.721Z</updated>
        <summary type="html"><![CDATA[Physics perception very often faces the problem that only limited data or
partial measurements on the scene are available. In this work, we propose a
strategy to learn the full state of sloshing liquids from measurements of the
free surface. Our approach is based on recurrent neural networks (RNN) that
project the limited information available to a reduced-order manifold so as to
not only reconstruct the unknown information, but also to be capable of
performing fluid reasoning about future scenarios in real time. To obtain
physically consistent predictions, we train deep neural networks on the
reduced-order manifold that, through the employ of inductive biases, ensure the
fulfillment of the principles of thermodynamics. RNNs learn from history the
required hidden information to correlate the limited information with the
latent space where the simulation occurs. Finally, a decoder returns data back
to the high-dimensional manifold, so as to provide the user with insightful
information in the form of augmented reality. This algorithm is connected to a
computer vision system to test the performance of the proposed methodology with
real information, resulting in a system capable of understanding and predicting
future states of the observed fluid in real-time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moya_B/0/1/0/all/0/1"&gt;Beatriz Moya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badias_A/0/1/0/all/0/1"&gt;Alberto Badias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_D/0/1/0/all/0/1"&gt;David Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chinesta_F/0/1/0/all/0/1"&gt;Francisco Chinesta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cueto_E/0/1/0/all/0/1"&gt;Elias Cueto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PVTv2: Improved Baselines with Pyramid Vision Transformer. (arXiv:2106.13797v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13797</id>
        <link href="http://arxiv.org/abs/2106.13797"/>
        <updated>2021-06-28T01:57:55.715Z</updated>
        <summary type="html"><![CDATA[Transformer in computer vision has recently shown encouraging progress. In
this work, we improve the original Pyramid Vision Transformer (PVTv1) by adding
three improvement designs, which include (1) locally continuous features with
convolutions, (2) position encodings with zero paddings, and (3) linear
complexity attention layers with average pooling. With these simple
modifications, our PVTv2 significantly improves PVTv1 on classification,
detection, and segmentation. Moreover, PVTv2 achieves much better performance
than recent works, including Swin Transformer, under ImageNet-1K pre-training.
We hope this work will make state-of-the-art vision Transformer research more
accessible. Code is available at https://github.com/whai362/PVT .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenhai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1"&gt;Enze Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1"&gt;Deng-Ping Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1"&gt;Kaitao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1"&gt;Ding Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1"&gt;Tong Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1"&gt;Ping Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks. (arXiv:2103.03231v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03231</id>
        <link href="http://arxiv.org/abs/2103.03231"/>
        <updated>2021-06-28T01:57:55.677Z</updated>
        <summary type="html"><![CDATA[The recent research explosion around implicit neural representations, such as
NeRF, shows that there is immense potential for implicitly storing high-quality
scene and lighting information in compact neural networks. However, one major
limitation preventing the use of NeRF in real-time rendering applications is
the prohibitive computational cost of excessive network evaluations along each
view ray, requiring dozens of petaFLOPS. In this work, we bring compact neural
representations closer to practical rendering of synthetic content in real-time
applications, such as games and virtual reality. We show that the number of
samples required for each view ray can be significantly reduced when samples
are placed around surfaces in the scene without compromising image quality. To
this end, we propose a depth oracle network that predicts ray sample locations
for each view ray with a single network evaluation. We show that using a
classification network around logarithmically discretized and spherically
warped depth values is essential to encode surface locations rather than
directly estimating depth. The combination of these techniques leads to DONeRF,
our compact dual network design with a depth oracle network as its first step
and a locally sampled shading network for ray accumulation. With DONeRF, we
reduce the inference costs by up to 48x compared to NeRF when conditioning on
available ground truth depth information. Compared to concurrent acceleration
methods for raymarching-based neural representations, DONeRF does not require
additional memory for explicit caching or acceleration structures, and can
render interactively (20 frames per second) on a single GPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Neff_T/0/1/0/all/0/1"&gt;Thomas Neff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stadlbauer_P/0/1/0/all/0/1"&gt;Pascal Stadlbauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parger_M/0/1/0/all/0/1"&gt;Mathias Parger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurz_A/0/1/0/all/0/1"&gt;Andreas Kurz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mueller_J/0/1/0/all/0/1"&gt;Joerg H. Mueller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaitanya_C/0/1/0/all/0/1"&gt;Chakravarty R. Alla Chaitanya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaplanyan_A/0/1/0/all/0/1"&gt;Anton Kaplanyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steinberger_M/0/1/0/all/0/1"&gt;Markus Steinberger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[byteSteady: Fast Classification Using Byte-Level n-Gram Embeddings. (arXiv:2106.13302v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13302</id>
        <link href="http://arxiv.org/abs/2106.13302"/>
        <updated>2021-06-28T01:57:55.663Z</updated>
        <summary type="html"><![CDATA[This article introduces byteSteady -- a fast model for classification using
byte-level n-gram embeddings. byteSteady assumes that each input comes as a
sequence of bytes. A representation vector is produced using the averaged
embedding vectors of byte-level n-grams, with a pre-defined set of n. The
hashing trick is used to reduce the number of embedding vectors. This input
representation vector is then fed into a linear classifier. A straightforward
application of byteSteady is text classification. We also apply byteSteady to
one type of non-language data -- DNA sequences for gene classification. For
both problems we achieved competitive classification results against strong
baselines, suggesting that byteSteady can be applied to both language and
non-language data. Furthermore, we find that simple compression using Huffman
coding does not significantly impact the results, which offers an
accuracy-speed trade-off previously unexplored in machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drouin_A/0/1/0/all/0/1"&gt;Alexandre Drouin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Raymond Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Post Selections Using Test Sets (PSUTS) and How Developmental Networks Avoid Them. (arXiv:2106.13233v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13233</id>
        <link href="http://arxiv.org/abs/2106.13233"/>
        <updated>2021-06-28T01:57:55.645Z</updated>
        <summary type="html"><![CDATA[This paper raises a rarely reported practice in Artificial Intelligence (AI)
called Post Selection Using Test Sets (PSUTS). Consequently, the popular
error-backprop methodology in deep learning lacks an acceptable generalization
power. All AI methods fall into two broad schools, connectionist and symbolic.
The PSUTS fall into two kinds, machine PSUTS and human PSUTS. The connectionist
school received criticisms for its "scruffiness" due to a huge number of
network parameters and now the worse machine PSUTS; but the seemingly "clean"
symbolic school seems more brittle because of a weaker generalization power
using human PSUTS. This paper formally defines what PSUTS is, analyzes why
error-backprop methods with random initial weights suffer from severe local
minima, why PSUTS violates well-established research ethics, and how every
paper that used PSUTS should have at least transparently reported PSUTS. For
improved transparency in future publications, this paper proposes a new
standard for performance evaluation of AI, called developmental errors for all
networks trained, along with Three Learning Conditions: (1) an incremental
learning architecture, (2) a training experience and (3) a limited amount of
computational resources. Developmental Networks avoid PSUTS and are not
"scruffy" because they drive Emergent Turing Machines and are optimal in the
sense of maximum-likelihood across lifetime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weng_J/0/1/0/all/0/1"&gt;Juyang Weng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single and Union Non-parallel Support Vector Machine Frameworks. (arXiv:1910.09734v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.09734</id>
        <link href="http://arxiv.org/abs/1910.09734"/>
        <updated>2021-06-28T01:57:55.627Z</updated>
        <summary type="html"><![CDATA[Considering the classification problem, we summarize the nonparallel support
vector machines with the nonparallel hyperplanes to two types of frameworks.
The first type constructs the hyperplanes separately. It solves a series of
small optimization problems to obtain a series of hyperplanes, but is hard to
measure the loss of each sample. The other type constructs all the hyperplanes
simultaneously, and it solves one big optimization problem with the ascertained
loss of each sample. We give the characteristics of each framework and compare
them carefully. In addition, based on the second framework, we construct a
max-min distance-based nonparallel support vector machine for multiclass
classification problem, called NSVM. It constructs hyperplanes with large
distance margin by solving an optimization problem. Experimental results on
benchmark data sets show the advantages of our NSVM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chun-Na Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yuan-Hai Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huajun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yu-Ting Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Ling-Wei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiu_N/0/1/0/all/0/1"&gt;Naihua Xiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_N/0/1/0/all/0/1"&gt;Nai-Yang Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Covariance-Aware Private Mean Estimation Without Private Covariance Estimation. (arXiv:2106.13329v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13329</id>
        <link href="http://arxiv.org/abs/2106.13329"/>
        <updated>2021-06-28T01:57:55.585Z</updated>
        <summary type="html"><![CDATA[We present two sample-efficient differentially private mean estimators for
$d$-dimensional (sub)Gaussian distributions with unknown covariance.
Informally, given $n \gtrsim d/\alpha^2$ samples from such a distribution with
mean $\mu$ and covariance $\Sigma$, our estimators output $\tilde\mu$ such that
$\| \tilde\mu - \mu \|_{\Sigma} \leq \alpha$, where $\| \cdot \|_{\Sigma}$ is
the Mahalanobis distance. All previous estimators with the same guarantee
either require strong a priori bounds on the covariance matrix or require
$\Omega(d^{3/2})$ samples.

Each of our estimators is based on a simple, general approach to designing
differentially private mechanisms, but with novel technical steps to make the
estimator private and sample-efficient. Our first estimator samples a point
with approximately maximum Tukey depth using the exponential mechanism, but
restricted to the set of points of large Tukey depth. Proving that this
mechanism is private requires a novel analysis. Our second estimator perturbs
the empirical mean of the data set with noise calibrated to the empirical
covariance, without releasing the covariance itself. Its sample complexity
guarantees hold more generally for subgaussian distributions, albeit with a
slightly worse dependence on the privacy parameter. For both estimators,
careful preprocessing of the data is required to satisfy differential privacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brown_G/0/1/0/all/0/1"&gt;Gavin Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaboardi_M/0/1/0/all/0/1"&gt;Marco Gaboardi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_A/0/1/0/all/0/1"&gt;Adam Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullman_J/0/1/0/all/0/1"&gt;Jonathan Ullman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zakynthinou_L/0/1/0/all/0/1"&gt;Lydia Zakynthinou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Over-and-Under Complete Convolutional RNN for MRI Reconstruction. (arXiv:2106.08886v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08886</id>
        <link href="http://arxiv.org/abs/2106.08886"/>
        <updated>2021-06-28T01:57:55.574Z</updated>
        <summary type="html"><![CDATA[Reconstructing magnetic resonance (MR) images from undersampled data is a
challenging problem due to various artifacts introduced by the under-sampling
operation. Recent deep learning-based methods for MR image reconstruction
usually leverage a generic auto-encoder architecture which captures low-level
features at the initial layers and high-level features at the deeper layers.
Such networks focus much on global features which may not be optimal to
reconstruct the fully-sampled image. In this paper, we propose an
Over-and-Under Complete Convolutional Recurrent Neural Network (OUCR), which
consists of an overcomplete and an undercomplete Convolutional Recurrent Neural
Network(CRNN). The overcomplete branch gives special attention in learning
local structures by restraining the receptive field of the network. Combining
it with the undercomplete branch leads to a network which focuses more on
low-level features without losing out on the global structures. Extensive
experiments on two datasets demonstrate that the proposed method achieves
significant improvements over the compressed sensing and popular deep
learning-based methods with less number of trainable parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Guo_P/0/1/0/all/0/1"&gt;Pengfei Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Valanarasu_J/0/1/0/all/0/1"&gt;Jeya Maria Jose Valanarasu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_P/0/1/0/all/0/1"&gt;Puyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jinyuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Shanshan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Patel_V/0/1/0/all/0/1"&gt;Vishal M. Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prior Image-Constrained Reconstruction using Style-Based Generative Models. (arXiv:2102.12525v2 [eess.IV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2102.12525</id>
        <link href="http://arxiv.org/abs/2102.12525"/>
        <updated>2021-06-28T01:57:55.566Z</updated>
        <summary type="html"><![CDATA[Obtaining a useful estimate of an object from highly incomplete imaging
measurements remains a holy grail of imaging science. Deep learning methods
have shown promise in learning object priors or constraints to improve the
conditioning of an ill-posed imaging inverse problem. In this study, a
framework for estimating an object of interest that is semantically related to
a known prior image, is proposed. An optimization problem is formulated in the
disentangled latent space of a style-based generative model, and semantically
meaningful constraints are imposed using the disentangled latent representation
of the prior image. Stable recovery from incomplete measurements with the help
of a prior image is theoretically analyzed. Numerical experiments demonstrating
the superior performance of our approach as compared to related methods are
presented.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kelkar_V/0/1/0/all/0/1"&gt;Varun A. Kelkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Anastasio_M/0/1/0/all/0/1"&gt;Mark A. Anastasio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multitask 3D CBCT-to-CT Translation and Organs-at-Risk Segmentation Using Physics-Based Data Augmentation. (arXiv:2103.05690v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05690</id>
        <link href="http://arxiv.org/abs/2103.05690"/>
        <updated>2021-06-28T01:57:55.546Z</updated>
        <summary type="html"><![CDATA[In current clinical practice, noisy and artifact-ridden weekly cone-beam
computed tomography (CBCT) images are only used for patient setup during
radiotherapy. Treatment planning is done once at the beginning of the treatment
using high-quality planning CT (pCT) images and manual contours for
organs-at-risk (OARs) structures. If the quality of the weekly CBCT images can
be improved while simultaneously segmenting OAR structures, this can provide
critical information for adapting radiotherapy mid-treatment as well as for
deriving biomarkers for treatment response. Using a novel physics-based data
augmentation strategy, we synthesize a large dataset of perfectly/inherently
registered planning CT and synthetic-CBCT pairs for locally advanced lung
cancer patient cohort, which are then used in a multitask 3D deep learning
framework to simultaneously segment and translate real weekly CBCT images to
high-quality planning CT-like images. We compared the synthetic CT and OAR
segmentations generated by the model to real planning CT and manual OAR
segmentations and showed promising results. The real week 1 (baseline) CBCT
images which had an average MAE of 162.77 HU compared to pCT images are
translated to synthetic CT images that exhibit a drastically improved average
MAE of 29.31 HU and average structural similarity of 92% with the pCT images.
The average DICE scores of the 3D organs-at-risk segmentations are: lungs 0.96,
heart 0.88, spinal cord 0.83 and esophagus 0.66. This approach could allow
clinicians to adjust treatment plans using only the routine low-quality CBCT
images, potentially improving patient outcomes. Our code, data, and pre-trained
models will be made available via our physics-based data augmentation library,
Physics-ArX, at https://github.com/nadeemlab/Physics-ArX.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dahiya_N/0/1/0/all/0/1"&gt;Navdeep Dahiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1"&gt;Sadegh R Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pengpeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Si-Yuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yezzi_A/0/1/0/all/0/1"&gt;Anthony Yezzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nadeem_S/0/1/0/all/0/1"&gt;Saad Nadeem&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Projection-wise Disentangling for Fair and Interpretable Representation Learning: Application to 3D Facial Shape Analysis. (arXiv:2106.13734v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13734</id>
        <link href="http://arxiv.org/abs/2106.13734"/>
        <updated>2021-06-28T01:57:55.532Z</updated>
        <summary type="html"><![CDATA[Confounding bias is a crucial problem when applying machine learning to
practice, especially in clinical practice. We consider the problem of learning
representations independent to multiple biases. In literature, this is mostly
solved by purging the bias information from learned representations. We however
expect this strategy to harm the diversity of information in the
representation, and thus limiting its prospective usage (e.g., interpretation).
Therefore, we propose to mitigate the bias while keeping almost all information
in the latent representations, which enables us to observe and interpret them
as well. To achieve this, we project latent features onto a learned vector
direction, and enforce the independence between biases and projected features
rather than all learned features. To interpret the mapping between projected
features and input data, we propose projection-wise disentangling: a sampling
and reconstruction along the learned vector direction. The proposed method was
evaluated on the analysis of 3D facial shape and patient characteristics
(N=5011). Experiments showed that this conceptually simple method achieved
state-of-the-art fair prediction performance and interpretability, showing its
great potential for clinical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xianjing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bron_E/0/1/0/all/0/1"&gt;Esther Bron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niessen_W/0/1/0/all/0/1"&gt;Wiro Niessen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolvius_E/0/1/0/all/0/1"&gt;Eppo Wolvius&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roshchupkin_G/0/1/0/all/0/1"&gt;Gennady Roshchupkin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interactive Multi-level Stroke Control for Neural Style Transfer. (arXiv:2106.13787v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13787</id>
        <link href="http://arxiv.org/abs/2106.13787"/>
        <updated>2021-06-28T01:57:55.447Z</updated>
        <summary type="html"><![CDATA[We present StyleTune, a mobile app for interactive multi-level control of
neural style transfers that facilitates creative adjustments of style elements
and enables high output fidelity. In contrast to current mobile neural style
transfer apps, StyleTune supports users to adjust both the size and orientation
of style elements, such as brushstrokes and texture patches, on a global as
well as local level. To this end, we propose a novel stroke-adaptive
feed-forward style transfer network, that enables control over stroke size and
intensity and allows a larger range of edits than current approaches. For
additional level-of-control, we propose a network agnostic method for
stroke-orientation adjustment by utilizing the rotation-variance of CNNs. To
achieve high output fidelity, we further add a patch-based style transfer
method that enables users to obtain output resolutions of more than 20
Megapixel. Our approach empowers users to create many novel results that are
not possible with current mobile neural style transfer apps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reimann_M/0/1/0/all/0/1"&gt;Max Reimann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buchheim_B/0/1/0/all/0/1"&gt;Benito Buchheim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Semmo_A/0/1/0/all/0/1"&gt;Amir Semmo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dollner_J/0/1/0/all/0/1"&gt;J&amp;#xfc;rgen D&amp;#xf6;llner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trapp_M/0/1/0/all/0/1"&gt;Matthias Trapp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MultiFace: A Generic Training Mechanism for Boosting Face Recognition Performance. (arXiv:2101.09899v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09899</id>
        <link href="http://arxiv.org/abs/2101.09899"/>
        <updated>2021-06-28T01:57:55.435Z</updated>
        <summary type="html"><![CDATA[Deep Convolutional Neural Networks (DCNNs) and their variants have been
widely used in large scale face recognition(FR) recently. Existing methods have
achieved good performance on many FR benchmarks. However, most of them suffer
from two major problems. First, these methods converge quite slowly since they
optimize the loss functions in a high-dimensional and sparse Gaussian Sphere.
Second, the high dimensionality of features, despite the powerful descriptive
ability, brings difficulty to the optimization, which may lead to a sub-optimal
local optimum. To address these problems, we propose a simple yet efficient
training mechanism called MultiFace, where we approximate the original
high-dimensional features by the ensemble of low-dimensional features. The
proposed mechanism is also generic and can be easily applied to many advanced
FR models. Moreover, it brings the benefits of good interpretability to FR
models via the clustering effect. In detail, the ensemble of these
low-dimensional features can capture complementary yet discriminative
information, which can increase the intra-class compactness and inter-class
separability. Experimental results show that the proposed mechanism can
accelerate 2-3 times with the softmax loss and 1.2-1.5 times with Arcface or
Cosface, while achieving state-of-the-art performances in several benchmark
datasets. Especially, the significant improvements on large-scale
datasets(e.g., IJB and MageFace) demonstrate the flexibility of our new
training mechanism.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1"&gt;Tszhang Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_K/0/1/0/all/0/1"&gt;Kun Bai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic annotation for computational pathology: Multidisciplinary experience and best practice recommendations. (arXiv:2106.13689v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.13689</id>
        <link href="http://arxiv.org/abs/2106.13689"/>
        <updated>2021-06-28T01:57:55.427Z</updated>
        <summary type="html"><![CDATA[Recent advances in whole slide imaging (WSI) technology have led to the
development of a myriad of computer vision and artificial intelligence (AI)
based diagnostic, prognostic, and predictive algorithms. Computational
Pathology (CPath) offers an integrated solution to utilize information embedded
in pathology WSIs beyond what we obtain through visual assessment. For
automated analysis of WSIs and validation of machine learning (ML) models,
annotations at the slide, tissue and cellular levels are required. The
annotation of important visual constructs in pathology images is an important
component of CPath projects. Improper annotations can result in algorithms
which are hard to interpret and can potentially produce inaccurate and
inconsistent results. Despite the crucial role of annotations in CPath
projects, there are no well-defined guidelines or best practices on how
annotations should be carried out. In this paper, we address this shortcoming
by presenting the experience and best practices acquired during the execution
of a large-scale annotation exercise involving a multidisciplinary team of
pathologists, ML experts and researchers as part of the Pathology image data
Lake for Analytics, Knowledge and Education (PathLAKE) consortium. We present a
real-world case study along with examples of different types of annotations,
diagnostic algorithm, annotation data dictionary and annotation constructs. The
analyses reported in this work highlight best practice recommendations that can
be used as annotation guidelines over the lifecycle of a CPath project.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wahab_N/0/1/0/all/0/1"&gt;Noorul Wahab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Miligy_I/0/1/0/all/0/1"&gt;Islam M Miligy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dodd_K/0/1/0/all/0/1"&gt;Katherine Dodd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sahota_H/0/1/0/all/0/1"&gt;Harvir Sahota&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Toss_M/0/1/0/all/0/1"&gt;Michael Toss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wenqi Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jahanifar_M/0/1/0/all/0/1"&gt;Mostafa Jahanifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bilal_M/0/1/0/all/0/1"&gt;Mohsin Bilal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Graham_S/0/1/0/all/0/1"&gt;Simon Graham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Park_Y/0/1/0/all/0/1"&gt;Young Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hadjigeorghiou_G/0/1/0/all/0/1"&gt;Giorgos Hadjigeorghiou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bhalerao_A/0/1/0/all/0/1"&gt;Abhir Bhalerao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lashen_A/0/1/0/all/0/1"&gt;Ayat Lashen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ibrahim_A/0/1/0/all/0/1"&gt;Asmaa Ibrahim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Katayama_A/0/1/0/all/0/1"&gt;Ayaka Katayama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ebili_H/0/1/0/all/0/1"&gt;Henry O Ebili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Parkin_M/0/1/0/all/0/1"&gt;Matthew Parkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sorell_T/0/1/0/all/0/1"&gt;Tom Sorell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Raza_S/0/1/0/all/0/1"&gt;Shan E Ahmed Raza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hero_E/0/1/0/all/0/1"&gt;Emily Hero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Eldaly_H/0/1/0/all/0/1"&gt;Hesham Eldaly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsang_Y/0/1/0/all/0/1"&gt;Yee Wah Tsang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gopalakrishnan_K/0/1/0/all/0/1"&gt;Kishore Gopalakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Snead_D/0/1/0/all/0/1"&gt;David Snead&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rakha_E/0/1/0/all/0/1"&gt;Emad Rakha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rajpoot_N/0/1/0/all/0/1"&gt;Nasir Rajpoot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Minhas_F/0/1/0/all/0/1"&gt;Fayyaz Minhas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Document Image Classification Using Region-Based Graph Neural Network. (arXiv:2106.13802v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13802</id>
        <link href="http://arxiv.org/abs/2106.13802"/>
        <updated>2021-06-28T01:57:55.419Z</updated>
        <summary type="html"><![CDATA[Document image classification remains a popular research area because it can
be commercialized in many enterprise applications across different industries.
Recent advancements in large pre-trained computer vision and language models
and graph neural networks has lent document image classification many tools.
However using large pre-trained models usually requires substantial computing
resources which could defeat the cost-saving advantages of automatic document
image classification. In the paper we propose an efficient document image
classification framework that uses graph convolution neural networks and
incorporates textual, visual and layout information of the document. We have
rigorously benchmarked our proposed algorithm against several state-of-art
vision and language models on both publicly available dataset and a real-life
insurance document classification dataset. Empirical results on both publicly
available and real-world data show that our methods achieve near SOTA
performance yet require much less computing resources and time for model
training and inference. This results in solutions than offer better cost
advantages, especially in scalable deployment for enterprise applications. The
results showed that our algorithm can achieve classification performance quite
close to SOTA. We also provide comprehensive comparisons of computing
resources, model sizes, train and inference time between our proposed methods
and baselines. In addition we delineate the cost per image using our method and
other baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mandivarapu_J/0/1/0/all/0/1"&gt;Jaya Krishna Mandivarapu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bunch_E/0/1/0/all/0/1"&gt;Eric Bunch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_Q/0/1/0/all/0/1"&gt;Qian You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fung_G/0/1/0/all/0/1"&gt;Glenn Fung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Re-parameterizing VAEs for stability. (arXiv:2106.13739v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13739</id>
        <link href="http://arxiv.org/abs/2106.13739"/>
        <updated>2021-06-28T01:57:55.399Z</updated>
        <summary type="html"><![CDATA[We propose a theoretical approach towards the training numerical stability of
Variational AutoEncoders (VAE). Our work is motivated by recent studies
empowering VAEs to reach state of the art generative results on complex image
datasets. These very deep VAE architectures, as well as VAEs using more complex
output distributions, highlight a tendency to haphazardly produce high training
gradients as well as NaN losses. The empirical fixes proposed to train them
despite their limitations are neither fully theoretically grounded nor
generally sufficient in practice. Building on this, we localize the source of
the problem at the interface between the model's neural networks and their
output probabilistic distributions. We explain a common source of instability
stemming from an incautious formulation of the encoded Normal distribution's
variance, and apply the same approach on other, less obvious sources. We show
that by implementing small changes to the way we parameterize the Normal
distributions on which they rely, VAEs can securely be trained.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dehaene_D/0/1/0/all/0/1"&gt;David Dehaene&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brossard_R/0/1/0/all/0/1"&gt;R&amp;#xe9;my Brossard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis. (arXiv:2103.11078v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11078</id>
        <link href="http://arxiv.org/abs/2103.11078"/>
        <updated>2021-06-28T01:57:55.390Z</updated>
        <summary type="html"><![CDATA[Generating high-fidelity talking head video by fitting with the input audio
sequence is a challenging problem that receives considerable attentions
recently. In this paper, we address this problem with the aid of neural scene
representation networks. Our method is completely different from existing
methods that rely on intermediate representations like 2D landmarks or 3D face
models to bridge the gap between audio input and video output. Specifically,
the feature of input audio signal is directly fed into a conditional implicit
function to generate a dynamic neural radiance field, from which a
high-fidelity talking-head video corresponding to the audio signal is
synthesized using volume rendering. Another advantage of our framework is that
not only the head (with hair) region is synthesized as previous methods did,
but also the upper body is generated via two individual neural radiance fields.
Experimental results demonstrate that our novel framework can (1) produce
high-fidelity and natural results, and (2) support free adjustment of audio
signals, viewing directions, and background images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yudong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Keyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1"&gt;Sen Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yongjin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1"&gt;Hujun Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Juyong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video Moment Retrieval with Text Query Considering Many-to-Many Correspondence Using Potentially Relevant Pair. (arXiv:2106.13566v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13566</id>
        <link href="http://arxiv.org/abs/2106.13566"/>
        <updated>2021-06-28T01:57:55.383Z</updated>
        <summary type="html"><![CDATA[In this paper we undertake the task of text-based video moment retrieval from
a corpus of videos. To train the model, text-moment paired datasets were used
to learn the correct correspondences. In typical training methods, ground-truth
text-moment pairs are used as positive pairs, whereas other pairs are regarded
as negative pairs. However, aside from the ground-truth pairs, some text-moment
pairs should be regarded as positive. In this case, one text annotation can be
positive for many video moments. Conversely, one video moment can be
corresponded to many text annotations. Thus, there are many-to-many
correspondences between the text annotations and video moments. Based on these
correspondences, we can form potentially relevant pairs, which are not given as
ground truth yet are not negative; effectively incorporating such relevant
pairs into training can improve the retrieval performance. The text query
should describe what is happening in a video moment. Hence, different video
moments annotated with similar texts, which contain a similar action, are
likely to hold the similar action, thus these pairs can be considered as
potentially relevant pairs. In this paper, we propose a novel training method
that takes advantage of potentially relevant pairs, which are detected based on
linguistic analysis about text annotation. Experiments on two benchmark
datasets revealed that our method improves the retrieval performance both
quantitatively and qualitatively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maeoki_S/0/1/0/all/0/1"&gt;Sho Maeoki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukuta_Y/0/1/0/all/0/1"&gt;Yusuke Mukuta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1"&gt;Tatsuya Harada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image-to-image Transformation with Auxiliary Condition. (arXiv:2106.13696v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13696</id>
        <link href="http://arxiv.org/abs/2106.13696"/>
        <updated>2021-06-28T01:57:55.361Z</updated>
        <summary type="html"><![CDATA[The performance of image recognition like human pose detection, trained with
simulated images would usually get worse due to the divergence between real and
simulated data. To make the distribution of a simulated image close to that of
real one, there are several works applying GAN-based image-to-image
transformation methods, e.g., SimGAN and CycleGAN. However, these methods would
not be sensitive enough to the various change in pose and shape of subjects,
especially when the training data are imbalanced, e.g., some particular poses
and shapes are minor in the training data. To overcome this problem, we propose
to introduce the label information of subjects, e.g., pose and type of objects
in the training of CycleGAN, and lead it to obtain label-wise transforamtion
models. We evaluate our proposed method called Label-CycleGAN, through
experiments on the digit image transformation from SVHN to MNIST and the
surveillance camera image transformation from simulated to real images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leer_R/0/1/0/all/0/1"&gt;Robert Leer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roma_H/0/1/0/all/0/1"&gt;Hessi Roma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amelia_J/0/1/0/all/0/1"&gt;James Amelia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Pose and Shape Estimation for Category-level 3D Object Perception. (arXiv:2104.08383v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08383</id>
        <link href="http://arxiv.org/abs/2104.08383"/>
        <updated>2021-06-28T01:57:55.354Z</updated>
        <summary type="html"><![CDATA[We consider a category-level perception problem, where one is given 3D sensor
data picturing an object of a given category (e.g. a car), and has to
reconstruct the pose and shape of the object despite intra-class variability
(i.e. different car models have different shapes). We consider an active shape
model, where -- for an object category -- we are given a library of potential
CAD models describing objects in that category, and we adopt a standard
formulation where pose and shape estimation are formulated as a non-convex
optimization. Our first contribution is to provide the first certifiably
optimal solver for pose and shape estimation. In particular, we show that
rotation estimation can be decoupled from the estimation of the object
translation and shape, and we demonstrate that (i) the optimal object rotation
can be computed via a tight (small-size) semidefinite relaxation, and (ii) the
translation and shape parameters can be computed in closed-form given the
rotation. Our second contribution is to add an outlier rejection layer to our
solver, hence making it robust to a large number of misdetections. Towards this
goal, we wrap our optimal solver in a robust estimation scheme based on
graduated non-convexity. To further enhance robustness to outliers, we also
develop the first graph-theoretic formulation to prune outliers in
category-level perception, which removes outliers via convex hull and maximum
clique computations; the resulting approach is robust to 70%-90% outliers. Our
third contribution is an extensive experimental evaluation. Besides providing
an ablation study on a simulated dataset and on the PASCAL3D+ dataset, we
combine our solver with a deep-learned keypoint detector, and show that the
resulting approach improves over the state of the art in vehicle pose
estimation in the ApolloScape datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jingnan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Heng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1"&gt;Luca Carlone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Self-Learning Framework for Bladder Cancer Grading Using Histopathological Images. (arXiv:2106.13559v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.13559</id>
        <link href="http://arxiv.org/abs/2106.13559"/>
        <updated>2021-06-28T01:57:55.346Z</updated>
        <summary type="html"><![CDATA[Recently, bladder cancer has been significantly increased in terms of
incidence and mortality. Currently, two subtypes are known based on tumour
growth: non-muscle invasive (NMIBC) and muscle-invasive bladder cancer (MIBC).
In this work, we focus on the MIBC subtype because it is of the worst prognosis
and can spread to adjacent organs. We present a self-learning framework to
grade bladder cancer from histological images stained via immunohistochemical
techniques. Specifically, we propose a novel Deep Convolutional Embedded
Attention Clustering (DCEAC) which allows classifying histological patches into
different severity levels of the disease, according to the patterns established
in the literature. The proposed DCEAC model follows a two-step fully
unsupervised learning methodology to discern between non-tumour, mild and
infiltrative patterns from high-resolution samples of 512x512 pixels. Our
system outperforms previous clustering-based methods by including a
convolutional attention module, which allows refining the features of the
latent space before the classification stage. The proposed network exceeds
state-of-the-art approaches by 2-3% across different metrics, achieving a final
average accuracy of 0.9034 in a multi-class scenario. Furthermore, the reported
class activation maps evidence that our model is able to learn by itself the
same patterns that clinicians consider relevant, without incurring prior
annotation steps. This fact supposes a breakthrough in muscle-invasive bladder
cancer grading which bridges the gap with respect to train the model on
labelled data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Garcia_G/0/1/0/all/0/1"&gt;Gabriel Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Esteve_A/0/1/0/all/0/1"&gt;Anna Esteve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Colomer_A/0/1/0/all/0/1"&gt;Adri&amp;#xe1;n Colomer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ramos_D/0/1/0/all/0/1"&gt;David Ramos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1"&gt;Valery Naranjo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Zero Shot" Point Cloud Upsampling. (arXiv:2106.13765v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13765</id>
        <link href="http://arxiv.org/abs/2106.13765"/>
        <updated>2021-06-28T01:57:55.328Z</updated>
        <summary type="html"><![CDATA[Point cloud upsampling using deep learning has been paid various efforts in
the past few years. Recent supervised deep learning methods are restricted to
the size of training data and is limited in terms of covering all shapes of
point clouds. Besides, the acquisition of such amount of data is unrealistic,
and the network generally performs less powerful than expected on unseen
records. In this paper, we present an unsupervised approach to upsample point
clouds internally referred as "Zero Shot" Point Cloud Upsampling (ZSPU) at
holistic level. Our approach is solely based on the internal information
provided by a particular point cloud without patching in both self-training and
testing phases. This single-stream design significantly reduces the training
time of the upsampling task, by learning the relation between low-resolution
(LR) point clouds and their high (original) resolution (HR) counterparts. This
association will provide super-resolution (SR) outputs when original point
clouds are loaded as input. We demonstrate competitive performance on benchmark
point cloud datasets when compared to other upsampling methods. Furthermore,
ZSPU achieves superior qualitative results on shapes with complex local details
or high curvatures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1"&gt;Kaiyue Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1"&gt;Ming Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arslanturk_S/0/1/0/all/0/1"&gt;Suzan Arslanturk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vision Transformer Architecture Search. (arXiv:2106.13700v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13700</id>
        <link href="http://arxiv.org/abs/2106.13700"/>
        <updated>2021-06-28T01:57:55.295Z</updated>
        <summary type="html"><![CDATA[Recently, transformers have shown great superiority in solving computer
vision tasks by modeling images as a sequence of manually-split patches with
self-attention mechanism. However, current architectures of vision transformers
(ViTs) are simply inherited from natural language processing (NLP) tasks and
have not been sufficiently investigated and optimized. In this paper, we make a
further step by examining the intrinsic structure of transformers for vision
tasks and propose an architecture search method, dubbed ViTAS, to search for
the optimal architecture with similar hardware budgets. Concretely, we design a
new effective yet efficient weight sharing paradigm for ViTs, such that
architectures with different token embedding, sequence size, number of heads,
width, and depth can be derived from a single super-transformer. Moreover, to
cater for the variance of distinct architectures, we introduce \textit{private}
class token and self-attention maps in the super-transformer. In addition, to
adapt the searching for different budgets, we propose to search the sampling
probability of identity operation. Experimental results show that our ViTAS
attains excellent results compared to existing pure transformer architectures.
For example, with $1.3$G FLOPs budget, our searched architecture achieves
$74.7\%$ top-$1$ accuracy on ImageNet and is $2.5\%$ superior than the current
baseline ViT architecture. Code is available at
\url{https://github.com/xiusu/ViTAS}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1"&gt;Xiu Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1"&gt;Shan You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiyang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1"&gt;Mingkai Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1"&gt;Chen Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Changshui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Pattern Loss based Diversified Attention Network for Cross-Modal Retrieval. (arXiv:2106.13552v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13552</id>
        <link href="http://arxiv.org/abs/2106.13552"/>
        <updated>2021-06-28T01:57:55.288Z</updated>
        <summary type="html"><![CDATA[Cross-modal retrieval aims to enable flexible retrieval experience by
combining multimedia data such as image, video, text, and audio. One core of
unsupervised approaches is to dig the correlations among different object
representations to complete satisfied retrieval performance without requiring
expensive labels. In this paper, we propose a Graph Pattern Loss based
Diversified Attention Network(GPLDAN) for unsupervised cross-modal retrieval to
deeply analyze correlations among representations. First, we propose a
diversified attention feature projector by considering the interaction between
different representations to generate multiple representations of an instance.
Then, we design a novel graph pattern loss to explore the correlations among
different representations, in this graph all possible distances between
different representations are considered. In addition, a modality classifier is
added to explicitly declare the corresponding modalities of features before
fusion and guide the network to enhance discrimination ability. We test GPLDAN
on four public datasets. Compared with the state-of-the-art cross-modal
retrieval methods, the experimental results demonstrate the performance and
competitiveness of GPLDAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xueying Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1"&gt;Yibing Zhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Circumpapillary OCT-Focused Hybrid Learning for Glaucoma Grading Using Tailored Prototypical Neural Networks. (arXiv:2106.13551v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.13551</id>
        <link href="http://arxiv.org/abs/2106.13551"/>
        <updated>2021-06-28T01:57:55.282Z</updated>
        <summary type="html"><![CDATA[Glaucoma is one of the leading causes of blindness worldwide and Optical
Coherence Tomography (OCT) is the quintessential imaging technique for its
detection. Unlike most of the state-of-the-art studies focused on glaucoma
detection, in this paper, we propose, for the first time, a novel framework for
glaucoma grading using raw circumpapillary B-scans. In particular, we set out a
new OCT-based hybrid network which combines hand-driven and deep learning
algorithms. An OCT-specific descriptor is proposed to extract hand-crafted
features related to the retinal nerve fibre layer (RNFL). In parallel, an
innovative CNN is developed using skip-connections to include tailored residual
and attention modules to refine the automatic features of the latent space. The
proposed architecture is used as a backbone to conduct a novel few-shot
learning based on static and dynamic prototypical networks. The k-shot paradigm
is redefined giving rise to a supervised end-to-end system which provides
substantial improvements discriminating between healthy, early and advanced
glaucoma samples. The training and evaluation processes of the dynamic
prototypical network are addressed from two fused databases acquired via
Heidelberg Spectralis system. Validation and testing results reach a
categorical accuracy of 0.9459 and 0.8788 for glaucoma grading, respectively.
Besides, the high performance reported by the proposed model for glaucoma
detection deserves a special mention. The findings from the class activation
maps are directly in line with the clinicians' opinion since the heatmaps
pointed out the RNFL as the most relevant structure for glaucoma diagnosis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Garcia_G/0/1/0/all/0/1"&gt;Gabriel Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Amor_R/0/1/0/all/0/1"&gt;Roc&amp;#xed;o del Amor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Colomer_A/0/1/0/all/0/1"&gt;Adri&amp;#xe1;n Colomer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Verdu_Monedero_R/0/1/0/all/0/1"&gt;Rafael Verd&amp;#xfa;-Monedero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Morales_Sanchez_J/0/1/0/all/0/1"&gt;Juan Morales-S&amp;#xe1;nchez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1"&gt;Valery Naranjo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probing Inter-modality: Visual Parsing with Self-Attention for Vision-Language Pre-training. (arXiv:2106.13488v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13488</id>
        <link href="http://arxiv.org/abs/2106.13488"/>
        <updated>2021-06-28T01:57:55.274Z</updated>
        <summary type="html"><![CDATA[Vision-Language Pre-training (VLP) aims to learn multi-modal representations
from image-text pairs and serves for downstream vision-language tasks in a
fine-tuning fashion. The dominant VLP models adopt a CNN-Transformer
architecture, which embeds images with a CNN, and then aligns images and text
with a Transformer. Visual relationship between visual contents plays an
important role in image understanding and is the basic for inter-modal
alignment learning. However, CNNs have limitations in visual relation learning
due to local receptive field's weakness in modeling long-range dependencies.
Thus the two objectives of learning visual relation and inter-modal alignment
are encapsulated in the same Transformer network. Such design might restrict
the inter-modal alignment learning in the Transformer by ignoring the
specialized characteristic of each objective. To tackle this, we propose a
fully Transformer visual embedding for VLP to better learn visual relation and
further promote inter-modal alignment. Specifically, we propose a metric named
Inter-Modality Flow (IMF) to measure the interaction between vision and
language modalities (i.e., inter-modality). We also design a novel masking
optimization mechanism named Masked Feature Regression (MFR) in Transformer to
further promote the inter-modality learning. To the best of our knowledge, this
is the first study to explore the benefit of Transformer for visual feature
learning in VLP. We verify our method on a wide range of vision-language tasks,
including Visual Question Answering (VQA), Visual Entailment and Visual
Reasoning. Our approach not only outperforms the state-of-the-art VLP
performance, but also shows benefits on the IMF metric.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1"&gt;Hongwei Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yupan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Houwen Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1"&gt;Jianlong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Houqiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jiebo Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Animatable Neural Radiance Fields from Monocular RGB Video. (arXiv:2106.13629v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13629</id>
        <link href="http://arxiv.org/abs/2106.13629"/>
        <updated>2021-06-28T01:57:55.255Z</updated>
        <summary type="html"><![CDATA[We present animatable neural radiance fields for detailed human avatar
creation from monocular videos. Our approach extends neural radiance fields
(NeRF) to the dynamic scenes with human movements via introducing explicit
pose-guided deformation while learning the scene representation network. In
particular, we estimate the human pose for each frame and learn a constant
canonical space for the detailed human template, which enables natural shape
deformation from the observation space to the canonical space under the
explicit control of the pose parameters. To compensate for inaccurate pose
estimation, we introduce the pose refinement strategy that updates the initial
pose during the learning process, which not only helps to learn more accurate
human reconstruction but also accelerates the convergence. In experiments we
show that the proposed approach achieves 1) implicit human geometry and
appearance reconstruction with high-quality details, 2) photo-realistic
rendering of the human from arbitrary views, and 3) animation of the human with
arbitrary poses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jianchuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Ying Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1"&gt;Di Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhe_X/0/1/0/all/0/1"&gt;Xuefei Zhe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1"&gt;Linchao Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Huchuan Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single Image Texture Translation for Data Augmentation. (arXiv:2106.13804v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13804</id>
        <link href="http://arxiv.org/abs/2106.13804"/>
        <updated>2021-06-28T01:57:55.244Z</updated>
        <summary type="html"><![CDATA[Recent advances in image synthesis enables one to translate images by
learning the mapping between a source domain and a target domain. Existing
methods tend to learn the distributions by training a model on a variety of
datasets, with results evaluated largely in a subjective manner. Relatively few
works in this area, however, study the potential use of semantic image
translation methods for image recognition tasks. In this paper, we explore the
use of Single Image Texture Translation (SITT) for data augmentation. We first
propose a lightweight model for translating texture to images based on a single
input of source texture, allowing for fast training and testing. Based on SITT,
we then explore the use of augmented data in long-tailed and few-shot image
classification tasks. We find the proposed method is capable of translating
input data into a target domain, leading to consistent improved image
recognition performance. Finally, we examine how SITT and related image
translation methods can provide a basis for a data-efficient, augmentation
engineering approach to model training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Boyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1"&gt;Yin Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tsung-Yi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1"&gt;Serge Belongie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RSN: Range Sparse Net for Efficient, Accurate LiDAR 3D Object Detection. (arXiv:2106.13365v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13365</id>
        <link href="http://arxiv.org/abs/2106.13365"/>
        <updated>2021-06-28T01:57:55.237Z</updated>
        <summary type="html"><![CDATA[The detection of 3D objects from LiDAR data is a critical component in most
autonomous driving systems. Safe, high speed driving needs larger detection
ranges, which are enabled by new LiDARs. These larger detection ranges require
more efficient and accurate detection models. Towards this goal, we propose
Range Sparse Net (RSN), a simple, efficient, and accurate 3D object detector in
order to tackle real time 3D object detection in this extended detection
regime. RSN predicts foreground points from range images and applies sparse
convolutions on the selected foreground points to detect objects. The
lightweight 2D convolutions on dense range images results in significantly
fewer selected foreground points, thus enabling the later sparse convolutions
in RSN to efficiently operate. Combining features from the range image further
enhance detection accuracy. RSN runs at more than 60 frames per second on a
150m x 150m detection region on Waymo Open Dataset (WOD) while being more
accurate than previously published detectors. As of 11/2020, RSN is ranked
first in the WOD leaderboard based on the APH/LEVEL 1 metrics for LiDAR-based
pedestrian and vehicle detection, while being several times faster than
alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1"&gt;Pei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weiyue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1"&gt;Yuning Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elsayed_G/0/1/0/all/0/1"&gt;Gamaleldin Elsayed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bewley_A/0/1/0/all/0/1"&gt;Alex Bewley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sminchisescu_C/0/1/0/all/0/1"&gt;Cristian Sminchisescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anguelov_D/0/1/0/all/0/1"&gt;Dragomir Anguelov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SRPN: similarity-based region proposal networks for nuclei and cells detection in histology images. (arXiv:2106.13556v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13556</id>
        <link href="http://arxiv.org/abs/2106.13556"/>
        <updated>2021-06-28T01:57:55.231Z</updated>
        <summary type="html"><![CDATA[The detection of nuclei and cells in histology images is of great value in
both clinical practice and pathological studies. However, multiple reasons such
as morphological variations of nuclei or cells make it a challenging task where
conventional object detection methods cannot obtain satisfactory performance in
many cases. A detection task consists of two sub-tasks, classification and
localization. Under the condition of dense object detection, classification is
a key to boost the detection performance. Considering this, we propose
similarity based region proposal networks (SRPN) for nuclei and cells detection
in histology images. In particular, a customized convolution layer termed as
embedding layer is designed for network building. The embedding layer is added
into the region proposal networks, enabling the networks to learn
discriminative features based on similarity learning. Features obtained by
similarity learning can significantly boost the classification performance
compared to conventional methods. SRPN can be easily integrated into standard
convolutional neural networks architectures such as the Faster R-CNN and
RetinaNet. We test the proposed approach on tasks of multi-organ nuclei
detection and signet ring cells detection in histological images. Experimental
results show that networks applying similarity learning achieved superior
performance on both tasks when compared to their counterparts. In particular,
the proposed SRPN achieve state-of-the-art performance on the MoNuSeg benchmark
for nuclei segmentation and detection while compared to previous methods, and
on the signet ring cell detection benchmark when compared with baselines. The
sourcecode is publicly available at:
https://github.com/sigma10010/nuclei_cells_det.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yibao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xingru Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Huiyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qianni Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Countering Adversarial Examples: Combining Input Transformation and Noisy Training. (arXiv:2106.13394v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13394</id>
        <link href="http://arxiv.org/abs/2106.13394"/>
        <updated>2021-06-28T01:57:55.222Z</updated>
        <summary type="html"><![CDATA[Recent studies have shown that neural network (NN) based image classifiers
are highly vulnerable to adversarial examples, which poses a threat to
security-sensitive image recognition task. Prior work has shown that JPEG
compression can combat the drop in classification accuracy on adversarial
examples to some extent. But, as the compression ratio increases, traditional
JPEG compression is insufficient to defend those attacks but can cause an
abrupt accuracy decline to the benign images. In this paper, with the aim of
fully filtering the adversarial perturbations, we firstly make modifications to
traditional JPEG compression algorithm which becomes more favorable for NN.
Specifically, based on an analysis of the frequency coefficient, we design a
NN-favored quantization table for compression. Considering compression as a
data augmentation strategy, we then combine our model-agnostic preprocess with
noisy training. We fine-tune the pre-trained model by training with images
encoded at different compression levels, thus generating multiple classifiers.
Finally, since lower (higher) compression ratio can remove both perturbations
and original features slightly (aggressively), we use these trained multiple
models for model ensemble. The majority vote of the ensemble of models is
adopted as final predictions. Experiments results show our method can improve
defense efficiency while maintaining original accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Cheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1"&gt;Pan Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Modeling for Multi-task Visual Learning. (arXiv:2106.13409v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13409</id>
        <link href="http://arxiv.org/abs/2106.13409"/>
        <updated>2021-06-28T01:57:55.202Z</updated>
        <summary type="html"><![CDATA[Generative modeling has recently shown great promise in computer vision, but
it has mostly focused on synthesizing visually realistic images. In this paper,
motivated by multi-task learning of shareable feature representations, we
consider a novel problem of learning a shared generative model that is useful
across various visual perception tasks. Correspondingly, we propose a general
multi-task oriented generative modeling (MGM) framework, by coupling a
discriminative multi-task network with a generative network. While it is
challenging to synthesize both RGB images and pixel-level annotations in
multi-task scenarios, our framework enables us to use synthesized images paired
with only weak annotations (i.e., image-level scene labels) to facilitate
multiple visual tasks. Experimental evaluation on challenging multi-task
benchmarks, including NYUv2 and Taskonomy, demonstrates that our MGM framework
improves the performance of all the tasks by large margins, consistently
outperforming state-of-the-art multi-task approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1"&gt;Zhipeng Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hebert_M/0/1/0/all/0/1"&gt;Martial Hebert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu-Xiong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NP-DRAW: A Non-Parametric Structured Latent Variable Modelfor Image Generation. (arXiv:2106.13435v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13435</id>
        <link href="http://arxiv.org/abs/2106.13435"/>
        <updated>2021-06-28T01:57:55.196Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a non-parametric structured latent variable model
for image generation, called NP-DRAW, which sequentially draws on a latent
canvas in a part-by-part fashion and then decodes the image from the canvas.
Our key contributions are as follows. 1) We propose a non-parametric prior
distribution over the appearance of image parts so that the latent variable
``what-to-draw'' per step becomes a categorical random variable. This improves
the expressiveness and greatly eases the learning compared to Gaussians used in
the literature. 2) We model the sequential dependency structure of parts via a
Transformer, which is more powerful and easier to train compared to RNNs used
in the literature. 3) We propose an effective heuristic parsing algorithm to
pre-train the prior. Experiments on MNIST, Omniglot, CIFAR-10, and CelebA show
that our method significantly outperforms previous structured image models like
DRAW and AIR and is competitive to other generic generative models. Moreover,
we show that our model's inherent compositionality and interpretability bring
significant benefits in the low-data learning regime and latent space editing.
Code is available at \url{https://github.com/ZENGXH/NPDRAW}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xiaohui Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1"&gt;Raquel Urtasun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1"&gt;Richard Zemel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1"&gt;Sanja Fidler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1"&gt;Renjie Liao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shape registration in the time of transformers. (arXiv:2106.13679v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13679</id>
        <link href="http://arxiv.org/abs/2106.13679"/>
        <updated>2021-06-28T01:57:55.190Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a transformer-based procedure for the efficient
registration of non-rigid 3D point clouds. The proposed approach is data-driven
and adopts for the first time the transformer architecture in the registration
task. Our method is general and applies to different settings. Given a fixed
template with some desired properties (e.g. skinning weights or other animation
cues), we can register raw acquired data to it, thereby transferring all the
template properties to the input geometry. Alternatively, given a pair of
shapes, our method can register the first onto the second (or vice-versa),
obtaining a high-quality dense correspondence between the two. In both
contexts, the quality of our results enables us to target real applications
such as texture transfer and shape interpolation. Furthermore, we also show
that including an estimation of the underlying density of the surface eases the
learning process. By exploiting the potential of this architecture, we can
train our model requiring only a sparse set of ground truth correspondences
($10\sim20\%$ of the total points). The proposed model and the analysis that we
perform pave the way for future exploration of transformer-based architectures
for registration and matching applications. Qualitative and quantitative
evaluations demonstrate that our pipeline outperforms state-of-the-art methods
for deformable and unordered 3D data registration on different datasets and
scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trappolini_G/0/1/0/all/0/1"&gt;Giovanni Trappolini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cosmo_L/0/1/0/all/0/1"&gt;Luca Cosmo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moschella_L/0/1/0/all/0/1"&gt;Luca Moschella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marin_R/0/1/0/all/0/1"&gt;Riccardo Marin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1"&gt;Emanuele Rodol&amp;#xe0;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Robustness of Pretraining and Self-Supervision for a Deep Learning-based Analysis of Diabetic Retinopathy. (arXiv:2106.13497v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13497</id>
        <link href="http://arxiv.org/abs/2106.13497"/>
        <updated>2021-06-28T01:57:55.181Z</updated>
        <summary type="html"><![CDATA[There is an increasing number of medical use-cases where classification
algorithms based on deep neural networks reach performance levels that are
competitive with human medical experts. To alleviate the challenges of small
dataset sizes, these systems often rely on pretraining. In this work, we aim to
assess the broader implications of these approaches. For diabetic retinopathy
grading as exemplary use case, we compare the impact of different training
procedures including recently established self-supervised pretraining methods
based on contrastive learning. To this end, we investigate different aspects
such as quantitative performance, statistics of the learned feature
representations, interpretability and robustness to image distortions. Our
results indicate that models initialized from ImageNet pretraining report a
significant increase in performance, generalization and robustness to image
distortions. In particular, self-supervised models show further benefits to
supervised models. Self-supervised models with initialization from ImageNet
pretraining not only report higher performance, they also reduce overfitting to
large lesions along with improvements in taking into account minute lesions
indicative of the progression of the disease. Understanding the effects of
pretraining in a broader sense that goes beyond simple performance comparisons
is of crucial importance for the broader medical imaging community beyond the
use-case considered in this work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasan_V/0/1/0/all/0/1"&gt;Vignesh Srinivasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strodthoff_N/0/1/0/all/0/1"&gt;Nils Strodthoff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jackie Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Binder_A/0/1/0/all/0/1"&gt;Alexander Binder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1"&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1"&gt;Wojciech Samek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[To the Point: Efficient 3D Object Detection in the Range Image with Graph Convolution Kernels. (arXiv:2106.13381v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13381</id>
        <link href="http://arxiv.org/abs/2106.13381"/>
        <updated>2021-06-28T01:57:55.175Z</updated>
        <summary type="html"><![CDATA[3D object detection is vital for many robotics applications. For tasks where
a 2D perspective range image exists, we propose to learn a 3D representation
directly from this range image view. To this end, we designed a 2D
convolutional network architecture that carries the 3D spherical coordinates of
each pixel throughout the network. Its layers can consume any arbitrary
convolution kernel in place of the default inner product kernel and exploit the
underlying local geometry around each pixel. We outline four such kernels: a
dense kernel according to the bag-of-words paradigm, and three graph kernels
inspired by recent graph neural network advances: the Transformer, the
PointNet, and the Edge Convolution. We also explore cross-modality fusion with
the camera image, facilitated by operating in the perspective range image view.
Our method performs competitively on the Waymo Open Dataset and improves the
state-of-the-art AP for pedestrian detection from 69.7% to 75.5%. It is also
efficient in that our smallest model, which still outperforms the popular
PointPillars in quality, requires 180 times fewer FLOPS and model parameters]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1"&gt;Yuning Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1"&gt;Pei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ngiam_J/0/1/0/all/0/1"&gt;Jiquan Ngiam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weiyue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caine_B/0/1/0/all/0/1"&gt;Benjamin Caine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasudevan_V/0/1/0/all/0/1"&gt;Vijay Vasudevan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anguelov_D/0/1/0/all/0/1"&gt;Dragomir Anguelov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Connecting Sphere Manifolds Hierarchically for Regularization. (arXiv:2106.13549v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13549</id>
        <link href="http://arxiv.org/abs/2106.13549"/>
        <updated>2021-06-28T01:57:55.155Z</updated>
        <summary type="html"><![CDATA[This paper considers classification problems with hierarchically organized
classes. We force the classifier (hyperplane) of each class to belong to a
sphere manifold, whose center is the classifier of its super-class. Then,
individual sphere manifolds are connected based on their hierarchical
relations. Our technique replaces the last layer of a neural network by
combining a spherical fully-connected layer with a hierarchical layer. This
regularization is shown to improve the performance of widely used deep neural
network architectures (ResNet and DenseNet) on publicly available datasets
(CIFAR100, CUB200, Stanford dogs, Stanford cars, and Tiny-ImageNet).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Scieur_D/0/1/0/all/0/1"&gt;Damien Scieur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Youngsung Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diversifying Semantic Image Synthesis and Editing via Class- and Layer-wise VAEs. (arXiv:2106.13416v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13416</id>
        <link href="http://arxiv.org/abs/2106.13416"/>
        <updated>2021-06-28T01:57:55.148Z</updated>
        <summary type="html"><![CDATA[Semantic image synthesis is a process for generating photorealistic images
from a single semantic mask. To enrich the diversity of multimodal image
synthesis, previous methods have controlled the global appearance of an output
image by learning a single latent space. However, a single latent code is often
insufficient for capturing various object styles because object appearance
depends on multiple factors. To handle individual factors that determine object
styles, we propose a class- and layer-wise extension to the variational
autoencoder (VAE) framework that allows flexible control over each object class
at the local to global levels by learning multiple latent spaces. Furthermore,
we demonstrate that our method generates images that are both plausible and
more diverse compared to state-of-the-art methods via extensive experiments
with real and synthetic datasets inthree different domains. We also show that
our method enables a wide range of applications in image synthesis and editing
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Endo_Y/0/1/0/all/0/1"&gt;Yuki Endo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanamori_Y/0/1/0/all/0/1"&gt;Yoshihiro Kanamori&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiview Video Compression Using Advanced HEVC Screen Content Coding. (arXiv:2106.13574v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2106.13574</id>
        <link href="http://arxiv.org/abs/2106.13574"/>
        <updated>2021-06-28T01:57:55.139Z</updated>
        <summary type="html"><![CDATA[The paper presents a new approach to multiview video coding using Screen
Content Coding. It is assumed that for a time instant the frames corresponding
to all views are packed into a single frame, i.e. the frame-compatible approach
to multiview coding is applied. For such coding scenario, the paper
demonstrates that Screen Content Coding can be efficiently used for multiview
video coding. Two approaches are considered: the first using standard HEVC
Screen Content Coding, and the second using Advanced Screen Content Coding. The
latter is the original proposal of the authors that exploits quarter-pel motion
vectors and other nonstandard extensions of HEVC Screen Content Coding. The
experimental results demonstrate that multiview video coding even using
standard HEVC Screen Content Coding is much more efficient than simulcast HEVC
coding. The proposed Advanced Screen Content Coding provides virtually the same
coding efficiency as MV-HEVC, which is the state-of-the-art multiview video
compression technique. The authors suggest that Advanced Screen Content Coding
can be efficiently used within the new Versatile Video Coding (VVC) technology.
Nevertheless a reference multiview extension of VVC does not exist yet,
therefore, for VVC-based coding, the experimental comparisons are left for
future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samelak_J/0/1/0/all/0/1"&gt;Jaros&amp;#x142;aw Samelak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Domanski_M/0/1/0/all/0/1"&gt;Marek Doma&amp;#x144;ski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Partially fake it till you make it: mixing real and fake thermal images for improved object detection. (arXiv:2106.13603v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13603</id>
        <link href="http://arxiv.org/abs/2106.13603"/>
        <updated>2021-06-28T01:57:55.114Z</updated>
        <summary type="html"><![CDATA[In this paper we propose a novel data augmentation approach for visual
content domains that have scarce training datasets, compositing synthetic 3D
objects within real scenes. We show the performance of the proposed system in
the context of object detection in thermal videos, a domain where 1) training
datasets are very limited compared to visible spectrum datasets and 2) creating
full realistic synthetic scenes is extremely cumbersome and expensive due to
the difficulty in modeling the thermal properties of the materials of the
scene. We compare different augmentation strategies, including state of the art
approaches obtained through RL techniques, the injection of simulated data and
the employment of a generative model, and study how to best combine our
proposed augmentation with these other techniques.Experimental results
demonstrate the effectiveness of our approach, and our single-modality detector
achieves state-of-the-art results on the FLIR ADAS dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bongini_F/0/1/0/all/0/1"&gt;Francesco Bongini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berlincioni_L/0/1/0/all/0/1"&gt;Lorenzo Berlincioni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bertini_M/0/1/0/all/0/1"&gt;Marco Bertini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1"&gt;Alberto Del Bimbo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Eye Tracking. (arXiv:2106.13387v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13387</id>
        <link href="http://arxiv.org/abs/2106.13387"/>
        <updated>2021-06-28T01:57:55.106Z</updated>
        <summary type="html"><![CDATA[Model-based eye tracking has been a dominant approach for eye gaze tracking
because of its ability to generalize to different subjects, without the need of
any training data and eye gaze annotations. Model-based eye tracking, however,
is susceptible to eye feature detection errors, in particular for eye tracking
in the wild. To address this issue, we propose a Bayesian framework for
model-based eye tracking. The proposed system consists of a cascade-Bayesian
Convolutional Neural Network (c-BCNN) to capture the probabilistic
relationships between eye appearance and its landmarks, and a geometric eye
model to estimate eye gaze from the eye landmarks. Given a testing eye image,
the Bayesian framework can generate, through Bayesian inference, the eye gaze
distribution without explicit landmark detection and model training, based on
which it not only estimates the most likely eye gaze but also its uncertainty.
Furthermore, with Bayesian inference instead of point-based inference, our
model can not only generalize better to different sub-jects, head poses, and
environments but also is robust to image noise and landmark detection errors.
Finally, with the estimated gaze uncertainty, we can construct a cascade
architecture that allows us to progressively improve gaze estimation accuracy.
Compared to state-of-the-art model-based and learning-based methods, the
proposed Bayesian framework demonstrates significant improvement in
generalization capability across several benchmark datasets and in accuracy and
robustness under challenging real-world conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_Q/0/1/0/all/0/1"&gt;Qiang Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CausalCity: Complex Simulations with Agency for Causal Discovery and Reasoning. (arXiv:2106.13364v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.13364</id>
        <link href="http://arxiv.org/abs/2106.13364"/>
        <updated>2021-06-28T01:57:55.087Z</updated>
        <summary type="html"><![CDATA[The ability to perform causal and counterfactual reasoning are central
properties of human intelligence. Decision-making systems that can perform
these types of reasoning have the potential to be more generalizable and
interpretable. Simulations have helped advance the state-of-the-art in this
domain, by providing the ability to systematically vary parameters (e.g.,
confounders) and generate examples of the outcomes in the case of
counterfactual scenarios. However, simulating complex temporal causal events in
multi-agent scenarios, such as those that exist in driving and vehicle
navigation, is challenging. To help address this, we present a high-fidelity
simulation environment that is designed for developing algorithms for causal
discovery and counterfactual reasoning in the safety-critical context. A core
component of our work is to introduce \textit{agency}, such that it is simple
to define and create complex scenarios using high-level definitions. The
vehicles then operate with agency to complete these objectives, meaning
low-level behaviors need only be controlled if necessary. We perform
experiments with three state-of-the-art methods to create baselines and
highlight the affordances of this environment. Finally, we highlight challenges
and opportunities for future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1"&gt;Daniel McDuff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yale Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jiyoung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1"&gt;Vibhav Vineet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vemprala_S/0/1/0/all/0/1"&gt;Sai Vemprala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gyde_N/0/1/0/all/0/1"&gt;Nicholas Gyde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salman_H/0/1/0/all/0/1"&gt;Hadi Salman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shuang Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1"&gt;Kwanghoon Sohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kapoor_A/0/1/0/all/0/1"&gt;Ashish Kapoor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HAN: An Efficient Hierarchical Self-Attention Network for Skeleton-Based Gesture Recognition. (arXiv:2106.13391v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13391</id>
        <link href="http://arxiv.org/abs/2106.13391"/>
        <updated>2021-06-28T01:57:55.010Z</updated>
        <summary type="html"><![CDATA[Previous methods for skeleton-based gesture recognition mostly arrange the
skeleton sequence into a pseudo picture or spatial-temporal graph and apply
deep Convolutional Neural Network (CNN) or Graph Convolutional Network (GCN)
for feature extraction. Although achieving superior results, these methods have
inherent limitations in dynamically capturing local features of interactive
hand parts, and the computing efficiency still remains a serious issue. In this
work, the self-attention mechanism is introduced to alleviate this problem.
Considering the hierarchical structure of hand joints, we propose an efficient
hierarchical self-attention network (HAN) for skeleton-based gesture
recognition, which is based on pure self-attention without any CNN, RNN or GCN
operators. Specifically, the joint self-attention module is used to capture
spatial features of fingers, the finger self-attention module is designed to
aggregate features of the whole hand. In terms of temporal features, the
temporal self-attention module is utilized to capture the temporal dynamics of
the fingers and the entire hand. Finally, these features are fused by the
fusion self-attention module for gesture classification. Experiments show that
our method achieves competitive results on three gesture recognition datasets
with much lower computational complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jianbo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Ying Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1"&gt;Shiming Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1"&gt;Chunhong Pan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics perception in sloshing scenes with guaranteed thermodynamic consistency. (arXiv:2106.13301v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13301</id>
        <link href="http://arxiv.org/abs/2106.13301"/>
        <updated>2021-06-28T01:57:55.003Z</updated>
        <summary type="html"><![CDATA[Physics perception very often faces the problem that only limited data or
partial measurements on the scene are available. In this work, we propose a
strategy to learn the full state of sloshing liquids from measurements of the
free surface. Our approach is based on recurrent neural networks (RNN) that
project the limited information available to a reduced-order manifold so as to
not only reconstruct the unknown information, but also to be capable of
performing fluid reasoning about future scenarios in real time. To obtain
physically consistent predictions, we train deep neural networks on the
reduced-order manifold that, through the employ of inductive biases, ensure the
fulfillment of the principles of thermodynamics. RNNs learn from history the
required hidden information to correlate the limited information with the
latent space where the simulation occurs. Finally, a decoder returns data back
to the high-dimensional manifold, so as to provide the user with insightful
information in the form of augmented reality. This algorithm is connected to a
computer vision system to test the performance of the proposed methodology with
real information, resulting in a system capable of understanding and predicting
future states of the observed fluid in real-time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moya_B/0/1/0/all/0/1"&gt;Beatriz Moya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badias_A/0/1/0/all/0/1"&gt;Alberto Badias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_D/0/1/0/all/0/1"&gt;David Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chinesta_F/0/1/0/all/0/1"&gt;Francisco Chinesta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cueto_E/0/1/0/all/0/1"&gt;Elias Cueto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FOVQA: Blind Foveated Video Quality Assessment. (arXiv:2106.13328v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.13328</id>
        <link href="http://arxiv.org/abs/2106.13328"/>
        <updated>2021-06-28T01:57:54.996Z</updated>
        <summary type="html"><![CDATA[Previous blind or No Reference (NR) video quality assessment (VQA) models
largely rely on features drawn from natural scene statistics (NSS), but under
the assumption that the image statistics are stationary in the spatial domain.
Several of these models are quite successful on standard pictures. However, in
Virtual Reality (VR) applications, foveated video compression is regaining
attention, and the concept of space-variant quality assessment is of interest,
given the availability of increasingly high spatial and temporal resolution
contents and practical ways of measuring gaze direction. Distortions from
foveated video compression increase with increased eccentricity, implying that
the natural scene statistics are space-variant. Towards advancing the
development of foveated compression / streaming algorithms, we have devised a
no-reference (NR) foveated video quality assessment model, called FOVQA, which
is based on new models of space-variant natural scene statistics (NSS) and
natural video statistics (NVS). Specifically, we deploy a space-variant
generalized Gaussian distribution (SV-GGD) model and a space-variant
asynchronous generalized Gaussian distribution (SV-AGGD) model of mean
subtracted contrast normalized (MSCN) coefficients and products of neighboring
MSCN coefficients, respectively. We devise a foveated video quality predictor
that extracts radial basis features, and other features that capture
perceptually annoying rapid quality fall-offs. We find that FOVQA achieves
state-of-the-art (SOTA) performance on the new 2D LIVE-FBT-FCVR database, as
compared with other leading FIQA / VQA models. we have made our implementation
of FOVQA available at: this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jin_Y/0/1/0/all/0/1"&gt;Yize Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Patney_A/0/1/0/all/0/1"&gt;Anjul Patney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Webb_R/0/1/0/all/0/1"&gt;Richard Webb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bovik_A/0/1/0/all/0/1"&gt;Alan Bovik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized Unsupervised Clustering of Hyperspectral Images of Geological Targets in the Near Infrared. (arXiv:2106.13315v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.13315</id>
        <link href="http://arxiv.org/abs/2106.13315"/>
        <updated>2021-06-28T01:57:54.988Z</updated>
        <summary type="html"><![CDATA[The application of infrared hyperspectral imagery to geological problems is
becoming more popular as data become more accessible and cost-effective.
Clustering and classifying spectrally similar materials is often a first step
in applications ranging from economic mineral exploration on Earth to planetary
exploration on Mars. Semi-manual classification guided by expertly developed
spectral parameters can be time consuming and biased, while supervised methods
require abundant labeled data and can be difficult to generalize. Here we
develop a fully unsupervised workflow for feature extraction and clustering
informed by both expert spectral geologist input and quantitative metrics. Our
pipeline uses a lightweight autoencoder followed by Gaussian mixture modeling
to map the spectral diversity within any image. We validate the performance of
our pipeline at submillimeter-scale with expert-labelled data from the Oman
ophiolite drill core and evaluate performance at meters-scale with partially
classified orbital data of Jezero Crater on Mars (the landing site for the
Perseverance rover). We additionally examine the effects of various
preprocessing techniques used in traditional analysis of hyperspectral imagery.
This pipeline provides a fast and accurate clustering map of similar geological
materials and consistently identifies and separates major mineral classes in
both laboratory imagery and remote sensing imagery. We refer to our pipeline as
"Generalized Pipeline for Spectroscopic Unsupervised clustering of Minerals
(GyPSUM)."]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gao_A/0/1/0/all/0/1"&gt;Angela F. Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rasmussen_B/0/1/0/all/0/1"&gt;Brandon Rasmussen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kulits_P/0/1/0/all/0/1"&gt;Peter Kulits&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Scheller_E/0/1/0/all/0/1"&gt;Eva L. Scheller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Greenberger_R/0/1/0/all/0/1"&gt;Rebecca Greenberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ehlmann_B/0/1/0/all/0/1"&gt;Bethany L. Ehlmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-guided Machine Learning for Remotely Sensed In-Season Crop Growth Estimation. (arXiv:2106.13323v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13323</id>
        <link href="http://arxiv.org/abs/2106.13323"/>
        <updated>2021-06-28T01:57:54.963Z</updated>
        <summary type="html"><![CDATA[Advanced machine learning techniques have been used in remote sensing (RS)
applications such as crop mapping and yield prediction, but remain
under-utilized for tracking crop progress. In this study, we demonstrate the
use of agronomic knowledge of crop growth drivers in a Long Short-Term
Memory-based, Domain-guided neural network (DgNN) for in-season crop progress
estimation. The DgNN uses a branched structure and attention to separate
independent crop growth drivers and capture their varying importance throughout
the growing season. The DgNN is implemented for corn, using RS data in Iowa for
the period 2003-2019, with USDA crop progress reports used as ground truth.
State-wide DgNN performance shows significant improvement over sequential and
dense-only NN structures, and a widely-used Hidden Markov Model method. The
DgNN had a 3.5% higher Nash-Sutfliffe efficiency over all growth stages and 33%
more weeks with highest cosine similarity than the other NNs during test years.
The DgNN and Sequential NN were more robust during periods of abnormal crop
progress, though estimating the Silking-Grainfill transition was difficult for
all methods. Finally, Uniform Manifold Approximation and Projection
visualizations of layer activations showed how LSTM-based NNs separate crop
growth time-series differently from a dense-only structure. Results from this
study exhibit both the viability of NNs in crop growth stage estimation (CGSE)
and the benefits of using domain knowledge. The DgNN methodology presented here
can be extended to provide near-real time CGSE of other crops.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Worrall_G/0/1/0/all/0/1"&gt;George Worrall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rangarajan_A/0/1/0/all/0/1"&gt;Anand Rangarajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Judge_J/0/1/0/all/0/1"&gt;Jasmeet Judge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpreting Depression From Question-wise Long-term Video Recording of SDS Evaluation. (arXiv:2106.13393v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13393</id>
        <link href="http://arxiv.org/abs/2106.13393"/>
        <updated>2021-06-28T01:57:54.947Z</updated>
        <summary type="html"><![CDATA[Self-Rating Depression Scale (SDS) questionnaire has frequently been used for
efficient depression preliminary screening. However, the uncontrollable
self-administered measure can be easily affected by insouciantly or deceptively
answering, and producing the different results with the clinician-administered
Hamilton Depression Rating Scale (HDRS) and the final diagnosis. Clinically,
facial expression (FE) and actions play a vital role in clinician-administered
evaluation, while FE and action are underexplored for self-administered
evaluations. In this work, we collect a novel dataset of 200 subjects to
evidence the validity of self-rating questionnaires with their corresponding
question-wise video recording. To automatically interpret depression from the
SDS evaluation and the paired video, we propose an end-to-end hierarchical
framework for the long-term variable-length video, which is also conditioned on
the questionnaire results and the answering time. Specifically, we resort to a
hierarchical model which utilizes a 3D CNN for local temporal pattern
exploration and a redundancy-aware self-attention (RAS) scheme for
question-wise global feature aggregation. Targeting for the redundant long-term
FE video processing, our RAS is able to effectively exploit the correlations of
each video clip within a question set to emphasize the discriminative
information and eliminate the redundancy based on feature pair-wise affinity.
Then, the question-wise video feature is concatenated with the questionnaire
scores for final depression detection. Our thorough evaluations also show the
validity of fusing SDS evaluation and its video recording, and the superiority
of our framework to the conventional state-of-the-art temporal modeling
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1"&gt;Wanqing Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1"&gt;Lizhong Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jihong Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Hui Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-supervised Meta-learning with Disentanglement for Domain-generalised Medical Image Segmentation. (arXiv:2106.13292v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13292</id>
        <link href="http://arxiv.org/abs/2106.13292"/>
        <updated>2021-06-28T01:57:54.877Z</updated>
        <summary type="html"><![CDATA[Generalising deep models to new data from new centres (termed here domains)
remains a challenge. This is largely attributed to shifts in data statistics
(domain shifts) between source and unseen domains. Recently, gradient-based
meta-learning approaches where the training data are split into meta-train and
meta-test sets to simulate and handle the domain shifts during training have
shown improved generalisation performance. However, the current fully
supervised meta-learning approaches are not scalable for medical image
segmentation, where large effort is required to create pixel-wise annotations.
Meanwhile, in a low data regime, the simulated domain shifts may not
approximate the true domain shifts well across source and unseen domains. To
address this problem, we propose a novel semi-supervised meta-learning
framework with disentanglement. We explicitly model the representations related
to domain shifts. Disentangling the representations and combining them to
reconstruct the input image allows unlabeled data to be used to better
approximate the true domain shifts for meta-learning. Hence, the model can
achieve better generalisation performance, especially when there is a limited
amount of labeled data. Experiments show that the proposed method is robust on
different segmentation tasks and achieves state-of-the-art generalisation
performance on two public benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thermos_S/0/1/0/all/0/1"&gt;Spyridon Thermos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+ONeil_A/0/1/0/all/0/1"&gt;Alison O&amp;#x27;Neil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1"&gt;Sotirios A. Tsaftaris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Picture May Be Worth a Hundred Words for Visual Question Answering. (arXiv:2106.13445v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13445</id>
        <link href="http://arxiv.org/abs/2106.13445"/>
        <updated>2021-06-28T01:57:54.820Z</updated>
        <summary type="html"><![CDATA[How far can we go with textual representations for understanding pictures? In
image understanding, it is essential to use concise but detailed image
representations. Deep visual features extracted by vision models, such as
Faster R-CNN, are prevailing used in multiple tasks, and especially in visual
question answering (VQA). However, conventional deep visual features may
struggle to convey all the details in an image as we humans do. Meanwhile, with
recent language models' progress, descriptive text may be an alternative to
this problem. This paper delves into the effectiveness of textual
representations for image understanding in the specific context of VQA. We
propose to take description-question pairs as input, instead of deep visual
features, and fed them into a language-only Transformer model, simplifying the
process and the computational cost. We also experiment with data augmentation
techniques to increase the diversity in the training set and avoid learning
statistical bias. Extensive evaluations have shown that textual representations
require only about a hundred words to compete with deep visual features on both
VQA 2.0 and VQA-CP v2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hirota_Y/0/1/0/all/0/1"&gt;Yusuke Hirota&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1"&gt;Noa Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Otani_M/0/1/0/all/0/1"&gt;Mayu Otani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1"&gt;Chenhui Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakashima_Y/0/1/0/all/0/1"&gt;Yuta Nakashima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taniguchi_I/0/1/0/all/0/1"&gt;Ittetsu Taniguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Onoye_T/0/1/0/all/0/1"&gt;Takao Onoye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Building Intelligent Autonomous Navigation Agents. (arXiv:2106.13415v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13415</id>
        <link href="http://arxiv.org/abs/2106.13415"/>
        <updated>2021-06-28T01:57:54.800Z</updated>
        <summary type="html"><![CDATA[Breakthroughs in machine learning in the last decade have led to `digital
intelligence', i.e. machine learning models capable of learning from vast
amounts of labeled data to perform several digital tasks such as speech
recognition, face recognition, machine translation and so on. The goal of this
thesis is to make progress towards designing algorithms capable of `physical
intelligence', i.e. building intelligent autonomous navigation agents capable
of learning to perform complex navigation tasks in the physical world involving
visual perception, natural language understanding, reasoning, planning, and
sequential decision making. Despite several advances in classical navigation
methods in the last few decades, current navigation agents struggle at
long-term semantic navigation tasks. In the first part of the thesis, we
discuss our work on short-term navigation using end-to-end reinforcement
learning to tackle challenges such as obstacle avoidance, semantic perception,
language grounding, and reasoning. In the second part, we present a new class
of navigation methods based on modular learning and structured explicit map
representations, which leverage the strengths of both classical and end-to-end
learning methods, to tackle long-term navigation tasks. We show that these
methods are able to effectively tackle challenges such as localization,
mapping, long-term planning, exploration and learning semantic priors. These
modular learning methods are capable of long-term spatial and semantic
understanding and achieve state-of-the-art results on various navigation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chaplot_D/0/1/0/all/0/1"&gt;Devendra Singh Chaplot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Energy-Based Generative Cooperative Saliency Prediction. (arXiv:2106.13389v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13389</id>
        <link href="http://arxiv.org/abs/2106.13389"/>
        <updated>2021-06-28T01:57:54.791Z</updated>
        <summary type="html"><![CDATA[Conventional saliency prediction models typically learn a deterministic
mapping from images to the corresponding ground truth saliency maps. In this
paper, we study the saliency prediction problem from the perspective of
generative models by learning a conditional probability distribution over
saliency maps given an image, and treating the prediction as a sampling
process. Specifically, we propose a generative cooperative saliency prediction
framework based on the generative cooperative networks, where a conditional
latent variable model and a conditional energy-based model are jointly trained
to predict saliency in a cooperative manner. We call our model the SalCoopNets.
The latent variable model serves as a fast but coarse predictor to efficiently
produce an initial prediction, which is then refined by the iterative Langevin
revision of the energy-based model that serves as a fine predictor. Such a
coarse-to-fine cooperative saliency prediction strategy offers the best of both
worlds. Moreover, we generalize our framework to the scenario of weakly
supervised saliency prediction, where saliency annotation of training images is
partially observed, by proposing a cooperative learning while recovering
strategy. Lastly, we show that the learned energy function can serve as a
refinement module that can refine the results of other pre-trained saliency
prediction models. Experimental results show that our generative model can
achieve state-of-the-art performance. Our code is publicly available at:
\url{https://github.com/JingZhang617/SalCoopNets}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jianwen Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zilong Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1"&gt;Nick Barnes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Object-oriented Spatio-Temporal Reasoning for Video Question Answering. (arXiv:2106.13432v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13432</id>
        <link href="http://arxiv.org/abs/2106.13432"/>
        <updated>2021-06-28T01:57:54.783Z</updated>
        <summary type="html"><![CDATA[Video Question Answering (Video QA) is a powerful testbed to develop new AI
capabilities. This task necessitates learning to reason about objects,
relations, and events across visual and linguistic domains in space-time.
High-level reasoning demands lifting from associative visual pattern
recognition to symbol-like manipulation over objects, their behavior and
interactions. Toward reaching this goal we propose an object-oriented reasoning
approach in that video is abstracted as a dynamic stream of interacting
objects. At each stage of the video event flow, these objects interact with
each other, and their interactions are reasoned about with respect to the query
and under the overall context of a video. This mechanism is materialized into a
family of general-purpose neural units and their multi-level architecture
called Hierarchical Object-oriented Spatio-Temporal Reasoning (HOSTR) networks.
This neural model maintains the objects' consistent lifelines in the form of a
hierarchically nested spatio-temporal graph. Within this graph, the dynamic
interactive object-oriented representations are built up along the video
sequence, hierarchically abstracted in a bottom-up manner, and converge toward
the key information for the correct answer. The method is evaluated on multiple
major Video QA datasets and establishes new state-of-the-arts in these tasks.
Analysis into the model's behavior indicates that object-oriented reasoning is
a reliable, interpretable and efficient approach to Video QA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dang_L/0/1/0/all/0/1"&gt;Long Hoang Dang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1"&gt;Thao Minh Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1"&gt;Vuong Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1"&gt;Truyen Tran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DnS: Distill-and-Select for Efficient and Accurate Video Indexing and Retrieval. (arXiv:2106.13266v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13266</id>
        <link href="http://arxiv.org/abs/2106.13266"/>
        <updated>2021-06-28T01:57:54.776Z</updated>
        <summary type="html"><![CDATA[In this paper, we address the problem of high performance and computationally
efficient content-based video retrieval in large-scale datasets. Current
methods typically propose either: (i) fine-grained approaches employing
spatio-temporal representations and similarity calculations, achieving high
performance at a high computational cost or (ii) coarse-grained approaches
representing/indexing videos as global vectors, where the spatio-temporal
structure is lost, providing low performance but also having low computational
cost. In this work, we propose a Knowledge Distillation framework, which we
call Distill-and-Select (DnS), that starting from a well-performing
fine-grained Teacher Network learns: a) Student Networks at different retrieval
performance and computational efficiency trade-offs and b) a Selection Network
that at test time rapidly directs samples to the appropriate student to
maintain both high retrieval performance and high computational efficiency. We
train several students with different architectures and arrive at different
trade-offs of performance and efficiency, i.e., speed and storage requirements,
including fine-grained students that store index videos using binary
representations. Importantly, the proposed scheme allows Knowledge Distillation
in large, unlabelled datasets -- this leads to good students. We evaluate DnS
on five public datasets on three different video retrieval tasks and
demonstrate a) that our students achieve state-of-the-art performance in
several cases and b) that our DnS framework provides an excellent trade-off
between retrieval performance, computational speed, and storage space. In
specific configurations, our method achieves similar mAP with the teacher but
is 20 times faster and requires 240 times less storage space. Our collected
dataset and implementation are publicly available:
https://github.com/mever-team/distill-and-select.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kordopatis_Zilos_G/0/1/0/all/0/1"&gt;Giorgos Kordopatis-Zilos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tzelepis_C/0/1/0/all/0/1"&gt;Christos Tzelepis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1"&gt;Symeon Papadopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kompatsiaris_I/0/1/0/all/0/1"&gt;Ioannis Kompatsiaris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patras_I/0/1/0/all/0/1"&gt;Ioannis Patras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized One-Class Learning Using Pairs of Complementary Classifiers. (arXiv:2106.13272v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13272</id>
        <link href="http://arxiv.org/abs/2106.13272"/>
        <updated>2021-06-28T01:57:54.764Z</updated>
        <summary type="html"><![CDATA[One-class learning is the classic problem of fitting a model to the data for
which annotations are available only for a single class. In this paper, we
explore novel objectives for one-class learning, which we collectively refer to
as Generalized One-class Discriminative Subspaces (GODS). Our key idea is to
learn a pair of complementary classifiers to flexibly bound the one-class data
distribution, where the data belongs to the positive half-space of one of the
classifiers in the complementary pair and to the negative half-space of the
other. To avoid redundancy while allowing non-linearity in the classifier
decision surfaces, we propose to design each classifier as an orthonormal frame
and seek to learn these frames via jointly optimizing for two conflicting
objectives, namely: i) to minimize the distance between the two frames, and ii)
to maximize the margin between the frames and the data. The learned orthonormal
frames will thus characterize a piecewise linear decision surface that allows
for efficient inference, while our objectives seek to bound the data within a
minimal volume that maximizes the decision margin, thereby robustly capturing
the data distribution. We explore several variants of our formulation under
different constraints on the constituent classifiers, including kernelized
feature maps. We demonstrate the empirical benefits of our approach via
experiments on data from several applications in computer vision, such as
anomaly detection in video sequences, human poses, and human activities. We
also explore the generality and effectiveness of GODS for non-vision tasks via
experiments on several UCI datasets, demonstrating state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cherian_A/0/1/0/all/0/1"&gt;Anoop Cherian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jue Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Summarization for Chat Logs with Topic-Oriented Ranking and Context-Aware Auto-Encoders. (arXiv:2012.07300v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07300</id>
        <link href="http://arxiv.org/abs/2012.07300"/>
        <updated>2021-06-28T01:57:54.755Z</updated>
        <summary type="html"><![CDATA[Automatic chat summarization can help people quickly grasp important
information from numerous chat messages. Unlike conventional documents, chat
logs usually have fragmented and evolving topics. In addition, these logs
contain a quantity of elliptical and interrogative sentences, which make the
chat summarization highly context dependent. In this work, we propose a novel
unsupervised framework called RankAE to perform chat summarization without
employing manually labeled data. RankAE consists of a topic-oriented ranking
strategy that selects topic utterances according to centrality and diversity
simultaneously, as well as a denoising auto-encoder that is carefully designed
to generate succinct but context-informative summaries based on the selected
utterances. To evaluate the proposed method, we collect a large-scale dataset
of chat logs from a customer service environment and build an annotated set
only for model evaluation. Experimental results show that RankAE significantly
outperforms other unsupervised methods and is able to generate high-quality
summaries in terms of relevance and topic coverage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yicheng Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jun Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Lujun Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1"&gt;Yangyang Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhuoren Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changlong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xuanjing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaozhong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Exploratory Analysis of the Relation Between Offensive Language and Mental Health. (arXiv:2105.14888v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14888</id>
        <link href="http://arxiv.org/abs/2105.14888"/>
        <updated>2021-06-28T01:57:54.735Z</updated>
        <summary type="html"><![CDATA[In this paper, we analyze the interplay between the use of offensive language
and mental health. We acquired publicly available datasets created for
offensive language identification and depression detection and we train
computational models to compare the use of offensive language in social media
posts written by groups of individuals with and without self-reported
depression diagnosis. We also look at samples written by groups of individuals
whose posts show signs of depression according to recent related studies. Our
analysis indicates that offensive language is more frequently used in the
samples written by individuals with self-reported depression as well as
individuals showing signs of depression. The results discussed here open new
avenues in research in politeness/offensiveness and mental health.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bucur_A/0/1/0/all/0/1"&gt;Ana-Maria Bucur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1"&gt;Marcos Zampieri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dinu_L/0/1/0/all/0/1"&gt;Liviu P. Dinu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STYLER: Style Factor Modeling with Rapidity and Robustness via Speech Decomposition for Expressive and Controllable Neural Text to Speech. (arXiv:2103.09474v4 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09474</id>
        <link href="http://arxiv.org/abs/2103.09474"/>
        <updated>2021-06-28T01:57:54.702Z</updated>
        <summary type="html"><![CDATA[Previous works on neural text-to-speech (TTS) have been addressed on limited
speed in training and inference time, robustness for difficult synthesis
conditions, expressiveness, and controllability. Although several approaches
resolve some limitations, there has been no attempt to solve all weaknesses at
once. In this paper, we propose STYLER, an expressive and controllable TTS
framework with high-speed and robust synthesis. Our novel audio-text aligning
method called Mel Calibrator and excluding autoregressive decoding enable rapid
training and inference and robust synthesis on unseen data. Also, disentangled
style factor modeling under supervision enlarges the controllability in
synthesizing process leading to expressive TTS. On top of it, a novel noise
modeling pipeline using domain adversarial training and Residual Decoding
empowers noise-robust style transfer, decomposing the noise without any
additional label. Various experiments demonstrate that STYLER is more effective
in speed and robustness than expressive TTS with autoregressive decoding and
more expressive and controllable than reading style non-autoregressive TTS.
Synthesis samples and experiment results are provided via our demo page, and
code is available publicly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lee_K/0/1/0/all/0/1"&gt;Keon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Park_K/0/1/0/all/0/1"&gt;Kyumin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_D/0/1/0/all/0/1"&gt;Daeyoung Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Emergent Fake News Detection via Meta Neural Process Networks. (arXiv:2106.13711v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.13711</id>
        <link href="http://arxiv.org/abs/2106.13711"/>
        <updated>2021-06-28T01:57:54.687Z</updated>
        <summary type="html"><![CDATA[Fake news travels at unprecedented speeds, reaches global audiences and puts
users and communities at great risk via social media platforms. Deep learning
based models show good performance when trained on large amounts of labeled
data on events of interest, whereas the performance of models tends to degrade
on other events due to domain shift. Therefore, significant challenges are
posed for existing detection approaches to detect fake news on emergent events,
where large-scale labeled datasets are difficult to obtain. Moreover, adding
the knowledge from newly emergent events requires to build a new model from
scratch or continue to fine-tune the model, which can be challenging,
expensive, and unrealistic for real-world settings. In order to address those
challenges, we propose an end-to-end fake news detection framework named
MetaFEND, which is able to learn quickly to detect fake news on emergent events
with a few verified posts. Specifically, the proposed model integrates
meta-learning and neural process methods together to enjoy the benefits of
these approaches. In particular, a label embedding module and a hard attention
mechanism are proposed to enhance the effectiveness by handling categorical
information and trimming irrelevant posts. Extensive experiments are conducted
on multimedia datasets collected from Twitter and Weibo. The experimental
results show our proposed MetaFEND model can detect fake news on never-seen
events effectively and outperform the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yaqing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1"&gt;Fenglong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haoyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_K/0/1/0/all/0/1"&gt;Kishlay Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jing Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privileged Zero-Shot AutoML. (arXiv:2106.13743v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13743</id>
        <link href="http://arxiv.org/abs/2106.13743"/>
        <updated>2021-06-28T01:57:54.647Z</updated>
        <summary type="html"><![CDATA[This work improves the quality of automated machine learning (AutoML) systems
by using dataset and function descriptions while significantly decreasing
computation time from minutes to milliseconds by using a zero-shot approach.
Given a new dataset and a well-defined machine learning task, humans begin by
reading a description of the dataset and documentation for the algorithms to be
used. This work is the first to use these textual descriptions, which we call
privileged information, for AutoML. We use a pre-trained Transformer model to
process the privileged text and demonstrate that using this information
improves AutoML performance. Thus, our approach leverages the progress of
unsupervised representation learning in natural language processing to provide
a significant boost to AutoML. We demonstrate that using only textual
descriptions of the data and functions achieves reasonable classification
performance, and adding textual descriptions to data meta-features improves
classification across tabular datasets. To achieve zero-shot AutoML we train a
graph neural network with these description embeddings and the data
meta-features. Each node represents a training dataset, which we use to predict
the best machine learning pipeline for a new test dataset in a zero-shot
fashion. Our zero-shot approach rapidly predicts a high-quality pipeline for a
supervised learning task and dataset. In contrast, most AutoML systems require
tens or hundreds of pipeline evaluations. We show that zero-shot AutoML reduces
running and prediction times from minutes to milliseconds, consistently across
datasets. By speeding up AutoML by orders of magnitude this work demonstrates
real-time AutoML.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1"&gt;Nikhil Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kates_B/0/1/0/all/0/1"&gt;Brandon Kates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mentch_J/0/1/0/all/0/1"&gt;Jeff Mentch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kharkar_A/0/1/0/all/0/1"&gt;Anant Kharkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Udell_M/0/1/0/all/0/1"&gt;Madeleine Udell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1"&gt;Iddo Drori&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Free-viewpoint Indoor Neural Relighting from Multi-view Stereo. (arXiv:2106.13299v1 [cs.GR])]]></title>
        <id>http://arxiv.org/abs/2106.13299</id>
        <link href="http://arxiv.org/abs/2106.13299"/>
        <updated>2021-06-28T01:57:54.640Z</updated>
        <summary type="html"><![CDATA[We introduce a neural relighting algorithm for captured indoors scenes, that
allows interactive free-viewpoint navigation. Our method allows illumination to
be changed synthetically, while coherently rendering cast shadows and complex
glossy materials. We start with multiple images of the scene and a 3D mesh
obtained by multi-view stereo (MVS) reconstruction. We assume that lighting is
well-explained as the sum of a view-independent diffuse component and a
view-dependent glossy term concentrated around the mirror reflection direction.
We design a convolutional network around input feature maps that facilitate
learning of an implicit representation of scene materials and illumination,
enabling both relighting and free-viewpoint navigation. We generate these input
maps by exploiting the best elements of both image-based and physically-based
rendering. We sample the input views to estimate diffuse scene irradiance, and
compute the new illumination caused by user-specified light sources using path
tracing. To facilitate the network's understanding of materials and synthesize
plausible glossy reflections, we reproject the views and compute mirror images.
We train the network on a synthetic dataset where each scene is also
reconstructed with MVS. We show results of our algorithm relighting real indoor
scenes and performing free-viewpoint navigation with complex and realistic
glossy reflections, which so far remained out of reach for view-synthesis
techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Philip_J/0/1/0/all/0/1"&gt;Julien Philip&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morgenthaler_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Morgenthaler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gharbi_M/0/1/0/all/0/1"&gt;Micha&amp;#xeb;l Gharbi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drettakis_G/0/1/0/all/0/1"&gt;George Drettakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extended Parallel Corpus for Amharic-English Machine Translation. (arXiv:2104.03543v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03543</id>
        <link href="http://arxiv.org/abs/2104.03543"/>
        <updated>2021-06-28T01:57:54.598Z</updated>
        <summary type="html"><![CDATA[This paper describes the acquisition, preprocessing, segmentation, and
alignment of an Amharic-English parallel corpus. It will be useful for machine
translation of an under-resourced language, Amharic. The corpus is larger than
previously compiled corpora; it is released for research purposes. We trained
neural machine translation and phrase-based statistical machine translation
models using the corpus. In the automatic evaluation, neural machine
translation models outperform phrase-based statistical machine translation
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gezmu_A/0/1/0/all/0/1"&gt;Andargachew Mekonnen Gezmu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nurnberger_A/0/1/0/all/0/1"&gt;Andreas N&amp;#xfc;rnberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bati_T/0/1/0/all/0/1"&gt;Tesfaye Bayu Bati&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Noisy Client Learning. (arXiv:2106.13239v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13239</id>
        <link href="http://arxiv.org/abs/2106.13239"/>
        <updated>2021-06-28T01:57:54.589Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) collaboratively aggregates a shared global model
depending on multiple local clients, while keeping the training data
decentralized in order to preserve data privacy. However, standard FL methods
ignore the noisy client issue, which may harm the overall performance of the
aggregated model. In this paper, we first analyze the noisy client statement,
and then model noisy clients with different noise distributions (e.g.,
Bernoulli and truncated Gaussian distributions). To learn with noisy clients,
we propose a simple yet effective FL framework, named Federated Noisy Client
Learning (Fed-NCL), which is a plug-and-play algorithm and contains two main
components: a data quality measurement (DQM) to dynamically quantify the data
quality of each participating client, and a noise robust aggregation (NRA) to
adaptively aggregate the local models of each client by jointly considering the
amount of local training data and the data quality of each client. Our Fed-NCL
can be easily applied in any standard FL workflow to handle the noisy client
issue. Experimental results on various datasets demonstrate that our algorithm
boosts the performances of different state-of-the-art systems with noisy
clients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Li Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1"&gt;Huazhu Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Cheng-Zhong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TTS-Portuguese Corpus: a corpus for speech synthesis in Brazilian Portuguese. (arXiv:2005.05144v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.05144</id>
        <link href="http://arxiv.org/abs/2005.05144"/>
        <updated>2021-06-28T01:57:54.581Z</updated>
        <summary type="html"><![CDATA[Speech provides a natural way for human-computer interaction. In particular,
speech synthesis systems are popular in different applications, such as
personal assistants, GPS applications, screen readers and accessibility tools.
However, not all languages are on the same level when in terms of resources and
systems for speech synthesis. This work consists of creating publicly available
resources for Brazilian Portuguese in the form of a novel dataset along with
deep learning models for end-to-end speech synthesis. Such dataset has 10.5
hours from a single speaker, from which a Tacotron 2 model with the RTISI-LA
vocoder presented the best performance, achieving a 4.03 MOS value. The
obtained results are comparable to related works covering English language and
the state-of-the-art in Portuguese.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Casanova_E/0/1/0/all/0/1"&gt;Edresson Casanova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Junior_A/0/1/0/all/0/1"&gt;Arnaldo Candido Junior&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shulby_C/0/1/0/all/0/1"&gt;Christopher Shulby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oliveira_F/0/1/0/all/0/1"&gt;Frederico Santos de Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Teixeira_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Paulo Teixeira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ponti_M/0/1/0/all/0/1"&gt;Moacir Antonelli Ponti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Aluisio_S/0/1/0/all/0/1"&gt;Sandra Maria Aluisio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing. (arXiv:2007.15779v5 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.15779</id>
        <link href="http://arxiv.org/abs/2007.15779"/>
        <updated>2021-06-28T01:57:54.572Z</updated>
        <summary type="html"><![CDATA[Pretraining large neural language models, such as BERT, has led to impressive
gains on many natural language processing (NLP) tasks. However, most
pretraining efforts focus on general domain corpora, such as newswire and Web.
A prevailing assumption is that even domain-specific pretraining can benefit by
starting from general-domain language models. In this paper, we challenge this
assumption by showing that for domains with abundant unlabeled text, such as
biomedicine, pretraining language models from scratch results in substantial
gains over continual pretraining of general-domain language models. To
facilitate this investigation, we compile a comprehensive biomedical NLP
benchmark from publicly-available datasets. Our experiments show that
domain-specific pretraining serves as a solid foundation for a wide range of
biomedical NLP tasks, leading to new state-of-the-art results across the board.
Further, in conducting a thorough evaluation of modeling choices, both for
pretraining and task-specific fine-tuning, we discover that some common
practices are unnecessary with BERT models, such as using complex tagging
schemes in named entity recognition (NER). To help accelerate research in
biomedical NLP, we have released our state-of-the-art pretrained and
task-specific models for the community, and created a leaderboard featuring our
BLURB benchmark (short for Biomedical Language Understanding & Reasoning
Benchmark) at https://aka.ms/BLURB.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yu Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tinn_R/0/1/0/all/0/1"&gt;Robert Tinn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Hao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lucas_M/0/1/0/all/0/1"&gt;Michael Lucas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1"&gt;Naoto Usuyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaodong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1"&gt;Tristan Naumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1"&gt;Hoifung Poon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring the Representation of Word Meanings in Context: A Case Study on Homonymy and Synonymy. (arXiv:2106.13553v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13553</id>
        <link href="http://arxiv.org/abs/2106.13553"/>
        <updated>2021-06-28T01:57:54.559Z</updated>
        <summary type="html"><![CDATA[This paper presents a multilingual study of word meaning representations in
context. We assess the ability of both static and contextualized models to
adequately represent different lexical-semantic relations, such as homonymy and
synonymy. To do so, we created a new multilingual dataset that allows us to
perform a controlled evaluation of several factors such as the impact of the
surrounding context or the overlap between words, conveying the same or
different senses. A systematic assessment on four scenarios shows that the best
monolingual models based on Transformers can adequately disambiguate homonyms
in context. However, as they rely heavily on context, these models fail at
representing words with different senses when occurring in similar sentences.
Experiments are performed in Galician, Portuguese, English, and Spanish, and
both the dataset (with more than 3,000 evaluation items) and new models are
freely released with this study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_M/0/1/0/all/0/1"&gt;Marcos Garcia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation. (arXiv:2006.10369v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.10369</id>
        <link href="http://arxiv.org/abs/2006.10369"/>
        <updated>2021-06-28T01:57:54.517Z</updated>
        <summary type="html"><![CDATA[Much recent effort has been invested in non-autoregressive neural machine
translation, which appears to be an efficient alternative to state-of-the-art
autoregressive machine translation on modern GPUs. In contrast to the latter,
where generation is sequential, the former allows generation to be parallelized
across target token positions. Some of the latest non-autoregressive models
have achieved impressive translation quality-speed tradeoffs compared to
autoregressive baselines. In this work, we reexamine this tradeoff and argue
that autoregressive baselines can be substantially sped up without loss in
accuracy. Specifically, we study autoregressive models with encoders and
decoders of varied depths. Our extensive experiments show that given a
sufficiently deep encoder, a single-layer autoregressive decoder can
substantially outperform strong non-autoregressive models with comparable
inference speed. We show that the speed disadvantage for autoregressive
baselines compared to non-autoregressive methods has been overestimated in
three aspects: suboptimal layer allocation, insufficient speed measurement, and
lack of knowledge distillation. Our results establish a new protocol for future
research toward fast, accurate machine translation. Our code is available at
https://github.com/jungokasai/deep-shallow.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1"&gt;Jungo Kasai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pappas_N/0/1/0/all/0/1"&gt;Nikolaos Pappas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Hao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cross_J/0/1/0/all/0/1"&gt;James Cross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1"&gt;Noah A. Smith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Masked Proxy Loss For Text-Independent Speaker Verification. (arXiv:2011.04491v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04491</id>
        <link href="http://arxiv.org/abs/2011.04491"/>
        <updated>2021-06-28T01:57:54.488Z</updated>
        <summary type="html"><![CDATA[Open-set speaker recognition can be regarded as a metric learning problem,
which is to maximize inter-class variance and minimize intra-class variance.
Supervised metric learning can be categorized into entity-based learning and
proxy-based learning. Most of the existing metric learning objectives like
Contrastive, Triplet, Prototypical, GE2E, etc all belong to the former
division, the performance of which is either highly dependent on sample mining
strategy or restricted by insufficient label information in the mini-batch.
Proxy-based losses mitigate both shortcomings, however, fine-grained
connections among entities are either not or indirectly leveraged. This paper
proposes a Masked Proxy (MP) loss which directly incorporates both proxy-based
relationships and pair-based relationships. We further propose Multinomial
Masked Proxy (MMP) loss to leverage the hardness of speaker pairs. These
methods have been applied to evaluate on VoxCeleb test set and reach
state-of-the-art Equal Error Rate(EER).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lian_J/0/1/0/all/0/1"&gt;Jiachen Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Aiswarya Vinod Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhamyal_H/0/1/0/all/0/1"&gt;Hira Dhamyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1"&gt;Bhiksha Raj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rita Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Sample Replacements for ELECTRA Pre-Training. (arXiv:2106.13715v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13715</id>
        <link href="http://arxiv.org/abs/2106.13715"/>
        <updated>2021-06-28T01:57:54.421Z</updated>
        <summary type="html"><![CDATA[ELECTRA pretrains a discriminator to detect replaced tokens, where the
replacements are sampled from a generator trained with masked language
modeling. Despite the compelling performance, ELECTRA suffers from the
following two issues. First, there is no direct feedback loop from
discriminator to generator, which renders replacement sampling inefficient.
Second, the generator's prediction tends to be over-confident along with
training, making replacements biased to correct tokens. In this paper, we
propose two methods to improve replacement sampling for ELECTRA pre-training.
Specifically, we augment sampling with a hardness prediction mechanism, so that
the generator can encourage the discriminator to learn what it has not
acquired. We also prove that efficient sampling reduces the training variance
of the discriminator. Moreover, we propose to use a focal loss for the
generator in order to relieve oversampling of correct tokens as replacements.
Experimental results show that our method improves ELECTRA pre-training on
various downstream tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1"&gt;Yaru Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1"&gt;Li Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1"&gt;Hangbo Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Ke Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Furu Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sentiment Progression based Searching and Indexing of Literary Textual Artefacts. (arXiv:2106.13767v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.13767</id>
        <link href="http://arxiv.org/abs/2106.13767"/>
        <updated>2021-06-28T01:57:54.388Z</updated>
        <summary type="html"><![CDATA[Literary artefacts are generally indexed and searched based on titles, meta
data and keywords over the years. This searching and indexing works well when
user/reader already knows about that particular creative textual artefact or
document. This indexing and search hardly takes into account interest and
emotional makeup of readers and its mapping to books. When a person is looking
for a literary textual artefact, he/she might be looking for not only
information but also to seek the joy of reading. In case of literary artefacts,
progression of emotions across the key events could prove to be the key for
indexing and searching. In this paper, we establish clusters among literary
artefacts based on computational relationships among sentiment progressions
using intelligent text analysis. We have created a database of 1076 English
titles + 20 Marathi titles and also used database
this http URL with 16559 titles and their
summaries. We have proposed Sentiment Progression based Indexing for searching
and recommending books. This can be used to create personalized clusters of
book titles of interest to readers. The analysis clearly suggests better
searching and indexing when we are targeting book lovers looking for a
particular type of book or creative artefact. This indexing and searching can
find many real-life applications for recommending books.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_H/0/1/0/all/0/1"&gt;Hrishikesh Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alicea_B/0/1/0/all/0/1"&gt;Bradly Alicea&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Sparse Expert Models and Beyond. (arXiv:2105.15082v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15082</id>
        <link href="http://arxiv.org/abs/2105.15082"/>
        <updated>2021-06-28T01:57:54.318Z</updated>
        <summary type="html"><![CDATA[Mixture-of-Experts (MoE) models can achieve promising results with outrageous
large amount of parameters but constant computation cost, and thus it has
become a trend in model scaling. Still it is a mystery how MoE layers bring
quality gains by leveraging the parameters with sparse activation. In this
work, we investigate several key factors in sparse expert models. We observe
that load imbalance may not be a significant problem affecting model quality,
contrary to the perspectives of recent studies, while the number of sparsely
activated experts $k$ and expert capacity $C$ in top-$k$ routing can
significantly make a difference in this context. Furthermore, we take a step
forward to propose a simple method called expert prototyping that splits
experts into different prototypes and applies $k$ top-$1$ routing. This
strategy improves the model quality but maintains constant computational costs,
and our further exploration on extremely large-scale models reflects that it is
more effective in training larger models. We push the model scale to over $1$
trillion parameters and implement it on solely $480$ NVIDIA V100-32GB GPUs, in
comparison with the recent SOTAs on $2048$ TPU cores. The proposed giant model
achieves substantial speedup in convergence over the same-size baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1"&gt;An Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Junyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1"&gt;Rui Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Le Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xianyan Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1"&gt;Ang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiamang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Di Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1"&gt;Lin Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Models are Good Translators. (arXiv:2106.13627v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13627</id>
        <link href="http://arxiv.org/abs/2106.13627"/>
        <updated>2021-06-28T01:57:54.292Z</updated>
        <summary type="html"><![CDATA[Recent years have witnessed the rapid advance in neural machine translation
(NMT), the core of which lies in the encoder-decoder architecture. Inspired by
the recent progress of large-scale pre-trained language models on machine
translation in a limited scenario, we firstly demonstrate that a single
language model (LM4MT) can achieve comparable performance with strong
encoder-decoder NMT models on standard machine translation benchmarks, using
the same training data and similar amount of model parameters. LM4MT can also
easily utilize source-side texts as additional supervision. Though modeling the
source- and target-language texts with the same mechanism, LM4MT can provide
unified representations for both source and target sentences, which can better
transfer knowledge across languages. Extensive experiments on pivot-based and
zero-shot translation tasks show that LM4MT can outperform the encoder-decoder
NMT model by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1"&gt;Zhaopeng Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1"&gt;Zhixing Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenxuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topic-Oriented Spoken Dialogue Summarization for Customer Service with Saliency-Aware Topic Modeling. (arXiv:2012.07311v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07311</id>
        <link href="http://arxiv.org/abs/2012.07311"/>
        <updated>2021-06-28T01:57:54.260Z</updated>
        <summary type="html"><![CDATA[In a customer service system, dialogue summarization can boost service
efficiency by automatically creating summaries for long spoken dialogues in
which customers and agents try to address issues about specific topics. In this
work, we focus on topic-oriented dialogue summarization, which generates highly
abstractive summaries that preserve the main ideas from dialogues. In spoken
dialogues, abundant dialogue noise and common semantics could obscure the
underlying informative content, making the general topic modeling approaches
difficult to apply. In addition, for customer service, role-specific
information matters and is an indispensable part of a summary. To effectively
perform topic modeling on dialogues and capture multi-role information, in this
work we propose a novel topic-augmented two-stage dialogue summarizer (TDS)
jointly with a saliency-aware neural topic model (SATM) for topic-oriented
summarization of customer service dialogues. Comprehensive studies on a
real-world Chinese customer service dataset demonstrated the superiority of our
method against several strong baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yicheng Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Lujun Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1"&gt;Yangyang Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jun Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_M/0/1/0/all/0/1"&gt;Minlong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhuoren Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changlong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xuanjing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaozhong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeltaLM: Encoder-Decoder Pre-training for Language Generation and Translation by Augmenting Pretrained Multilingual Encoders. (arXiv:2106.13736v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13736</id>
        <link href="http://arxiv.org/abs/2106.13736"/>
        <updated>2021-06-28T01:57:54.252Z</updated>
        <summary type="html"><![CDATA[While pretrained encoders have achieved success in various natural language
understanding (NLU) tasks, there is a gap between these pretrained encoders and
natural language generation (NLG). NLG tasks are often based on the
encoder-decoder framework, where the pretrained encoders can only benefit part
of it. To reduce this gap, we introduce DeltaLM, a pretrained multilingual
encoder-decoder model that regards the decoder as the task layer of
off-the-shelf pretrained encoders. Specifically, we augment the pretrained
multilingual encoder with a decoder and pre-train it in a self-supervised way.
To take advantage of both the large-scale monolingual data and bilingual data,
we adopt the span corruption and translation span corruption as the
pre-training tasks. Experiments show that DeltaLM outperforms various strong
baselines on both natural language generation and translation tasks, including
machine translation, abstractive text summarization, data-to-text, and question
generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shuming Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1"&gt;Li Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shaohan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongdong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muzio_A/0/1/0/all/0/1"&gt;Alexandre Muzio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1"&gt;Saksham Singhal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awadalla_H/0/1/0/all/0/1"&gt;Hany Hassan Awadalla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xia Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Furu Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to marry a star: probabilistic constraints for meaning in context. (arXiv:2009.07936v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.07936</id>
        <link href="http://arxiv.org/abs/2009.07936"/>
        <updated>2021-06-28T01:57:54.245Z</updated>
        <summary type="html"><![CDATA[In this paper, we derive a notion of 'word meaning in context' which accounts
for the wide range of lexical shifts and ambiguities observed in utterance
interpretation. We characterize the lexical comprehension process as a
combination of cognitive semantics and Discourse Representation Theory,
formalized as a 'situation description system': a probabilistic model which
takes utterance understanding to be the mental process of describing one or
more situations that would account for an observed utterance. Our model uses
insights from different types of generative models to capture the interplay of
local and global contexts and their joint influence upon the lexical
representation of sentence constituents. We implement the system using a
directed graphical model, and apply it to examples containing various
contextualisation phenomena.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Erk_K/0/1/0/all/0/1"&gt;Katrin Erk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herbelot_A/0/1/0/all/0/1"&gt;Aurelie Herbelot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation. (arXiv:2010.10907v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10907</id>
        <link href="http://arxiv.org/abs/2010.10907"/>
        <updated>2021-06-28T01:57:54.237Z</updated>
        <summary type="html"><![CDATA[In Neural Machine Translation (and, more generally, conditional language
modeling), the generation of a target token is influenced by two types of
context: the source and the prefix of the target sequence. While many attempts
to understand the internal workings of NMT models have been made, none of them
explicitly evaluates relative source and target contributions to a generation
decision. We argue that this relative contribution can be evaluated by adopting
a variant of Layerwise Relevance Propagation (LRP). Its underlying
'conservation principle' makes relevance propagation unique: differently from
other methods, it evaluates not an abstract quantity reflecting token
importance, but the proportion of each token's influence. We extend LRP to the
Transformer and conduct an analysis of NMT models which explicitly evaluates
the source and target relative contributions to the generation process. We
analyze changes in these contributions when conditioning on different types of
prefixes, when varying the training objective or the amount of training data,
and during the training process. We find that models trained with more data
tend to rely on source information more and to have more sharp token
contributions; the training process is non-monotonic with several stages of
different nature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Voita_E/0/1/0/all/0/1"&gt;Elena Voita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1"&gt;Rico Sennrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1"&gt;Ivan Titov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Task Effects in Human Reading with Neural Attention. (arXiv:1808.00054v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1808.00054</id>
        <link href="http://arxiv.org/abs/1808.00054"/>
        <updated>2021-06-28T01:57:54.194Z</updated>
        <summary type="html"><![CDATA[Humans read by making a sequence of fixations and saccades. They often skip
words, without apparent detriment to understanding. We offer a novel
explanation for skipping: readers optimize a tradeoff between performing a
language-related task and fixating as few words as possible. We propose a
neural architecture that combines an attention module (deciding whether to skip
words) and a task module (memorizing the input). We show that our model
predicts human skipping behavior, while also modeling reading times well, even
though it skips 40% of the input. A key prediction of our model is that
different reading tasks should result in different skipping behaviors. We
confirm this prediction in an eye-tracking experiment in which participants
answers questions about a text. We are able to capture these experimental
results using the our model, replacing the memorization module with a task
module that performs neural question answering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hahn_M/0/1/0/all/0/1"&gt;Michael Hahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1"&gt;Frank Keller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-grained Geolocation Prediction of Tweets with Human Machine Collaboration. (arXiv:2106.13411v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13411</id>
        <link href="http://arxiv.org/abs/2106.13411"/>
        <updated>2021-06-28T01:57:54.127Z</updated>
        <summary type="html"><![CDATA[Twitter is a useful resource to analyze peoples' opinions on various topics.
Often these topics are correlated or associated with locations from where these
Tweet posts are made. For example, restaurant owners may need to know where
their target customers eat with respect to the sentiment of the posts made
related to food, policy planners may need to analyze citizens' opinion on
relevant issues such as crime, safety, congestion, etc. with respect to
specific parts of the city, or county or state. As promising as this is, less
than $1\%$ of the crawled Tweet posts come with geolocation tags. That makes
accurate prediction of Tweet posts for the non geo-tagged tweets very critical
to analyze data in various domains. In this research, we utilized millions of
Twitter posts and end-users domain expertise to build a set of deep neural
network models using natural language processing (NLP) techniques, that
predicts the geolocation of non geo-tagged Tweet posts at various level of
granularities such as neighborhood, zipcode, and longitude with latitudes. With
multiple neural architecture experiments, and a collaborative human-machine
workflow design, our ongoing work on geolocation detection shows promising
results that empower end-users to correlate relationship between variables of
choice with the location information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dutt_F/0/1/0/all/0/1"&gt;Florina Dutt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1"&gt;Subhajit Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manually Annotated Spelling Error Corpus for Amharic. (arXiv:2106.13521v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13521</id>
        <link href="http://arxiv.org/abs/2106.13521"/>
        <updated>2021-06-28T01:57:54.098Z</updated>
        <summary type="html"><![CDATA[This paper presents a manually annotated spelling error corpus for Amharic,
lingua franca in Ethiopia. The corpus is designed to be used for the evaluation
of spelling error detection and correction. The misspellings are tagged as
non-word and real-word errors. In addition, the contextual information
available in the corpus makes it useful in dealing with both types of spelling
errors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gezmu_A/0/1/0/all/0/1"&gt;Andargachew Mekonnen Gezmu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lema_T/0/1/0/all/0/1"&gt;Tirufat Tesifaye Lema&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seyoum_B/0/1/0/all/0/1"&gt;Binyam Ephrem Seyoum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nurnberger_A/0/1/0/all/0/1"&gt;Andreas N&amp;#xfc;rnberger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Preliminary study on using vector quantization latent spaces for TTS/VC systems with consistent performance. (arXiv:2106.13479v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.13479</id>
        <link href="http://arxiv.org/abs/2106.13479"/>
        <updated>2021-06-28T01:57:54.043Z</updated>
        <summary type="html"><![CDATA[Generally speaking, the main objective when training a neural speech
synthesis system is to synthesize natural and expressive speech from the output
layer of the neural network without much attention given to the hidden layers.
However, by learning useful latent representation, the system can be used for
many more practical scenarios. In this paper, we investigate the use of
quantized vectors to model the latent linguistic embedding and compare it with
the continuous counterpart. By enforcing different policies over the latent
spaces in the training, we are able to obtain a latent linguistic embedding
that takes on different properties while having a similar performance in terms
of quality and speaker similarity. Our experiments show that the voice cloning
system built with vector quantization has only a small degradation in terms of
perceptive evaluations, but has a discrete latent space that is useful for
reducing the representation bit-rate, which is desirable for data transferring,
or limiting the information leaking, which is important for speaker
anonymization and other tasks of that nature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luong_H/0/1/0/all/0/1"&gt;Hieu-Thi Luong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamagishi_J/0/1/0/all/0/1"&gt;Junichi Yamagishi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ParaLaw Nets -- Cross-lingual Sentence-level Pretraining for Legal Text Processing. (arXiv:2106.13403v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13403</id>
        <link href="http://arxiv.org/abs/2106.13403"/>
        <updated>2021-06-28T01:57:54.010Z</updated>
        <summary type="html"><![CDATA[Ambiguity is a characteristic of natural language, which makes expression
ideas flexible. However, in a domain that requires accurate statements, it
becomes a barrier. Specifically, a single word can have many meanings and
multiple words can have the same meaning. When translating a text into a
foreign language, the translator needs to determine the exact meaning of each
element in the original sentence to produce the correct translation sentence.
From that observation, in this paper, we propose ParaLaw Nets, a pretrained
model family using sentence-level cross-lingual information to reduce ambiguity
and increase the performance in legal text processing. This approach achieved
the best result in the Question Answering task of COLIEE-2021.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Ha-Thanh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1"&gt;Vu Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1"&gt;Phuong Minh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vuong_T/0/1/0/all/0/1"&gt;Thi-Hai-Yen Vuong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_Q/0/1/0/all/0/1"&gt;Quan Minh Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1"&gt;Chau Minh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dang_B/0/1/0/all/0/1"&gt;Binh Tran Dang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1"&gt;Minh Le Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Satoh_K/0/1/0/all/0/1"&gt;Ken Satoh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models. (arXiv:2106.13353v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13353</id>
        <link href="http://arxiv.org/abs/2106.13353"/>
        <updated>2021-06-28T01:57:54.004Z</updated>
        <summary type="html"><![CDATA[Prompting language models (LMs) with training examples and task descriptions
has been seen as critical to recent successes in few-shot learning. In this
work, we show that finetuning LMs in the few-shot setting can considerably
reduce the need for prompt engineering. In fact, one can use null prompts,
prompts that contain neither task-specific templates nor training examples, and
achieve competitive accuracy to manually-tuned prompts across a wide range of
tasks. While finetuning LMs does introduce new parameters for each downstream
task, we show that this memory overhead can be substantially reduced:
finetuning only the bias terms can achieve comparable or better accuracy than
standard finetuning while only updating 0.1% of the parameters. All in all, we
recommend finetuning LMs for few-shot learning as it is more accurate, robust
to different prompts, and can be made nearly as efficient as using frozen LMs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Logan_R/0/1/0/all/0/1"&gt;Robert L. Logan IV&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balazevic_I/0/1/0/all/0/1"&gt;Ivana Bala&amp;#x17e;evi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1"&gt;Eric Wallace&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petroni_F/0/1/0/all/0/1"&gt;Fabio Petroni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sameer Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1"&gt;Sebastian Riedel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-trained Language Model for Web-scale Retrieval in Baidu Search. (arXiv:2106.03373v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03373</id>
        <link href="http://arxiv.org/abs/2106.03373"/>
        <updated>2021-06-28T01:57:53.900Z</updated>
        <summary type="html"><![CDATA[Retrieval is a crucial stage in web search that identifies a small set of
query-relevant candidates from a billion-scale corpus. Discovering more
semantically-related candidates in the retrieval stage is very promising to
expose more high-quality results to the end users. However, it still remains
non-trivial challenges of building and deploying effective retrieval models for
semantic matching in real search engine. In this paper, we describe the
retrieval system that we developed and deployed in Baidu Search. The system
exploits the recent state-of-the-art Chinese pretrained language model, namely
Enhanced Representation through kNowledge IntEgration (ERNIE), which
facilitates the system with expressive semantic matching. In particular, we
developed an ERNIE-based retrieval model, which is equipped with 1) expressive
Transformer-based semantic encoders, and 2) a comprehensive multi-stage
training paradigm. More importantly, we present a practical system workflow for
deploying the model in web-scale retrieval. Eventually, the system is fully
deployed into production, where rigorous offline and online experiments were
conducted. The results show that the system can perform high-quality candidate
retrieval, especially for those tail queries with uncommon demands. Overall,
the new retrieval system facilitated by pretrained language model (i.e., ERNIE)
can largely improve the usability and applicability of our search engine.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yiding Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1"&gt;Guan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiaxiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Weixue Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1"&gt;Suqi Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yukun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1"&gt;Daiting Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuaiqiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1"&gt;Zhicong Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1"&gt;Dawei Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Session-aware Linear Item-Item Models for Session-based Recommendation. (arXiv:2103.16104v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16104</id>
        <link href="http://arxiv.org/abs/2103.16104"/>
        <updated>2021-06-28T01:57:53.803Z</updated>
        <summary type="html"><![CDATA[Session-based recommendation aims at predicting the next item given a
sequence of previous items consumed in the session, e.g., on e-commerce or
multimedia streaming services. Specifically, session data exhibits some unique
characteristics, i.e., session consistency and sequential dependency over items
within the session, repeated item consumption, and session timeliness. In this
paper, we propose simple-yet-effective linear models for considering the
holistic aspects of the sessions. The comprehensive nature of our models helps
improve the quality of session-based recommendation. More importantly, it
provides a generalized framework for reflecting different perspectives of
session data. Furthermore, since our models can be solved by closed-form
solutions, they are highly scalable. Experimental results demonstrate that the
proposed linear models show competitive or state-of-the-art performance in
various metrics on several real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1"&gt;Minjin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_j/0/1/0/all/0/1"&gt;jinhong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Joonseok Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shim_H/0/1/0/all/0/1"&gt;Hyunjung Shim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jongwuk Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains. (arXiv:2106.13474v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13474</id>
        <link href="http://arxiv.org/abs/2106.13474"/>
        <updated>2021-06-28T01:57:53.783Z</updated>
        <summary type="html"><![CDATA[Large pre-trained models have achieved great success in many natural language
processing tasks. However, when they are applied in specific domains, these
models suffer from domain shift and bring challenges in fine-tuning and online
serving for latency and capacity constraints. In this paper, we present a
general approach to developing small, fast and effective pre-trained models for
specific domains. This is achieved by adapting the off-the-shelf general
pre-trained models and performing task-agnostic knowledge distillation in
target domains. Specifically, we propose domain-specific vocabulary expansion
in the adaptation stage and employ corpus level occurrence probability to
choose the size of incremental vocabulary automatically. Then we systematically
explore different strategies to compress the large pre-trained models for
specific domains. We conduct our experiments in the biomedical and computer
science domain. The experimental results demonstrate that our approach achieves
better performance over the BERT BASE model in domain-specific tasks while 3.3x
smaller and 5.1x faster than BERT BASE. The code and pre-trained models are
available at https://aka.ms/adalm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yunzhi Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shaohan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenhui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1"&gt;Li Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Furu Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sentiment Progression based Searching and Indexing of Literary Textual Artefacts. (arXiv:2106.13767v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.13767</id>
        <link href="http://arxiv.org/abs/2106.13767"/>
        <updated>2021-06-28T01:57:53.746Z</updated>
        <summary type="html"><![CDATA[Literary artefacts are generally indexed and searched based on titles, meta
data and keywords over the years. This searching and indexing works well when
user/reader already knows about that particular creative textual artefact or
document. This indexing and search hardly takes into account interest and
emotional makeup of readers and its mapping to books. When a person is looking
for a literary textual artefact, he/she might be looking for not only
information but also to seek the joy of reading. In case of literary artefacts,
progression of emotions across the key events could prove to be the key for
indexing and searching. In this paper, we establish clusters among literary
artefacts based on computational relationships among sentiment progressions
using intelligent text analysis. We have created a database of 1076 English
titles + 20 Marathi titles and also used database
this http URL with 16559 titles and their
summaries. We have proposed Sentiment Progression based Indexing for searching
and recommending books. This can be used to create personalized clusters of
book titles of interest to readers. The analysis clearly suggests better
searching and indexing when we are targeting book lovers looking for a
particular type of book or creative artefact. This indexing and searching can
find many real-life applications for recommending books.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_H/0/1/0/all/0/1"&gt;Hrishikesh Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alicea_B/0/1/0/all/0/1"&gt;Bradly Alicea&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[JNLP Team: Deep Learning Approaches for Legal Processing Tasks in COLIEE 2021. (arXiv:2106.13405v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13405</id>
        <link href="http://arxiv.org/abs/2106.13405"/>
        <updated>2021-06-28T01:57:53.734Z</updated>
        <summary type="html"><![CDATA[COLIEE is an annual competition in automatic computerized legal text
processing. Automatic legal document processing is an ambitious goal, and the
structure and semantics of the law are often far more complex than everyday
language. In this article, we survey and report our methods and experimental
results in using deep learning in legal document processing. The results show
the difficulties as well as potentials in this family of approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Ha-Thanh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1"&gt;Phuong Minh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vuong_T/0/1/0/all/0/1"&gt;Thi-Hai-Yen Vuong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_Q/0/1/0/all/0/1"&gt;Quan Minh Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1"&gt;Chau Minh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dang_B/0/1/0/all/0/1"&gt;Binh Tran Dang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1"&gt;Vu Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1"&gt;Minh Le Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Satoh_K/0/1/0/all/0/1"&gt;Ken Satoh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[byteSteady: Fast Classification Using Byte-Level n-Gram Embeddings. (arXiv:2106.13302v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13302</id>
        <link href="http://arxiv.org/abs/2106.13302"/>
        <updated>2021-06-28T01:57:53.715Z</updated>
        <summary type="html"><![CDATA[This article introduces byteSteady -- a fast model for classification using
byte-level n-gram embeddings. byteSteady assumes that each input comes as a
sequence of bytes. A representation vector is produced using the averaged
embedding vectors of byte-level n-grams, with a pre-defined set of n. The
hashing trick is used to reduce the number of embedding vectors. This input
representation vector is then fed into a linear classifier. A straightforward
application of byteSteady is text classification. We also apply byteSteady to
one type of non-language data -- DNA sequences for gene classification. For
both problems we achieved competitive classification results against strong
baselines, suggesting that byteSteady can be applied to both language and
non-language data. Furthermore, we find that simple compression using Huffman
coding does not significantly impact the results, which offers an
accuracy-speed trade-off previously unexplored in machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drouin_A/0/1/0/all/0/1"&gt;Alexandre Drouin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Raymond Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-Specific Pretraining for Vertical Search: Case Study on Biomedical Literature. (arXiv:2106.13375v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.13375</id>
        <link href="http://arxiv.org/abs/2106.13375"/>
        <updated>2021-06-28T01:57:53.704Z</updated>
        <summary type="html"><![CDATA[Information overload is a prevalent challenge in many high-value domains. A
prominent case in point is the explosion of the biomedical literature on
COVID-19, which swelled to hundreds of thousands of papers in a matter of
months. In general, biomedical literature expands by two papers every minute,
totalling over a million new papers every year. Search in the biomedical realm,
and many other vertical domains is challenging due to the scarcity of direct
supervision from click logs. Self-supervised learning has emerged as a
promising direction to overcome the annotation bottleneck. We propose a general
approach for vertical search based on domain-specific pretraining and present a
case study for the biomedical domain. Despite being substantially simpler and
not using any relevance labels for training or development, our method performs
comparably or better than the best systems in the official TREC-COVID
evaluation, a COVID-related biomedical search competition. Using distributed
computing in modern cloud infrastructure, our system can scale to tens of
millions of articles on PubMed and has been deployed as Microsoft Biomedical
Search, a new search experience for biomedical literature:
https://aka.ms/biomedsearch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jinchao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1"&gt;Tristan Naumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1"&gt;Chenyan Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Hao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tinn_R/0/1/0/all/0/1"&gt;Robert Tinn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1"&gt;Cliff Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1"&gt;Naoto Usuyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rogahn_R/0/1/0/all/0/1"&gt;Richard Rogahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zhihong Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1"&gt;Yang Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1"&gt;Eric Horvitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennett_P/0/1/0/all/0/1"&gt;Paul N. Bennett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1"&gt;Hoifung Poon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Modern Perspective on Query Likelihood with Deep Generative Retrieval Models. (arXiv:2106.13618v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.13618</id>
        <link href="http://arxiv.org/abs/2106.13618"/>
        <updated>2021-06-28T01:57:53.674Z</updated>
        <summary type="html"><![CDATA[Existing neural ranking models follow the text matching paradigm, where
document-to-query relevance is estimated through predicting the matching score.
Drawing from the rich literature of classical generative retrieval models, we
introduce and formalize the paradigm of deep generative retrieval models
defined via the cumulative probabilities of generating query terms. This
paradigm offers a grounded probabilistic view on relevance estimation while
still enabling the use of modern neural architectures. In contrast to the
matching paradigm, the probabilistic nature of generative rankers readily
offers a fine-grained measure of uncertainty. We adopt several current neural
generative models in our framework and introduce a novel generative ranker
(T-PGN), which combines the encoding capacity of Transformers with the Pointer
Generator Network model. We conduct an extensive set of evaluation experiments
on passage retrieval, leveraging the MS MARCO Passage Re-ranking and TREC Deep
Learning 2019 Passage Re-ranking collections. Our results show the
significantly higher performance of the T-PGN model when compared with other
generative models. Lastly, we demonstrate that exploiting the uncertainty
information of deep generative rankers opens new perspectives to
query/collection understanding, and significantly improves the cut-off
prediction task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lesota_O/0/1/0/all/0/1"&gt;Oleg Lesota&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rekabsaz_N/0/1/0/all/0/1"&gt;Navid Rekabsaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_D/0/1/0/all/0/1"&gt;Daniel Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grasserbauer_K/0/1/0/all/0/1"&gt;Klaus Antonius Grasserbauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1"&gt;Carsten Eickhoff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schedl_M/0/1/0/all/0/1"&gt;Markus Schedl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Source-Criticism Debiasing Method for GloVe Embeddings. (arXiv:2106.13382v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13382</id>
        <link href="http://arxiv.org/abs/2106.13382"/>
        <updated>2021-06-28T01:57:53.661Z</updated>
        <summary type="html"><![CDATA[It is well-documented that word embeddings trained on large public corpora
consistently exhibit known human social biases. Although many methods for
debiasing exist, almost all fixate on completely eliminating biased information
from the embeddings and often diminish training set size in the process. In
this paper, we present a simple yet effective method for debiasing GloVe word
embeddings (Pennington et al., 2014) which works by incorporating explicit
information about training set bias rather than removing biased data outright.
Our method runs quickly and efficiently with the help of a fast bias gradient
approximation method from Brunet et al. (2019). As our approach is akin to the
notion of 'source criticism' in the humanities, we term our method
Source-Critical GloVe (SC-GloVe). We show that SC-GloVe reduces the effect size
on Word Embedding Association Test (WEAT) sets without sacrificing training
data or TOP-1 performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McGovern_H/0/1/0/all/0/1"&gt;Hope McGovern&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Emergent Fake News Detection via Meta Neural Process Networks. (arXiv:2106.13711v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.13711</id>
        <link href="http://arxiv.org/abs/2106.13711"/>
        <updated>2021-06-28T01:57:53.608Z</updated>
        <summary type="html"><![CDATA[Fake news travels at unprecedented speeds, reaches global audiences and puts
users and communities at great risk via social media platforms. Deep learning
based models show good performance when trained on large amounts of labeled
data on events of interest, whereas the performance of models tends to degrade
on other events due to domain shift. Therefore, significant challenges are
posed for existing detection approaches to detect fake news on emergent events,
where large-scale labeled datasets are difficult to obtain. Moreover, adding
the knowledge from newly emergent events requires to build a new model from
scratch or continue to fine-tune the model, which can be challenging,
expensive, and unrealistic for real-world settings. In order to address those
challenges, we propose an end-to-end fake news detection framework named
MetaFEND, which is able to learn quickly to detect fake news on emergent events
with a few verified posts. Specifically, the proposed model integrates
meta-learning and neural process methods together to enjoy the benefits of
these approaches. In particular, a label embedding module and a hard attention
mechanism are proposed to enhance the effectiveness by handling categorical
information and trimming irrelevant posts. Extensive experiments are conducted
on multimedia datasets collected from Twitter and Weibo. The experimental
results show our proposed MetaFEND model can detect fake news on never-seen
events effectively and outperform the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yaqing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1"&gt;Fenglong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haoyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_K/0/1/0/all/0/1"&gt;Kishlay Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jing Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VOGUE: Answer Verbalization through Multi-Task Learning. (arXiv:2106.13316v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13316</id>
        <link href="http://arxiv.org/abs/2106.13316"/>
        <updated>2021-06-28T01:57:53.600Z</updated>
        <summary type="html"><![CDATA[In recent years, there have been significant developments in Question
Answering over Knowledge Graphs (KGQA). Despite all the notable advancements,
current KGQA systems only focus on answer generation techniques and not on
answer verbalization. However, in real-world scenarios (e.g., voice assistants
such as Alexa, Siri, etc.), users prefer verbalized answers instead of a
generated response. This paper addresses the task of answer verbalization for
(complex) question answering over knowledge graphs. In this context, we propose
a multi-task-based answer verbalization framework: VOGUE (Verbalization thrOuGh
mUlti-task lEarning). The VOGUE framework attempts to generate a verbalized
answer using a hybrid approach through a multi-task learning paradigm. Our
framework can generate results based on using questions and queries as inputs
concurrently. VOGUE comprises four modules that are trained simultaneously
through multi-task learning. We evaluate our framework on existing datasets for
answer verbalization, and it outperforms all current baselines on both BLEU and
METEOR scores.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kacupaj_E/0/1/0/all/0/1"&gt;Endri Kacupaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Premnadh_S/0/1/0/all/0/1"&gt;Shyamnath Premnadh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1"&gt;Kuldeep Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1"&gt;Jens Lehmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maleshkova_M/0/1/0/all/0/1"&gt;Maria Maleshkova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-trained Language Model based Ranking in Baidu Search. (arXiv:2105.11108v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11108</id>
        <link href="http://arxiv.org/abs/2105.11108"/>
        <updated>2021-06-28T01:57:53.587Z</updated>
        <summary type="html"><![CDATA[As the heart of a search engine, the ranking system plays a crucial role in
satisfying users' information demands. More recently, neural rankers fine-tuned
from pre-trained language models (PLMs) establish state-of-the-art ranking
effectiveness. However, it is nontrivial to directly apply these PLM-based
rankers to the large-scale web search system due to the following challenging
issues:(1) the prohibitively expensive computations of massive neural PLMs,
especially for long texts in the web-document, prohibit their deployments in an
online ranking system that demands extremely low latency;(2) the discrepancy
between existing ranking-agnostic pre-training objectives and the ad-hoc
retrieval scenarios that demand comprehensive relevance modeling is another
main barrier for improving the online ranking system;(3) a real-world search
engine typically involves a committee of ranking components, and thus the
compatibility of the individually fine-tuned ranking model is critical for a
cooperative ranking system. In this work, we contribute a series of
successfully applied techniques in tackling these exposed issues when deploying
the state-of-the-art Chinese pre-trained language model, i.e., ERNIE, in the
online search engine system. We first articulate a novel practice to
cost-efficiently summarize the web document and contextualize the resultant
summary content with the query using a cheap yet powerful Pyramid-ERNIE
architecture. Then we endow an innovative paradigm to finely exploit the
large-scale noisy and biased post-click behavioral data for relevance-oriented
pre-training. We also propose a human-anchored fine-tuning strategy tailored
for the online ranking system, aiming to stabilize the ranking signals across
various online components. Extensive offline and online experimental results
show that the proposed techniques significantly boost the search engine's
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1"&gt;Lixin Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shengqiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1"&gt;Hengyi Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1"&gt;Dehong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1"&gt;Suqi Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1"&gt;Daiting Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhifan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1"&gt;Weiyue Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuaiqiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1"&gt;Zhicong Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1"&gt;Dawei Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Balancing Accuracy and Fairness for Interactive Recommendation with Reinforcement Learning. (arXiv:2106.13386v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.13386</id>
        <link href="http://arxiv.org/abs/2106.13386"/>
        <updated>2021-06-28T01:57:53.503Z</updated>
        <summary type="html"><![CDATA[Fairness in recommendation has attracted increasing attention due to bias and
discrimination possibly caused by traditional recommenders. In Interactive
Recommender Systems (IRS), user preferences and the system's fairness status
are constantly changing over time. Existing fairness-aware recommenders mainly
consider fairness in static settings. Directly applying existing methods to IRS
will result in poor recommendation. To resolve this problem, we propose a
reinforcement learning based framework, FairRec, to dynamically maintain a
long-term balance between accuracy and fairness in IRS. User preferences and
the system's fairness status are jointly compressed into the state
representation to generate recommendations. FairRec aims at maximizing our
designed cumulative reward that combines accuracy and fairness. Extensive
experiments validate that FairRec can improve fairness, while preserving good
recommendation quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weiwen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Feng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1"&gt;Ruiming Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1"&gt;Ben Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guangyong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1"&gt;Pheng Ann Heng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interactive query expansion for professional search applications. (arXiv:2106.13528v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.13528</id>
        <link href="http://arxiv.org/abs/2106.13528"/>
        <updated>2021-06-28T01:57:53.495Z</updated>
        <summary type="html"><![CDATA[Knowledge workers (such as healthcare information professionals, patent
agents and recruitment professionals) undertake work tasks where search forms a
core part of their duties. In these instances, the search task is often complex
and time-consuming and requires specialist expert knowledge to formulate
accurate search strategies. Interactive features such as query expansion can
play a key role in supporting these tasks. However, generating query
suggestions within a professional search context requires that consideration be
given to the specialist, structured nature of the search strategies they
employ. In this paper, we investigate a variety of query expansion methods
applied to a collection of Boolean search strategies used in a variety of
real-world professional search tasks. The results demonstrate the utility of
context-free distributional language models and the value of using linguistic
cues such as ngram order to optimise the balance between precision and recall.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Russell_Rose_T/0/1/0/all/0/1"&gt;Tony Russell-Rose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gooch_P/0/1/0/all/0/1"&gt;Philip Gooch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kruschwitz_U/0/1/0/all/0/1"&gt;Udo Kruschwitz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Modal Knowledge Distillation Method for Automatic Cued Speech Recognition. (arXiv:2106.13686v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2106.13686</id>
        <link href="http://arxiv.org/abs/2106.13686"/>
        <updated>2021-06-28T01:57:53.477Z</updated>
        <summary type="html"><![CDATA[Cued Speech (CS) is a visual communication system for the deaf or hearing
impaired people. It combines lip movements with hand cues to obtain a complete
phonetic repertoire. Current deep learning based methods on automatic CS
recognition suffer from a common problem, which is the data scarcity. Until
now, there are only two public single speaker datasets for French (238
sentences) and British English (97 sentences). In this work, we propose a
cross-modal knowledge distillation method with teacher-student structure, which
transfers audio speech information to CS to overcome the limited data problem.
Firstly, we pretrain a teacher model for CS recognition with a large amount of
open source audio speech data, and simultaneously pretrain the feature
extractors for lips and hands using CS data. Then, we distill the knowledge
from teacher model to the student model with frame-level and sequence-level
distillation strategies. Importantly, for frame-level, we exploit multi-task
learning to weigh losses automatically, to obtain the balance coefficient.
Besides, we establish a five-speaker British English CS dataset for the first
time. The proposed method is evaluated on French and British English CS
datasets, showing superior CS recognition performance to the state-of-the-art
(SOTA) by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianrong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Ziyue Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuewei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1"&gt;Mei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1"&gt;Qiang Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Li Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrent Coupled Topic Modeling over Sequential Documents. (arXiv:2106.13732v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.13732</id>
        <link href="http://arxiv.org/abs/2106.13732"/>
        <updated>2021-06-28T01:57:53.232Z</updated>
        <summary type="html"><![CDATA[The abundant sequential documents such as online archival, social media and
news feeds are streamingly updated, where each chunk of documents is
incorporated with smoothly evolving yet dependent topics. Such digital texts
have attracted extensive research on dynamic topic modeling to infer hidden
evolving topics and their temporal dependencies. However, most of the existing
approaches focus on single-topic-thread evolution and ignore the fact that a
current topic may be coupled with multiple relevant prior topics. In addition,
these approaches also incur the intractable inference problem when inferring
latent parameters, resulting in a high computational cost and performance
degradation. In this work, we assume that a current topic evolves from all
prior topics with corresponding coupling weights, forming the
multi-topic-thread evolution. Our method models the dependencies between
evolving topics and thoroughly encodes their complex multi-couplings across
time steps. To conquer the intractable inference challenge, a new solution with
a set of novel data augmentation techniques is proposed, which successfully
discomposes the multi-couplings between evolving topics. A fully conjugate
model is thus obtained to guarantee the effectiveness and efficiency of the
inference technique. A novel Gibbs sampler with a backward-forward filter
algorithm efficiently learns latent timeevolving parameters in a closed-form.
In addition, the latent Indian Buffet Process (IBP) compound distribution is
exploited to automatically infer the overall topic number and customize the
sparse topic proportions for each sequential document without bias. The
proposed method is evaluated on both synthetic and real-world datasets against
the competitive baselines, demonstrating its superiority over the baselines in
terms of the low per-word perplexity, high coherent topics, and better document
time prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jinjin Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1"&gt;Longbing Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1"&gt;Zhiguo Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DnS: Distill-and-Select for Efficient and Accurate Video Indexing and Retrieval. (arXiv:2106.13266v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13266</id>
        <link href="http://arxiv.org/abs/2106.13266"/>
        <updated>2021-06-28T01:57:53.203Z</updated>
        <summary type="html"><![CDATA[In this paper, we address the problem of high performance and computationally
efficient content-based video retrieval in large-scale datasets. Current
methods typically propose either: (i) fine-grained approaches employing
spatio-temporal representations and similarity calculations, achieving high
performance at a high computational cost or (ii) coarse-grained approaches
representing/indexing videos as global vectors, where the spatio-temporal
structure is lost, providing low performance but also having low computational
cost. In this work, we propose a Knowledge Distillation framework, which we
call Distill-and-Select (DnS), that starting from a well-performing
fine-grained Teacher Network learns: a) Student Networks at different retrieval
performance and computational efficiency trade-offs and b) a Selection Network
that at test time rapidly directs samples to the appropriate student to
maintain both high retrieval performance and high computational efficiency. We
train several students with different architectures and arrive at different
trade-offs of performance and efficiency, i.e., speed and storage requirements,
including fine-grained students that store index videos using binary
representations. Importantly, the proposed scheme allows Knowledge Distillation
in large, unlabelled datasets -- this leads to good students. We evaluate DnS
on five public datasets on three different video retrieval tasks and
demonstrate a) that our students achieve state-of-the-art performance in
several cases and b) that our DnS framework provides an excellent trade-off
between retrieval performance, computational speed, and storage space. In
specific configurations, our method achieves similar mAP with the teacher but
is 20 times faster and requires 240 times less storage space. Our collected
dataset and implementation are publicly available:
https://github.com/mever-team/distill-and-select.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kordopatis_Zilos_G/0/1/0/all/0/1"&gt;Giorgos Kordopatis-Zilos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tzelepis_C/0/1/0/all/0/1"&gt;Christos Tzelepis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1"&gt;Symeon Papadopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kompatsiaris_I/0/1/0/all/0/1"&gt;Ioannis Kompatsiaris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patras_I/0/1/0/all/0/1"&gt;Ioannis Patras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TableSense: Spreadsheet Table Detection with Convolutional Neural Networks. (arXiv:2106.13500v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.13500</id>
        <link href="http://arxiv.org/abs/2106.13500"/>
        <updated>2021-06-28T01:57:53.189Z</updated>
        <summary type="html"><![CDATA[Spreadsheet table detection is the task of detecting all tables on a given
sheet and locating their respective ranges. Automatic table detection is a key
enabling technique and an initial step in spreadsheet data intelligence.
However, the detection task is challenged by the diversity of table structures
and table layouts on the spreadsheet. Considering the analogy between a cell
matrix as spreadsheet and a pixel matrix as image, and encouraged by the
successful application of Convolutional Neural Networks (CNN) in computer
vision, we have developed TableSense, a novel end-to-end framework for
spreadsheet table detection. First, we devise an effective cell featurization
scheme to better leverage the rich information in each cell; second, we develop
an enhanced convolutional neural network model for table detection to meet the
domain-specific requirement on precise table boundary detection; third, we
propose an effective uncertainty metric to guide an active learning based smart
sampling algorithm, which enables the efficient build-up of a training dataset
with 22,176 tables on 10,220 sheets with broad coverage of diverse table
structures and layouts. Our evaluation shows that TableSense is highly
effective with 91.3\% recall and 86.5\% precision in EoB-2 metric, a
significant improvement over both the current detection algorithm that are used
in commodity spreadsheet tools and state-of-the-art convolutional neural
networks in computer vision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Haoyu Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shijie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Shi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1"&gt;Zhouyu Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongmei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-Specific Pretraining for Vertical Search: Case Study on Biomedical Literature. (arXiv:2106.13375v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.13375</id>
        <link href="http://arxiv.org/abs/2106.13375"/>
        <updated>2021-06-28T01:57:53.178Z</updated>
        <summary type="html"><![CDATA[Information overload is a prevalent challenge in many high-value domains. A
prominent case in point is the explosion of the biomedical literature on
COVID-19, which swelled to hundreds of thousands of papers in a matter of
months. In general, biomedical literature expands by two papers every minute,
totalling over a million new papers every year. Search in the biomedical realm,
and many other vertical domains is challenging due to the scarcity of direct
supervision from click logs. Self-supervised learning has emerged as a
promising direction to overcome the annotation bottleneck. We propose a general
approach for vertical search based on domain-specific pretraining and present a
case study for the biomedical domain. Despite being substantially simpler and
not using any relevance labels for training or development, our method performs
comparably or better than the best systems in the official TREC-COVID
evaluation, a COVID-related biomedical search competition. Using distributed
computing in modern cloud infrastructure, our system can scale to tens of
millions of articles on PubMed and has been deployed as Microsoft Biomedical
Search, a new search experience for biomedical literature:
https://aka.ms/biomedsearch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jinchao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1"&gt;Tristan Naumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1"&gt;Chenyan Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Hao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tinn_R/0/1/0/all/0/1"&gt;Robert Tinn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1"&gt;Cliff Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1"&gt;Naoto Usuyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rogahn_R/0/1/0/all/0/1"&gt;Richard Rogahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zhihong Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1"&gt;Yang Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1"&gt;Eric Horvitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennett_P/0/1/0/all/0/1"&gt;Paul N. Bennett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1"&gt;Hoifung Poon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DnS: Distill-and-Select for Efficient and Accurate Video Indexing and Retrieval. (arXiv:2106.13266v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13266</id>
        <link href="http://arxiv.org/abs/2106.13266"/>
        <updated>2021-06-28T01:57:53.164Z</updated>
        <summary type="html"><![CDATA[In this paper, we address the problem of high performance and computationally
efficient content-based video retrieval in large-scale datasets. Current
methods typically propose either: (i) fine-grained approaches employing
spatio-temporal representations and similarity calculations, achieving high
performance at a high computational cost or (ii) coarse-grained approaches
representing/indexing videos as global vectors, where the spatio-temporal
structure is lost, providing low performance but also having low computational
cost. In this work, we propose a Knowledge Distillation framework, which we
call Distill-and-Select (DnS), that starting from a well-performing
fine-grained Teacher Network learns: a) Student Networks at different retrieval
performance and computational efficiency trade-offs and b) a Selection Network
that at test time rapidly directs samples to the appropriate student to
maintain both high retrieval performance and high computational efficiency. We
train several students with different architectures and arrive at different
trade-offs of performance and efficiency, i.e., speed and storage requirements,
including fine-grained students that store index videos using binary
representations. Importantly, the proposed scheme allows Knowledge Distillation
in large, unlabelled datasets -- this leads to good students. We evaluate DnS
on five public datasets on three different video retrieval tasks and
demonstrate a) that our students achieve state-of-the-art performance in
several cases and b) that our DnS framework provides an excellent trade-off
between retrieval performance, computational speed, and storage space. In
specific configurations, our method achieves similar mAP with the teacher but
is 20 times faster and requires 240 times less storage space. Our collected
dataset and implementation are publicly available:
https://github.com/mever-team/distill-and-select.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kordopatis_Zilos_G/0/1/0/all/0/1"&gt;Giorgos Kordopatis-Zilos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tzelepis_C/0/1/0/all/0/1"&gt;Christos Tzelepis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1"&gt;Symeon Papadopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kompatsiaris_I/0/1/0/all/0/1"&gt;Ioannis Kompatsiaris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patras_I/0/1/0/all/0/1"&gt;Ioannis Patras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpreting Depression From Question-wise Long-term Video Recording of SDS Evaluation. (arXiv:2106.13393v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13393</id>
        <link href="http://arxiv.org/abs/2106.13393"/>
        <updated>2021-06-28T01:57:53.133Z</updated>
        <summary type="html"><![CDATA[Self-Rating Depression Scale (SDS) questionnaire has frequently been used for
efficient depression preliminary screening. However, the uncontrollable
self-administered measure can be easily affected by insouciantly or deceptively
answering, and producing the different results with the clinician-administered
Hamilton Depression Rating Scale (HDRS) and the final diagnosis. Clinically,
facial expression (FE) and actions play a vital role in clinician-administered
evaluation, while FE and action are underexplored for self-administered
evaluations. In this work, we collect a novel dataset of 200 subjects to
evidence the validity of self-rating questionnaires with their corresponding
question-wise video recording. To automatically interpret depression from the
SDS evaluation and the paired video, we propose an end-to-end hierarchical
framework for the long-term variable-length video, which is also conditioned on
the questionnaire results and the answering time. Specifically, we resort to a
hierarchical model which utilizes a 3D CNN for local temporal pattern
exploration and a redundancy-aware self-attention (RAS) scheme for
question-wise global feature aggregation. Targeting for the redundant long-term
FE video processing, our RAS is able to effectively exploit the correlations of
each video clip within a question set to emphasize the discriminative
information and eliminate the redundancy based on feature pair-wise affinity.
Then, the question-wise video feature is concatenated with the questionnaire
scores for final depression detection. Our thorough evaluations also show the
validity of fusing SDS evaluation and its video recording, and the superiority
of our framework to the conventional state-of-the-art temporal modeling
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1"&gt;Wanqing Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1"&gt;Lizhong Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jihong Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Hui Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Usage-based Summaries of Learning Videos. (arXiv:2106.13504v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2106.13504</id>
        <link href="http://arxiv.org/abs/2106.13504"/>
        <updated>2021-06-28T01:57:53.067Z</updated>
        <summary type="html"><![CDATA[Much of the delivery of University education is now by synchronous or
asynchronous video. For students, one of the challenges is managing the sheer
volume of such video material as video presentations of taught material are
difficult to abbreviate and summarise because they do not have highlights which
stand out. Apart from video bookmarks there are no tools available to determine
which parts of video content should be replayed at revision time or just before
examinations. We have developed and deployed a digital library for managing
video learning material which has many dozens of hours of short-form video
content from a range of taught courses for hundreds of students at
undergraduate level. Through a web browser we allow students to access and play
these videos and we log their anonymised playback usage. From these logs we
score to each segment of each video based on the amount of playback it receives
from across all students, whether the segment has been re-wound and re-played
in the same student session, whether the on-screen window is the window in
focus on the student's desktop/laptop, and speed of playback. We also
incorporate negative scoring if a video segment is skipped or fast-forward, and
overarching all this we include a decay function based on recency of playback,
so the most recent days of playback contribute more to the video segment
scores. For each video in the library we present a usage-based graph which
allows students to see which parts of each video attract the most playback from
their peers, which helps them select material at revision time. Usage of the
system is fully anonymised and GDPR-compliant.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hyowon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mingming Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scriney_M/0/1/0/all/0/1"&gt;Michael Scriney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smeaton_A/0/1/0/all/0/1"&gt;Alan F. Smeaton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiview Video Compression Using Advanced HEVC Screen Content Coding. (arXiv:2106.13574v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2106.13574</id>
        <link href="http://arxiv.org/abs/2106.13574"/>
        <updated>2021-06-28T01:57:53.036Z</updated>
        <summary type="html"><![CDATA[The paper presents a new approach to multiview video coding using Screen
Content Coding. It is assumed that for a time instant the frames corresponding
to all views are packed into a single frame, i.e. the frame-compatible approach
to multiview coding is applied. For such coding scenario, the paper
demonstrates that Screen Content Coding can be efficiently used for multiview
video coding. Two approaches are considered: the first using standard HEVC
Screen Content Coding, and the second using Advanced Screen Content Coding. The
latter is the original proposal of the authors that exploits quarter-pel motion
vectors and other nonstandard extensions of HEVC Screen Content Coding. The
experimental results demonstrate that multiview video coding even using
standard HEVC Screen Content Coding is much more efficient than simulcast HEVC
coding. The proposed Advanced Screen Content Coding provides virtually the same
coding efficiency as MV-HEVC, which is the state-of-the-art multiview video
compression technique. The authors suggest that Advanced Screen Content Coding
can be efficiently used within the new Versatile Video Coding (VVC) technology.
Nevertheless a reference multiview extension of VVC does not exist yet,
therefore, for VVC-based coding, the experimental comparisons are left for
future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samelak_J/0/1/0/all/0/1"&gt;Jaros&amp;#x142;aw Samelak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Domanski_M/0/1/0/all/0/1"&gt;Marek Doma&amp;#x144;ski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast and Efficient Locomotion via Learned Gait Transitions. (arXiv:2104.04644v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04644</id>
        <link href="http://arxiv.org/abs/2104.04644"/>
        <updated>2021-06-25T02:00:47.701Z</updated>
        <summary type="html"><![CDATA[We focus on the problem of developing energy efficient controllers for
quadrupedal robots. Animals can actively switch gaits at different speeds to
lower their energy consumption. In this paper, we devise a hierarchical
learning framework, in which distinctive locomotion gaits and natural gait
transitions emerge automatically with a simple reward of energy minimization.
We use reinforcement learning to train a high-level gait policy that specifies
gait patterns of each foot, while the low-level whole-body controller optimizes
the motor commands so that the robot can walk at a desired velocity using that
gait pattern. We test our learning framework on a quadruped robot and
demonstrate automatic gait transitions, from walking to trotting and to
fly-trotting, as the robot increases its speed. We show that the learned
hierarchical controller consumes much less energy across a wide range of
locomotion speed than baseline controllers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yuxiang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tingnan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coumans_E/0/1/0/all/0/1"&gt;Erwin Coumans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1"&gt;Jie Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boots_B/0/1/0/all/0/1"&gt;Byron Boots&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Black-box Metrics with Iterative Example Weighting. (arXiv:2102.09492v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09492</id>
        <link href="http://arxiv.org/abs/2102.09492"/>
        <updated>2021-06-25T02:00:47.695Z</updated>
        <summary type="html"><![CDATA[We consider learning to optimize a classification metric defined by a
black-box function of the confusion matrix. Such black-box learning settings
are ubiquitous, for example, when the learner only has query access to the
metric of interest, or in noisy-label and domain adaptation applications where
the learner must evaluate the metric via performance evaluation using a small
validation sample. Our approach is to adaptively learn example weights on the
training dataset such that the resulting weighted objective best approximates
the metric on the validation sample. We show how to model and estimate the
example weights and use them to iteratively post-shift a pre-trained class
probability estimator to construct a classifier. We also analyze the resulting
procedure's statistical properties. Experiments on various label noise, domain
shift, and fair classification setups confirm that our proposal compares
favorably to the state-of-the-art baselines for each application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hiranandani_G/0/1/0/all/0/1"&gt;Gaurush Hiranandani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathur_J/0/1/0/all/0/1"&gt;Jatin Mathur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narasimhan_H/0/1/0/all/0/1"&gt;Harikrishna Narasimhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fard_M/0/1/0/all/0/1"&gt;Mahdi Milani Fard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koyejo_O/0/1/0/all/0/1"&gt;Oluwasanmi Koyejo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BABEL: Bodies, Action and Behavior with English Labels. (arXiv:2106.09696v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09696</id>
        <link href="http://arxiv.org/abs/2106.09696"/>
        <updated>2021-06-25T02:00:47.678Z</updated>
        <summary type="html"><![CDATA[Understanding the semantics of human movement -- the what, how and why of the
movement -- is an important problem that requires datasets of human actions
with semantic labels. Existing datasets take one of two approaches. Large-scale
video datasets contain many action labels but do not contain ground-truth 3D
human motion. Alternatively, motion-capture (mocap) datasets have precise body
motions but are limited to a small number of actions. To address this, we
present BABEL, a large dataset with language labels describing the actions
being performed in mocap sequences. BABEL consists of action labels for about
43 hours of mocap sequences from AMASS. Action labels are at two levels of
abstraction -- sequence labels describe the overall action in the sequence, and
frame labels describe all actions in every frame of the sequence. Each frame
label is precisely aligned with the duration of the corresponding action in the
mocap sequence, and multiple actions can overlap. There are over 28k sequence
labels, and 63k frame labels in BABEL, which belong to over 250 unique action
categories. Labels from BABEL can be leveraged for tasks like action
recognition, temporal action localization, motion synthesis, etc. To
demonstrate the value of BABEL as a benchmark, we evaluate the performance of
models on 3D action recognition. We demonstrate that BABEL poses interesting
learning challenges that are applicable to real-world scenarios, and can serve
as a useful benchmark of progress in 3D action recognition. The dataset,
baseline method, and evaluation code is made available, and supported for
academic research purposes at https://babel.is.tue.mpg.de/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Punnakkal_A/0/1/0/all/0/1"&gt;Abhinanda R. Punnakkal&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Chandrasekaran_A/0/1/0/all/0/1"&gt;Arjun Chandrasekaran&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Athanasiou_N/0/1/0/all/0/1"&gt;Nikos Athanasiou&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Quiros_Ramirez_A/0/1/0/all/0/1"&gt;Alejandra Quiros-Ramirez&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1"&gt;Michael J. Black&lt;/a&gt; (1) ((1) Max Planck Institute for Intelligent Systems, (2) Universitat Konstanz)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Quantum Singular Value Decomposition. (arXiv:2006.02336v3 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.02336</id>
        <link href="http://arxiv.org/abs/2006.02336"/>
        <updated>2021-06-25T02:00:47.602Z</updated>
        <summary type="html"><![CDATA[Singular value decomposition is central to many problems in engineering and
scientific fields. Several quantum algorithms have been proposed to determine
the singular values and their associated singular vectors of a given matrix.
Although these algorithms are promising, the required quantum subroutines and
resources are too costly on near-term quantum devices. In this work, we propose
a variational quantum algorithm for singular value decomposition (VQSVD). By
exploiting the variational principles for singular values and the Ky Fan
Theorem, we design a novel loss function such that two quantum neural networks
(or parameterized quantum circuits) could be trained to learn the singular
vectors and output the corresponding singular values. Furthermore, we conduct
numerical simulations of VQSVD for random matrices as well as its applications
in image compression of handwritten digits. Finally, we discuss the
applications of our algorithm in recommendation systems and polar
decomposition. Our work explores new avenues for quantum information processing
beyond the conventional protocols that only works for Hermitian data, and
reveals the capability of matrix decomposition on near-term quantum devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Song_Z/0/1/0/all/0/1"&gt;Zhixin Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Youle Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-supervised Vector-valued Learning: From Theory to Algorithm. (arXiv:1909.04883v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.04883</id>
        <link href="http://arxiv.org/abs/1909.04883"/>
        <updated>2021-06-25T02:00:47.597Z</updated>
        <summary type="html"><![CDATA[Vector-valued learning, where the output space admits a vector-valued
structure, is an important problem that covers a broad family of important
domains, e.g. multi-label learning and multi-class classification. Using local
Rademacher complexity and unlabeled data, we derive novel data-dependent excess
risk bounds for learning vector-valued functions in both the kernel space and
linear space. The derived bounds are much sharper than existing ones, where
convergence rates are improved from $\mathcal{O}(1/\sqrt{n})$ to
$\mathcal{O}(1/\sqrt{n+u}),$ and $\mathcal{O}(1/n)$ in special cases. Motivated
by our theoretical analysis, we propose a unified framework for learning
vector-valued functions, incorporating both local Rademacher complexity and
Laplacian regularization. Empirical results on a wide number of benchmark
datasets show that the proposed algorithm significantly outperforms baseline
methods, which coincides with our theoretical findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weiping Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Understanding and Mitigating Social Biases in Language Models. (arXiv:2106.13219v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13219</id>
        <link href="http://arxiv.org/abs/2106.13219"/>
        <updated>2021-06-25T02:00:47.592Z</updated>
        <summary type="html"><![CDATA[As machine learning methods are deployed in real-world settings such as
healthcare, legal systems, and social science, it is crucial to recognize how
they shape social biases and stereotypes in these sensitive decision-making
processes. Among such real-world deployments are large-scale pretrained
language models (LMs) that can be potentially dangerous in manifesting
undesirable representational biases - harmful biases resulting from
stereotyping that propagate negative generalizations involving gender, race,
religion, and other social constructs. As a step towards improving the fairness
of LMs, we carefully define several sources of representational biases before
proposing new benchmarks and metrics to measure them. With these tools, we
propose steps towards mitigating social biases during text generation. Our
empirical results and human evaluation demonstrate effectiveness in mitigating
bias while retaining crucial contextual information for high-fidelity text
generation, thereby pushing forward the performance-fairness Pareto frontier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chiyu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1"&gt;Louis-Philippe Morency&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Non-parametric Bayesian Hawkes Processes. (arXiv:1810.03730v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1810.03730</id>
        <link href="http://arxiv.org/abs/1810.03730"/>
        <updated>2021-06-25T02:00:47.587Z</updated>
        <summary type="html"><![CDATA[In this paper, we develop an efficient nonparametric Bayesian estimation of
the kernel function of Hawkes processes. The non-parametric Bayesian approach
is important because it provides flexible Hawkes kernels and quantifies their
uncertainty. Our method is based on the cluster representation of Hawkes
processes. Utilizing the stationarity of the Hawkes process, we efficiently
sample random branching structures and thus, we split the Hawkes process into
clusters of Poisson processes. We derive two algorithms -- a block Gibbs
sampler and a maximum a posteriori estimator based on expectation maximization
-- and we show that our methods have a linear time complexity, both
theoretically and empirically. On synthetic data, we show our methods to be
able to infer flexible Hawkes triggering kernels. On two large-scale Twitter
diffusion datasets, we show that our methods outperform the current
state-of-the-art in goodness-of-fit and that the time complexity is linear in
the size of the dataset. We also observe that on diffusions related to online
videos, the learned kernels reflect the perceived longevity for different
content types such as music or pets videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walder_C/0/1/0/all/0/1"&gt;Christian Walder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rizoiu_M/0/1/0/all/0/1"&gt;Marian-Andrei Rizoiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1"&gt;Lexing Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FF-NSL: Feed-Forward Neural-Symbolic Learner. (arXiv:2106.13103v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13103</id>
        <link href="http://arxiv.org/abs/2106.13103"/>
        <updated>2021-06-25T02:00:47.568Z</updated>
        <summary type="html"><![CDATA[Inductive Logic Programming (ILP) aims to learn generalised, interpretable
hypotheses in a data-efficient manner. However, current ILP systems require
training examples to be specified in a structured logical form. This paper
introduces a neural-symbolic learning framework, called Feed-Forward
Neural-Symbolic Learner (FF-NSL), that integrates state-of-the-art ILP systems
based on the Answer Set semantics, with neural networks, in order to learn
interpretable hypotheses from labelled unstructured data. FF-NSL uses a
pre-trained neural network to extract symbolic facts from unstructured data and
an ILP system to learn a hypothesis that performs a downstream classification
task. In order to evaluate the applicability of our approach to real-world
applications, the framework is evaluated on tasks where distributional shifts
are introduced to unstructured input data, for which pre-trained neural
networks are likely to predict incorrectly and with high confidence.
Experimental results show that FF-NSL outperforms baseline approaches such as a
random forest and deep neural networks by learning more accurate and
interpretable hypotheses with fewer examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cunnington_D/0/1/0/all/0/1"&gt;Daniel Cunnington&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1"&gt;Mark Law&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russo_A/0/1/0/all/0/1"&gt;Alessandra Russo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lobo_J/0/1/0/all/0/1"&gt;Jorge Lobo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video Swin Transformer. (arXiv:2106.13230v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13230</id>
        <link href="http://arxiv.org/abs/2106.13230"/>
        <updated>2021-06-25T02:00:47.562Z</updated>
        <summary type="html"><![CDATA[The vision community is witnessing a modeling shift from CNNs to
Transformers, where pure Transformer architectures have attained top accuracy
on the major video recognition benchmarks. These video models are all built on
Transformer layers that globally connect patches across the spatial and
temporal dimensions. In this paper, we instead advocate an inductive bias of
locality in video Transformers, which leads to a better speed-accuracy
trade-off compared to previous approaches which compute self-attention globally
even with spatial-temporal factorization. The locality of the proposed video
architecture is realized by adapting the Swin Transformer designed for the
image domain, while continuing to leverage the power of pre-trained image
models. Our approach achieves state-of-the-art accuracy on a broad range of
video recognition benchmarks, including on action recognition (84.9 top-1
accuracy on Kinetics-400 and 86.1 top-1 accuracy on Kinetics-600 with ~20x less
pre-training data and ~3x smaller model size) and temporal modeling (69.6 top-1
accuracy on Something-Something v2). The code and models will be made publicly
available at https://github.com/SwinTransformer/Video-Swin-Transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ze Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ning_J/0/1/0/all/0/1"&gt;Jia Ning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yue Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yixuan Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Stephen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Han Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SHINE: SHaring the INverse Estimate from the forward pass for bi-level optimization and implicit models. (arXiv:2106.00553v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00553</id>
        <link href="http://arxiv.org/abs/2106.00553"/>
        <updated>2021-06-25T02:00:47.557Z</updated>
        <summary type="html"><![CDATA[In recent years, implicit deep learning has emerged as a method to increase
the depth of deep neural networks. While their training is memory-efficient,
they are still significantly slower to train than their explicit counterparts.
In Deep Equilibrium Models (DEQs), the training is performed as a bi-level
problem, and its computational complexity is partially driven by the iterative
inversion of a huge Jacobian matrix. In this paper, we propose a novel strategy
to tackle this computational bottleneck from which many bi-level problems
suffer. The main idea is to use the quasi-Newton matrices from the forward pass
to efficiently approximate the inverse Jacobian matrix in the direction needed
for the gradient computation. We provide a theorem that motivates using our
method with the original forward algorithms. In addition, by modifying these
forward algorithms, we further provide theoretical guarantees that our method
asymptotically estimates the true implicit gradient. We empirically study this
approach in many settings, ranging from hyperparameter optimization to large
Multiscale DEQs applied to CIFAR and ImageNet. We show that it reduces the
computational cost of the backward pass by up to two orders of magnitude. All
this is achieved while retaining the excellent performance of the original
models in hyperparameter optimization and on CIFAR, and giving encouraging and
competitive results on ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ramzi_Z/0/1/0/all/0/1"&gt;Zaccharie Ramzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannel_F/0/1/0/all/0/1"&gt;Florian Mannel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1"&gt;Shaojie Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Starck_J/0/1/0/all/0/1"&gt;Jean-Luc Starck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciuciu_P/0/1/0/all/0/1"&gt;Philippe Ciuciu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreau_T/0/1/0/all/0/1"&gt;Thomas Moreau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiclass Disease Predictions Based on Integrated Clinical and Genomics Datasets. (arXiv:2006.07879v1 [q-bio.GN] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2006.07879</id>
        <link href="http://arxiv.org/abs/2006.07879"/>
        <updated>2021-06-25T02:00:47.552Z</updated>
        <summary type="html"><![CDATA[Clinical predictions using clinical data by computational methods are common
in bioinformatics. However, clinical predictions using information from
genomics datasets as well is not a frequently observed phenomenon in research.
Precision medicine research requires information from all available datasets to
provide intelligent clinical solutions. In this paper, we have attempted to
create a prediction model which uses information from both clinical and
genomics datasets. We have demonstrated multiclass disease predictions based on
combined clinical and genomics datasets using machine learning methods. We have
created an integrated dataset, using a clinical (ClinVar) and a genomics (gene
expression) dataset, and trained it using instance-based learner to predict
clinical diseases. We have used an innovative but simple way for multiclass
classification, where the number of output classes is as high as 75. We have
used Principal Component Analysis for feature selection. The classifier
predicted diseases with 73\% accuracy on the integrated dataset. The results
were consistent and competent when compared with other classification models.
The results show that genomics information can be reliably included in datasets
for clinical predictions and it can prove to be valuable in clinical
diagnostics and precision medicine.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Subhani_M/0/1/0/all/0/1"&gt;Moeez M. Subhani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Anjum_A/0/1/0/all/0/1"&gt;Ashiq Anjum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Empirical Study of Transformers for Source Code. (arXiv:2010.07987v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07987</id>
        <link href="http://arxiv.org/abs/2010.07987"/>
        <updated>2021-06-25T02:00:47.547Z</updated>
        <summary type="html"><![CDATA[Initially developed for natural language processing (NLP), Transformers are
now widely used for source code processing, due to the format similarity
between source code and text. In contrast to natural language, source code is
strictly structured, i.e., it follows the syntax of the programming language.
Several recent works develop Transformer modifications for capturing syntactic
information in source code. The drawback of these works is that they do not
compare to each other and consider different tasks. In this work, we conduct a
thorough empirical study of the capabilities of Transformers to utilize
syntactic information in different tasks. We consider three tasks (code
completion, function naming and bug fixing) and re-implement different
syntax-capturing modifications in a unified framework. We show that
Transformers are able to make meaningful predictions based purely on syntactic
information and underline the best practices of taking the syntactic
information into account for improving the performance of the model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chirkova_N/0/1/0/all/0/1"&gt;Nadezhda Chirkova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Troshin_S/0/1/0/all/0/1"&gt;Sergey Troshin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Autoencoder-Based Vehicle Trajectory Prediction with an Interpretable Latent Space. (arXiv:2103.13726v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13726</id>
        <link href="http://arxiv.org/abs/2103.13726"/>
        <updated>2021-06-25T02:00:47.533Z</updated>
        <summary type="html"><![CDATA[This paper introduces the Descriptive Variational Autoencoder (DVAE), an
unsupervised and end-to-end trainable neural network for predicting vehicle
trajectories that provides partial interpretability. The novel approach is
based on the architecture and objective of common variational autoencoders. By
introducing expert knowledge within the decoder part of the autoencoder, the
encoder learns to extract latent parameters that provide a graspable meaning in
human terms. Such an interpretable latent space enables the validation by
expert defined rule sets. The evaluation of the DVAE is performed using the
publicly available highD dataset for highway traffic scenarios. In comparison
to a conventional variational autoencoder with equivalent complexity, the
proposed model provides a similar prediction accuracy but with the great
advantage of having an interpretable latent space. For crucial decision making
and assessing trustworthiness of a prediction this property is highly
desirable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Neumeier_M/0/1/0/all/0/1"&gt;Marion Neumeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tollkuhn_A/0/1/0/all/0/1"&gt;Andreas Tollk&amp;#xfc;hn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berberich_T/0/1/0/all/0/1"&gt;Thomas Berberich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Botsch_M/0/1/0/all/0/1"&gt;Michael Botsch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting the Success of Domain Adaptation in Text Similarity. (arXiv:2106.04641v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04641</id>
        <link href="http://arxiv.org/abs/2106.04641"/>
        <updated>2021-06-25T02:00:47.527Z</updated>
        <summary type="html"><![CDATA[Transfer learning methods, and in particular domain adaptation, help exploit
labeled data in one domain to improve the performance of a certain task in
another domain. However, it is still not clear what factors affect the success
of domain adaptation. This paper models adaptation success and selection of the
most suitable source domains among several candidates in text similarity. We
use descriptive domain information and cross-domain similarity metrics as
predictive features. While mostly positive, the results also point to some
domains where adaptation success was difficult to predict.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pogrebnyakov_N/0/1/0/all/0/1"&gt;Nicolai Pogrebnyakov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaghaghian_S/0/1/0/all/0/1"&gt;Shohreh Shaghaghian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decision Transformer: Reinforcement Learning via Sequence Modeling. (arXiv:2106.01345v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01345</id>
        <link href="http://arxiv.org/abs/2106.01345"/>
        <updated>2021-06-25T02:00:47.522Z</updated>
        <summary type="html"><![CDATA[We introduce a framework that abstracts Reinforcement Learning (RL) as a
sequence modeling problem. This allows us to draw upon the simplicity and
scalability of the Transformer architecture, and associated advances in
language modeling such as GPT-x and BERT. In particular, we present Decision
Transformer, an architecture that casts the problem of RL as conditional
sequence modeling. Unlike prior approaches to RL that fit value functions or
compute policy gradients, Decision Transformer simply outputs the optimal
actions by leveraging a causally masked Transformer. By conditioning an
autoregressive model on the desired return (reward), past states, and actions,
our Decision Transformer model can generate future actions that achieve the
desired return. Despite its simplicity, Decision Transformer matches or exceeds
the performance of state-of-the-art model-free offline RL baselines on Atari,
OpenAI Gym, and Key-to-Door tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lili Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1"&gt;Kevin Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajeswaran_A/0/1/0/all/0/1"&gt;Aravind Rajeswaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kimin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1"&gt;Aditya Grover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laskin_M/0/1/0/all/0/1"&gt;Michael Laskin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1"&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivas_A/0/1/0/all/0/1"&gt;Aravind Srinivas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1"&gt;Igor Mordatch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Policy Gradient Methods for the Noisy Linear Quadratic Regulator over a Finite Horizon. (arXiv:2011.10300v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10300</id>
        <link href="http://arxiv.org/abs/2011.10300"/>
        <updated>2021-06-25T02:00:47.517Z</updated>
        <summary type="html"><![CDATA[We explore reinforcement learning methods for finding the optimal policy in
the linear quadratic regulator (LQR) problem. In particular, we consider the
convergence of policy gradient methods in the setting of known and unknown
parameters. We are able to produce a global linear convergence guarantee for
this approach in the setting of finite time horizon and stochastic state
dynamics under weak assumptions. The convergence of a projected policy gradient
method is also established in order to handle problems with constraints. We
illustrate the performance of the algorithm with two examples. The first
example is the optimal liquidation of a holding in an asset. We show results
for the case where we assume a model for the underlying dynamics and where we
apply the method to the data directly. The empirical evidence suggests that the
policy gradient method can learn the global optimal solution for a larger class
of stochastic systems containing the LQR framework and that it is more robust
with respect to model mis-specification when compared to a model-based
approach. The second example is an LQR system in a higher dimensional setting
with synthetic data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hambly_B/0/1/0/all/0/1"&gt;Ben Hambly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Renyuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Huining Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differential Privacy and Byzantine Resilience in SGD: Do They Add Up?. (arXiv:2102.08166v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08166</id>
        <link href="http://arxiv.org/abs/2102.08166"/>
        <updated>2021-06-25T02:00:47.512Z</updated>
        <summary type="html"><![CDATA[This paper addresses the problem of combining Byzantine resilience with
privacy in machine learning (ML). Specifically, we study if a distributed
implementation of the renowned Stochastic Gradient Descent (SGD) learning
algorithm is feasible with both differential privacy (DP) and
$(\alpha,f)$-Byzantine resilience. To the best of our knowledge, this is the
first work to tackle this problem from a theoretical point of view. A key
finding of our analyses is that the classical approaches to these two
(seemingly) orthogonal issues are incompatible. More precisely, we show that a
direct composition of these techniques makes the guarantees of the resulting
SGD algorithm depend unfavourably upon the number of parameters of the ML
model, making the training of large models practically infeasible. We validate
our theoretical results through numerical experiments on publicly-available
datasets; showing that it is impractical to ensure DP and Byzantine resilience
simultaneously.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guerraoui_R/0/1/0/all/0/1"&gt;Rachid Guerraoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1"&gt;Nirupam Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pinot_R/0/1/0/all/0/1"&gt;Rafa&amp;#xeb;l Pinot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rouault_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Rouault&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stephan_J/0/1/0/all/0/1"&gt;John Stephan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Instance-based Counterfactual Explanations for Time Series Classification. (arXiv:2009.13211v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.13211</id>
        <link href="http://arxiv.org/abs/2009.13211"/>
        <updated>2021-06-25T02:00:47.498Z</updated>
        <summary type="html"><![CDATA[In recent years, there has been a rapidly expanding focus on explaining the
predictions made by black-box AI systems that handle image and tabular data.
However, considerably less attention has been paid to explaining the
predictions of opaque AI systems handling time series data. In this paper, we
advance a novel model-agnostic, case-based technique -- Native Guide -- that
generates counterfactual explanations for time series classifiers. Given a
query time series, $T_{q}$, for which a black-box classification system
predicts class, $c$, a counterfactual time series explanation shows how $T_{q}$
could change, such that the system predicts an alternative class, $c'$. The
proposed instance-based technique adapts existing counterfactual instances in
the case-base by highlighting and modifying discriminative areas of the time
series that underlie the classification. Quantitative and qualitative results
from two comparative experiments indicate that Native Guide generates
plausible, proximal, sparse and diverse explanations that are better than those
produced by key benchmark counterfactual methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Delaney_E/0/1/0/all/0/1"&gt;Eoin Delaney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greene_D/0/1/0/all/0/1"&gt;Derek Greene&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keane_M/0/1/0/all/0/1"&gt;Mark T. Keane&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autoencoding Under Normalization Constraints. (arXiv:2105.05735v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05735</id>
        <link href="http://arxiv.org/abs/2105.05735"/>
        <updated>2021-06-25T02:00:47.492Z</updated>
        <summary type="html"><![CDATA[Likelihood is a standard estimate for outlier detection. The specific role of
the normalization constraint is to ensure that the out-of-distribution (OOD)
regime has a small likelihood when samples are learned using maximum
likelihood. Because autoencoders do not possess such a process of
normalization, they often fail to recognize outliers even when they are
obviously OOD. We propose the Normalized Autoencoder (NAE), a normalized
probabilistic model constructed from an autoencoder. The probability density of
NAE is defined using the reconstruction error of an autoencoder, which is
differently defined in the conventional energy-based model. In our model,
normalization is enforced by suppressing the reconstruction of negative
samples, significantly improving the outlier detection performance. Our
experimental results confirm the efficacy of NAE, both in detecting outliers
and in generating in-distribution samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1"&gt;Sangwoong Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noh_Y/0/1/0/all/0/1"&gt;Yung-Kyun Noh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_F/0/1/0/all/0/1"&gt;Frank Chongwoo Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Isotonic regression with unknown permutations: Statistics, computation, and adaptation. (arXiv:2009.02609v2 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.02609</id>
        <link href="http://arxiv.org/abs/2009.02609"/>
        <updated>2021-06-25T02:00:47.487Z</updated>
        <summary type="html"><![CDATA[Motivated by models for multiway comparison data, we consider the problem of
estimating a coordinate-wise isotonic function on the domain $[0, 1]^d$ from
noisy observations collected on a uniform lattice, but where the design points
have been permuted along each dimension. While the univariate and bivariate
versions of this problem have received significant attention, our focus is on
the multivariate case $d \geq 3$. We study both the minimax risk of estimation
(in empirical $L_2$ loss) and the fundamental limits of adaptation (quantified
by the adaptivity index) to a family of piecewise constant functions. We
provide a computationally efficient Mirsky partition estimator that is minimax
optimal while also achieving the smallest adaptivity index possible for
polynomial time procedures. Thus, from a worst-case perspective and in sharp
contrast to the bivariate case, the latent permutations in the model do not
introduce significant computational difficulties over and above vanilla
isotonic regression. On the other hand, the fundamental limits of adaptation
are significantly different with and without unknown permutations: Assuming a
hardness conjecture from average-case complexity theory, a
statistical-computational gap manifests in the former case. In a complementary
direction, we show that natural modifications of existing estimators fail to
satisfy at least one of the desiderata of optimal worst-case statistical
performance, computational efficiency, and fast adaptation. Along the way to
showing our results, we improve adaptation results in the special case $d = 2$
and establish some properties of estimators for vanilla isotonic regression,
both of which may be of independent interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Pananjady_A/0/1/0/all/0/1"&gt;Ashwin Pananjady&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Samworth_R/0/1/0/all/0/1"&gt;Richard J. Samworth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Federated Learning Approach to Anomaly Detection in Smart Buildings. (arXiv:2010.10293v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10293</id>
        <link href="http://arxiv.org/abs/2010.10293"/>
        <updated>2021-06-25T02:00:47.480Z</updated>
        <summary type="html"><![CDATA[Internet of Things (IoT) sensors in smart buildings are becoming increasingly
ubiquitous, making buildings more livable, energy efficient, and sustainable.
These devices sense the environment and generate multivariate temporal data of
paramount importance for detecting anomalies and improving the prediction of
energy usage in smart buildings. However, detecting these anomalies in
centralized systems is often plagued by a huge delay in response time. To
overcome this issue, we formulate the anomaly detection problem in a federated
learning setting by leveraging the multi-task learning paradigm, which aims at
solving multiple tasks simultaneously while taking advantage of the
similarities and differences across tasks. We propose a novel privacy-by-design
federated learning model using a stacked long short-time memory (LSTM) model,
and we demonstrate that it is more than twice as fast during training
convergence compared to the centralized LSTM. The effectiveness of our
federated learning approach is demonstrated on three real-world datasets
generated by the IoT production system at General Electric Current smart
building, achieving state-of-the-art performance compared to baseline methods
in both classification and regression tasks. Our experimental results
demonstrate the effectiveness of the proposed framework in reducing the overall
training cost without compromising the prediction performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sater_R/0/1/0/all/0/1"&gt;Raed Abdel Sater&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamza_A/0/1/0/all/0/1"&gt;A. Ben Hamza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rank $2r$ iterative least squares: efficient recovery of ill-conditioned low rank matrices from few entries. (arXiv:2002.01849v2 [math.OC] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2002.01849</id>
        <link href="http://arxiv.org/abs/2002.01849"/>
        <updated>2021-06-25T02:00:47.466Z</updated>
        <summary type="html"><![CDATA[We present a new, simple and computationally efficient iterative method for
low rank matrix completion. Our method is inspired by the class of
factorization-type iterative algorithms, but substantially differs from them in
the way the problem is cast. Precisely, given a target rank $r$, instead of
optimizing on the manifold of rank $r$ matrices, we allow our interim estimated
matrix to have a specific over-parametrized rank $2r$ structure. Our algorithm,
denoted R2RILS for rank $2r$ iterative least squares, has low memory
requirements, and at each iteration it solves a computationally cheap sparse
least-squares problem. We motivate our algorithm by its theoretical analysis
for the simplified case of a rank-1 matrix. Empirically, R2RILS is able to
recover ill conditioned low rank matrices from very few observations -- near
the information limit, and it is stable to additive noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Bauch_J/0/1/0/all/0/1"&gt;Jonathan Bauch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Nadler_B/0/1/0/all/0/1"&gt;Boaz Nadler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zilber_P/0/1/0/all/0/1"&gt;Pini Zilber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SALT: Sea lice Adaptive Lattice Tracking -- An Unsupervised Approach to Generate an Improved Ocean Model. (arXiv:2106.13202v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2106.13202</id>
        <link href="http://arxiv.org/abs/2106.13202"/>
        <updated>2021-06-25T02:00:47.455Z</updated>
        <summary type="html"><![CDATA[Warming oceans due to climate change are leading to increased numbers of
ectoparasitic copepods, also known as sea lice, which can cause significant
ecological loss to wild salmon populations and major economic loss to
aquaculture sites. The main transport mechanism driving the spread of sea lice
populations are near-surface ocean currents. Present strategies to estimate the
distribution of sea lice larvae are computationally complex and limit
full-scale analysis. Motivated to address this challenge, we propose SALT: Sea
lice Adaptive Lattice Tracking approach for efficient estimation of sea lice
dispersion and distribution in space and time. Specifically, an adaptive
spatial mesh is generated by merging nodes in the lattice graph of the Ocean
Model based on local ocean properties, thus enabling highly efficient graph
representation. SALT demonstrates improved efficiency while maintaining
consistent results with the standard method, using near-surface current data
for Hardangerfjord, Norway. The proposed SALT technique shows promise for
enhancing proactive aquaculture management through predictive modelling of sea
lice infestation pressure maps in a changing climate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Park_J/0/1/0/all/0/1"&gt;Ju An Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Voleti_V/0/1/0/all/0/1"&gt;Vikram Voleti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Thomas_K/0/1/0/all/0/1"&gt;Kathryn E. Thomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alexander Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Deglint_J/0/1/0/all/0/1"&gt;Jason L. Deglint&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wav2vec-C: A Self-supervised Model for Speech Representation Learning. (arXiv:2103.08393v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08393</id>
        <link href="http://arxiv.org/abs/2103.08393"/>
        <updated>2021-06-25T02:00:47.438Z</updated>
        <summary type="html"><![CDATA[Wav2vec-C introduces a novel representation learning technique combining
elements from wav2vec 2.0 and VQ-VAE. Our model learns to reproduce quantized
representations from partially masked speech encoding using a contrastive loss
in a way similar to Wav2vec 2.0. However, the quantization process is
regularized by an additional consistency network that learns to reconstruct the
input features to the wav2vec 2.0 network from the quantized representations in
a way similar to a VQ-VAE model. The proposed self-supervised model is trained
on 10k hours of unlabeled data and subsequently used as the speech encoder in a
RNN-T ASR model and fine-tuned with 1k hours of labeled data. This work is one
of only a few studies of self-supervised learning on speech tasks with a large
volume of real far-field labeled data. The Wav2vec-C encoded representations
achieves, on average, twice the error reduction over baseline and a higher
codebook utilization in comparison to wav2vec 2.0]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sadhu_S/0/1/0/all/0/1"&gt;Samik Sadhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+He_D/0/1/0/all/0/1"&gt;Di He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1"&gt;Che-Wei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mallidi_S/0/1/0/all/0/1"&gt;Sri Harish Mallidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_M/0/1/0/all/0/1"&gt;Minhua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rastrow_A/0/1/0/all/0/1"&gt;Ariya Rastrow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Stolcke_A/0/1/0/all/0/1"&gt;Andreas Stolcke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Droppo_J/0/1/0/all/0/1"&gt;Jasha Droppo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Maas_R/0/1/0/all/0/1"&gt;Roland Maas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonlinear Matrix Approximation with Radial Basis Function Components. (arXiv:2106.02018v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02018</id>
        <link href="http://arxiv.org/abs/2106.02018"/>
        <updated>2021-06-25T02:00:47.425Z</updated>
        <summary type="html"><![CDATA[We introduce and investigate matrix approximation by decomposition into a sum
of radial basis function (RBF) components. An RBF component is a generalization
of the outer product between a pair of vectors, where an RBF function replaces
the scalar multiplication between individual vector elements. Even though the
RBF functions are positive definite, the summation across components is not
restricted to convex combinations and allows us to compute the decomposition
for any real matrix that is not necessarily symmetric or positive definite. We
formulate the problem of seeking such a decomposition as an optimization
problem with a nonlinear and non-convex loss function. Several modern versions
of the gradient descent method, including their scalable stochastic
counterparts, are used to solve this problem. We provide extensive empirical
evidence of the effectiveness of the RBF decomposition and that of the
gradient-based fitting algorithm. While being conceptually motivated by
singular value decomposition (SVD), our proposed nonlinear counterpart
outperforms SVD by drastically reducing the memory required to approximate a
data matrix with the same L2 error for a wide range of matrix types. For
example, it leads to 2 to 6 times memory save for Gaussian noise, graph
adjacency matrices, and kernel matrices. Moreover, this proximity-based
decomposition can offer additional interpretability in applications that
involve, e.g., capturing the inner low-dimensional structure of the data,
retaining graph connectivity structure, and preserving the acutance of images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rebrova_E/0/1/0/all/0/1"&gt;Elizaveta Rebrova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yu-Hang Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Q-space Conditioned Translation Networks for Directional Synthesis of Diffusion Weighted Images from Multi-modal Structural MRI. (arXiv:2106.13188v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.13188</id>
        <link href="http://arxiv.org/abs/2106.13188"/>
        <updated>2021-06-25T02:00:47.419Z</updated>
        <summary type="html"><![CDATA[Current deep learning approaches for diffusion MRI modeling circumvent the
need for densely-sampled diffusion-weighted images (DWIs) by directly
predicting microstructural indices from sparsely-sampled DWIs. However, they
implicitly make unrealistic assumptions of static $q$-space sampling during
training and reconstruction. Further, such approaches can restrict downstream
usage of variably sampled DWIs for usages including the estimation of
microstructural indices or tractography. We propose a generative adversarial
translation framework for high-quality DWI synthesis with arbitrary $q$-space
sampling given commonly acquired structural images (e.g., B0, T1, T2). Our
translation network linearly modulates its internal representations conditioned
on continuous $q$-space information, thus removing the need for fixed sampling
schemes. Moreover, this approach enables downstream estimation of high-quality
microstructural maps from arbitrarily subsampled DWIs, which may be
particularly important in cases with sparsely sampled DWIs. Across several
recent methodologies, the proposed approach yields improved DWI synthesis
accuracy and fidelity with enhanced downstream utility as quantified by the
accuracy of scalar microstructure indices estimated from the synthesized
images. Code is available at
https://github.com/mengweiren/q-space-conditioned-dwi-synthesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ren_M/0/1/0/all/0/1"&gt;Mengwei Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1"&gt;Heejong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dey_N/0/1/0/all/0/1"&gt;Neel Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gerig_G/0/1/0/all/0/1"&gt;Guido Gerig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PocketNet: A Smaller Neural Network for Medical Image Analysis. (arXiv:2104.10745v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10745</id>
        <link href="http://arxiv.org/abs/2104.10745"/>
        <updated>2021-06-25T02:00:47.411Z</updated>
        <summary type="html"><![CDATA[Medical imaging deep learning models are often large and complex, requiring
specialized hardware to train and evaluate these models. To address such
issues, we propose the PocketNet paradigm to reduce the size of deep learning
models by throttling the growth of the number of channels in convolutional
neural networks. We demonstrate that, for a range of segmentation and
classification tasks, PocketNet architectures produce results comparable to
that of conventional neural networks while reducing the number of parameters by
multiple orders of magnitude, using up to 90% less GPU memory, and speeding up
training times by up to 40%, thereby allowing such models to be trained and
deployed in resource-constrained settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Celaya_A/0/1/0/all/0/1"&gt;Adrian Celaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Actor_J/0/1/0/all/0/1"&gt;Jonas A. Actor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Muthusivarajan_R/0/1/0/all/0/1"&gt;Rajarajeswari Muthusivarajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gates_E/0/1/0/all/0/1"&gt;Evan Gates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chung_C/0/1/0/all/0/1"&gt;Caroline Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schellingerhout_D/0/1/0/all/0/1"&gt;Dawid Schellingerhout&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Riviere_B/0/1/0/all/0/1"&gt;Beatrice Riviere&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fuentes_D/0/1/0/all/0/1"&gt;David Fuentes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Playtesting Coverage via Curiosity Driven Reinforcement Learning Agents. (arXiv:2103.13798v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13798</id>
        <link href="http://arxiv.org/abs/2103.13798"/>
        <updated>2021-06-25T02:00:47.406Z</updated>
        <summary type="html"><![CDATA[As modern games continue growing both in size and complexity, it has become
more challenging to ensure that all the relevant content is tested and that any
potential issue is properly identified and fixed. Attempting to maximize
testing coverage using only human participants, however, results in a tedious
and hard to orchestrate process which normally slows down the development
cycle. Complementing playtesting via autonomous agents has shown great promise
accelerating and simplifying this process. This paper addresses the problem of
automatically exploring and testing a given scenario using reinforcement
learning agents trained to maximize game state coverage. Each of these agents
is rewarded based on the novelty of its actions, thus encouraging a curious and
exploratory behaviour on a complex 3D scenario where previously proposed
exploration techniques perform poorly. The curious agents are able to learn the
complex navigation mechanics required to reach the different areas around the
map, thus providing the necessary data to identify potential issues. Moreover,
the paper also explores different visualization strategies and evaluates how to
make better use of the collected data to drive design decisions and to
recognize possible problems and oversights.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gordillo_C/0/1/0/all/0/1"&gt;Camilo Gordillo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bergdahl_J/0/1/0/all/0/1"&gt;Joakim Bergdahl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tollmar_K/0/1/0/all/0/1"&gt;Konrad Tollmar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gisslen_L/0/1/0/all/0/1"&gt;Linus Gissl&amp;#xe9;n&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy Threats Analysis to Secure Federated Learning. (arXiv:2106.13076v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13076</id>
        <link href="http://arxiv.org/abs/2106.13076"/>
        <updated>2021-06-25T02:00:47.389Z</updated>
        <summary type="html"><![CDATA[Federated learning is emerging as a machine learning technique that trains a
model across multiple decentralized parties. It is renowned for preserving
privacy as the data never leaves the computational devices, and recent
approaches further enhance its privacy by hiding messages transferred in
encryption. However, we found that despite the efforts, federated learning
remains privacy-threatening, due to its interactive nature across different
parties. In this paper, we analyze the privacy threats in industrial-level
federated learning frameworks with secure computation, and reveal such threats
widely exist in typical machine learning models such as linear regression,
logistic regression and decision tree. For the linear and logistic regression,
we show through theoretical analysis that it is possible for the attacker to
invert the entire private input of the victim, given very few information. For
the decision tree model, we launch an attack to infer the range of victim's
private inputs. All attacks are evaluated on popular federated learning
frameworks and real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuchen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1"&gt;Yifan Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1"&gt;Liyao Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Junhan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Cen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Li Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinbing Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IA-RED$^2$: Interpretability-Aware Redundancy Reduction for Vision Transformers. (arXiv:2106.12620v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12620</id>
        <link href="http://arxiv.org/abs/2106.12620"/>
        <updated>2021-06-25T02:00:47.371Z</updated>
        <summary type="html"><![CDATA[The self-attention-based model, transformer, is recently becoming the leading
backbone in the field of computer vision. In spite of the impressive success
made by transformers in a variety of vision tasks, it still suffers from heavy
computation and intensive memory cost. To address this limitation, this paper
presents an Interpretability-Aware REDundancy REDuction framework (IA-RED$^2$).
We start by observing a large amount of redundant computation, mainly spent on
uncorrelated input patches, and then introduce an interpretable module to
dynamically and gracefully drop these redundant patches. This novel framework
is then extended to a hierarchical structure, where uncorrelated tokens at
different stages are gradually removed, resulting in a considerable shrinkage
of computational cost. We include extensive experiments on both image and video
tasks, where our method could deliver up to 1.4X speed-up for state-of-the-art
models like DeiT and TimeSformer, by only sacrificing less than 0.7% accuracy.
More importantly, contrary to other acceleration approaches, our method is
inherently interpretable with substantial visual evidence, making vision
transformer closer to a more human-understandable architecture while being
lighter. We demonstrate that the interpretability that naturally emerged in our
framework can outperform the raw attention learned by the original visual
transformer, as well as those generated by off-the-shelf interpretation
methods, with both qualitative and quantitative results. Project Page:
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1"&gt;Bowen Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yifan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1"&gt;Rameswar Panda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1"&gt;Rogerio Feris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliva_A/0/1/0/all/0/1"&gt;Aude Oliva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Planning with Exploration: Addressing Dynamics Bottleneck in Model-based Reinforcement Learning. (arXiv:2010.12914v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12914</id>
        <link href="http://arxiv.org/abs/2010.12914"/>
        <updated>2021-06-25T02:00:47.357Z</updated>
        <summary type="html"><![CDATA[Model-based reinforcement learning (MBRL) is believed to have higher sample
efficiency compared with model-free reinforcement learning (MFRL). However,
MBRL is plagued by dynamics bottleneck dilemma. Dynamics bottleneck dilemma is
the phenomenon that the performance of the algorithm falls into the local
optimum instead of increasing when the interaction step with the environment
increases, which means more data can not bring better performance. In this
paper, we find that the trajectory reward estimation error is the main reason
that causes dynamics bottleneck dilemma through theoretical analysis. We give
an upper bound of the trajectory reward estimation error and point out that
increasing the agent's exploration ability is the key to reduce trajectory
reward estimation error, thereby alleviating dynamics bottleneck dilemma.
Motivated by this, a model-based control method combined with exploration named
MOdel-based Progressive Entropy-based Exploration (MOPE2) is proposed. We
conduct experiments on several complex continuous control benchmark tasks. The
results verify that MOPE2 can effectively alleviate dynamics bottleneck dilemma
and have higher sample efficiency than previous MBRL and MFRL algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiyao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Junge Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wenzhen Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1"&gt;Qiyue Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fea2Fea: Exploring Structural Feature Correlations via Graph Neural Networks. (arXiv:2106.13061v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13061</id>
        <link href="http://arxiv.org/abs/2106.13061"/>
        <updated>2021-06-25T02:00:47.351Z</updated>
        <summary type="html"><![CDATA[Structural features are important features in graph datasets. However,
although there are some correlation analysis of features based on covariance,
there is no relevant research on exploring structural feature correlation on
graphs with graph neural network based models. In this paper, we introduce
graph feature to feature (Fea2Fea) prediction pipelines in a low dimensional
space to explore some preliminary results on structural feature correlation,
which is based on graph neural network. The results show that there exists high
correlation between some of the structural features. A redundant feature
combination with initial node features, which is filtered by graph neural
network has improved its classification accuracy in some graph datasets. We
compare the difference between concatenation methods on connecting embeddings
between features and show that the simplest is the best. We generalize on the
synthetic geometric graphs and certify the results on prediction difficulty
between two structural features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiaqing Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1"&gt;Rex Ying&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time Spatio-temporal Event Detection on Geotagged Social Media. (arXiv:2106.13121v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.13121</id>
        <link href="http://arxiv.org/abs/2106.13121"/>
        <updated>2021-06-25T02:00:47.336Z</updated>
        <summary type="html"><![CDATA[A key challenge in mining social media data streams is to identify events
which are actively discussed by a group of people in a specific local or global
area. Such events are useful for early warning for accident, protest, election
or breaking news. However, neither the list of events nor the resolution of
both event time and space is fixed or known beforehand. In this work, we
propose an online spatio-temporal event detection system using social media
that is able to detect events at different time and space resolutions. First,
to address the challenge related to the unknown spatial resolution of events, a
quad-tree method is exploited in order to split the geographical space into
multiscale regions based on the density of social media data. Then, a
statistical unsupervised approach is performed that involves Poisson
distribution and a smoothing method for highlighting regions with unexpected
density of social posts. Further, event duration is precisely estimated by
merging events happening in the same region at consecutive time intervals. A
post processing stage is introduced to filter out events that are spam, fake or
wrong. Finally, we incorporate simple semantics by using social media entities
to assess the integrity, and accuracy of detected events. The proposed method
is evaluated using different social media datasets: Twitter and Flickr for
different cities: Melbourne, London, Paris and New York. To verify the
effectiveness of the proposed method, we compare our results with two baseline
algorithms based on fixed split of geographical space and clustering method.
For performance evaluation, we manually compute recall and precision. We also
propose a new quality measure named strength index, which automatically
measures how accurate the reported event is.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+George_Y/0/1/0/all/0/1"&gt;Yasmeen George&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karunasekera_S/0/1/0/all/0/1"&gt;Shanika Karunasekera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harwood_A/0/1/0/all/0/1"&gt;Aaron Harwood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1"&gt;Kwan Hui Lim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alternative Microfoundations for Strategic Classification. (arXiv:2106.12705v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12705</id>
        <link href="http://arxiv.org/abs/2106.12705"/>
        <updated>2021-06-25T02:00:47.330Z</updated>
        <summary type="html"><![CDATA[When reasoning about strategic behavior in a machine learning context it is
tempting to combine standard microfoundations of rational agents with the
statistical decision theory underlying classification. In this work, we argue
that a direct combination of these standard ingredients leads to brittle
solution concepts of limited descriptive and prescriptive value. First, we show
that rational agents with perfect information produce discontinuities in the
aggregate response to a decision rule that we often do not observe empirically.
Second, when any positive fraction of agents is not perfectly strategic,
desirable stable points -- where the classifier is optimal for the data it
entails -- cease to exist. Third, optimal decision rules under standard
microfoundations maximize a measure of negative externality known as social
burden within a broad class of possible assumptions about agent behavior.

Recognizing these limitations we explore alternatives to standard
microfoundations for binary classification. We start by describing a set of
desiderata that help navigate the space of possible assumptions about how
agents respond to a decision rule. In particular, we analyze a natural
constraint on feature manipulations, and discuss properties that are sufficient
to guarantee the robust existence of stable points. Building on these insights,
we then propose the noisy response model. Inspired by smoothed analysis and
empirical observations, noisy response incorporates imperfection in the agent
responses, which we show mitigates the limitations of standard
microfoundations. Our model retains analytical tractability, leads to more
robust insights about stable points, and imposes a lower social burden at
optimality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jagadeesan_M/0/1/0/all/0/1"&gt;Meena Jagadeesan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mendler_Dunner_C/0/1/0/all/0/1"&gt;Celestine Mendler-D&amp;#xfc;nner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hardt_M/0/1/0/all/0/1"&gt;Moritz Hardt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A comprehensive empirical analysis on cross-domain semantic enrichment for detection of depressive language. (arXiv:2106.12797v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12797</id>
        <link href="http://arxiv.org/abs/2106.12797"/>
        <updated>2021-06-25T02:00:47.325Z</updated>
        <summary type="html"><![CDATA[We analyze the process of creating word embedding feature representations
designed for a learning task when annotated data is scarce, for example, in
depressive language detection from Tweets. We start with a rich word embedding
pre-trained from a large general dataset, which is then augmented with
embeddings learned from a much smaller and more specific domain dataset through
a simple non-linear mapping mechanism. We also experimented with several other
more sophisticated methods of such mapping including, several auto-encoder
based and custom loss-function based methods that learn embedding
representations through gradually learning to be close to the words of similar
semantics and distant to dissimilar semantics. Our strengthened representations
better capture the semantics of the depression domain, as it combines the
semantics learned from the specific domain coupled with word coverage from the
general language. We also present a comparative performance analyses of our
word embedding representations with a simple bag-of-words model, well known
sentiment and psycholinguistic lexicons, and a general pre-trained word
embedding. When used as feature representations for several different machine
learning methods, including deep learning models in a depressive Tweets
identification task, we show that our augmented word embedding representations
achieve a significantly better F1 score than the others, specially when applied
to a high quality dataset. Also, we present several data ablation tests which
confirm the efficacy of our augmentation techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farruque_N/0/1/0/all/0/1"&gt;Nawshad Farruque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1"&gt;Randy Goebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1"&gt;Osmar Zaiane&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Long-term Cross Adversarial Training: A Robust Meta-learning Method for Few-shot Classification Tasks. (arXiv:2106.12900v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12900</id>
        <link href="http://arxiv.org/abs/2106.12900"/>
        <updated>2021-06-25T02:00:47.304Z</updated>
        <summary type="html"><![CDATA[Meta-learning model can quickly adapt to new tasks using few-shot labeled
data. However, despite achieving good generalization on few-shot classification
tasks, it is still challenging to improve the adversarial robustness of the
meta-learning model in few-shot learning. Although adversarial training (AT)
methods such as Adversarial Query (AQ) can improve the adversarially robust
performance of meta-learning models, AT is still computationally expensive
training. On the other hand, meta-learning models trained with AT will drop
significant accuracy on the original clean images. This paper proposed a
meta-learning method on the adversarially robust neural network called
Long-term Cross Adversarial Training (LCAT). LCAT will update meta-learning
model parameters cross along the natural and adversarial sample distribution
direction with long-term to improve both adversarial and clean few-shot
classification accuracy. Due to cross-adversarial training, LCAT only needs
half of the adversarial training epoch than AQ, resulting in a low adversarial
training computation. Experiment results show that LCAT achieves superior
performance both on the clean and adversarial few-shot classification accuracy
than SOTA adversarial training methods for meta-learning models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Shuyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1"&gt;Xuelong Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1"&gt;Bin Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safe Learning and Optimization Techniques: Towards a Survey of the State of the Art. (arXiv:2101.09505v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09505</id>
        <link href="http://arxiv.org/abs/2101.09505"/>
        <updated>2021-06-25T02:00:47.287Z</updated>
        <summary type="html"><![CDATA[Safe learning and optimization deals with learning and optimization problems
that avoid, as much as possible, the evaluation of non-safe input points, which
are solutions, policies, or strategies that cause an irrecoverable loss (e.g.,
breakage of a machine or equipment, or life threat). Although a comprehensive
survey of safe reinforcement learning algorithms was published in 2015, a
number of new algorithms have been proposed thereafter, and related works in
active learning and in optimization were not considered. This paper reviews
those algorithms from a number of domains including reinforcement learning,
Gaussian process regression and classification, evolutionary algorithms, and
active learning. We provide the fundamental concepts on which the reviewed
algorithms are based and a characterization of the individual algorithms. We
conclude by explaining how the algorithms are connected and suggestions for
future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Youngmin Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Allmendinger_R/0/1/0/all/0/1"&gt;Richard Allmendinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lopez_Ibanez_M/0/1/0/all/0/1"&gt;Manuel L&amp;#xf3;pez-Ib&amp;#xe1;&amp;#xf1;ez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Fast Sampling of Diffusion Probabilistic Models. (arXiv:2106.00132v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00132</id>
        <link href="http://arxiv.org/abs/2106.00132"/>
        <updated>2021-06-25T02:00:47.272Z</updated>
        <summary type="html"><![CDATA[In this work, we propose FastDPM, a unified framework for fast sampling in
diffusion probabilistic models. FastDPM generalizes previous methods and gives
rise to new algorithms with improved sample quality. We systematically
investigate the fast sampling methods under this framework across different
domains, on different datasets, and with different amount of conditional
information provided for generation. We find the performance of a particular
method depends on data domains (e.g., image or audio), the trade-off between
sampling speed and sample quality, and the amount of conditional information.
We further provide insights and recipes on the choice of methods for
practitioners.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1"&gt;Zhifeng Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1"&gt;Wei Ping&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of deep lift pose models for 3D rodent pose estimation based on geometrically triangulated data. (arXiv:2106.12993v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12993</id>
        <link href="http://arxiv.org/abs/2106.12993"/>
        <updated>2021-06-25T02:00:47.267Z</updated>
        <summary type="html"><![CDATA[The assessment of laboratory animal behavior is of central interest in modern
neuroscience research. Behavior is typically studied in terms of pose changes,
which are ideally captured in three dimensions. This requires triangulation
over a multi-camera system which view the animal from different angles.
However, this is challenging in realistic laboratory setups due to occlusions
and other technical constrains. Here we propose the usage of lift-pose models
that allow for robust 3D pose estimation of freely moving rodents from a single
view camera view. To obtain high-quality training data for the pose-lifting, we
first perform geometric calibration in a camera setup involving bottom as well
as side views of the behaving animal. We then evaluate the performance of two
previously proposed model architectures under given inference perspectives and
conclude that reliable 3D pose inference can be obtained using temporal
convolutions. With this work we would like to contribute to a more robust and
diverse behavior tracking of freely moving rodents for a wide range of
experiments and setups in the neuroscience community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_I/0/1/0/all/0/1"&gt;Indrani Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maji_I/0/1/0/all/0/1"&gt;Indranil Maji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Omprakash_C/0/1/0/all/0/1"&gt;Charitha Omprakash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stober_S/0/1/0/all/0/1"&gt;Sebastian Stober&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mikulovic_S/0/1/0/all/0/1"&gt;Sanja Mikulovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauer_P/0/1/0/all/0/1"&gt;Pavol Bauer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large Scale Private Learning via Low-rank Reparametrization. (arXiv:2106.09352v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09352</id>
        <link href="http://arxiv.org/abs/2106.09352"/>
        <updated>2021-06-25T02:00:47.261Z</updated>
        <summary type="html"><![CDATA[We propose a reparametrization scheme to address the challenges of applying
differentially private SGD on large neural networks, which are 1) the huge
memory cost of storing individual gradients, 2) the added noise suffering
notorious dimensional dependence. Specifically, we reparametrize each weight
matrix with two \emph{gradient-carrier} matrices of small dimension and a
\emph{residual weight} matrix. We argue that such reparametrization keeps the
forward/backward process unchanged while enabling us to compute the projected
gradient without computing the gradient itself. To learn with differential
privacy, we design \emph{reparametrized gradient perturbation (RGP)} that
perturbs the gradients on gradient-carrier matrices and reconstructs an update
for the original weight from the noisy gradients. Importantly, we use
historical updates to find the gradient-carrier matrices, whose optimality is
rigorously justified under linear regression and empirically verified with deep
learning tasks. RGP significantly reduces the memory cost and improves the
utility. For example, we are the first able to apply differential privacy on
the BERT model and achieve an average accuracy of $83.9\%$ on four downstream
tasks with $\epsilon=8$, which is within $5\%$ loss compared to the non-private
baseline but enjoys much lower privacy leakage risk.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1"&gt;Da Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huishuai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1"&gt;Jian Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Dialogue Learning for Spoken Conversational Question Answering. (arXiv:2106.02182v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02182</id>
        <link href="http://arxiv.org/abs/2106.02182"/>
        <updated>2021-06-25T02:00:47.230Z</updated>
        <summary type="html"><![CDATA[In spoken conversational question answering (SCQA), the answer to the
corresponding question is generated by retrieving and then analyzing a fixed
spoken document, including multi-part conversations. Most SCQA systems have
considered only retrieving information from ordered utterances. However, the
sequential order of dialogue is important to build a robust spoken
conversational question answering system, and the changes of utterances order
may severely result in low-quality and incoherent corpora. To this end, we
introduce a self-supervised learning approach, including incoherence
discrimination, insertion detection, and question prediction, to explicitly
capture the coreference resolution and dialogue coherence among spoken
documents. Specifically, we design a joint learning framework where the
auxiliary self-supervised tasks can enable the pre-trained SCQA systems towards
more coherent and meaningful spoken dialogue learning. We also utilize the
proposed self-supervised learning tasks to capture intra-sentence coherence.
Experimental results demonstrate that our proposed method provides more
coherent, meaningful, and appropriate responses, yielding superior performance
gains compared to the original pre-trained language models. Our method achieves
state-of-the-art results on the Spoken-CoQA dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenyu You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unifying Gradient Estimators for Meta-Reinforcement Learning via Off-Policy Evaluation. (arXiv:2106.13125v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13125</id>
        <link href="http://arxiv.org/abs/2106.13125"/>
        <updated>2021-06-25T02:00:47.219Z</updated>
        <summary type="html"><![CDATA[Model-agnostic meta-reinforcement learning requires estimating the Hessian
matrix of value functions. This is challenging from an implementation
perspective, as repeatedly differentiating policy gradient estimates may lead
to biased Hessian estimates. In this work, we provide a unifying framework for
estimating higher-order derivatives of value functions, based on off-policy
evaluation. Our framework interprets a number of prior approaches as special
cases and elucidates the bias and variance trade-off of Hessian estimates. This
framework also opens the door to a new family of estimates, which can be easily
implemented with auto-differentiation libraries, and lead to performance gains
in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yunhao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kozuno_T/0/1/0/all/0/1"&gt;Tadashi Kozuno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rowland_M/0/1/0/all/0/1"&gt;Mark Rowland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Munos_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Munos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valko_M/0/1/0/all/0/1"&gt;Michal Valko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shallow Representation is Deep: Learning Uncertainty-aware and Worst-case Random Feature Dynamics. (arXiv:2106.13066v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13066</id>
        <link href="http://arxiv.org/abs/2106.13066"/>
        <updated>2021-06-25T02:00:47.188Z</updated>
        <summary type="html"><![CDATA[Random features is a powerful universal function approximator that inherits
the theoretical rigor of kernel methods and can scale up to modern learning
tasks. This paper views uncertain system models as unknown or uncertain smooth
functions in universal reproducing kernel Hilbert spaces. By directly
approximating the one-step dynamics function using random features with
uncertain parameters, which are equivalent to a shallow Bayesian neural
network, we then view the whole dynamical system as a multi-layer neural
network. Exploiting the structure of Hamiltonian dynamics, we show that finding
worst-case dynamics realizations using Pontryagin's minimum principle is
equivalent to performing the Frank-Wolfe algorithm on the deep net. Various
numerical experiments on dynamics learning showcase the capacity of our
modeling methodology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agudelo_Espana_D/0/1/0/all/0/1"&gt;Diego Agudelo-Espa&amp;#xf1;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nemmour_Y/0/1/0/all/0/1"&gt;Yassine Nemmour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jia-Jie Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine learning to tame divergent density functional approximations: a new path to consensus materials design principles. (arXiv:2106.13109v1 [cond-mat.mtrl-sci])]]></title>
        <id>http://arxiv.org/abs/2106.13109</id>
        <link href="http://arxiv.org/abs/2106.13109"/>
        <updated>2021-06-25T02:00:47.183Z</updated>
        <summary type="html"><![CDATA[Computational virtual high-throughput screening (VHTS) with density
functional theory (DFT) and machine-learning (ML)-acceleration is essential in
rapid materials discovery. By necessity, efficient DFT-based workflows are
carried out with a single density functional approximation (DFA). Nevertheless,
properties evaluated with different DFAs can be expected to disagree for the
cases with challenging electronic structure (e.g., open shell transition metal
complexes, TMCs) for which rapid screening is most needed and accurate
benchmarks are often unavailable. To quantify the effect of DFA bias, we
introduce an approach to rapidly obtain property predictions from 23
representative DFAs spanning multiple families and "rungs" (e.g., semi-local to
double hybrid) and basis sets on over 2,000 TMCs. Although computed properties
(e.g., spin-state ordering and frontier orbital gap) naturally differ by DFA,
high linear correlations persist across all DFAs. We train independent ML
models for each DFA and observe convergent trends in feature importance; these
features thus provide DFA-invariant, universal design rules. We devise a
strategy to train ML models informed by all 23 DFAs and use them to predict
properties (e.g., spin-splitting energy) of over 182k TMCs. By requiring
consensus of the ANN-predicted DFA properties, we improve correspondence of
these computational lead compounds with literature-mined, experimental
compounds over the single-DFA approach typically employed. Both feature
analysis and consensus-based ML provide efficient, alternative paths to
overcome accuracy limitations of practical DFT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Duan_C/0/1/0/all/0/1"&gt;Chenru Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shuxin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Taylor_M/0/1/0/all/0/1"&gt;Michael G. Taylor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Kulik_H/0/1/0/all/0/1"&gt;Heather J. Kulik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Federated Learning with Clustered Generalization. (arXiv:2106.13044v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13044</id>
        <link href="http://arxiv.org/abs/2106.13044"/>
        <updated>2021-06-25T02:00:47.172Z</updated>
        <summary type="html"><![CDATA[We study the recent emerging personalized federated learning (PFL) that aims
at dealing with the challenging problem of Non-I.I.D. data in the federated
learning (FL) setting. The key difference between PFL and conventional FL lies
in the training target, of which the personalized models in PFL usually pursue
a trade-off between personalization (i.e., usually from local models) and
generalization (i.e., usually from the global model) on trained models.
Conventional FL methods can hardly meet this target because of their both
well-developed global and local models. The prevalent PFL approaches usually
maintain a global model to guide the training process of local models and
transfer a proper degree of generalization to them. However, the sole global
model can only provide one direction of generalization and may even transfer
negative effects to some local models when rich statistical diversity exists
across multiple local datasets. Based on our observation, most real or
synthetic data distributions usually tend to be clustered to some degree, of
which we argue different directions of generalization can facilitate the PFL.
In this paper, we propose a novel concept called clustered generalization to
handle the challenge of statistical heterogeneity in FL. Specifically, we
maintain multiple global (generalized) models in the server to associate with
the corresponding amount of local model clusters in clients, and further
formulate the PFL as a bi-level optimization problem that can be solved
efficiently and robustly. We also conduct detailed theoretical analysis and
provide the convergence guarantee for the smooth non-convex objectives.
Experimental results on both synthetic and real datasets show that our approach
surpasses the state-of-the-art by a significant margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xueyang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1"&gt;Song Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jingcai Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Option Keyboard: Combining Skills in Reinforcement Learning. (arXiv:2106.13105v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.13105</id>
        <link href="http://arxiv.org/abs/2106.13105"/>
        <updated>2021-06-25T02:00:47.167Z</updated>
        <summary type="html"><![CDATA[The ability to combine known skills to create new ones may be crucial in the
solution of complex reinforcement learning problems that unfold over extended
periods. We argue that a robust way of combining skills is to define and
manipulate them in the space of pseudo-rewards (or "cumulants"). Based on this
premise, we propose a framework for combining skills using the formalism of
options. We show that every deterministic option can be unambiguously
represented as a cumulant defined in an extended domain. Building on this
insight and on previous results on transfer learning, we show how to
approximate options whose cumulants are linear combinations of the cumulants of
known options. This means that, once we have learned options associated with a
set of cumulants, we can instantaneously synthesise options induced by any
linear combination of them, without any learning involved. We describe how this
framework provides a hierarchical interface to the environment whose abstract
actions correspond to combinations of basic skills. We demonstrate the
practical benefits of our approach in a resource management problem and a
navigation task involving a quadrupedal simulated robot.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barreto_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Barreto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borsa_D/0/1/0/all/0/1"&gt;Diana Borsa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_S/0/1/0/all/0/1"&gt;Shaobo Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Comanici_G/0/1/0/all/0/1"&gt;Gheorghe Comanici&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aygun_E/0/1/0/all/0/1"&gt;Eser Ayg&amp;#xfc;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamel_P/0/1/0/all/0/1"&gt;Philippe Hamel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toyama_D/0/1/0/all/0/1"&gt;Daniel Toyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hunt_J/0/1/0/all/0/1"&gt;Jonathan Hunt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mourad_S/0/1/0/all/0/1"&gt;Shibl Mourad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silver_D/0/1/0/all/0/1"&gt;David Silver&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1"&gt;Doina Precup&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Learning of Depth and Depth-of-Field Effect from Natural Images with Aperture Rendering Generative Adversarial Networks. (arXiv:2106.13041v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13041</id>
        <link href="http://arxiv.org/abs/2106.13041"/>
        <updated>2021-06-25T02:00:47.161Z</updated>
        <summary type="html"><![CDATA[Understanding the 3D world from 2D projected natural images is a fundamental
challenge in computer vision and graphics. Recently, an unsupervised learning
approach has garnered considerable attention owing to its advantages in data
collection. However, to mitigate training limitations, typical methods need to
impose assumptions for viewpoint distribution (e.g., a dataset containing
various viewpoint images) or object shape (e.g., symmetric objects). These
assumptions often restrict applications; for instance, the application to
non-rigid objects or images captured from similar viewpoints (e.g., flower or
bird images) remains a challenge. To complement these approaches, we propose
aperture rendering generative adversarial networks (AR-GANs), which equip
aperture rendering on top of GANs, and adopt focus cues to learn the depth and
depth-of-field (DoF) effect of unlabeled natural images. To address the
ambiguities triggered by unsupervised setting (i.e., ambiguities between smooth
texture and out-of-focus blurs, and between foreground and background blurs),
we develop DoF mixture learning, which enables the generator to learn real
image distribution while generating diverse DoF images. In addition, we devise
a center focus prior to guiding the learning direction. In the experiments, we
demonstrate the effectiveness of AR-GANs in various datasets, such as flower,
bird, and face images, demonstrate their portability by incorporating them into
other 3D representation learning GANs, and validate their applicability in
shallow DoF rendering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaneko_T/0/1/0/all/0/1"&gt;Takuhiro Kaneko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artifact Detection and Correction in EEG data: A Review. (arXiv:2106.13081v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.13081</id>
        <link href="http://arxiv.org/abs/2106.13081"/>
        <updated>2021-06-25T02:00:47.145Z</updated>
        <summary type="html"><![CDATA[Electroencephalography (EEG) has countless applications across many of
fields. However, EEG applications are limited by low signal-to-noise ratios.
Multiple types of artifacts contribute to the noisiness of EEG, and many
techniques have been proposed to detect and correct these artifacts. These
techniques range from simply detecting and rejecting artifact ridden segments,
to extracting the noise component from the EEG signal. In this paper we review
a variety of recent and classical techniques for EEG data artifact detection
and correction with a focus on the last half-decade. We compare the strengths
and weaknesses of the approaches and conclude with proposed future directions
for the field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sadiya_S/0/1/0/all/0/1"&gt;S Sadiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alhanai_T/0/1/0/all/0/1"&gt;T Alhanai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ghassemi_M/0/1/0/all/0/1"&gt;MM Ghassemi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Learning Approach to Private Data Sharing of Medical Images Using Conditional GANs. (arXiv:2106.13199v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13199</id>
        <link href="http://arxiv.org/abs/2106.13199"/>
        <updated>2021-06-25T02:00:47.140Z</updated>
        <summary type="html"><![CDATA[Sharing data from clinical studies can facilitate innovative data-driven
research and ultimately lead to better public health. However, sharing
biomedical data can put sensitive personal information at risk. This is usually
solved by anonymization, which is a slow and expensive process. An alternative
to anonymization is sharing a synthetic dataset that bears a behaviour similar
to the real data but preserves privacy. As part of the collaboration between
Novartis and the Oxford Big Data Institute, we generate a synthetic dataset
based on COSENTYX (secukinumab) Ankylosing Spondylitis (AS) clinical study. We
apply an Auxiliary Classifier GAN (ac-GAN) to generate synthetic magnetic
resonance images (MRIs) of vertebral units (VUs). The images are conditioned on
the VU location (cervical, thoracic and lumbar). In this paper, we present a
method for generating a synthetic dataset and conduct an in-depth analysis on
its properties of along three key metrics: image fidelity, sample diversity and
dataset privacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Hanxi Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plawinski_J/0/1/0/all/0/1"&gt;Jason Plawinski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Subramaniam_S/0/1/0/all/0/1"&gt;Sajanth Subramaniam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jamaludin_A/0/1/0/all/0/1"&gt;Amir Jamaludin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadir_T/0/1/0/all/0/1"&gt;Timor Kadir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Readie_A/0/1/0/all/0/1"&gt;Aimee Readie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ligozio_G/0/1/0/all/0/1"&gt;Gregory Ligozio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ohlssen_D/0/1/0/all/0/1"&gt;David Ohlssen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baillie_M/0/1/0/all/0/1"&gt;Mark Baillie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coroller_T/0/1/0/all/0/1"&gt;Thibaud Coroller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Software for Dataset-wide XAI: From Local Explanations to Global Insights with Zennit, CoRelAy, and ViRelAy. (arXiv:2106.13200v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13200</id>
        <link href="http://arxiv.org/abs/2106.13200"/>
        <updated>2021-06-25T02:00:47.135Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) are known to be strong predictors, but their
prediction strategies can rarely be understood. With recent advances in
Explainable Artificial Intelligence, approaches are available to explore the
reasoning behind those complex models' predictions. One class of approaches are
post-hoc attribution methods, among which Layer-wise Relevance Propagation
(LRP) shows high performance. However, the attempt at understanding a DNN's
reasoning often stops at the attributions obtained for individual samples in
input space, leaving the potential for deeper quantitative analyses untouched.
As a manual analysis without the right tools is often unnecessarily labor
intensive, we introduce three software packages targeted at scientists to
explore model reasoning using attribution approaches and beyond: (1) Zennit - a
highly customizable and intuitive attribution framework implementing LRP and
related approaches in PyTorch, (2) CoRelAy - a framework to easily and quickly
construct quantitative analysis pipelines for dataset-wide analyses of
explanations, and (3) ViRelAy - a web-application to interactively explore
data, attributions, and analysis results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anders_C/0/1/0/all/0/1"&gt;Christopher J. Anders&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neumann_D/0/1/0/all/0/1"&gt;David Neumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1"&gt;Wojciech Samek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1"&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1"&gt;Sebastian Lapuschkin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Reference Alignment for sparse signals, Uniform Uncertainty Principles and the Beltway Problem. (arXiv:2106.12996v1 [math.ST])]]></title>
        <id>http://arxiv.org/abs/2106.12996</id>
        <link href="http://arxiv.org/abs/2106.12996"/>
        <updated>2021-06-25T02:00:47.129Z</updated>
        <summary type="html"><![CDATA[Motivated by cutting-edge applications like cryo-electron microscopy
(cryo-EM), the Multi-Reference Alignment (MRA) model entails the learning of an
unknown signal from repeated measurements of its images under the latent action
of a group of isometries and additive noise of magnitude $\sigma$. Despite
significant interest, a clear picture for understanding rates of estimation in
this model has emerged only recently, particularly in the high-noise regime
$\sigma \gg 1$ that is highly relevant in applications. Recent investigations
have revealed a remarkable asymptotic sample complexity of order $\sigma^6$ for
certain signals whose Fourier transforms have full support, in stark contrast
to the traditional $\sigma^2$ that arise in regular models. Often prohibitively
large in practice, these results have prompted the investigation of variations
around the MRA model where better sample complexity may be achieved. In this
paper, we show that \emph{sparse} signals exhibit an intermediate $\sigma^4$
sample complexity even in the classical MRA model. Our results explore and
exploit connections of the MRA estimation problem with two classical topics in
applied mathematics: the \textit{beltway problem} from combinatorial
optimization, and \textit{uniform uncertainty principles} from harmonic
analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Ghosh_S/0/1/0/all/0/1"&gt;Subhro Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Rigollet_P/0/1/0/all/0/1"&gt;Philippe Rigollet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Symmetric Wasserstein Autoencoders. (arXiv:2106.13024v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13024</id>
        <link href="http://arxiv.org/abs/2106.13024"/>
        <updated>2021-06-25T02:00:47.123Z</updated>
        <summary type="html"><![CDATA[Leveraging the framework of Optimal Transport, we introduce a new family of
generative autoencoders with a learnable prior, called Symmetric Wasserstein
Autoencoders (SWAEs). We propose to symmetrically match the joint distributions
of the observed data and the latent representation induced by the encoder and
the decoder. The resulting algorithm jointly optimizes the modelling losses in
both the data and the latent spaces with the loss in the data space leading to
the denoising effect. With the symmetric treatment of the data and the latent
representation, the algorithm implicitly preserves the local structure of the
data in the latent space. To further improve the quality of the latent
representation, we incorporate a reconstruction loss into the objective, which
significantly benefits both the generation and reconstruction. We empirically
show the superior performance of SWAEs over the state-of-the-art generative
autoencoders in terms of classification, reconstruction, and generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1"&gt;Sun Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Hongyu Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Corruption Robustness: Inductive Biases in Vision Transformers and MLP-Mixers. (arXiv:2106.13122v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13122</id>
        <link href="http://arxiv.org/abs/2106.13122"/>
        <updated>2021-06-25T02:00:47.107Z</updated>
        <summary type="html"><![CDATA[Recently, vision transformers and MLP-based models have been developed in
order to address some of the prevalent weaknesses in convolutional neural
networks. Due to the novelty of transformers being used in this domain along
with the self-attention mechanism, it remains unclear to what degree these
architectures are robust to corruptions. Despite some works proposing that data
augmentation remains essential for a model to be robust against corruptions, we
propose to explore the impact that the architecture has on corruption
robustness. We find that vision transformer architectures are inherently more
robust to corruptions than the ResNet-50 and MLP-Mixers. We also find that
vision transformers with 5 times fewer parameters than a ResNet-50 have more
shape bias. Our code is available to reproduce.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Morrison_K/0/1/0/all/0/1"&gt;Katelyn Morrison&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gilby_B/0/1/0/all/0/1"&gt;Benjamin Gilby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipchak_C/0/1/0/all/0/1"&gt;Colton Lipchak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mattioli_A/0/1/0/all/0/1"&gt;Adam Mattioli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kovashka_A/0/1/0/all/0/1"&gt;Adriana Kovashka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the relationship between predictive coding and backpropagation. (arXiv:2106.13082v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2106.13082</id>
        <link href="http://arxiv.org/abs/2106.13082"/>
        <updated>2021-06-25T02:00:47.102Z</updated>
        <summary type="html"><![CDATA[In this manuscript, I review and extend recent work on the relationship
between predictive coding and backpropagation for training artificial neural
networks on supervised learning tasks. I also discuss some implications of
these results for the interpretation of predictive coding and deep neural
networks as models of biological learning and I describe a repository of
functions, Torch2PC, for performing predictive coding with PyTorch neural
network models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Rosenbaum_R/0/1/0/all/0/1"&gt;Robert Rosenbaum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AVHYAS: A Free and Open Source QGIS Plugin for Advanced Hyperspectral Image Analysis. (arXiv:2106.12776v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12776</id>
        <link href="http://arxiv.org/abs/2106.12776"/>
        <updated>2021-06-25T02:00:47.097Z</updated>
        <summary type="html"><![CDATA[Advanced Hyperspectral Data Analysis Software (AVHYAS) plugin is a python3
based quantum GIS (QGIS) plugin designed to process and analyse hyperspectral
(Hx) images. It is developed to guarantee full usage of present and future Hx
airborne or spaceborne sensors and provides access to advanced algorithms for
Hx data processing. The software is freely available and offers a range of
basic and advanced tools such as atmospheric correction (for airborne AVIRISNG
image), standard processing tools as well as powerful machine learning and Deep
Learning interfaces for Hx data analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lyngdoh_R/0/1/0/all/0/1"&gt;Rosly Boy Lyngdoh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sahadevan_A/0/1/0/all/0/1"&gt;Anand S Sahadevan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ahmad_T/0/1/0/all/0/1"&gt;Touseef Ahmad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rathore_P/0/1/0/all/0/1"&gt;Pradyuman Singh Rathore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mishra_M/0/1/0/all/0/1"&gt;Manoj Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gupta_P/0/1/0/all/0/1"&gt;Praveen Kumar Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Misra_A/0/1/0/all/0/1"&gt;Arundhati Misra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Language and Multimodal Privacy-Preserving Markers of Mood from Mobile Data. (arXiv:2106.13213v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13213</id>
        <link href="http://arxiv.org/abs/2106.13213"/>
        <updated>2021-06-25T02:00:47.092Z</updated>
        <summary type="html"><![CDATA[Mental health conditions remain underdiagnosed even in countries with common
access to advanced medical care. The ability to accurately and efficiently
predict mood from easily collectible data has several important implications
for the early detection, intervention, and treatment of mental health
disorders. One promising data source to help monitor human behavior is daily
smartphone usage. However, care must be taken to summarize behaviors without
identifying the user through personal (e.g., personally identifiable
information) or protected (e.g., race, gender) attributes. In this paper, we
study behavioral markers of daily mood using a recent dataset of mobile
behaviors from adolescent populations at high risk of suicidal behaviors. Using
computational models, we find that language and multimodal representations of
mobile typed text (spanning typed characters, words, keystroke timings, and app
usage) are predictive of daily mood. However, we find that models trained to
predict mood often also capture private user identities in their intermediate
representations. To tackle this problem, we evaluate approaches that obfuscate
user identity while remaining predictive. By combining multimodal
representations with privacy-preserving learning, we are able to push forward
the performance-privacy frontier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Terrance Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_A/0/1/0/all/0/1"&gt;Anna Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muszynski_M/0/1/0/all/0/1"&gt;Michal Muszynski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ishii_R/0/1/0/all/0/1"&gt;Ryo Ishii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Allen_N/0/1/0/all/0/1"&gt;Nicholas Allen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Auerbach_R/0/1/0/all/0/1"&gt;Randy Auerbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brent_D/0/1/0/all/0/1"&gt;David Brent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1"&gt;Louis-Philippe Morency&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MIxBN: library for learning Bayesian networks from mixed data. (arXiv:2106.13194v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.13194</id>
        <link href="http://arxiv.org/abs/2106.13194"/>
        <updated>2021-06-25T02:00:47.087Z</updated>
        <summary type="html"><![CDATA[This paper describes a new library for learning Bayesian networks from data
containing discrete and continuous variables (mixed data). In addition to the
classical learning methods on discretized data, this library proposes its
algorithm that allows structural learning and parameters learning from mixed
data without discretization since data discretization leads to information
loss. This algorithm based on mixed MI score function for structural learning,
and also linear regression and Gaussian distribution approximation for
parameters learning. The library also offers two algorithms for enumerating
graph structures - the greedy Hill-Climbing algorithm and the evolutionary
algorithm. Thus the key capabilities of the proposed library are as follows:
(1) structural and parameters learning of a Bayesian network on discretized
data, (2) structural and parameters learning of a Bayesian network on mixed
data using the MI mixed score function and Gaussian approximation, (3)
launching learning algorithms on one of two algorithms for enumerating graph
structures - Hill-Climbing and the evolutionary algorithm. Since the need for
mixed data representation comes from practical necessity, the advantages of our
implementations are evaluated in the context of solving approximation and gap
recovery problems on synthetic data and real datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bubnova_A/0/1/0/all/0/1"&gt;Anna V. Bubnova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Deeva_I/0/1/0/all/0/1"&gt;Irina Deeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kalyuzhnaya_A/0/1/0/all/0/1"&gt;Anna V. Kalyuzhnaya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Fully Problem-Dependent Regret Lower Bound for Finite-Horizon MDPs. (arXiv:2106.13013v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13013</id>
        <link href="http://arxiv.org/abs/2106.13013"/>
        <updated>2021-06-25T02:00:47.072Z</updated>
        <summary type="html"><![CDATA[We derive a novel asymptotic problem-dependent lower-bound for regret
minimization in finite-horizon tabular Markov Decision Processes (MDPs). While,
similar to prior work (e.g., for ergodic MDPs), the lower-bound is the solution
to an optimization problem, our derivation reveals the need for an additional
constraint on the visitation distribution over state-action pairs that
explicitly accounts for the dynamics of the MDP. We provide a characterization
of our lower-bound through a series of examples illustrating how different MDPs
may have significantly different complexity. 1) We first consider a "difficult"
MDP instance, where the novel constraint based on the dynamics leads to a
larger lower-bound (i.e., a larger regret) compared to the classical analysis.
2) We then show that our lower-bound recovers results previously derived for
specific MDP instances. 3) Finally, we show that, in certain "simple" MDPs, the
lower bound is considerably smaller than in the general case and it does not
scale with the minimum action gap at all. We show that this last result is
attainable (up to $poly(H)$ terms, where $H$ is the horizon) by providing a
regret upper-bound based on policy gaps for an optimistic algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tirinzoni_A/0/1/0/all/0/1"&gt;Andrea Tirinzoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pirotta_M/0/1/0/all/0/1"&gt;Matteo Pirotta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lazaric_A/0/1/0/all/0/1"&gt;Alessandro Lazaric&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Biologically Plausible Convolutional Networks. (arXiv:2106.13031v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13031</id>
        <link href="http://arxiv.org/abs/2106.13031"/>
        <updated>2021-06-25T02:00:47.066Z</updated>
        <summary type="html"><![CDATA[Convolutional networks are ubiquitous in deep learning. They are particularly
useful for images, as they reduce the number of parameters, reduce training
time, and increase accuracy. However, as a model of the brain they are
seriously problematic, since they require weight sharing - something real
neurons simply cannot do. Consequently, while neurons in the brain can be
locally connected (one of the features of convolutional networks), they cannot
be convolutional. Locally connected but non-convolutional networks, however,
significantly underperform convolutional ones. This is troublesome for studies
that use convolutional networks to explain activity in the visual system. Here
we study plausible alternatives to weight sharing that aim at the same
regularization principle, which is to make each neuron within a pool react
similarly to identical inputs. The most natural way to do that is by showing
the network multiple translations of the same image, akin to saccades in animal
vision. However, this approach requires many translations, and doesn't remove
the performance gap. We propose instead to add lateral connectivity to a
locally connected network, and allow learning via Hebbian plasticity. This
requires the network to pause occasionally for a sleep-like phase of "weight
sharing". This method enables locally connected networks to achieve nearly
convolutional performance on ImageNet, thus supporting convolutional networks
as a model of the visual stream.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pogodin_R/0/1/0/all/0/1"&gt;Roman Pogodin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehta_Y/0/1/0/all/0/1"&gt;Yash Mehta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lillicrap_T/0/1/0/all/0/1"&gt;Timothy P. Lillicrap&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Latham_P/0/1/0/all/0/1"&gt;Peter E. Latham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-training transformer-based framework on large-scale pediatric claims data for downstream population-specific tasks. (arXiv:2106.13095v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13095</id>
        <link href="http://arxiv.org/abs/2106.13095"/>
        <updated>2021-06-25T02:00:47.059Z</updated>
        <summary type="html"><![CDATA[The adoption of electronic health records (EHR) has become universal during
the past decade, which has afforded in-depth data-based research. By learning
from the large amount of healthcare data, various data-driven models have been
built to predict future events for different medical tasks, such as auto
diagnosis and heart-attack prediction. Although EHR is abundant, the population
that satisfies specific criteria for learning population-specific tasks is
scarce, making it challenging to train data-hungry deep learning models. This
study presents the Claim Pre-Training (Claim-PT) framework, a generic
pre-training model that first trains on the entire pediatric claims dataset,
followed by a discriminative fine-tuning on each population-specific task. The
semantic meaning of medical events can be captured in the pre-training stage,
and the effective knowledge transfer is completed through the task-aware
fine-tuning stage. The fine-tuning process requires minimal parameter
modification without changing the model architecture, which mitigates the data
scarcity issue and helps train the deep learning model adequately on small
patient cohorts. We conducted experiments on a real-world claims dataset with
more than one million patient records. Experimental results on two downstream
tasks demonstrated the effectiveness of our method: our general task-agnostic
pre-training framework outperformed tailored task-specific models, achieving
more than 10\% higher in model performance as compared to baselines. In
addition, our framework showed a great generalizability potential to transfer
learned knowledge from one institution to another, paving the way for future
healthcare model pre-training across institutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xianlong Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Simon Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FitVid: Overfitting in Pixel-Level Video Prediction. (arXiv:2106.13195v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13195</id>
        <link href="http://arxiv.org/abs/2106.13195"/>
        <updated>2021-06-25T02:00:47.052Z</updated>
        <summary type="html"><![CDATA[An agent that is capable of predicting what happens next can perform a
variety of tasks through planning with no additional training. Furthermore,
such an agent can internally represent the complex dynamics of the real-world
and therefore can acquire a representation useful for a variety of visual
perception tasks. This makes predicting the future frames of a video,
conditioned on the observed past and potentially future actions, an interesting
task which remains exceptionally challenging despite many recent advances.
Existing video prediction models have shown promising results on simple narrow
benchmarks but they generate low quality predictions on real-life datasets with
more complicated dynamics or broader domain. There is a growing body of
evidence that underfitting on the training data is one of the primary causes
for the low quality predictions. In this paper, we argue that the inefficient
use of parameters in the current video models is the main reason for
underfitting. Therefore, we introduce a new architecture, named FitVid, which
is capable of severe overfitting on the common benchmarks while having similar
parameter count as the current state-of-the-art models. We analyze the
consequences of overfitting, illustrating how it can produce unexpected
outcomes such as generating high quality output by repeating the training data,
and how it can be mitigated using existing image augmentation techniques. As a
result, FitVid outperforms the current state-of-the-art models across four
different video prediction benchmarks on four different metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Babaeizadeh_M/0/1/0/all/0/1"&gt;Mohammad Babaeizadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saffar_M/0/1/0/all/0/1"&gt;Mohammad Taghi Saffar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1"&gt;Suraj Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1"&gt;Chelsea Finn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erhan_D/0/1/0/all/0/1"&gt;Dumitru Erhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using machine learning techniques to predict hospital admission at the emergency department. (arXiv:2106.12921v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12921</id>
        <link href="http://arxiv.org/abs/2106.12921"/>
        <updated>2021-06-25T02:00:47.047Z</updated>
        <summary type="html"><![CDATA[Introduction: One of the most important tasks in the Emergency Department
(ED) is to promptly identify the patients who will benefit from hospital
admission. Machine Learning (ML) techniques show promise as diagnostic aids in
healthcare. Material and methods: We investigated the following features
seeking to investigate their performance in predicting hospital admission:
serum levels of Urea, Creatinine, Lactate Dehydrogenase, Creatine Kinase,
C-Reactive Protein, Complete Blood Count with differential, Activated Partial
Thromboplastin Time, D Dimer, International Normalized Ratio, age, gender,
triage disposition to ED unit and ambulance utilization. A total of 3,204 ED
visits were analyzed. Results: The proposed algorithms generated models which
demonstrated acceptable performance in predicting hospital admission of ED
patients. The range of F-measure and ROC Area values of all eight evaluated
algorithms were [0.679-0.708] and [0.734-0.774], respectively. Discussion: The
main advantages of this tool include easy access, availability, yes/no result,
and low cost. The clinical implications of our approach might facilitate a
shift from traditional clinical decision-making to a more sophisticated model.
Conclusion: Developing robust prognostic models with the utilization of common
biomarkers is a project that might shape the future of emergency medicine. Our
findings warrant confirmation with implementation in pragmatic ED trials.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feretzakis_G/0/1/0/all/0/1"&gt;Georgios Feretzakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlis_G/0/1/0/all/0/1"&gt;George Karlis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loupelis_E/0/1/0/all/0/1"&gt;Evangelos Loupelis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalles_D/0/1/0/all/0/1"&gt;Dimitris Kalles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatzikyriakou_R/0/1/0/all/0/1"&gt;Rea Chatzikyriakou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trakas_N/0/1/0/all/0/1"&gt;Nikolaos Trakas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karakou_E/0/1/0/all/0/1"&gt;Eugenia Karakou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sakagianni_A/0/1/0/all/0/1"&gt;Aikaterini Sakagianni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tzelves_L/0/1/0/all/0/1"&gt;Lazaros Tzelves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petropoulou_S/0/1/0/all/0/1"&gt;Stavroula Petropoulou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tika_A/0/1/0/all/0/1"&gt;Aikaterini Tika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dalainas_I/0/1/0/all/0/1"&gt;Ilias Dalainas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaldis_V/0/1/0/all/0/1"&gt;Vasileios Kaldis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Next-Day Bitcoin Price Forecast Based on Artificial intelligence Methods. (arXiv:2106.12961v1 [q-fin.ST])]]></title>
        <id>http://arxiv.org/abs/2106.12961</id>
        <link href="http://arxiv.org/abs/2106.12961"/>
        <updated>2021-06-25T02:00:47.032Z</updated>
        <summary type="html"><![CDATA[In recent years, Bitcoin price prediction has attracted the interest of
researchers and investors. However, the accuracy of previous studies is not
well enough. Machine learning and deep learning methods have been proved to
have strong prediction ability in this area. This paper proposed a method
combined with Ensemble Empirical Mode Decomposition (EEMD) and a deep learning
method called long short-term memory (LSTM) to research the problem of next-day
Bitcoin price forecast.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Yang_L/0/1/0/all/0/1"&gt;Liping Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rate Distortion Characteristic Modeling for Neural Image Compression. (arXiv:2106.12954v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12954</id>
        <link href="http://arxiv.org/abs/2106.12954"/>
        <updated>2021-06-25T02:00:47.022Z</updated>
        <summary type="html"><![CDATA[End-to-end optimization capability offers neural image compression (NIC)
superior lossy compression performance. However, distinct models are required
to be trained to reach different points in the rate-distortion (R-D) space. In
this paper, we consider the problem of R-D characteristic analysis and modeling
for NIC. We make efforts to formulate the essential mathematical functions to
describe the R-D behavior of NIC using deep network and statistical modeling.
Thus continuous bit-rate points could be elegantly realized by leveraging such
model via a single trained network. In this regard, we propose a plugin-in
module to learn the relationship between the target bit-rate and the binary
representation for the latent variable of auto-encoder. Furthermore, we model
the rate and distortion characteristic of NIC as a function of the coding
parameter $\lambda$ respectively. Our experiments show our proposed method is
easy to adopt and obtains competitive coding performance with fixed-rate coding
approaches, which would benefit the practical deployment of NIC. In addition,
the proposed model could be applied to NIC rate control with limited bit-rate
error using a single network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jia_C/0/1/0/all/0/1"&gt;Chuanmin Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ge_Z/0/1/0/all/0/1"&gt;Ziqing Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shanshe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ma_S/0/1/0/all/0/1"&gt;Siwei Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gao_W/0/1/0/all/0/1"&gt;Wen Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantization Aware Training, ERNIE and Kurtosis Regularizer: a short empirical study. (arXiv:2106.13035v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.13035</id>
        <link href="http://arxiv.org/abs/2106.13035"/>
        <updated>2021-06-25T02:00:47.016Z</updated>
        <summary type="html"><![CDATA[Pre-trained language models like Ernie or Bert are currently used in many
applications. These models come with a set of pre-trained weights typically
obtained in unsupervised/self-supervised modality on a huge amount of data.
After that, they are fine-tuned on a specific task. Applications then use these
models for inference, and often some additional constraints apply, like low
power-budget or low latency between input and output. The main avenue to meet
these additional requirements for the inference settings, is to use low
precision computation (e.g. INT8 rather than FP32), but this comes with a cost
of deteriorating the functional performance (e.g. accuracy) of the model. Some
approaches have been developed to tackle the problem and go beyond the
limitations of the PTO (Post-Training Quantization), more specifically the QAT
(Quantization Aware Training, see [4]) is a procedure that interferes with the
training process in order to make it affected (or simply disturbed) by the
quantization phase during the training itself. Besides QAT, recently
Intel-Habana Labs have proposed an additional and more direct way to make the
training results more robust to subsequent quantization which uses a
regularizer, therefore changing the loss function that drives the training
procedure. But their proposal does not work out-of-the-box for pre-trained
models like Ernie, for example. In this short paper we show why this is not
happening (for the Ernie case) and we propose a very basic way to deal with it,
sharing as well some initial results (increase in final INT8 accuracy) that
might be of interest to practitioners willing to use Ernie in their
applications, in low precision regime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zanetti_A/0/1/0/all/0/1"&gt;Andrea Zanetti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Uncertainty in Bayesian Deep Learning. (arXiv:2106.13055v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.13055</id>
        <link href="http://arxiv.org/abs/2106.13055"/>
        <updated>2021-06-25T02:00:47.010Z</updated>
        <summary type="html"><![CDATA[Neural Linear Models (NLM) are deep Bayesian models that produce predictive
uncertainty by learning features from the data and then performing Bayesian
linear regression over these features. Despite their popularity, few works have
focused on formally evaluating the predictive uncertainties of these models.
Furthermore, existing works point out the difficulties of encoding domain
knowledge in models like NLMs, making them unsuitable for applications where
interpretability is required. In this work, we show that traditional training
procedures for NLMs can drastically underestimate uncertainty in data-scarce
regions. We identify the underlying reasons for this behavior and propose a
novel training method that can both capture useful predictive uncertainties as
well as allow for incorporation of domain knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lorsung_C/0/1/0/all/0/1"&gt;Cooper Lorsung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploration-Exploitation in Multi-Agent Competition: Convergence with Bounded Rationality. (arXiv:2106.12928v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2106.12928</id>
        <link href="http://arxiv.org/abs/2106.12928"/>
        <updated>2021-06-25T02:00:47.000Z</updated>
        <summary type="html"><![CDATA[The interplay between exploration and exploitation in competitive multi-agent
learning is still far from being well understood. Motivated by this, we study
smooth Q-learning, a prototypical learning model that explicitly captures the
balance between game rewards and exploration costs. We show that Q-learning
always converges to the unique quantal-response equilibrium (QRE), the standard
solution concept for games under bounded rationality, in weighted zero-sum
polymatrix games with heterogeneous learning agents using positive exploration
rates. Complementing recent results about convergence in weighted potential
games, we show that fast convergence of Q-learning in competitive settings is
obtained regardless of the number of agents and without any need for parameter
fine-tuning. As showcased by our experiments in network zero-sum games, these
theoretical results provide the necessary guarantees for an algorithmic
approach to the currently open problem of equilibrium selection in competitive
multi-agent settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leonardos_S/0/1/0/all/0/1"&gt;Stefanos Leonardos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piliouras_G/0/1/0/all/0/1"&gt;Georgios Piliouras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spendlove_K/0/1/0/all/0/1"&gt;Kelly Spendlove&lt;/a&gt;,</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Regret Bounds for Tracking Experts with Memory. (arXiv:2106.13021v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13021</id>
        <link href="http://arxiv.org/abs/2106.13021"/>
        <updated>2021-06-25T02:00:46.983Z</updated>
        <summary type="html"><![CDATA[We address the problem of sequential prediction with expert advice in a
non-stationary environment with long-term memory guarantees in the sense of
Bousquet and Warmuth [4]. We give a linear-time algorithm that improves on the
best known regret bounds [26]. This algorithm incorporates a relative entropy
projection step. This projection is advantageous over previous weight-sharing
approaches in that weight updates may come with implicit costs as in for
example portfolio optimization. We give an algorithm to compute this projection
step in linear time, which may be of independent interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Robinson_J/0/1/0/all/0/1"&gt;James Robinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herbster_M/0/1/0/all/0/1"&gt;Mark Herbster&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Networks for Dengue Prediction: A Systematic Review. (arXiv:2106.12905v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12905</id>
        <link href="http://arxiv.org/abs/2106.12905"/>
        <updated>2021-06-25T02:00:46.967Z</updated>
        <summary type="html"><![CDATA[Due to a lack of treatments and universal vaccine, early forecasts of Dengue
are an important tool for disease control. Neural networks are powerful
predictive models that have made contributions to many areas of public health.
In this systematic review, we provide an introduction to the neural networks
relevant to Dengue forecasting and review their applications in the literature.
The objective is to help inform model design for future work. Following the
PRISMA guidelines, we conduct a systematic search of studies that use neural
networks to forecast Dengue in human populations. We summarize the relative
performance of neural networks and comparator models, model architectures and
hyper-parameters, as well as choices of input features. Nineteen papers were
included. Most studies implement shallow neural networks using historical
Dengue incidence and meteorological input features. Prediction horizons tend to
be short. Building on the strengths of neural networks, most studies use
granular observations at the city or sub-national level. Performance of neural
networks relative to comparators such as Support Vector Machines varies across
study contexts. The studies suggest that neural networks can provide good
predictions of Dengue and should be included in the set of candidate models.
The use of convolutional, recurrent, or deep networks is relatively unexplored
but offers promising avenues for further research, as does the use of a broader
set of input features such as social media or mobile phone data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roster_K/0/1/0/all/0/1"&gt;Kirstin Roster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodrigues_F/0/1/0/all/0/1"&gt;Francisco A. Rodrigues&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Abstraction of Markov Population Dynamics via Generative Adversarial Nets. (arXiv:2106.12981v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12981</id>
        <link href="http://arxiv.org/abs/2106.12981"/>
        <updated>2021-06-25T02:00:46.958Z</updated>
        <summary type="html"><![CDATA[Markov Population Models are a widespread formalism used to model the
dynamics of complex systems, with applications in Systems Biology and many
other fields. The associated Markov stochastic process in continuous time is
often analyzed by simulation, which can be costly for large or stiff systems,
particularly when a massive number of simulations has to be performed (e.g. in
a multi-scale model). A strategy to reduce computational load is to abstract
the population model, replacing it with a simpler stochastic model, faster to
simulate. Here we pursue this idea, building on previous works and constructing
a generator capable of producing stochastic trajectories in continuous space
and discrete time. This generator is learned automatically from simulations of
the original model in a Generative Adversarial setting. Compared to previous
works, which rely on deep neural networks and Dirichlet processes, we explore
the use of state of the art generative models, which are flexible enough to
learn a full trajectory rather than a single transition kernel.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cairoli_F/0/1/0/all/0/1"&gt;Francesca Cairoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carbone_G/0/1/0/all/0/1"&gt;Ginevra Carbone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bortolussi_L/0/1/0/all/0/1"&gt;Luca Bortolussi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[rSoccer: A Framework for Studying Reinforcement Learning in Small and Very Small Size Robot Soccer. (arXiv:2106.12895v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12895</id>
        <link href="http://arxiv.org/abs/2106.12895"/>
        <updated>2021-06-25T02:00:46.948Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning is an active research area with a vast number of
applications in robotics, and the RoboCup competition is an interesting
environment for studying and evaluating reinforcement learning methods. A known
difficulty in applying reinforcement learning to robotics is the high number of
experience samples required, being the use of simulated environments for
training the agents followed by transfer learning to real-world (sim-to-real) a
viable path. This article introduces an open-source simulator for the IEEE Very
Small Size Soccer and the Small Size League optimized for reinforcement
learning experiments. We also propose a framework for creating OpenAI Gym
environments with a set of benchmarks tasks for evaluating single-agent and
multi-agent robot soccer skills. We then demonstrate the learning capabilities
of two state-of-the-art reinforcement learning methods as well as their
limitations in certain scenarios introduced in this framework. We believe this
will make it easier for more teams to compete in these categories using
end-to-end reinforcement learning approaches and further develop this research
area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Martins_F/0/1/0/all/0/1"&gt;Felipe B. Martins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Machado_M/0/1/0/all/0/1"&gt;Mateus G. Machado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bassani_H/0/1/0/all/0/1"&gt;Hansenclever F. Bassani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Braga_P/0/1/0/all/0/1"&gt;Pedro H. M. Braga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barros_E/0/1/0/all/0/1"&gt;Edna S. Barros&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SofaMyRoom: a fast and multiplatform "shoebox" room simulator for binaural room impulse response dataset generation. (arXiv:2106.12992v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.12992</id>
        <link href="http://arxiv.org/abs/2106.12992"/>
        <updated>2021-06-25T02:00:46.931Z</updated>
        <summary type="html"><![CDATA[This paper introduces a shoebox room simulator able to systematically
generate synthetic datasets of binaural room impulse responses (BRIRs) given an
arbitrary set of head-related transfer functions (HRTFs). The evaluation of
machine hearing algorithms frequently requires BRIR datasets in order to
simulate the acoustics of any environment. However, currently available
solutions typically consider only HRTFs measured on dummy heads, which poorly
characterize the high variability in spatial sound perception. Our solution
allows to integrate a room impulse response (RIR) simulator with different HRTF
sets represented in Spatially Oriented Format for Acoustics (SOFA). The source
code and the compiled binaries for different operating systems allow to both
advanced and non-expert users to benefit from our toolbox, see
https://github.com/spatialaudiotools/sofamyroom/ .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barumerli_R/0/1/0/all/0/1"&gt;Roberto Barumerli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bianchi_D/0/1/0/all/0/1"&gt;Daniele Bianchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geronazzo_M/0/1/0/all/0/1"&gt;Michele Geronazzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avanzini_F/0/1/0/all/0/1"&gt;Federico Avanzini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RikoNet: A Novel Anime Recommendation Engine. (arXiv:2106.12970v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.12970</id>
        <link href="http://arxiv.org/abs/2106.12970"/>
        <updated>2021-06-25T02:00:46.924Z</updated>
        <summary type="html"><![CDATA[Anime is quite well-received today, especially among the younger generations.
With many genres of available shows, more and more people are increasingly
getting attracted to this niche section of the entertainment industry. As anime
has recently garnered mainstream attention, we have insufficient information
regarding users' penchant and watching habits. Therefore, it is an uphill task
to build a recommendation engine for this relatively obscure entertainment
medium. In this attempt, we have built a novel hybrid recommendation system
that could act both as a recommendation system and as a means of exploring new
anime genres and titles. We have analyzed the general trends in this field and
the users' watching habits for coming up with our efficacious solution. Our
solution employs deep autoencoders for the tasks of predicting ratings and
generating embeddings. Following this, we formed clusters using the embeddings
of the anime titles. These clusters form the search space for anime with
similarities and are used to find anime similar to the ones liked and disliked
by the user. This method, combined with the predicted ratings, forms the novel
hybrid filter. In this article, we have demonstrated this idea and compared the
performance of our implemented model with the existing state-of-the-art
techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soni_B/0/1/0/all/0/1"&gt;Badal Soni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thakuria_D/0/1/0/all/0/1"&gt;Debangan Thakuria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nath_N/0/1/0/all/0/1"&gt;Nilutpal Nath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_N/0/1/0/all/0/1"&gt;Navarun Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boro_B/0/1/0/all/0/1"&gt;Bhaskarananda Boro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Modern Techniques in Optimization: Frank-Wolfe, Nesterov's Momentum, and Polyak's Momentum. (arXiv:2106.12923v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.12923</id>
        <link href="http://arxiv.org/abs/2106.12923"/>
        <updated>2021-06-25T02:00:46.916Z</updated>
        <summary type="html"><![CDATA[In the first part of this dissertation research, we develop a modular
framework that can serve as a recipe for constructing and analyzing iterative
algorithms for convex optimization. Specifically, our work casts optimization
as iteratively playing a two-player zero-sum game. Many existing optimization
algorithms including Frank-Wolfe and Nesterov's acceleration methods can be
recovered from the game by pitting two online learners with appropriate
strategies against each other. Furthermore, the sum of the weighted average
regrets of the players in the game implies the convergence rate. As a result,
our approach provides simple alternative proofs to these algorithms. Moreover,
we demonstrate that our approach of optimization as iteratively playing a game
leads to three new fast Frank-Wolfe-like algorithms for some constraint sets,
which further shows that our framework is indeed generic, modular, and
easy-to-use.

In the second part, we develop a modular analysis of provable acceleration
via Polyak's momentum for certain problems, which include solving the classical
strongly quadratic convex problems, training a wide ReLU network under the
neural tangent kernel regime, and training a deep linear network with an
orthogonal initialization. We develop a meta theorem and show that when
applying Polyak's momentum for these problems, the induced dynamics exhibit a
form where we can directly apply our meta theorem.

In the last part of the dissertation, we show another advantage of the use of
Polyak's momentum -- it facilitates fast saddle point escape in smooth
non-convex optimization. This result, together with those of the second part,
sheds new light on Polyak's momentum in modern non-convex optimization and deep
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun-Kun Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information Bottleneck: Exact Analysis of (Quantized) Neural Networks. (arXiv:2106.12912v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12912</id>
        <link href="http://arxiv.org/abs/2106.12912"/>
        <updated>2021-06-25T02:00:46.910Z</updated>
        <summary type="html"><![CDATA[The information bottleneck (IB) principle has been suggested as a way to
analyze deep neural networks. The learning dynamics are studied by inspecting
the mutual information (MI) between the hidden layers and the input and output.
Notably, separate fitting and compression phases during training have been
reported. This led to some controversy including claims that the observations
are not reproducible and strongly dependent on the type of activation function
used as well as on the way the MI is estimated. Our study confirms that
different ways of binning when computing the MI lead to qualitatively different
results, either supporting or refusing IB conjectures. To resolve the
controversy, we study the IB principle in settings where MI is non-trivial and
can be computed exactly. We monitor the dynamics of quantized neural networks,
that is, we discretize the whole deep learning system so that no approximation
is required when computing the MI. This allows us to quantify the information
flow without measurement errors. In this setting, we observed a fitting phase
for all layers and a compression phase for the output layer in all experiments;
the compression in the hidden layers was dependent on the type of activation
function. Our study shows that the initial IB results were not artifacts of
binning when computing the MI. However, the critical claim that the compression
phase may not be observed for some networks also holds true.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lorenzen_S/0/1/0/all/0/1"&gt;Stephan Sloth Lorenzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Igel_C/0/1/0/all/0/1"&gt;Christian Igel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nielsen_M/0/1/0/all/0/1"&gt;Mads Nielsen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Topic Segmentation of Meetings with BERT Embeddings. (arXiv:2106.12978v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12978</id>
        <link href="http://arxiv.org/abs/2106.12978"/>
        <updated>2021-06-25T02:00:46.903Z</updated>
        <summary type="html"><![CDATA[Topic segmentation of meetings is the task of dividing multi-person meeting
transcripts into topic blocks. Supervised approaches to the problem have proven
intractable due to the difficulties in collecting and accurately annotating
large datasets. In this paper we show how previous unsupervised topic
segmentation methods can be improved using pre-trained neural architectures. We
introduce an unsupervised approach based on BERT embeddings that achieves a
15.5% reduction in error rate over existing unsupervised approaches applied to
two popular datasets for meeting transcripts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Solbiati_A/0/1/0/all/0/1"&gt;Alessandro Solbiati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heffernan_K/0/1/0/all/0/1"&gt;Kevin Heffernan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Damaskinos_G/0/1/0/all/0/1"&gt;Georgios Damaskinos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poddar_S/0/1/0/all/0/1"&gt;Shivani Poddar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Modi_S/0/1/0/all/0/1"&gt;Shubham Modi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cali_J/0/1/0/all/0/1"&gt;Jacques Cali&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatial-Temporal Graph ODE Networks for Traffic Flow Forecasting. (arXiv:2106.12931v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12931</id>
        <link href="http://arxiv.org/abs/2106.12931"/>
        <updated>2021-06-25T02:00:46.897Z</updated>
        <summary type="html"><![CDATA[Spatial-temporal forecasting has attracted tremendous attention in a wide
range of applications, and traffic flow prediction is a canonical and typical
example. The complex and long-range spatial-temporal correlations of traffic
flow bring it to a most intractable challenge. Existing works typically utilize
shallow graph convolution networks (GNNs) and temporal extracting modules to
model spatial and temporal dependencies respectively. However, the
representation ability of such models is limited due to: (1) shallow GNNs are
incapable to capture long-range spatial correlations, (2) only spatial
connections are considered and a mass of semantic connections are ignored,
which are of great importance for a comprehensive understanding of traffic
networks. To this end, we propose Spatial-Temporal Graph Ordinary Differential
Equation Networks (STGODE). Specifically, we capture spatial-temporal dynamics
through a tensor-based ordinary differential equation (ODE), as a result,
deeper networks can be constructed and spatial-temporal features are utilized
synchronously. To understand the network more comprehensively, semantical
adjacency matrix is considered in our model, and a well-design temporal
dialated convolution structure is used to capture long term temporal
dependencies. We evaluate our model on multiple real-world traffic datasets and
superior performance is achieved over state-of-the-art baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1"&gt;Zheng Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_Q/0/1/0/all/0/1"&gt;Qingqing Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1"&gt;Guojie Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1"&gt;Kunqing Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learnt Sparsification for Interpretable Graph Neural Networks. (arXiv:2106.12920v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12920</id>
        <link href="http://arxiv.org/abs/2106.12920"/>
        <updated>2021-06-25T02:00:46.881Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) have achieved great success on various tasks and
fields that require relational modeling. GNNs aggregate node features using the
graph structure as inductive biases resulting in flexible and powerful models.
However, GNNs remain hard to interpret as the interplay between node features
and graph structure is only implicitly learned. In this paper, we propose a
novel method called Kedge for explicitly sparsifying the underlying graph by
removing unnecessary neighbors. Our key idea is based on a tractable method for
sparsification using the Hard Kumaraswamy distribution that can be used in
conjugation with any GNN model. Kedge learns edge masks in a modular fashion
trained with any GNN allowing for gradient based optimization in an end-to-end
fashion. We demonstrate through extensive experiments that our model Kedge can
prune a large proportion of the edges with only a minor effect on the test
accuracy. Specifically, in the PubMed dataset, Kedge learns to drop more than
80% of the edges with an accuracy drop of merely 2% showing that graph
structure has only a small contribution in comparison to node features.
Finally, we also show that Kedge effectively counters the over-smoothing
phenomena in deep GNNs by maintaining good task performance with increasing GNN
layers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rathee_M/0/1/0/all/0/1"&gt;Mandeep Rathee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zijian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Funke_T/0/1/0/all/0/1"&gt;Thorben Funke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khosla_M/0/1/0/all/0/1"&gt;Megha Khosla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1"&gt;Avishek Anand&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fundamental limits for learning hidden Markov model parameters. (arXiv:2106.12936v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.12936</id>
        <link href="http://arxiv.org/abs/2106.12936"/>
        <updated>2021-06-25T02:00:46.876Z</updated>
        <summary type="html"><![CDATA[We study the frontier between learnable and unlearnable hidden Markov models
(HMMs). HMMs are flexible tools for clustering dependent data coming from
unknown populations. The model parameters are known to be identifiable as soon
as the clusters are distinct and the hidden chain is ergodic with a full rank
transition matrix. In the limit as any one of these conditions fails, it
becomes impossible to identify parameters. For a chain with two hidden states
we prove nonasymptotic minimax upper and lower bounds, matching up to
constants, which exhibit thresholds at which the parameters become learnable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Abraham_K/0/1/0/all/0/1"&gt;Kweku Abraham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Naulet_Z/0/1/0/all/0/1"&gt;Zacharie Naulet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gassiat_E/0/1/0/all/0/1"&gt;Elisabeth Gassiat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Long-term Cross Adversarial Training: A Robust Meta-learning Method for Few-shot Classification Tasks. (arXiv:2106.12900v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12900</id>
        <link href="http://arxiv.org/abs/2106.12900"/>
        <updated>2021-06-25T02:00:46.870Z</updated>
        <summary type="html"><![CDATA[Meta-learning model can quickly adapt to new tasks using few-shot labeled
data. However, despite achieving good generalization on few-shot classification
tasks, it is still challenging to improve the adversarial robustness of the
meta-learning model in few-shot learning. Although adversarial training (AT)
methods such as Adversarial Query (AQ) can improve the adversarially robust
performance of meta-learning models, AT is still computationally expensive
training. On the other hand, meta-learning models trained with AT will drop
significant accuracy on the original clean images. This paper proposed a
meta-learning method on the adversarially robust neural network called
Long-term Cross Adversarial Training (LCAT). LCAT will update meta-learning
model parameters cross along the natural and adversarial sample distribution
direction with long-term to improve both adversarial and clean few-shot
classification accuracy. Due to cross-adversarial training, LCAT only needs
half of the adversarial training epoch than AQ, resulting in a low adversarial
training computation. Experiment results show that LCAT achieves superior
performance both on the clean and adversarial few-shot classification accuracy
than SOTA adversarial training methods for meta-learning models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Shuyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1"&gt;Xuelong Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1"&gt;Bin Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-Autoregressive TTS with Explicit Duration Modelling for Low-Resource Highly Expressive Speech. (arXiv:2106.12896v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.12896</id>
        <link href="http://arxiv.org/abs/2106.12896"/>
        <updated>2021-06-25T02:00:46.856Z</updated>
        <summary type="html"><![CDATA[Whilst recent neural text-to-speech (TTS) approaches produce high-quality
speech, they typically require a large amount of recordings from the target
speaker. In previous work, a 3-step method was proposed to generate
high-quality TTS while greatly reducing the amount of data required for
training. However, we have observed a ceiling effect in the level of
naturalness achievable for highly expressive voices when using this approach.
In this paper, we present a method for building highly expressive TTS voices
with as little as 15 minutes of speech data from the target speaker. Compared
to the current state-of-the-art approach, our proposed improvements close the
gap to recordings by 23.3% for naturalness of speech and by 16.3% for speaker
similarity. Further, we match the naturalness and speaker similarity of a
Tacotron2-based full-data (~10 hours) model using only 15 minutes of target
speaker data, whereas with 30 minutes or more, we significantly outperform it.
The following improvements are proposed: 1) changing from an autoregressive,
attention-based TTS model to a non-autoregressive model replacing attention
with an external duration model and 2) an additional Conditional Generative
Adversarial Network (cGAN) based fine-tuning step.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1"&gt;Raahil Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pokora_K/0/1/0/all/0/1"&gt;Kamil Pokora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ezzerg_A/0/1/0/all/0/1"&gt;Abdelhamid Ezzerg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klimkov_V/0/1/0/all/0/1"&gt;Viacheslav Klimkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huybrechts_G/0/1/0/all/0/1"&gt;Goeric Huybrechts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Putrycz_B/0/1/0/all/0/1"&gt;Bartosz Putrycz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korzekwa_D/0/1/0/all/0/1"&gt;Daniel Korzekwa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merritt_T/0/1/0/all/0/1"&gt;Thomas Merritt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tensor networks for unsupervised machine learning. (arXiv:2106.12974v1 [cond-mat.stat-mech])]]></title>
        <id>http://arxiv.org/abs/2106.12974</id>
        <link href="http://arxiv.org/abs/2106.12974"/>
        <updated>2021-06-25T02:00:46.851Z</updated>
        <summary type="html"><![CDATA[Modeling the joint distribution of high-dimensional data is a central task in
unsupervised machine learning. In recent years, many interests have been
attracted to developing learning models based on tensor networks, which have
advantages of theoretical understandings of the expressive power using
entanglement properties, and as a bridge connecting the classical computation
and the quantum computation. Despite the great potential, however, existing
tensor-network-based unsupervised models only work as a proof of principle, as
their performances are much worse than the standard models such as the
restricted Boltzmann machines and neural networks. In this work, we present the
Autoregressive Matrix Product States (AMPS), a tensor-network-based model
combining the matrix product states from quantum many-body physics and the
autoregressive models from machine learning. The model enjoys exact calculation
of normalized probability and unbiased sampling, as well as a clear theoretical
understanding of expressive power. We demonstrate the performance of our model
using two applications, the generative modeling on synthetic and real-world
data, and the reinforcement learning in statistical physics. Using extensive
numerical experiments, we show that the proposed model significantly
outperforms the existing tensor-network-based models and the restricted
Boltzmann machines, and is competitive with the state-of-the-art neural network
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Li_S/0/1/0/all/0/1"&gt;Sujie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stock Market Analysis with Text Data: A Review. (arXiv:2106.12985v1 [q-fin.ST])]]></title>
        <id>http://arxiv.org/abs/2106.12985</id>
        <link href="http://arxiv.org/abs/2106.12985"/>
        <updated>2021-06-25T02:00:46.845Z</updated>
        <summary type="html"><![CDATA[Stock market movements are influenced by public and private information
shared through news articles, company reports, and social media discussions.
Analyzing these vast sources of data can give market participants an edge to
make profit. However, the majority of the studies in the literature are based
on traditional approaches that come short in analyzing unstructured, vast
textual data. In this study, we provide a review on the immense amount of
existing literature of text-based stock market analysis. We present input data
types and cover main textual data sources and variations. Feature
representation techniques are then presented. Then, we cover the analysis
techniques and create a taxonomy of the main stock market forecast models.
Importantly, we discuss representative work in each category of the taxonomy,
analyzing their respective contributions. Finally, this paper shows the
findings on unaddressed open problems and gives suggestions for future work.
The aim of this study is to survey the main stock market analysis models, text
representation techniques for financial market prediction, shortcomings of
existing techniques, and propose promising directions for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Fataliyev_K/0/1/0/all/0/1"&gt;Kamaladdin Fataliyev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Chivukula_A/0/1/0/all/0/1"&gt;Aneesh Chivukula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Prasad_M/0/1/0/all/0/1"&gt;Mukesh Prasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Numerical influence of ReLU'(0) on backpropagation. (arXiv:2106.12915v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12915</id>
        <link href="http://arxiv.org/abs/2106.12915"/>
        <updated>2021-06-25T02:00:46.839Z</updated>
        <summary type="html"><![CDATA[In theory, the choice of ReLU (0) in [0, 1] for a neural network has a
negligible influence both on backpropagation and training. Yet, in the real
world, 32 bits default precision combined with the size of deep learning
problems makes it a hyperparameter of training methods. We investigate the
importance of the value of ReLU (0) for several precision levels (16, 32, 64
bits), on various networks (fully connected, VGG, ResNet) and datasets (MNIST,
CIFAR10, SVHN). We observe considerable variations of backpropagation outputs
which occur around half of the time in 32 bits precision. The effect disappears
with double precision, while it is systematic at 16 bits. For vanilla SGD
training, the choice ReLU (0) = 0 seems to be the most efficient. We also
evidence that reconditioning approaches as batch-norm or ADAM tend to buffer
the influence of ReLU (0)'s value. Overall, the message we want to convey is
that algorithmic differentiation of nonsmooth problems potentially hides
parameters that could be tuned advantageously.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bertoin_D/0/1/0/all/0/1"&gt;David Bertoin&lt;/a&gt; (ISAE-SUPAERO), &lt;a href="http://arxiv.org/find/cs/1/au:+Bolte_J/0/1/0/all/0/1"&gt;J&amp;#xe9;r&amp;#xf4;me Bolte&lt;/a&gt; (UT1, TSE), &lt;a href="http://arxiv.org/find/cs/1/au:+Gerchinovitz_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Gerchinovitz&lt;/a&gt; (IMT), &lt;a href="http://arxiv.org/find/cs/1/au:+Pauwels_E/0/1/0/all/0/1"&gt;Edouard Pauwels&lt;/a&gt; (CNRS, IRIT)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High Performance Hyperspectral Image Classification using Graphics Processing Units. (arXiv:2106.12942v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2106.12942</id>
        <link href="http://arxiv.org/abs/2106.12942"/>
        <updated>2021-06-25T02:00:46.834Z</updated>
        <summary type="html"><![CDATA[Real-time remote sensing applications like search and rescue missions,
military target detection, environmental monitoring, hazard prevention and
other time-critical applications require onboard real time processing
capabilities or autonomous decision making. Some unmanned remote systems like
satellites are physically remote from their operators, and all control of the
spacecraft and data returned by the spacecraft must be transmitted over a
wireless radio link. This link may not be available for extended periods when
the satellite is out of line of sight of its ground station. Therefore,
lightweight, small size and low power consumption hardware is essential for
onboard real time processing systems. With increasing dimensionality, size and
resolution of recent hyperspectral imaging sensors, additional challenges are
posed upon remote sensing processing systems and more capable computing
architectures are needed. Graphical Processing Units (GPUs) emerged as
promising architecture for light weight high performance computing that can
address these computational requirements for onboard systems. The goal of this
study is to build high performance methods for onboard hyperspectral analysis.
We propose accelerated methods for the well-known recursive hierarchical
segmentation (RHSEG) clustering method, using GPUs, hybrid multicore CPU with a
GPU and hybrid multi-core CPU/GPU clusters. RHSEG is a method developed by the
National Aeronautics and Space Administration (NASA), which is designed to
provide rich classification information with several output levels. The
achieved speedups by parallel solutions compared to CPU sequential
implementations are 21x for parallel single GPU and 240x for hybrid multi-node
computer clusters with 16 computing nodes. The energy consumption is reduced to
74% using a single GPU compared to the equivalent parallel CPU cluster.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hossam_M/0/1/0/all/0/1"&gt;Mahmoud Hossam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Label Disentanglement in Partition-based Extreme Multilabel Classification. (arXiv:2106.12751v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.12751</id>
        <link href="http://arxiv.org/abs/2106.12751"/>
        <updated>2021-06-25T02:00:46.827Z</updated>
        <summary type="html"><![CDATA[Partition-based methods are increasingly-used in extreme multi-label
classification (XMC) problems due to their scalability to large output spaces
(e.g., millions or more). However, existing methods partition the large label
space into mutually exclusive clusters, which is sub-optimal when labels have
multi-modality and rich semantics. For instance, the label "Apple" can be the
fruit or the brand name, which leads to the following research question: can we
disentangle these multi-modal labels with non-exclusive clustering tailored for
downstream XMC tasks? In this paper, we show that the label assignment problem
in partition-based XMC can be formulated as an optimization problem, with the
objective of maximizing precision rates. This leads to an efficient algorithm
to form flexible and overlapped label clusters, and a method that can
alternatively optimizes the cluster assignments and the model parameters for
partition-based XMC. Experimental results on synthetic and real datasets show
that our method can successfully disentangle multi-modal labels, leading to
state-of-the-art (SOTA) results on four XMC benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xuanqing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chang_W/0/1/0/all/0/1"&gt;Wei-Cheng Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hsiang-Fu Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dhillon_I/0/1/0/all/0/1"&gt;Inderjit S. Dhillon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Fake Detection: Survey of Facial Manipulation Detection Solutions. (arXiv:2106.12605v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12605</id>
        <link href="http://arxiv.org/abs/2106.12605"/>
        <updated>2021-06-25T02:00:46.822Z</updated>
        <summary type="html"><![CDATA[Deep Learning as a field has been successfully used to solve a plethora of
complex problems, the likes of which we could not have imagined a few decades
back. But as many benefits as it brings, there are still ways in which it can
be used to bring harm to our society. Deep fakes have been proven to be one
such problem, and now more than ever, when any individual can create a fake
image or video simply using an application on the smartphone, there need to be
some countermeasures, with which we can detect if the image or video is a fake
or real and dispose of the problem threatening the trustworthiness of online
information. Although the Deep fakes created by neural networks, may seem to be
as real as a real image or video, it still leaves behind spatial and temporal
traces or signatures after moderation, these signatures while being invisible
to a human eye can be detected with the help of a neural network trained to
specialize in Deep fake detection. In this paper, we analyze several such
states of the art neural networks (MesoNet, ResNet-50, VGG-19, and Xception
Net) and compare them against each other, to find an optimal solution for
various scenarios like real-time deep fake detection to be deployed in online
social media platforms where the classification should be made as fast as
possible or for a small news agency where the classification need not be in
real-time but requires utmost accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pashine_S/0/1/0/all/0/1"&gt;Samay Pashine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mandiya_S/0/1/0/all/0/1"&gt;Sagar Mandiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1"&gt;Praveen Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheikh_R/0/1/0/all/0/1"&gt;Rashid Sheikh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A review of systematic selection of clustering algorithms and their evaluation. (arXiv:2106.12792v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12792</id>
        <link href="http://arxiv.org/abs/2106.12792"/>
        <updated>2021-06-25T02:00:46.816Z</updated>
        <summary type="html"><![CDATA[Data analysis plays an indispensable role for value creation in industry.
Cluster analysis in this context is able to explore given datasets with little
or no prior knowledge and to identify unknown patterns. As (big) data
complexity increases in the dimensions volume, variety, and velocity, this
becomes even more important. Many tools for cluster analysis have been
developed from early on and the variety of different clustering algorithms is
huge. As the selection of the right clustering procedure is crucial to the
results of the data analysis, users are in need for support on their journey of
extracting knowledge from raw data. Thus, the objective of this paper lies in
the identification of a systematic selection logic for clustering algorithms
and corresponding validation concepts. The goal is to enable potential users to
choose an algorithm that fits best to their needs and the properties of their
underlying data clustering problem. Moreover, users are supported in selecting
the right validation concepts to make sense of the clustering results. Based on
a comprehensive literature review, this paper provides assessment criteria for
clustering method evaluation and validation concept selection. The criteria are
applied to several common algorithms and the selection process of an algorithm
is supported by the introduction of pseudocode-based routines that consider the
underlying data structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wegmann_M/0/1/0/all/0/1"&gt;Marc Wegmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zipperling_D/0/1/0/all/0/1"&gt;Domenique Zipperling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hillenbrand_J/0/1/0/all/0/1"&gt;Jonas Hillenbrand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fleischer_J/0/1/0/all/0/1"&gt;J&amp;#xfc;rgen Fleischer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Partial Wasserstein and Maximum Mean Discrepancy distances for bridging the gap between outlier detection and drift detection. (arXiv:2106.12893v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12893</id>
        <link href="http://arxiv.org/abs/2106.12893"/>
        <updated>2021-06-25T02:00:46.789Z</updated>
        <summary type="html"><![CDATA[With the rise of machine learning and deep learning based applications in
practice, monitoring, i.e. verifying that these operate within specification,
has become an important practical problem. An important aspect of this
monitoring is to check whether the inputs (or intermediates) have strayed from
the distribution they were validated for, which can void the performance
assurances obtained during testing.

There are two common approaches for this. The, perhaps, more classical one is
outlier detection or novelty detection, where, for a single input we ask
whether it is an outlier, i.e. exceedingly unlikely to have originated from a
reference distribution. The second, perhaps more recent approach, is to
consider a larger number of inputs and compare its distribution to a reference
distribution (e.g. sampled during testing). This is done under the label drift
detection.

In this work, we bridge the gap between outlier detection and drift detection
through comparing a given number of inputs to an automatically chosen part of
the reference distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Viehmann_T/0/1/0/all/0/1"&gt;Thomas Viehmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learnable Fourier Features for Multi-Dimensional Spatial Positional Encoding. (arXiv:2106.02795v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02795</id>
        <link href="http://arxiv.org/abs/2106.02795"/>
        <updated>2021-06-25T02:00:46.778Z</updated>
        <summary type="html"><![CDATA[Attentional mechanisms are order-invariant. Positional encoding is a crucial
component to allow attention-based deep model architectures such as Transformer
to address sequences or images where the position of information matters. In
this paper, we propose a novel positional encoding method based on learnable
Fourier features. Instead of hard-coding each position as a token or a vector,
we represent each position, which can be multi-dimensional, as a trainable
encoding based on learnable Fourier feature mapping, modulated with a
multi-layer perceptron. The representation is particularly advantageous for a
spatial multi-dimensional position, e.g., pixel positions on an image, where
$L_2$ distances or more complex positional relationships need to be captured.
Our experiments based on several public benchmark tasks show that our learnable
Fourier feature representation for multi-dimensional positional encoding
outperforms existing methods by both improving the accuracy and allowing faster
convergence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1"&gt;Si Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Gang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_S/0/1/0/all/0/1"&gt;Samy Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting. (arXiv:2106.13008v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13008</id>
        <link href="http://arxiv.org/abs/2106.13008"/>
        <updated>2021-06-25T02:00:46.763Z</updated>
        <summary type="html"><![CDATA[Extending the forecasting time is a critical demand for real applications,
such as extreme weather early warning and long-term energy consumption
planning. This paper studies the \textit{long-term forecasting} problem of time
series. Prior Transformer-based models adopt various self-attention mechanisms
to discover the long-range dependencies. However, intricate temporal patterns
of the long-term future prohibit the model from finding reliable dependencies.
Also, Transformers have to adopt the sparse versions of point-wise
self-attentions for long series efficiency, resulting in the information
utilization bottleneck. Towards these challenges, we propose Autoformer as a
novel decomposition architecture with an Auto-Correlation mechanism. We go
beyond the pre-processing convention of series decomposition and renovate it as
a basic inner block of deep models. This design empowers Autoformer with
progressive decomposition capacities for complex time series. Further, inspired
by the stochastic process theory, we design the Auto-Correlation mechanism
based on the series periodicity, which conducts the dependencies discovery and
representation aggregation at the sub-series level. Auto-Correlation
outperforms self-attention in both efficiency and accuracy. In long-term
forecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative
improvement on six benchmarks, covering five practical applications: energy,
traffic, economics, weather and disease.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Haixu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jiehui Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianmin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1"&gt;Mingsheng Long&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Optimization with High-Dimensional Outputs. (arXiv:2106.12997v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12997</id>
        <link href="http://arxiv.org/abs/2106.12997"/>
        <updated>2021-06-25T02:00:46.752Z</updated>
        <summary type="html"><![CDATA[Bayesian Optimization is a sample-efficient black-box optimization procedure
that is typically applied to problems with a small number of independent
objectives. However, in practice we often wish to optimize objectives defined
over many correlated outcomes (or ``tasks"). For example, scientists may want
to optimize the coverage of a cell tower network across a dense grid of
locations. Similarly, engineers may seek to balance the performance of a robot
across dozens of different environments via constrained or robust optimization.
However, the Gaussian Process (GP) models typically used as probabilistic
surrogates for multi-task Bayesian Optimization scale poorly with the number of
outcomes, greatly limiting applicability. We devise an efficient technique for
exact multi-task GP sampling that combines exploiting Kronecker structure in
the covariance matrices with Matheron's identity, allowing us to perform
Bayesian Optimization using exact multi-task GP models with tens of thousands
of correlated outputs. In doing so, we achieve substantial improvements in
sample efficiency compared to existing approaches that only model aggregate
functions of the outcomes. We demonstrate how this unlocks a new class of
applications for Bayesian Optimization across a range of tasks in science and
engineering, including optimizing interference patterns of an optical
interferometer with more than 65,000 outputs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maddox_W/0/1/0/all/0/1"&gt;Wesley J. Maddox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balandat_M/0/1/0/all/0/1"&gt;Maximilian Balandat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1"&gt;Andrew Gordon Wilson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bakshy_E/0/1/0/all/0/1"&gt;Eytan Bakshy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Density Constrained Reinforcement Learning. (arXiv:2106.12764v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12764</id>
        <link href="http://arxiv.org/abs/2106.12764"/>
        <updated>2021-06-25T02:00:46.744Z</updated>
        <summary type="html"><![CDATA[We study constrained reinforcement learning (CRL) from a novel perspective by
setting constraints directly on state density functions, rather than the value
functions considered by previous works. State density has a clear physical and
mathematical interpretation, and is able to express a wide variety of
constraints such as resource limits and safety requirements. Density
constraints can also avoid the time-consuming process of designing and tuning
cost functions required by value function-based constraints to encode system
specifications. We leverage the duality between density functions and Q
functions to develop an effective algorithm to solve the density constrained RL
problem optimally and the constrains are guaranteed to be satisfied. We prove
that the proposed algorithm converges to a near-optimal solution with a bounded
error even when the policy update is imperfect. We use a set of comprehensive
experiments to demonstrate the advantages of our approach over state-of-the-art
CRL methods, with a wide range of density constrained tasks as well as standard
CRL benchmarks such as Safety-Gym.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1"&gt;Zengyi Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuxiao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1"&gt;Chuchu Fan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Objective discovery of dominant dynamical processes with intelligible machine learning. (arXiv:2106.12963v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12963</id>
        <link href="http://arxiv.org/abs/2106.12963"/>
        <updated>2021-06-25T02:00:46.739Z</updated>
        <summary type="html"><![CDATA[The advent of big data has vast potential for discovery in natural phenomena
ranging from climate science to medicine, but overwhelming complexity stymies
insight. Existing theory is often not able to succinctly describe salient
phenomena, and progress has largely relied on ad hoc definitions of dynamical
regimes to guide and focus exploration. We present a formal definition in which
the identification of dynamical regimes is formulated as an optimization
problem, and we propose an intelligible objective function. Furthermore, we
propose an unsupervised learning framework which eliminates the need for a
priori knowledge and ad hoc definitions; instead, the user need only choose
appropriate clustering and dimensionality reduction algorithms, and this choice
can be guided using our proposed objective function. We illustrate its
applicability with example problems drawn from ocean dynamics, tumor
angiogenesis, and turbulent boundary layers. Our method is a step towards
unbiased data exploration that allows serendipitous discovery within dynamical
systems, with the potential to propel the physical sciences forward.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaiser_B/0/1/0/all/0/1"&gt;Bryan E. Kaiser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saenz_J/0/1/0/all/0/1"&gt;Juan A. Saenz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sonnewald_M/0/1/0/all/0/1"&gt;Maike Sonnewald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Livescu_D/0/1/0/all/0/1"&gt;Daniel Livescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Handwritten Digit Recognition using Machine and Deep Learning Algorithms. (arXiv:2106.12614v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12614</id>
        <link href="http://arxiv.org/abs/2106.12614"/>
        <updated>2021-06-25T02:00:46.719Z</updated>
        <summary type="html"><![CDATA[The reliance of humans over machines has never been so high such that from
object classification in photographs to adding sound to silent movies
everything can be performed with the help of deep learning and machine learning
algorithms. Likewise, Handwritten text recognition is one of the significant
areas of research and development with a streaming number of possibilities that
could be attained. Handwriting recognition (HWR), also known as Handwritten
Text Recognition (HTR), is the ability of a computer to receive and interpret
intelligible handwritten input from sources such as paper documents,
photographs, touch-screens and other devices [1]. Apparently, in this paper, we
have performed handwritten digit recognition with the help of MNIST datasets
using Support Vector Machines (SVM), Multi-Layer Perceptron (MLP) and
Convolution Neural Network (CNN) models. Our main objective is to compare the
accuracy of the models stated above along with their execution time to get the
best possible model for digit recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pashine_S/0/1/0/all/0/1"&gt;Samay Pashine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dixit_R/0/1/0/all/0/1"&gt;Ritik Dixit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kushwah_R/0/1/0/all/0/1"&gt;Rishika Kushwah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TagRuler: Interactive Tool for Span-Level Data Programming by Demonstration. (arXiv:2106.12767v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12767</id>
        <link href="http://arxiv.org/abs/2106.12767"/>
        <updated>2021-06-25T02:00:46.712Z</updated>
        <summary type="html"><![CDATA[Despite rapid developments in the field of machine learning research,
collecting high-quality labels for supervised learning remains a bottleneck for
many applications. This difficulty is exacerbated by the fact that
state-of-the-art models for NLP tasks are becoming deeper and more complex,
often increasing the amount of training data required even for fine-tuning.
Weak supervision methods, including data programming, address this problem and
reduce the cost of label collection by using noisy label sources for
supervision. However, until recently, data programming was only accessible to
users who knew how to program. To bridge this gap, the Data Programming by
Demonstration framework was proposed to facilitate the automatic creation of
labeling functions based on a few examples labeled by a domain expert. This
framework has proven successful for generating high-accuracy labeling models
for document classification. In this work, we extend the DPBD framework to
span-level annotation tasks, arguably one of the most time-consuming NLP
labeling tasks. We built a novel tool, TagRuler, that makes it easy for
annotators to build span-level labeling functions without programming and
encourages them to explore trade-offs between different labeling models and
active learning strategies. We empirically demonstrated that an annotator could
achieve a higher F1 score using the proposed tool compared to manual labeling
for different span-level annotation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1"&gt;Dongjin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Evensen_S/0/1/0/all/0/1"&gt;Sara Evensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demiralp_C/0/1/0/all/0/1"&gt;&amp;#xc7;a&amp;#x11f;atay Demiralp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hruschka_E/0/1/0/all/0/1"&gt;Estevam Hruschka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Latency Federated Learning over Wireless Channels with Differential Privacy. (arXiv:2106.13039v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2106.13039</id>
        <link href="http://arxiv.org/abs/2106.13039"/>
        <updated>2021-06-25T02:00:46.696Z</updated>
        <summary type="html"><![CDATA[In federated learning (FL), model training is distributed over clients and
local models are aggregated by a central server. The performance of uploaded
models in such situations can vary widely due to imbalanced data distributions,
potential demands on privacy protections, and quality of transmissions. In this
paper, we aim to minimize FL training delay over wireless channels, constrained
by overall training performance as well as each client's differential privacy
(DP) requirement. We solve this problem in the framework of multi-agent
multi-armed bandit (MAMAB) to deal with the situation where there are multiple
clients confornting different unknown transmission environments, e.g., channel
fading and interferences. Specifically, we first transform the long-term
constraints on both training performance and each client's DP into a virtual
queue based on the Lyapunov drift technique. Then, we convert the MAMAB to a
max-min bipartite matching problem at each communication round, by estimating
rewards with the upper confidence bound (UCB) approach. More importantly, we
propose two efficient solutions to this matching problem, i.e., modified
Hungarian algorithm and greedy matching with a better alternative (GMBA), in
which the first one can achieve the optimal solution with a high complexity
while the second one approaches a better trade-off by enabling a verified
low-complexity with little performance loss. In addition, we develop an upper
bound on the expected regret of this MAMAB based FL framework, which shows a
linear growth over the logarithm of communication rounds, justifying its
theoretical feasibility. Extensive experimental results are conducted to
validate the effectiveness of our proposed algorithms, and the impacts of
various parameters on the FL performance over wireless edge networks are also
discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1"&gt;Kang Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1"&gt;Chuan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1"&gt;Ming Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Cailian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1"&gt;Shi Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1"&gt;Zhu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1"&gt;H. Vincent Poor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graceful Degradation and Related Fields. (arXiv:2106.11119v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11119</id>
        <link href="http://arxiv.org/abs/2106.11119"/>
        <updated>2021-06-25T02:00:46.682Z</updated>
        <summary type="html"><![CDATA[When machine learning models encounter data which is out of the distribution
on which they were trained they have a tendency to behave poorly, most
prominently over-confidence in erroneous predictions. Such behaviours will have
disastrous effects on real-world machine learning systems. In this field
graceful degradation refers to the optimisation of model performance as it
encounters this out-of-distribution data. This work presents a definition and
discussion of graceful degradation and where it can be applied in deployed
visual systems. Following this a survey of relevant areas is undertaken,
novelly splitting the graceful degradation problem into active and passive
approaches. In passive approaches, graceful degradation is handled and achieved
by the model in a self-contained manner, in active approaches the model is
updated upon encountering epistemic uncertainties. This work communicates the
importance of the problem and aims to prompt the development of machine
learning strategies that are aware of graceful degradation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dymond_J/0/1/0/all/0/1"&gt;Jack Dymond&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Partial Maximum Correntropy Regression for Robust Trajectory Decoding from Noisy Epidural Electrocorticographic Signals. (arXiv:2106.13086v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.13086</id>
        <link href="http://arxiv.org/abs/2106.13086"/>
        <updated>2021-06-25T02:00:46.676Z</updated>
        <summary type="html"><![CDATA[The Partial Least Square Regression (PLSR) algorithm exhibits exceptional
competence for predicting continuous variables from inter-correlated brain
recordings in brain-computer interfaces, which achieved successful prediction
from epidural electrocorticography of macaques to three-dimensional continuous
hand trajectories recently. Nevertheless, PLSR is in essence formulated based
on the least square criterion, thus, being non-robust with respect to
complicated noises consequently. The aim of the present study is to propose a
robust version of PLSR. To this end, the maximum correntropy criterion is
adopted to structure a new robust variant of PLSR, namely Partial Maximum
Correntropy Regression (PMCR). Half-quadratic optimization technique is
utilized to calculate the robust latent variables. We assess the proposed PMCR
on a synthetic example and the public Neurotycho dataset. Compared with the
conventional PLSR and the state-of-the-art variant, PMCR realized superior
prediction competence on three different performance indicators with
contaminated training set. The proposed PMCR was demonstrated as an effective
approach for robust decoding from noisy brain measurements, which could reduce
the performance degradation resulting from adverse noises, thus, improving the
decoding robustness of brain-computer interfaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuanhao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_B/0/1/0/all/0/1"&gt;Badong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1"&gt;Gang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yoshimura_N/0/1/0/all/0/1"&gt;Natsue Yoshimura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Koike_Y/0/1/0/all/0/1"&gt;Yasuharu Koike&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-learning for Multi-variable Non-convex Optimization Problems: Iterating Non-optimums Makes Optimum Possible. (arXiv:2009.04899v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04899</id>
        <link href="http://arxiv.org/abs/2009.04899"/>
        <updated>2021-06-25T02:00:46.653Z</updated>
        <summary type="html"><![CDATA[In this paper, we aim to address the problem of solving a non-convex
optimization problem over an intersection of multiple variable sets. This kind
of problems is typically solved by using an alternating minimization (AM)
strategy which splits the overall problem into a set of sub-problems
corresponding to each variable, and then iteratively performs minimization over
each sub-problem using a fixed updating rule. However, due to the intrinsic
non-convexity of the overall problem, the optimization can usually be trapped
into bad local minimum even when each sub-problem can be globally optimized at
each iteration. To tackle this problem, we propose a meta-learning based Global
Scope Optimization (GSO) method. It adaptively generates optimizers for
sub-problems via meta-learners and constantly updates these meta-learners with
respect to the global loss information of the overall problem. Therefore, the
sub-problems are optimized with the objective of minimizing the global loss
specifically. We evaluate the proposed model on a number of simulations,
including solving bi-linear inverse problems: matrix completion, and non-linear
problems: Gaussian mixture models. The experimental results show that our
proposed approach outperforms AM-based methods in standard settings, and is
able to achieve effective optimization in some challenging cases while other
methods would typically fail.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1"&gt;Jingyuan Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shengxi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jun-Jie Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaimoukha_I/0/1/0/all/0/1"&gt;Imad Jaimoukha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xinwang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-objective Asynchronous Successive Halving. (arXiv:2106.12639v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.12639</id>
        <link href="http://arxiv.org/abs/2106.12639"/>
        <updated>2021-06-25T02:00:46.633Z</updated>
        <summary type="html"><![CDATA[Hyperparameter optimization (HPO) is increasingly used to automatically tune
the predictive performance (e.g., accuracy) of machine learning models.
However, in a plethora of real-world applications, accuracy is only one of the
multiple -- often conflicting -- performance criteria, necessitating the
adoption of a multi-objective (MO) perspective. While the literature on MO
optimization is rich, few prior studies have focused on HPO. In this paper, we
propose algorithms that extend asynchronous successive halving (ASHA) to the MO
setting. Considering multiple evaluation metrics, we assess the performance of
these methods on three real world tasks: (i) Neural architecture search, (ii)
algorithmic fairness and (iii) language model optimization. Our empirical
analysis shows that MO ASHA enables to perform MO HPO at scale. Further, we
observe that that taking the entire Pareto front into account for candidate
selection consistently outperforms multi-fidelity HPO based on MO scalarization
in terms of wall-clock time. Our algorithms (to be open-sourced) establish new
baselines for future research in the area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Schmucker_R/0/1/0/all/0/1"&gt;Robin Schmucker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Donini_M/0/1/0/all/0/1"&gt;Michele Donini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zafar_M/0/1/0/all/0/1"&gt;Muhammad Bilal Zafar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Salinas_D/0/1/0/all/0/1"&gt;David Salinas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Archambeau_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric Archambeau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Monocular Depth Estimation of Untextured Indoor Rotated Scenes. (arXiv:2106.12958v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12958</id>
        <link href="http://arxiv.org/abs/2106.12958"/>
        <updated>2021-06-25T02:00:46.620Z</updated>
        <summary type="html"><![CDATA[Self-supervised deep learning methods have leveraged stereo images for
training monocular depth estimation. Although these methods show strong results
on outdoor datasets such as KITTI, they do not match performance of supervised
methods on indoor environments with camera rotation. Indoor, rotated scenes are
common for less constrained applications and pose problems for two reasons:
abundance of low texture regions and increased complexity of depth cues for
images under rotation. In an effort to extend self-supervised learning to more
generalised environments we propose two additions. First, we propose a novel
Filled Disparity Loss term that corrects for ambiguity of image reconstruction
error loss in textureless regions. Specifically, we interpolate disparity in
untextured regions, using the estimated disparity from surrounding textured
areas, and use L1 loss to correct the original estimation. Our experiments show
that depth estimation is substantially improved on low-texture scenes, without
any loss on textured scenes, when compared to Monodepth by Godard et al.
Secondly, we show that training with an application's representative rotations,
in both pitch and roll, is sufficient to significantly improve performance over
the entire range of expected rotation. We demonstrate that depth estimation is
successfully generalised as performance is not lost when evaluated on test sets
with no camera rotation. Together these developments enable a broader use of
self-supervised learning of monocular depth estimation for complex
environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Keltjens_B/0/1/0/all/0/1"&gt;Benjamin Keltjens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dijk_T/0/1/0/all/0/1"&gt;Tom van Dijk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Croon_G/0/1/0/all/0/1"&gt;Guido de Croon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Where can I drive? A System Approach: Deep Ego Corridor Estimation for Robust Automated Driving. (arXiv:2004.07639v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.07639</id>
        <link href="http://arxiv.org/abs/2004.07639"/>
        <updated>2021-06-25T02:00:46.614Z</updated>
        <summary type="html"><![CDATA[Lane detection is an essential part of the perception sub-architecture of any
automated driving (AD) or advanced driver assistance system (ADAS). When
focusing on low-cost, large scale products for automated driving, model-driven
approaches for the detection of lane markings have proven good performance.
More recently, data-driven approaches have been proposed that target the
drivable area / freespace mainly in inner-city applications. Focus of these
approaches is less on lane-based driving due to the fact that the lane concept
does not fully apply in unstructured, residential inner-city environments.
So-far the concept of drivable area is seldom used for highway and inter-urban
applications due to the specific requirements of these scenarios that require
clear lane associations of all traffic participants. We believe that
lane-based, mapless driving in inter-urban and highway scenarios is still not
fully handled with sufficient robustness and availability. Especially for
challenging weather situations such as heavy rain, fog, low-standing sun,
darkness or reflections in puddles, the mapless detection of lane markings
decreases significantly or completely fails. We see potential in applying
specifically designed data-driven freespace approaches in more lane-based
driving applications for highways and inter-urban use. Therefore, we propose to
classify specifically a drivable corridor of the ego lane on pixel level with a
deep learning approach. Our approach is kept computationally efficient with
only 0.66 million parameters allowing its application in large scale products.
Thus, we were able to easily integrate into an online AD system of a test
vehicle. We demonstrate the performance of our approach under challenging
conditions qualitatively and quantitatively in comparison to a state-of-the-art
model-driven approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Michalke_T/0/1/0/all/0/1"&gt;Thomas Michalke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1"&gt;Di Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glaser_C/0/1/0/all/0/1"&gt;Claudius Gl&amp;#xe4;ser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Timm_F/0/1/0/all/0/1"&gt;Fabian Timm&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse Needlets for Lighting Estimation with Spherical Transport Loss. (arXiv:2106.13090v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13090</id>
        <link href="http://arxiv.org/abs/2106.13090"/>
        <updated>2021-06-25T02:00:46.597Z</updated>
        <summary type="html"><![CDATA[Accurate lighting estimation is challenging yet critical to many computer
vision and computer graphics tasks such as high-dynamic-range (HDR) relighting.
Existing approaches model lighting in either frequency domain or spatial domain
which is insufficient to represent the complex lighting conditions in scenes
and tends to produce inaccurate estimation. This paper presents NeedleLight, a
new lighting estimation model that represents illumination with needlets and
allows lighting estimation in both frequency domain and spatial domain jointly.
An optimal thresholding function is designed to achieve sparse needlets which
trims redundant lighting parameters and demonstrates superior localization
properties for illumination representation. In addition, a novel spherical
transport loss is designed based on optimal transport theory which guides to
regress lighting representation parameters with consideration of the spatial
information. Furthermore, we propose a new metric that is concise yet effective
by directly evaluating the estimated illumination maps rather than rendered
images. Extensive experiments show that NeedleLight achieves superior lighting
estimation consistently across multiple evaluation metrics as compared with
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1"&gt;Fangneng Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Changgong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Wenbo Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shijian Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1"&gt;Feiying Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xuansong Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-Based Reinforcement Learning via Latent-Space Collocation. (arXiv:2106.13229v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13229</id>
        <link href="http://arxiv.org/abs/2106.13229"/>
        <updated>2021-06-25T02:00:46.581Z</updated>
        <summary type="html"><![CDATA[The ability to plan into the future while utilizing only raw high-dimensional
observations, such as images, can provide autonomous agents with broad
capabilities. Visual model-based reinforcement learning (RL) methods that plan
future actions directly have shown impressive results on tasks that require
only short-horizon reasoning, however, these methods struggle on temporally
extended tasks. We argue that it is easier to solve long-horizon tasks by
planning sequences of states rather than just actions, as the effects of
actions greatly compound over time and are harder to optimize. To achieve this,
we draw on the idea of collocation, which has shown good results on
long-horizon tasks in optimal control literature, and adapt it to the
image-based setting by utilizing learned latent state space models. The
resulting latent collocation method (LatCo) optimizes trajectories of latent
states, which improves over previously proposed shooting methods for visual
model-based RL on tasks with sparse rewards and long-term goals. Videos and
code at https://orybkin.github.io/latco/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rybkin_O/0/1/0/all/0/1"&gt;Oleh Rybkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1"&gt;Chuning Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagabandi_A/0/1/0/all/0/1"&gt;Anusha Nagabandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1"&gt;Kostas Daniilidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1"&gt;Igor Mordatch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextualized Attention-based Knowledge Transfer for Spoken Conversational Question Answering. (arXiv:2010.11066v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11066</id>
        <link href="http://arxiv.org/abs/2010.11066"/>
        <updated>2021-06-25T02:00:46.576Z</updated>
        <summary type="html"><![CDATA[Spoken conversational question answering (SCQA) requires machines to model
complex dialogue flow given the speech utterances and text corpora. Different
from traditional text question answering (QA) tasks, SCQA involves audio signal
processing, passage comprehension, and contextual understanding. However, ASR
systems introduce unexpected noisy signals to the transcriptions, which result
in performance degradation on SCQA. To overcome the problem, we propose CADNet,
a novel contextualized attention-based distillation approach, which applies
both cross-attention and self-attention to obtain ASR-robust contextualized
embedding representations of the passage and dialogue history for performance
improvements. We also introduce the spoken conventional knowledge distillation
framework to distill the ASR-robust knowledge from the estimated probabilities
of the teacher model to the student. We conduct extensive experiments on the
Spoken-CoQA dataset and demonstrate that our approach achieves remarkable
performance in this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenyu You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Uniform OPE and Model-based Offline Reinforcement Learning in Time-Homogeneous, Reward-Free and Task-Agnostic Settings. (arXiv:2105.06029v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06029</id>
        <link href="http://arxiv.org/abs/2105.06029"/>
        <updated>2021-06-25T02:00:46.570Z</updated>
        <summary type="html"><![CDATA[This work studies the statistical limits of uniform convergence for offline
policy evaluation (OPE) problems with model-based methods (for episodic MDP)
and provides a unified framework towards optimal learning for several
well-motivated offline tasks. Uniform OPE
$\sup_\Pi|Q^\pi-\hat{Q}^\pi|<\epsilon$ is a stronger measure than the
point-wise OPE and ensures offline learning when $\Pi$ contains all policies
(the global class). In this paper, we establish an $\Omega(H^2
S/d_m\epsilon^2)$ lower bound (over model-based family) for the global uniform
OPE and our main result establishes an upper bound of
$\tilde{O}(H^2/d_m\epsilon^2)$ for the \emph{local} uniform convergence that
applies to all \emph{near-empirically optimal} policies for the MDPs with
\emph{stationary} transition. Here $d_m$ is the minimal marginal state-action
probability. Critically, the highlight in achieving the optimal rate
$\tilde{O}(H^2/d_m\epsilon^2)$ is our design of \emph{singleton absorbing MDP},
which is a new sharp analysis tool that works with the model-based approach. We
generalize such a model-based framework to the new settings: offline
task-agnostic and the offline reward-free with optimal complexity
$\tilde{O}(H^2\log(K)/d_m\epsilon^2)$ ($K$ is the number of tasks) and
$\tilde{O}(H^2S/d_m\epsilon^2)$ respectively. These results provide a unified
solution for simultaneously solving different offline RL problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1"&gt;Ming Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu-Xiang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Black-Box Planning Using Macro-Actions with Focused Effects. (arXiv:2004.13242v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.13242</id>
        <link href="http://arxiv.org/abs/2004.13242"/>
        <updated>2021-06-25T02:00:46.565Z</updated>
        <summary type="html"><![CDATA[The difficulty of deterministic planning increases exponentially with
search-tree depth. Black-box planning presents an even greater challenge, since
planners must operate without an explicit model of the domain. Heuristics can
make search more efficient, but goal-aware heuristics for black-box planning
usually rely on goal counting, which is often quite uninformative. In this
work, we show how to overcome this limitation by discovering macro-actions that
make the goal-count heuristic more accurate. Our approach searches for
macro-actions with focused effects (i.e. macros that modify only a small number
of state variables), which align well with the assumptions made by the
goal-count heuristic. Focused macros dramatically improve black-box planning
efficiency across a wide range of planning domains, sometimes beating even
state-of-the-art planners with access to a full domain model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Allen_C/0/1/0/all/0/1"&gt;Cameron Allen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katz_M/0/1/0/all/0/1"&gt;Michael Katz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klinger_T/0/1/0/all/0/1"&gt;Tim Klinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konidaris_G/0/1/0/all/0/1"&gt;George Konidaris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riemer_M/0/1/0/all/0/1"&gt;Matthew Riemer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tesauro_G/0/1/0/all/0/1"&gt;Gerald Tesauro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language for Description of Worlds. (arXiv:2010.16243v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.16243</id>
        <link href="http://arxiv.org/abs/2010.16243"/>
        <updated>2021-06-25T02:00:46.547Z</updated>
        <summary type="html"><![CDATA[We will reduce the task of creating AI to the task of finding an appropriate
language for description of the world. This will not be a programing language
because programing languages describe only computable functions, while our
language will describe a somewhat broader class of functions. Another
specificity of this language will be that the description will consist of
separate modules. This will enable us look for the description of the world
automatically such that we discover it module after module. Our approach to the
creation of this new language will be to start with a particular world and
write the description of that particular world. The point is that the language
which can describe this particular world will be appropriate for describing any
world.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dobrev_D/0/1/0/all/0/1"&gt;Dimiter Dobrev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Agriculture Commodity Price Prediction System with Machine Learning Techniques. (arXiv:2106.12747v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12747</id>
        <link href="http://arxiv.org/abs/2106.12747"/>
        <updated>2021-06-25T02:00:46.541Z</updated>
        <summary type="html"><![CDATA[The intention of this research is to study and design an automated
agriculture commodity price prediction system with novel machine learning
techniques. Due to the increasing large amounts historical data of agricultural
commodity prices and the need of performing accurate prediction of price
fluctuations, the solution has largely shifted from statistical methods to
machine learning area. However, the selection of proper set from historical
data for forecasting still has limited consideration. On the other hand, when
implementing machine learning techniques, finding a suitable model with optimal
parameters for global solution, nonlinearity and avoiding curse of
dimensionality are still biggest challenges, therefore machine learning
strategies study are needed. In this research, we propose a web-based automated
system to predict agriculture commodity price. In the two series experiments,
five popular machine learning algorithms, ARIMA, SVR, Prophet, XGBoost and LSTM
have been compared with large historical datasets in Malaysia and the most
optimal algorithm, LSTM model with an average of 0.304 mean-square error has
been selected as the prediction engine of the proposed system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhiyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goh_H/0/1/0/all/0/1"&gt;Howe Seng Goh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sin_K/0/1/0/all/0/1"&gt;Kai Ling Sin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1"&gt;Kelly Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_N/0/1/0/all/0/1"&gt;Nicole Ka Hei Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liew_X/0/1/0/all/0/1"&gt;Xin Yu Liew&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Systematic Collection of Medical Image Datasets for Deep Learning. (arXiv:2106.12864v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12864</id>
        <link href="http://arxiv.org/abs/2106.12864"/>
        <updated>2021-06-25T02:00:46.535Z</updated>
        <summary type="html"><![CDATA[The astounding success made by artificial intelligence (AI) in healthcare and
other fields proves that AI can achieve human-like performance. However,
success always comes with challenges. Deep learning algorithms are
data-dependent and require large datasets for training. The lack of data in the
medical imaging field creates a bottleneck for the application of deep learning
to medical image analysis. Medical image acquisition, annotation, and analysis
are costly, and their usage is constrained by ethical restrictions. They also
require many resources, such as human expertise and funding. That makes it
difficult for non-medical researchers to have access to useful and large
medical data. Thus, as comprehensive as possible, this paper provides a
collection of medical image datasets with their associated challenges for deep
learning research. We have collected information of around three hundred
datasets and challenges mainly reported between 2013 and 2020 and categorized
them into four categories: head & neck, chest & abdomen, pathology & blood, and
``others''. Our paper has three purposes: 1) to provide a most up to date and
complete list that can be used as a universal reference to easily find the
datasets for clinical image analysis, 2) to guide researchers on the
methodology to test and evaluate their methods' performance and robustness on
relevant datasets, 3) to provide a ``route'' to relevant algorithms for the
relevant medical topics, and challenge leaderboards.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Johann Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_G/0/1/0/all/0/1"&gt;Guangming Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hua_C/0/1/0/all/0/1"&gt;Cong Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Feng_M/0/1/0/all/0/1"&gt;Mingtao Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+BasheerBennamoun/0/1/0/all/0/1"&gt;BasheerBennamoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_P/0/1/0/all/0/1"&gt;Ping Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaoyuan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Song_J/0/1/0/all/0/1"&gt;Juan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_P/0/1/0/all/0/1"&gt;Peiyi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mei_L/0/1/0/all/0/1"&gt;Lin Mei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shah_S/0/1/0/all/0/1"&gt;Syed Afaq Ali Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bennamoun_M/0/1/0/all/0/1"&gt;Mohammed Bennamoun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meaningfully Explaining a Model's Mistakes. (arXiv:2106.12723v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12723</id>
        <link href="http://arxiv.org/abs/2106.12723"/>
        <updated>2021-06-25T02:00:46.529Z</updated>
        <summary type="html"><![CDATA[Understanding and explaining the mistakes made by trained models is critical
to many machine learning objectives, such as improving robustness, addressing
concept drift, and mitigating biases. However, this is often an ad hoc process
that involves manually looking at the model's mistakes on many test samples and
guessing at the underlying reasons for those incorrect predictions. In this
paper, we propose a systematic approach, conceptual explanation scores (CES),
that explains why a classifier makes a mistake on a particular test sample(s)
in terms of human-understandable concepts (e.g. this zebra is misclassified as
a dog because of faint stripes). We base CES on two prior ideas: counterfactual
explanations and concept activation vectors, and validate our approach on
well-known pretrained models, showing that it explains the models' mistakes
meaningfully. We also train new models with intentional and known spurious
correlations, which CES successfully identifies from a single misclassified
test sample. The code for CES is publicly available and can easily be applied
to new models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abid_A/0/1/0/all/0/1"&gt;Abubakar Abid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1"&gt;James Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hamiltonian-based Neural ODE Networks on the SE(3) Manifold For Dynamics Learning and Control. (arXiv:2106.12782v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.12782</id>
        <link href="http://arxiv.org/abs/2106.12782"/>
        <updated>2021-06-25T02:00:46.524Z</updated>
        <summary type="html"><![CDATA[Accurate models of robot dynamics are critical for safe and stable control
and generalization to novel operational conditions. Hand-designed models,
however, may be insufficiently accurate, even after careful parameter tuning.
This motivates the use of machine learning techniques to approximate the robot
dynamics over a training set of state-control trajectories. The dynamics of
many robots, including ground, aerial, and underwater vehicles, are described
in terms of their SE(3) pose and generalized velocity, and satisfy conservation
of energy principles. This paper proposes a Hamiltonian formulation over the
SE(3) manifold of the structure of a neural ordinary differential equation
(ODE) network to approximate the dynamics of a rigid body. In contrast to a
black-box ODE network, our formulation guarantees total energy conservation by
construction. We develop energy shaping and damping injection control for the
learned, potentially under-actuated SE(3) Hamiltonian dynamics to enable a
unified approach for stabilization and trajectory tracking with various
platforms, including pendulum, rigid-body, and quadrotor systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duong_T/0/1/0/all/0/1"&gt;Thai Duong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atanasov_N/0/1/0/all/0/1"&gt;Nikolay Atanasov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating variational quantum algorithms with multiple quantum processors. (arXiv:2106.12819v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2106.12819</id>
        <link href="http://arxiv.org/abs/2106.12819"/>
        <updated>2021-06-25T02:00:46.508Z</updated>
        <summary type="html"><![CDATA[Variational quantum algorithms (VQAs) have the potential of utilizing
near-term quantum machines to gain certain computational advantages over
classical methods. Nevertheless, modern VQAs suffer from cumbersome
computational overhead, hampered by the tradition of employing a solitary
quantum processor to handle large-volume data. As such, to better exert the
superiority of VQAs, it is of great significance to improve their runtime
efficiency. Here we devise an efficient distributed optimization scheme, called
QUDIO, to address this issue. Specifically, in QUDIO, a classical central
server partitions the learning problem into multiple subproblems and allocate
them to multiple local nodes where each of them consists of a quantum processor
and a classical optimizer. During the training procedure, all local nodes
proceed parallel optimization and the classical server synchronizes
optimization information among local nodes timely. In doing so, we prove a
sublinear convergence rate of QUDIO in terms of the number of global iteration
under the ideal scenario, while the system imperfection may incur divergent
optimization. Numerical results on standard benchmarks demonstrate that QUDIO
can surprisingly achieve a superlinear runtime speedup with respect to the
number of local nodes. Our proposal can be readily mixed with other advanced
VQAs-based techniques to narrow the gap between the state of the art and
applications with quantum advantage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yuxuan Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Qian_Y/0/1/0/all/0/1"&gt;Yang Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Near-Optimal Algorithm for Debiasing Trained Machine Learning Models. (arXiv:2106.12887v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12887</id>
        <link href="http://arxiv.org/abs/2106.12887"/>
        <updated>2021-06-25T02:00:46.503Z</updated>
        <summary type="html"><![CDATA[We present a scalable post-processing algorithm for debiasing trained models,
including deep neural networks (DNNs), which we prove to be near-optimal by
bounding its excess Bayes risk. We empirically validate its advantages on
standard benchmark datasets across both classical algorithms as well as modern
DNN architectures and demonstrate that it outperforms previous post-processing
methods while performing on par with in-processing. In addition, we show that
the proposed algorithm is particularly effective for models trained at scale
where post-processing is a natural and practical choice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alabdulmohsin_I/0/1/0/all/0/1"&gt;Ibrahim Alabdulmohsin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lucic_M/0/1/0/all/0/1"&gt;Mario Lucic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mix and Mask Actor-Critic Methods. (arXiv:2106.13037v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13037</id>
        <link href="http://arxiv.org/abs/2106.13037"/>
        <updated>2021-06-25T02:00:46.498Z</updated>
        <summary type="html"><![CDATA[Shared feature spaces for actor-critic methods aims to capture generalized
latent representations to be used by the policy and value function with the
hopes for a more stable and sample-efficient optimization. However, such a
paradigm present a number of challenges in practice, as parameters generating a
shared representation must learn off two distinct objectives, resulting in
competing updates and learning perturbations. In this paper, we present a novel
feature-sharing framework to address these difficulties by introducing the mix
and mask mechanisms and the distributional scalarization technique. These
mechanisms behaves dynamically to couple and decouple connected latent features
variably between the policy and value function, while the distributional
scalarization standardizes the two objectives using a probabilistic standpoint.
From our experimental results, we demonstrate significant performance
improvements compared to alternative methods using separate networks and
networks with a shared backbone.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huh_D/0/1/0/all/0/1"&gt;Dom Huh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-19 cases prediction using regression and novel SSM model for non-converged countries. (arXiv:2106.12888v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12888</id>
        <link href="http://arxiv.org/abs/2106.12888"/>
        <updated>2021-06-25T02:00:46.493Z</updated>
        <summary type="html"><![CDATA[Anticipating the quantity of new associated or affirmed cases with novel
coronavirus ailment 2019 (COVID-19) is critical in the counteraction and
control of the COVID-19 flare-up. The new associated cases with COVID-19
information were gathered from 20 January 2020 to 21 July 2020. We filtered out
the countries which are converging and used those for training the network. We
utilized the SARIMAX, Linear regression model to anticipate new suspected
COVID-19 cases for the countries which did not converge yet. We predict the
curve of non-converged countries with the help of proposed Statistical SARIMAX
model (SSM). We present new information investigation-based forecast results
that can assist governments with planning their future activities and help
clinical administrations to be more ready for what's to come. Our framework can
foresee peak corona cases with an R-Squared value of 0.986 utilizing linear
regression and fall of this pandemic at various levels for countries like
India, US, and Brazil. We found that considering more countries for training
degrades the prediction process as constraints vary from nation to nation.
Thus, we expect that the outcomes referenced in this work will help individuals
to better understand the possibilities of this pandemic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_T/0/1/0/all/0/1"&gt;Tushar Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_U/0/1/0/all/0/1"&gt;Umang Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patil_R/0/1/0/all/0/1"&gt;Rupali Patil&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Construction Kit for Efficient Low Power Neural Network Accelerator Designs. (arXiv:2106.12810v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2106.12810</id>
        <link href="http://arxiv.org/abs/2106.12810"/>
        <updated>2021-06-25T02:00:46.488Z</updated>
        <summary type="html"><![CDATA[Implementing embedded neural network processing at the edge requires
efficient hardware acceleration that couples high computational performance
with low power consumption. Driven by the rapid evolution of network
architectures and their algorithmic features, accelerator designs are
constantly updated and improved. To evaluate and compare hardware design
choices, designers can refer to a myriad of accelerator implementations in the
literature. Surveys provide an overview of these works but are often limited to
system-level and benchmark-specific performance metrics, making it difficult to
quantitatively compare the individual effect of each utilized optimization
technique. This complicates the evaluation of optimizations for new accelerator
designs, slowing-down the research progress. This work provides a survey of
neural network accelerator optimization approaches that have been used in
recent works and reports their individual effects on edge processing
performance. It presents the list of optimizations and their quantitative
effects as a construction kit, allowing to assess the design choices for each
building block separately. Reported optimizations range from up to 10'000x
memory savings to 33x energy reductions, providing chip designers an overview
of design choices for implementing efficient low power neural network
accelerators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jokic_P/0/1/0/all/0/1"&gt;Petar Jokic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azarkhish_E/0/1/0/all/0/1"&gt;Erfan Azarkhish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bonetti_A/0/1/0/all/0/1"&gt;Andrea Bonetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pons_M/0/1/0/all/0/1"&gt;Marc Pons&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Emery_S/0/1/0/all/0/1"&gt;Stephane Emery&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1"&gt;Luca Benini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport. (arXiv:2106.12950v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12950</id>
        <link href="http://arxiv.org/abs/2106.12950"/>
        <updated>2021-06-25T02:00:46.472Z</updated>
        <summary type="html"><![CDATA[Successful quantitative investment usually relies on precise predictions of
the future movement of the stock price. Recently, machine learning based
solutions have shown their capacity to give more accurate stock prediction and
become indispensable components in modern quantitative investment systems.
However, the i.i.d. assumption behind existing methods is inconsistent with the
existence of diverse trading patterns in the stock market, which inevitably
limits their ability to achieve better stock prediction performance. In this
paper, we propose a novel architecture, Temporal Routing Adaptor (TRA), to
empower existing stock prediction models with the ability to model multiple
stock trading patterns. Essentially, TRA is a lightweight module that consists
of a set of independent predictors for learning multiple patterns as well as a
router to dispatch samples to different predictors. Nevertheless, the lack of
explicit pattern identifiers makes it quite challenging to train an effective
TRA-based model. To tackle this challenge, we further design a learning
algorithm based on Optimal Transport (OT) to obtain the optimal sample to
predictor assignment and effectively optimize the router with such assignment
through an auxiliary loss term. Experiments on the real-world stock ranking
task show that compared to the state-of-the-art baselines, e.g., Attention LSTM
and Transformer, the proposed method can improve information coefficient (IC)
from 0.053 to 0.059 and 0.051 to 0.056 respectively. Our dataset and code used
in this work are publicly available: https://github.com/microsoft/qlib.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Hengxu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Dong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weiqing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1"&gt;Jiang Bian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lettuce: PyTorch-based Lattice Boltzmann Framework. (arXiv:2106.12929v1 [physics.comp-ph])]]></title>
        <id>http://arxiv.org/abs/2106.12929</id>
        <link href="http://arxiv.org/abs/2106.12929"/>
        <updated>2021-06-25T02:00:46.467Z</updated>
        <summary type="html"><![CDATA[The lattice Boltzmann method (LBM) is an efficient simulation technique for
computational fluid mechanics and beyond. It is based on a simple
stream-and-collide algorithm on Cartesian grids, which is easily compatible
with modern machine learning architectures. While it is becoming increasingly
clear that deep learning can provide a decisive stimulus for classical
simulation techniques, recent studies have not addressed possible connections
between machine learning and LBM. Here, we introduce Lettuce, a PyTorch-based
LBM code with a threefold aim. Lettuce enables GPU accelerated calculations
with minimal source code, facilitates rapid prototyping of LBM models, and
enables integrating LBM simulations with PyTorch's deep learning and automatic
differentiation facility. As a proof of concept for combining machine learning
with the LBM, a neural collision model is developed, trained on a doubly
periodic shear layer and then transferred to a different flow, a decaying
turbulence. We also exemplify the added benefit of PyTorch's automatic
differentiation framework in flow control and optimization. To this end, the
spectrum of a forced isotropic turbulence is maintained without further
constraining the velocity field. The source code is freely available from
https://github.com/lettucecfd/lettuce.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Bedrunka_M/0/1/0/all/0/1"&gt;Mario Christopher Bedrunka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wilde_D/0/1/0/all/0/1"&gt;Dominik Wilde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kliemank_M/0/1/0/all/0/1"&gt;Martin Kliemank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Reith_D/0/1/0/all/0/1"&gt;Dirk Reith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Foysi_H/0/1/0/all/0/1"&gt;Holger Foysi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kramer_A/0/1/0/all/0/1"&gt;Andreas Kr&amp;#xe4;mer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[InFlow: Robust outlier detection utilizing Normalizing Flows. (arXiv:2106.12894v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12894</id>
        <link href="http://arxiv.org/abs/2106.12894"/>
        <updated>2021-06-25T02:00:46.453Z</updated>
        <summary type="html"><![CDATA[Normalizing flows are prominent deep generative models that provide tractable
probability distributions and efficient density estimation. However, they are
well known to fail while detecting Out-of-Distribution (OOD) inputs as they
directly encode the local features of the input representations in their latent
space. In this paper, we solve this overconfidence issue of normalizing flows
by demonstrating that flows, if extended by an attention mechanism, can
reliably detect outliers including adversarial attacks. Our approach does not
require outlier data for training and we showcase the efficiency of our method
for OOD detection by reporting state-of-the-art performance in diverse
experimental settings. Code available at
https://github.com/ComputationalRadiationPhysics/InFlow .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1"&gt;Nishant Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanfeld_P/0/1/0/all/0/1"&gt;Pia Hanfeld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hecht_M/0/1/0/all/0/1"&gt;Michael Hecht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bussmann_M/0/1/0/all/0/1"&gt;Michael Bussmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gumhold_S/0/1/0/all/0/1"&gt;Stefan Gumhold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoffmannn_N/0/1/0/all/0/1"&gt;Nico Hoffmannn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning-based Orchestration of Containers: A Taxonomy and Future Directions. (arXiv:2106.12739v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12739</id>
        <link href="http://arxiv.org/abs/2106.12739"/>
        <updated>2021-06-25T02:00:46.447Z</updated>
        <summary type="html"><![CDATA[Containerization is a lightweight application virtualization technology,
providing high environmental consistency, operating system distribution
portability, and resource isolation. Existing mainstream cloud service
providers have prevalently adopted container technologies in their distributed
system infrastructures for automated application management. To handle the
automation of deployment, maintenance, autoscaling, and networking of
containerized applications, container orchestration is proposed as an essential
research problem. However, the highly dynamic and diverse feature of cloud
workloads and environments considerably raises the complexity of orchestration
mechanisms. Machine learning algorithms are accordingly employed by container
orchestration systems for behavior modelling and prediction of
multi-dimensional performance metrics. Such insights could further improve the
quality of resource provisioning decisions in response to the changing
workloads under complex environments. In this paper, we present a comprehensive
literature review of existing machine learning-based container orchestration
approaches. Detailed taxonomies are proposed to classify the current researches
by their common features. Moreover, the evolution of machine learning-based
container orchestration technologies from the year 2016 to 2021 has been
designed based on objectives and metrics. A comparative analysis of the
reviewed techniques is conducted according to the proposed taxonomies, with
emphasis on their key characteristics. Finally, various open research
challenges and potential future directions are highlighted.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1"&gt;Zhiheng Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Minxian Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_M/0/1/0/all/0/1"&gt;Maria Alejandra Rodriguez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chengzhong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buyya_R/0/1/0/all/0/1"&gt;Rajkumar Buyya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural ODE to model and prognose thermoacoustic instability. (arXiv:2106.12758v1 [physics.flu-dyn])]]></title>
        <id>http://arxiv.org/abs/2106.12758</id>
        <link href="http://arxiv.org/abs/2106.12758"/>
        <updated>2021-06-25T02:00:46.441Z</updated>
        <summary type="html"><![CDATA[In reacting flow systems, thermoacoustic instability characterized by high
amplitude pressure fluctuations, is driven by a positive coupling between the
unsteady heat release rate and the acoustic field of the combustor. When the
underlying flow is turbulent, as a control parameter of the system is varied
and the system approach thermoacoustic instability, the acoustic pressure
oscillations synchronize with heat release rate oscillations. Consequently,
during the onset of thermoacoustic instability in turbulent combustors, the
system dynamics transition from chaotic oscillations to periodic oscillations
via a state of intermittency. Thermoacoustic systems are traditionally modeled
by coupling the model for the unsteady heat source and the acoustic subsystem,
each estimated independently. The response of the unsteady heat source, the
flame, to acoustic fluctuations are characterized by introducing external
unsteady forcing. This necessitates a powerful excitation module to obtain the
nonlinear response of the flame to acoustic perturbations. Instead of
characterizing individual subsystems, we introduce a neural ordinary
differential equation (neural ODE) framework to model the thermoacoustic system
as a whole. The neural ODE model for the thermoacoustic system uses time series
of the heat release rate and the pressure fluctuations, measured simultaneously
without introducing any external perturbations, to model their coupled
interaction. Further, we use the parameters of neural ODE to define an anomaly
measure that represents the proximity of system dynamics to limit cycle
oscillations and thus provide an early warning signal for the onset of
thermoacoustic instability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Dhadphale_J/0/1/0/all/0/1"&gt;Jayesh Dhadphale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Unni_V/0/1/0/all/0/1"&gt;Vishnu R. Unni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Saha_A/0/1/0/all/0/1"&gt;Abhishek Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Sujith_R/0/1/0/all/0/1"&gt;R. I. Sujith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Task-agnostic Continual Learning with Hybrid Probabilistic Models. (arXiv:2106.12772v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12772</id>
        <link href="http://arxiv.org/abs/2106.12772"/>
        <updated>2021-06-25T02:00:46.425Z</updated>
        <summary type="html"><![CDATA[Learning new tasks continuously without forgetting on a constantly changing
data distribution is essential for real-world problems but extremely
challenging for modern deep learning. In this work we propose HCL, a Hybrid
generative-discriminative approach to Continual Learning for classification. We
model the distribution of each task and each class with a normalizing flow. The
flow is used to learn the data distribution, perform classification, identify
task changes, and avoid forgetting, all leveraging the invertibility and exact
likelihood which are uniquely enabled by the normalizing flow model. We use the
generative capabilities of the flow to avoid catastrophic forgetting through
generative replay and a novel functional regularization technique. For task
identification, we use state-of-the-art anomaly detection techniques based on
measuring the typicality of the model's statistics. We demonstrate the strong
performance of HCL on a range of continual learning benchmarks such as
split-MNIST, split-CIFAR, and SVHN-MNIST.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kirichenko_P/0/1/0/all/0/1"&gt;Polina Kirichenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farajtabar_M/0/1/0/all/0/1"&gt;Mehrdad Farajtabar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_D/0/1/0/all/0/1"&gt;Dushyant Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1"&gt;Balaji Lakshminarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_N/0/1/0/all/0/1"&gt;Nir Levine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1"&gt;Ang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Huiyi Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1"&gt;Andrew Gordon Wilson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1"&gt;Razvan Pascanu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Private Algorithms for Clustering with Stability Assumptions. (arXiv:2106.12959v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12959</id>
        <link href="http://arxiv.org/abs/2106.12959"/>
        <updated>2021-06-25T02:00:46.406Z</updated>
        <summary type="html"><![CDATA[We study the problem of differentially private clustering under
input-stability assumptions. Despite the ever-growing volume of works on
differential privacy in general and differentially private clustering in
particular, only three works (Nissim et al. 2007, Wang et al. 2015, Huang et
al. 2018) looked at the problem of privately clustering "nice" k-means
instances, all three relying on the sample-and-aggregate framework and all
three measuring utility in terms of Wasserstein distance between the true
cluster centers and the centers returned by the private algorithm. In this work
we improve upon this line of works on multiple axes. We present a far simpler
algorithm for clustering stable inputs (not relying on the sample-and-aggregate
framework), and analyze its utility in both the Wasserstein distance and the
k-means cost. Moreover, our algorithm has straight-forward analogues for "nice"
k-median instances and for the local-model of differential privacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shechner_M/0/1/0/all/0/1"&gt;Moshe Shechner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DCoM: A Deep Column Mapper for Semantic Data Type Detection. (arXiv:2106.12871v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12871</id>
        <link href="http://arxiv.org/abs/2106.12871"/>
        <updated>2021-06-25T02:00:46.389Z</updated>
        <summary type="html"><![CDATA[Detection of semantic data types is a very crucial task in data science for
automated data cleaning, schema matching, data discovery, semantic data type
normalization and sensitive data identification. Existing methods include
regular expression-based or dictionary lookup-based methods that are not robust
to dirty as well unseen data and are limited to a very less number of semantic
data types to predict. Existing Machine Learning methods extract large number
of engineered features from data and build logistic regression, random forest
or feedforward neural network for this purpose. In this paper, we introduce
DCoM, a collection of multi-input NLP-based deep neural networks to detect
semantic data types where instead of extracting large number of features from
the data, we feed the raw values of columns (or instances) to the model as
texts. We train DCoM on 686,765 data columns extracted from VizNet corpus with
78 different semantic data types. DCoM outperforms other contemporary results
with a quite significant margin on the same dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1"&gt;Subhadip Maji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rout_S/0/1/0/all/0/1"&gt;Swapna Sourav Rout&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1"&gt;Sudeep Choudhary&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrent Neural Network from Adder's Perspective: Carry-lookahead RNN. (arXiv:2106.12901v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12901</id>
        <link href="http://arxiv.org/abs/2106.12901"/>
        <updated>2021-06-25T02:00:46.382Z</updated>
        <summary type="html"><![CDATA[The recurrent network architecture is a widely used model in sequence
modeling, but its serial dependency hinders the computation parallelization,
which makes the operation inefficient. The same problem was encountered in
serial adder at the early stage of digital electronics. In this paper, we
discuss the similarities between recurrent neural network (RNN) and serial
adder. Inspired by carry-lookahead adder, we introduce carry-lookahead module
to RNN, which makes it possible for RNN to run in parallel. Then, we design the
method of parallel RNN computation, and finally Carry-lookahead RNN (CL-RNN) is
proposed. CL-RNN takes advantages in parallelism and flexible receptive field.
Through a comprehensive set of tests, we verify that CL-RNN can perform better
than existing typical RNNs in sequence modeling tasks which are specially
designed for RNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haowei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_F/0/1/0/all/0/1"&gt;Feiwei Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jin Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1"&gt;Yong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yanli Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simple Truncated SVD based Model for Node Classification on Heterophilic Graphs. (arXiv:2106.12807v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12807</id>
        <link href="http://arxiv.org/abs/2106.12807"/>
        <updated>2021-06-25T02:00:46.362Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) have shown excellent performance on graphs that
exhibit strong homophily with respect to the node labels i.e. connected nodes
have same labels. However, they perform poorly on heterophilic graphs. Recent
approaches have typically modified aggregation schemes, designed adaptive graph
filters, etc. to address this limitation. In spite of this, the performance on
heterophilic graphs can still be poor. We propose a simple alternative method
that exploits Truncated Singular Value Decomposition (TSVD) of topological
structure and node features. Our approach achieves up to ~30% improvement in
performance over state-of-the-art methods on heterophilic graphs. This work is
an early investigation into methods that differ from aggregation based
approaches. Our experimental results suggest that it might be important to
explore other alternatives to aggregation methods for heterophilic setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lingam_V/0/1/0/all/0/1"&gt;Vijay Lingam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ragesh_R/0/1/0/all/0/1"&gt;Rahul Ragesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyer_A/0/1/0/all/0/1"&gt;Arun Iyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sellamanickam_S/0/1/0/all/0/1"&gt;Sundararajan Sellamanickam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Factors affecting the COVID-19 risk in the US counties: an innovative approach by combining unsupervised and supervised learning. (arXiv:2106.12766v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12766</id>
        <link href="http://arxiv.org/abs/2106.12766"/>
        <updated>2021-06-25T02:00:46.344Z</updated>
        <summary type="html"><![CDATA[The COVID-19 disease spreads swiftly, and nearly three months after the first
positive case was confirmed in China, Coronavirus started to spread all over
the United States. Some states and counties reported high number of positive
cases and deaths, while some reported lower COVID-19 related cases and
mortality. In this paper, the factors that could affect the risk of COVID-19
infection and mortality were analyzed in county level. An innovative method by
using K-means clustering and several classification models is utilized to
determine the most critical factors. Results showed that mean temperature,
percent of people below poverty, percent of adults with obesity, air pressure,
population density, wind speed, longitude, and percent of uninsured people were
the most significant attributes]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ziyadidegan_S/0/1/0/all/0/1"&gt;Samira Ziyadidegan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Razavi_M/0/1/0/all/0/1"&gt;Moein Razavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pesarakli_H/0/1/0/all/0/1"&gt;Homa Pesarakli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javid_A/0/1/0/all/0/1"&gt;Amir Hossein Javid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erraguntla_M/0/1/0/all/0/1"&gt;Madhav Erraguntla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visualizing Graph Neural Networks with CorGIE: Corresponding a Graph to Its Embedding. (arXiv:2106.12839v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12839</id>
        <link href="http://arxiv.org/abs/2106.12839"/>
        <updated>2021-06-25T02:00:46.338Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) are a class of powerful machine learning tools
that model node relations for making predictions of nodes or links. GNN
developers rely on quantitative metrics of the predictions to evaluate a GNN,
but similar to many other neural networks, it is difficult for them to
understand if the GNN truly learns characteristics of a graph as expected. We
propose an approach to corresponding an input graph to its node embedding (aka
latent space), a common component of GNNs that is later used for prediction. We
abstract the data and tasks, and develop an interactive multi-view interface
called CorGIE to instantiate the abstraction. As the key function in CorGIE, we
propose the K-hop graph layout to show topological neighbors in hops and their
clustering structure. To evaluate the functionality and usability of CorGIE, we
present how to use CorGIE in two usage scenarios, and conduct a case study with
two GNN experts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zipeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bernard_J/0/1/0/all/0/1"&gt;J&amp;#xfc;rgen Bernard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Munzner_T/0/1/0/all/0/1"&gt;Tamara Munzner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MARL with General Utilities via Decentralized Shadow Reward Actor-Critic. (arXiv:2106.00543v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00543</id>
        <link href="http://arxiv.org/abs/2106.00543"/>
        <updated>2021-06-25T02:00:46.318Z</updated>
        <summary type="html"><![CDATA[We posit a new mechanism for cooperation in multi-agent reinforcement
learning (MARL) based upon any nonlinear function of the team's long-term
state-action occupancy measure, i.e., a \emph{general utility}. This subsumes
the cumulative return but also allows one to incorporate risk-sensitivity,
exploration, and priors. % We derive the {\bf D}ecentralized {\bf S}hadow
Reward {\bf A}ctor-{\bf C}ritic (DSAC) in which agents alternate between policy
evaluation (critic), weighted averaging with neighbors (information mixing),
and local gradient updates for their policy parameters (actor). DSAC augments
the classic critic step by requiring agents to (i) estimate their local
occupancy measure in order to (ii) estimate the derivative of the local utility
with respect to their occupancy measure, i.e., the "shadow reward". DSAC
converges to $\epsilon$-stationarity in $\mathcal{O}(1/\epsilon^{2.5})$
(Theorem \ref{theorem:final}) or faster $\mathcal{O}(1/\epsilon^{2})$
(Corollary \ref{corollary:communication}) steps with high probability,
depending on the amount of communications. We further establish the
non-existence of spurious stationary points for this problem, that is, DSAC
finds the globally optimal policy (Corollary \ref{corollary:global}).
Experiments demonstrate the merits of goals beyond the cumulative return in
cooperative MARL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Junyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bedi_A/0/1/0/all/0/1"&gt;Amrit Singh Bedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mengdi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Koppel_A/0/1/0/all/0/1"&gt;Alec Koppel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Graph Neural Architecture Search from Message-passing. (arXiv:2103.14282v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14282</id>
        <link href="http://arxiv.org/abs/2103.14282"/>
        <updated>2021-06-25T02:00:46.312Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) emerged recently as a standard toolkit for
learning from data on graphs. Current GNN designing works depend on immense
human expertise to explore different message-passing mechanisms, and require
manual enumeration to determine the proper message-passing depth. Inspired by
the strong searching capability of neural architecture search (NAS) in CNN,
this paper proposes Graph Neural Architecture Search (GNAS) with novel-designed
search space. The GNAS can automatically learn better architecture with the
optimal depth of message passing on the graph. Specifically, we design Graph
Neural Architecture Paradigm (GAP) with tree-topology computation procedure and
two types of fine-grained atomic operations (feature filtering and neighbor
aggregation) from message-passing mechanism to construct powerful graph network
search space. Feature filtering performs adaptive feature selection, and
neighbor aggregation captures structural information and calculates neighbors'
statistics. Experiments show that our GNAS can search for better GNNs with
multiple message-passing mechanisms and optimal message-passing depth. The
searched network achieves remarkable improvement over state-of-the-art manual
designed and search-based GNNs on five large-scale datasets at three classical
graph tasks. Codes can be found at https://github.com/phython96/GNAS-MP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1"&gt;Shaofei Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Liang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Jincan Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Beichen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zheng-Jun Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1"&gt;Li Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qingming Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Composition of Saliency Metrics for Channel Pruning with a Myopic Oracle. (arXiv:2004.03376v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.03376</id>
        <link href="http://arxiv.org/abs/2004.03376"/>
        <updated>2021-06-25T02:00:46.306Z</updated>
        <summary type="html"><![CDATA[The computation and memory needed for Convolutional Neural Network (CNN)
inference can be reduced by pruning weights from the trained network. Pruning
is guided by a pruning saliency, which heuristically approximates the change in
the loss function associated with the removal of specific weights. Many pruning
signals have been proposed, but the performance of each heuristic depends on
the particular trained network. This leaves the data scientist with a difficult
choice. When using any one saliency metric for the entire pruning process, we
run the risk of the metric assumptions being invalidated, leading to poor
decisions being made by the metric. Ideally we could combine the best aspects
of different saliency metrics. However, despite an extensive literature review,
we are unable to find any prior work on composing different saliency metrics.
The chief difficulty lies in combining the numerical output of different
saliency metrics, which are not directly comparable.

We propose a method to compose several primitive pruning saliencies, to
exploit the cases where each saliency measure does well. Our experiments show
that the composition of saliencies avoids many poor pruning choices identified
by individual saliencies. In most cases our method finds better selections than
even the best individual pruning saliency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Persand_K/0/1/0/all/0/1"&gt;Kaveena Persand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_A/0/1/0/all/0/1"&gt;Andrew Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gregg_D/0/1/0/all/0/1"&gt;David Gregg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Receiver operating characteristic (ROC) movies, universal ROC (UROC) curves, and coefficient of predictive ability (CPA). (arXiv:1912.01956v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.01956</id>
        <link href="http://arxiv.org/abs/1912.01956"/>
        <updated>2021-06-25T02:00:46.288Z</updated>
        <summary type="html"><![CDATA[Throughout science and technology, receiver operating characteristic (ROC)
curves and associated area under the curve (AUC) measures constitute powerful
tools for assessing the predictive abilities of features, markers and tests in
binary classification problems. Despite its immense popularity, ROC analysis
has been subject to a fundamental restriction, in that it applies to
dichotomous (yes or no) outcomes only. Here we introduce ROC movies and
universal ROC (UROC) curves that apply to just any linearly ordered outcome,
along with an associated coefficient of predictive ability (CPA) measure. CPA
equals the area under the UROC curve, and admits appealing interpretations in
terms of probabilities and rank based covariances. For binary outcomes CPA
equals AUC, and for pairwise distinct outcomes CPA relates linearly to
Spearman's coefficient, in the same way that the C index relates linearly to
Kendall's coefficient. ROC movies, UROC curves, and CPA nest and generalize the
tools of classical ROC analysis, and are bound to supersede them in a wealth of
applications. Their usage is illustrated in data examples from biomedicine and
meteorology, where rank based measures yield new insights in the WeatherBench
comparison of the predictive performance of convolutional neural networks and
physical-numerical models for weather prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Gneiting_T/0/1/0/all/0/1"&gt;Tilmann Gneiting&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Walz_E/0/1/0/all/0/1"&gt;Eva-Maria Walz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Universal Adversarial Perturbations for CNN Classifiers in EEG-Based BCIs. (arXiv:1912.01171v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.01171</id>
        <link href="http://arxiv.org/abs/1912.01171"/>
        <updated>2021-06-25T02:00:46.268Z</updated>
        <summary type="html"><![CDATA[Multiple convolutional neural network (CNN) classifiers have been proposed
for electroencephalogram (EEG) based brain-computer interfaces (BCIs). However,
CNN models have been found vulnerable to universal adversarial perturbations
(UAPs), which are small and example-independent, yet powerful enough to degrade
the performance of a CNN model, when added to a benign example. This paper
proposes a novel total loss minimization (TLM) approach to generate UAPs for
EEG-based BCIs. Experimental results demonstrated the effectiveness of TLM on
three popular CNN classifiers for both target and non-target attacks. We also
verified the transferability of UAPs in EEG-based BCI systems. To our
knowledge, this is the first study on UAPs of CNN classifiers in EEG-based
BCIs. UAPs are easy to construct, and can attack BCIs in real-time, exposing a
potentially critical security concern of BCIs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zihan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1"&gt;Lubin Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1"&gt;Weili Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dongrui Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Bearing Fault Diagnosis Based on Model-Agnostic Meta-Learning. (arXiv:2007.12851v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.12851</id>
        <link href="http://arxiv.org/abs/2007.12851"/>
        <updated>2021-06-25T02:00:46.240Z</updated>
        <summary type="html"><![CDATA[The rapid development of artificial intelligence and deep learning has
provided many opportunities to further enhance the safety, stability, and
accuracy of industrial Cyber-Physical Systems (CPS). As indispensable
components to many mission-critical CPS assets and equipment, mechanical
bearings need to be monitored to identify any trace of abnormal conditions.
Most of the data-driven approaches applied to bearing fault diagnosis
up-to-date are trained using a large amount of fault data collected a priori.
In many practical applications, however, it can be unsafe and time-consuming to
collect sufficient data samples for each fault category, making it challenging
to train a robust classifier. In this paper, we propose a few-shot learning
framework for bearing fault diagnosis based on model-agnostic meta-learning
(MAML), which targets for training an effective fault classifier using limited
data. In addition, it can leverage the training data and learn to identify new
fault scenarios more efficiently. Case studies on the generalization to new
artificial faults show that the proposed framework achieves an overall accuracy
up to 25% higher than a Siamese network-based benchmark study. Finally, the
robustness and the generalization capability of the proposed framework are
further validated by applying it to identify real bearing damages using data
from artificial damages, which compares favorably against 6 state-of-the-art
few-shot learning algorithms using consistent test environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1"&gt;Fei Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bingnan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Habetler_T/0/1/0/all/0/1"&gt;Thomas G. Habetler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[L'Apprentissage Automatique dans la planification et le contr{\^o}le de la production : un {\'e}tat de l'art. (arXiv:2106.12916v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12916</id>
        <link href="http://arxiv.org/abs/2106.12916"/>
        <updated>2021-06-25T02:00:46.235Z</updated>
        <summary type="html"><![CDATA[Proper Production Planning and Control (PPC) is capital to have an edge over
competitors, reduce costs and respect delivery dates. With regard to PPC,
Machine Learning (ML) provides new opportunities to make intelligent decisions
based on data. Therefore, this communication provides an initial systematic
review of publications on ML applied in PPC. The research objective of this
study is twofold: firstly, it aims to identify techniques and tools allowing to
apply ML in PPC, and secondly, it reviews the characteristics of Industry 4.0
(I4.0) in recent research papers. Concerning the second objective, seven
characteristics of I4.0 are used in the analysis framework, from which two of
them are proposed by the authors. Additionally, the addressed domains of
ML-aided PPC in scientific literature are identified. Finally, results are
analyzed and gaps that may motivate further research are highlighted.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cadavid_J/0/1/0/all/0/1"&gt;Juan Pablo Usuga Cadavid&lt;/a&gt; (LAMIH, ENSAM), &lt;a href="http://arxiv.org/find/cs/1/au:+Lamouri_S/0/1/0/all/0/1"&gt;Samir Lamouri&lt;/a&gt; (LAMIH, ENSAM), &lt;a href="http://arxiv.org/find/cs/1/au:+Grabot_B/0/1/0/all/0/1"&gt;Bernard Grabot&lt;/a&gt; (LGP, ENIT), &lt;a href="http://arxiv.org/find/cs/1/au:+Fortin_A/0/1/0/all/0/1"&gt;Arnaud Fortin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the Spread of COVID-19 Epidemic: A Spatio-Temporal Point Process View. (arXiv:2106.13097v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13097</id>
        <link href="http://arxiv.org/abs/2106.13097"/>
        <updated>2021-06-25T02:00:46.219Z</updated>
        <summary type="html"><![CDATA[Since the first coronavirus case was identified in the U.S. on Jan. 21, more
than 1 million people in the U.S. have confirmed cases of COVID-19. This
infectious respiratory disease has spread rapidly across more than 3000
counties and 50 states in the U.S. and have exhibited evolutionary clustering
and complex triggering patterns. It is essential to understand the complex
spacetime intertwined propagation of this disease so that accurate prediction
or smart external intervention can be carried out. In this paper, we model the
propagation of the COVID-19 as spatio-temporal point processes and propose a
generative and intensity-free model to track the spread of the disease. We
further adopt a generative adversarial imitation learning framework to learn
the model parameters. In comparison with the traditional likelihood-based
learning methods, this imitation learning framework does not need to prespecify
an intensity function, which alleviates the model-misspecification. Moreover,
the adversarial learning procedure bypasses the difficult-to-evaluate integral
involved in the likelihood evaluation, which makes the model inference more
scalable with the data and variables. We showcase the dynamic learning
performance on the COVID-19 confirmed cases in the U.S. and evaluate the social
distancing policy based on the learned generative model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xinyun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yixiang Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yan Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prototype Completion with Primitive Knowledge for Few-Shot Learning. (arXiv:2009.04960v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04960</id>
        <link href="http://arxiv.org/abs/2009.04960"/>
        <updated>2021-06-25T02:00:46.214Z</updated>
        <summary type="html"><![CDATA[Few-shot learning is a challenging task, which aims to learn a classifier for
novel classes with few examples. Pre-training based meta-learning methods
effectively tackle the problem by pre-training a feature extractor and then
fine-tuning it through the nearest centroid based meta-learning. However,
results show that the fine-tuning step makes very marginal improvements. In
this paper, 1) we figure out the key reason, i.e., in the pre-trained feature
space, the base classes already form compact clusters while novel classes
spread as groups with large variances, which implies that fine-tuning the
feature extractor is less meaningful; 2) instead of fine-tuning the feature
extractor, we focus on estimating more representative prototypes during
meta-learning. Consequently, we propose a novel prototype completion based
meta-learning framework. This framework first introduces primitive knowledge
(i.e., class-level part or attribute annotations) and extracts representative
attribute features as priors. Then, we design a prototype completion network to
learn to complete prototypes with these priors. To avoid the prototype
completion error caused by primitive knowledge noises or class differences, we
further develop a Gaussian based prototype fusion strategy that combines the
mean-based and completed prototypes by exploiting the unlabeled samples.
Extensive experiments show that our method: (i) can obtain more accurate
prototypes; (ii) outperforms state-of-the-art techniques by 2% - 9% in terms of
classification accuracy. Our code is available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Baoquan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xutao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yunming Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhichao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lisai Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Projective Splitting: Solving Saddle-Point Problems with Multiple Regularizers. (arXiv:2106.13067v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.13067</id>
        <link href="http://arxiv.org/abs/2106.13067"/>
        <updated>2021-06-25T02:00:46.208Z</updated>
        <summary type="html"><![CDATA[We present a new, stochastic variant of the projective splitting (PS) family
of algorithms for monotone inclusion problems. It can solve min-max and
noncooperative game formulations arising in applications such as robust ML
without the convergence issues associated with gradient descent-ascent, the
current de facto standard approach in such situations. Our proposal is the
first version of PS able to use stochastic (as opposed to deterministic)
gradient oracles. It is also the first stochastic method that can solve min-max
games while easily handling multiple constraints and nonsmooth regularizers via
projection and proximal operators. We close with numerical experiments on a
distributionally robust sparse logistic regression problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Johnstone_P/0/1/0/all/0/1"&gt;Patrick R. Johnstone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Eckstein_J/0/1/0/all/0/1"&gt;Jonathan Eckstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Flynn_T/0/1/0/all/0/1"&gt;Thomas Flynn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yoo_S/0/1/0/all/0/1"&gt;Shinjae Yoo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GNMR: A provable one-line algorithm for low rank matrix recovery. (arXiv:2106.12933v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.12933</id>
        <link href="http://arxiv.org/abs/2106.12933"/>
        <updated>2021-06-25T02:00:46.203Z</updated>
        <summary type="html"><![CDATA[Low rank matrix recovery problems, including matrix completion and matrix
sensing, appear in a broad range of applications. In this work we present GNMR
-- an extremely simple iterative algorithm for low rank matrix recovery, based
on a Gauss-Newton linearization. On the theoretical front, we derive recovery
guarantees for GNMR in both the matrix sensing and matrix completion settings.
A key property of GNMR is that it implicitly keeps the factor matrices
approximately balanced throughout its iterations. On the empirical front, we
show that for matrix completion with uniform sampling, GNMR performs better
than several popular methods, especially when given very few observations close
to the information limit.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Zilber_P/0/1/0/all/0/1"&gt;Pini Zilber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Nadler_B/0/1/0/all/0/1"&gt;Boaz Nadler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Saliency-based Explainability Method. (arXiv:2106.12773v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12773</id>
        <link href="http://arxiv.org/abs/2106.12773"/>
        <updated>2021-06-25T02:00:46.198Z</updated>
        <summary type="html"><![CDATA[A particular class of Explainable AI (XAI) methods provide saliency maps to
highlight part of the image a Convolutional Neural Network (CNN) model looks at
to classify the image as a way to explain its working. These methods provide an
intuitive way for users to understand predictions made by CNNs. Other than
quantitative computational tests, the vast majority of evidence to highlight
that the methods are valuable is anecdotal. Given that humans would be the
end-users of such methods, we devise three human subject experiments through
which we gauge the effectiveness of these saliency-based explainability
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samuel_S/0/1/0/all/0/1"&gt;Sam Zabdiel Sunder Samuel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamakshi_V/0/1/0/all/0/1"&gt;Vidhya Kamakshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lodhi_N/0/1/0/all/0/1"&gt;Namrata Lodhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1"&gt;Narayanan C Krishnan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Fully Interpretable Deep Neural Networks: Are We There Yet?. (arXiv:2106.13164v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13164</id>
        <link href="http://arxiv.org/abs/2106.13164"/>
        <updated>2021-06-25T02:00:46.193Z</updated>
        <summary type="html"><![CDATA[Despite the remarkable performance, Deep Neural Networks (DNNs) behave as
black-boxes hindering user trust in Artificial Intelligence (AI) systems.
Research on opening black-box DNN can be broadly categorized into post-hoc
methods and inherently interpretable DNNs. While many surveys have been
conducted on post-hoc interpretation methods, little effort is devoted to
inherently interpretable DNNs. This paper provides a review of existing methods
to develop DNNs with intrinsic interpretability, with a focus on Convolutional
Neural Networks (CNNs). The aim is to understand the current progress towards
fully interpretable DNNs that can cater to different interpretation
requirements. Finally, we identify gaps in current work and suggest potential
research directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wickramanayake_S/0/1/0/all/0/1"&gt;Sandareka Wickramanayake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Wynne Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Mong Li Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Representation Models for Text Classification with AutoML Tools. (arXiv:2106.12798v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12798</id>
        <link href="http://arxiv.org/abs/2106.12798"/>
        <updated>2021-06-25T02:00:46.188Z</updated>
        <summary type="html"><![CDATA[Automated Machine Learning (AutoML) has gained increasing success on tabular
data in recent years. However, processing unstructured data like text is a
challenge and not widely supported by open-source AutoML tools. This work
compares three manually created text representations and text embeddings
automatically created by AutoML tools. Our benchmark includes four popular
open-source AutoML tools and eight datasets for text classification purposes.
The results show that straightforward text representations perform better than
AutoML tools with automatically created text embeddings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brandle_S/0/1/0/all/0/1"&gt;Sebastian Br&amp;#xe4;ndle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanussek_M/0/1/0/all/0/1"&gt;Marc Hanussek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blohm_M/0/1/0/all/0/1"&gt;Matthias Blohm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kintz_M/0/1/0/all/0/1"&gt;Maximilien Kintz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VinDr-SpineXR: A deep learning framework for spinal lesions detection and classification from radiographs. (arXiv:2106.12930v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12930</id>
        <link href="http://arxiv.org/abs/2106.12930"/>
        <updated>2021-06-25T02:00:46.173Z</updated>
        <summary type="html"><![CDATA[Radiographs are used as the most important imaging tool for identifying spine
anomalies in clinical practice. The evaluation of spinal bone lesions, however,
is a challenging task for radiologists. This work aims at developing and
evaluating a deep learning-based framework, named VinDr-SpineXR, for the
classification and localization of abnormalities from spine X-rays. First, we
build a large dataset, comprising 10,468 spine X-ray images from 5,000 studies,
each of which is manually annotated by an experienced radiologist with bounding
boxes around abnormal findings in 13 categories. Using this dataset, we then
train a deep learning classifier to determine whether a spine scan is abnormal
and a detector to localize 7 crucial findings amongst the total 13. The
VinDr-SpineXR is evaluated on a test set of 2,078 images from 1,000 studies,
which is kept separate from the training set. It demonstrates an area under the
receiver operating characteristic curve (AUROC) of 88.61% (95% CI 87.19%,
90.02%) for the image-level classification task and a mean average precision
(mAP@0.5) of 33.56% for the lesion-level localization task. These results serve
as a proof of concept and set a baseline for future research in this direction.
To encourage advances, the dataset, codes, and trained deep learning models are
made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Hieu T. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pham_H/0/1/0/all/0/1"&gt;Hieu H. Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_N/0/1/0/all/0/1"&gt;Nghia T. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Ha Q. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huynh_T/0/1/0/all/0/1"&gt;Thang Q. Huynh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dao_M/0/1/0/all/0/1"&gt;Minh Dao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vu_V/0/1/0/all/0/1"&gt;Van Vu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Monocular Depth Estimation of Untextured Indoor Rotated Scenes. (arXiv:2106.12958v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12958</id>
        <link href="http://arxiv.org/abs/2106.12958"/>
        <updated>2021-06-25T02:00:46.167Z</updated>
        <summary type="html"><![CDATA[Self-supervised deep learning methods have leveraged stereo images for
training monocular depth estimation. Although these methods show strong results
on outdoor datasets such as KITTI, they do not match performance of supervised
methods on indoor environments with camera rotation. Indoor, rotated scenes are
common for less constrained applications and pose problems for two reasons:
abundance of low texture regions and increased complexity of depth cues for
images under rotation. In an effort to extend self-supervised learning to more
generalised environments we propose two additions. First, we propose a novel
Filled Disparity Loss term that corrects for ambiguity of image reconstruction
error loss in textureless regions. Specifically, we interpolate disparity in
untextured regions, using the estimated disparity from surrounding textured
areas, and use L1 loss to correct the original estimation. Our experiments show
that depth estimation is substantially improved on low-texture scenes, without
any loss on textured scenes, when compared to Monodepth by Godard et al.
Secondly, we show that training with an application's representative rotations,
in both pitch and roll, is sufficient to significantly improve performance over
the entire range of expected rotation. We demonstrate that depth estimation is
successfully generalised as performance is not lost when evaluated on test sets
with no camera rotation. Together these developments enable a broader use of
self-supervised learning of monocular depth estimation for complex
environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Keltjens_B/0/1/0/all/0/1"&gt;Benjamin Keltjens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dijk_T/0/1/0/all/0/1"&gt;Tom van Dijk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Croon_G/0/1/0/all/0/1"&gt;Guido de Croon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Tensor Contraction via Fast Count Sketch. (arXiv:2106.13062v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13062</id>
        <link href="http://arxiv.org/abs/2106.13062"/>
        <updated>2021-06-25T02:00:46.162Z</updated>
        <summary type="html"><![CDATA[Sketching uses randomized Hash functions for dimensionality reduction and
acceleration. The existing sketching methods, such as count sketch (CS), tensor
sketch (TS), and higher-order count sketch (HCS), either suffer from low
accuracy or slow speed in some tensor based applications. In this paper, the
proposed fast count sketch (FCS) applies multiple shorter Hash functions based
CS to the vector form of the input tensor, which is more accurate than TS since
the spatial information of the input tensor can be preserved more sufficiently.
When the input tensor admits CANDECOMP/PARAFAC decomposition (CPD), FCS can
accelerate CS and HCS by using fast Fourier transform, which exhibits a
computational complexity asymptotically identical to TS for low-order tensors.
The effectiveness of FCS is validated by CPD, tensor regression network
compression, and Kronecker product compression. Experimental results show its
superior performance in terms of approximation accuracy and computational
efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1"&gt;Xingyu Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiani Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fold2Seq: A Joint Sequence(1D)-Fold(3D) Embedding-based Generative Model for Protein Design. (arXiv:2106.13058v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13058</id>
        <link href="http://arxiv.org/abs/2106.13058"/>
        <updated>2021-06-25T02:00:46.147Z</updated>
        <summary type="html"><![CDATA[Designing novel protein sequences for a desired 3D topological fold is a
fundamental yet non-trivial task in protein engineering. Challenges exist due
to the complex sequence--fold relationship, as well as the difficulties to
capture the diversity of the sequences (therefore structures and functions)
within a fold. To overcome these challenges, we propose Fold2Seq, a novel
transformer-based generative framework for designing protein sequences
conditioned on a specific target fold. To model the complex sequence--structure
relationship, Fold2Seq jointly learns a sequence embedding using a transformer
and a fold embedding from the density of secondary structural elements in 3D
voxels. On test sets with single, high-resolution and complete structure inputs
for individual folds, our experiments demonstrate improved or comparable
performance of Fold2Seq in terms of speed, coverage, and reliability for
sequence design, when compared to existing state-of-the-art methods that
include data-driven deep generative models and physics-based RosettaDesign. The
unique advantages of fold-based Fold2Seq, in comparison to a structure-based
deep model and RosettaDesign, become more evident on three additional
real-world challenges originating from low-quality, incomplete, or ambiguous
input structures. Source code and data are available at
https://github.com/IBM/fold2seq.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yue Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1"&gt;Payel Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chenthamarakshan_V/0/1/0/all/0/1"&gt;Vijil Chenthamarakshan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melnyk_I/0/1/0/all/0/1"&gt;Igor Melnyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yang Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Frequency Domain Convolutional Neural Network: Accelerated CNN for Large Diabetic Retinopathy Image Classification. (arXiv:2106.12736v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12736</id>
        <link href="http://arxiv.org/abs/2106.12736"/>
        <updated>2021-06-25T02:00:46.141Z</updated>
        <summary type="html"><![CDATA[The conventional spatial convolution layers in the Convolutional Neural
Networks (CNNs) are computationally expensive at the point where the training
time could take days unless the number of layers, the number of training images
or the size of the training images are reduced. The image size of 256x256
pixels is commonly used for most of the applications of CNN, but this image
size is too small for applications like Diabetic Retinopathy (DR)
classification where the image details are important for accurate
classification. This research proposed Frequency Domain Convolution (FDC) and
Frequency Domain Pooling (FDP) layers which were built with RFFT, kernel
initialization strategy, convolution artifact removal and Channel Independent
Convolution (CIC) to replace the conventional convolution and pooling layers.
The FDC and FDP layers are used to build a Frequency Domain Convolutional
Neural Network (FDCNN) to accelerate the training of large images for DR
classification. The Full FDC layer is an extension of the FDC layer to allow
direct use in conventional CNNs, it is also used to modify the VGG16
architecture. FDCNN is shown to be at least 54.21% faster and 70.74% more
memory efficient compared to an equivalent CNN architecture. The modified VGG16
architecture with Full FDC layer is reported to achieve a shorter training time
and a higher accuracy at 95.63% compared to the original VGG16 architecture for
DR classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goh_E/0/1/0/all/0/1"&gt;Ee Fey Goh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;ZhiYuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_W/0/1/0/all/0/1"&gt;Wei Xiang Lim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AudioCLIP: Extending CLIP to Image, Text and Audio. (arXiv:2106.13043v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.13043</id>
        <link href="http://arxiv.org/abs/2106.13043"/>
        <updated>2021-06-25T02:00:46.136Z</updated>
        <summary type="html"><![CDATA[In the past, the rapidly evolving field of sound classification greatly
benefited from the application of methods from other domains. Today, we observe
the trend to fuse domain-specific tasks and approaches together, which provides
the community with new outstanding models.

In this work, we present an extension of the CLIP model that handles audio in
addition to text and images. Our proposed model incorporates the ESResNeXt
audio-model into the CLIP framework using the AudioSet dataset. Such a
combination enables the proposed model to perform bimodal and unimodal
classification and querying, while keeping CLIP's ability to generalize to
unseen datasets in a zero-shot inference fashion.

AudioCLIP achieves new state-of-the-art results in the Environmental Sound
Classification (ESC) task, out-performing other approaches by reaching
accuracies of 90.07% on the UrbanSound8K and 97.15% on the ESC-50 datasets.
Further it sets new baselines in the zero-shot ESC-task on the same datasets
68.78% and 69.40%, respectively).

Finally, we also assess the cross-modal querying performance of the proposed
model as well as the influence of full and partial training on the results. For
the sake of reproducibility, our code is published.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guzhov_A/0/1/0/all/0/1"&gt;Andrey Guzhov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raue_F/0/1/0/all/0/1"&gt;Federico Raue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hees_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rn Hees&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1"&gt;Andreas Dengel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning by Planning: Language-Guided Global Image Editing. (arXiv:2106.13156v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13156</id>
        <link href="http://arxiv.org/abs/2106.13156"/>
        <updated>2021-06-25T02:00:46.129Z</updated>
        <summary type="html"><![CDATA[Recently, language-guided global image editing draws increasing attention
with growing application potentials. However, previous GAN-based methods are
not only confined to domain-specific, low-resolution data but also lacking in
interpretability. To overcome the collective difficulties, we develop a
text-to-operation model to map the vague editing language request into a series
of editing operations, e.g., change contrast, brightness, and saturation. Each
operation is interpretable and differentiable. Furthermore, the only
supervision in the task is the target image, which is insufficient for a stable
training of sequential decisions. Hence, we propose a novel operation planning
algorithm to generate possible editing sequences from the target image as
pseudo ground truth. Comparison experiments on the newly collected MA5k-Req
dataset and GIER dataset show the advantages of our methods. Code is available
at https://jshi31.github.io/T2ONet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jing Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1"&gt;Ning Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yihang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1"&gt;Trung Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1"&gt;Franck Dernoncourt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chenliang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fund2Vec: Mutual Funds Similarity using Graph Learning. (arXiv:2106.12987v1 [q-fin.ST])]]></title>
        <id>http://arxiv.org/abs/2106.12987</id>
        <link href="http://arxiv.org/abs/2106.12987"/>
        <updated>2021-06-25T02:00:46.123Z</updated>
        <summary type="html"><![CDATA[Identifying similar mutual funds with respect to the underlying portfolios
has found many applications in financial services ranging from fund recommender
systems, competitors analysis, portfolio analytics, marketing and sales, etc.
The traditional methods are either qualitative, and hence prone to biases and
often not reproducible, or, are known not to capture all the nuances
(non-linearities) among the portfolios from the raw data. We propose a
radically new approach to identify similar funds based on the weighted
bipartite network representation of funds and their underlying assets data
using a sophisticated machine learning method called Node2Vec which learns an
embedded low-dimensional representation of the network. We call the embedding
\emph{Fund2Vec}. Ours is the first ever study of the weighted bipartite network
representation of the funds-assets network in its original form that identifies
structural similarity among portfolios as opposed to merely portfolio overlaps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Satone_V/0/1/0/all/0/1"&gt;Vipul Satone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Desai_D/0/1/0/all/0/1"&gt;Dhruv Desai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Mehta_D/0/1/0/all/0/1"&gt;Dhagash Mehta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conversational Question Answering over Knowledge Graphs with Transformer and Graph Attention Networks. (arXiv:2104.01569v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01569</id>
        <link href="http://arxiv.org/abs/2104.01569"/>
        <updated>2021-06-25T02:00:46.117Z</updated>
        <summary type="html"><![CDATA[This paper addresses the task of (complex) conversational question answering
over a knowledge graph. For this task, we propose LASAGNE (muLti-task semAntic
parSing with trAnsformer and Graph atteNtion nEtworks). It is the first
approach, which employs a transformer architecture extended with Graph
Attention Networks for multi-task neural semantic parsing. LASAGNE uses a
transformer model for generating the base logical forms, while the Graph
Attention model is used to exploit correlations between (entity) types and
predicates to produce node representations. LASAGNE also includes a novel
entity recognition module which detects, links, and ranks all relevant entities
in the question context. We evaluate LASAGNE on a standard dataset for complex
sequential question answering, on which it outperforms existing baseline
averages on all question types. Specifically, we show that LASAGNE improves the
F1-score on eight out of ten question types; in some cases, the increase in
F1-score is more than 20% compared to the state of the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kacupaj_E/0/1/0/all/0/1"&gt;Endri Kacupaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plepi_J/0/1/0/all/0/1"&gt;Joan Plepi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1"&gt;Kuldeep Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thakkar_H/0/1/0/all/0/1"&gt;Harsh Thakkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1"&gt;Jens Lehmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maleshkova_M/0/1/0/all/0/1"&gt;Maria Maleshkova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepAuditor: Distributed Online Intrusion Detection System for IoT devices via Power Side-channel Auditing. (arXiv:2106.12753v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.12753</id>
        <link href="http://arxiv.org/abs/2106.12753"/>
        <updated>2021-06-25T02:00:46.105Z</updated>
        <summary type="html"><![CDATA[As the number of IoT devices has increased rapidly, IoT botnets have
exploited the vulnerabilities of IoT devices. However, it is still challenging
to detect the initial intrusion on IoT devices prior to massive attacks. Recent
studies have utilized power side-channel information to characterize this
intrusion behavior on IoT devices but still lack real-time detection
approaches. This study aimed to design an online intrusion detection system
called DeepAuditor for IoT devices via power auditing. To realize the real-time
system, we first proposed a lightweight power auditing device called Power
Auditor. With the Power Auditor, we developed a Distributed CNN classifier for
online inference in our laboratory setting. In order to protect data leakage
and reduce networking redundancy, we also proposed a privacy-preserved
inference protocol via Packed Homomorphic Encryption and a sliding window
protocol in our system. The classification accuracy and processing time were
measured in our laboratory settings. We also demonstrated that the distributed
CNN design is secure against any distributed components. Overall, the
measurements were shown to the feasibility of our real-time distributed system
for intrusion detection on IoT devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jung_W/0/1/0/all/0/1"&gt;Woosub Jung&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yizhou Feng&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1"&gt;Sabbir Ahmed Khan&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_C/0/1/0/all/0/1"&gt;Chunsheng Xin&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Danella Zhao&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1"&gt;Gang Zhou&lt;/a&gt; (1) ((1) William &amp; Mary, (2) Old Dominion University)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Encoding Involutory Invariance in Neural Networks. (arXiv:2106.12891v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12891</id>
        <link href="http://arxiv.org/abs/2106.12891"/>
        <updated>2021-06-25T02:00:46.099Z</updated>
        <summary type="html"><![CDATA[In certain situations, Neural Networks (NN) are trained upon data that obey
underlying physical symmetries. However, it is not guaranteed that NNs will
obey the underlying symmetry unless embedded in the network structure. In this
work, we explore a special kind of symmetry where functions are invariant with
respect to involutory linear/affine transformations up to parity $p=\pm 1$. We
develop mathematical theorems and propose NN architectures that ensure
invariance and universal approximation properties. Numerical experiments
indicate that the proposed models outperform baseline networks while respecting
the imposed symmetry. An adaption of our technique to convolutional NN
classification tasks for datasets with inherent horizontal/vertical reflection
symmetry has also been proposed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1"&gt;Anwesh Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mattheakis_M/0/1/0/all/0/1"&gt;Marios Mattheakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Protopapas_P/0/1/0/all/0/1"&gt;Pavlos Protopapas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best-Case Lower Bounds in Online Learning. (arXiv:2106.12688v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12688</id>
        <link href="http://arxiv.org/abs/2106.12688"/>
        <updated>2021-06-25T02:00:46.064Z</updated>
        <summary type="html"><![CDATA[Much of the work in online learning focuses on the study of sublinear upper
bounds on the regret. In this work, we initiate the study of best-case lower
bounds in online convex optimization, wherein we bound the largest improvement
an algorithm can obtain relative to the single best action in hindsight. This
problem is motivated by the goal of better understanding the adaptivity of a
learning algorithm. Another motivation comes from fairness: it is known that
best-case lower bounds are instrumental in obtaining algorithms for
decision-theoretic online learning (DTOL) that satisfy a notion of group
fairness. Our contributions are a general method to provide best-case lower
bounds in Follow The Regularized Leader (FTRL) algorithms with time-varying
regularizers, which we use to show that best-case lower bounds are of the same
order as existing upper regret bounds: this includes situations with a fixed
learning rate, decreasing learning rates, timeless methods, and adaptive
gradient methods. In stark contrast, we show that the linearized version of
FTRL can attain negative linear regret. Finally, in DTOL with two experts and
binary predictions, we fully characterize the best-case sequences, which
provides a finer understanding of the best-case lower bounds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guzman_C/0/1/0/all/0/1"&gt;Crist&amp;#xf3;bal Guzm&amp;#xe1;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehta_N/0/1/0/all/0/1"&gt;Nishant A. Mehta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mortazavi_A/0/1/0/all/0/1"&gt;Ali Mortazavi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Splitting EUD graphs into trees: A quick and clatty approach. (arXiv:2106.13155v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13155</id>
        <link href="http://arxiv.org/abs/2106.13155"/>
        <updated>2021-06-25T02:00:46.058Z</updated>
        <summary type="html"><![CDATA[We present the system submission from the FASTPARSE team for the EUD Shared
Task at IWPT 2021. We engaged in the task last year by focusing on efficiency.
This year we have focused on experimenting with new ideas on a limited time
budget. Our system is based on splitting the EUD graph into several trees,
based on linguistic criteria. We predict these trees using a sequence-labelling
parser and combine them into an EUD graph. The results were relatively poor,
although not a total disaster and could probably be improved with some
polishing of the system's rough edges.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_M/0/1/0/all/0/1"&gt;Mark Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_C/0/1/0/all/0/1"&gt;Carlos G&amp;#xf3;mez Rodr&amp;#xed;guez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Long short-term relevance learning. (arXiv:2106.12694v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12694</id>
        <link href="http://arxiv.org/abs/2106.12694"/>
        <updated>2021-06-25T02:00:46.048Z</updated>
        <summary type="html"><![CDATA[To incorporate prior knowledge as well as measurement uncertainties in the
traditional long short term memory (LSTM) neural networks, an efficient sparse
Bayesian training algorithm is introduced to the network architecture. The
proposed scheme automatically determines relevant neural connections and adapts
accordingly, in contrast to the classical LSTM solution. Due to its
flexibility, the new LSTM scheme is less prone to overfitting, and hence can
approximate time dependent solutions by use of a smaller data set. On a
structural nonlinear finite element application we show that the
self-regulating framework does not require prior knowledge of a suitable
network architecture and size, while ensuring satisfying accuracy at reasonable
computational cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weg_B/0/1/0/all/0/1"&gt;Bram van de Weg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greve_L/0/1/0/all/0/1"&gt;Lars Greve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosic_B/0/1/0/all/0/1"&gt;Bojana Rosic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine learning structure preserving brackets for forecasting irreversible processes. (arXiv:2106.12619v1 [physics.comp-ph])]]></title>
        <id>http://arxiv.org/abs/2106.12619</id>
        <link href="http://arxiv.org/abs/2106.12619"/>
        <updated>2021-06-25T02:00:46.032Z</updated>
        <summary type="html"><![CDATA[Forecasting of time-series data requires imposition of inductive biases to
obtain predictive extrapolation, and recent works have imposed
Hamiltonian/Lagrangian form to preserve structure for systems with reversible
dynamics. In this work we present a novel parameterization of dissipative
brackets from metriplectic dynamical systems appropriate for learning
irreversible dynamics with unknown a priori model form. The process learns
generalized Casimirs for energy and entropy guaranteed to be conserved and
nondecreasing, respectively. Furthermore, for the case of added thermal noise,
we guarantee exact preservation of a fluctuation-dissipation theorem, ensuring
thermodynamic consistency. We provide benchmarks for dissipative systems
demonstrating learned dynamics are more robust and generalize better than
either "black-box" or penalty-based approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kookjin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Trask_N/0/1/0/all/0/1"&gt;Nathaniel A. Trask&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Stinis_P/0/1/0/all/0/1"&gt;Panos Stinis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting Semi-supervised Image Segmentation with Global and Local Mutual Information Regularization. (arXiv:2103.04813v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04813</id>
        <link href="http://arxiv.org/abs/2103.04813"/>
        <updated>2021-06-25T02:00:46.008Z</updated>
        <summary type="html"><![CDATA[The scarcity of labeled data often impedes the application of deep learning
to the segmentation of medical images. Semi-supervised learning seeks to
overcome this limitation by exploiting unlabeled examples in the learning
process. In this paper, we present a novel semi-supervised segmentation method
that leverages mutual information (MI) on categorical distributions to achieve
both global representation invariance and local smoothness. In this method, we
maximize the MI for intermediate feature embeddings that are taken from both
the encoder and decoder of a segmentation network. We first propose a global MI
loss constraining the encoder to learn an image representation that is
invariant to geometric transformations. Instead of resorting to
computationally-expensive techniques for estimating the MI on continuous
feature embeddings, we use projection heads to map them to a discrete cluster
assignment where MI can be computed efficiently. Our method also includes a
local MI loss to promote spatial consistency in the feature maps of the decoder
and provide a smoother segmentation. Since mutual information does not require
a strict ordering of clusters in two different assignments, we incorporate a
final consistency regularization loss on the output which helps align the
cluster labels throughout the network. We evaluate the method on four
challenging publicly-available datasets for medical image segmentation.
Experimental results show our method to outperform recently-proposed approaches
for semi-supervised segmentation and provide an accuracy near to full
supervision while training with very few annotated images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jizong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1"&gt;Marco Pedersoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desrosiers_C/0/1/0/all/0/1"&gt;Christian Desrosiers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness via Representation Neutralization. (arXiv:2106.12674v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12674</id>
        <link href="http://arxiv.org/abs/2106.12674"/>
        <updated>2021-06-25T02:00:45.997Z</updated>
        <summary type="html"><![CDATA[Existing bias mitigation methods for DNN models primarily work on learning
debiased encoders. This process not only requires a lot of instance-level
annotations for sensitive attributes, it also does not guarantee that all
fairness sensitive information has been removed from the encoder. To address
these limitations, we explore the following research question: Can we reduce
the discrimination of DNN models by only debiasing the classification head,
even with biased representations as inputs? To this end, we propose a new
mitigation technique, namely, Representation Neutralization for Fairness (RNF)
that achieves fairness by debiasing only the task-specific classification head
of DNN models. To this end, we leverage samples with the same ground-truth
label but different sensitive attributes, and use their neutralized
representations to train the classification head of the DNN model. The key idea
of RNF is to discourage the classification head from capturing spurious
correlation between fairness sensitive information in encoder representations
with specific class labels. To address low-resource settings with no access to
sensitive attribute annotations, we leverage a bias-amplified model to generate
proxy annotations for sensitive attributes. Experimental results over several
benchmark datasets demonstrate our RNF framework to effectively reduce
discrimination of DNN models with minimal degradation in task-specific
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1"&gt;Mengnan Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1"&gt;Subhabrata Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guanchu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1"&gt;Ruixiang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1"&gt;Ahmed Hassan Awadallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xia Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Inducing Point Gaussian Process for Inter-domain Observations. (arXiv:2103.00393v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00393</id>
        <link href="http://arxiv.org/abs/2103.00393"/>
        <updated>2021-06-25T02:00:45.992Z</updated>
        <summary type="html"><![CDATA[We examine the general problem of inter-domain Gaussian Processes (GPs):
problems where the GP realization and the noisy observations of that
realization lie on different domains. When the mapping between those domains is
linear, such as integration or differentiation, inference is still closed form.
However, many of the scaling and approximation techniques that our community
has developed do not apply to this setting. In this work, we introduce the
hierarchical inducing point GP (HIP-GP), a scalable inter-domain GP inference
method that enables us to improve the approximation accuracy by increasing the
number of inducing points to the millions. HIP-GP, which relies on inducing
points with grid structure and a stationary kernel assumption, is suitable for
low-dimensional problems. In developing HIP-GP, we introduce (1) a fast
whitening strategy, and (2) a novel preconditioner for conjugate gradients
which can be helpful in general GP settings. Our code is available at https:
//github.com/cunningham-lab/hipgp.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Luhuan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miller_A/0/1/0/all/0/1"&gt;Andrew Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_L/0/1/0/all/0/1"&gt;Lauren Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pleiss_G/0/1/0/all/0/1"&gt;Geoff Pleiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blei_D/0/1/0/all/0/1"&gt;David Blei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cunningham_J/0/1/0/all/0/1"&gt;John Cunningham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BABEL: Bodies, Action and Behavior with English Labels. (arXiv:2106.09696v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09696</id>
        <link href="http://arxiv.org/abs/2106.09696"/>
        <updated>2021-06-25T02:00:45.986Z</updated>
        <summary type="html"><![CDATA[Understanding the semantics of human movement -- the what, how and why of the
movement -- is an important problem that requires datasets of human actions
with semantic labels. Existing datasets take one of two approaches. Large-scale
video datasets contain many action labels but do not contain ground-truth 3D
human motion. Alternatively, motion-capture (mocap) datasets have precise body
motions but are limited to a small number of actions. To address this, we
present BABEL, a large dataset with language labels describing the actions
being performed in mocap sequences. BABEL consists of action labels for about
43 hours of mocap sequences from AMASS. Action labels are at two levels of
abstraction -- sequence labels describe the overall action in the sequence, and
frame labels describe all actions in every frame of the sequence. Each frame
label is precisely aligned with the duration of the corresponding action in the
mocap sequence, and multiple actions can overlap. There are over 28k sequence
labels, and 63k frame labels in BABEL, which belong to over 250 unique action
categories. Labels from BABEL can be leveraged for tasks like action
recognition, temporal action localization, motion synthesis, etc. To
demonstrate the value of BABEL as a benchmark, we evaluate the performance of
models on 3D action recognition. We demonstrate that BABEL poses interesting
learning challenges that are applicable to real-world scenarios, and can serve
as a useful benchmark of progress in 3D action recognition. The dataset,
baseline method, and evaluation code is made available, and supported for
academic research purposes at https://babel.is.tue.mpg.de/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Punnakkal_A/0/1/0/all/0/1"&gt;Abhinanda R. Punnakkal&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Chandrasekaran_A/0/1/0/all/0/1"&gt;Arjun Chandrasekaran&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Athanasiou_N/0/1/0/all/0/1"&gt;Nikos Athanasiou&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Quiros_Ramirez_A/0/1/0/all/0/1"&gt;Alejandra Quiros-Ramirez&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1"&gt;Michael J. Black&lt;/a&gt; (1) ((1) Max Planck Institute for Intelligent Systems, (2) Universitat Konstanz)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CAGAN: Text-To-Image Generation with Combined Attention GANs. (arXiv:2104.12663v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12663</id>
        <link href="http://arxiv.org/abs/2104.12663"/>
        <updated>2021-06-25T02:00:45.981Z</updated>
        <summary type="html"><![CDATA[Generating images according to natural language descriptions is a challenging
task. Prior research has mainly focused to enhance the quality of generation by
investigating the use of spatial attention and/or textual attention thereby
neglecting the relationship between channels. In this work, we propose the
Combined Attention Generative Adversarial Network (CAGAN) to generate
photo-realistic images according to textual descriptions. The proposed CAGAN
utilises two attention models: word attention to draw different sub-regions
conditioned on related words; and squeeze-and-excitation attention to capture
non-linear interaction among channels. With spectral normalisation to stabilise
training, our proposed CAGAN improves the state of the art on the IS and FID on
the CUB dataset and the FID on the more challenging COCO dataset. Furthermore,
we demonstrate that judging a model by a single evaluation metric can be
misleading by developing an additional model adding local self-attention which
scores a higher IS, outperforming the state of the art on the CUB dataset, but
generates unrealistic images through feature repetition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schulze_H/0/1/0/all/0/1"&gt;Henning Schulze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yaman_D/0/1/0/all/0/1"&gt;Dogucan Yaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1"&gt;Alexander Waibel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Faster Policy Learning with Continuous-Time Gradients. (arXiv:2012.06684v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06684</id>
        <link href="http://arxiv.org/abs/2012.06684"/>
        <updated>2021-06-25T02:00:45.965Z</updated>
        <summary type="html"><![CDATA[We study the estimation of policy gradients for continuous-time systems with
known dynamics. By reframing policy learning in continuous-time, we show that
it is possible construct a more efficient and accurate gradient estimator. The
standard back-propagation through time estimator (BPTT) computes exact
gradients for a crude discretization of the continuous-time system. In
contrast, we approximate continuous-time gradients in the original system. With
the explicit goal of estimating continuous-time gradients, we are able to
discretize adaptively and construct a more efficient policy gradient estimator
which we call the Continuous-Time Policy Gradient (CTPG). We show that
replacing BPTT policy gradients with more efficient CTPG estimates results in
faster and more robust learning in a variety of control tasks and simulators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ainsworth_S/0/1/0/all/0/1"&gt;Samuel Ainsworth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lowrey_K/0/1/0/all/0/1"&gt;Kendall Lowrey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thickstun_J/0/1/0/all/0/1"&gt;John Thickstun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harchaoui_Z/0/1/0/all/0/1"&gt;Zaid Harchaoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasa_S/0/1/0/all/0/1"&gt;Siddhartha Srinivasa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reimagining GNN Explanations with ideas from Tabular Data. (arXiv:2106.12665v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12665</id>
        <link href="http://arxiv.org/abs/2106.12665"/>
        <updated>2021-06-25T02:00:45.959Z</updated>
        <summary type="html"><![CDATA[Explainability techniques for Graph Neural Networks still have a long way to
go compared to explanations available for both neural and decision decision
tree-based models trained on tabular data. Using a task that straddles both
graphs and tabular data, namely Entity Matching, we comment on key aspects of
explainability that are missing in GNN model explanations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Anjali Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+K_S/0/1/0/all/0/1"&gt;Shamanth R Nayak K&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganesan_B/0/1/0/all/0/1"&gt;Balaji Ganesan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Stereotyping Problem in Collaboratively Filtered Recommender Systems. (arXiv:2106.12622v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.12622</id>
        <link href="http://arxiv.org/abs/2106.12622"/>
        <updated>2021-06-25T02:00:45.954Z</updated>
        <summary type="html"><![CDATA[Recommender systems -- and especially matrix factorization-based
collaborative filtering algorithms -- play a crucial role in mediating our
access to online information. We show that such algorithms induce a particular
kind of stereotyping: if preferences for a \textit{set} of items are
anti-correlated in the general user population, then those items may not be
recommended together to a user, regardless of that user's preferences and
ratings history. First, we introduce a notion of \textit{joint accessibility},
which measures the extent to which a set of items can jointly be accessed by
users. We then study joint accessibility under the standard factorization-based
collaborative filtering framework, and provide theoretical necessary and
sufficient conditions when joint accessibility is violated. Moreover, we show
that these conditions can easily be violated when the users are represented by
a single feature vector. To improve joint accessibility, we further propose an
alternative modelling fix, which is designed to capture the diverse multiple
interests of each user using a multi-vector representation. We conduct
extensive experiments on real and simulated datasets, demonstrating the
stereotyping problem with standard single-vector matrix factorization models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1"&gt;Wenshuo Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krauth_K/0/1/0/all/0/1"&gt;Karl Krauth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garg_N/0/1/0/all/0/1"&gt;Nikhil Garg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UXLA: A Robust Unsupervised Data Augmentation Framework for {Zero-Resource} Cross-Lingual NLP. (arXiv:2004.13240v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.13240</id>
        <link href="http://arxiv.org/abs/2004.13240"/>
        <updated>2021-06-25T02:00:45.948Z</updated>
        <summary type="html"><![CDATA[Transfer learning has yielded state-of-the-art (SoTA) results in many
supervised NLP tasks. However, annotated data for every target task in every
target language is rare, especially for low-resource languages. We propose
UXLA, a novel unsupervised data augmentation framework for zero-resource
transfer learning scenarios. In particular, UXLA aims to solve cross-lingual
adaptation problems from a source language task distribution to an unknown
target language task distribution, assuming no training label in the target
language. At its core, UXLA performs simultaneous self-training with data
augmentation and unsupervised sample selection. To show its effectiveness, we
conduct extensive experiments on three diverse zero-resource cross-lingual
transfer tasks. UXLA achieves SoTA results in all the tasks, outperforming the
baselines by a good margin. With an in-depth framework dissection, we
demonstrate the cumulative contributions of different components to its
success.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1"&gt;M Saiful Bari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohiuddin_T/0/1/0/all/0/1"&gt;Tasnim Mohiuddin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1"&gt;Shafiq Joty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Handwritten Digit Recognition using Machine and Deep Learning Algorithms. (arXiv:2106.12614v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12614</id>
        <link href="http://arxiv.org/abs/2106.12614"/>
        <updated>2021-06-25T02:00:45.942Z</updated>
        <summary type="html"><![CDATA[The reliance of humans over machines has never been so high such that from
object classification in photographs to adding sound to silent movies
everything can be performed with the help of deep learning and machine learning
algorithms. Likewise, Handwritten text recognition is one of the significant
areas of research and development with a streaming number of possibilities that
could be attained. Handwriting recognition (HWR), also known as Handwritten
Text Recognition (HTR), is the ability of a computer to receive and interpret
intelligible handwritten input from sources such as paper documents,
photographs, touch-screens and other devices [1]. Apparently, in this paper, we
have performed handwritten digit recognition with the help of MNIST datasets
using Support Vector Machines (SVM), Multi-Layer Perceptron (MLP) and
Convolution Neural Network (CNN) models. Our main objective is to compare the
accuracy of the models stated above along with their execution time to get the
best possible model for digit recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pashine_S/0/1/0/all/0/1"&gt;Samay Pashine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dixit_R/0/1/0/all/0/1"&gt;Ritik Dixit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kushwah_R/0/1/0/all/0/1"&gt;Rishika Kushwah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Verification of Deep Neural Networks under Domain or Weight Shift. (arXiv:2106.12732v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12732</id>
        <link href="http://arxiv.org/abs/2106.12732"/>
        <updated>2021-06-25T02:00:45.926Z</updated>
        <summary type="html"><![CDATA[Although neural networks are widely used, it remains challenging to formally
verify the safety and robustness of neural networks in real-world applications.
Existing methods are designed to verify the network before use, which is
limited to relatively simple specifications and fixed networks. These methods
are not ready to be applied to real-world problems with complex and/or
dynamically changing specifications and networks. To effectively handle
dynamically changing specifications and networks, the verification needs to be
performed online when these changes take place. However, it is still
challenging to run existing verification algorithms online. Our key insight is
that we can leverage the temporal dependencies of these changes to accelerate
the verification process, e.g., by warm starting new online verification using
previous verified results. This paper establishes a novel framework for
scalable online verification to solve real-world verification problems with
dynamically changing specifications and/or networks, known as domain shift and
weight shift respectively. We propose three types of techniques (branch
management, perturbation tolerance analysis, and incremental computation) to
accelerate the online verification of deep neural networks. Experiment results
show that our online verification algorithm is up to two orders of magnitude
faster than existing verification algorithms, and thus can scale to real-world
applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1"&gt;Tianhao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Changliu Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse Flows: Pruning Continuous-depth Models. (arXiv:2106.12718v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12718</id>
        <link href="http://arxiv.org/abs/2106.12718"/>
        <updated>2021-06-25T02:00:45.921Z</updated>
        <summary type="html"><![CDATA[Continuous deep learning architectures enable learning of flexible
probabilistic models for predictive modeling as neural ordinary differential
equations (ODEs), and for generative modeling as continuous normalizing flows.
In this work, we design a framework to decipher the internal dynamics of these
continuous depth models by pruning their network architectures. Our empirical
results suggest that pruning improves generalization for neural ODEs in
generative modeling. Moreover, pruning finds minimal and efficient neural ODE
representations with up to 98\% less parameters compared to the original
network, without loss of accuracy. Finally, we show that by applying pruning we
can obtain insightful information about the design of better neural ODEs.We
hope our results will invigorate further research into the performance-size
trade-offs of modern continuous-depth models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liebenwein_L/0/1/0/all/0/1"&gt;Lucas Liebenwein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1"&gt;Ramin Hasani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1"&gt;Alexander Amini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1"&gt;Daniela Rus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time gravitational-wave science with neural posterior estimation. (arXiv:2106.12594v1 [gr-qc])]]></title>
        <id>http://arxiv.org/abs/2106.12594</id>
        <link href="http://arxiv.org/abs/2106.12594"/>
        <updated>2021-06-25T02:00:45.915Z</updated>
        <summary type="html"><![CDATA[We demonstrate unprecedented accuracy for rapid gravitational-wave parameter
estimation with deep learning. Using neural networks as surrogates for Bayesian
posterior distributions, we analyze eight gravitational-wave events from the
first LIGO-Virgo Gravitational-Wave Transient Catalog and find very close
quantitative agreement with standard inference codes, but with inference times
reduced from O(day) to a minute per event. Our networks are trained using
simulated data, including an estimate of the detector-noise characteristics
near the event. This encodes the signal and noise models within millions of
neural-network parameters, and enables inference for any observed data
consistent with the training distribution, accounting for noise nonstationarity
from event to event. Our algorithm -- called "DINGO" -- sets a new standard in
fast-and-accurate inference of physical parameters of detected
gravitational-wave events, which should enable real-time data analysis without
sacrificing accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/gr-qc/1/au:+Dax_M/0/1/0/all/0/1"&gt;Maximilian Dax&lt;/a&gt;, &lt;a href="http://arxiv.org/find/gr-qc/1/au:+Green_S/0/1/0/all/0/1"&gt;Stephen R. Green&lt;/a&gt;, &lt;a href="http://arxiv.org/find/gr-qc/1/au:+Gair_J/0/1/0/all/0/1"&gt;Jonathan Gair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/gr-qc/1/au:+Macke_J/0/1/0/all/0/1"&gt;Jakob H. Macke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/gr-qc/1/au:+Buonanno_A/0/1/0/all/0/1"&gt;Alessandra Buonanno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/gr-qc/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Study of Robust Adaptive Beamforming Based on Low-Complexity DFT Spatial Sampling. (arXiv:2106.12663v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.12663</id>
        <link href="http://arxiv.org/abs/2106.12663"/>
        <updated>2021-06-25T02:00:45.909Z</updated>
        <summary type="html"><![CDATA[In this paper, a novel and robust algorithm is proposed for adaptive
beamforming based on the idea of reconstructing the autocorrelation sequence
(ACS) of a random process from a set of measured data. This is obtained from
the first column and the first row of the sample covariance matrix (SCM) after
averaging along its diagonals. Then, the power spectrum of the correlation
sequence is estimated using the discrete Fourier transform (DFT). The DFT
coefficients corresponding to the angles within the noise-plus-interference
region are used to reconstruct the noise-plus-interference covariance matrix
(NPICM), while the desired signal covariance matrix (DSCM) is estimated by
identifying and removing the noise-plus-interference component from the SCM. In
particular, the spatial power spectrum of the estimated received signal is
utilized to compute the correlation sequence corresponding to the
noise-plus-interference in which the dominant DFT coefficient of the
noise-plus-interference is captured. A key advantage of the proposed adaptive
beamforming is that only little prior information is required. Specifically, an
imprecise knowledge of the array geometry and of the angular sectors in which
the interferences are located is needed. Simulation results demonstrate that
compared with previous reconstruction-based beamformers, the proposed approach
can achieve better overall performance in the case of multiple mismatches over
a very large range of input signal-to-noise ratios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohammadzadeh_S/0/1/0/all/0/1"&gt;Saeed Mohammadzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nascimento_V/0/1/0/all/0/1"&gt;Vitor H.Nascimento&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lamare_R/0/1/0/all/0/1"&gt;Rodrigo C. de Lamare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kukrer_O/0/1/0/all/0/1"&gt;Osman Kukrer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably efficient machine learning for quantum many-body problems. (arXiv:2106.12627v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2106.12627</id>
        <link href="http://arxiv.org/abs/2106.12627"/>
        <updated>2021-06-25T02:00:45.903Z</updated>
        <summary type="html"><![CDATA[Classical machine learning (ML) provides a potentially powerful approach to
solving challenging quantum many-body problems in physics and chemistry.
However, the advantages of ML over more traditional methods have not been
firmly established. In this work, we prove that classical ML algorithms can
efficiently predict ground state properties of gapped Hamiltonians in finite
spatial dimensions, after learning from data obtained by measuring other
Hamiltonians in the same quantum phase of matter. In contrast, under widely
accepted complexity theory assumptions, classical algorithms that do not learn
from data cannot achieve the same guarantee. We also prove that classical ML
algorithms can efficiently classify a wide range of quantum phases of matter.
Our arguments are based on the concept of a classical shadow, a succinct
classical description of a many-body quantum state that can be constructed in
feasible quantum experiments and be used to predict many properties of the
state. Extensive numerical experiments corroborate our theoretical results in a
variety of scenarios, including Rydberg atom systems, 2D random Heisenberg
models, symmetry-protected topological phases, and topologically ordered
phases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hsin-Yuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kueng_R/0/1/0/all/0/1"&gt;Richard Kueng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Torlai_G/0/1/0/all/0/1"&gt;Giacomo Torlai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Albert_V/0/1/0/all/0/1"&gt;Victor V. Albert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Preskill_J/0/1/0/all/0/1"&gt;John Preskill&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finite-Sample Analysis of Off-Policy TD-Learning via Generalized Bellman Operators. (arXiv:2106.12729v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12729</id>
        <link href="http://arxiv.org/abs/2106.12729"/>
        <updated>2021-06-25T02:00:45.888Z</updated>
        <summary type="html"><![CDATA[In temporal difference (TD) learning, off-policy sampling is known to be more
practical than on-policy sampling, and by decoupling learning from data
collection, it enables data reuse. It is known that policy evaluation
(including multi-step off-policy importance sampling) has the interpretation of
solving a generalized Bellman equation. In this paper, we derive finite-sample
bounds for any general off-policy TD-like stochastic approximation algorithm
that solves for the fixed-point of this generalized Bellman operator. Our key
step is to show that the generalized Bellman operator is simultaneously a
contraction mapping with respect to a weighted $\ell_p$-norm for each $p$ in
$[1,\infty)$, with a common contraction factor.

Off-policy TD-learning is known to suffer from high variance due to the
product of importance sampling ratios. A number of algorithms (e.g.
$Q^\pi(\lambda)$, Tree-Backup$(\lambda)$, Retrace$(\lambda)$, and $Q$-trace)
have been proposed in the literature to address this issue. Our results
immediately imply finite-sample bounds of these algorithms. In particular, we
provide first-known finite-sample guarantees for $Q^\pi(\lambda)$,
Tree-Backup$(\lambda)$, and Retrace$(\lambda)$, and improve the best known
bounds of $Q$-trace in [19]. Moreover, we show the bias-variance trade-offs in
each of these algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zaiwei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maguluri_S/0/1/0/all/0/1"&gt;Siva Theja Maguluri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1"&gt;Sanjay Shakkottai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1"&gt;Karthikeyan Shanmugam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Fake Detection: Survey of Facial Manipulation Detection Solutions. (arXiv:2106.12605v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12605</id>
        <link href="http://arxiv.org/abs/2106.12605"/>
        <updated>2021-06-25T02:00:45.883Z</updated>
        <summary type="html"><![CDATA[Deep Learning as a field has been successfully used to solve a plethora of
complex problems, the likes of which we could not have imagined a few decades
back. But as many benefits as it brings, there are still ways in which it can
be used to bring harm to our society. Deep fakes have been proven to be one
such problem, and now more than ever, when any individual can create a fake
image or video simply using an application on the smartphone, there need to be
some countermeasures, with which we can detect if the image or video is a fake
or real and dispose of the problem threatening the trustworthiness of online
information. Although the Deep fakes created by neural networks, may seem to be
as real as a real image or video, it still leaves behind spatial and temporal
traces or signatures after moderation, these signatures while being invisible
to a human eye can be detected with the help of a neural network trained to
specialize in Deep fake detection. In this paper, we analyze several such
states of the art neural networks (MesoNet, ResNet-50, VGG-19, and Xception
Net) and compare them against each other, to find an optimal solution for
various scenarios like real-time deep fake detection to be deployed in online
social media platforms where the classification should be made as fast as
possible or for a small news agency where the classification need not be in
real-time but requires utmost accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pashine_S/0/1/0/all/0/1"&gt;Samay Pashine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mandiya_S/0/1/0/all/0/1"&gt;Sagar Mandiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1"&gt;Praveen Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheikh_R/0/1/0/all/0/1"&gt;Rashid Sheikh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EfficientNetV2: Smaller Models and Faster Training. (arXiv:2104.00298v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00298</id>
        <link href="http://arxiv.org/abs/2104.00298"/>
        <updated>2021-06-25T02:00:45.853Z</updated>
        <summary type="html"><![CDATA[This paper introduces EfficientNetV2, a new family of convolutional networks
that have faster training speed and better parameter efficiency than previous
models. To develop this family of models, we use a combination of
training-aware neural architecture search and scaling, to jointly optimize
training speed and parameter efficiency. The models were searched from the
search space enriched with new ops such as Fused-MBConv. Our experiments show
that EfficientNetV2 models train much faster than state-of-the-art models while
being up to 6.8x smaller.

Our training can be further sped up by progressively increasing the image
size during training, but it often causes a drop in accuracy. To compensate for
this accuracy drop, we propose to adaptively adjust regularization (e.g.,
dropout and data augmentation) as well, such that we can achieve both fast
training and good accuracy.

With progressive learning, our EfficientNetV2 significantly outperforms
previous models on ImageNet and CIFAR/Cars/Flowers datasets. By pretraining on
the same ImageNet21k, our EfficientNetV2 achieves 87.3% top-1 accuracy on
ImageNet ILSVRC2012, outperforming the recent ViT by 2.0% accuracy while
training 5x-11x faster using the same computing resources. Code will be
available at https://github.com/google/automl/tree/master/efficientnetv2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1"&gt;Mingxing Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1"&gt;Quoc V. Le&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[All unconstrained strongly convex problems are weakly simplicial. (arXiv:2106.12704v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.12704</id>
        <link href="http://arxiv.org/abs/2106.12704"/>
        <updated>2021-06-25T02:00:45.847Z</updated>
        <summary type="html"><![CDATA[A multi-objective optimization problem is $C^r$ weakly simplicial if there
exists a $C^r$ surjection from a simplex onto the Pareto set/front such that
the image of each subsimplex is the Pareto set/front of a subproblem, where
$0\leq r\leq \infty$. This property is helpful to compute a parametric-surface
approximation of the entire Pareto set and Pareto front. It is known that all
unconstrained strongly convex $C^r$ problems are $C^{r-1}$ weakly simplicial
for $1\leq r \leq \infty$. In this paper, we show that all unconstrained
strongly convex problems are $C^0$ weakly simplicial. The usefulness of this
theorem is demonstrated in a sparse modeling application: we reformulate the
elastic net as a non-differentiable multi-objective strongly convex problem and
approximate its Pareto set (the set of all trained models with different
hyper-parameters) and Pareto front (the set of performance metrics of the
trained models) by using a B\'ezier simplex fitting method, which accelerates
hyper-parameter search.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Mizota_Y/0/1/0/all/0/1"&gt;Yusuke Mizota&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hamada_N/0/1/0/all/0/1"&gt;Naoki Hamada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ichiki_S/0/1/0/all/0/1"&gt;Shunsuke Ichiki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dungeon and Platformer Level Blending and Generation using Conditional VAEs. (arXiv:2106.12692v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12692</id>
        <link href="http://arxiv.org/abs/2106.12692"/>
        <updated>2021-06-25T02:00:45.831Z</updated>
        <summary type="html"><![CDATA[Variational autoencoders (VAEs) have been used in prior works for generating
and blending levels from different games. To add controllability to these
models, conditional VAEs (CVAEs) were recently shown capable of generating
output that can be modified using labels specifying desired content, albeit
working with segments of levels and platformers exclusively. We expand these
works by using CVAEs for generating whole platformer and dungeon levels, and
blending levels across these genres. We show that CVAEs can reliably control
door placement in dungeons and progression direction in platformer levels.
Thus, by using appropriate labels, our approach can generate whole dungeons and
platformer levels of interconnected rooms and segments respectively as well as
levels that blend dungeons and platformers. We demonstrate our approach using
The Legend of Zelda, Metroid, Mega Man and Lode Runner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1"&gt;Anurag Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cooper_S/0/1/0/all/0/1"&gt;Seth Cooper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distilling the Knowledge from Normalizing Flows. (arXiv:2106.12699v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12699</id>
        <link href="http://arxiv.org/abs/2106.12699"/>
        <updated>2021-06-25T02:00:45.816Z</updated>
        <summary type="html"><![CDATA[Normalizing flows are a powerful class of generative models demonstrating
strong performance in several speech and vision problems. In contrast to other
generative models, normalizing flows have tractable likelihoods and allow for
stable training. However, they have to be carefully designed to represent
invertible functions with efficient Jacobian determinant calculation. In
practice, these requirements lead to overparameterized and sophisticated
architectures that are inferior to alternative feed-forward models in terms of
inference time and memory consumption. In this work, we investigate whether one
can distill knowledge from flow-based models to more efficient alternatives. We
provide a positive answer to this question by proposing a simple distillation
approach and demonstrating its effectiveness on state-of-the-art conditional
flow-based models for image super-resolution and speech synthesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baranchuk_D/0/1/0/all/0/1"&gt;Dmitry Baranchuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aliev_V/0/1/0/all/0/1"&gt;Vladimir Aliev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babenko_A/0/1/0/all/0/1"&gt;Artem Babenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time Semantic Segmentation via Spatial-detail Guided Context Propagation. (arXiv:2005.11034v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.11034</id>
        <link href="http://arxiv.org/abs/2005.11034"/>
        <updated>2021-06-25T02:00:45.810Z</updated>
        <summary type="html"><![CDATA[Nowadays, vision-based computing tasks play an important role in various
real-world applications. However, many vision computing tasks, e.g. semantic
segmentation, are usually computationally expensive, posing a challenge to the
computing systems that are resource-constrained but require fast response
speed. Therefore, it is valuable to develop accurate and real-time vision
processing models that only require limited computational resources. To this
end, we propose the Spatial-detail Guided Context Propagation Network (SGCPNet)
for achieving real-time semantic segmentation. In SGCPNet, we propose the
strategy of spatial-detail guided context propagation. It uses the spatial
details of shallow layers to guide the propagation of the low-resolution global
contexts, in which the lost spatial information can be effectively
reconstructed. In this way, the need for maintaining high-resolution features
along the network is freed, therefore largely improving the model efficiency.
On the other hand, due to the effective reconstruction of spatial details, the
segmentation accuracy can be still preserved. In the experiments, we validate
the effectiveness and efficiency of the proposed SGCPNet model. On the
Citysacpes dataset, for example, our SGCPNet achieves 69.5 % mIoU segmentation
accuracy, while its speed reaches 178.5 FPS on 768x1536 images on a GeForce GTX
1080 Ti GPU card.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hao_S/0/1/0/all/0/1"&gt;Shijie Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yanrong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1"&gt;Richang Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1"&gt;Jun Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Meng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graceful Degradation and Related Fields. (arXiv:2106.11119v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11119</id>
        <link href="http://arxiv.org/abs/2106.11119"/>
        <updated>2021-06-25T02:00:45.805Z</updated>
        <summary type="html"><![CDATA[When machine learning models encounter data which is out of the distribution
on which they were trained they have a tendency to behave poorly, most
prominently over-confidence in erroneous predictions. Such behaviours will have
disastrous effects on real-world machine learning systems. In this field
graceful degradation refers to the optimisation of model performance as it
encounters this out-of-distribution data. This work presents a definition and
discussion of graceful degradation and where it can be applied in deployed
visual systems. Following this a survey of relevant areas is undertaken,
novelly splitting the graceful degradation problem into active and passive
approaches. In passive approaches, graceful degradation is handled and achieved
by the model in a self-contained manner, in active approaches the model is
updated upon encountering epistemic uncertainties. This work communicates the
importance of the problem and aims to prompt the development of machine
learning strategies that are aware of graceful degradation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dymond_J/0/1/0/all/0/1"&gt;Jack Dymond&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extreme Multi-label Learning for Semantic Matching in Product Search. (arXiv:2106.12657v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.12657</id>
        <link href="http://arxiv.org/abs/2106.12657"/>
        <updated>2021-06-25T02:00:45.800Z</updated>
        <summary type="html"><![CDATA[We consider the problem of semantic matching in product search: given a
customer query, retrieve all semantically related products from a huge catalog
of size 100 million, or more. Because of large catalog spaces and real-time
latency constraints, semantic matching algorithms not only desire high recall
but also need to have low latency. Conventional lexical matching approaches
(e.g., Okapi-BM25) exploit inverted indices to achieve fast inference time, but
fail to capture behavioral signals between queries and products. In contrast,
embedding-based models learn semantic representations from customer behavior
data, but the performance is often limited by shallow neural encoders due to
latency constraints. Semantic product search can be viewed as an eXtreme
Multi-label Classification (XMC) problem, where customer queries are input
instances and products are output labels. In this paper, we aim to improve
semantic product search by using tree-based XMC models where inference time
complexity is logarithmic in the number of products. We consider hierarchical
linear models with n-gram features for fast real-time inference.
Quantitatively, our method maintains a low latency of 1.25 milliseconds per
query and achieves a 65% improvement of Recall@100 (60.9% v.s. 36.8%) over a
competing embedding-based DSSM model. Our model is robust to weight pruning
with varying thresholds, which can flexibly meet different system requirements
for online deployments. Qualitatively, our method can retrieve products that
are complementary to existing product search system and add diversity to the
match set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_W/0/1/0/all/0/1"&gt;Wei-Cheng Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1"&gt;Daniel Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hsiang-Fu Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teo_C/0/1/0/all/0/1"&gt;Choon-Hui Teo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_K/0/1/0/all/0/1"&gt;Kai Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolluri_K/0/1/0/all/0/1"&gt;Kedarnath Kolluri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1"&gt;Qie Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shandilya_N/0/1/0/all/0/1"&gt;Nikhil Shandilya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ievgrafov_V/0/1/0/all/0/1"&gt;Vyacheslav Ievgrafov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1"&gt;Japinder Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1"&gt;Inderjit S. Dhillon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ST-HOI: A Spatial-Temporal Baseline for Human-Object Interaction Detection in Videos. (arXiv:2105.11731v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11731</id>
        <link href="http://arxiv.org/abs/2105.11731"/>
        <updated>2021-06-25T02:00:45.784Z</updated>
        <summary type="html"><![CDATA[Detecting human-object interactions (HOI) is an important step toward a
comprehensive visual understanding of machines. While detecting non-temporal
HOIs (e.g., sitting on a chair) from static images is feasible, it is unlikely
even for humans to guess temporal-related HOIs (e.g., opening/closing a door)
from a single video frame, where the neighboring frames play an essential role.
However, conventional HOI methods operating on only static images have been
used to predict temporal-related interactions, which is essentially guessing
without temporal contexts and may lead to sub-optimal performance. In this
paper, we bridge this gap by detecting video-based HOIs with explicit temporal
information. We first show that a naive temporal-aware variant of a common
action detection baseline does not work on video-based HOIs due to a
feature-inconsistency issue. We then propose a simple yet effective
architecture named Spatial-Temporal HOI Detection (ST-HOI) utilizing temporal
information such as human and object trajectories, correctly-localized visual
features, and spatial-temporal masking pose features. We construct a new video
HOI benchmark dubbed VidHOI where our proposed approach serves as a solid
baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chiou_M/0/1/0/all/0/1"&gt;Meng-Jiun Chiou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1"&gt;Chun-Yu Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Li-Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1"&gt;Roger Zimmermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiashi Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer-based unsupervised patient representation learning based on medical claims for risk stratification and analysis. (arXiv:2106.12658v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12658</id>
        <link href="http://arxiv.org/abs/2106.12658"/>
        <updated>2021-06-25T02:00:45.777Z</updated>
        <summary type="html"><![CDATA[The claims data, containing medical codes, services information, and incurred
expenditure, can be a good resource for estimating an individual's health
condition and medical risk level. In this study, we developed Transformer-based
Multimodal AutoEncoder (TMAE), an unsupervised learning framework that can
learn efficient patient representation by encoding meaningful information from
the claims data. TMAE is motivated by the practical needs in healthcare to
stratify patients into different risk levels for improving care delivery and
management. Compared to previous approaches, TMAE is able to 1) model
inpatient, outpatient, and medication claims collectively, 2) handle irregular
time intervals between medical events, 3) alleviate the sparsity issue of the
rare medical codes, and 4) incorporate medical expenditure information. We
trained TMAE using a real-world pediatric claims dataset containing more than
600,000 patients and compared its performance with various approaches in two
clustering tasks. Experimental results demonstrate that TMAE has superior
performance compared to all baselines. Multiple downstream applications are
also conducted to illustrate the effectiveness of our framework. The promising
results confirm that the TMAE framework is scalable to large claims data and is
able to generate efficient patient embeddings for risk stratification and
analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xianlong Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Simon Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimum sharpness: Scale-invariant parameter-robustness of neural networks. (arXiv:2106.12612v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12612</id>
        <link href="http://arxiv.org/abs/2106.12612"/>
        <updated>2021-06-25T02:00:45.761Z</updated>
        <summary type="html"><![CDATA[Toward achieving robust and defensive neural networks, the robustness against
the weight parameters perturbations, i.e., sharpness, attracts attention in
recent years (Sun et al., 2020). However, sharpness is known to remain a
critical issue, "scale-sensitivity." In this paper, we propose a novel
sharpness measure, Minimum Sharpness. It is known that NNs have a specific
scale transformation that constitutes equivalent classes where functional
properties are completely identical, and at the same time, their sharpness
could change unlimitedly. We define our sharpness through a minimization
problem over the equivalent NNs being invariant to the scale transformation. We
also develop an efficient and exact technique to make the sharpness tractable,
which reduces the heavy computational costs involved with Hessian. In the
experiment, we observed that our sharpness has a valid correlation with the
generalization of NNs and runs with less computational cost than existing
sharpness measures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ibayashi_H/0/1/0/all/0/1"&gt;Hikaru Ibayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamaguchi_T/0/1/0/all/0/1"&gt;Takuo Hamaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Imaizum_M/0/1/0/all/0/1"&gt;Masaaki Imaizum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Charformer: Fast Character Transformers via Gradient-based Subword Tokenization. (arXiv:2106.12672v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12672</id>
        <link href="http://arxiv.org/abs/2106.12672"/>
        <updated>2021-06-25T02:00:45.755Z</updated>
        <summary type="html"><![CDATA[State-of-the-art models in natural language processing rely on separate rigid
subword tokenization algorithms, which limit their generalization ability and
adaptation to new settings. In this paper, we propose a new model inductive
bias that learns a subword tokenization end-to-end as part of the model. To
this end, we introduce a soft gradient-based subword tokenization module (GBST)
that automatically learns latent subword representations from characters in a
data-driven fashion. Concretely, GBST enumerates candidate subword blocks and
learns to score them in a position-wise fashion using a block scoring network.
We additionally introduce Charformer, a deep Transformer model that integrates
GBST and operates on the byte level. Via extensive experiments on English GLUE,
multilingual, and noisy text datasets, we show that Charformer outperforms a
series of competitive byte-level baselines while generally performing on par
and sometimes outperforming subword-based models. Additionally, Charformer is
fast, improving the speed of both vanilla byte-level and subword-level
Transformers by 28%-100% while maintaining competitive quality. We believe this
work paves the way for highly performant token-free models that are trained
completely end-to-end.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1"&gt;Yi Tay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1"&gt;Vinh Q. Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1"&gt;Sebastian Ruder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1"&gt;Jai Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1"&gt;Hyung Won Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1"&gt;Dara Bahri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1"&gt;Zhen Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baumgartner_S/0/1/0/all/0/1"&gt;Simon Baumgartner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Cong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1"&gt;Donald Metzler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Examples in Multi-Layer Random ReLU Networks. (arXiv:2106.12611v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12611</id>
        <link href="http://arxiv.org/abs/2106.12611"/>
        <updated>2021-06-25T02:00:45.750Z</updated>
        <summary type="html"><![CDATA[We consider the phenomenon of adversarial examples in ReLU networks with
independent gaussian parameters. For networks of constant depth and with a
large range of widths (for instance, it suffices if the width of each layer is
polynomial in that of any other layer), small perturbations of input vectors
lead to large changes of outputs. This generalizes results of Daniely and
Schacham (2020) for networks of rapidly decreasing width and of Bubeck et al
(2021) for two-layer networks. The proof shows that adversarial examples arise
in these networks because the functions that they compute are very close to
linear. Bottleneck layers in the network play a key role: the minimal width up
to some point in the network determines scales and sensitivities of mappings
computed up to that point. The main result is for networks with constant depth,
but we also show that some constraint on depth is necessary for a result of
this kind, because there are suitably deep networks that, with constant
probability, compute a function that is close to constant.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bartlett_P/0/1/0/all/0/1"&gt;Peter L. Bartlett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bubeck_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Bubeck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cherapanamjeri_Y/0/1/0/all/0/1"&gt;Yeshwanth Cherapanamjeri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FDRN: A Fast Deformable Registration Network for Medical Images. (arXiv:2011.02307v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.02307</id>
        <link href="http://arxiv.org/abs/2011.02307"/>
        <updated>2021-06-25T02:00:45.735Z</updated>
        <summary type="html"><![CDATA[Deformable image registration is a fundamental task in medical imaging. Due
to the large computational complexity of deformable registration of volumetric
images, conventional iterative methods usually face the tradeoff between the
registration accuracy and the computation time in practice. In order to boost
the registration performance in both accuracy and runtime, we propose a fast
convolutional neural network. Specially, to efficiently utilize the memory
resources and enlarge the model capacity, we adopt additive forwarding instead
of channel concatenation and deepen the network in each encoder and decoder
stage. To facilitate the learning efficiency, we leverage skip connection
within the encoder and decoder stages to enable residual learning and employ an
auxiliary loss at the bottom layer with lowest resolution to involve deep
supervision. Particularly, the low-resolution auxiliary loss is weighted by an
exponentially decayed parameter during the training phase. In conjunction with
the main loss in high-resolution grid, a coarse-to-fine learning strategy is
achieved. Last but not least, we introduce an auxiliary loss based on the
segmentation prior to improve the registration performance in Dice score.
Comparing to the auxiliary loss using average Dice score, the proposed
multi-label segmentation loss does not induce additional memory cost in the
training phase and can be employed on images with arbitrary amount of
categories. In the experiments, we show FDRN outperforms the existing
state-of-the-art registration methods for brain MR images by resorting to the
compact network structure and efficient learning. Besides, FDRN is a
generalized framework for image registration which is not confined to a
particular type of medical images or anatomy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1"&gt;Kaicong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simon_S/0/1/0/all/0/1"&gt;Sven Simon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging semantically similar queries for ranking via combining representations. (arXiv:2106.12621v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12621</id>
        <link href="http://arxiv.org/abs/2106.12621"/>
        <updated>2021-06-25T02:00:45.729Z</updated>
        <summary type="html"><![CDATA[In modern ranking problems, different and disparate representations of the
items to be ranked are often available. It is sensible, then, to try to combine
these representations to improve ranking. Indeed, learning to rank via
combining representations is both principled and practical for learning a
ranking function for a particular query. In extremely data-scarce settings,
however, the amount of labeled data available for a particular query can lead
to a highly variable and ineffective ranking function. One way to mitigate the
effect of the small amount of data is to leverage information from semantically
similar queries. Indeed, as we demonstrate in simulation settings and real data
examples, when semantically similar queries are available it is possible to
gainfully use them when ranking with respect to a particular query. We describe
and explore this phenomenon in the context of the bias-variance trade off and
apply it to the data-scarce settings of a Bing navigational graph and the
Drosophila larva connectome.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Helm_H/0/1/0/all/0/1"&gt;Hayden S. Helm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdin_M/0/1/0/all/0/1"&gt;Marah Abdin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedigo_B/0/1/0/all/0/1"&gt;Benjamin D. Pedigo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahajan_S/0/1/0/all/0/1"&gt;Shweti Mahajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyzinski_V/0/1/0/all/0/1"&gt;Vince Lyzinski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1"&gt;Youngser Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1"&gt;Amitabh Basu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhury_P/0/1/0/all/0/1"&gt;Piali~Choudhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1"&gt;Christopher M. White&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Weiwei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1"&gt;Carey E. Priebe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Three-stream network for enriched Action Recognition. (arXiv:2104.13051v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13051</id>
        <link href="http://arxiv.org/abs/2104.13051"/>
        <updated>2021-06-25T02:00:45.724Z</updated>
        <summary type="html"><![CDATA[Understanding accurate information on human behaviours is one of the most
important tasks in machine intelligence. Human Activity Recognition that aims
to understand human activities from a video is a challenging task due to
various problems including background, camera motion and dataset variations.
This paper proposes two CNN based architectures with three streams which allow
the model to exploit the dataset under different settings. The three pathways
are differentiated in frame rates. The single pathway, operates at a single
frame rate captures spatial information, the slow pathway operates at low frame
rates captures the spatial information and the fast pathway operates at high
frame rates that capture fine temporal information. Post CNN encoders, we add
bidirectional LSTM and attention heads respectively to capture the context and
temporal features. By experimenting with various algorithms on UCF-101,
Kinetics-600 and AVA dataset, we observe that the proposed models achieve
state-of-art performance for human action recognition task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sheth_I/0/1/0/all/0/1"&gt;Ivaxi Sheth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learnable Fourier Features for Multi-Dimensional Spatial Positional Encoding. (arXiv:2106.02795v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02795</id>
        <link href="http://arxiv.org/abs/2106.02795"/>
        <updated>2021-06-25T02:00:45.715Z</updated>
        <summary type="html"><![CDATA[Attentional mechanisms are order-invariant. Positional encoding is a crucial
component to allow attention-based deep model architectures such as Transformer
to address sequences or images where the position of information matters. In
this paper, we propose a novel positional encoding method based on learnable
Fourier features. Instead of hard-coding each position as a token or a vector,
we represent each position, which can be multi-dimensional, as a trainable
encoding based on learnable Fourier feature mapping, modulated with a
multi-layer perceptron. The representation is particularly advantageous for a
spatial multi-dimensional position, e.g., pixel positions on an image, where
$L_2$ distances or more complex positional relationships need to be captured.
Our experiments based on several public benchmark tasks show that our learnable
Fourier feature representation for multi-dimensional positional encoding
outperforms existing methods by both improving the accuracy and allowing faster
convergence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1"&gt;Si Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Gang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_S/0/1/0/all/0/1"&gt;Samy Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PocketNet: A Smaller Neural Network for Medical Image Analysis. (arXiv:2104.10745v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10745</id>
        <link href="http://arxiv.org/abs/2104.10745"/>
        <updated>2021-06-25T02:00:45.674Z</updated>
        <summary type="html"><![CDATA[Medical imaging deep learning models are often large and complex, requiring
specialized hardware to train and evaluate these models. To address such
issues, we propose the PocketNet paradigm to reduce the size of deep learning
models by throttling the growth of the number of channels in convolutional
neural networks. We demonstrate that, for a range of segmentation and
classification tasks, PocketNet architectures produce results comparable to
that of conventional neural networks while reducing the number of parameters by
multiple orders of magnitude, using up to 90% less GPU memory, and speeding up
training times by up to 40%, thereby allowing such models to be trained and
deployed in resource-constrained settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Celaya_A/0/1/0/all/0/1"&gt;Adrian Celaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Actor_J/0/1/0/all/0/1"&gt;Jonas A. Actor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Muthusivarajan_R/0/1/0/all/0/1"&gt;Rajarajeswari Muthusivarajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gates_E/0/1/0/all/0/1"&gt;Evan Gates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chung_C/0/1/0/all/0/1"&gt;Caroline Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schellingerhout_D/0/1/0/all/0/1"&gt;Dawid Schellingerhout&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Riviere_B/0/1/0/all/0/1"&gt;Beatrice Riviere&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fuentes_D/0/1/0/all/0/1"&gt;David Fuentes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for Network Traffic Classification. (arXiv:2106.12693v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2106.12693</id>
        <link href="http://arxiv.org/abs/2106.12693"/>
        <updated>2021-06-25T02:00:45.666Z</updated>
        <summary type="html"><![CDATA[Monitoring network traffic to identify content, services, and applications is
an active research topic in network traffic control systems. While modern
firewalls provide the capability to decrypt packets, this is not appealing for
privacy advocates. Hence, identifying any information from encrypted traffic is
a challenging task. Nonetheless, previous work has identified machine learning
methods that may enable application and service identification. The process
involves high level feature extraction from network packet data then training a
robust machine learning classifier for traffic identification. We propose a
classification technique using an ensemble of deep learning architectures on
packet, payload, and inter-arrival time sequences. To our knowledge, this is
the first time such deep learning architectures have been applied to the Server
Name Indication (SNI) classification problem. Our ensemble model beats the
state of the art machine learning methods and our up-to-date model can be found
on github: \url{https://github.com/niloofarbayat/NetworkClassification}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bayat_N/0/1/0/all/0/1"&gt;Niloofar Bayat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jackson_W/0/1/0/all/0/1"&gt;Weston Jackson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Derrick Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DP-SGD vs PATE: Which Has Less Disparate Impact on Model Accuracy?. (arXiv:2106.12576v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12576</id>
        <link href="http://arxiv.org/abs/2106.12576"/>
        <updated>2021-06-25T02:00:45.661Z</updated>
        <summary type="html"><![CDATA[Recent advances in differentially private deep learning have demonstrated
that application of differential privacy, specifically the DP-SGD algorithm,
has a disparate impact on different sub-groups in the population, which leads
to a significantly high drop-in model utility for sub-populations that are
under-represented (minorities), compared to well-represented ones. In this
work, we aim to compare PATE, another mechanism for training deep learning
models using differential privacy, with DP-SGD in terms of fairness. We show
that PATE does have a disparate impact too, however, it is much less severe
than DP-SGD. We draw insights from this observation on what might be promising
directions in achieving better fairness-privacy trade-offs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Uniyal_A/0/1/0/all/0/1"&gt;Archit Uniyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naidu_R/0/1/0/all/0/1"&gt;Rakshit Naidu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kotti_S/0/1/0/all/0/1"&gt;Sasikanth Kotti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sahib Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kenfack_P/0/1/0/all/0/1"&gt;Patrik Joslin Kenfack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1"&gt;Fatemehsadat Mireshghallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trask_A/0/1/0/all/0/1"&gt;Andrew Trask&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video Swin Transformer. (arXiv:2106.13230v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13230</id>
        <link href="http://arxiv.org/abs/2106.13230"/>
        <updated>2021-06-25T02:00:45.650Z</updated>
        <summary type="html"><![CDATA[The vision community is witnessing a modeling shift from CNNs to
Transformers, where pure Transformer architectures have attained top accuracy
on the major video recognition benchmarks. These video models are all built on
Transformer layers that globally connect patches across the spatial and
temporal dimensions. In this paper, we instead advocate an inductive bias of
locality in video Transformers, which leads to a better speed-accuracy
trade-off compared to previous approaches which compute self-attention globally
even with spatial-temporal factorization. The locality of the proposed video
architecture is realized by adapting the Swin Transformer designed for the
image domain, while continuing to leverage the power of pre-trained image
models. Our approach achieves state-of-the-art accuracy on a broad range of
video recognition benchmarks, including on action recognition (84.9 top-1
accuracy on Kinetics-400 and 86.1 top-1 accuracy on Kinetics-600 with ~20x less
pre-training data and ~3x smaller model size) and temporal modeling (69.6 top-1
accuracy on Something-Something v2). The code and models will be made publicly
available at https://github.com/SwinTransformer/Video-Swin-Transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ze Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ning_J/0/1/0/all/0/1"&gt;Jia Ning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yue Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yixuan Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Stephen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Han Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedFace: Collaborative Learning of Face Recognition Model. (arXiv:2104.03008v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03008</id>
        <link href="http://arxiv.org/abs/2104.03008"/>
        <updated>2021-06-25T02:00:45.641Z</updated>
        <summary type="html"><![CDATA[DNN-based face recognition models require large centrally aggregated face
datasets for training. However, due to the growing data privacy concerns and
legal restrictions, accessing and sharing face datasets has become exceedingly
difficult. We propose FedFace, a federated learning (FL) framework for
collaborative learning of face recognition models in a privacy-aware manner.
FedFace utilizes the face images available on multiple clients to learn an
accurate and generalizable face recognition model where the face images stored
at each client are neither shared with other clients nor the central host and
each client is a mobile device containing face images pertaining to only the
owner of the device (one identity per client). Our experiments show the
effectiveness of FedFace in enhancing the verification performance of
pre-trained face recognition system on standard face verification benchmarks
namely LFW, IJB-A, and IJB-C.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_D/0/1/0/all/0/1"&gt;Divyansh Aggarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jiayu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1"&gt;Anil K. Jain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields. (arXiv:2106.13228v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13228</id>
        <link href="http://arxiv.org/abs/2106.13228"/>
        <updated>2021-06-25T02:00:45.625Z</updated>
        <summary type="html"><![CDATA[Neural Radiance Fields (NeRF) are able to reconstruct scenes with
unprecedented fidelity, and various recent works have extended NeRF to handle
dynamic scenes. A common approach to reconstruct such non-rigid scenes is
through the use of a learned deformation field mapping from coordinates in each
input image into a canonical template coordinate space. However, these
deformation-based approaches struggle to model changes in topology, as
topological changes require a discontinuity in the deformation field, but these
deformation fields are necessarily continuous. We address this limitation by
lifting NeRFs into a higher dimensional space, and by representing the 5D
radiance field corresponding to each individual input image as a slice through
this "hyper-space". Our method is inspired by level set methods, which model
the evolution of surfaces as slices through a higher dimensional surface. We
evaluate our method on two tasks: (i) interpolating smoothly between "moments",
i.e., configurations of the scene, seen in the input images while maintaining
visual plausibility, and (ii) novel-view synthesis at fixed moments. We show
that our method, which we dub HyperNeRF, outperforms existing methods on both
tasks by significant margins. Compared to Nerfies, HyperNeRF reduces average
error rates by 8.6% for interpolation and 8.8% for novel-view synthesis, as
measured by LPIPS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1"&gt;Keunhong Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sinha_U/0/1/0/all/0/1"&gt;Utkarsh Sinha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hedman_P/0/1/0/all/0/1"&gt;Peter Hedman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1"&gt;Jonathan T. Barron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouaziz_S/0/1/0/all/0/1"&gt;Sofien Bouaziz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldman_D/0/1/0/all/0/1"&gt;Dan B Goldman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_Brualla_R/0/1/0/all/0/1"&gt;Ricardo Martin-Brualla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seitz_S/0/1/0/all/0/1"&gt;Steven M. Seitz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Network Slimming with Nonconvex Regularization. (arXiv:2010.01242v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.01242</id>
        <link href="http://arxiv.org/abs/2010.01242"/>
        <updated>2021-06-25T02:00:45.613Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) have developed to become powerful models
for various computer vision tasks ranging from object detection to semantic
segmentation. However, most of state-of-the-art CNNs can not be deployed
directly on edge devices such as smartphones and drones, which need low latency
under limited power and memory bandwidth. One popular, straightforward approach
to compressing CNNs is network slimming, which imposes $\ell_1$ regularization
on the channel-associated scaling factors via the batch normalization layers
during training. Network slimming thereby identifies insignificant channels
that can be pruned for inference. In this paper, we propose replacing the
$\ell_1$ penalty with an alternative sparse, nonconvex penalty in order to
yield a more compressed and/or accurate CNN architecture. We investigate
$\ell_p (0 < p < 1)$, transformed $\ell_1$ (T$\ell_1$), minimax concave penalty
(MCP), and smoothly clipped absolute deviation (SCAD) due to their recent
successes and popularity in solving sparse optimization problems, such as
compressed sensing and variable selection. We demonstrate the effectiveness of
network slimming with nonconvex penalties on VGGNet, Densenet, and Resnet on
standard image classification datasets. Based on the numerical experiments,
T$\ell_1$ preserves model accuracy against channel pruning, $\ell_{1/2, 3/4}$
yield better compressed models with similar accuracies after retraining as
$\ell_1$, and MCP and SCAD provide more accurate models after retraining with
similar compression as $\ell_1$. Network slimming with T$\ell_1$ regularization
also outperforms the latest Bayesian modification of network slimming in
compressing a CNN architecture in terms of memory storage while preserving its
model accuracy after channel pruning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bui_K/0/1/0/all/0/1"&gt;Kevin Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_F/0/1/0/all/0/1"&gt;Fredrick Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shuai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1"&gt;Yingyong Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1"&gt;Jack Xin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Embracing Uncertainty: Decoupling and De-bias for Robust Temporal Grounding. (arXiv:2103.16848v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16848</id>
        <link href="http://arxiv.org/abs/2103.16848"/>
        <updated>2021-06-25T02:00:45.599Z</updated>
        <summary type="html"><![CDATA[Temporal grounding aims to localize temporal boundaries within untrimmed
videos by language queries, but it faces the challenge of two types of
inevitable human uncertainties: query uncertainty and label uncertainty. The
two uncertainties stem from human subjectivity, leading to limited
generalization ability of temporal grounding. In this work, we propose a novel
DeNet (Decoupling and De-bias) to embrace human uncertainty: Decoupling - We
explicitly disentangle each query into a relation feature and a modified
feature. The relation feature, which is mainly based on skeleton-like words
(including nouns and verbs), aims to extract basic and consistent information
in the presence of query uncertainty. Meanwhile, modified feature assigned with
style-like words (including adjectives, adverbs, etc) represents the subjective
information, and thus brings personalized predictions; De-bias - We propose a
de-bias mechanism to generate diverse predictions, aim to alleviate the bias
caused by single-style annotations in the presence of label uncertainty.
Moreover, we put forward new multi-label metrics to diversify the performance
evaluation. Extensive experiments show that our approach is more effective and
robust than state-of-the-arts on Charades-STA and ActivityNet Captions
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chongyang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yan Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yanjun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1"&gt;Chuanping Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Composition of Saliency Metrics for Channel Pruning with a Myopic Oracle. (arXiv:2004.03376v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.03376</id>
        <link href="http://arxiv.org/abs/2004.03376"/>
        <updated>2021-06-25T02:00:45.594Z</updated>
        <summary type="html"><![CDATA[The computation and memory needed for Convolutional Neural Network (CNN)
inference can be reduced by pruning weights from the trained network. Pruning
is guided by a pruning saliency, which heuristically approximates the change in
the loss function associated with the removal of specific weights. Many pruning
signals have been proposed, but the performance of each heuristic depends on
the particular trained network. This leaves the data scientist with a difficult
choice. When using any one saliency metric for the entire pruning process, we
run the risk of the metric assumptions being invalidated, leading to poor
decisions being made by the metric. Ideally we could combine the best aspects
of different saliency metrics. However, despite an extensive literature review,
we are unable to find any prior work on composing different saliency metrics.
The chief difficulty lies in combining the numerical output of different
saliency metrics, which are not directly comparable.

We propose a method to compose several primitive pruning saliencies, to
exploit the cases where each saliency measure does well. Our experiments show
that the composition of saliencies avoids many poor pruning choices identified
by individual saliencies. In most cases our method finds better selections than
even the best individual pruning saliency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Persand_K/0/1/0/all/0/1"&gt;Kaveena Persand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_A/0/1/0/all/0/1"&gt;Andrew Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gregg_D/0/1/0/all/0/1"&gt;David Gregg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The effectiveness of feature attribution methods and its correlation with automatic evaluation scores. (arXiv:2105.14944v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14944</id>
        <link href="http://arxiv.org/abs/2105.14944"/>
        <updated>2021-06-25T02:00:45.588Z</updated>
        <summary type="html"><![CDATA[Explaining the decisions of an Artificial Intelligence (AI) model is
increasingly critical in many real-world, high-stake applications. Hundreds of
papers have either proposed new feature attribution methods, discussed or
harnessed these tools in their work. However, despite humans being the target
end-users, most attribution methods were only evaluated on proxy
automatic-evaluation metrics. In this paper, we conduct the first, large-scale
user study on 320 lay and 11 expert users to shed light on the effectiveness of
state-of-the-art attribution methods in assisting humans in ImageNet
classification, Stanford Dogs fine-grained classification, and these two tasks
but when the input image contains adversarial perturbations. We found that, in
overall, feature attribution is surprisingly not more effective than showing
humans nearest training-set examples. On a hard task of fine-grained dog
categorization, presenting attribution maps to humans does not help, but
instead hurts the performance of human-AI teams compared to AI alone.
Importantly, we found automatic attribution-map evaluation measures to
correlate poorly with the actual human-AI team performance. Our findings
encourage the community to rigorously test their methods on the downstream
human-in-the-loop applications and to rethink the existing evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_G/0/1/0/all/0/1"&gt;Giang Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Daeyoung Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;Anh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When Differential Privacy Meets Interpretability: A Case Study. (arXiv:2106.13203v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13203</id>
        <link href="http://arxiv.org/abs/2106.13203"/>
        <updated>2021-06-25T02:00:45.571Z</updated>
        <summary type="html"><![CDATA[Given the increase in the use of personal data for training Deep Neural
Networks (DNNs) in tasks such as medical imaging and diagnosis, differentially
private training of DNNs is surging in importance and there is a huge body of
work focusing on providing better privacy-utility trade-off. However, little
attention is given to the interpretability of these models, and how the
application of DP affects the quality of interpretations. We propose an
extensive study into the effects of DP training on DNNs, especially on medical
imaging applications, on the APTOS dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naidu_R/0/1/0/all/0/1"&gt;Rakshit Naidu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Priyanshu_A/0/1/0/all/0/1"&gt;Aman Priyanshu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Aadith Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kotti_S/0/1/0/all/0/1"&gt;Sasikanth Kotti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haofan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1"&gt;Fatemehsadat Mireshghallah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GaussiGAN: Controllable Image Synthesis with 3D Gaussians from Unposed Silhouettes. (arXiv:2106.13215v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13215</id>
        <link href="http://arxiv.org/abs/2106.13215"/>
        <updated>2021-06-25T02:00:45.565Z</updated>
        <summary type="html"><![CDATA[We present an algorithm that learns a coarse 3D representation of objects
from unposed multi-view 2D mask supervision, then uses it to generate detailed
mask and image texture. In contrast to existing voxel-based methods for unposed
object reconstruction, our approach learns to represent the generated shape and
pose with a set of self-supervised canonical 3D anisotropic Gaussians via a
perspective camera, and a set of per-image transforms. We show that this
approach can robustly estimate a 3D space for the camera and object, while
recent baselines sometimes struggle to reconstruct coherent 3D spaces in this
setting. We show results on synthetic datasets with realistic lighting, and
demonstrate object insertion with interactive posing. With our work, we help
move towards structured representations that handle more real-world variation
in learning-based object reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mejjati_Y/0/1/0/all/0/1"&gt;Youssef A.Mejjati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milefchik_I/0/1/0/all/0/1"&gt;Isa Milefchik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gokaslan_A/0/1/0/all/0/1"&gt;Aaron Gokaslan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_O/0/1/0/all/0/1"&gt;Oliver Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kwang In Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tompkin_J/0/1/0/all/0/1"&gt;James Tompkin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FitVid: Overfitting in Pixel-Level Video Prediction. (arXiv:2106.13195v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13195</id>
        <link href="http://arxiv.org/abs/2106.13195"/>
        <updated>2021-06-25T02:00:45.559Z</updated>
        <summary type="html"><![CDATA[An agent that is capable of predicting what happens next can perform a
variety of tasks through planning with no additional training. Furthermore,
such an agent can internally represent the complex dynamics of the real-world
and therefore can acquire a representation useful for a variety of visual
perception tasks. This makes predicting the future frames of a video,
conditioned on the observed past and potentially future actions, an interesting
task which remains exceptionally challenging despite many recent advances.
Existing video prediction models have shown promising results on simple narrow
benchmarks but they generate low quality predictions on real-life datasets with
more complicated dynamics or broader domain. There is a growing body of
evidence that underfitting on the training data is one of the primary causes
for the low quality predictions. In this paper, we argue that the inefficient
use of parameters in the current video models is the main reason for
underfitting. Therefore, we introduce a new architecture, named FitVid, which
is capable of severe overfitting on the common benchmarks while having similar
parameter count as the current state-of-the-art models. We analyze the
consequences of overfitting, illustrating how it can produce unexpected
outcomes such as generating high quality output by repeating the training data,
and how it can be mitigated using existing image augmentation techniques. As a
result, FitVid outperforms the current state-of-the-art models across four
different video prediction benchmarks on four different metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Babaeizadeh_M/0/1/0/all/0/1"&gt;Mohammad Babaeizadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saffar_M/0/1/0/all/0/1"&gt;Mohammad Taghi Saffar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1"&gt;Suraj Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1"&gt;Chelsea Finn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erhan_D/0/1/0/all/0/1"&gt;Dumitru Erhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Sparse and Locally Coherent Morphable Face Model for Dense Semantic Correspondence Across Heterogeneous 3D Faces. (arXiv:2006.03840v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03840</id>
        <link href="http://arxiv.org/abs/2006.03840"/>
        <updated>2021-06-25T02:00:45.554Z</updated>
        <summary type="html"><![CDATA[The 3D Morphable Model (3DMM) is a powerful statistical tool for representing
3D face shapes. To build a 3DMM, a training set of face scans in full
point-to-point correspondence is required, and its modeling capabilities
directly depend on the variability contained in the training data. Thus, to
increase the descriptive power of the 3DMM, establishing a dense correspondence
across heterogeneous scans with sufficient diversity in terms of identities,
ethnicities, or expressions becomes essential. In this manuscript, we present a
fully automatic approach that leverages a 3DMM to transfer its dense semantic
annotation across raw 3D faces, establishing a dense correspondence between
them. We propose a novel formulation to learn a set of sparse deformation
components with local support on the face that, together with an original
non-rigid deformation algorithm, allow the 3DMM to precisely fit unseen faces
and transfer its semantic annotation. We extensively experimented our approach,
showing it can effectively generalize to highly diverse samples and accurately
establish a dense correspondence even in presence of complex facial
expressions. The accuracy of the dense registration is demonstrated by building
a heterogeneous, large-scale 3DMM from more than 9,000 fully registered scans
obtained by joining three large datasets together.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ferrari_C/0/1/0/all/0/1"&gt;Claudio Ferrari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berretti_S/0/1/0/all/0/1"&gt;Stefano Berretti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pala_P/0/1/0/all/0/1"&gt;Pietro Pala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1"&gt;Alberto Del Bimbo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-resolution Image Registration of Consecutive and Re-stained Sections in Histopathology. (arXiv:2106.13150v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.13150</id>
        <link href="http://arxiv.org/abs/2106.13150"/>
        <updated>2021-06-25T02:00:45.548Z</updated>
        <summary type="html"><![CDATA[We compare variational image registration in consectutive and re-stained
sections from histopathology. We present a fully-automatic algorithm for
non-parametric (nonlinear) image registration and apply it to a previously
existing dataset from the ANHIR challenge (230 slide pairs, consecutive
sections) and a new dataset (hybrid re-stained and consecutive, 81 slide pairs,
ca. 3000 landmarks) which is made publicly available. Registration
hyperparameters are obtained in the ANHIR dataset and applied to the new
dataset without modification. In the new dataset, landmark errors after
registration range from 13.2 micrometers for consecutive sections to 1
micrometer for re-stained sections. We observe that non-parametric registration
leads to lower landmark errors in both cases, even though the effect is smaller
in re-stained sections. The nucleus-level alignment after non-parametric
registration of re-stained sections provides a valuable tool to generate
automatic ground-truth for machine learning applications in histopathology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lotz_J/0/1/0/all/0/1"&gt;Johannes Lotz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Weiss_N/0/1/0/all/0/1"&gt;Nick Weiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Laak_J/0/1/0/all/0/1"&gt;Jeroen van der Laak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+StefanHeldmann/0/1/0/all/0/1"&gt;StefanHeldmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differential Morph Face Detection using Discriminative Wavelet Sub-bands. (arXiv:2106.13178v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13178</id>
        <link href="http://arxiv.org/abs/2106.13178"/>
        <updated>2021-06-25T02:00:45.531Z</updated>
        <summary type="html"><![CDATA[Face recognition systems are extremely vulnerable to morphing attacks, in
which a morphed facial reference image can be successfully verified as two or
more distinct identities. In this paper, we propose a morph attack detection
algorithm that leverages an undecimated 2D Discrete Wavelet Transform (DWT) for
identifying morphed face images. The core of our framework is that artifacts
resulting from the morphing process that are not discernible in the image
domain can be more easily identified in the spatial frequency domain. A
discriminative wavelet sub-band can accentuate the disparity between a real and
a morphed image. To this end, multi-level DWT is applied to all images,
yielding 48 mid and high-frequency sub-bands each. The entropy distributions
for each sub-band are calculated separately for both bona fide and morph
images. For some of the sub-bands, there is a marked difference between the
entropy of the sub-band in a bona fide image and the identical sub-band's
entropy in a morphed image. Consequently, we employ Kullback-Liebler Divergence
(KLD) to exploit these differences and isolate the sub-bands that are the most
discriminative. We measure how discriminative a sub-band is by its KLD value
and the 22 sub-bands with the highest KLD values are chosen for network
training. Then, we train a deep Siamese neural network using these 22 selected
sub-bands for differential morph attack detection. We examine the efficacy of
discriminative wavelet sub-bands for morph attack detection and show that a
deep neural network trained on these sub-bands can accurately identify morph
imagery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhary_B/0/1/0/all/0/1"&gt;Baaria Chaudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aghdaie_P/0/1/0/all/0/1"&gt;Poorya Aghdaie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1"&gt;Sobhan Soleymani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1"&gt;Jeremy Dawson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1"&gt;Nasser M. Nasrabadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularisation for PCA- and SVD-type matrix factorisations. (arXiv:2106.12955v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12955</id>
        <link href="http://arxiv.org/abs/2106.12955"/>
        <updated>2021-06-25T02:00:45.526Z</updated>
        <summary type="html"><![CDATA[Singular Value Decomposition (SVD) and its close relative, Principal
Component Analysis (PCA), are well-known linear matrix decomposition techniques
that are widely used in applications such as dimension reduction and
clustering. However, an important limitation of SVD/PCA is its sensitivity to
noise in the input data. In this paper, we take another look at the problem of
regularisation and show that different formulations of the minimisation problem
lead to qualitatively different solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khoshrou_A/0/1/0/all/0/1"&gt;Abdolrahman Khoshrou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pauwels_E/0/1/0/all/0/1"&gt;Eric J. Pauwels&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Transformer-based Cross-modal Fusion Model with Adversarial Training for VQA Challenge 2021. (arXiv:2106.13033v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13033</id>
        <link href="http://arxiv.org/abs/2106.13033"/>
        <updated>2021-06-25T02:00:45.521Z</updated>
        <summary type="html"><![CDATA[In this paper, inspired by the successes of visionlanguage pre-trained models
and the benefits from training with adversarial attacks, we present a novel
transformerbased cross-modal fusion modeling by incorporating the both notions
for VQA challenge 2021. Specifically, the proposed model is on top of the
architecture of VinVL model [19], and the adversarial training strategy [4] is
applied to make the model robust and generalized. Moreover, two implementation
tricks are also used in our system to obtain better results. The experiments
demonstrate that the novel framework can achieve 76.72% on VQAv2 test-std set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1"&gt;Ke-Han Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1"&gt;Bo-Han Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kuan-Yu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ShapeFlow: Learnable Deformations Among 3D Shapes. (arXiv:2006.07982v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07982</id>
        <link href="http://arxiv.org/abs/2006.07982"/>
        <updated>2021-06-25T02:00:45.506Z</updated>
        <summary type="html"><![CDATA[We present ShapeFlow, a flow-based model for learning a deformation space for
entire classes of 3D shapes with large intra-class variations. ShapeFlow allows
learning a multi-template deformation space that is agnostic to shape topology,
yet preserves fine geometric details. Different from a generative space where a
latent vector is directly decoded into a shape, a deformation space decodes a
vector into a continuous flow that can advect a source shape towards a target.
Such a space naturally allows the disentanglement of geometric style (coming
from the source) and structural pose (conforming to the target). We parametrize
the deformation between geometries as a learned continuous flow field via a
neural network and show that such deformations can be guaranteed to have
desirable properties, such as be bijectivity, freedom from self-intersections,
or volume preservation. We illustrate the effectiveness of this learned
deformation space for various downstream applications, including shape
generation via deformation, geometric style transfer, unsupervised learning of
a consistent parameterization for entire classes of shapes, and shape
interpolation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1"&gt;Chiyu &amp;quot;Max&amp;quot; Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jingwei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1"&gt;Andrea Tagliasacchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1"&gt;Leonidas Guibas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient $k$-modes Algorithm for Clustering Categorical Datasets. (arXiv:2006.03936v3 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03936</id>
        <link href="http://arxiv.org/abs/2006.03936"/>
        <updated>2021-06-25T02:00:45.499Z</updated>
        <summary type="html"><![CDATA[Mining clusters from data is an important endeavor in many applications. The
$k$-means method is a popular, efficient, and distribution-free approach for
clustering numerical-valued data, but does not apply for categorical-valued
observations. The $k$-modes method addresses this lacuna by replacing the
Euclidean with the Hamming distance and the means with the modes in the
$k$-means objective function. We provide a novel, computationally efficient
implementation of $k$-modes, called OTQT. We prove that OTQT finds updates to
improve the objective function that are undetectable to existing $k$-modes
algorithms. Although slightly slower per iteration due to algorithmic
complexity, OTQT is always more accurate per iteration and almost always faster
(and only barely slower on some datasets) to the final optimum. Thus, we
recommend OTQT as the preferred, default algorithm for $k$-modes optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Dorman_K/0/1/0/all/0/1"&gt;Karin S. Dorman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Maitra_R/0/1/0/all/0/1"&gt;Ranjan Maitra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Advancing biological super-resolution microscopy through deep learning: a brief review. (arXiv:2106.13064v1 [physics.bio-ph])]]></title>
        <id>http://arxiv.org/abs/2106.13064</id>
        <link href="http://arxiv.org/abs/2106.13064"/>
        <updated>2021-06-25T02:00:45.484Z</updated>
        <summary type="html"><![CDATA[Super-resolution microscopy overcomes the diffraction limit of conventional
light microscopy in spatial resolution. By providing novel spatial or
spatio-temporal information on biological processes at nanometer resolution
with molecular specificity, it plays an increasingly important role in life
sciences. However, its technical limitations require trade-offs to balance its
spatial resolution, temporal resolution, and light exposure of samples.
Recently, deep learning has achieved breakthrough performance in many image
processing and computer vision tasks. It has also shown great promise in
pushing the performance envelope of super-resolution microscopy. In this brief
Review, we survey recent advances in using deep learning to enhance performance
of super-resolution microscopy. We focus primarily on how deep learning
ad-vances reconstruction of super-resolution images. Related key technical
challenges are discussed. Despite the challenges, deep learning is set to play
an indispensable and transformative role in the development of super-resolution
microscopy. We conclude with an outlook on how deep learning could shape the
future of this new generation of light microscopy technology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Yang_T/0/1/0/all/0/1"&gt;Tianjie Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yaoru Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ji_W/0/1/0/all/0/1"&gt;Wei Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Yang_G/0/1/0/all/0/1"&gt;Ge Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High Performance Hyperspectral Image Classification using Graphics Processing Units. (arXiv:2106.12942v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2106.12942</id>
        <link href="http://arxiv.org/abs/2106.12942"/>
        <updated>2021-06-25T02:00:45.478Z</updated>
        <summary type="html"><![CDATA[Real-time remote sensing applications like search and rescue missions,
military target detection, environmental monitoring, hazard prevention and
other time-critical applications require onboard real time processing
capabilities or autonomous decision making. Some unmanned remote systems like
satellites are physically remote from their operators, and all control of the
spacecraft and data returned by the spacecraft must be transmitted over a
wireless radio link. This link may not be available for extended periods when
the satellite is out of line of sight of its ground station. Therefore,
lightweight, small size and low power consumption hardware is essential for
onboard real time processing systems. With increasing dimensionality, size and
resolution of recent hyperspectral imaging sensors, additional challenges are
posed upon remote sensing processing systems and more capable computing
architectures are needed. Graphical Processing Units (GPUs) emerged as
promising architecture for light weight high performance computing that can
address these computational requirements for onboard systems. The goal of this
study is to build high performance methods for onboard hyperspectral analysis.
We propose accelerated methods for the well-known recursive hierarchical
segmentation (RHSEG) clustering method, using GPUs, hybrid multicore CPU with a
GPU and hybrid multi-core CPU/GPU clusters. RHSEG is a method developed by the
National Aeronautics and Space Administration (NASA), which is designed to
provide rich classification information with several output levels. The
achieved speedups by parallel solutions compared to CPU sequential
implementations are 21x for parallel single GPU and 240x for hybrid multi-node
computer clusters with 16 computing nodes. The energy consumption is reduced to
74% using a single GPU compared to the equivalent parallel CPU cluster.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hossam_M/0/1/0/all/0/1"&gt;Mahmoud Hossam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Handling Data Heterogeneity with Generative Replay in Collaborative Learning for Medical Imaging. (arXiv:2106.13208v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13208</id>
        <link href="http://arxiv.org/abs/2106.13208"/>
        <updated>2021-06-25T02:00:45.471Z</updated>
        <summary type="html"><![CDATA[Collaborative learning, which enables collaborative and decentralized
training of deep neural networks at multiple institutions in a
privacy-preserving manner, is rapidly emerging as a valuable technique in
healthcare applications. However, its distributed nature often leads to
significant heterogeneity in data distributions across institutions. Existing
collaborative learning approaches generally do not account for the presence of
heterogeneity in data among institutions, or only mildly skewed label
distributions are studied. In this paper, we present a novel generative replay
strategy to address the challenge of data heterogeneity in collaborative
learning methods. Instead of directly training a model for task performance, we
leverage recent image synthesis techniques to develop a novel dual model
architecture: a primary model learns the desired task, and an auxiliary
"generative replay model" either synthesizes images that closely resemble the
input images or helps extract latent variables. The generative replay strategy
is flexible to use, can either be incorporated into existing collaborative
learning methods to improve their capability of handling data heterogeneity
across institutions, or be used as a novel and individual collaborative
learning framework (termed FedReplay) to reduce communication cost.
Experimental results demonstrate the capability of the proposed method in
handling heterogeneous data across institutions. On highly heterogeneous data
partitions, our model achieves ~4.88% improvement in the prediction accuracy on
a diabetic retinopathy classification dataset, and ~49.8% reduction of mean
absolution value on a Bone Age prediction dataset, respectively, compared to
the state-of-the art collaborative learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1"&gt;Liangqiong Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balachandar_N/0/1/0/all/0/1"&gt;Niranjan Balachandar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Miao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1"&gt;Daniel Rubin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Depth Confidence-aware Camouflaged Object Detection. (arXiv:2106.13217v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13217</id>
        <link href="http://arxiv.org/abs/2106.13217"/>
        <updated>2021-06-25T02:00:45.466Z</updated>
        <summary type="html"><![CDATA[Camouflaged object detection (COD) aims to segment camouflaged objects hiding
in the environment, which is challenging due to the similar appearance of
camouflaged objects and their surroundings. Research in biology suggests that
depth can provide useful object localization cues for camouflaged object
discovery, as all the animals have 3D perception ability. However, the depth
information has not been exploited for camouflaged object detection. To explore
the contribution of depth for camouflage detection, we present a depth-guided
camouflaged object detection network with pre-computed depth maps from existing
monocular depth estimation methods. Due to the domain gap between the depth
estimation dataset and our camouflaged object detection dataset, the generated
depth may not be accurate enough to be directly used in our framework. We then
introduce a depth quality assessment module to evaluate the quality of depth
based on the model prediction from both RGB COD branch and RGB-D COD branch.
During training, only high-quality depth is used to update the modal
interaction module for multi-modal learning. During testing, our depth quality
assessment module can effectively determine the contribution of depth and
select the RGB branch or RGB-D branch for camouflage prediction. Extensive
experiments on various camouflaged object detection datasets prove the
effectiveness of our solution in exploring the depth information for
camouflaged object detection. Our code and data is publicly available at:
\url{https://github.com/JingZhang617/RGBD-COD}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1"&gt;Yunqiu Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_M/0/1/0/all/0/1"&gt;Mochu Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1"&gt;Aixuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1"&gt;Yuchao Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1"&gt;Yiran Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Driver-centric Risk Object Identification. (arXiv:2106.13201v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13201</id>
        <link href="http://arxiv.org/abs/2106.13201"/>
        <updated>2021-06-25T02:00:45.460Z</updated>
        <summary type="html"><![CDATA[A massive number of traffic fatalities are due to driver errors. To reduce
fatalities, developing intelligent driving systems assisting drivers to
identify potential risks is in urgent need. Risky situations are generally
defined based on collision prediction in existing research. However, collisions
are only one type of risk in traffic scenarios. We believe a more generic
definition is required. In this work, we propose a novel driver-centric
definition of risk, i.e., risky objects influence driver behavior. Based on
this definition, a new task called risk object identification is introduced. We
formulate the task as a cause-effect problem and present a novel two-stage risk
object identification framework, taking inspiration from models of situation
awareness and causal inference. A driver-centric Risk Object Identification
(ROI) dataset is curated to evaluate the proposed system. We demonstrate
state-of-the-art risk object identification performance compared with strong
baselines on the ROI dataset. In addition, we conduct extensive ablative
studies to justify our design choices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chengxi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1"&gt;Stanley H. Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yi-Ting Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ChaLearn Looking at People: Inpainting and Denoising challenges. (arXiv:2106.13071v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13071</id>
        <link href="http://arxiv.org/abs/2106.13071"/>
        <updated>2021-06-25T02:00:45.445Z</updated>
        <summary type="html"><![CDATA[Dealing with incomplete information is a well studied problem in the context
of machine learning and computational intelligence. However, in the context of
computer vision, the problem has only been studied in specific scenarios (e.g.,
certain types of occlusions in specific types of images), although it is common
to have incomplete information in visual data. This chapter describes the
design of an academic competition focusing on inpainting of images and video
sequences that was part of the competition program of WCCI2018 and had a
satellite event collocated with ECCV2018. The ChaLearn Looking at People
Inpainting Challenge aimed at advancing the state of the art on visual
inpainting by promoting the development of methods for recovering missing and
occluded information from images and video. Three tracks were proposed in which
visual inpainting might be helpful but still challenging: human body pose
estimation, text overlays removal and fingerprint denoising. This chapter
describes the design of the challenge, which includes the release of three
novel datasets, and the description of evaluation metrics, baselines and
evaluation protocol. The results of the challenge are analyzed and discussed in
detail and conclusions derived from this event are outlined.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1"&gt;Sergio Escalera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soler_M/0/1/0/all/0/1"&gt;Marti Soler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayache_S/0/1/0/all/0/1"&gt;Stephane Ayache&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guclu_U/0/1/0/all/0/1"&gt;Umut Guclu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1"&gt;Jun Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madadi_M/0/1/0/all/0/1"&gt;Meysam Madadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baro_X/0/1/0/all/0/1"&gt;Xavier Baro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Escalante_H/0/1/0/all/0/1"&gt;Hugo Jair Escalante&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guyon_I/0/1/0/all/0/1"&gt;Isabelle Guyon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Corruption Robustness: Inductive Biases in Vision Transformers and MLP-Mixers. (arXiv:2106.13122v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13122</id>
        <link href="http://arxiv.org/abs/2106.13122"/>
        <updated>2021-06-25T02:00:45.439Z</updated>
        <summary type="html"><![CDATA[Recently, vision transformers and MLP-based models have been developed in
order to address some of the prevalent weaknesses in convolutional neural
networks. Due to the novelty of transformers being used in this domain along
with the self-attention mechanism, it remains unclear to what degree these
architectures are robust to corruptions. Despite some works proposing that data
augmentation remains essential for a model to be robust against corruptions, we
propose to explore the impact that the architecture has on corruption
robustness. We find that vision transformer architectures are inherently more
robust to corruptions than the ResNet-50 and MLP-Mixers. We also find that
vision transformers with 5 times fewer parameters than a ResNet-50 have more
shape bias. Our code is available to reproduce.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Morrison_K/0/1/0/all/0/1"&gt;Katelyn Morrison&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gilby_B/0/1/0/all/0/1"&gt;Benjamin Gilby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipchak_C/0/1/0/all/0/1"&gt;Colton Lipchak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mattioli_A/0/1/0/all/0/1"&gt;Adam Mattioli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kovashka_A/0/1/0/all/0/1"&gt;Adriana Kovashka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoAdapt: Automated Segmentation Network Search for Unsupervised Domain Adaptation. (arXiv:2106.13227v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13227</id>
        <link href="http://arxiv.org/abs/2106.13227"/>
        <updated>2021-06-25T02:00:45.434Z</updated>
        <summary type="html"><![CDATA[Neural network-based semantic segmentation has achieved remarkable results
when large amounts of annotated data are available, that is, in the supervised
case. However, such data is expensive to collect and so methods have been
developed to adapt models trained on related, often synthetic data for which
labels are readily available. Current adaptation approaches do not consider the
dependence of the generalization/transferability of these models on network
architecture. In this paper, we perform neural architecture search (NAS) to
provide architecture-level perspective and analysis for domain adaptation. We
identify the optimization gap that exists when searching architectures for
unsupervised domain adaptation which makes this NAS problem uniquely difficult.
We propose bridging this gap by using maximum mean discrepancy and regional
weighted entropy to estimate the accuracy metric. Experimental results on
several widely adopted benchmarks show that our proposed AutoAdapt framework
indeed discovers architectures that improve the performance of a number of
existing adaptation techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1"&gt;Xueqing Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuxin Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Newsam_S/0/1/0/all/0/1"&gt;Shawn Newsam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Class agnostic moving target detection by color and location prediction of moving area. (arXiv:2106.12966v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12966</id>
        <link href="http://arxiv.org/abs/2106.12966"/>
        <updated>2021-06-25T02:00:45.429Z</updated>
        <summary type="html"><![CDATA[Moving target detection plays an important role in computer vision. However,
traditional algorithms such as frame difference and optical flow usually suffer
from low accuracy or heavy computation. Recent algorithms such as deep
learning-based convolutional neural networks have achieved high accuracy and
real-time performance, but they usually need to know the classes of targets in
advance, which limits the practical applications. Therefore, we proposed a
model free moving target detection algorithm. This algorithm extracts the
moving area through the difference of image features. Then, the color and
location probability map of the moving area will be calculated through maximum
a posteriori probability. And the target probability map can be obtained
through the dot multiply between the two maps. Finally, the optimal moving
target area can be solved by stochastic gradient descent on the target
probability map. Results show that the proposed algorithm achieves the highest
accuracy compared with state-of-the-art algorithms, without needing to know the
classes of targets. Furthermore, as the existing datasets are not suitable for
moving target detection, we proposed a method for producing evaluation dataset.
Besides, we also proved the proposed algorithm can be used to assist target
tracking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zhuang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1"&gt;Huajun Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhihai Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Q-space Conditioned Translation Networks for Directional Synthesis of Diffusion Weighted Images from Multi-modal Structural MRI. (arXiv:2106.13188v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.13188</id>
        <link href="http://arxiv.org/abs/2106.13188"/>
        <updated>2021-06-25T02:00:45.410Z</updated>
        <summary type="html"><![CDATA[Current deep learning approaches for diffusion MRI modeling circumvent the
need for densely-sampled diffusion-weighted images (DWIs) by directly
predicting microstructural indices from sparsely-sampled DWIs. However, they
implicitly make unrealistic assumptions of static $q$-space sampling during
training and reconstruction. Further, such approaches can restrict downstream
usage of variably sampled DWIs for usages including the estimation of
microstructural indices or tractography. We propose a generative adversarial
translation framework for high-quality DWI synthesis with arbitrary $q$-space
sampling given commonly acquired structural images (e.g., B0, T1, T2). Our
translation network linearly modulates its internal representations conditioned
on continuous $q$-space information, thus removing the need for fixed sampling
schemes. Moreover, this approach enables downstream estimation of high-quality
microstructural maps from arbitrarily subsampled DWIs, which may be
particularly important in cases with sparsely sampled DWIs. Across several
recent methodologies, the proposed approach yields improved DWI synthesis
accuracy and fidelity with enhanced downstream utility as quantified by the
accuracy of scalar microstructure indices estimated from the synthesized
images. Code is available at
https://github.com/mengweiren/q-space-conditioned-dwi-synthesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ren_M/0/1/0/all/0/1"&gt;Mengwei Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1"&gt;Heejong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dey_N/0/1/0/all/0/1"&gt;Neel Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gerig_G/0/1/0/all/0/1"&gt;Guido Gerig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Stronger Feature for Temporal Action Localization. (arXiv:2106.13014v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13014</id>
        <link href="http://arxiv.org/abs/2106.13014"/>
        <updated>2021-06-25T02:00:45.398Z</updated>
        <summary type="html"><![CDATA[Temporal action localization aims to localize starting and ending time with
action category. Limited by GPU memory, mainstream methods pre-extract features
for each video. Therefore, feature quality determines the upper bound of
detection performance. In this technical report, we explored classic
convolution-based backbones and the recent surge of transformer-based
backbones. We found that the transformer-based methods can achieve better
classification performance than convolution-based, but they cannot generate
accuracy action proposals. In addition, extracting features with larger frame
resolution to reduce the loss of spatial information can also effectively
improve the performance of temporal action localization. Finally, we achieve
42.42% in terms of mAP on validation set with a single SlowFast feature by a
simple combination: BMN+TCANet, which is 1.87% higher than the result of 2020's
multi-model ensemble. Finally, we achieve Rank 1st on the CVPR2021 HACS
supervised Temporal Action Localization Challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1"&gt;Zhiwu Qing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Ziyuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yutong Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shiwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_j/0/1/0/all/0/1"&gt;jianwen Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1"&gt;Mingqian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1"&gt;Changxin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1"&gt;Nong Sang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FaDIV-Syn: Fast Depth-Independent View Synthesis. (arXiv:2106.13139v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13139</id>
        <link href="http://arxiv.org/abs/2106.13139"/>
        <updated>2021-06-25T02:00:45.392Z</updated>
        <summary type="html"><![CDATA[We introduce FaDIV-Syn, a fast depth-independent view synthesis method. Our
multi-view approach addresses the problem that view synthesis methods are often
limited by their depth estimation stage, where incorrect depth predictions can
lead to large projection errors. To avoid this issue, we efficiently warp
multiple input images into the target frame for a range of assumed depth
planes. The resulting tensor representation is fed into a U-Net-like CNN with
gated convolutions, which directly produces the novel output view. We therefore
side-step explicit depth estimation. This improves efficiency and performance
on transparent, reflective, and feature-less scene parts. FaDIV-Syn can handle
both interpolation and extrapolation tasks and outperforms state-of-the-art
extrapolation methods on the large-scale RealEstate10k dataset. In contrast to
comparable methods, it is capable of real-time operation due to its lightweight
architecture. We further demonstrate data efficiency of FaDIV-Syn by training
from fewer examples as well as its generalization to higher resolutions and
arbitrary depth ranges under severe depth discretization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rochow_A/0/1/0/all/0/1"&gt;Andre Rochow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwarz_M/0/1/0/all/0/1"&gt;Max Schwarz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weinmann_M/0/1/0/all/0/1"&gt;Michael Weinmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1"&gt;Sven Behnke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Symmetric Wasserstein Autoencoders. (arXiv:2106.13024v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13024</id>
        <link href="http://arxiv.org/abs/2106.13024"/>
        <updated>2021-06-25T02:00:45.386Z</updated>
        <summary type="html"><![CDATA[Leveraging the framework of Optimal Transport, we introduce a new family of
generative autoencoders with a learnable prior, called Symmetric Wasserstein
Autoencoders (SWAEs). We propose to symmetrically match the joint distributions
of the observed data and the latent representation induced by the encoder and
the decoder. The resulting algorithm jointly optimizes the modelling losses in
both the data and the latent spaces with the loss in the data space leading to
the denoising effect. With the symmetric treatment of the data and the latent
representation, the algorithm implicitly preserves the local structure of the
data in the latent space. To further improve the quality of the latent
representation, we incorporate a reconstruction loss into the objective, which
significantly benefits both the generation and reconstruction. We empirically
show the superior performance of SWAEs over the state-of-the-art generative
autoencoders in terms of classification, reconstruction, and generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1"&gt;Sun Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Hongyu Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VOLO: Vision Outlooker for Visual Recognition. (arXiv:2106.13112v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13112</id>
        <link href="http://arxiv.org/abs/2106.13112"/>
        <updated>2021-06-25T02:00:45.360Z</updated>
        <summary type="html"><![CDATA[Visual recognition has been dominated by convolutionalneural networks (CNNs)
for years. Though recently the pre-vailing vision transformers (ViTs) have
shown great poten-tial of self-attention based models in ImageNet
classifica-tion, their performance is still inferior to latest SOTA CNNsif no
extra data are provided. In this work, we aim to closethe performance gap and
demonstrate that attention-basedmodels are indeed able to outperform CNNs. We
found thatthe main factor limiting the performance of ViTs for Ima-geNet
classification is their low efficacy in encoding fine-level features into the
token representations. To resolvethis, we introduce a noveloutlook attentionand
present asimple and general architecture, termed Vision Outlooker(VOLO). Unlike
self-attention that focuses on global depen-dency modeling at a coarse level,
the outlook attention aimsto efficiently encode finer-level features and
contexts intotokens, which are shown to be critical for recognition
per-formance but largely ignored by the self-attention. Experi-ments show that
our VOLO achieves 87.1% top-1 accuracyon ImageNet-1K classification, being the
first model exceed-ing 87% accuracy on this competitive benchmark, withoutusing
any extra training data. In addition, the pre-trainedVOLO transfers well to
downstream tasks, such as seman-tic segmentation. We achieve 84.3% mIoU score
on thecityscapes validation set and 54.3% on the ADE20K valida-tion set. Code
is available at https://github.com/sail-sg/volo.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Li Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1"&gt;Qibin Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zihang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiashi Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1"&gt;Shuicheng Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Fully Interpretable Deep Neural Networks: Are We There Yet?. (arXiv:2106.13164v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13164</id>
        <link href="http://arxiv.org/abs/2106.13164"/>
        <updated>2021-06-25T02:00:45.352Z</updated>
        <summary type="html"><![CDATA[Despite the remarkable performance, Deep Neural Networks (DNNs) behave as
black-boxes hindering user trust in Artificial Intelligence (AI) systems.
Research on opening black-box DNN can be broadly categorized into post-hoc
methods and inherently interpretable DNNs. While many surveys have been
conducted on post-hoc interpretation methods, little effort is devoted to
inherently interpretable DNNs. This paper provides a review of existing methods
to develop DNNs with intrinsic interpretability, with a focus on Convolutional
Neural Networks (CNNs). The aim is to understand the current progress towards
fully interpretable DNNs that can cater to different interpretation
requirements. Finally, we identify gaps in current work and suggest potential
research directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wickramanayake_S/0/1/0/all/0/1"&gt;Sandareka Wickramanayake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Wynne Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Mong Li Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuous-Time Deep Glioma Growth Models. (arXiv:2106.12917v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12917</id>
        <link href="http://arxiv.org/abs/2106.12917"/>
        <updated>2021-06-25T02:00:45.325Z</updated>
        <summary type="html"><![CDATA[The ability to estimate how a tumor might evolve in the future could have
tremendous clinical benefits, from improved treatment decisions to better dose
distribution in radiation therapy. Recent work has approached the glioma growth
modeling problem via deep learning and variational inference, thus learning
growth dynamics entirely from a real patient data distribution. So far, this
approach was constrained to predefined image acquisition intervals and
sequences of fixed length, which limits its applicability in more realistic
scenarios. We overcome these limitations by extending Neural Processes, a class
of conditional generative models for stochastic time series, with a
hierarchical multi-scale representation encoding including a spatio-temporal
attention mechanism. The result is a learned growth model that can be
conditioned on an arbitrary number of observations, and that can produce a
distribution of temporally consistent growth trajectories on a continuous time
axis. On a dataset of 379 patients, the approach successfully captures both
global and finer-grained variations in the images, exhibiting superior
performance compared to other learned growth models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Petersen_J/0/1/0/all/0/1"&gt;Jens Petersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Isensee_F/0/1/0/all/0/1"&gt;Fabian Isensee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kohler_G/0/1/0/all/0/1"&gt;Gregor K&amp;#xf6;hler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jager_P/0/1/0/all/0/1"&gt;Paul F. J&amp;#xe4;ger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zimmerer_D/0/1/0/all/0/1"&gt;David Zimmerer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Neuberger_U/0/1/0/all/0/1"&gt;Ulf Neuberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wick_W/0/1/0/all/0/1"&gt;Wolfgang Wick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Debus_J/0/1/0/all/0/1"&gt;J&amp;#xfc;rgen Debus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Heiland_S/0/1/0/all/0/1"&gt;Sabine Heiland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bendszus_M/0/1/0/all/0/1"&gt;Martin Bendszus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vollmuth_P/0/1/0/all/0/1"&gt;Philipp Vollmuth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Maier_Hein_K/0/1/0/all/0/1"&gt;Klaus H. Maier-Hein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SGTBN: Generating Dense Depth Maps from Single-Line LiDAR. (arXiv:2106.12994v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12994</id>
        <link href="http://arxiv.org/abs/2106.12994"/>
        <updated>2021-06-25T02:00:45.319Z</updated>
        <summary type="html"><![CDATA[Depth completion aims to generate a dense depth map from the sparse depth map
and aligned RGB image. However, current depth completion methods use extremely
expensive 64-line LiDAR(about $100,000) to obtain sparse depth maps, which will
limit their application scenarios. Compared with the 64-line LiDAR, the
single-line LiDAR is much less expensive and much more robust. Therefore, we
propose a method to tackle the problem of single-line depth completion, in
which we aim to generate a dense depth map from the single-line LiDAR info and
the aligned RGB image. A single-line depth completion dataset is proposed based
on the existing 64-line depth completion dataset(KITTI). A network called
Semantic Guided Two-Branch Network(SGTBN) which contains global and local
branches to extract and fuse global and local info is proposed for this task. A
Semantic guided depth upsampling module is used in our network to make full use
of the semantic info in RGB images. Except for the usual MSE loss, we add the
virtual normal loss to increase the constraint of high-order 3D geometry in our
network. Our network outperforms the state-of-the-art in the single-line depth
completion task. Besides, compared with the monocular depth estimation, our
method also has significant advantages in precision and model size.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Hengjie Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shugong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1"&gt;Shan Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Learning of Depth and Depth-of-Field Effect from Natural Images with Aperture Rendering Generative Adversarial Networks. (arXiv:2106.13041v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13041</id>
        <link href="http://arxiv.org/abs/2106.13041"/>
        <updated>2021-06-25T02:00:45.303Z</updated>
        <summary type="html"><![CDATA[Understanding the 3D world from 2D projected natural images is a fundamental
challenge in computer vision and graphics. Recently, an unsupervised learning
approach has garnered considerable attention owing to its advantages in data
collection. However, to mitigate training limitations, typical methods need to
impose assumptions for viewpoint distribution (e.g., a dataset containing
various viewpoint images) or object shape (e.g., symmetric objects). These
assumptions often restrict applications; for instance, the application to
non-rigid objects or images captured from similar viewpoints (e.g., flower or
bird images) remains a challenge. To complement these approaches, we propose
aperture rendering generative adversarial networks (AR-GANs), which equip
aperture rendering on top of GANs, and adopt focus cues to learn the depth and
depth-of-field (DoF) effect of unlabeled natural images. To address the
ambiguities triggered by unsupervised setting (i.e., ambiguities between smooth
texture and out-of-focus blurs, and between foreground and background blurs),
we develop DoF mixture learning, which enables the generator to learn real
image distribution while generating diverse DoF images. In addition, we devise
a center focus prior to guiding the learning direction. In the experiments, we
demonstrate the effectiveness of AR-GANs in various datasets, such as flower,
bird, and face images, demonstrate their portability by incorporating them into
other 3D representation learning GANs, and validate their applicability in
shallow DoF rendering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaneko_T/0/1/0/all/0/1"&gt;Takuhiro Kaneko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Simple and Strong Baseline: Progressively Region-based Scene Text Removal Networks. (arXiv:2106.13029v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13029</id>
        <link href="http://arxiv.org/abs/2106.13029"/>
        <updated>2021-06-25T02:00:45.297Z</updated>
        <summary type="html"><![CDATA[Existing scene text removal methods mainly train an elaborate network with
paired images to realize the function of text localization and background
reconstruction simultaneously, but there exists two problems: 1) lacking the
exhaustive erasure of text region and 2) causing the excessive erasure to
text-free areas. To handle these issues, this paper provides a novel
ProgrEssively Region-based scene Text eraser (PERT), which introduces
region-based modification strategy to progressively erase the pixels in only
text region. Firstly, PERT decomposes the STR task to several erasing stages.
As each stage aims to take a further step toward the text-removed image rather
than directly regress to the final result, the decomposed operation reduces the
learning difficulty in each stage, and an exhaustive erasure result can be
obtained by iterating over lightweight erasing blocks with shared parameters.
Then, PERT introduces a region-based modification strategy to ensure the
integrity of text-free areas by decoupling text localization from erasure
process to guide the removal. Benefiting from the simplicity architecture, PERT
is a simple and strong baseline, and is easy to be followed and developed.
Extensive experiments demonstrate that PERT obtains the state-of-the-art
results on both synthetic and real-world datasets. Code is available
athttps://github.com/wangyuxin87/PERT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1"&gt;Hongtao Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1"&gt;Shancheng Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1"&gt;Yadong Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongdong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MatchVIE: Exploiting Match Relevancy between Entities for Visual Information Extraction. (arXiv:2106.12940v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12940</id>
        <link href="http://arxiv.org/abs/2106.12940"/>
        <updated>2021-06-25T02:00:45.288Z</updated>
        <summary type="html"><![CDATA[Visual Information Extraction (VIE) task aims to extract key information from
multifarious document images (e.g., invoices and purchase receipts). Most
previous methods treat the VIE task simply as a sequence labeling problem or
classification problem, which requires models to carefully identify each kind
of semantics by introducing multimodal features, such as font, color, layout.
But simply introducing multimodal features couldn't work well when faced with
numeric semantic categories or some ambiguous texts. To address this issue, in
this paper we propose a novel key-value matching model based on a graph neural
network for VIE (MatchVIE). Through key-value matching based on relevancy
evaluation, the proposed MatchVIE can bypass the recognitions to various
semantics, and simply focuses on the strong relevancy between entities.
Besides, we introduce a simple but effective operation, Num2Vec, to tackle the
instability of encoded values, which helps model converge more smoothly.
Comprehensive experiments demonstrate that the proposed MatchVIE can
significantly outperform previous methods. Notably, to the best of our
knowledge, MatchVIE may be the first attempt to tackle the VIE task by modeling
the relevancy between keys and values and it is a good complement to the
existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_G/0/1/0/all/0/1"&gt;Guozhi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1"&gt;Lele Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1"&gt;Lianwen Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiapeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jingdong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qianying Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yaqiang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hui Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Monte Carlo Rendering via Multi-Resolution Sampling. (arXiv:2106.12802v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12802</id>
        <link href="http://arxiv.org/abs/2106.12802"/>
        <updated>2021-06-25T02:00:45.282Z</updated>
        <summary type="html"><![CDATA[Monte Carlo rendering algorithms are widely used to produce photorealistic
computer graphics images. However, these algorithms need to sample a
substantial amount of rays per pixel to enable proper global illumination and
thus require an immense amount of computation. In this paper, we present a
hybrid rendering method to speed up Monte Carlo rendering algorithms. Our
method first generates two versions of a rendering: one at a low resolution
with a high sample rate (LRHS) and the other at a high resolution with a low
sample rate (HRLS). We then develop a deep convolutional neural network to fuse
these two renderings into a high-quality image as if it were rendered at a high
resolution with a high sample rate. Specifically, we formulate this fusion task
as a super resolution problem that generates a high resolution rendering from a
low resolution input (LRHS), assisted with the HRLS rendering. The HRLS
rendering provides critical high frequency details which are difficult to
recover from the LRHS for any super resolution methods. Our experiments show
that our hybrid rendering algorithm is significantly faster than the
state-of-the-art Monte Carlo denoising methods while rendering high-quality
images when tested on both our own BCR dataset and the Gharbi dataset.
\url{https://github.com/hqqxyy/msspl}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1"&gt;Qiqi Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marshall_C/0/1/0/all/0/1"&gt;Carl S Marshall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panneer_S/0/1/0/all/0/1"&gt;Selvakumar Panneer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Feng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Completion for Occluded Person Re-Identification. (arXiv:2106.12733v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12733</id>
        <link href="http://arxiv.org/abs/2106.12733"/>
        <updated>2021-06-25T02:00:45.276Z</updated>
        <summary type="html"><![CDATA[Person re-identification (reID) plays an important role in computer vision.
However, existing methods suffer from performance degradation in occluded
scenes. In this work, we propose an occlusion-robust block, Region Feature
Completion (RFC), for occluded reID. Different from most previous works that
discard the occluded regions, RFC block can recover the semantics of occluded
regions in feature space. Firstly, a Spatial RFC (SRFC) module is developed.
SRFC exploits the long-range spatial contexts from non-occluded regions to
predict the features of occluded regions. The unit-wise prediction task leads
to an encoder/decoder architecture, where the region-encoder models the
correlation between non-occluded and occluded region, and the region-decoder
utilizes the spatial correlation to recover occluded region features. Secondly,
we introduce Temporal RFC (TRFC) module which captures the long-term temporal
contexts to refine the prediction of SRFC. RFC block is lightweight, end-to-end
trainable and can be easily plugged into existing CNNs to form RFCnet.
Extensive experiments are conducted on occluded and commonly holistic reID
benchmarks. Our method significantly outperforms existing methods on the
occlusion datasets, while remains top even superior performance on holistic
datasets. The source code is available at
https://github.com/blue-blue272/OccludedReID-RFCnet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1"&gt;Ruibing Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1"&gt;Bingpeng Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Hong Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1"&gt;Xinqian Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1"&gt;Shiguang Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xilin Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relationship between pulmonary nodule malignancy and surrounding pleurae, airways and vessels: a quantitative study using the public LIDC-IDRI dataset. (arXiv:2106.12991v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12991</id>
        <link href="http://arxiv.org/abs/2106.12991"/>
        <updated>2021-06-25T02:00:45.270Z</updated>
        <summary type="html"><![CDATA[To investigate whether the pleurae, airways and vessels surrounding a nodule
on non-contrast computed tomography (CT) can discriminate benign and malignant
pulmonary nodules. The LIDC-IDRI dataset, one of the largest publicly available
CT database, was exploited for study. A total of 1556 nodules from 694 patients
were involved in statistical analysis, where nodules with average scorings <3
and >3 were respectively denoted as benign and malignant. Besides, 339 nodules
from 113 patients with diagnosis ground-truth were independently evaluated.
Computer algorithms were developed to segment pulmonary structures and quantify
the distances to pleural surface, airways and vessels, as well as the counting
number and normalized volume of airways and vessels near a nodule. Odds ratio
(OR) and Chi-square (\chi^2) testing were performed to demonstrate the
correlation between features of surrounding structures and nodule malignancy. A
non-parametric receiver operating characteristic (ROC) analysis was conducted
in logistic regression to evaluate discrimination ability of each structure.
For benign and malignant groups, the average distances from nodules to pleural
surface, airways and vessels are respectively (6.56, 5.19), (37.08, 26.43) and
(1.42, 1.07) mm. The correlation between nodules and the counting number of
airways and vessels that contact or project towards nodules are respectively
(OR=22.96, \chi^2=105.04) and (OR=7.06, \chi^2=290.11). The correlation between
nodules and the volume of airways and vessels are (OR=9.19, \chi^2=159.02) and
(OR=2.29, \chi^2=55.89). The areas-under-curves (AUCs) for pleurae, airways and
vessels are respectively 0.5202, 0.6943 and 0.6529. Our results show that
malignant nodules are often surrounded by more pulmonary structures compared
with benign ones, suggesting that features of these structures could be viewed
as lung cancer biomarkers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1"&gt;Yulei Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yun Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hanxiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jie Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lihui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_F/0/1/0/all/0/1"&gt;Feng Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yue-Min Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Global Appearance and Local Coding Distortion based Fusion Framework for CNN based Filtering in Video Coding. (arXiv:2106.12746v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12746</id>
        <link href="http://arxiv.org/abs/2106.12746"/>
        <updated>2021-06-25T02:00:45.264Z</updated>
        <summary type="html"><![CDATA[In-loop filtering is used in video coding to process the reconstructed frame
in order to remove blocking artifacts. With the development of convolutional
neural networks (CNNs), CNNs have been explored for in-loop filtering
considering it can be treated as an image de-noising task. However, in addition
to being a distorted image, the reconstructed frame is also obtained by a fixed
line of block based encoding operations in video coding. It carries coding-unit
based coding distortion of some similar characteristics. Therefore, in this
paper, we address the filtering problem from two aspects, global appearance
restoration for disrupted texture and local coding distortion restoration
caused by fixed pipeline of coding. Accordingly, a three-stream global
appearance and local coding distortion based fusion network is developed with a
high-level global feature stream, a high-level local feature stream and a
low-level local feature stream. Ablation study is conducted to validate the
necessity of different features, demonstrating that the global features and
local features can complement each other in filtering and achieve better
performance when combined. To the best of our knowledge, we are the first one
that clearly characterizes the video filtering process from the above global
appearance and local coding distortion restoration aspects with experimental
verification, providing a clear pathway to developing filter techniques.
Experimental results demonstrate that the proposed method significantly
outperforms the existing single-frame based methods and achieves 13.5%, 11.3%,
11.7% BD-Rate saving on average for AI, LDP and RA configurations,
respectively, compared with the HEVC reference software.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yue_J/0/1/0/all/0/1"&gt;Jian Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yanbo Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yuan_H/0/1/0/all/0/1"&gt;Hui Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dufaux_F/0/1/0/all/0/1"&gt;Fr&amp;#xe9;d&amp;#xe9;ric Dufaux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Automatic Speech to Sign Language Generation. (arXiv:2106.12790v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12790</id>
        <link href="http://arxiv.org/abs/2106.12790"/>
        <updated>2021-06-25T02:00:45.258Z</updated>
        <summary type="html"><![CDATA[We aim to solve the highly challenging task of generating continuous sign
language videos solely from speech segments for the first time. Recent efforts
in this space have focused on generating such videos from human-annotated text
transcripts without considering other modalities. However, replacing speech
with sign language proves to be a practical solution while communicating with
people suffering from hearing loss. Therefore, we eliminate the need of using
text as input and design techniques that work for more natural, continuous,
freely uttered speech covering an extensive vocabulary. Since the current
datasets are inadequate for generating sign language directly from speech, we
collect and release the first Indian sign language dataset comprising
speech-level annotations, text transcripts, and the corresponding sign-language
videos. Next, we propose a multi-tasking transformer network trained to
generate signer's poses from speech segments. With speech-to-text as an
auxiliary task and an additional cross-modal discriminator, our model learns to
generate continuous sign pose sequences in an end-to-end manner. Extensive
experiments and comparisons with other baselines demonstrate the effectiveness
of our approach. We also conduct additional ablation studies to analyze the
effect of different modules of our network. A demo video containing several
results is attached to the supplementary material.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kapoor_P/0/1/0/all/0/1"&gt;Parul Kapoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukhopadhyay_R/0/1/0/all/0/1"&gt;Rudrabha Mukhopadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hegde_S/0/1/0/all/0/1"&gt;Sindhu B Hegde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1"&gt;Vinay Namboodiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1"&gt;C V Jawahar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention Toward Neighbors: A Context Aware Framework for High Resolution Image Segmentation. (arXiv:2106.12902v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12902</id>
        <link href="http://arxiv.org/abs/2106.12902"/>
        <updated>2021-06-25T02:00:45.238Z</updated>
        <summary type="html"><![CDATA[High-resolution image segmentation remains challenging and error-prone due to
the enormous size of intermediate feature maps. Conventional methods avoid this
problem by using patch based approaches where each patch is segmented
independently. However, independent patch segmentation induces errors,
particularly at the patch boundary due to the lack of contextual information in
very high-resolution images where the patch size is much smaller compared to
the full image. To overcome these limitations, in this paper, we propose a
novel framework to segment a particular patch by incorporating contextual
information from its neighboring patches. This allows the segmentation network
to see the target patch with a wider field of view without the need of larger
feature maps. Comparative analysis from a number of experiments shows that our
proposed framework is able to segment high resolution images with significantly
improved mean Intersection over Union and overall accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Niloy_F/0/1/0/all/0/1"&gt;Fahim Faisal Niloy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amin_M/0/1/0/all/0/1"&gt;M. Ashraful Amin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1"&gt;Amin Ahsan Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1"&gt;AKM Mahbubur Rahman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VinDr-SpineXR: A deep learning framework for spinal lesions detection and classification from radiographs. (arXiv:2106.12930v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12930</id>
        <link href="http://arxiv.org/abs/2106.12930"/>
        <updated>2021-06-25T02:00:45.228Z</updated>
        <summary type="html"><![CDATA[Radiographs are used as the most important imaging tool for identifying spine
anomalies in clinical practice. The evaluation of spinal bone lesions, however,
is a challenging task for radiologists. This work aims at developing and
evaluating a deep learning-based framework, named VinDr-SpineXR, for the
classification and localization of abnormalities from spine X-rays. First, we
build a large dataset, comprising 10,468 spine X-ray images from 5,000 studies,
each of which is manually annotated by an experienced radiologist with bounding
boxes around abnormal findings in 13 categories. Using this dataset, we then
train a deep learning classifier to determine whether a spine scan is abnormal
and a detector to localize 7 crucial findings amongst the total 13. The
VinDr-SpineXR is evaluated on a test set of 2,078 images from 1,000 studies,
which is kept separate from the training set. It demonstrates an area under the
receiver operating characteristic curve (AUROC) of 88.61% (95% CI 87.19%,
90.02%) for the image-level classification task and a mean average precision
(mAP@0.5) of 33.56% for the lesion-level localization task. These results serve
as a proof of concept and set a baseline for future research in this direction.
To encourage advances, the dataset, codes, and trained deep learning models are
made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Hieu T. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pham_H/0/1/0/all/0/1"&gt;Hieu H. Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_N/0/1/0/all/0/1"&gt;Nghia T. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Ha Q. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huynh_T/0/1/0/all/0/1"&gt;Thang Q. Huynh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dao_M/0/1/0/all/0/1"&gt;Minh Dao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vu_V/0/1/0/all/0/1"&gt;Van Vu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video Super-Resolution with Long-Term Self-Exemplars. (arXiv:2106.12778v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12778</id>
        <link href="http://arxiv.org/abs/2106.12778"/>
        <updated>2021-06-25T02:00:45.220Z</updated>
        <summary type="html"><![CDATA[Existing video super-resolution methods often utilize a few neighboring
frames to generate a higher-resolution image for each frame. However, the
redundant information between distant frames has not been fully exploited in
these methods: corresponding patches of the same instance appear across distant
frames at different scales. Based on this observation, we propose a video
super-resolution method with long-term cross-scale aggregation that leverages
similar patches (self-exemplars) across distant frames. Our model also consists
of a multi-reference alignment module to fuse the features derived from similar
patches: we fuse the features of distant references to perform high-quality
super-resolution. We also propose a novel and practical training strategy for
referenced-based super-resolution. To evaluate the performance of our proposed
method, we conduct extensive experiments on our collected CarCam dataset and
the Waymo Open dataset, and the results demonstrate our method outperforms
state-of-the-art methods. Our source code will be publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_G/0/1/0/all/0/1"&gt;Guotao Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yue Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Sijin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DCoM: A Deep Column Mapper for Semantic Data Type Detection. (arXiv:2106.12871v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12871</id>
        <link href="http://arxiv.org/abs/2106.12871"/>
        <updated>2021-06-25T02:00:45.198Z</updated>
        <summary type="html"><![CDATA[Detection of semantic data types is a very crucial task in data science for
automated data cleaning, schema matching, data discovery, semantic data type
normalization and sensitive data identification. Existing methods include
regular expression-based or dictionary lookup-based methods that are not robust
to dirty as well unseen data and are limited to a very less number of semantic
data types to predict. Existing Machine Learning methods extract large number
of engineered features from data and build logistic regression, random forest
or feedforward neural network for this purpose. In this paper, we introduce
DCoM, a collection of multi-input NLP-based deep neural networks to detect
semantic data types where instead of extracting large number of features from
the data, we feed the raw values of columns (or instances) to the model as
texts. We train DCoM on 686,765 data columns extracted from VizNet corpus with
78 different semantic data types. DCoM outperforms other contemporary results
with a quite significant margin on the same dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1"&gt;Subhadip Maji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rout_S/0/1/0/all/0/1"&gt;Swapna Sourav Rout&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1"&gt;Sudeep Choudhary&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rate Distortion Characteristic Modeling for Neural Image Compression. (arXiv:2106.12954v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12954</id>
        <link href="http://arxiv.org/abs/2106.12954"/>
        <updated>2021-06-25T02:00:45.183Z</updated>
        <summary type="html"><![CDATA[End-to-end optimization capability offers neural image compression (NIC)
superior lossy compression performance. However, distinct models are required
to be trained to reach different points in the rate-distortion (R-D) space. In
this paper, we consider the problem of R-D characteristic analysis and modeling
for NIC. We make efforts to formulate the essential mathematical functions to
describe the R-D behavior of NIC using deep network and statistical modeling.
Thus continuous bit-rate points could be elegantly realized by leveraging such
model via a single trained network. In this regard, we propose a plugin-in
module to learn the relationship between the target bit-rate and the binary
representation for the latent variable of auto-encoder. Furthermore, we model
the rate and distortion characteristic of NIC as a function of the coding
parameter $\lambda$ respectively. Our experiments show our proposed method is
easy to adopt and obtains competitive coding performance with fixed-rate coding
approaches, which would benefit the practical deployment of NIC. In addition,
the proposed model could be applied to NIC rate control with limited bit-rate
error using a single network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jia_C/0/1/0/all/0/1"&gt;Chuanmin Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ge_Z/0/1/0/all/0/1"&gt;Ziqing Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shanshe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ma_S/0/1/0/all/0/1"&gt;Siwei Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gao_W/0/1/0/all/0/1"&gt;Wen Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Systematic Collection of Medical Image Datasets for Deep Learning. (arXiv:2106.12864v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12864</id>
        <link href="http://arxiv.org/abs/2106.12864"/>
        <updated>2021-06-25T02:00:45.167Z</updated>
        <summary type="html"><![CDATA[The astounding success made by artificial intelligence (AI) in healthcare and
other fields proves that AI can achieve human-like performance. However,
success always comes with challenges. Deep learning algorithms are
data-dependent and require large datasets for training. The lack of data in the
medical imaging field creates a bottleneck for the application of deep learning
to medical image analysis. Medical image acquisition, annotation, and analysis
are costly, and their usage is constrained by ethical restrictions. They also
require many resources, such as human expertise and funding. That makes it
difficult for non-medical researchers to have access to useful and large
medical data. Thus, as comprehensive as possible, this paper provides a
collection of medical image datasets with their associated challenges for deep
learning research. We have collected information of around three hundred
datasets and challenges mainly reported between 2013 and 2020 and categorized
them into four categories: head & neck, chest & abdomen, pathology & blood, and
``others''. Our paper has three purposes: 1) to provide a most up to date and
complete list that can be used as a universal reference to easily find the
datasets for clinical image analysis, 2) to guide researchers on the
methodology to test and evaluate their methods' performance and robustness on
relevant datasets, 3) to provide a ``route'' to relevant algorithms for the
relevant medical topics, and challenge leaderboards.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Johann Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_G/0/1/0/all/0/1"&gt;Guangming Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hua_C/0/1/0/all/0/1"&gt;Cong Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Feng_M/0/1/0/all/0/1"&gt;Mingtao Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+BasheerBennamoun/0/1/0/all/0/1"&gt;BasheerBennamoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_P/0/1/0/all/0/1"&gt;Ping Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaoyuan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Song_J/0/1/0/all/0/1"&gt;Juan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_P/0/1/0/all/0/1"&gt;Peiyi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mei_L/0/1/0/all/0/1"&gt;Lin Mei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shah_S/0/1/0/all/0/1"&gt;Syed Afaq Ali Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bennamoun_M/0/1/0/all/0/1"&gt;Mohammed Bennamoun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of Deepfake Videos Using Long Distance Attention. (arXiv:2106.12832v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12832</id>
        <link href="http://arxiv.org/abs/2106.12832"/>
        <updated>2021-06-25T02:00:45.150Z</updated>
        <summary type="html"><![CDATA[With the rapid progress of deepfake techniques in recent years, facial video
forgery can generate highly deceptive video contents and bring severe security
threats. And detection of such forgery videos is much more urgent and
challenging. Most existing detection methods treat the problem as a vanilla
binary classification problem. In this paper, the problem is treated as a
special fine-grained classification problem since the differences between fake
and real faces are very subtle. It is observed that most existing face forgery
methods left some common artifacts in the spatial domain and time domain,
including generative defects in the spatial domain and inter-frame
inconsistencies in the time domain. And a spatial-temporal model is proposed
which has two components for capturing spatial and temporal forgery traces in
global perspective respectively. The two components are designed using a novel
long distance attention mechanism. The one component of the spatial domain is
used to capture artifacts in a single frame, and the other component of the
time domain is used to capture artifacts in consecutive frames. They generate
attention maps in the form of patches. The attention method has a broader
vision which contributes to better assembling global information and extracting
local statistic information. Finally, the attention maps are used to guide the
network to focus on pivotal parts of the face, just like other fine-grained
classification methods. The experimental results on different public datasets
demonstrate that the proposed method achieves the state-of-the-art performance,
and the proposed long distance attention method can effectively capture pivotal
parts for face forgery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wei Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingyi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Junwei Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xianfeng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yicong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiwu Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Frequency Domain Convolutional Neural Network: Accelerated CNN for Large Diabetic Retinopathy Image Classification. (arXiv:2106.12736v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12736</id>
        <link href="http://arxiv.org/abs/2106.12736"/>
        <updated>2021-06-25T02:00:45.143Z</updated>
        <summary type="html"><![CDATA[The conventional spatial convolution layers in the Convolutional Neural
Networks (CNNs) are computationally expensive at the point where the training
time could take days unless the number of layers, the number of training images
or the size of the training images are reduced. The image size of 256x256
pixels is commonly used for most of the applications of CNN, but this image
size is too small for applications like Diabetic Retinopathy (DR)
classification where the image details are important for accurate
classification. This research proposed Frequency Domain Convolution (FDC) and
Frequency Domain Pooling (FDP) layers which were built with RFFT, kernel
initialization strategy, convolution artifact removal and Channel Independent
Convolution (CIC) to replace the conventional convolution and pooling layers.
The FDC and FDP layers are used to build a Frequency Domain Convolutional
Neural Network (FDCNN) to accelerate the training of large images for DR
classification. The Full FDC layer is an extension of the FDC layer to allow
direct use in conventional CNNs, it is also used to modify the VGG16
architecture. FDCNN is shown to be at least 54.21% faster and 70.74% more
memory efficient compared to an equivalent CNN architecture. The modified VGG16
architecture with Full FDC layer is reported to achieve a shorter training time
and a higher accuracy at 95.63% compared to the original VGG16 architecture for
DR classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goh_E/0/1/0/all/0/1"&gt;Ee Fey Goh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;ZhiYuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_W/0/1/0/all/0/1"&gt;Wei Xiang Lim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Modal 3D Object Detection in Autonomous Driving: a Survey. (arXiv:2106.12735v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12735</id>
        <link href="http://arxiv.org/abs/2106.12735"/>
        <updated>2021-06-25T02:00:45.127Z</updated>
        <summary type="html"><![CDATA[In the past few years, we have witnessed rapid development of autonomous
driving. However, achieving full autonomy remains a daunting task due to the
complex and dynamic driving environment. As a result, self-driving cars are
equipped with a suite of sensors to conduct robust and accurate environment
perception. As the number and type of sensors keep increasing, combining them
for better perception is becoming a natural trend. So far, there has been no
indepth review that focuses on multi-sensor fusion based perception. To bridge
this gap and motivate future research, this survey devotes to review recent
fusion-based 3D detection deep learning models that leverage multiple sensor
data sources, especially cameras and LiDARs. In this survey, we first introduce
the background of popular sensors for autonomous cars, including their common
data representations as well as object detection networks developed for each
type of sensor data. Next, we discuss some popular datasets for multi-modal 3D
object detection, with a special focus on the sensor data included in each
dataset. Then we present in-depth reviews of recent multi-modal 3D detection
networks by considering the following three aspects of the fusion: fusion
location, fusion data representation, and fusion granularity. After a detailed
review, we discuss open challenges and point out possible solutions. We hope
that our detailed review can help researchers to embark investigations in the
area of multi-modal 3D object detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yingjie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_Q/0/1/0/all/0/1"&gt;Qiuyu Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hanqi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1"&gt;Jianmin Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yanyong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Planetary UAV localization based on Multi-modal Registration with Pre-existing Digital Terrain Model. (arXiv:2106.12738v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12738</id>
        <link href="http://arxiv.org/abs/2106.12738"/>
        <updated>2021-06-25T02:00:45.120Z</updated>
        <summary type="html"><![CDATA[The autonomous real-time optical navigation of planetary UAV is of the key
technologies to ensure the success of the exploration. In such a GPS denied
environment, vision-based localization is an optimal approach. In this paper,
we proposed a multi-modal registration based SLAM algorithm, which estimates
the location of a planet UAV using a nadir view camera on the UAV compared with
pre-existing digital terrain model. To overcome the scale and appearance
difference between on-board UAV images and pre-installed digital terrain model,
a theoretical model is proposed to prove that topographic features of UAV image
and DEM can be correlated in frequency domain via cross power spectrum. To
provide the six-DOF of the UAV, we also developed an optimization approach
which fuses the geo-referencing result into a SLAM system via LBA (Local Bundle
Adjustment) to achieve robust and accurate vision-based navigation even in
featureless planetary areas. To test the robustness and effectiveness of the
proposed localization algorithm, a new cross-source drone-based localization
dataset for planetary exploration is proposed. The proposed dataset includes
40200 synthetic drone images taken from nine planetary scenes with related DEM
query images. Comparison experiments carried out demonstrate that over the
flight distance of 33.8km, the proposed method achieved average localization
error of 0.45 meters, compared to 1.31 meters by ORB-SLAM, with the processing
speed of 12hz which will ensure a real-time performance. We will make our
datasets available to encourage further work on this promising topic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1"&gt;Xue Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yuanbin Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shengyang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Novelty Detection. (arXiv:2106.12964v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12964</id>
        <link href="http://arxiv.org/abs/2106.12964"/>
        <updated>2021-06-25T02:00:45.114Z</updated>
        <summary type="html"><![CDATA[Novelty Detection methods identify samples that are not representative of a
model's training set thereby flagging misleading predictions and bringing a
greater flexibility and transparency at deployment time. However, research in
this area has only considered Novelty Detection in the offline setting.
Recently, there has been a growing realization in the computer vision community
that applications demand a more flexible framework - Continual Learning - where
new batches of data representing new domains, new classes or new tasks become
available at different points in time. In this setting, Novelty Detection
becomes more important, interesting and challenging. This work identifies the
crucial link between the two problems and investigates the Novelty Detection
problem under the Continual Learning setting. We formulate the Continual
Novelty Detection problem and present a benchmark, where we compare several
Novelty Detection methods under different Continual Learning settings.

We show that Continual Learning affects the behaviour of novelty detection
algorithms, while novelty detection can pinpoint insights in the behaviour of a
continual learner. We further propose baselines and discuss possible research
directions. We believe that the coupling of the two problems is a promising
direction to bring vision models into practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aljundi_R/0/1/0/all/0/1"&gt;Rahaf Aljundi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reino_D/0/1/0/all/0/1"&gt;Daniel Olmeda Reino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chumerin_N/0/1/0/all/0/1"&gt;Nikolay Chumerin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1"&gt;Richard E. Turner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What makes visual place recognition easy or hard?. (arXiv:2106.12671v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12671</id>
        <link href="http://arxiv.org/abs/2106.12671"/>
        <updated>2021-06-25T02:00:45.108Z</updated>
        <summary type="html"><![CDATA[Visual place recognition is a fundamental capability for the localization of
mobile robots. It places image retrieval in the practical context of physical
agents operating in a physical world. It is an active field of research and
many different approaches have been proposed and evaluated in many different
experiments. In the following, we argue that due to variations of this
practical context and individual design decisions, place recognition
experiments are barely comparable across different papers and that there is a
variety of properties that can change from one experiment to another. We
provide an extensive list of such properties and give examples how they can be
used to setup a place recognition experiment easier or harder. This might be
interesting for different involved parties: (1) people who just want to select
a place recognition approach that is suitable for the properties of their
particular task at hand, (2) researchers that look for open research questions
and are interested in particularly difficult instances, (3) authors that want
to create reproducible papers on this topic, and (4) also reviewers that have
the task to identify potential problems in papers under review.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schubert_S/0/1/0/all/0/1"&gt;Stefan Schubert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neubert_P/0/1/0/all/0/1"&gt;Peer Neubert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Deep Image Stitching: Reconstructing Stitched Features to Images. (arXiv:2106.12859v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12859</id>
        <link href="http://arxiv.org/abs/2106.12859"/>
        <updated>2021-06-25T02:00:45.101Z</updated>
        <summary type="html"><![CDATA[Traditional feature-based image stitching technologies rely heavily on
feature detection quality, often failing to stitch images with few features or
low resolution. The learning-based image stitching solutions are rarely studied
due to the lack of labeled data, making the supervised methods unreliable. To
address the above limitations, we propose an unsupervised deep image stitching
framework consisting of two stages: unsupervised coarse image alignment and
unsupervised image reconstruction. In the first stage, we design an
ablation-based loss to constrain an unsupervised homography network, which is
more suitable for large-baseline scenes. Moreover, a transformer layer is
introduced to warp the input images in the stitching-domain space. In the
second stage, motivated by the insight that the misalignments in pixel-level
can be eliminated to a certain extent in feature-level, we design an
unsupervised image reconstruction network to eliminate the artifacts from
features to pixels. Specifically, the reconstruction network can be implemented
by a low-resolution deformation branch and a high-resolution refined branch,
learning the deformation rules of image stitching and enhancing the resolution
simultaneously. To establish an evaluation benchmark and train the learning
framework, a comprehensive real-world image dataset for unsupervised deep image
stitching is presented and released. Extensive experiments well demonstrate the
superiority of our method over other state-of-the-art solutions. Even compared
with the supervised solutions, our image stitching quality is still preferred
by users.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1"&gt;Lang Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chunyu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1"&gt;Kang Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shuaicheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yao Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Florida Wildlife Camera Trap Dataset. (arXiv:2106.12628v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12628</id>
        <link href="http://arxiv.org/abs/2106.12628"/>
        <updated>2021-06-25T02:00:45.078Z</updated>
        <summary type="html"><![CDATA[Trail camera imagery has increasingly gained popularity amongst biologists
for conservation and ecological research. Minimal human interference required
to operate camera traps allows capturing unbiased species activities. Several
studies - based on human and wildlife interactions, migratory patterns of
various species, risk of extinction in endangered populations - are limited by
the lack of rich data and the time-consuming nature of manually annotating
trail camera imagery. We introduce a challenging wildlife camera trap
classification dataset collected from two different locations in Southwestern
Florida, consisting of 104,495 images featuring visually similar species,
varying illumination conditions, skewed class distribution, and including
samples of endangered species, i.e. Florida panthers. Experimental evaluations
with ResNet-50 architecture indicate that this image classification-based
dataset can further push the advancements in wildlife statistical modeling. We
will make the dataset publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gagne_C/0/1/0/all/0/1"&gt;Crystal Gagne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kini_J/0/1/0/all/0/1"&gt;Jyoti Kini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_D/0/1/0/all/0/1"&gt;Daniel Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Mubarak Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[All You Need is a Second Look: Towards Arbitrary-Shaped Text Detection. (arXiv:2106.12720v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12720</id>
        <link href="http://arxiv.org/abs/2106.12720"/>
        <updated>2021-06-25T02:00:45.072Z</updated>
        <summary type="html"><![CDATA[Arbitrary-shaped text detection is a challenging task since curved texts in
the wild are of the complex geometric layouts. Existing mainstream methods
follow the instance segmentation pipeline to obtain the text regions. However,
arbitraryshaped texts are difficult to be depicted through one single
segmentation network because of the varying scales. In this paper, we propose a
two-stage segmentation-based detector, termed as NASK (Need A Second looK), for
arbitrary-shaped text detection. Compared to the traditional single-stage
segmentation network, our NASK conducts the detection in a coarse-to-fine
manner with the first stage segmentation spotting the rectangle text proposals
and the second one retrieving compact representations. Specifically, NASK is
composed of a Text Instance Segmentation (TIS) network (1st stage), a
Geometry-aware Text RoI Alignment (GeoAlign) module, and a Fiducial pOint
eXpression (FOX) module (2nd stage). Firstly, TIS extracts the augmented
features with a novel Group Spatial and Channel Attention (GSCA) module and
conducts instance segmentation to obtain rectangle proposals. Then, GeoAlign
converts these rectangles into the fixed size and encodes RoI-wise feature
representation. Finally, FOX disintegrates the text instance into serval
pivotal geometrical attributes to refine the detection results. Extensive
experimental results on three public benchmarks including Total-Text,
SCUTCTW1500, and ICDAR 2015 verify that our NASK outperforms recent
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1"&gt;Meng Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Can Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1"&gt;Dongming Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ATP-Net: An Attention-based Ternary Projection Network For Compressed Sensing. (arXiv:2106.12728v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.12728</id>
        <link href="http://arxiv.org/abs/2106.12728"/>
        <updated>2021-06-25T02:00:45.044Z</updated>
        <summary type="html"><![CDATA[Compressed Sensing (CS) theory simultaneously realizes the signal sampling
and compression process, and can use fewer observations to achieve accurate
signal recovery, providing a solution for better and faster transmission of
massive data. In this paper, a ternary sampling matrix-based method with
attention mechanism is proposed with the purpose to solve the problem that the
CS sampling matrices in most cases are random matrices, which are irrelative to
the sampled signal and need a large storage space. The proposed method consists
of three components, i.e., ternary sampling, initial reconstruction and deep
reconstruction, with the emphasis on the ternary sampling. The main idea of the
ternary method (-1, 0, +1) is to introduce the attention mechanism to evaluate
the importance of parameters at the sampling layer after the sampling matrix is
binarized (-1, +1), followed by pruning weight of parameters, whose importance
is below a predefined threshold, to achieve ternarization. Furthermore, a
compressed sensing algorithm especially for image reconstruction is
implemented, on the basis of the ternary sampling matrix, which is called
ATP-Net, i.e., Attention-based ternary projection network. Experimental results
show that the quality of image reconstruction by means of ATP-Net maintains a
satisfactory level with the employment of the ternary sampling matrix, i.e.,
the average PSNR on Set11 is 30.4 when the sampling rate is 0.25, approximately
6% improvement compared with that of DR2-Net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nie_G/0/1/0/all/0/1"&gt;Guanxiong Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yajian Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human Activity Recognition using Continuous Wavelet Transform and Convolutional Neural Networks. (arXiv:2106.12666v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12666</id>
        <link href="http://arxiv.org/abs/2106.12666"/>
        <updated>2021-06-25T02:00:45.035Z</updated>
        <summary type="html"><![CDATA[Quite a few people in the world have to stay under permanent surveillance for
health reasons; they include diabetic people or people with some other chronic
conditions, the elderly and the disabled.These groups may face heightened risk
of having life-threatening falls or of being struck by a syncope. Due to
limited availability of resources a substantial part of people at risk can not
receive necessary monitoring and thus are exposed to excessive danger.
Nowadays, this problem is usually solved via applying Human Activity
Recognition (HAR) methods. HAR is a perspective and fast-paced Data Science
field, which has a wide range of application areas such as healthcare, sport,
security etc. However, the currently techniques of recognition are markedly
lacking in accuracy, hence, the present paper suggests a highly accurate method
for human activity classification. Wepropose a new workflow to address the HAR
problem and evaluate it on the UniMiB SHAR dataset, which consists of the
accelerometer signals. The model we suggest is based on continuous wavelet
transform (CWT) and convolutional neural networks (CNNs). Wavelet transform
localizes signal features both in time and frequency domains and after that a
CNN extracts these features and recognizes activity. It is also worth noting
that CWT converts 1D accelerometer signal into 2D images and thus enables to
obtain better results as 2D networks have a significantly higher predictive
capacity. In the course of the work we build a convolutional neural network and
vary such model parameters as number of spatial axes, number of layers, number
of neurons in each layer, image size, type of mother wavelet, the order of zero
moment of mother wavelet etc. Besides, we also apply models with residual
blocks which resulted in significantly higher metric values. Finally, we
succeed to reach 99.26 % accuracy and it is a worthy performance for this
problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nedorubova_A/0/1/0/all/0/1"&gt;Anna Nedorubova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadyrova_A/0/1/0/all/0/1"&gt;Alena Kadyrova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khlyupin_A/0/1/0/all/0/1"&gt;Aleksey Khlyupin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topological Semantic Mapping by Consolidation of Deep Visual Features. (arXiv:2106.12709v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12709</id>
        <link href="http://arxiv.org/abs/2106.12709"/>
        <updated>2021-06-25T02:00:45.015Z</updated>
        <summary type="html"><![CDATA[Many works in the recent literature introduce semantic mapping methods that
use CNNs (Convolutional Neural Networks) to recognize semantic properties in
images. The types of properties (eg.: room size, place category, and objects)
and their classes (eg.: kitchen and bathroom, for place category) are usually
predefined and restricted to a specific task. Thus, all the visual data
acquired and processed during the construction of the maps are lost and only
the recognized semantic properties remain on the maps. In contrast, this work
introduces a topological semantic mapping method that uses deep visual features
extracted by a CNN, the GoogLeNet, from 2D images captured in multiple views of
the environment as the robot operates, to create consolidated representations
of visual features acquired in the regions covered by each topological node.
These consolidated representations allow flexible recognition of semantic
properties of the regions and use in a range of visual tasks. The experiments,
performed using a real-world indoor dataset, showed that the method is able to
consolidate the visual features of regions and use them to recognize objects
and place categories as semantic properties, and to indicate the topological
location of images, with very promising results. The objects are classified
using the classification layer of GoogLeNet, without retraining, and the place
categories are recognized using a shallow Multilayer Perceptron.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sousa_Y/0/1/0/all/0/1"&gt;Ygor C. N. Sousa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bassani_H/0/1/0/all/0/1"&gt;Hansenclever F. Bassani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Dialogue Learning for Spoken Conversational Question Answering. (arXiv:2106.02182v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02182</id>
        <link href="http://arxiv.org/abs/2106.02182"/>
        <updated>2021-06-25T02:00:44.993Z</updated>
        <summary type="html"><![CDATA[In spoken conversational question answering (SCQA), the answer to the
corresponding question is generated by retrieving and then analyzing a fixed
spoken document, including multi-part conversations. Most SCQA systems have
considered only retrieving information from ordered utterances. However, the
sequential order of dialogue is important to build a robust spoken
conversational question answering system, and the changes of utterances order
may severely result in low-quality and incoherent corpora. To this end, we
introduce a self-supervised learning approach, including incoherence
discrimination, insertion detection, and question prediction, to explicitly
capture the coreference resolution and dialogue coherence among spoken
documents. Specifically, we design a joint learning framework where the
auxiliary self-supervised tasks can enable the pre-trained SCQA systems towards
more coherent and meaningful spoken dialogue learning. We also utilize the
proposed self-supervised learning tasks to capture intra-sentence coherence.
Experimental results demonstrate that our proposed method provides more
coherent, meaningful, and appropriate responses, yielding superior performance
gains compared to the original pre-trained language models. Our method achieves
state-of-the-art results on the Spoken-CoQA dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenyu You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional Deformable Image Registration with Convolutional Neural Network. (arXiv:2106.12673v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12673</id>
        <link href="http://arxiv.org/abs/2106.12673"/>
        <updated>2021-06-25T02:00:44.981Z</updated>
        <summary type="html"><![CDATA[Recent deep learning-based methods have shown promising results and runtime
advantages in deformable image registration. However, analyzing the effects of
hyperparameters and searching for optimal regularization parameters prove to be
too prohibitive in deep learning-based methods. This is because it involves
training a substantial number of separate models with distinct hyperparameter
values. In this paper, we propose a conditional image registration method and a
new self-supervised learning paradigm for deep deformable image registration.
By learning the conditional features that correlated with the regularization
hyperparameter, we demonstrate that optimal solutions with arbitrary
hyperparameters can be captured by a single deep convolutional neural network.
In addition, the smoothness of the resulting deformation field can be
manipulated with arbitrary strength of smoothness regularization during
inference. Extensive experiments on a large-scale brain MRI dataset show that
our proposed method enables the precise control of the smoothness of the
deformation field without sacrificing the runtime advantage or registration
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mok_T/0/1/0/all/0/1"&gt;Tony C. W. Mok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_A/0/1/0/all/0/1"&gt;Albert C. S. Chung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Perturb Word Embeddings for Out-of-distribution QA. (arXiv:2105.02692v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02692</id>
        <link href="http://arxiv.org/abs/2105.02692"/>
        <updated>2021-06-25T02:00:44.973Z</updated>
        <summary type="html"><![CDATA[QA models based on pretrained language mod-els have achieved remarkable
performance on various benchmark datasets.However, QA models do not generalize
well to unseen data that falls outside the training distribution, due to
distributional shifts.Data augmentation (DA) techniques which drop/replace
words have shown to be effective in regularizing the model from overfitting to
the training data.Yet, they may adversely affect the QA tasks since they incur
semantic changes that may lead to wrong answers for the QA task. To tackle this
problem, we propose a simple yet effective DA method based on a stochastic
noise generator, which learns to perturb the word embedding of the input
questions and context without changing their semantics. We validate the
performance of the QA models trained with our word embedding perturbation on a
single source dataset, on five different target domains.The results show that
our method significantly outperforms the baselineDA methods. Notably, the model
trained with ours outperforms the model trained with more than 240K
artificially generated QA pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seanie Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1"&gt;Minki Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Juho Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Sung Ju Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting the Success of Domain Adaptation in Text Similarity. (arXiv:2106.04641v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04641</id>
        <link href="http://arxiv.org/abs/2106.04641"/>
        <updated>2021-06-25T02:00:44.963Z</updated>
        <summary type="html"><![CDATA[Transfer learning methods, and in particular domain adaptation, help exploit
labeled data in one domain to improve the performance of a certain task in
another domain. However, it is still not clear what factors affect the success
of domain adaptation. This paper models adaptation success and selection of the
most suitable source domains among several candidates in text similarity. We
use descriptive domain information and cross-domain similarity metrics as
predictive features. While mostly positive, the results also point to some
domains where adaptation success was difficult to predict.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pogrebnyakov_N/0/1/0/all/0/1"&gt;Nicolai Pogrebnyakov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaghaghian_S/0/1/0/all/0/1"&gt;Shohreh Shaghaghian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Context Transformer with Stacked Pointer Networks for Conversational Question Answering over Knowledge Graphs. (arXiv:2103.07766v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07766</id>
        <link href="http://arxiv.org/abs/2103.07766"/>
        <updated>2021-06-25T02:00:44.946Z</updated>
        <summary type="html"><![CDATA[Neural semantic parsing approaches have been widely used for Question
Answering (QA) systems over knowledge graphs. Such methods provide the
flexibility to handle QA datasets with complex queries and a large number of
entities. In this work, we propose a novel framework named CARTON, which
performs multi-task semantic parsing for handling the problem of conversational
question answering over a large-scale knowledge graph. Our framework consists
of a stack of pointer networks as an extension of a context transformer model
for parsing the input question and the dialog history. The framework generates
a sequence of actions that can be executed on the knowledge graph. We evaluate
CARTON on a standard dataset for complex sequential question answering on which
CARTON outperforms all baselines. Specifically, we observe performance
improvements in F1-score on eight out of ten question types compared to the
previous state of the art. For logical reasoning questions, an improvement of
11 absolute points is reached.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Plepi_J/0/1/0/all/0/1"&gt;Joan Plepi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kacupaj_E/0/1/0/all/0/1"&gt;Endri Kacupaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1"&gt;Kuldeep Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thakkar_H/0/1/0/all/0/1"&gt;Harsh Thakkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1"&gt;Jens Lehmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SocAoG: Incremental Graph Parsing for Social Relation Inference in Dialogues. (arXiv:2106.01006v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01006</id>
        <link href="http://arxiv.org/abs/2106.01006"/>
        <updated>2021-06-25T02:00:44.940Z</updated>
        <summary type="html"><![CDATA[Inferring social relations from dialogues is vital for building emotionally
intelligent robots to interpret human language better and act accordingly. We
model the social network as an And-or Graph, named SocAoG, for the consistency
of relations among a group and leveraging attributes as inference cues.
Moreover, we formulate a sequential structure prediction task, and propose an
$\alpha$-$\beta$-$\gamma$ strategy to incrementally parse SocAoG for the
dynamic inference upon any incoming utterance: (i) an $\alpha$ process
predicting attributes and relations conditioned on the semantics of dialogues,
(ii) a $\beta$ process updating the social relations based on related
attributes, and (iii) a $\gamma$ process updating individual's attributes based
on interpersonal social relations. Empirical results on DialogRE and MovieGraph
show that our model infers social relations more accurately than the
state-of-the-art methods. Moreover, the ablation study shows the three
processes complement each other, and the case study demonstrates the dynamic
relational inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1"&gt;Liang Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yuan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yizhou Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1"&gt;Pan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1"&gt;Baolin Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhou Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Ying Nian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Song-Chun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Understanding and Mitigating Social Biases in Language Models. (arXiv:2106.13219v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13219</id>
        <link href="http://arxiv.org/abs/2106.13219"/>
        <updated>2021-06-25T02:00:44.927Z</updated>
        <summary type="html"><![CDATA[As machine learning methods are deployed in real-world settings such as
healthcare, legal systems, and social science, it is crucial to recognize how
they shape social biases and stereotypes in these sensitive decision-making
processes. Among such real-world deployments are large-scale pretrained
language models (LMs) that can be potentially dangerous in manifesting
undesirable representational biases - harmful biases resulting from
stereotyping that propagate negative generalizations involving gender, race,
religion, and other social constructs. As a step towards improving the fairness
of LMs, we carefully define several sources of representational biases before
proposing new benchmarks and metrics to measure them. With these tools, we
propose steps towards mitigating social biases during text generation. Our
empirical results and human evaluation demonstrate effectiveness in mitigating
bias while retaining crucial contextual information for high-fidelity text
generation, thereby pushing forward the performance-fairness Pareto frontier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chiyu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1"&gt;Louis-Philippe Morency&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextualized Attention-based Knowledge Transfer for Spoken Conversational Question Answering. (arXiv:2010.11066v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11066</id>
        <link href="http://arxiv.org/abs/2010.11066"/>
        <updated>2021-06-25T02:00:44.921Z</updated>
        <summary type="html"><![CDATA[Spoken conversational question answering (SCQA) requires machines to model
complex dialogue flow given the speech utterances and text corpora. Different
from traditional text question answering (QA) tasks, SCQA involves audio signal
processing, passage comprehension, and contextual understanding. However, ASR
systems introduce unexpected noisy signals to the transcriptions, which result
in performance degradation on SCQA. To overcome the problem, we propose CADNet,
a novel contextualized attention-based distillation approach, which applies
both cross-attention and self-attention to obtain ASR-robust contextualized
embedding representations of the passage and dialogue history for performance
improvements. We also introduce the spoken conventional knowledge distillation
framework to distill the ASR-robust knowledge from the estimated probabilities
of the teacher model to the student. We conduct extensive experiments on the
Spoken-CoQA dataset and demonstrate that our approach achieves remarkable
performance in this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenyu You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing Orthogonal Constraint in Structural Probes. (arXiv:2012.15228v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15228</id>
        <link href="http://arxiv.org/abs/2012.15228"/>
        <updated>2021-06-25T02:00:44.903Z</updated>
        <summary type="html"><![CDATA[With the recent success of pre-trained models in NLP, a significant focus was
put on interpreting their representations. One of the most prominent approaches
is structural probing (Hewitt and Manning, 2019), where a linear projection of
word embeddings is performed in order to approximate the topology of dependency
structures. In this work, we introduce a new type of structural probing, where
the linear projection is decomposed into 1. isomorphic space rotation; 2.
linear scaling that identifies and scales the most relevant dimensions. In
addition to syntactic dependency, we evaluate our method on novel tasks
(lexical hypernymy and position in a sentence). We jointly train the probes for
multiple tasks and experimentally show that lexical and syntactic information
is separated in the representations. Moreover, the orthogonal constraint makes
the Structural Probes less vulnerable to memorization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Limisiewicz_T/0/1/0/all/0/1"&gt;Tomasz Limisiewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marecek_D/0/1/0/all/0/1"&gt;David Mare&amp;#x10d;ek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emotion Carrier Recognition from Personal Narratives. (arXiv:2008.07481v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.07481</id>
        <link href="http://arxiv.org/abs/2008.07481"/>
        <updated>2021-06-25T02:00:44.896Z</updated>
        <summary type="html"><![CDATA[Personal Narratives (PN) - recollections of facts, events, and thoughts from
one's own experience - are often used in everyday conversations. So far, PNs
have mainly been explored for tasks such as valence prediction or emotion
classification (e.g. happy, sad). However, these tasks might overlook more
fine-grained information that could prove to be relevant for understanding PNs.
In this work, we propose a novel task for Narrative Understanding: Emotion
Carrier Recognition (ECR). Emotion carriers, the text fragments that carry the
emotions of the narrator (e.g. loss of a grandpa, high school reunion), provide
a fine-grained description of the emotion state. We explore the task of ECR in
a corpus of PNs manually annotated with emotion carriers and investigate
different machine learning models for the task. We propose evaluation
strategies for ECR including metrics that can be appropriate for different
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tammewar_A/0/1/0/all/0/1"&gt;Aniruddha Tammewar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cervone_A/0/1/0/all/0/1"&gt;Alessandra Cervone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riccardi_G/0/1/0/all/0/1"&gt;Giuseppe Riccardi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Language and Multimodal Privacy-Preserving Markers of Mood from Mobile Data. (arXiv:2106.13213v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.13213</id>
        <link href="http://arxiv.org/abs/2106.13213"/>
        <updated>2021-06-25T02:00:44.881Z</updated>
        <summary type="html"><![CDATA[Mental health conditions remain underdiagnosed even in countries with common
access to advanced medical care. The ability to accurately and efficiently
predict mood from easily collectible data has several important implications
for the early detection, intervention, and treatment of mental health
disorders. One promising data source to help monitor human behavior is daily
smartphone usage. However, care must be taken to summarize behaviors without
identifying the user through personal (e.g., personally identifiable
information) or protected (e.g., race, gender) attributes. In this paper, we
study behavioral markers of daily mood using a recent dataset of mobile
behaviors from adolescent populations at high risk of suicidal behaviors. Using
computational models, we find that language and multimodal representations of
mobile typed text (spanning typed characters, words, keystroke timings, and app
usage) are predictive of daily mood. However, we find that models trained to
predict mood often also capture private user identities in their intermediate
representations. To tackle this problem, we evaluate approaches that obfuscate
user identity while remaining predictive. By combining multimodal
representations with privacy-preserving learning, we are able to push forward
the performance-privacy frontier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Terrance Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_A/0/1/0/all/0/1"&gt;Anna Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muszynski_M/0/1/0/all/0/1"&gt;Michal Muszynski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ishii_R/0/1/0/all/0/1"&gt;Ryo Ishii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Allen_N/0/1/0/all/0/1"&gt;Nicholas Allen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Auerbach_R/0/1/0/all/0/1"&gt;Randy Auerbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brent_D/0/1/0/all/0/1"&gt;David Brent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1"&gt;Louis-Philippe Morency&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Transformer-based Cross-modal Fusion Model with Adversarial Training for VQA Challenge 2021. (arXiv:2106.13033v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.13033</id>
        <link href="http://arxiv.org/abs/2106.13033"/>
        <updated>2021-06-25T02:00:44.853Z</updated>
        <summary type="html"><![CDATA[In this paper, inspired by the successes of visionlanguage pre-trained models
and the benefits from training with adversarial attacks, we present a novel
transformerbased cross-modal fusion modeling by incorporating the both notions
for VQA challenge 2021. Specifically, the proposed model is on top of the
architecture of VinVL model [19], and the adversarial training strategy [4] is
applied to make the model robust and generalized. Moreover, two implementation
tricks are also used in our system to obtain better results. The experiments
demonstrate that the novel framework can achieve 76.72% on VQAv2 test-std set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1"&gt;Ke-Han Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1"&gt;Bo-Han Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kuan-Yu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Empirical Study of Transformers for Source Code. (arXiv:2010.07987v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07987</id>
        <link href="http://arxiv.org/abs/2010.07987"/>
        <updated>2021-06-25T02:00:44.846Z</updated>
        <summary type="html"><![CDATA[Initially developed for natural language processing (NLP), Transformers are
now widely used for source code processing, due to the format similarity
between source code and text. In contrast to natural language, source code is
strictly structured, i.e., it follows the syntax of the programming language.
Several recent works develop Transformer modifications for capturing syntactic
information in source code. The drawback of these works is that they do not
compare to each other and consider different tasks. In this work, we conduct a
thorough empirical study of the capabilities of Transformers to utilize
syntactic information in different tasks. We consider three tasks (code
completion, function naming and bug fixing) and re-implement different
syntax-capturing modifications in a unified framework. We show that
Transformers are able to make meaningful predictions based purely on syntactic
information and underline the best practices of taking the syntactic
information into account for improving the performance of the model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chirkova_N/0/1/0/all/0/1"&gt;Nadezhda Chirkova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Troshin_S/0/1/0/all/0/1"&gt;Sergey Troshin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UXLA: A Robust Unsupervised Data Augmentation Framework for {Zero-Resource} Cross-Lingual NLP. (arXiv:2004.13240v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.13240</id>
        <link href="http://arxiv.org/abs/2004.13240"/>
        <updated>2021-06-25T02:00:44.829Z</updated>
        <summary type="html"><![CDATA[Transfer learning has yielded state-of-the-art (SoTA) results in many
supervised NLP tasks. However, annotated data for every target task in every
target language is rare, especially for low-resource languages. We propose
UXLA, a novel unsupervised data augmentation framework for zero-resource
transfer learning scenarios. In particular, UXLA aims to solve cross-lingual
adaptation problems from a source language task distribution to an unknown
target language task distribution, assuming no training label in the target
language. At its core, UXLA performs simultaneous self-training with data
augmentation and unsupervised sample selection. To show its effectiveness, we
conduct extensive experiments on three diverse zero-resource cross-lingual
transfer tasks. UXLA achieves SoTA results in all the tasks, outperforming the
baselines by a good margin. With an in-depth framework dissection, we
demonstrate the cumulative contributions of different components to its
success.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1"&gt;M Saiful Bari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohiuddin_T/0/1/0/all/0/1"&gt;Tasnim Mohiuddin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1"&gt;Shafiq Joty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[QASR: QCRI Aljazeera Speech Resource -- A Large Scale Annotated Arabic Speech Corpus. (arXiv:2106.13000v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13000</id>
        <link href="http://arxiv.org/abs/2106.13000"/>
        <updated>2021-06-25T02:00:44.817Z</updated>
        <summary type="html"><![CDATA[We introduce the largest transcribed Arabic speech corpus, QASR, collected
from the broadcast domain. This multi-dialect speech dataset contains 2,000
hours of speech sampled at 16kHz crawled from Aljazeera news channel. The
dataset is released with lightly supervised transcriptions, aligned with the
audio segments. Unlike previous datasets, QASR contains linguistically
motivated segmentation, punctuation, speaker information among others. QASR is
suitable for training and evaluating speech recognition systems, acoustics-
and/or linguistics- based Arabic dialect identification, punctuation
restoration, speaker identification, speaker linking, and potentially other NLP
modules for spoken data. In addition to QASR transcription, we release a
dataset of 130M words to aid in designing and training a better language model.
We show that end-to-end automatic speech recognition trained on QASR reports a
competitive word error rate compared to the previous MGB-2 corpus. We report
baseline results for downstream natural language processing tasks such as named
entity recognition using speech transcript. We also report the first baseline
for Arabic punctuation restoration. We make the corpus available for the
research community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mubarak_H/0/1/0/all/0/1"&gt;Hamdy Mubarak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hussein_A/0/1/0/all/0/1"&gt;Amir Hussein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1"&gt;Shammur Absar Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1"&gt;Ahmed Ali&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explaining NLP Models via Minimal Contrastive Editing (MiCE). (arXiv:2012.13985v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13985</id>
        <link href="http://arxiv.org/abs/2012.13985"/>
        <updated>2021-06-25T02:00:44.796Z</updated>
        <summary type="html"><![CDATA[Humans have been shown to give contrastive explanations, which explain why an
observed event happened rather than some other counterfactual event (the
contrast case). Despite the influential role that contrastivity plays in how
humans explain, this property is largely missing from current methods for
explaining NLP models. We present Minimal Contrastive Editing (MiCE), a method
for producing contrastive explanations of model predictions in the form of
edits to inputs that change model outputs to the contrast case. Our experiments
across three tasks--binary sentiment classification, topic classification, and
multiple-choice question answering--show that MiCE is able to produce edits
that are not only contrastive, but also minimal and fluent, consistent with
human contrastive edits. We demonstrate how MiCE edits can be used for two use
cases in NLP system development--debugging incorrect model outputs and
uncovering dataset artifacts--and thereby illustrate that producing contrastive
explanations is a promising research direction for model interpretability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1"&gt;Alexis Ross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marasovic_A/0/1/0/all/0/1"&gt;Ana Marasovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peters_M/0/1/0/all/0/1"&gt;Matthew E. Peters&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Where are we in semantic concept extraction for Spoken Language Understanding?. (arXiv:2106.13045v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.13045</id>
        <link href="http://arxiv.org/abs/2106.13045"/>
        <updated>2021-06-25T02:00:44.789Z</updated>
        <summary type="html"><![CDATA[Spoken language understanding (SLU) topic has seen a lot of progress these
last three years, with the emergence of end-to-end neural approaches. Spoken
language understanding refers to natural language processing tasks related to
semantic extraction from speech signal, like named entity recognition from
speech or slot filling task in a context of human-machine dialogue.
Classically, SLU tasks were processed through a cascade approach that consists
in applying, firstly, an automatic speech recognition process, followed by a
natural language processing module applied to the automatic transcriptions.
These three last years, end-to-end neural approaches, based on deep neural
networks, have been proposed in order to directly extract the semantics from
speech signal, by using a single neural model. More recent works on
self-supervised training with unlabeled data open new perspectives in term of
performance for automatic speech recognition and natural language processing.
In this paper, we present a brief overview of the recent advances on the French
MEDIA benchmark dataset for SLU, with or without the use of additional data. We
also present our last results that significantly outperform the current
state-of-the-art with a Concept Error Rate (CER) of 11.2%, instead of 13.6% for
the last state-of-the-art system presented this year.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghannay_S/0/1/0/all/0/1"&gt;Sahar Ghannay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caubriere_A/0/1/0/all/0/1"&gt;Antoine Caubri&amp;#xe8;re&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mdhaffar_S/0/1/0/all/0/1"&gt;Salima Mdhaffar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laperriere_G/0/1/0/all/0/1"&gt;Ga&amp;#xeb;lle Laperri&amp;#xe8;re&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jabaian_B/0/1/0/all/0/1"&gt;Bassam Jabaian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Esteve_Y/0/1/0/all/0/1"&gt;Yannick Est&amp;#xe8;ve&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AIT-QA: Question Answering Dataset over Complex Tables in the Airline Industry. (arXiv:2106.12944v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12944</id>
        <link href="http://arxiv.org/abs/2106.12944"/>
        <updated>2021-06-25T02:00:44.771Z</updated>
        <summary type="html"><![CDATA[Recent advances in transformers have enabled Table Question Answering (Table
QA) systems to achieve high accuracy and SOTA results on open domain datasets
like WikiTableQuestions and WikiSQL. Such transformers are frequently
pre-trained on open-domain content such as Wikipedia, where they effectively
encode questions and corresponding tables from Wikipedia as seen in Table QA
dataset. However, web tables in Wikipedia are notably flat in their layout,
with the first row as the sole column header. The layout lends to a relational
view of tables where each row is a tuple. Whereas, tables in domain-specific
business or scientific documents often have a much more complex layout,
including hierarchical row and column headers, in addition to having
specialized vocabulary terms from that domain.

To address this problem, we introduce the domain-specific Table QA dataset
AIT-QA (Airline Industry Table QA). The dataset consists of 515 questions
authored by human annotators on 116 tables extracted from public U.S. SEC
filings (publicly available at: https://www.sec.gov/edgar.shtml) of major
airline companies for the fiscal years 2017-2019. We also provide annotations
pertaining to the nature of questions, marking those that require hierarchical
headers, domain-specific terminology, and paraphrased forms. Our zero-shot
baseline evaluation of three transformer-based SOTA Table QA methods - TaPAS
(end-to-end), TaBERT (semantic parsing-based), and RCI (row-column
encoding-based) - clearly exposes the limitation of these methods in this
practical setting, with the best accuracy at just 51.8\% (RCI). We also present
pragmatic table preprocessing steps used to pivot and project these complex
tables into a layout suitable for the SOTA Table QA models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Katsis_Y/0/1/0/all/0/1"&gt;Yannis Katsis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chemmengath_S/0/1/0/all/0/1"&gt;Saneem Chemmengath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vishwajeet Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bharadwaj_S/0/1/0/all/0/1"&gt;Samarth Bharadwaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Canim_M/0/1/0/all/0/1"&gt;Mustafa Canim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glass_M/0/1/0/all/0/1"&gt;Michael Glass&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1"&gt;Alfio Gliozzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1"&gt;Feifei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sen_J/0/1/0/all/0/1"&gt;Jaydeep Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sankaranarayanan_K/0/1/0/all/0/1"&gt;Karthik Sankaranarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1"&gt;Soumen Chakrabarti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Self-Identified Counseling Expertise in Online Support Forums. (arXiv:2106.12976v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12976</id>
        <link href="http://arxiv.org/abs/2106.12976"/>
        <updated>2021-06-25T02:00:44.764Z</updated>
        <summary type="html"><![CDATA[A growing number of people engage in online health forums, making it
important to understand the quality of the advice they receive. In this paper,
we explore the role of expertise in responses provided to help-seeking posts
regarding mental health. We study the differences between (1) interactions with
peers; and (2) interactions with self-identified mental health professionals.
First, we show that a classifier can distinguish between these two groups,
indicating that their language use does in fact differ. To understand this
difference, we perform several analyses addressing engagement aspects,
including whether their comments engage the support-seeker further as well as
linguistic aspects, such as dominant language and linguistic style matching.
Our work contributes toward the developing efforts of understanding how health
experts engage with health information- and support-seekers in social networks.
More broadly, it is a step toward a deeper understanding of the styles of
interactions that cultivate supportive engagement in online communities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lahnala_A/0/1/0/all/0/1"&gt;Allison Lahnala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yuntian Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welch_C/0/1/0/all/0/1"&gt;Charles Welch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kummerfeld_J/0/1/0/all/0/1"&gt;Jonathan K. Kummerfeld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_L/0/1/0/all/0/1"&gt;Lawrence An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Resnicow_K/0/1/0/all/0/1"&gt;Kenneth Resnicow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1"&gt;Rada Mihalcea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Rosas_V/0/1/0/all/0/1"&gt;Ver&amp;#xf3;nica P&amp;#xe9;rez-Rosas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Topic Segmentation of Meetings with BERT Embeddings. (arXiv:2106.12978v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12978</id>
        <link href="http://arxiv.org/abs/2106.12978"/>
        <updated>2021-06-25T02:00:44.700Z</updated>
        <summary type="html"><![CDATA[Topic segmentation of meetings is the task of dividing multi-person meeting
transcripts into topic blocks. Supervised approaches to the problem have proven
intractable due to the difficulties in collecting and accurately annotating
large datasets. In this paper we show how previous unsupervised topic
segmentation methods can be improved using pre-trained neural architectures. We
introduce an unsupervised approach based on BERT embeddings that achieves a
15.5% reduction in error rate over existing unsupervised approaches applied to
two popular datasets for meeting transcripts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Solbiati_A/0/1/0/all/0/1"&gt;Alessandro Solbiati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heffernan_K/0/1/0/all/0/1"&gt;Kevin Heffernan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Damaskinos_G/0/1/0/all/0/1"&gt;Georgios Damaskinos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poddar_S/0/1/0/all/0/1"&gt;Shivani Poddar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Modi_S/0/1/0/all/0/1"&gt;Shubham Modi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cali_J/0/1/0/all/0/1"&gt;Jacques Cali&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OKGIT: Open Knowledge Graph Link Prediction with Implicit Types. (arXiv:2106.12806v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12806</id>
        <link href="http://arxiv.org/abs/2106.12806"/>
        <updated>2021-06-25T02:00:44.690Z</updated>
        <summary type="html"><![CDATA[Open Knowledge Graphs (OpenKG) refer to a set of (head noun phrase, relation
phrase, tail noun phrase) triples such as (tesla, return to, new york)
extracted from a corpus using OpenIE tools. While OpenKGs are easy to bootstrap
for a domain, they are very sparse and far from being directly usable in an end
task. Therefore, the task of predicting new facts, i.e., link prediction,
becomes an important step while using these graphs in downstream tasks such as
text comprehension, question answering, and web search query recommendation.
Learning embeddings for OpenKGs is one approach for link prediction that has
received some attention lately. However, on careful examination, we found that
current OpenKG link prediction algorithms often predict noun phrases (NPs) with
incompatible types for given noun and relation phrases. We address this problem
in this work and propose OKGIT that improves OpenKG link prediction using novel
type compatibility score and type regularization. With extensive experiments on
multiple datasets, we show that the proposed method achieves state-of-the-art
performance while producing type compatible NPs in the link prediction task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chandrahas/0/1/0/all/0/1"&gt;Chandrahas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1"&gt;Partha Pratim Talukdar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilingual transfer of acoustic word embeddings improves when training on languages related to the target zero-resource language. (arXiv:2106.12834v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12834</id>
        <link href="http://arxiv.org/abs/2106.12834"/>
        <updated>2021-06-25T02:00:44.681Z</updated>
        <summary type="html"><![CDATA[Acoustic word embedding models map variable duration speech segments to fixed
dimensional vectors, enabling efficient speech search and discovery. Previous
work explored how embeddings can be obtained in zero-resource settings where no
labelled data is available in the target language. The current best approach
uses transfer learning: a single supervised multilingual model is trained using
labelled data from multiple well-resourced languages and then applied to a
target zero-resource language (without fine-tuning). However, it is still
unclear how the specific choice of training languages affect downstream
performance. Concretely, here we ask whether it is beneficial to use training
languages related to the target. Using data from eleven languages spoken in
Southern Africa, we experiment with adding data from different language
families while controlling for the amount of data per language. In word
discrimination and query-by-example search evaluations, we show that training
on languages from the same family gives large improvements. Through
finer-grained analysis, we show that training on even just a single related
language gives the largest gain. We also find that adding data from unrelated
languages generally doesn't hurt performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jacobs_C/0/1/0/all/0/1"&gt;Christiaan Jacobs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamper_H/0/1/0/all/0/1"&gt;Herman Kamper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Influence of Machine Translation on Language Origin Obfuscation. (arXiv:2106.12830v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12830</id>
        <link href="http://arxiv.org/abs/2106.12830"/>
        <updated>2021-06-25T02:00:44.648Z</updated>
        <summary type="html"><![CDATA[In the last decade, machine translation has become a popular means to deal
with multilingual digital content. By providing higher quality translations,
obfuscating the source language of a text becomes more attractive. In this
paper, we analyze the ability to detect the source language from the translated
output of two widely used commercial machine translation systems by utilizing
machine-learning algorithms with basic textual features like n-grams.
Evaluations show that the source language can be reconstructed with high
accuracy for documents that contain a sufficient amount of translated text. In
addition, we analyze how the document size influences the performance of the
prediction, as well as how limiting the set of possible source languages
improves the classification accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Murauer_B/0/1/0/all/0/1"&gt;Benjamin Murauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tschuggnall_M/0/1/0/all/0/1"&gt;Michael Tschuggnall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Specht_G/0/1/0/all/0/1"&gt;G&amp;#xfc;nther Specht&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Representation Models for Text Classification with AutoML Tools. (arXiv:2106.12798v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12798</id>
        <link href="http://arxiv.org/abs/2106.12798"/>
        <updated>2021-06-25T02:00:44.639Z</updated>
        <summary type="html"><![CDATA[Automated Machine Learning (AutoML) has gained increasing success on tabular
data in recent years. However, processing unstructured data like text is a
challenge and not widely supported by open-source AutoML tools. This work
compares three manually created text representations and text embeddings
automatically created by AutoML tools. Our benchmark includes four popular
open-source AutoML tools and eight datasets for text classification purposes.
The results show that straightforward text representations perform better than
AutoML tools with automatically created text embeddings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brandle_S/0/1/0/all/0/1"&gt;Sebastian Br&amp;#xe4;ndle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanussek_M/0/1/0/all/0/1"&gt;Marc Hanussek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blohm_M/0/1/0/all/0/1"&gt;Matthias Blohm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kintz_M/0/1/0/all/0/1"&gt;Maximilien Kintz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovering novel drug-supplement interactions using a dietary supplements knowledge graph generated from the biomedical literature. (arXiv:2106.12741v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.12741</id>
        <link href="http://arxiv.org/abs/2106.12741"/>
        <updated>2021-06-25T02:00:44.619Z</updated>
        <summary type="html"><![CDATA[OBJECTIVE: Leverage existing biomedical NLP tools and DS domain terminology
to produce a novel and comprehensive knowledge graph containing dietary
supplement (DS) information for discovering interactions between DS and drugs,
or Drug-Supplement Interactions (DSI). MATERIALS AND METHODS: We created
SemRepDS (an extension of SemRep), capable of extracting semantic relations
from abstracts by leveraging a DS-specific terminology (iDISK) containing
28,884 DS terms not found in the UMLS. PubMed abstracts were processed using
SemRepDS to generate semantic relations, which were then filtered using a
PubMedBERT-based model to remove incorrect relations before generating our
knowledge graph (SuppKG). Two pathways are used to identify potential DS-Drug
interactions which are then evaluated by medical professionals for mechanistic
plausibility. RESULTS: Comparison analysis found that SemRepDS returned 206.9%
more DS relations and 158.5% more DS entities than SemRep. The fine-tuned BERT
model obtained an F1 score of 0.8605 and removed 43.86% of the relations,
improving the precision of the relations by 26.4% compared to pre-filtering.
SuppKG consists of 2,928 DS-specific nodes. Manual review of findings
identified 44 (88%) proposed DS-Gene-Drug and 32 (64%) proposed
DS-Gene1-Function-Gene2-Drug pathways to be mechanistically plausible.
DISCUSSION: The additional relations extracted using SemRepDS generated SuppKG
that was used to find plausible DSI not found in the current literature. By the
nature of the SuppKG, these interactions are unlikely to have been found using
SemRep without the expanded DS terminology. CONCLUSION: We successfully extend
SemRep to include DS information and produce SuppKG which can be used to find
potential DS-Drug interactions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schutte_D/0/1/0/all/0/1"&gt;Dalton Schutte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasilakes_J/0/1/0/all/0/1"&gt;Jake Vasilakes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bompelli_A/0/1/0/all/0/1"&gt;Anu Bompelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuqi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fiszman_M/0/1/0/all/0/1"&gt;Marcelo Fiszman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hua Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kilicoglu_H/0/1/0/all/0/1"&gt;Halil Kilicoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bishop_J/0/1/0/all/0/1"&gt;Jeffrey R. Bishop&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adam_T/0/1/0/all/0/1"&gt;Terrence Adam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Diagnostic Label Correlation for Automatic ICD Coding. (arXiv:2106.12800v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12800</id>
        <link href="http://arxiv.org/abs/2106.12800"/>
        <updated>2021-06-25T02:00:44.598Z</updated>
        <summary type="html"><![CDATA[Given the clinical notes written in electronic health records (EHRs), it is
challenging to predict the diagnostic codes which is formulated as a
multi-label classification task. The large set of labels, the hierarchical
dependency, and the imbalanced data make this prediction task extremely hard.
Most existing work built a binary prediction for each label independently,
ignoring the dependencies between labels. To address this problem, we propose a
two-stage framework to improve automatic ICD coding by capturing the label
correlation. Specifically, we train a label set distribution estimator to
rescore the probability of each label set candidate generated by a base
predictor. This paper is the first attempt at learning the label set
distribution as a reranking module for medical code prediction. In the
experiments, our proposed framework is able to improve upon best-performing
predictors on the benchmark MIMIC datasets. The source code of this project is
available at https://github.com/MiuLab/ICD-Correlation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_S/0/1/0/all/0/1"&gt;Shang-Chi Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chao-Wei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yun-Nung Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dealing with training and test segmentation mismatch: FBK@IWSLT2021. (arXiv:2106.12607v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12607</id>
        <link href="http://arxiv.org/abs/2106.12607"/>
        <updated>2021-06-25T02:00:44.574Z</updated>
        <summary type="html"><![CDATA[This paper describes FBK's system submission to the IWSLT 2021 Offline Speech
Translation task. We participated with a direct model, which is a
Transformer-based architecture trained to translate English speech audio data
into German texts. The training pipeline is characterized by knowledge
distillation and a two-step fine-tuning procedure. Both knowledge distillation
and the first fine-tuning step are carried out on manually segmented real and
synthetic data, the latter being generated with an MT system trained on the
available corpora. Differently, the second fine-tuning step is carried out on a
random segmentation of the MuST-C v2 En-De dataset. Its main goal is to reduce
the performance drops occurring when a speech translation model trained on
manually segmented data (i.e. an ideal, sentence-like segmentation) is
evaluated on automatically segmented audio (i.e. actual, more realistic testing
conditions). For the same purpose, a custom hybrid segmentation procedure that
accounts for both audio content (pauses) and for the length of the produced
segments is applied to the test data before passing them to the system. At
inference time, we compared this procedure with a baseline segmentation method
based on Voice Activity Detection (VAD). Our results indicate the effectiveness
of the proposed hybrid approach, shown by a reduction of the gap with manual
segmentation from 8.3 to 1.4 BLEU points.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1"&gt;Sara Papi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1"&gt;Marco Gaido&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1"&gt;Matteo Negri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1"&gt;Marco Turchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TagRuler: Interactive Tool for Span-Level Data Programming by Demonstration. (arXiv:2106.12767v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12767</id>
        <link href="http://arxiv.org/abs/2106.12767"/>
        <updated>2021-06-25T02:00:44.536Z</updated>
        <summary type="html"><![CDATA[Despite rapid developments in the field of machine learning research,
collecting high-quality labels for supervised learning remains a bottleneck for
many applications. This difficulty is exacerbated by the fact that
state-of-the-art models for NLP tasks are becoming deeper and more complex,
often increasing the amount of training data required even for fine-tuning.
Weak supervision methods, including data programming, address this problem and
reduce the cost of label collection by using noisy label sources for
supervision. However, until recently, data programming was only accessible to
users who knew how to program. To bridge this gap, the Data Programming by
Demonstration framework was proposed to facilitate the automatic creation of
labeling functions based on a few examples labeled by a domain expert. This
framework has proven successful for generating high-accuracy labeling models
for document classification. In this work, we extend the DPBD framework to
span-level annotation tasks, arguably one of the most time-consuming NLP
labeling tasks. We built a novel tool, TagRuler, that makes it easy for
annotators to build span-level labeling functions without programming and
encourages them to explore trade-offs between different labeling models and
active learning strategies. We empirically demonstrated that an annotator could
achieve a higher F1 score using the proposed tool compared to manual labeling
for different span-level annotation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1"&gt;Dongjin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Evensen_S/0/1/0/all/0/1"&gt;Sara Evensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demiralp_C/0/1/0/all/0/1"&gt;&amp;#xc7;a&amp;#x11f;atay Demiralp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hruschka_E/0/1/0/all/0/1"&gt;Estevam Hruschka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Automated Knowledge Mining and Document Classification System with Multi-model Transfer Learning. (arXiv:2106.12744v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12744</id>
        <link href="http://arxiv.org/abs/2106.12744"/>
        <updated>2021-06-25T02:00:44.512Z</updated>
        <summary type="html"><![CDATA[Service manual documents are crucial to the engineering company as they
provide guidelines and knowledge to service engineers. However, it has become
inconvenient and inefficient for service engineers to retrieve specific
knowledge from documents due to the complexity of resources. In this research,
we propose an automated knowledge mining and document classification system
with novel multi-model transfer learning approaches. Particularly, the
classification performance of the system has been improved with three effective
techniques: fine-tuning, pruning, and multi-model method. The fine-tuning
technique optimizes a pre-trained BERT model by adding a feed-forward neural
network layer and the pruning technique is used to retrain the BERT model with
new data. The multi-model method initializes and trains multiple BERT models to
overcome the randomness of data ordering during the fine-tuning process. In the
first iteration of the training process, multiple BERT models are being trained
simultaneously. The best model is then selected for the next phase of the
training process with another two iterations and the training processes for
other BERT models will be terminated. The performance of the proposed system
has been evaluated by comparing with two robust baseline methods, BERT and
BERT-CNN. Experimental results on a widely used Corpus of Linguistic
Acceptability (CoLA) dataset have shown that the proposed techniques perform
better than these baseline methods in terms of accuracy and MCC score.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chong_J/0/1/0/all/0/1"&gt;Jia Wei Chong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhiyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_M/0/1/0/all/0/1"&gt;Mei Shin Oh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A comprehensive empirical analysis on cross-domain semantic enrichment for detection of depressive language. (arXiv:2106.12797v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12797</id>
        <link href="http://arxiv.org/abs/2106.12797"/>
        <updated>2021-06-25T02:00:44.500Z</updated>
        <summary type="html"><![CDATA[We analyze the process of creating word embedding feature representations
designed for a learning task when annotated data is scarce, for example, in
depressive language detection from Tweets. We start with a rich word embedding
pre-trained from a large general dataset, which is then augmented with
embeddings learned from a much smaller and more specific domain dataset through
a simple non-linear mapping mechanism. We also experimented with several other
more sophisticated methods of such mapping including, several auto-encoder
based and custom loss-function based methods that learn embedding
representations through gradually learning to be close to the words of similar
semantics and distant to dissimilar semantics. Our strengthened representations
better capture the semantics of the depression domain, as it combines the
semantics learned from the specific domain coupled with word coverage from the
general language. We also present a comparative performance analyses of our
word embedding representations with a simple bag-of-words model, well known
sentiment and psycholinguistic lexicons, and a general pre-trained word
embedding. When used as feature representations for several different machine
learning methods, including deep learning models in a depressive Tweets
identification task, we show that our augmented word embedding representations
achieve a significantly better F1 score than the others, specially when applied
to a high quality dataset. Also, we present several data ablation tests which
confirm the efficacy of our augmentation techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farruque_N/0/1/0/all/0/1"&gt;Nawshad Farruque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1"&gt;Randy Goebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1"&gt;Osmar Zaiane&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparative Error Analysis in Neural and Finite-state Models for Unsupervised Character-level Transduction. (arXiv:2106.12698v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12698</id>
        <link href="http://arxiv.org/abs/2106.12698"/>
        <updated>2021-06-25T02:00:44.478Z</updated>
        <summary type="html"><![CDATA[Traditionally, character-level transduction problems have been solved with
finite-state models designed to encode structural and linguistic knowledge of
the underlying process, whereas recent approaches rely on the power and
flexibility of sequence-to-sequence models with attention. Focusing on the less
explored unsupervised learning scenario, we compare the two model classes side
by side and find that they tend to make different types of errors even when
achieving comparable performance. We analyze the distributions of different
error classes using two unsupervised tasks as testbeds: converting informally
romanized text into the native script of its language (for Russian, Arabic, and
Kannada) and translating between a pair of closely related languages (Serbian
and Bosnian). Finally, we investigate how combining finite-state and
sequence-to-sequence models at decoding time affects the output quantitatively
and qualitatively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ryskina_M/0/1/0/all/0/1"&gt;Maria Ryskina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1"&gt;Eduard Hovy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1"&gt;Taylor Berg-Kirkpatrick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gormley_M/0/1/0/all/0/1"&gt;Matthew R. Gormley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bidding via Clustering Ads Intentions: an Efficient Search Engine Marketing System for E-commerce. (arXiv:2106.12700v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12700</id>
        <link href="http://arxiv.org/abs/2106.12700"/>
        <updated>2021-06-25T02:00:44.455Z</updated>
        <summary type="html"><![CDATA[With the increasing scale of search engine marketing, designing an efficient
bidding system is becoming paramount for the success of e-commerce companies.
The critical challenges faced by a modern industrial-level bidding system
include: 1. the catalog is enormous, and the relevant bidding features are of
high sparsity; 2. the large volume of bidding requests induces significant
computation burden to both the offline and online serving. Leveraging
extraneous user-item information proves essential to mitigate the sparsity
issue, for which we exploit the natural language signals from the users' query
and the contextual knowledge from the products. In particular, we extract the
vector representations of ads via the Transformer model and leverage their
geometric relation to building collaborative bidding predictions via
clustering. The two-step procedure also significantly reduces the computation
stress of bid evaluation and optimization. In this paper, we introduce the
end-to-end structure of the bidding system for search engine marketing for
Walmart e-commerce, which successfully handles tens of millions of bids each
day. We analyze the online and offline performances of our approach and discuss
how we find it as a production-efficient solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jie_C/0/1/0/all/0/1"&gt;Cheng Jie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Da Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zigeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1"&gt;Wei Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Charformer: Fast Character Transformers via Gradient-based Subword Tokenization. (arXiv:2106.12672v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12672</id>
        <link href="http://arxiv.org/abs/2106.12672"/>
        <updated>2021-06-25T02:00:44.409Z</updated>
        <summary type="html"><![CDATA[State-of-the-art models in natural language processing rely on separate rigid
subword tokenization algorithms, which limit their generalization ability and
adaptation to new settings. In this paper, we propose a new model inductive
bias that learns a subword tokenization end-to-end as part of the model. To
this end, we introduce a soft gradient-based subword tokenization module (GBST)
that automatically learns latent subword representations from characters in a
data-driven fashion. Concretely, GBST enumerates candidate subword blocks and
learns to score them in a position-wise fashion using a block scoring network.
We additionally introduce Charformer, a deep Transformer model that integrates
GBST and operates on the byte level. Via extensive experiments on English GLUE,
multilingual, and noisy text datasets, we show that Charformer outperforms a
series of competitive byte-level baselines while generally performing on par
and sometimes outperforming subword-based models. Additionally, Charformer is
fast, improving the speed of both vanilla byte-level and subword-level
Transformers by 28%-100% while maintaining competitive quality. We believe this
work paves the way for highly performant token-free models that are trained
completely end-to-end.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1"&gt;Yi Tay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1"&gt;Vinh Q. Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1"&gt;Sebastian Ruder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1"&gt;Jai Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1"&gt;Hyung Won Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1"&gt;Dara Bahri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1"&gt;Zhen Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baumgartner_S/0/1/0/all/0/1"&gt;Simon Baumgartner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Cong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1"&gt;Donald Metzler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clinical Named Entity Recognition using Contextualized Token Representations. (arXiv:2106.12608v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12608</id>
        <link href="http://arxiv.org/abs/2106.12608"/>
        <updated>2021-06-25T02:00:44.401Z</updated>
        <summary type="html"><![CDATA[The clinical named entity recognition (CNER) task seeks to locate and
classify clinical terminologies into predefined categories, such as diagnostic
procedure, disease disorder, severity, medication, medication dosage, and sign
symptom. CNER facilitates the study of side-effect on medications including
identification of novel phenomena and human-focused information extraction.
Existing approaches in extracting the entities of interests focus on using
static word embeddings to represent each word. However, one word can have
different interpretations that depend on the context of the sentences.
Evidently, static word embeddings are insufficient to integrate the diverse
interpretation of a word. To overcome this challenge, the technique of
contextualized word embedding has been introduced to better capture the
semantic meaning of each word based on its context. Two of these language
models, ELMo and Flair, have been widely used in the field of Natural Language
Processing to generate the contextualized word embeddings on domain-generic
documents. However, these embeddings are usually too general to capture the
proximity among vocabularies of specific domains. To facilitate various
downstream applications using clinical case reports (CCRs), we pre-train two
deep contextualized language models, Clinical Embeddings from Language Model
(C-ELMo) and Clinical Contextual String Embeddings (C-Flair) using the
clinical-related corpus from the PubMed Central. Explicit experiments show that
our models gain dramatic improvements compared to both static word embeddings
and domain-generic language models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yichao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ju_C/0/1/0/all/0/1"&gt;Chelsea Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caufield_J/0/1/0/all/0/1"&gt;J. Harry Caufield&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shih_K/0/1/0/all/0/1"&gt;Kevin Shih&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Calvin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yizhou Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ping_P/0/1/0/all/0/1"&gt;Peipei Ping&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RikoNet: A Novel Anime Recommendation Engine. (arXiv:2106.12970v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.12970</id>
        <link href="http://arxiv.org/abs/2106.12970"/>
        <updated>2021-06-25T02:00:44.393Z</updated>
        <summary type="html"><![CDATA[Anime is quite well-received today, especially among the younger generations.
With many genres of available shows, more and more people are increasingly
getting attracted to this niche section of the entertainment industry. As anime
has recently garnered mainstream attention, we have insufficient information
regarding users' penchant and watching habits. Therefore, it is an uphill task
to build a recommendation engine for this relatively obscure entertainment
medium. In this attempt, we have built a novel hybrid recommendation system
that could act both as a recommendation system and as a means of exploring new
anime genres and titles. We have analyzed the general trends in this field and
the users' watching habits for coming up with our efficacious solution. Our
solution employs deep autoencoders for the tasks of predicting ratings and
generating embeddings. Following this, we formed clusters using the embeddings
of the anime titles. These clusters form the search space for anime with
similarities and are used to find anime similar to the ones liked and disliked
by the user. This method, combined with the predicted ratings, forms the novel
hybrid filter. In this article, we have demonstrated this idea and compared the
performance of our implemented model with the existing state-of-the-art
techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soni_B/0/1/0/all/0/1"&gt;Badal Soni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thakuria_D/0/1/0/all/0/1"&gt;Debangan Thakuria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nath_N/0/1/0/all/0/1"&gt;Nilutpal Nath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_N/0/1/0/all/0/1"&gt;Navarun Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boro_B/0/1/0/all/0/1"&gt;Bhaskarananda Boro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextualized Attention-based Knowledge Transfer for Spoken Conversational Question Answering. (arXiv:2010.11066v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11066</id>
        <link href="http://arxiv.org/abs/2010.11066"/>
        <updated>2021-06-25T02:00:44.372Z</updated>
        <summary type="html"><![CDATA[Spoken conversational question answering (SCQA) requires machines to model
complex dialogue flow given the speech utterances and text corpora. Different
from traditional text question answering (QA) tasks, SCQA involves audio signal
processing, passage comprehension, and contextual understanding. However, ASR
systems introduce unexpected noisy signals to the transcriptions, which result
in performance degradation on SCQA. To overcome the problem, we propose CADNet,
a novel contextualized attention-based distillation approach, which applies
both cross-attention and self-attention to obtain ASR-robust contextualized
embedding representations of the passage and dialogue history for performance
improvements. We also introduce the spoken conventional knowledge distillation
framework to distill the ASR-robust knowledge from the estimated probabilities
of the teacher model to the student. We conduct extensive experiments on the
Spoken-CoQA dataset and demonstrate that our approach achieves remarkable
performance in this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenyu You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection, Analysis, and Prediction of Research Topics with Scientific Knowledge Graphs. (arXiv:2106.12875v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2106.12875</id>
        <link href="http://arxiv.org/abs/2106.12875"/>
        <updated>2021-06-25T02:00:44.353Z</updated>
        <summary type="html"><![CDATA[Analysing research trends and predicting their impact on academia and
industry is crucial to gain a deeper understanding of the advances in a
research field and to inform critical decisions about research funding and
technology adoption. In the last years, we saw the emergence of several
publicly-available and large-scale Scientific Knowledge Graphs fostering the
development of many data-driven approaches for performing quantitative analyses
of research trends. This chapter presents an innovative framework for
detecting, analysing, and forecasting research topics based on a large-scale
knowledge graph characterising research articles according to the research
topics from the Computer Science Ontology. We discuss the advantages of a
solution based on a formal representation of topics and describe how it was
applied to produce bibliometric studies and innovative tools for analysing and
predicting research dynamics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Salatino_A/0/1/0/all/0/1"&gt;Angelo Salatino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannocci_A/0/1/0/all/0/1"&gt;Andrea Mannocci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osborne_F/0/1/0/all/0/1"&gt;Francesco Osborne&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Approach to Discover Switch Behaviours in Process Mining. (arXiv:2106.12765v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2106.12765</id>
        <link href="http://arxiv.org/abs/2106.12765"/>
        <updated>2021-06-25T02:00:44.307Z</updated>
        <summary type="html"><![CDATA[Process mining is a relatively new subject which builds a bridge between
process modelling and data mining. An exclusive choice in a process model
usually splits the process into different branches. However, in some processes,
it is possible to switch from one branch to another. The inductive miner
guarantees to return sound process models, but fails to return a precise model
when there are switch behaviours between different exclusive choice branches
due to the limitation of process trees. In this paper, we present a novel
extension to the process tree model to support switch behaviours between
different branches of the exclusive choice operator and propose a novel
extension to the inductive miner to discover sound process models with switch
behaviours. The proposed discovery technique utilizes the theory of a previous
study to detect possible switch behaviours. We apply both artificial and
publicly-available datasets to evaluate our approach. Our results show that our
approach can improve the precision of discovered models by 36% while
maintaining high fitness values compared to the original inductive miner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qifan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poon_S/0/1/0/all/0/1"&gt;Simon Poon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pattern-based Visualization of Knowledge Graphs. (arXiv:2106.12857v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2106.12857</id>
        <link href="http://arxiv.org/abs/2106.12857"/>
        <updated>2021-06-25T02:00:44.277Z</updated>
        <summary type="html"><![CDATA[We present a novel approach to knowledge graph visualization based on
ontology design patterns. This approach relies on OPLa (Ontology Pattern
Language) annotations and on a catalogue of visual frames, which are associated
with foundational ontology design patterns. We demonstrate that this approach
significantly reduces the cognitive load required to users for visualizing and
interpreting a knowledge graph and guides the user in exploring it through
meaningful thematic paths provided by ontology patterns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Asprino_L/0/1/0/all/0/1"&gt;Luigi Asprino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colonna_C/0/1/0/all/0/1"&gt;Christian Colonna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mongiovi_M/0/1/0/all/0/1"&gt;Misael Mongiov&amp;#xec;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Porena_M/0/1/0/all/0/1"&gt;Margherita Porena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Presutti_V/0/1/0/all/0/1"&gt;Valentina Presutti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Automated Knowledge Mining and Document Classification System with Multi-model Transfer Learning. (arXiv:2106.12744v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12744</id>
        <link href="http://arxiv.org/abs/2106.12744"/>
        <updated>2021-06-25T02:00:44.245Z</updated>
        <summary type="html"><![CDATA[Service manual documents are crucial to the engineering company as they
provide guidelines and knowledge to service engineers. However, it has become
inconvenient and inefficient for service engineers to retrieve specific
knowledge from documents due to the complexity of resources. In this research,
we propose an automated knowledge mining and document classification system
with novel multi-model transfer learning approaches. Particularly, the
classification performance of the system has been improved with three effective
techniques: fine-tuning, pruning, and multi-model method. The fine-tuning
technique optimizes a pre-trained BERT model by adding a feed-forward neural
network layer and the pruning technique is used to retrain the BERT model with
new data. The multi-model method initializes and trains multiple BERT models to
overcome the randomness of data ordering during the fine-tuning process. In the
first iteration of the training process, multiple BERT models are being trained
simultaneously. The best model is then selected for the next phase of the
training process with another two iterations and the training processes for
other BERT models will be terminated. The performance of the proposed system
has been evaluated by comparing with two robust baseline methods, BERT and
BERT-CNN. Experimental results on a widely used Corpus of Linguistic
Acceptability (CoLA) dataset have shown that the proposed techniques perform
better than these baseline methods in terms of accuracy and MCC score.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chong_J/0/1/0/all/0/1"&gt;Jia Wei Chong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhiyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_M/0/1/0/all/0/1"&gt;Mei Shin Oh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovering novel drug-supplement interactions using a dietary supplements knowledge graph generated from the biomedical literature. (arXiv:2106.12741v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.12741</id>
        <link href="http://arxiv.org/abs/2106.12741"/>
        <updated>2021-06-25T02:00:44.227Z</updated>
        <summary type="html"><![CDATA[OBJECTIVE: Leverage existing biomedical NLP tools and DS domain terminology
to produce a novel and comprehensive knowledge graph containing dietary
supplement (DS) information for discovering interactions between DS and drugs,
or Drug-Supplement Interactions (DSI). MATERIALS AND METHODS: We created
SemRepDS (an extension of SemRep), capable of extracting semantic relations
from abstracts by leveraging a DS-specific terminology (iDISK) containing
28,884 DS terms not found in the UMLS. PubMed abstracts were processed using
SemRepDS to generate semantic relations, which were then filtered using a
PubMedBERT-based model to remove incorrect relations before generating our
knowledge graph (SuppKG). Two pathways are used to identify potential DS-Drug
interactions which are then evaluated by medical professionals for mechanistic
plausibility. RESULTS: Comparison analysis found that SemRepDS returned 206.9%
more DS relations and 158.5% more DS entities than SemRep. The fine-tuned BERT
model obtained an F1 score of 0.8605 and removed 43.86% of the relations,
improving the precision of the relations by 26.4% compared to pre-filtering.
SuppKG consists of 2,928 DS-specific nodes. Manual review of findings
identified 44 (88%) proposed DS-Gene-Drug and 32 (64%) proposed
DS-Gene1-Function-Gene2-Drug pathways to be mechanistically plausible.
DISCUSSION: The additional relations extracted using SemRepDS generated SuppKG
that was used to find plausible DSI not found in the current literature. By the
nature of the SuppKG, these interactions are unlikely to have been found using
SemRep without the expanded DS terminology. CONCLUSION: We successfully extend
SemRep to include DS information and produce SuppKG which can be used to find
potential DS-Drug interactions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schutte_D/0/1/0/all/0/1"&gt;Dalton Schutte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasilakes_J/0/1/0/all/0/1"&gt;Jake Vasilakes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bompelli_A/0/1/0/all/0/1"&gt;Anu Bompelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuqi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fiszman_M/0/1/0/all/0/1"&gt;Marcelo Fiszman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hua Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kilicoglu_H/0/1/0/all/0/1"&gt;Halil Kilicoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bishop_J/0/1/0/all/0/1"&gt;Jeffrey R. Bishop&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adam_T/0/1/0/all/0/1"&gt;Terrence Adam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extreme Multi-label Learning for Semantic Matching in Product Search. (arXiv:2106.12657v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.12657</id>
        <link href="http://arxiv.org/abs/2106.12657"/>
        <updated>2021-06-25T02:00:44.209Z</updated>
        <summary type="html"><![CDATA[We consider the problem of semantic matching in product search: given a
customer query, retrieve all semantically related products from a huge catalog
of size 100 million, or more. Because of large catalog spaces and real-time
latency constraints, semantic matching algorithms not only desire high recall
but also need to have low latency. Conventional lexical matching approaches
(e.g., Okapi-BM25) exploit inverted indices to achieve fast inference time, but
fail to capture behavioral signals between queries and products. In contrast,
embedding-based models learn semantic representations from customer behavior
data, but the performance is often limited by shallow neural encoders due to
latency constraints. Semantic product search can be viewed as an eXtreme
Multi-label Classification (XMC) problem, where customer queries are input
instances and products are output labels. In this paper, we aim to improve
semantic product search by using tree-based XMC models where inference time
complexity is logarithmic in the number of products. We consider hierarchical
linear models with n-gram features for fast real-time inference.
Quantitatively, our method maintains a low latency of 1.25 milliseconds per
query and achieves a 65% improvement of Recall@100 (60.9% v.s. 36.8%) over a
competing embedding-based DSSM model. Our model is robust to weight pruning
with varying thresholds, which can flexibly meet different system requirements
for online deployments. Qualitatively, our method can retrieve products that
are complementary to existing product search system and add diversity to the
match set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_W/0/1/0/all/0/1"&gt;Wei-Cheng Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1"&gt;Daniel Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hsiang-Fu Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teo_C/0/1/0/all/0/1"&gt;Choon-Hui Teo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_K/0/1/0/all/0/1"&gt;Kai Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolluri_K/0/1/0/all/0/1"&gt;Kedarnath Kolluri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1"&gt;Qie Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shandilya_N/0/1/0/all/0/1"&gt;Nikhil Shandilya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ievgrafov_V/0/1/0/all/0/1"&gt;Vyacheslav Ievgrafov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1"&gt;Japinder Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1"&gt;Inderjit S. Dhillon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Stereotyping Problem in Collaboratively Filtered Recommender Systems. (arXiv:2106.12622v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.12622</id>
        <link href="http://arxiv.org/abs/2106.12622"/>
        <updated>2021-06-25T02:00:44.164Z</updated>
        <summary type="html"><![CDATA[Recommender systems -- and especially matrix factorization-based
collaborative filtering algorithms -- play a crucial role in mediating our
access to online information. We show that such algorithms induce a particular
kind of stereotyping: if preferences for a \textit{set} of items are
anti-correlated in the general user population, then those items may not be
recommended together to a user, regardless of that user's preferences and
ratings history. First, we introduce a notion of \textit{joint accessibility},
which measures the extent to which a set of items can jointly be accessed by
users. We then study joint accessibility under the standard factorization-based
collaborative filtering framework, and provide theoretical necessary and
sufficient conditions when joint accessibility is violated. Moreover, we show
that these conditions can easily be violated when the users are represented by
a single feature vector. To improve joint accessibility, we further propose an
alternative modelling fix, which is designed to capture the diverse multiple
interests of each user using a multi-vector representation. We conduct
extensive experiments on real and simulated datasets, demonstrating the
stereotyping problem with standard single-vector matrix factorization models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1"&gt;Wenshuo Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krauth_K/0/1/0/all/0/1"&gt;Karl Krauth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garg_N/0/1/0/all/0/1"&gt;Nikhil Garg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging semantically similar queries for ranking via combining representations. (arXiv:2106.12621v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12621</id>
        <link href="http://arxiv.org/abs/2106.12621"/>
        <updated>2021-06-25T02:00:44.150Z</updated>
        <summary type="html"><![CDATA[In modern ranking problems, different and disparate representations of the
items to be ranked are often available. It is sensible, then, to try to combine
these representations to improve ranking. Indeed, learning to rank via
combining representations is both principled and practical for learning a
ranking function for a particular query. In extremely data-scarce settings,
however, the amount of labeled data available for a particular query can lead
to a highly variable and ineffective ranking function. One way to mitigate the
effect of the small amount of data is to leverage information from semantically
similar queries. Indeed, as we demonstrate in simulation settings and real data
examples, when semantically similar queries are available it is possible to
gainfully use them when ranking with respect to a particular query. We describe
and explore this phenomenon in the context of the bias-variance trade off and
apply it to the data-scarce settings of a Bing navigational graph and the
Drosophila larva connectome.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Helm_H/0/1/0/all/0/1"&gt;Hayden S. Helm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdin_M/0/1/0/all/0/1"&gt;Marah Abdin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedigo_B/0/1/0/all/0/1"&gt;Benjamin D. Pedigo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahajan_S/0/1/0/all/0/1"&gt;Shweti Mahajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyzinski_V/0/1/0/all/0/1"&gt;Vince Lyzinski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1"&gt;Youngser Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1"&gt;Amitabh Basu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhury_P/0/1/0/all/0/1"&gt;Piali~Choudhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1"&gt;Christopher M. White&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Weiwei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1"&gt;Carey E. Priebe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CTU Depth Decision Algorithms for HEVC: A Survey. (arXiv:2104.08328v2 [cs.MM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08328</id>
        <link href="http://arxiv.org/abs/2104.08328"/>
        <updated>2021-06-25T02:00:43.994Z</updated>
        <summary type="html"><![CDATA[High-Efficiency Video Coding (HEVC) surpasses its predecessors in encoding
efficiency by introducing new coding tools at the cost of an increased encoding
time-complexity. The Coding Tree Unit (CTU) is the main building block used in
HEVC. In the HEVC standard, frames are divided into CTUs with the predetermined
size of up to 64x64 pixels. Each CTU is then divided recursively into a number
of equally sized square areas, known as Coding Units (CUs). Although this
diversity of frame partitioning increases encoding efficiency, it also causes
an increase in the time complexity due to the increased number of ways to find
the optimal partitioning. To address this complexity, numerous algorithms have
been proposed to eliminate unnecessary searches during partitioning CTUs by
exploiting the correlation in the video. In this paper, existing CTU depth
decision algorithms for HEVC are surveyed. These algorithms are categorized
into two groups, namely statistics and machine learning approaches. Statistics
approaches are further subdivided into neighboring and inherent approaches.
Neighboring approaches exploit the similarity between adjacent CTUs to limit
the depth range of the current CTU, while inherent approaches use only the
available information within the current CTU. Machine learning approaches try
to extract and exploit similarities implicitly. Traditional methods like
support vector machines or random forests use manually selected features, while
recently proposed deep learning methods extract features during training.
Finally, this paper discusses extending these methods to more recent video
coding formats such as Versatile Video Coding (VVC) and AOMedia Video 1(AV1).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cetinkaya_E/0/1/0/all/0/1"&gt;Ekrem Cetinkaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amirpour_H/0/1/0/all/0/1"&gt;Hadi Amirpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghanbari_M/0/1/0/all/0/1"&gt;Mohammad Ghanbari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Timmerer_C/0/1/0/all/0/1"&gt;Christian Timmerer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ABCD: A Graph Framework to Convert Complex Sentences to a Covering Set of Simple Sentences. (arXiv:2106.12027v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12027</id>
        <link href="http://arxiv.org/abs/2106.12027"/>
        <updated>2021-06-24T01:51:45.769Z</updated>
        <summary type="html"><![CDATA[Atomic clauses are fundamental text units for understanding complex
sentences. Identifying the atomic sentences within complex sentences is
important for applications such as summarization, argument mining, discourse
analysis, discourse parsing, and question answering. Previous work mainly
relies on rule-based methods dependent on parsing. We propose a new task to
decompose each complex sentence into simple sentences derived from the tensed
clauses in the source, and a novel problem formulation as a graph edit task.
Our neural model learns to Accept, Break, Copy or Drop elements of a graph that
combines word adjacency and grammatical dependencies. The full processing
pipeline includes modules for graph construction, graph editing, and sentence
generation from the output graph. We introduce DeSSE, a new dataset designed to
train and evaluate complex sentence decomposition, and MinWiki, a subset of
MinWikiSplit. ABCD achieves comparable performance as two parsing baselines on
MinWiki. On DeSSE, which has a more even balance of complex sentence types, our
model achieves higher accuracy on the number of atomic sentences than an
encoder-decoder baseline. Results include a detailed error analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yanjun Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ting-hao/0/1/0/all/0/1"&gt;Ting-hao&lt;/a&gt; (Kenneth) &lt;a href="http://arxiv.org/find/cs/1/au:+Huang/0/1/0/all/0/1"&gt;Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Passonneau_R/0/1/0/all/0/1"&gt;Rebecca J. Passonneau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Well do Feature Visualizations Support Causal Understanding of CNN Activations?. (arXiv:2106.12447v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12447</id>
        <link href="http://arxiv.org/abs/2106.12447"/>
        <updated>2021-06-24T01:51:45.764Z</updated>
        <summary type="html"><![CDATA[One widely used approach towards understanding the inner workings of deep
convolutional neural networks is to visualize unit responses via activation
maximization. Feature visualizations via activation maximization are thought to
provide humans with precise information about the image features that cause a
unit to be activated. If this is indeed true, these synthetic images should
enable humans to predict the effect of an intervention, such as whether
occluding a certain patch of the image (say, a dog's head) changes a unit's
activation. Here, we test this hypothesis by asking humans to predict which of
two square occlusions causes a larger change to a unit's activation. Both a
large-scale crowdsourced experiment and measurements with experts show that on
average, the extremely activating feature visualizations by Olah et al. (2017)
indeed help humans on this task ($67 \pm 4\%$ accuracy; baseline performance
without any visualizations is $60 \pm 3\%$). However, they do not provide any
significant advantage over other visualizations (such as e.g. dataset samples),
which yield similar performance ($66 \pm 3\%$ to $67 \pm 3\%$ accuracy). Taken
together, we propose an objective psychophysical task to quantify the benefit
of unit-level interpretability methods for humans, and find no evidence that
feature visualizations provide humans with better "causal understanding" than
simple alternative visualizations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1"&gt;Roland S. Zimmermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borowski_J/0/1/0/all/0/1"&gt;Judy Borowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1"&gt;Robert Geirhos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1"&gt;Matthias Bethge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallis_T/0/1/0/all/0/1"&gt;Thomas S. A. Wallis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1"&gt;Wieland Brendel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Neurally-Guided Shape Parser: A Monte Carlo Method for Hierarchical Labeling of Over-segmented 3D Shapes. (arXiv:2106.12026v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12026</id>
        <link href="http://arxiv.org/abs/2106.12026"/>
        <updated>2021-06-24T01:51:45.758Z</updated>
        <summary type="html"><![CDATA[Many learning-based 3D shape semantic segmentation methods assign labels to
shape atoms (e.g. points in a point cloud or faces in a mesh) with a
single-pass approach trained in an end-to-end fashion. Such methods achieve
impressive performance but require large amounts of labeled training data. This
paradigm entangles two separable subproblems: (1) decomposing a shape into
regions and (2) assigning semantic labels to these regions. We claim that
disentangling these subproblems reduces the labeled data burden: (1) region
decomposition requires no semantic labels and could be performed in an
unsupervised fashion, and (2) labeling shape regions instead of atoms results
in a smaller search space and should be learnable with less labeled training
data. In this paper, we investigate this second claim by presenting the
Neurally-Guided Shape Parser (NGSP), a method that learns how to assign
semantic labels to regions of an over-segmented 3D shape. We solve this problem
via MAP inference, modeling the posterior probability of a labeling assignment
conditioned on an input shape. We employ a Monte Carlo importance sampling
approach guided by a neural proposal network, a search-based approach made
feasible by assuming the input shape is decomposed into discrete regions. We
evaluate NGSP on the task of hierarchical semantic segmentation on manufactured
3D shapes from PartNet. We find that NGSP delivers significant performance
improvements over baselines that learn to label shape atoms and then aggregate
predictions for each shape region, especially in low-data regimes. Finally, we
demonstrate that NGSP is robust to region granularity, as it maintains strong
segmentation performance even as the regions undergo significant corruption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1"&gt;R. Kenny Jones&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanocka_R/0/1/0/all/0/1"&gt;Rana Hanocka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1"&gt;Daniel Ritchie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stronger NAS with Weaker Predictors. (arXiv:2102.10490v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10490</id>
        <link href="http://arxiv.org/abs/2102.10490"/>
        <updated>2021-06-24T01:51:45.753Z</updated>
        <summary type="html"><![CDATA[Neural Architecture Search (NAS) often trains and evaluates a large number of
architectures. Recent predictor-based NAS approaches attempt to address such
heavy computation costs with two key steps: sampling some
architecture-performance pairs and fitting a proxy accuracy predictor. Given
limited samples, these predictors, however, are far from accurate to locate top
architectures due to the difficulty of fitting the huge search space. This
paper reflects on a simple yet crucial question: if our final goal is to find
the best architecture, do we really need to model the whole space well?. We
propose a paradigm shift from fitting the whole architecture space using one
strong predictor, to progressively fitting a search path towards the
high-performance sub-space through a set of weaker predictors. As a key
property of the proposed weak predictors, their probabilities of sampling
better architectures keep increasing. Hence we only sample a few well-performed
architectures guided by the previously learned predictor and estimate a new
better weak predictor. This embarrassingly easy framework produces
coarse-to-fine iteration to refine the ranking of sampling space gradually.
Extensive experiments demonstrate that our method costs fewer samples to find
top-performance architectures on NAS-Bench-101 and NAS-Bench-201, as well as
achieves the state-of-the-art ImageNet performance on the NASNet search space.
In particular, compared to state-of-the-art (SOTA) predictor-based NAS methods,
WeakNAS outperforms all of them with notable margins, e.g., requiring at least
7.5x less samples to find global optimal on NAS-Bench-101; and WeakNAS can also
absorb them for further performance boost. We further strike the new SOTA
result of 81.3% in the ImageNet MobileNet Search Space. The code is available
at https://github.com/VITA-Group/WeakNAS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Junru Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1"&gt;Xiyang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dongdong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yinpeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mengchen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Ye Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zicheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Lu Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random Effect Bandits. (arXiv:2106.12200v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12200</id>
        <link href="http://arxiv.org/abs/2106.12200"/>
        <updated>2021-06-24T01:51:45.748Z</updated>
        <summary type="html"><![CDATA[This paper studies regret minimization in multi-armed bandits, a classical
online learning problem. To develop more statistically-efficient algorithms, we
propose to use the assumption of a random-effect model. In this model, the mean
rewards of arms are drawn independently from an unknown distribution, whose
parameters we estimate. We provide an estimator of the arm means in this model
and also analyze its uncertainty. Based on these results, we design a UCB
algorithm, which we call ReUCB. We analyze ReUCB and prove a Bayes regret bound
on its $n$-round regret, which matches an existing lower bound. Our experiments
show that ReUCB can outperform Thompson sampling in various scenarios, without
assuming that the prior distribution of arm means is known.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1"&gt;Rong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1"&gt;Branislav Kveton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regret Bounds for Stochastic Shortest Path Problems with Linear Function Approximation. (arXiv:2105.01593v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01593</id>
        <link href="http://arxiv.org/abs/2105.01593"/>
        <updated>2021-06-24T01:51:45.741Z</updated>
        <summary type="html"><![CDATA[We propose two algorithms that use linear function approximation (LFA) for
stochastic shortest path (SSP) and bound their regret over $K$ episodes. When
all stationary policies are proper, our first algorithm obtains sublinear
regret ($K^{3/4}$), is computationally efficient, and uses stationary policies.
This is the first LFA algorithm with these three properties, to the best of our
knowledge. Our second algorithm improves the regret to $\sqrt{K}$ when the
feature vectors satisfy certain assumptions. Both algorithms are special cases
of a more general one, which has $\sqrt{K}$ regret for general features given
access to a certain computation oracle. These algorithms and regret bounds are
the first for SSP with function approximation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vial_D/0/1/0/all/0/1"&gt;Daniel Vial&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parulekar_A/0/1/0/all/0/1"&gt;Advait Parulekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1"&gt;Sanjay Shakkottai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srikant_R/0/1/0/all/0/1"&gt;R. Srikant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Innovations Autoencoder and its Application in Real-Time Anomaly Detection. (arXiv:2106.12382v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.12382</id>
        <link href="http://arxiv.org/abs/2106.12382"/>
        <updated>2021-06-24T01:51:45.726Z</updated>
        <summary type="html"><![CDATA[An innovations sequence of a time series is a sequence of independent and
identically distributed random variables with which the original time series
has a causal representation. The innovation at a time is statistically
independent of the prior history of the time series. As such, it represents the
new information contained at present but not in the past. Because of its simple
probability structure, an innovations sequence is the most efficient signature
of the original. Unlike the principle or independent analysis (PCA/ICA)
representations, an innovations sequence preserves not only the complete
statistical properties but also the temporal order of the original time series.
An long-standing open problem is to find a computationally tractable way to
extract an innovations sequence of non-Gaussian processes. This paper presents
a deep learning approach, referred to as Innovations Autoencoder (IAE), that
extracts innovations sequences using a causal convolutional neural network. An
application of IAE to nonparametric anomaly detection with unknown anomaly and
anomaly-free models is also presented.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tong_L/0/1/0/all/0/1"&gt;Lang Tong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-Grained Data Selection for Improved Energy Efficiency of Federated Edge Learning. (arXiv:2106.12561v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12561</id>
        <link href="http://arxiv.org/abs/2106.12561"/>
        <updated>2021-06-24T01:51:45.721Z</updated>
        <summary type="html"><![CDATA[In Federated edge learning (FEEL), energy-constrained devices at the network
edge consume significant energy when training and uploading their local machine
learning models, leading to a decrease in their lifetime. This work proposes
novel solutions for energy-efficient FEEL by jointly considering local training
data, available computation, and communications resources, and deadline
constraints of FEEL rounds to reduce energy consumption. This paper considers a
system model where the edge server is equipped with multiple antennas employing
beamforming techniques to communicate with the local users through orthogonal
channels. Specifically, we consider a problem that aims to find the optimal
user's resources, including the fine-grained selection of relevant training
samples, bandwidth, transmission power, beamforming weights, and processing
speed with the goal of minimizing the total energy consumption given a deadline
constraint on the communication rounds of FEEL. Then, we devise tractable
solutions by first proposing a novel fine-grained training algorithm that
excludes less relevant training samples and effectively chooses only the
samples that improve the model's performance. After that, we derive closed-form
solutions, followed by a Golden-Section-based iterative algorithm to find the
optimal computation and communication resources that minimize energy
consumption. Experiments using MNIST and CIFAR-10 datasets demonstrate that our
proposed algorithms considerably outperform the state-of-the-art solutions as
energy consumption decreases by 79% for MNIST and 73% for CIFAR-10 datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Albaseer_A/0/1/0/all/0/1"&gt;Abdullatif Albaseer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdallah_M/0/1/0/all/0/1"&gt;Mohamed Abdallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Fuqaha_A/0/1/0/all/0/1"&gt;Ala Al-Fuqaha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1"&gt;Aiman Erbad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Co-advise: Cross Inductive Bias Distillation. (arXiv:2106.12378v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12378</id>
        <link href="http://arxiv.org/abs/2106.12378"/>
        <updated>2021-06-24T01:51:45.707Z</updated>
        <summary type="html"><![CDATA[Transformers recently are adapted from the community of natural language
processing as a promising substitute of convolution-based neural networks for
visual learning tasks. However, its supremacy degenerates given an insufficient
amount of training data (e.g., ImageNet). To make it into practical utility, we
propose a novel distillation-based method to train vision transformers. Unlike
previous works, where merely heavy convolution-based teachers are provided, we
introduce lightweight teachers with different architectural inductive biases
(e.g., convolution and involution) to co-advise the student transformer. The
key is that teachers with different inductive biases attain different knowledge
despite that they are trained on the same dataset, and such different knowledge
compounds and boosts the student's performance during distillation. Equipped
with this cross inductive bias distillation method, our vision transformers
(termed as CivT) outperform all previous transformers of the same architecture
on ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Sucheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhengqi Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_T/0/1/0/all/0/1"&gt;Tianyu Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1"&gt;Zihui Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonglong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shengfeng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hang Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Closed-Form, Provable, and Robust PCA via Leverage Statistics and Innovation Search. (arXiv:2106.12190v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.12190</id>
        <link href="http://arxiv.org/abs/2106.12190"/>
        <updated>2021-06-24T01:51:45.703Z</updated>
        <summary type="html"><![CDATA[The idea of Innovation Search, which was initially proposed for data
clustering, was recently used for outlier detection. In the application of
Innovation Search for outlier detection, the directions of innovation were
utilized to measure the innovation of the data points. We study the Innovation
Values computed by the Innovation Search algorithm under a quadratic cost
function and it is proved that Innovation Values with the new cost function are
equivalent to Leverage Scores. This interesting connection is utilized to
establish several theoretical guarantees for a Leverage Score based robust PCA
method and to design a new robust PCA method. The theoretical results include
performance guarantees with different models for the distribution of outliers
and the distribution of inliers. In addition, we demonstrate the robustness of
the algorithms against the presence of noise. The numerical and theoretical
studies indicate that while the presented approach is fast and closed-form, it
can outperform most of the existing algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Rahmani_M/0/1/0/all/0/1"&gt;Mostafa Rahmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_P/0/1/0/all/0/1"&gt;Ping Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[It's All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning. (arXiv:2106.12066v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12066</id>
        <link href="http://arxiv.org/abs/2106.12066"/>
        <updated>2021-06-24T01:51:45.698Z</updated>
        <summary type="html"><![CDATA[Commonsense reasoning is one of the key problems in natural language
processing, but the relative scarcity of labeled data holds back the progress
for languages other than English. Pretrained cross-lingual models are a source
of powerful language-agnostic representations, yet their inherent reasoning
capabilities are still actively studied. In this work, we design a simple
approach to commonsense reasoning which trains a linear classifier with weights
of multi-head attention as features. To evaluate this approach, we create a
multilingual Winograd Schema corpus by processing several datasets from prior
work within a standardized pipeline and measure cross-lingual generalization
ability in terms of out-of-sample performance. The method performs
competitively with recent supervised and unsupervised approaches for
commonsense reasoning, even when applied to other languages in a zero-shot
manner. Also, we demonstrate that most of the performance is given by the same
small subset of attention heads for all studied languages, which provides
evidence of universal reasoning capabilities in multilingual encoders.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1"&gt;Alexey Tikhonov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ryabinin_M/0/1/0/all/0/1"&gt;Max Ryabinin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Quantized Gradient Methods. (arXiv:2002.02508v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.02508</id>
        <link href="http://arxiv.org/abs/2002.02508"/>
        <updated>2021-06-24T01:51:45.693Z</updated>
        <summary type="html"><![CDATA[This paper considers quantized distributed optimization algorithms in the
parameter server framework of distributed training. We introduce the principle
we call Differential Quantization (DQ) that prescribes that the past
quantization errors should be compensated in such a way as to direct the
descent trajectory of a quantized algorithm towards that of its unquantized
counterpart. Assuming that the objective function is smooth and strongly
convex, we prove that in the limit of large problem dimension, Differentially
Quantized Gradient Descent (DQ-GD) attains a linear contraction factor of
$\max\{\sigma_{\mathrm{GD}}, 2^{-R}\}$, where $\sigma_{\mathrm{GD}}$ is the
contraction factor of unquantized gradient descent (GD). Thus at any
$R\geq\log_2 1 /\sigma_{\mathrm{GD}}$ bits, the contraction factor of DQ-GD is
the same as that of unquantized GD, i.e., there is no loss due to quantization.
We show a converse demonstrating that no quantized gradient descent algorithm
can converge faster than $\max\{\sigma_{\mathrm{GD}}, 2^{-R}\}$. In contrast,
naively quantized GD where the worker directly quantizes the gradient barely
attains $\sigma_{\mathrm{GD}} + 2^{-R}$. The principle of differential
quantization continues to apply to gradient methods with momentum such as
Nesterov's accelerated gradient descent, and Polyak's heavy ball method. For
these algorithms as well, if the rate is above a certain threshold, there is no
loss in contraction factor obtained by the differentially quantized algorithm
compared to its unquantized counterpart, and furthermore, the differentially
quantized heavy ball method attains the optimal contraction achievable among
all (even unquantized) gradient methods. Experimental results on both simulated
and real-world least-squares problems validate our theoretical analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chung-Yi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kostina_V/0/1/0/all/0/1"&gt;Victoria Kostina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassibi_B/0/1/0/all/0/1"&gt;Babak Hassibi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-trained Models for Natural Language Processing: A Survey. (arXiv:2003.08271v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.08271</id>
        <link href="http://arxiv.org/abs/2003.08271"/>
        <updated>2021-06-24T01:51:45.687Z</updated>
        <summary type="html"><![CDATA[Recently, the emergence of pre-trained models (PTMs) has brought natural
language processing (NLP) to a new era. In this survey, we provide a
comprehensive review of PTMs for NLP. We first briefly introduce language
representation learning and its research progress. Then we systematically
categorize existing PTMs based on a taxonomy with four perspectives. Next, we
describe how to adapt the knowledge of PTMs to the downstream tasks. Finally,
we outline some potential directions of PTMs for future research. This survey
is purposed to be a hands-on guide for understanding, using, and developing
PTMs for various NLP tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1"&gt;Xipeng Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1"&gt;Tianxiang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yige Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yunfan Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_N/0/1/0/all/0/1"&gt;Ning Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xuanjing Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured in Space, Randomized in Time: Leveraging Dropout in RNNs for Efficient Training. (arXiv:2106.12089v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12089</id>
        <link href="http://arxiv.org/abs/2106.12089"/>
        <updated>2021-06-24T01:51:45.682Z</updated>
        <summary type="html"><![CDATA[Recurrent Neural Networks (RNNs), more specifically their Long Short-Term
Memory (LSTM) variants, have been widely used as a deep learning tool for
tackling sequence-based learning tasks in text and speech. Training of such
LSTM applications is computationally intensive due to the recurrent nature of
hidden state computation that repeats for each time step. While sparsity in
Deep Neural Nets has been widely seen as an opportunity for reducing
computation time in both training and inference phases, the usage of non-ReLU
activation in LSTM RNNs renders the opportunities for such dynamic sparsity
associated with neuron activation and gradient values to be limited or
non-existent. In this work, we identify dropout induced sparsity for LSTMs as a
suitable mode of computation reduction. Dropout is a widely used regularization
mechanism, which randomly drops computed neuron values during each iteration of
training. We propose to structure dropout patterns, by dropping out the same
set of physical neurons within a batch, resulting in column (row) level hidden
state sparsity, which are well amenable to computation reduction at run-time in
general-purpose SIMD hardware as well as systolic arrays. We conduct our
experiments for three representative NLP tasks: language modelling on the PTB
dataset, OpenNMT based machine translation using the IWSLT De-En and En-Vi
datasets, and named entity recognition sequence labelling using the CoNLL-2003
shared task. We demonstrate that our proposed approach can be used to translate
dropout-based computation reduction into reduced training time, with
improvement ranging from 1.23x to 1.64x, without sacrificing the target metric.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sarma_A/0/1/0/all/0/1"&gt;Anup Sarma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sonali Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Huaipan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kandemir_M/0/1/0/all/0/1"&gt;Mahmut T Kandemir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_C/0/1/0/all/0/1"&gt;Chita R Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IQ-Learn: Inverse soft-Q Learning for Imitation. (arXiv:2106.12142v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12142</id>
        <link href="http://arxiv.org/abs/2106.12142"/>
        <updated>2021-06-24T01:51:45.677Z</updated>
        <summary type="html"><![CDATA[In many sequential decision-making problems (e.g., robotics control, game
playing, sequential prediction), human or expert data is available containing
useful information about the task. However, imitation learning (IL) from a
small amount of expert data can be challenging in high-dimensional environments
with complex dynamics. Behavioral cloning is a simple method that is widely
used due to its simplicity of implementation and stable convergence but doesn't
utilize any information involving the environment's dynamics. Many existing
methods that exploit dynamics information are difficult to train in practice
due to an adversarial optimization process over reward and policy approximators
or biased, high variance gradient estimators. We introduce a method for
dynamics-aware IL which avoids adversarial training by learning a single
Q-function, implicitly representing both reward and policy. On standard
benchmarks, the implicitly learned rewards show a high positive correlation
with the ground-truth rewards, illustrating our method can also be used for
inverse reinforcement learning (IRL). Our method, Inverse soft-Q learning
(IQ-Learn) obtains state-of-the-art results in offline and online imitation
learning settings, surpassing existing methods both in the number of required
environment interactions and scalability in high-dimensional spaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garg_D/0/1/0/all/0/1"&gt;Divyansh Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1"&gt;Shuvam Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cundy_C/0/1/0/all/0/1"&gt;Chris Cundy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jiaming Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1"&gt;Stefano Ermon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Stochastic Majority Votes by Minimizing a PAC-Bayes Generalization Bound. (arXiv:2106.12535v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12535</id>
        <link href="http://arxiv.org/abs/2106.12535"/>
        <updated>2021-06-24T01:51:45.672Z</updated>
        <summary type="html"><![CDATA[We investigate a stochastic counterpart of majority votes over finite
ensembles of classifiers, and study its generalization properties. While our
approach holds for arbitrary distributions, we instantiate it with Dirichlet
distributions: this allows for a closed-form and differentiable expression for
the expected risk, which then turns the generalization bound into a tractable
training objective. The resulting stochastic majority vote learning algorithm
achieves state-of-the-art accuracy and benefits from (non-vacuous) tight
generalization bounds, in a series of numerical experiments when compared to
competing algorithms which also minimize PAC-Bayes objectives -- both with
uninformed (data-independent) and informed (data-dependent) priors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zantedeschi_V/0/1/0/all/0/1"&gt;Valentina Zantedeschi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viallard_P/0/1/0/all/0/1"&gt;Paul Viallard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morvant_E/0/1/0/all/0/1"&gt;Emilie Morvant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Emonet_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Emonet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Habrard_A/0/1/0/all/0/1"&gt;Amaury Habrard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Germain_P/0/1/0/all/0/1"&gt;Pascal Germain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guedj_B/0/1/0/all/0/1"&gt;Benjamin Guedj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraphConfRec: A Graph Neural Network-Based Conference Recommender System. (arXiv:2106.12340v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.12340</id>
        <link href="http://arxiv.org/abs/2106.12340"/>
        <updated>2021-06-24T01:51:45.656Z</updated>
        <summary type="html"><![CDATA[In today's academic publishing model, especially in Computer Science,
conferences commonly constitute the main platforms for releasing the latest
peer-reviewed advancements in their respective fields. However, choosing a
suitable academic venue for publishing one's research can represent a
challenging task considering the plethora of available conferences,
particularly for those at the start of their academic careers, or for those
seeking to publish outside of their usual domain. In this paper, we propose
GraphConfRec, a conference recommender system which combines SciGraph and graph
neural networks, to infer suggestions based not only on title and abstract, but
also on co-authorship and citation relationships. GraphConfRec achieves a
recall@10 of up to 0.580 and a MAP of up to 0.336 with a graph attention
network-based recommendation model. A user study with 25 subjects supports the
positive results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Iana_A/0/1/0/all/0/1"&gt;Andreea Iana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1"&gt;Heiko Paulheim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Who Leads and Who Follows in Strategic Classification?. (arXiv:2106.12529v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12529</id>
        <link href="http://arxiv.org/abs/2106.12529"/>
        <updated>2021-06-24T01:51:45.642Z</updated>
        <summary type="html"><![CDATA[As predictive models are deployed into the real world, they must increasingly
contend with strategic behavior. A growing body of work on strategic
classification treats this problem as a Stackelberg game: the decision-maker
"leads" in the game by deploying a model, and the strategic agents "follow" by
playing their best response to the deployed model. Importantly, in this
framing, the burden of learning is placed solely on the decision-maker, while
the agents' best responses are implicitly treated as instantaneous. In this
work, we argue that the order of play in strategic classification is
fundamentally determined by the relative frequencies at which the
decision-maker and the agents adapt to each other's actions. In particular, by
generalizing the standard model to allow both players to learn over time, we
show that a decision-maker that makes updates faster than the agents can
reverse the order of play, meaning that the agents lead and the decision-maker
follows. We observe in standard learning settings that such a role reversal can
be desirable for both the decision-maker and the strategic agents. Finally, we
show that a decision-maker with the freedom to choose their update frequency
can induce learning dynamics that converge to Stackelberg equilibria with
either order of play.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zrnic_T/0/1/0/all/0/1"&gt;Tijana Zrnic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazumdar_E/0/1/0/all/0/1"&gt;Eric Mazumdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sastry_S/0/1/0/all/0/1"&gt;S. Shankar Sastry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CIFS: Improving Adversarial Robustness of CNNs via Channel-wise Importance-based Feature Selection. (arXiv:2102.05311v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05311</id>
        <link href="http://arxiv.org/abs/2102.05311"/>
        <updated>2021-06-24T01:51:45.620Z</updated>
        <summary type="html"><![CDATA[We investigate the adversarial robustness of CNNs from the perspective of
channel-wise activations. By comparing \textit{non-robust} (normally trained)
and \textit{robustified} (adversarially trained) models, we observe that
adversarial training (AT) robustifies CNNs by aligning the channel-wise
activations of adversarial data with those of their natural counterparts.
However, the channels that are \textit{negatively-relevant} (NR) to predictions
are still over-activated when processing adversarial data. Besides, we also
observe that AT does not result in similar robustness for all classes. For the
robust classes, channels with larger activation magnitudes are usually more
\textit{positively-relevant} (PR) to predictions, but this alignment does not
hold for the non-robust classes. Given these observations, we hypothesize that
suppressing NR channels and aligning PR ones with their relevances further
enhances the robustness of CNNs under AT. To examine this hypothesis, we
introduce a novel mechanism, i.e., \underline{C}hannel-wise
\underline{I}mportance-based \underline{F}eature \underline{S}election (CIFS).
The CIFS manipulates channels' activations of certain layers by generating
non-negative multipliers to these channels based on their relevances to
predictions. Extensive experiments on benchmark datasets including CIFAR10 and
SVHN clearly verify the hypothesis and CIFS's effectiveness of robustifying
CNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1"&gt;Hanshu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingfeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiashi Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1"&gt;Vincent Y. F. Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HILONet: Hierarchical Imitation Learning from Non-Aligned Observations. (arXiv:2011.02671v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.02671</id>
        <link href="http://arxiv.org/abs/2011.02671"/>
        <updated>2021-06-24T01:51:45.607Z</updated>
        <summary type="html"><![CDATA[It is challenging learning from demonstrated observation-only trajectories in
a non-time-aligned environment because most imitation learning methods aim to
imitate experts by following the demonstration step-by-step. However, aligned
demonstrations are seldom obtainable in real-world scenarios. In this work, we
propose a new imitation learning approach called Hierarchical Imitation
Learning from Observation(HILONet), which adopts a hierarchical structure to
choose feasible sub-goals from demonstrated observations dynamically. Our
method can solve all kinds of tasks by achieving these sub-goals, whether it
has a single goal position or not. We also present three different ways to
increase sample efficiency in the hierarchical structure. We conduct extensive
experiments using several environments. The results show the improvement in
both performance and learning efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shanqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Junjie Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wenzhou Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1"&gt;Licheng Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Euro-PVI: Pedestrian Vehicle Interactions in Dense Urban Centers. (arXiv:2106.12442v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12442</id>
        <link href="http://arxiv.org/abs/2106.12442"/>
        <updated>2021-06-24T01:51:45.580Z</updated>
        <summary type="html"><![CDATA[Accurate prediction of pedestrian and bicyclist paths is integral to the
development of reliable autonomous vehicles in dense urban environments. The
interactions between vehicle and pedestrian or bicyclist have a significant
impact on the trajectories of traffic participants e.g. stopping or turning to
avoid collisions. Although recent datasets and trajectory prediction approaches
have fostered the development of autonomous vehicles yet the amount of
vehicle-pedestrian (bicyclist) interactions modeled are sparse. In this work,
we propose Euro-PVI, a dataset of pedestrian and bicyclist trajectories. In
particular, our dataset caters more diverse and complex interactions in dense
urban scenarios compared to the existing datasets. To address the challenges in
predicting future trajectories with dense interactions, we develop a joint
inference model that learns an expressive multi-modal shared latent space
across agents in the urban scene. This enables our Joint-$\beta$-cVAE approach
to better model the distribution of future trajectories. We achieve state of
the art results on the nuScenes and Euro-PVI datasets demonstrating the
importance of capturing interactions between ego-vehicle and pedestrians
(bicyclists) for accurate predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1"&gt;Apratim Bhattacharyya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reino_D/0/1/0/all/0/1"&gt;Daniel Olmeda Reino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1"&gt;Mario Fritz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1"&gt;Bernt Schiele&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample-Optimal PAC Learning of Halfspaces with Malicious Noise. (arXiv:2102.06247v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06247</id>
        <link href="http://arxiv.org/abs/2102.06247"/>
        <updated>2021-06-24T01:51:45.014Z</updated>
        <summary type="html"><![CDATA[We study efficient PAC learning of homogeneous halfspaces in $\mathbb{R}^d$
in the presence of malicious noise of Valiant~(1985). This is a challenging
noise model and only until recently has near-optimal noise tolerance bound been
established under the mild condition that the unlabeled data distribution is
isotropic log-concave. However, it remains unsettled how to obtain the optimal
sample complexity simultaneously. In this work, we present a new analysis for
the algorithm of Awasthi~et~al.~(2017) and show that it essentially achieves
the near-optimal sample complexity bound of $\tilde{O}(d)$, improving the best
known result of $\tilde{O}(d^2)$. Our main ingredient is a novel incorporation
of a matrix Chernoff-type inequality to bound the spectrum of an empirical
covariance matrix for well-behaved distributions, in conjunction with a careful
exploration of the localization schemes of Awasthi~et~al.~(2017). We further
extend the algorithm and analysis to the more general and stronger nasty noise
model of Bshouty~et~al.~(2002), showing that it is still possible to achieve
near-optimal noise tolerance and sample complexity in polynomial time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jie Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Certified Robust Training with Short Warmup. (arXiv:2103.17268v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.17268</id>
        <link href="http://arxiv.org/abs/2103.17268"/>
        <updated>2021-06-24T01:51:45.008Z</updated>
        <summary type="html"><![CDATA[Recently, bound propagation based certified robust training methods have been
proposed for training neural networks with certifiable robustness guarantees.
Despite that state-of-the-art (SOTA) methods including interval bound
propagation (IBP) and CROWN-IBP have per-batch training complexity similar to
standard neural network training, they usually use a long warmup schedule with
hundreds or thousands epochs to reach SOTA performance and are thus still
costly. In this paper, we identify two important issues in existing methods,
namely exploded bounds at initialization, and the imbalance in ReLU activation
states. These two issues make certified training difficult and unstable, and
thereby long warmup schedules were needed in prior works. To mitigate these
issues and conduct certified training with shorter warmup, we propose three
improvements: 1) We derive a new weight initialization method for IBP training;
2) We propose to fully add Batch Normalization (BN) to each layer in the model,
since we find BN can reduce the imbalance in ReLU activation states; 3) We also
design regularization to explicitly tighten certified bounds and balance ReLU
activation states. In our experiments, we are able to obtain 65.03% verified
error on CIFAR-10 ($\epsilon=\frac{8}{255}$) and 82.36% verified error on
TinyImageNet ($\epsilon=\frac{1}{255}$) using very short training schedules
(160 and 80 total epochs, respectively), outperforming literature SOTA trained
with hundreds or thousands epochs under the same network architecture.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1"&gt;Zhouxing Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yihan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1"&gt;Jinfeng Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the Transferability of Adversarial Examples with New Iteration Framework and Input Dropout. (arXiv:2106.01617v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01617</id>
        <link href="http://arxiv.org/abs/2106.01617"/>
        <updated>2021-06-24T01:51:44.984Z</updated>
        <summary type="html"><![CDATA[Deep neural networks(DNNs) is vulnerable to be attacked by adversarial
examples. Black-box attack is the most threatening attack. At present,
black-box attack methods mainly adopt gradient-based iterative attack methods,
which usually limit the relationship between the iteration step size, the
number of iterations, and the maximum perturbation. In this paper, we propose a
new gradient iteration framework, which redefines the relationship between the
above three. Under this framework, we easily improve the attack success rate of
DI-TI-MIM. In addition, we propose a gradient iterative attack method based on
input dropout, which can be well combined with our framework. We further
propose a multi dropout rate version of this method. Experimental results show
that our best method can achieve attack success rate of 96.2\% for defense
model on average, which is higher than the state-of-the-art gradient-based
attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1"&gt;Pengfei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Linyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1"&gt;Ruoxi Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_K/0/1/0/all/0/1"&gt;Kai Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1"&gt;Shuhao Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1"&gt;Guoen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1"&gt;Bin Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Data Subset Selection for Regression with Controlled Generalization Error. (arXiv:2106.12491v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12491</id>
        <link href="http://arxiv.org/abs/2106.12491"/>
        <updated>2021-06-24T01:51:44.978Z</updated>
        <summary type="html"><![CDATA[Data subset selection from a large number of training instances has been a
successful approach toward efficient and cost-effective machine learning.
However, models trained on a smaller subset may show poor generalization
ability. In this paper, our goal is to design an algorithm for selecting a
subset of the training data, so that the model can be trained quickly, without
significantly sacrificing on accuracy. More specifically, we focus on data
subset selection for L2 regularized regression problems and provide a novel
problem formulation which seeks to minimize the training loss with respect to
both the trainable parameters and the subset of training data, subject to error
bounds on the validation set. We tackle this problem using several technical
innovations. First, we represent this problem with simplified constraints using
the dual of the original training problem and show that the objective of this
new representation is a monotone and alpha-submodular function, for a wide
variety of modeling choices. Such properties lead us to develop SELCON, an
efficient majorization-minimization algorithm for data subset selection, that
admits an approximation guarantee even when the training provides an imperfect
estimate of the trained model. Finally, our experiments on several datasets
show that SELCON trades off accuracy and efficiency more effectively than the
current state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sivasubramanian_D/0/1/0/all/0/1"&gt;Durga Sivasubramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1"&gt;Rishabh Iyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1"&gt;Ganesh Ramakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1"&gt;Abir De&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal training of variational quantum algorithms without barren plateaus. (arXiv:2104.14543v3 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14543</id>
        <link href="http://arxiv.org/abs/2104.14543"/>
        <updated>2021-06-24T01:51:44.972Z</updated>
        <summary type="html"><![CDATA[Variational quantum algorithms (VQAs) promise efficient use of near-term
quantum computers. However, training VQAs often requires an extensive amount of
time and suffers from the barren plateau problem where the magnitude of the
gradients vanishes with increasing number of qubits. Here, we show how to
optimally train VQAs for learning quantum states. Parameterized quantum
circuits can form Gaussian kernels, which we use to derive adaptive learning
rates for gradient ascent. We introduce the generalized quantum natural
gradient that features stability and optimized movement in parameter space.
Both methods together outperform other optimization routines in training VQAs.
Our methods also excel at numerically optimizing driving protocols for quantum
control problems. The gradients of the VQA do not vanish when the fidelity
between the initial state and the state to be learned is bounded from below. We
identify a VQA for quantum simulation with such a constraint that thus can be
trained free of barren plateaus. Finally, we propose the application of
Gaussian kernels for quantum machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Haug_T/0/1/0/all/0/1"&gt;Tobias Haug&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kim_M/0/1/0/all/0/1"&gt;M.S. Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weisfeiler and Lehman Go Cellular: CW Networks. (arXiv:2106.12575v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12575</id>
        <link href="http://arxiv.org/abs/2106.12575"/>
        <updated>2021-06-24T01:51:44.967Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) are limited in their expressive power, struggle
with long-range interactions and lack a principled way to model higher-order
structures. These problems can be attributed to the strong coupling between the
computational graph and the input graph structure. The recently proposed
Message Passing Simplicial Networks naturally decouple these elements by
performing message passing on the clique complex of the graph. Nevertheless,
these models are severely constrained by the rigid combinatorial structure of
Simplicial Complexes (SCs). In this work, we extend recent theoretical results
on SCs to regular Cell Complexes, topological objects that flexibly subsume SCs
and graphs. We show that this generalisation provides a powerful set of graph
``lifting'' transformations, each leading to a unique hierarchical message
passing procedure. The resulting methods, which we collectively call CW
Networks (CWNs), are strictly more powerful than the WL test and, in certain
cases, not less powerful than the 3-WL test. In particular, we demonstrate the
effectiveness of one such scheme, based on rings, when applied to molecular
graph problems. The proposed architecture benefits from provably larger
expressivity than commonly used GNNs, principled modelling of higher-order
signals and from compressing the distances between nodes. We demonstrate that
our model achieves state-of-the-art results on a variety of molecular datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bodnar_C/0/1/0/all/0/1"&gt;Cristian Bodnar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frasca_F/0/1/0/all/0/1"&gt;Fabrizio Frasca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Otter_N/0/1/0/all/0/1"&gt;Nina Otter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Guang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1"&gt;Pietro Li&amp;#xf2;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Montufar_G/0/1/0/all/0/1"&gt;Guido Mont&amp;#xfa;far&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bronstein_M/0/1/0/all/0/1"&gt;Michael Bronstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding. (arXiv:2106.12566v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12566</id>
        <link href="http://arxiv.org/abs/2106.12566"/>
        <updated>2021-06-24T01:51:44.952Z</updated>
        <summary type="html"><![CDATA[The attention module, which is a crucial component in Transformer, cannot
scale efficiently to long sequences due to its quadratic complexity. Many works
focus on approximating the dot-then-exponentiate softmax function in the
original attention, leading to sub-quadratic or even linear-complexity
Transformer architectures. However, we show that these methods cannot be
applied to more powerful attention modules that go beyond the
dot-then-exponentiate style, e.g., Transformers with relative positional
encoding (RPE). Since in many state-of-the-art models, relative positional
encoding is used as default, designing efficient Transformers that can
incorporate RPE is appealing. In this paper, we propose a novel way to
accelerate attention calculation for Transformers with RPE on top of the
kernelized attention. Based upon the observation that relative positional
encoding forms a Toeplitz matrix, we mathematically show that kernelized
attention with RPE can be calculated efficiently using Fast Fourier Transform
(FFT). With FFT, our method achieves $\mathcal{O}(n\log n)$ time complexity.
Interestingly, we further demonstrate that properly using relative positional
encoding can mitigate the training instability problem of vanilla kernelized
attention. On a wide range of tasks, we empirically show that our models can be
trained from scratch without any optimization issues. The learned model
performs better than many efficient Transformer variants and is faster than
standard Transformer in the long-sequence regime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1"&gt;Shengjie Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shanda Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1"&gt;Tianle Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Di He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1"&gt;Dinglan Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Shuxin Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ke_G/0/1/0/all/0/1"&gt;Guolin Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessment of the influence of features on a classification problem: an application to COVID-19 patients. (arXiv:2104.14958v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14958</id>
        <link href="http://arxiv.org/abs/2104.14958"/>
        <updated>2021-06-24T01:51:44.947Z</updated>
        <summary type="html"><![CDATA[This paper deals with an important subject in classification problems
addressed by machine learning techniques: the evaluation of the influence of
each of the features on the classification of individuals. Specifically, a
measure of that influence is introduced using the Shapley value of cooperative
games. In addition, an axiomatic characterisation of the proposed measure is
provided based on properties of efficiency and balanced contributions.
Furthermore, some experiments have been designed in order to validate the
appropriate performance of such measure. Finally, the methodology introduced is
applied to a sample of COVID-19 patients to study the influence of certain
demographic or risk factors on various events of interest related to the
evolution of the disease.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Davila_Pena_L/0/1/0/all/0/1"&gt;L. Davila-Pena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Garcia_Jurado_I/0/1/0/all/0/1"&gt;Ignacio Garc&amp;#xed;a-Jurado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Casas_Mendez_B/0/1/0/all/0/1"&gt;B. Casas-M&amp;#xe9;ndez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A novel multi-classifier information fusion based on Dempster-Shafer theory: application to vibration-based fault detection. (arXiv:2012.02481v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02481</id>
        <link href="http://arxiv.org/abs/2012.02481"/>
        <updated>2021-06-24T01:51:44.942Z</updated>
        <summary type="html"><![CDATA[Achieving a high prediction rate is a crucial task in fault detection.
Although various classification procedures are available, none of them can give
high accuracy in all applications. Therefore, in this paper, a novel
multi-classifier fusion approach is developed to boost the performance of the
individual classifiers. This is acquired by using Dempster-Shafer theory (DST).
However, in cases with conflicting evidences, the DST may give
counter-intuitive results. In this regard, a preprocessing technique based on a
new metric is devised in order to measure and mitigate the conflict between the
evidences. To evaluate and validate the effectiveness of the proposed approach,
the method is applied to 15 benchmarks datasets from UCI and KEEL. Further, it
is applied for classifying polycrystalline Nickel alloy first-stage turbine
blades based on their broadband vibrational response. Through statistical
analysis with different noise levels, and by comparing with four
state-of-the-art fusion techniques, it is shown that that the proposed method
improves the classification accuracy and outperforms the individual
classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yaghoubi_V/0/1/0/all/0/1"&gt;Vahid Yaghoubi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1"&gt;Liangliang Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paepegem_W/0/1/0/all/0/1"&gt;Wim Van Paepegem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kersemans_M/0/1/0/all/0/1"&gt;Mathias Kersemans&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ShapeMOD: Macro Operation Discovery for 3D Shape Programs. (arXiv:2104.06392v2 [cs.GR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06392</id>
        <link href="http://arxiv.org/abs/2104.06392"/>
        <updated>2021-06-24T01:51:44.936Z</updated>
        <summary type="html"><![CDATA[A popular way to create detailed yet easily controllable 3D shapes is via
procedural modeling, i.e. generating geometry using programs. Such programs
consist of a series of instructions along with their associated parameter
values. To fully realize the benefits of this representation, a shape program
should be compact and only expose degrees of freedom that allow for meaningful
manipulation of output geometry. One way to achieve this goal is to design
higher-level macro operators that, when executed, expand into a series of
commands from the base shape modeling language. However, manually authoring
such macros, much like shape programs themselves, is difficult and largely
restricted to domain experts. In this paper, we present ShapeMOD, an algorithm
for automatically discovering macros that are useful across large datasets of
3D shape programs. ShapeMOD operates on shape programs expressed in an
imperative, statement-based language. It is designed to discover macros that
make programs more compact by minimizing the number of function calls and free
parameters required to represent an input shape collection. We run ShapeMOD on
multiple collections of programs expressed in a domain-specific language for 3D
shape structures. We show that it automatically discovers a concise set of
macros that abstract out common structural and parametric patterns that
generalize over large shape collections. We also demonstrate that the macros
found by ShapeMOD improve performance on downstream tasks including shape
generative modeling and inferring programs from point clouds. Finally, we
conduct a user study that indicates that ShapeMOD's discovered macros make
interactive shape editing more efficient.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1"&gt;R. Kenny Jones&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charatan_D/0/1/0/all/0/1"&gt;David Charatan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guerrero_P/0/1/0/all/0/1"&gt;Paul Guerrero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1"&gt;Niloy J. Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1"&gt;Daniel Ritchie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fixed-Budget Best-Arm Identification in Contextual Bandits: A Static-Adaptive Algorithm. (arXiv:2106.04763v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04763</id>
        <link href="http://arxiv.org/abs/2106.04763"/>
        <updated>2021-06-24T01:51:44.930Z</updated>
        <summary type="html"><![CDATA[We study the problem of best-arm identification (BAI) in contextual bandits
in the fixed-budget setting. We propose a general successive elimination
algorithm that proceeds in stages and eliminates a fixed fraction of suboptimal
arms in each stage. This design takes advantage of the strengths of static and
adaptive allocations. We analyze the algorithm in linear models and obtain a
better error bound than prior work. We also apply it to generalized linear
models (GLMs) and bound its error. This is the first BAI algorithm for GLMs in
the fixed-budget setting. Our extensive numerical experiments show that our
algorithm outperforms the state of art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Azizi_M/0/1/0/all/0/1"&gt;MohammadJavad Azizi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1"&gt;Branislav Kveton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1"&gt;Mohammad Ghavamzadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Power of Localized Perceptron for Label-Optimal Learning of Halfspaces with Adversarial Noise. (arXiv:2012.10793v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10793</id>
        <link href="http://arxiv.org/abs/2012.10793"/>
        <updated>2021-06-24T01:51:44.890Z</updated>
        <summary type="html"><![CDATA[We study {\em online} active learning of homogeneous halfspaces in
$\mathbb{R}^d$ with adversarial noise where the overall probability of a noisy
label is constrained to be at most $\nu$. Our main contribution is a
Perceptron-like online active learning algorithm that runs in polynomial time,
and under the conditions that the marginal distribution is isotropic
log-concave and $\nu = \Omega(\epsilon)$, where $\epsilon \in (0, 1)$ is the
target error rate, our algorithm PAC learns the underlying halfspace with
near-optimal label complexity of $\tilde{O}\big(d \cdot
polylog(\frac{1}{\epsilon})\big)$ and sample complexity of
$\tilde{O}\big(\frac{d}{\epsilon} \big)$. Prior to this work, existing online
algorithms designed for tolerating the adversarial noise are subject to either
label complexity polynomial in $\frac{1}{\epsilon}$, or suboptimal noise
tolerance, or restrictive marginal distributions. With the additional prior
knowledge that the underlying halfspace is $s$-sparse, we obtain
attribute-efficient label complexity of $\tilde{O}\big( s \cdot polylog(d,
\frac{1}{\epsilon}) \big)$ and sample complexity of
$\tilde{O}\big(\frac{s}{\epsilon} \cdot polylog(d) \big)$. As an immediate
corollary, we show that under the agnostic model where no assumption is made on
the noise rate $\nu$, our active learner achieves an error rate of $O(OPT) +
\epsilon$ with the same running time and label and sample complexity, where
$OPT$ is the best possible error rate achievable by any homogeneous halfspace.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jie Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis of the Evolution of Parametric Drivers of High-End Sea-Level Hazards. (arXiv:2106.12041v1 [physics.ao-ph])]]></title>
        <id>http://arxiv.org/abs/2106.12041</id>
        <link href="http://arxiv.org/abs/2106.12041"/>
        <updated>2021-06-24T01:51:44.883Z</updated>
        <summary type="html"><![CDATA[Climate models are critical tools for developing strategies to manage the
risks posed by sea-level rise to coastal communities. While these models are
necessary for understanding climate risks, there is a level of uncertainty
inherent in each parameter in the models. This model parametric uncertainty
leads to uncertainty in future climate risks. Consequently, there is a need to
understand how those parameter uncertainties impact our assessment of future
climate risks and the efficacy of strategies to manage them. Here, we use
random forests to examine the parametric drivers of future climate risk and how
the relative importances of those drivers change over time. We find that the
equilibrium climate sensitivity and a factor that scales the effect of aerosols
on radiative forcing are consistently the most important climate model
parametric uncertainties throughout the 2020 to 2150 interval for both low and
high radiative forcing scenarios. The near-term hazards of high-end sea-level
rise are driven primarily by thermal expansion, while the longer-term hazards
are associated with mass loss from the Antarctic and Greenland ice sheets. Our
results highlight the practical importance of considering time-evolving
parametric uncertainties when developing strategies to manage future climate
risks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Hough_A/0/1/0/all/0/1"&gt;Alana Hough&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wong_T/0/1/0/all/0/1"&gt;Tony E. Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing. (arXiv:2104.14754v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14754</id>
        <link href="http://arxiv.org/abs/2104.14754"/>
        <updated>2021-06-24T01:51:44.878Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) synthesize realistic images from
random latent vectors. Although manipulating the latent vectors controls the
synthesized outputs, editing real images with GANs suffers from i)
time-consuming optimization for projecting real images to the latent vectors,
ii) or inaccurate embedding through an encoder. We propose StyleMapGAN: the
intermediate latent space has spatial dimensions, and a spatially variant
modulation replaces AdaIN. It makes the embedding through an encoder more
accurate than existing optimization-based methods while maintaining the
properties of GANs. Experimental results demonstrate that our method
significantly outperforms state-of-the-art models in various image manipulation
tasks such as local editing and image interpolation. Last but not least,
conventional editing methods on GANs are still valid on our StyleMapGAN. Source
code is available at https://github.com/naver-ai/StyleMapGAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyunsu Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yunjey Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Junho Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1"&gt;Sungjoo Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uh_Y/0/1/0/all/0/1"&gt;Youngjung Uh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tractable structured natural gradient descent using local parameterizations. (arXiv:2102.07405v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07405</id>
        <link href="http://arxiv.org/abs/2102.07405"/>
        <updated>2021-06-24T01:51:44.872Z</updated>
        <summary type="html"><![CDATA[Natural-gradient descent on structured parameter spaces (e.g., low-rank
covariances) is computationally challenging due to complicated inverse
Fisher-matrix computations. We address this issue for optimization, inference,
and search problems by using \emph{local-parameter coordinates}. Our method
generalizes an existing evolutionary-strategy method, recovers Newton and
Riemannian-gradient methods as special cases, and also yields new tractable
natural-gradient algorithms for learning flexible covariance structures of
Gaussian and Wishart-based distributions via \emph{matrix groups}. We show
results on a range of applications on deep learning, variational inference, and
evolution strategies. Our work opens a new direction for scalable structured
geometric methods via local parameterizations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nielsen_F/0/1/0/all/0/1"&gt;Frank Nielsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mohammad Emtiyaz Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schmidt_M/0/1/0/all/0/1"&gt;Mark Schmidt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modality Attention and Sampling Enables Deep Learning with Heterogeneous Marker Combinations in Fluorescence Microscopy. (arXiv:2008.12380v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.12380</id>
        <link href="http://arxiv.org/abs/2008.12380"/>
        <updated>2021-06-24T01:51:44.867Z</updated>
        <summary type="html"><![CDATA[Fluorescence microscopy allows for a detailed inspection of cells, cellular
networks, and anatomical landmarks by staining with a variety of
carefully-selected markers visualized as color channels. Quantitative
characterization of structures in acquired images often relies on automatic
image analysis methods. Despite the success of deep learning methods in other
vision applications, their potential for fluorescence image analysis remains
underexploited. One reason lies in the considerable workload required to train
accurate models, which are normally specific for a given combination of
markers, and therefore applicable to a very restricted number of experimental
settings. We herein propose Marker Sampling and Excite, a neural network
approach with a modality sampling strategy and a novel attention module that
together enable (i) flexible training with heterogeneous datasets with
combinations of markers and (ii) successful utility of learned models on
arbitrary subsets of markers prospectively. We show that our single neural
network solution performs comparably to an upper bound scenario where an
ensemble of many networks is na\"ively trained for each possible marker
combination separately. In addition, we demonstrate the feasibility of this
framework in high-throughput biological analysis by revising a recent
quantitative characterization of bone marrow vasculature in 3D confocal
microscopy datasets and further confirm the validity of our approach on an
additional, significantly different dataset of microvessels in fetal liver
tissues. Not only can our work substantially ameliorate the use of deep
learning in fluorescence microscopy analysis, but it can also be utilized in
other fields with incomplete data acquisitions and missing modalities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gomariz_A/0/1/0/all/0/1"&gt;Alvaro Gomariz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Portenier_T/0/1/0/all/0/1"&gt;Tiziano Portenier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Helbling_P/0/1/0/all/0/1"&gt;Patrick M. Helbling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Isringhausen_S/0/1/0/all/0/1"&gt;Stephan Isringhausen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suessbier_U/0/1/0/all/0/1"&gt;Ute Suessbier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nombela_Arrieta_C/0/1/0/all/0/1"&gt;C&amp;#xe9;sar Nombela-Arrieta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goksel_O/0/1/0/all/0/1"&gt;Orcun Goksel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bregman Gradient Policy Optimization. (arXiv:2106.12112v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12112</id>
        <link href="http://arxiv.org/abs/2106.12112"/>
        <updated>2021-06-24T01:51:44.852Z</updated>
        <summary type="html"><![CDATA[In this paper, we design a novel Bregman gradient policy optimization
framework for reinforcement learning based on Bregman divergences and momentum
techniques. Specifically, we propose a Bregman gradient policy optimization
(BGPO) algorithm based on the basic momentum technique and mirror descent
iteration. At the same time, we present an accelerated Bregman gradient policy
optimization (VR-BGPO) algorithm based on a momentum variance-reduced
technique. Moreover, we introduce a convergence analysis framework for our
Bregman gradient policy optimization under the nonconvex setting. Specifically,
we prove that BGPO achieves the sample complexity of $\tilde{O}(\epsilon^{-4})$
for finding $\epsilon$-stationary point only requiring one trajectory at each
iteration, and VR-BGPO reaches the best known sample complexity of
$\tilde{O}(\epsilon^{-3})$ for finding an $\epsilon$-stationary point, which
also only requires one trajectory at each iteration. In particular, by using
different Bregman divergences, our methods unify many existing policy
optimization algorithms and their new variants such as the existing
(variance-reduced) policy gradient algorithms and (variance-reduced) natural
policy gradient algorithms. Extensive experimental results on multiple
reinforcement learning tasks demonstrate the efficiency of our new algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feihu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1"&gt;Shangqian Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Clustering on Dynamic Graphs with Recurrent Graph Neural Networks. (arXiv:2012.08740v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08740</id>
        <link href="http://arxiv.org/abs/2012.08740"/>
        <updated>2021-06-24T01:51:44.835Z</updated>
        <summary type="html"><![CDATA[We study the problem of clustering nodes in a dynamic graph, where the
connections between nodes and nodes' cluster memberships may change over time,
e.g., due to community migration. We first propose a dynamic stochastic block
model that captures these changes, and a simple decay-based clustering
algorithm that clusters nodes based on weighted connections between them, where
the weight decreases at a fixed rate over time. This decay rate can then be
interpreted as signifying the importance of including historical connection
information in the clustering. However, the optimal decay rate may differ for
clusters with different rates of turnover. We characterize the optimal decay
rate for each cluster and propose a clustering method that achieves almost
exact recovery of the true clusters. We then demonstrate the efficacy of our
clustering algorithm with optimized decay rates on simulated graph data.
Recurrent neural networks (RNNs), a popular algorithm for sequence learning,
use a similar decay-based method, and we use this insight to propose two new
RNN-GCN (graph convolutional network) architectures for semi-supervised graph
clustering. We finally demonstrate that the proposed architectures perform well
on real data compared to state-of-the-art graph clustering algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yuhang Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joe_Wong_C/0/1/0/all/0/1"&gt;Carlee Joe-Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Augmented Tensor Decomposition with Stochastic Optimization. (arXiv:2106.07900v2 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07900</id>
        <link href="http://arxiv.org/abs/2106.07900"/>
        <updated>2021-06-24T01:51:44.828Z</updated>
        <summary type="html"><![CDATA[Tensor decompositions are powerful tools for dimensionality reduction and
feature interpretation of multidimensional data such as signals. Existing
tensor decomposition objectives (e.g., Frobenius norm) are designed for fitting
raw data under statistical assumptions, which may not align with downstream
classification tasks. Also, real-world tensor data are usually high-ordered and
have large dimensions with millions or billions of entries. Thus, it is
expensive to decompose the whole tensor with traditional algorithms. In
practice, raw tensor data also contains redundant information while data
augmentation techniques may be used to smooth out noise in samples. This paper
addresses the above challenges by proposing augmented tensor decomposition
(ATD), which effectively incorporates data augmentations to boost downstream
classification. To reduce the memory footprint of the decomposition, we propose
a stochastic algorithm that updates the factor matrices in a batch fashion. We
evaluate ATD on multiple signal datasets. It shows comparable or better
performance (e.g., up to 15% in accuracy) over self-supervised and autoencoder
baselines with less than 5% of model parameters, achieves 0.6% ~ 1.3% accuracy
gain over other tensor-based baselines, and reduces the memory footprint by 9X
when compared to standard tensor decomposition algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chaoqi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Qian_C/0/1/0/all/0/1"&gt;Cheng Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Singh_N/0/1/0/all/0/1"&gt;Navjot Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Cao Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Westover_M/0/1/0/all/0/1"&gt;M Brandon Westover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Solomonik_E/0/1/0/all/0/1"&gt;Edgar Solomonik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jimeng Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SketchEmbedNet: Learning Novel Concepts by Imitating Drawings. (arXiv:2009.04806v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04806</id>
        <link href="http://arxiv.org/abs/2009.04806"/>
        <updated>2021-06-24T01:51:44.821Z</updated>
        <summary type="html"><![CDATA[Sketch drawings capture the salient information of visual concepts. Previous
work has shown that neural networks are capable of producing sketches of
natural objects drawn from a small number of classes. While earlier approaches
focus on generation quality or retrieval, we explore properties of image
representations learned by training a model to produce sketches of images. We
show that this generative, class-agnostic model produces informative embeddings
of images from novel examples, classes, and even novel datasets in a few-shot
setting. Additionally, we find that these learned representations exhibit
interesting structure and compositionality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1"&gt;Alexander Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1"&gt;Mengye Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1"&gt;Richard S. Zemel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GANMEX: One-vs-One Attributions Guided by GAN-based Counterfactual Explanation Baselines. (arXiv:2011.06015v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.06015</id>
        <link href="http://arxiv.org/abs/2011.06015"/>
        <updated>2021-06-24T01:51:44.816Z</updated>
        <summary type="html"><![CDATA[Attribution methods have been shown as promising approaches for identifying
key features that led to learned model predictions. While most existing
attribution methods rely on a baseline input for performing feature
perturbations, limited research has been conducted to address the baseline
selection issues. Poor choices of baselines limit the ability of one-vs-one
(1-vs-1) explanations for multi-class classifiers, which means the attribution
methods were not able to explain why an input belongs to its original class but
not the other specified target class. 1-vs-1 explanation is crucial when
certain classes are more similar than others, e.g. two bird types among
multiple animals, by focusing on key differentiating features rather than
shared features across classes. In this paper, we present GAN-based Model
EXplainability (GANMEX), a novel approach applying Generative Adversarial
Networks (GAN) by incorporating the to-be-explained classifier as part of the
adversarial networks. Our approach effectively selects the counterfactual
baseline as the closest realistic sample belong to the target class, which
allows attribution methods to provide true 1-vs-1 explanations. We showed that
GANMEX baselines improved the saliency maps and led to stronger performance on
perturbation-based evaluation metrics over the existing baselines. Existing
attribution results are known for being insensitive to model randomization, and
we demonstrated that GANMEX baselines led to better outcome under the cascading
randomization of the model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shih_S/0/1/0/all/0/1"&gt;Sheng-Min Shih&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tien_P/0/1/0/all/0/1"&gt;Pin-Ju Tien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karnin_Z/0/1/0/all/0/1"&gt;Zohar Karnin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Perceiver: General Perception with Iterative Attention. (arXiv:2103.03206v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03206</id>
        <link href="http://arxiv.org/abs/2103.03206"/>
        <updated>2021-06-24T01:51:44.801Z</updated>
        <summary type="html"><![CDATA[Biological systems perceive the world by simultaneously processing
high-dimensional inputs from modalities as diverse as vision, audition, touch,
proprioception, etc. The perception models used in deep learning on the other
hand are designed for individual modalities, often relying on domain-specific
assumptions such as the local grid structures exploited by virtually all
existing vision models. These priors introduce helpful inductive biases, but
also lock models to individual modalities. In this paper we introduce the
Perceiver - a model that builds upon Transformers and hence makes few
architectural assumptions about the relationship between its inputs, but that
also scales to hundreds of thousands of inputs, like ConvNets. The model
leverages an asymmetric attention mechanism to iteratively distill inputs into
a tight latent bottleneck, allowing it to scale to handle very large inputs. We
show that this architecture is competitive with or outperforms strong,
specialized models on classification tasks across various modalities: images,
point clouds, audio, video, and video+audio. The Perceiver obtains performance
comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly
attending to 50,000 pixels. It is also competitive in all modalities in
AudioSet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1"&gt;Andrew Jaegle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gimeno_F/0/1/0/all/0/1"&gt;Felix Gimeno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brock_A/0/1/0/all/0/1"&gt;Andrew Brock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1"&gt;Andrew Zisserman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1"&gt;Oriol Vinyals&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1"&gt;Joao Carreira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PHEW: Constructing Sparse Networks that Learn Fast and Generalize Well without Training Data. (arXiv:2010.11354v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11354</id>
        <link href="http://arxiv.org/abs/2010.11354"/>
        <updated>2021-06-24T01:51:44.795Z</updated>
        <summary type="html"><![CDATA[Methods that sparsify a network at initialization are important in practice
because they greatly improve the efficiency of both learning and inference. Our
work is based on a recently proposed decomposition of the Neural Tangent Kernel
(NTK) that has decoupled the dynamics of the training process into a
data-dependent component and an architecture-dependent kernel - the latter
referred to as Path Kernel. That work has shown how to design sparse neural
networks for faster convergence, without any training data, using the
Synflow-L2 algorithm. We first show that even though Synflow-L2 is optimal in
terms of convergence, for a given network density, it results in sub-networks
with "bottleneck" (narrow) layers - leading to poor performance as compared to
other data-agnostic methods that use the same number of parameters. Then we
propose a new method to construct sparse networks, without any training data,
referred to as Paths with Higher-Edge Weights (PHEW). PHEW is a probabilistic
network formation method based on biased random walks that only depends on the
initial weights. It has similar path kernel properties as Synflow-L2 but it
generates much wider layers, resulting in better generalization and
performance. PHEW achieves significant improvements over the data-independent
SynFlow and SynFlow-L2 methods at a wide range of network densities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patil_S/0/1/0/all/0/1"&gt;Shreyas Malakarjun Patil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dovrolis_C/0/1/0/all/0/1"&gt;Constantine Dovrolis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing. (arXiv:2104.14754v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14754</id>
        <link href="http://arxiv.org/abs/2104.14754"/>
        <updated>2021-06-24T01:51:44.790Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) synthesize realistic images from
random latent vectors. Although manipulating the latent vectors controls the
synthesized outputs, editing real images with GANs suffers from i)
time-consuming optimization for projecting real images to the latent vectors,
ii) or inaccurate embedding through an encoder. We propose StyleMapGAN: the
intermediate latent space has spatial dimensions, and a spatially variant
modulation replaces AdaIN. It makes the embedding through an encoder more
accurate than existing optimization-based methods while maintaining the
properties of GANs. Experimental results demonstrate that our method
significantly outperforms state-of-the-art models in various image manipulation
tasks such as local editing and image interpolation. Last but not least,
conventional editing methods on GANs are still valid on our StyleMapGAN. Source
code is available at https://github.com/naver-ai/StyleMapGAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyunsu Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yunjey Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Junho Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1"&gt;Sungjoo Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uh_Y/0/1/0/all/0/1"&gt;Youngjung Uh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Taming GANs with Lookahead-Minmax. (arXiv:2006.14567v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.14567</id>
        <link href="http://arxiv.org/abs/2006.14567"/>
        <updated>2021-06-24T01:51:44.784Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks are notoriously challenging to train. The
underlying minmax optimization is highly susceptible to the variance of the
stochastic gradient and the rotational component of the associated game vector
field. To tackle these challenges, we propose the Lookahead algorithm for
minmax optimization, originally developed for single objective minimization
only. The backtracking step of our Lookahead-minmax naturally handles the
rotational game dynamics, a property which was identified to be key for
enabling gradient ascent descent methods to converge on challenging examples
often analyzed in the literature. Moreover, it implicitly handles high variance
without using large mini-batches, known to be essential for reaching state of
the art performance. Experimental results on MNIST, SVHN, CIFAR-10, and
ImageNet demonstrate a clear advantage of combining Lookahead-minmax with Adam
or extragradient, in terms of performance and improved stability, for
negligible memory and computational cost. Using 30-fold fewer parameters and
16-fold smaller minibatches we outperform the reported performance of the
class-dependent BigGAN on CIFAR-10 by obtaining FID of 12.19 without using the
class labels, bringing state-of-the-art GAN training within reach of common
computational resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chavdarova_T/0/1/0/all/0/1"&gt;Tatjana Chavdarova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pagliardini_M/0/1/0/all/0/1"&gt;Matteo Pagliardini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Stich_S/0/1/0/all/0/1"&gt;Sebastian U. Stich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Fleuret_F/0/1/0/all/0/1"&gt;Francois Fleuret&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maximum Likelihood Training of Score-Based Diffusion Models. (arXiv:2101.09258v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09258</id>
        <link href="http://arxiv.org/abs/2101.09258"/>
        <updated>2021-06-24T01:51:44.779Z</updated>
        <summary type="html"><![CDATA[Score-based diffusion models synthesize samples by reversing a stochastic
process that diffuses data to noise, and are trained by minimizing a weighted
combination of score matching losses. The log-likelihood of score-based models
can be tractably computed through a connection to continuous normalizing flows,
but log-likelihood is not directly optimized by the weighted combination of
score matching losses. We show that for a specific weighting scheme, the
objective upper bounds the negative log-likelihood, thus enabling approximate
maximum likelihood training of score-based models. We empirically observe that
maximum likelihood training consistently improves the likelihood of score-based
models across multiple datasets, stochastic processes, and model architectures.
Our best models achieve negative log-likelihoods of 2.74 and 3.76 bits/dim on
CIFAR-10 and ImageNet 32x32, outperforming autoregressive models on these
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Durkan_C/0/1/0/all/0/1"&gt;Conor Durkan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Murray_I/0/1/0/all/0/1"&gt;Iain Murray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ermon_S/0/1/0/all/0/1"&gt;Stefano Ermon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ShapeMOD: Macro Operation Discovery for 3D Shape Programs. (arXiv:2104.06392v2 [cs.GR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06392</id>
        <link href="http://arxiv.org/abs/2104.06392"/>
        <updated>2021-06-24T01:51:44.766Z</updated>
        <summary type="html"><![CDATA[A popular way to create detailed yet easily controllable 3D shapes is via
procedural modeling, i.e. generating geometry using programs. Such programs
consist of a series of instructions along with their associated parameter
values. To fully realize the benefits of this representation, a shape program
should be compact and only expose degrees of freedom that allow for meaningful
manipulation of output geometry. One way to achieve this goal is to design
higher-level macro operators that, when executed, expand into a series of
commands from the base shape modeling language. However, manually authoring
such macros, much like shape programs themselves, is difficult and largely
restricted to domain experts. In this paper, we present ShapeMOD, an algorithm
for automatically discovering macros that are useful across large datasets of
3D shape programs. ShapeMOD operates on shape programs expressed in an
imperative, statement-based language. It is designed to discover macros that
make programs more compact by minimizing the number of function calls and free
parameters required to represent an input shape collection. We run ShapeMOD on
multiple collections of programs expressed in a domain-specific language for 3D
shape structures. We show that it automatically discovers a concise set of
macros that abstract out common structural and parametric patterns that
generalize over large shape collections. We also demonstrate that the macros
found by ShapeMOD improve performance on downstream tasks including shape
generative modeling and inferring programs from point clouds. Finally, we
conduct a user study that indicates that ShapeMOD's discovered macros make
interactive shape editing more efficient.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1"&gt;R. Kenny Jones&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charatan_D/0/1/0/all/0/1"&gt;David Charatan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guerrero_P/0/1/0/all/0/1"&gt;Paul Guerrero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1"&gt;Niloy J. Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1"&gt;Daniel Ritchie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Entropic Out-of-Distribution Detection using Isometric Distances and the Minimum Distance Score. (arXiv:2105.14399v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14399</id>
        <link href="http://arxiv.org/abs/2105.14399"/>
        <updated>2021-06-24T01:51:44.760Z</updated>
        <summary type="html"><![CDATA[Current out-of-distribution detection approaches usually present special
requirements (e.g., collecting outlier data and hyperparameter validation) and
produce side effects (classification accuracy drop and slow/inefficient
inferences). Recently, entropic out-of-distribution detection has been proposed
as a seamless approach (i.e., a solution that avoids all the previously
mentioned drawbacks). The entropic out-of-distribution detection solution
comprises the IsoMax loss for training and the entropic score for
out-of-distribution detection. The IsoMax loss works as a SoftMax loss drop-in
replacement because swapping the SoftMax loss with the IsoMax loss requires no
changes in the model's architecture or training procedures/hyperparameters. In
this paper, we propose to perform what we call an isometrization of the
distances used in the IsoMax loss. Additionally, we propose to replace the
entropic score with the minimum distance score. Our experiments showed that
these simple modifications increase out-of-distribution detection performance
while keeping the solution seamless.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1"&gt;David Mac&amp;#xea;do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1"&gt;Teresa Ludermir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SIGL: Securing Software Installations Through Deep Graph Learning. (arXiv:2008.11533v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.11533</id>
        <link href="http://arxiv.org/abs/2008.11533"/>
        <updated>2021-06-24T01:51:44.754Z</updated>
        <summary type="html"><![CDATA[Many users implicitly assume that software can only be exploited after it is
installed. However, recent supply-chain attacks demonstrate that application
integrity must be ensured during installation itself. We introduce SIGL, a new
tool for detecting malicious behavior during software installation. SIGL
collects traces of system call activity, building a data provenance graph that
it analyzes using a novel autoencoder architecture with a graph long short-term
memory network (graph LSTM) for the encoder and a standard multilayer
perceptron for the decoder. SIGL flags suspicious installations as well as the
specific installation-time processes that are likely to be malicious. Using a
test corpus of 625 malicious installers containing real-world malware, we
demonstrate that SIGL has a detection accuracy of 96%, outperforming similar
systems from industry and academia by up to 87% in precision and recall and 45%
in accuracy. We also demonstrate that SIGL can pinpoint the processes most
likely to have triggered malicious behavior, works on different audit platforms
and operating systems, and is robust to training data contamination and
adversarial attack. It can be used with application-specific models, even in
the presence of new software versions, as well as application-agnostic
meta-models that encompass a wide range of applications and installers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xueyuan Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xiao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pasquier_T/0/1/0/all/0/1"&gt;Thomas Pasquier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Ding Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rhee_J/0/1/0/all/0/1"&gt;Junghwan Rhee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mickens_J/0/1/0/all/0/1"&gt;James Mickens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1"&gt;Margo Seltzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Entropic Out-of-Distribution Detection using Isometric Distances and the Minimum Distance Score. (arXiv:2105.14399v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14399</id>
        <link href="http://arxiv.org/abs/2105.14399"/>
        <updated>2021-06-24T01:51:44.749Z</updated>
        <summary type="html"><![CDATA[Current out-of-distribution detection approaches usually present special
requirements (e.g., collecting outlier data and hyperparameter validation) and
produce side effects (classification accuracy drop and slow/inefficient
inferences). Recently, entropic out-of-distribution detection has been proposed
as a seamless approach (i.e., a solution that avoids all the previously
mentioned drawbacks). The entropic out-of-distribution detection solution
comprises the IsoMax loss for training and the entropic score for
out-of-distribution detection. The IsoMax loss works as a SoftMax loss drop-in
replacement because swapping the SoftMax loss with the IsoMax loss requires no
changes in the model's architecture or training procedures/hyperparameters. In
this paper, we propose to perform what we call an isometrization of the
distances used in the IsoMax loss. Additionally, we propose to replace the
entropic score with the minimum distance score. Our experiments showed that
these simple modifications increase out-of-distribution detection performance
while keeping the solution seamless.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1"&gt;David Mac&amp;#xea;do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1"&gt;Teresa Ludermir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Thompson Sampling. (arXiv:2102.06129v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06129</id>
        <link href="http://arxiv.org/abs/2102.06129"/>
        <updated>2021-06-24T01:51:44.744Z</updated>
        <summary type="html"><![CDATA[Efficient exploration in bandits is a fundamental online learning problem. We
propose a variant of Thompson sampling that learns to explore better as it
interacts with bandit instances drawn from an unknown prior. The algorithm
meta-learns the prior and thus we call it MetaTS. We propose several efficient
implementations of MetaTS and analyze it in Gaussian bandits. Our analysis
shows the benefit of meta-learning and is of a broader interest, because we
derive a novel prior-dependent Bayes regret bound for Thompson sampling. Our
theory is complemented by empirical evaluation, which shows that MetaTS quickly
adapts to the unknown prior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1"&gt;Branislav Kveton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konobeev_M/0/1/0/all/0/1"&gt;Mikhail Konobeev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1"&gt;Manzil Zaheer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1"&gt;Chih-wei Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mladenov_M/0/1/0/all/0/1"&gt;Martin Mladenov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boutilier_C/0/1/0/all/0/1"&gt;Craig Boutilier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1"&gt;Csaba Szepesvari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clustering of check-in sequences using the mixture Markov chain process. (arXiv:2106.12039v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.12039</id>
        <link href="http://arxiv.org/abs/2106.12039"/>
        <updated>2021-06-24T01:51:44.729Z</updated>
        <summary type="html"><![CDATA[This work is devoted to the clustering of check-in sequences from a geosocial
network. We used the mixture Markov chain process as a mathematical model for
time-dependent types of data. For clustering, we adjusted the
Expectation-Maximization (EM) algorithm. As a result, we obtained highly
detailed communities (clusters) of users of the now defunct geosocial network,
Weeplaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shmileva_E/0/1/0/all/0/1"&gt;Elena Shmileva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarzhan_V/0/1/0/all/0/1"&gt;Viktor Sarzhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A LightGBM based Forecasting of Dominant Wave Periods in Oceanic Waters. (arXiv:2105.08721v3 [physics.ao-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08721</id>
        <link href="http://arxiv.org/abs/2105.08721"/>
        <updated>2021-06-24T01:51:44.724Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a Light Gradient Boosting (LightGBM) to forecast
dominant wave periods in oceanic waters. First, we use the data collected from
CDIP buoys and apply various data filtering methods. The data filtering methods
allow us to obtain a high-quality dataset for training and validation purposes.
We then extract various wave-based features like wave heights, periods,
skewness, kurtosis, etc., and atmospheric features like humidity, pressure, and
air temperature for the buoys. Afterward, we train algorithms that use LightGBM
and Extra Trees through a hv-block cross-validation scheme to forecast dominant
wave periods for up to 30 days ahead. LightGBM has the R2 score of 0.94, 0.94,
and 0.94 for 1-day ahead, 15-day ahead, and 30-day ahead prediction. Similarly,
Extra Trees (ET) has an R2 score of 0.88, 0.86, and 0.85 for 1-day ahead,
15-day ahead, and 30 day ahead prediction. In case of the test dataset,
LightGBM has R2 score of 0.94, 0.94, and 0.94 for 1-day ahead, 15-day ahead and
30-day ahead prediction. ET has R2 score of 0.88, 0.86, and 0.85 for 1-day
ahead, 15-day ahead, and 30-day ahead prediction. A similar R2 score for both
training and the test dataset suggests that the machine learning models
developed in this paper are robust. Since the LightGBM algorithm outperforms ET
for all the windows tested, it is taken as the final algorithm. Note that the
performance of both methods does not decrease significantly as the forecast
horizon increases. Likewise, the proposed method outperforms the numerical
approaches included in this paper in the test dataset. For 1 day ahead
prediction, the proposed algorithm has SI, Bias, CC, and RMSE of 0.09, 0.00,
0.97, and 1.78 compared to 0.268, 0.40, 0.63, and 2.18 for the European Centre
for Medium-range Weather Forecasts (ECMWF) model, which outperforms all the
other methods in the test dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Pokhrel_P/0/1/0/all/0/1"&gt;Pujan Pokhrel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Hoque_M/0/1/0/all/0/1"&gt;Md Tamjidul Hoque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Abdelguerfi_M/0/1/0/all/0/1"&gt;Mahdi Abdelguerfi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finding simplicity: unsupervised discovery of features, patterns, and order parameters via shift-invariant variational autoencoders. (arXiv:2106.12472v1 [cond-mat.dis-nn])]]></title>
        <id>http://arxiv.org/abs/2106.12472</id>
        <link href="http://arxiv.org/abs/2106.12472"/>
        <updated>2021-06-24T01:51:44.718Z</updated>
        <summary type="html"><![CDATA[Recent advances in scanning tunneling and transmission electron microscopies
(STM and STEM) have allowed routine generation of large volumes of imaging data
containing information on the structure and functionality of materials. The
experimental data sets contain signatures of long-range phenomena such as
physical order parameter fields, polarization and strain gradients in STEM, or
standing electronic waves and carrier-mediated exchange interactions in STM,
all superimposed onto scanning system distortions and gradual changes of
contrast due to drift and/or mis-tilt effects. Correspondingly, while the human
eye can readily identify certain patterns in the images such as lattice
periodicities, repeating structural elements, or microstructures, their
automatic extraction and classification are highly non-trivial and universal
pathways to accomplish such analyses are absent. We pose that the most
distinctive elements of the patterns observed in STM and (S)TEM images are
similarity and (almost-) periodicity, behaviors stemming directly from the
parsimony of elementary atomic structures, superimposed on the gradual changes
reflective of order parameter distributions. However, the discovery of these
elements via global Fourier methods is non-trivial due to variability and lack
of ideal discrete translation symmetry. To address this problem, we develop
shift-invariant variational autoencoders (shift-VAE) that allow disentangling
characteristic repeating features in the images, their variations, and shifts
inevitable for random sampling of image space. Shift-VAEs balance the
uncertainty in the position of the object of interest with the uncertainty in
shape reconstruction. This approach is illustrated for model 1D data, and
further extended to synthetic and experimental STM and STEM 2D data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Ziatdinov_M/0/1/0/all/0/1"&gt;Maxim Ziatdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Wong_C/0/1/0/all/0/1"&gt;Chun Yin Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Kalinin_S/0/1/0/all/0/1"&gt;Sergei V. Kalinin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HDR Environment Map Estimation for Real-Time Augmented Reality. (arXiv:2011.10687v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10687</id>
        <link href="http://arxiv.org/abs/2011.10687"/>
        <updated>2021-06-24T01:51:44.713Z</updated>
        <summary type="html"><![CDATA[We present a method to estimate an HDR environment map from a narrow
field-of-view LDR camera image in real-time. This enables perceptually
appealing reflections and shading on virtual objects of any material finish,
from mirror to diffuse, rendered into a real physical environment using
augmented reality. Our method is based on our efficient convolutional neural
network architecture, EnvMapNet, trained end-to-end with two novel losses,
ProjectionLoss for the generated image, and ClusterLoss for adversarial
training. Through qualitative and quantitative comparison to state-of-the-art
methods, we demonstrate that our algorithm reduces the directional error of
estimated light sources by more than 50%, and achieves 3.7 times lower Frechet
Inception Distance (FID). We further showcase a mobile application that is able
to run our neural network model in under 9 ms on an iPhone XS, and render in
real-time, visually coherent virtual objects in previously unseen real-world
environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Somanath_G/0/1/0/all/0/1"&gt;Gowri Somanath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurz_D/0/1/0/all/0/1"&gt;Daniel Kurz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Speech Enhancement using Dynamical Variational Auto-Encoders. (arXiv:2106.12271v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.12271</id>
        <link href="http://arxiv.org/abs/2106.12271"/>
        <updated>2021-06-24T01:51:44.707Z</updated>
        <summary type="html"><![CDATA[Dynamical variational auto-encoders (DVAEs) are a class of deep generative
models with latent variables, dedicated to time series data modeling. DVAEs can
be considered as extensions of the variational autoencoder (VAE) that include
the modeling of temporal dependencies between successive observed and/or latent
vectors in data sequences. Previous work has shown the interest of DVAEs and
their better performance over the VAE for speech signals (spectrogram)
modeling. Independently, the VAE has been successfully applied to speech
enhancement in noise, in an unsupervised noise-agnostic set-up that does not
require the use of a parallel dataset of clean and noisy speech samples for
training, but only requires clean speech signals. In this paper, we extend
those works to DVAE-based single-channel unsupervised speech enhancement, hence
exploiting both speech signals unsupervised representation learning and
dynamics modeling. We propose an unsupervised speech enhancement algorithm
based on the most general form of DVAEs, that we then adapt to three specific
DVAE models to illustrate the versatility of the framework. More precisely, we
combine DVAE-based speech priors with a noise model based on nonnegative matrix
factorization, and we derive a variational expectation-maximization (VEM)
algorithm to perform speech enhancement. Experimental results show that the
proposed approach based on DVAEs outperforms its VAE counterpart and a
supervised speech enhancement baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bie_X/0/1/0/all/0/1"&gt;Xiaoyu Bie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leglaive_S/0/1/0/all/0/1"&gt;Simon Leglaive&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1"&gt;Xavier Alameda-Pineda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Girin_L/0/1/0/all/0/1"&gt;Laurent Girin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HDR Environment Map Estimation for Real-Time Augmented Reality. (arXiv:2011.10687v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10687</id>
        <link href="http://arxiv.org/abs/2011.10687"/>
        <updated>2021-06-24T01:51:44.693Z</updated>
        <summary type="html"><![CDATA[We present a method to estimate an HDR environment map from a narrow
field-of-view LDR camera image in real-time. This enables perceptually
appealing reflections and shading on virtual objects of any material finish,
from mirror to diffuse, rendered into a real physical environment using
augmented reality. Our method is based on our efficient convolutional neural
network architecture, EnvMapNet, trained end-to-end with two novel losses,
ProjectionLoss for the generated image, and ClusterLoss for adversarial
training. Through qualitative and quantitative comparison to state-of-the-art
methods, we demonstrate that our algorithm reduces the directional error of
estimated light sources by more than 50%, and achieves 3.7 times lower Frechet
Inception Distance (FID). We further showcase a mobile application that is able
to run our neural network model in under 9 ms on an iPhone XS, and render in
real-time, visually coherent virtual objects in previously unseen real-world
environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Somanath_G/0/1/0/all/0/1"&gt;Gowri Somanath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurz_D/0/1/0/all/0/1"&gt;Daniel Kurz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Algorithm Based on One Monocular Video Delivers Highly Valid and Reliable Gait Parameters. (arXiv:2008.08045v5 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.08045</id>
        <link href="http://arxiv.org/abs/2008.08045"/>
        <updated>2021-06-24T01:51:44.688Z</updated>
        <summary type="html"><![CDATA[Despite its paramount importance for manifold use cases (e.g., in the health
care industry, sports, rehabilitation and fitness assessment), sufficiently
valid and reliable gait parameter measurement is still limited to high-tech
gait laboratories mostly. Here, we demonstrate the excellent validity and
test-retest repeatability of a novel gait assessment system which is built upon
modern convolutional neural networks to extract three-dimensional skeleton
joints from monocular frontal-view videos of walking humans. The validity study
is based on a comparison to the GAITRite pressure-sensitive walkway system. All
measured gait parameters (gait speed, cadence, step length and step time)
showed excellent concurrent validity for multiple walk trials at normal and
fast gait speeds. The test-retest-repeatability is on the same level as the
GAITRite system. In conclusion, we are convinced that our results can pave the
way for cost, space and operationally effective gait analysis in broad
mainstream applications. Most sensor-based systems are costly, must be operated
by extensively trained personnel (e.g., motion capture systems) or - even if
not quite as costly - still possess considerable complexity (e.g., wearable
sensors). In contrast, a video sufficient for the assessment method presented
here can be obtained by anyone, without much training, via a smartphone camera.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Azhand_D/0/1/0/all/0/1"&gt;Dr. Arash Azhand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rabe_D/0/1/0/all/0/1"&gt;Dr. Sophie Rabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Muller_D/0/1/0/all/0/1"&gt;Dr. Swantje M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sattler_I/0/1/0/all/0/1"&gt;Igor Sattler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Steinert_D/0/1/0/all/0/1"&gt;Dr. Anika Steinert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Flows with Invertible Attentions. (arXiv:2106.03959v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03959</id>
        <link href="http://arxiv.org/abs/2106.03959"/>
        <updated>2021-06-24T01:51:44.683Z</updated>
        <summary type="html"><![CDATA[Flow-based generative models have shown excellent ability to explicitly learn
the probability density function of data via a sequence of invertible
transformations. Yet, modeling long-range dependencies over normalizing flows
remains understudied. To fill the gap, in this paper, we introduce two types of
invertible attention mechanisms for generative flow models. To be precise, we
propose map-based and scaled dot-product attention for unconditional and
conditional generative flow models. The key idea is to exploit split-based
attention mechanisms to learn the attention weights and input representations
on every two splits of flow feature maps. Our method provides invertible
attention modules with tractable Jacobian determinants, enabling seamless
integration of it at any positions of the flow-based models. The proposed
attention mechanism can model the global data dependencies, leading to more
comprehensive flow models. Evaluation on multiple generation tasks demonstrates
that the introduced attention flow idea results in efficient flow models and
compares favorably against the state-of-the-art unconditional and conditional
generative flow methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sukthanker_R/0/1/0/all/0/1"&gt;Rhea Sanjay Sukthanker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhiwu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Suryansh Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1"&gt;Radu Timofte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BanditMF: Multi-Armed Bandit Based Matrix Factorization Recommender System. (arXiv:2106.10898v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10898</id>
        <link href="http://arxiv.org/abs/2106.10898"/>
        <updated>2021-06-24T01:51:44.678Z</updated>
        <summary type="html"><![CDATA[Multi-armed bandits (MAB) provide a principled online learning approach to
attain the balance between exploration and exploitation. Due to the superior
performance and low feedback learning without the learning to act in multiple
situations, Multi-armed Bandits drawing widespread attention in applications
ranging such as recommender systems. Likewise, within the recommender system,
collaborative filtering (CF) is arguably the earliest and most influential
method in the recommender system. Crucially, new users and an ever-changing
pool of recommended items are the challenges that recommender systems need to
address. For collaborative filtering, the classical method is training the
model offline, then perform the online testing, but this approach can no longer
handle the dynamic changes in user preferences which is the so-called cold
start. So how to effectively recommend items to users in the absence of
effective information? To address the aforementioned problems, a multi-armed
bandit based collaborative filtering recommender system has been proposed,
named BanditMF. BanditMF is designed to address two challenges in the
multi-armed bandits algorithm and collaborative filtering: (1) how to solve the
cold start problem for collaborative filtering under the condition of scarcity
of valid information, (2) how to solve the sub-optimal problem of bandit
algorithms in strong social relations domains caused by independently
estimating unknown parameters associated with each user and ignoring
correlations between users.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shenghao Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Test-time Collective Prediction. (arXiv:2106.12012v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12012</id>
        <link href="http://arxiv.org/abs/2106.12012"/>
        <updated>2021-06-24T01:51:44.673Z</updated>
        <summary type="html"><![CDATA[An increasingly common setting in machine learning involves multiple parties,
each with their own data, who want to jointly make predictions on future test
points. Agents wish to benefit from the collective expertise of the full set of
agents to make better predictions than they would individually, but may not be
willing to release their data or model parameters. In this work, we explore a
decentralized mechanism to make collective predictions at test time, leveraging
each agent's pre-trained model without relying on external validation, model
retraining, or data pooling. Our approach takes inspiration from the literature
in social science on human consensus-making. We analyze our mechanism
theoretically, showing that it converges to inverse meansquared-error (MSE)
weighting in the large-sample limit. To compute error bars on the collective
predictions we propose a decentralized Jackknife procedure that evaluates the
sensitivity of our mechanism to a single agent's prediction. Empirically, we
demonstrate that our scheme effectively combines models with differing quality
across the input space. The proposed consensus prediction achieves significant
gains over classical model averaging, and even outperforms weighted averaging
schemes that have access to additional validation data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mendler_Dunner_C/0/1/0/all/0/1"&gt;Celestine Mendler-D&amp;#xfc;nner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1"&gt;Wenshuo Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bates_S/0/1/0/all/0/1"&gt;Stephen Bates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time Outdoor Localization Using Radio Maps: A Deep Learning Approach. (arXiv:2106.12556v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12556</id>
        <link href="http://arxiv.org/abs/2106.12556"/>
        <updated>2021-06-24T01:51:44.668Z</updated>
        <summary type="html"><![CDATA[This paper deals with the problem of localization in a cellular network in a
dense urban scenario. Global Navigation Satellite Systems typically perform
poorly in urban environments, where the likelihood of line-of-sight conditions
between the devices and the satellites is low, and thus alternative
localization methods are required for good accuracy. We present a deep learning
method for localization, based merely on pathloss, which does not require any
increase in computation complexity at the user devices with respect to the
device standard operations, unlike methods that rely on time of arrival or
angle of arrival information. In a wireless network, user devices scan the base
station beacon slots and identify the few strongest base station signals for
handover and user-base station association purposes. In the proposed method,
the user to be localized simply reports such received signal strengths to a
central processing unit, which may be located in the cloud. For each base
station we have good approximation of the pathloss at every location in a dense
grid in the map. This approximation is provided by RadioUNet, a deep
learning-based simulator of pathloss functions in urban environment, that we
have previously proposed and published. Using the estimated pathloss radio maps
of all base stations and the corresponding reported signal strengths, the
proposed deep learning algorithm can extract a very accurate localization of
the user. The proposed method, called LocUNet, enjoys high robustness to
inaccuracies in the estimated radio maps. We demonstrate this by numerical
experiments, which obtain state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yapar_C/0/1/0/all/0/1"&gt;&amp;#xc7;a&amp;#x11f;kan Yapar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levie_R/0/1/0/all/0/1"&gt;Ron Levie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kutyniok_G/0/1/0/all/0/1"&gt;Gitta Kutyniok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caire_G/0/1/0/all/0/1"&gt;Giuseppe Caire&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpenML-Python: an extensible Python API for OpenML. (arXiv:1911.02490v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.02490</id>
        <link href="http://arxiv.org/abs/1911.02490"/>
        <updated>2021-06-24T01:51:44.653Z</updated>
        <summary type="html"><![CDATA[OpenML is an online platform for open science collaboration in machine
learning, used to share datasets and results of machine learning experiments.
In this paper we introduce OpenML-Python, a client API for Python, opening up
the OpenML platform for a wide range of Python-based tools. It provides easy
access to all datasets, tasks and experiments on OpenML from within Python. It
also provides functionality to conduct machine learning experiments, upload the
results to OpenML, and reproduce results which are stored on OpenML.
Furthermore, it comes with a scikit-learn plugin and a plugin mechanism to
easily integrate other machine learning libraries written in Python into the
OpenML ecosystem. Source code and documentation is available at
https://github.com/openml/openml-python/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feurer_M/0/1/0/all/0/1"&gt;Matthias Feurer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rijn_J/0/1/0/all/0/1"&gt;Jan N. van Rijn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadra_A/0/1/0/all/0/1"&gt;Arlind Kadra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gijsbers_P/0/1/0/all/0/1"&gt;Pieter Gijsbers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mallik_N/0/1/0/all/0/1"&gt;Neeratyoy Mallik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1"&gt;Sahithya Ravi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_A/0/1/0/all/0/1"&gt;Andreas M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vanschoren_J/0/1/0/all/0/1"&gt;Joaquin Vanschoren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1"&gt;Frank Hutter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VariTex: Variational Neural Face Textures. (arXiv:2104.05988v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05988</id>
        <link href="http://arxiv.org/abs/2104.05988"/>
        <updated>2021-06-24T01:51:44.648Z</updated>
        <summary type="html"><![CDATA[Deep generative models have recently demonstrated the ability to synthesize
photorealistic images of human faces with novel identities. A key challenge to
the wide applicability of such techniques is to provide independent control
over semantically meaningful parameters: appearance, head pose, face shape, and
facial expressions. In this paper, we propose VariTex - to the best of our
knowledge the first method that learns a variational latent feature space of
neural face textures, which allows sampling of novel identities. We combine
this generative model with a parametric face model and gain explicit control
over head pose and facial expressions. To generate images of complete human
heads, we propose an additive decoder that generates plausible additional
details such as hair. A novel training scheme enforces a pose independent
latent space and in consequence, allows learning of a one-to-many mapping
between latent codes and pose-conditioned exterior regions. The resulting
method can generate geometrically consistent images of novel identities
allowing fine-grained control over head pose, face shape, and facial
expressions, facilitating a broad range of downstream tasks, like sampling
novel identities, re-posing, expression transfer, and more.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Buhler_M/0/1/0/all/0/1"&gt;Marcel C. B&amp;#xfc;hler&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Meka_A/0/1/0/all/0/1"&gt;Abhimitra Meka&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Gengyan Li&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Beeler_T/0/1/0/all/0/1"&gt;Thabo Beeler&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1"&gt;Otmar Hilliges&lt;/a&gt; (1) ((1) ETH Zurich, (2) Google)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dissecting Supervised Constrastive Learning. (arXiv:2102.08817v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08817</id>
        <link href="http://arxiv.org/abs/2102.08817"/>
        <updated>2021-06-24T01:51:44.642Z</updated>
        <summary type="html"><![CDATA[Minimizing cross-entropy over the softmax scores of a linear map composed
with a high-capacity encoder is arguably the most popular choice for training
neural networks on supervised learning tasks. However, recent works show that
one can directly optimize the encoder instead, to obtain equally (or even more)
discriminative representations via a supervised variant of a contrastive
objective. In this work, we address the question whether there are fundamental
differences in the sought-for representation geometry in the output space of
the encoder at minimal loss. Specifically, we prove, under mild assumptions,
that both losses attain their minimum once the representations of each class
collapse to the vertices of a regular simplex, inscribed in a hypersphere. We
provide empirical evidence that this configuration is attained in practice and
that reaching a close-to-optimal state typically indicates good generalization
performance. Yet, the two losses show remarkably different optimization
behavior. The number of iterations required to perfectly fit to data scales
superlinearly with the amount of randomly flipped labels for the supervised
contrastive loss. This is in contrast to the approximately linear scaling
previously reported for networks trained with cross-entropy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Graf_F/0/1/0/all/0/1"&gt;Florian Graf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hofer_C/0/1/0/all/0/1"&gt;Christoph D. Hofer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Niethammer_M/0/1/0/all/0/1"&gt;Marc Niethammer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kwitt_R/0/1/0/all/0/1"&gt;Roland Kwitt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emergent Social Learning via Multi-agent Reinforcement Learning. (arXiv:2010.00581v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.00581</id>
        <link href="http://arxiv.org/abs/2010.00581"/>
        <updated>2021-06-24T01:51:44.637Z</updated>
        <summary type="html"><![CDATA[Social learning is a key component of human and animal intelligence. By
taking cues from the behavior of experts in their environment, social learners
can acquire sophisticated behavior and rapidly adapt to new circumstances. This
paper investigates whether independent reinforcement learning (RL) agents in a
multi-agent environment can learn to use social learning to improve their
performance. We find that in most circumstances, vanilla model-free RL agents
do not use social learning. We analyze the reasons for this deficiency, and
show that by imposing constraints on the training environment and introducing a
model-based auxiliary loss we are able to obtain generalized social learning
policies which enable agents to: i) discover complex skills that are not
learned from single-agent training, and ii) adapt online to novel environments
by taking cues from experts present in the new environment. In contrast, agents
trained with model-free RL or imitation learning generalize poorly and do not
succeed in the transfer tasks. By mixing multi-agent and solo training, we can
obtain agents that use social learning to gain skills that they can deploy when
alone, even out-performing agents trained alone from the start.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ndousse_K/0/1/0/all/0/1"&gt;Kamal Ndousse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eck_D/0/1/0/all/0/1"&gt;Douglas Eck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaques_N/0/1/0/all/0/1"&gt;Natasha Jaques&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Generalize Across Long-Horizon Tasks from Human Demonstrations. (arXiv:2003.06085v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.06085</id>
        <link href="http://arxiv.org/abs/2003.06085"/>
        <updated>2021-06-24T01:51:44.623Z</updated>
        <summary type="html"><![CDATA[Imitation learning is an effective and safe technique to train robot policies
in the real world because it does not depend on an expensive random exploration
process. However, due to the lack of exploration, learning policies that
generalize beyond the demonstrated behaviors is still an open challenge. We
present a novel imitation learning framework to enable robots to 1) learn
complex real world manipulation tasks efficiently from a small number of human
demonstrations, and 2) synthesize new behaviors not contained in the collected
demonstrations. Our key insight is that multi-task domains often present a
latent structure, where demonstrated trajectories for different tasks intersect
at common regions of the state space. We present Generalization Through
Imitation (GTI), a two-stage offline imitation learning algorithm that exploits
this intersecting structure to train goal-directed policies that generalize to
unseen start and goal state combinations. In the first stage of GTI, we train a
stochastic policy that leverages trajectory intersections to have the capacity
to compose behaviors from different demonstration trajectories together. In the
second stage of GTI, we collect a small set of rollouts from the unconditioned
stochastic policy of the first stage, and train a goal-directed agent to
generalize to novel start and goal configurations. We validate GTI in both
simulated domains and a challenging long-horizon robotic manipulation domain in
the real world. Additional results and videos are available at
https://sites.google.com/view/gti2020/ .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mandlekar_A/0/1/0/all/0/1"&gt;Ajay Mandlekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Danfei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1"&gt;Roberto Mart&amp;#xed;n-Mart&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1"&gt;Silvio Savarese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Attributions and Counterfactual Explanations Can Be Manipulated. (arXiv:2106.12563v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12563</id>
        <link href="http://arxiv.org/abs/2106.12563"/>
        <updated>2021-06-24T01:51:44.617Z</updated>
        <summary type="html"><![CDATA[As machine learning models are increasingly used in critical decision-making
settings (e.g., healthcare, finance), there has been a growing emphasis on
developing methods to explain model predictions. Such \textit{explanations} are
used to understand and establish trust in models and are vital components in
machine learning pipelines. Though explanations are a critical piece in these
systems, there is little understanding about how they are vulnerable to
manipulation by adversaries. In this paper, we discuss how two broad classes of
explanations are vulnerable to manipulation. We demonstrate how adversaries can
design biased models that manipulate model agnostic feature attribution methods
(e.g., LIME \& SHAP) and counterfactual explanations that hill-climb during the
counterfactual search (e.g., Wachter's Algorithm \& DiCE) into
\textit{concealing} the model's biases. These vulnerabilities allow an
adversary to deploy a biased model, yet explanations will not reveal this bias,
thereby deceiving stakeholders into trusting the model. We evaluate the
manipulations on real world data sets, including COMPAS and Communities \&
Crime, and find explanations can be manipulated in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Slack_D/0/1/0/all/0/1"&gt;Dylan Slack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hilgard_S/0/1/0/all/0/1"&gt;Sophie Hilgard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sameer Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1"&gt;Hima Lakkaraju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Performance and Complexity Analysis of bi-directional Recurrent Neural Network Models vs. Volterra Nonlinear Equalizers in Digital Coherent Systems. (arXiv:2103.03832v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03832</id>
        <link href="http://arxiv.org/abs/2103.03832"/>
        <updated>2021-06-24T01:51:44.591Z</updated>
        <summary type="html"><![CDATA[We investigate the complexity and performance of recurrent neural network
(RNN) models as post-processing units for the compensation of fibre
nonlinearities in digital coherent systems carrying polarization multiplexed
16-QAM and 32-QAM signals. We evaluate three bi-directional RNN models, namely
the bi-LSTM, bi-GRU and bi-Vanilla-RNN and show that all of them are promising
nonlinearity compensators especially in dispersion unmanaged systems. Our
simulations show that during inference the three models provide similar
compensation performance, therefore in real-life systems the simplest scheme
based on Vanilla-RNN units should be preferred. We compare bi-Vanilla-RNN with
Volterra nonlinear equalizers and exhibit its superiority both in terms of
performance and complexity, thus highlighting that RNN processing is a very
promising pathway for the upgrade of long-haul optical communication systems
utilizing coherent detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Deligiannidis_S/0/1/0/all/0/1"&gt;Stavros Deligiannidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mesaritakis_C/0/1/0/all/0/1"&gt;Charis Mesaritakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bogris_A/0/1/0/all/0/1"&gt;Adonis Bogris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information Bottleneck Attribution for Visual Explanations of Diagnosis and Prognosis. (arXiv:2104.02869v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02869</id>
        <link href="http://arxiv.org/abs/2104.02869"/>
        <updated>2021-06-24T01:51:44.578Z</updated>
        <summary type="html"><![CDATA[Visual explanation methods have an important role in the prognosis of the
patients where the annotated data is limited or unavailable. There have been
several attempts to use gradient-based attribution methods to localize
pathology from medical scans without using segmentation labels. This research
direction has been impeded by the lack of robustness and reliability. These
methods are highly sensitive to the network parameters. In this study, we
introduce a robust visual explanation method to address this problem for
medical applications. We provide an innovative visual explanation algorithm for
general purpose and as an example application, we demonstrate its effectiveness
for quantifying lesions in the lungs caused by the Covid-19 with high accuracy
and robustness without using dense segmentation labels. This approach overcomes
the drawbacks of commonly used Grad-CAM and its extended versions. The premise
behind our proposed strategy is that the information flow is minimized while
ensuring the classifier prediction stays similar. Our findings indicate that
the bottleneck condition provides a more stable severity estimation than the
similar attribution methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Demir_U/0/1/0/all/0/1"&gt;Ugur Demir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Irmakci_I/0/1/0/all/0/1"&gt;Ismail Irmakci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Keles_E/0/1/0/all/0/1"&gt;Elif Keles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Topcu_A/0/1/0/all/0/1"&gt;Ahmet Topcu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Ziyue Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Spampinato_C/0/1/0/all/0/1"&gt;Concetto Spampinato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jambawalikar_S/0/1/0/all/0/1"&gt;Sachin Jambawalikar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Turkbey_E/0/1/0/all/0/1"&gt;Evrim Turkbey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Turkbey_B/0/1/0/all/0/1"&gt;Baris Turkbey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bagci_U/0/1/0/all/0/1"&gt;Ulas Bagci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Universal Adversarial Attacks: A Few Bad Actors Ruin Graph Learning Models. (arXiv:2002.04784v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.04784</id>
        <link href="http://arxiv.org/abs/2002.04784"/>
        <updated>2021-06-24T01:51:44.572Z</updated>
        <summary type="html"><![CDATA[Deep neural networks, while generalize well, are known to be sensitive to
small adversarial perturbations. This phenomenon poses severe security threat
and calls for in-depth investigation of the robustness of deep learning models.
With the emergence of neural networks for graph structured data, similar
investigations are urged to understand their robustness. It has been found that
adversarially perturbing the graph structure and/or node features may result in
a significant degradation of the model performance. In this work, we show from
a different angle that such fragility similarly occurs if the graph contains a
few bad-actor nodes, which compromise a trained graph neural network through
flipping the connections to any targeted victim. Worse, the bad actors found
for one graph model severely compromise other models as well. We call the bad
actors ``anchor nodes'' and propose an algorithm, named GUA, to identify them.
Thorough empirical investigations suggest an interesting finding that the
anchor nodes often belong to the same class; and they also corroborate the
intuitive trade-off between the number of anchor nodes and the attack success
rate. For the dataset Cora which contains 2708 nodes, as few as six anchor
nodes will result in an attack success rate higher than 80\% for GCN and other
three models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zang_X/0/1/0/all/0/1"&gt;Xiao Zang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yi Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1"&gt;Bo Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Optimization Kernel: Towards Robust Deep Learning. (arXiv:2106.06097v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06097</id>
        <link href="http://arxiv.org/abs/2106.06097"/>
        <updated>2021-06-24T01:51:44.565Z</updated>
        <summary type="html"><![CDATA[Recent studies show a close connection between neural networks (NN) and
kernel methods. However, most of these analyses (e.g., NTK) focus on the
influence of (infinite) width instead of the depth of NN models. There remains
a gap between theory and practical network designs that benefit from the depth.
This paper first proposes a novel kernel family named Neural Optimization
Kernel (NOK). Our kernel is defined as the inner product between two $T$-step
updated functionals in RKHS w.r.t. a regularized optimization problem.
Theoretically, we proved the monotonic descent property of our update rule for
both convex and non-convex problems, and a $O(1/T)$ convergence rate of our
updates for convex problems. Moreover, we propose a data-dependent structured
approximation of our NOK, which builds the connection between training deep NNs
and kernel methods associated with NOK. The resultant computational graph is a
ResNet-type finite width NN. Our structured approximation preserved the
monotonic descent property and $O(1/T)$ convergence rate. Namely, a $T$-layer
NN performs $T$-step monotonic descent updates. Notably, we show our
$T$-layered structured NN with ReLU maintains a $O(1/T)$ convergence rate
w.r.t. a convex regularized problem, which explains the success of ReLU on
training deep NN from a NN architecture optimization perspective. For the
unsupervised learning and the shared parameter case, we show the equivalence of
training structured NN with GD and performing functional gradient descent in
RKHS associated with a fixed (data-dependent) NOK at an infinity-width regime.
For finite NOKs, we prove generalization bounds. Remarkably, we show that
overparameterized deep NN (NOK) can increase the expressive power to reduce
empirical risk and reduce the generalization bound at the same time. Extensive
experiments verify the robustness of our structured NOK blocks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lyu_Y/0/1/0/all/0/1"&gt;Yueming Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tsang_I/0/1/0/all/0/1"&gt;Ivor Tsang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Information Obfuscation for Split Inference of Neural Networks. (arXiv:2104.11413v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11413</id>
        <link href="http://arxiv.org/abs/2104.11413"/>
        <updated>2021-06-24T01:51:44.521Z</updated>
        <summary type="html"><![CDATA[Splitting network computations between the edge device and a server enables
low edge-compute inference of neural networks but might expose sensitive
information about the test query to the server. To address this problem,
existing techniques train the model to minimize information leakage for a given
set of sensitive attributes. In practice, however, the test queries might
contain attributes that are not foreseen during training. We propose instead an
unsupervised obfuscation method to discard the information irrelevant to the
main task. We formulate the problem via an information theoretical framework
and derive an analytical solution for a given distortion to the model output.
In our method, the edge device runs the model up to a split layer determined
based on its computational capacity. It then obfuscates the obtained feature
vector based on the first layer of the server model by removing the components
in the null space as well as the low-energy components of the remaining signal.
Our experimental results show that our method outperforms existing techniques
in removing the information of the irrelevant attributes and maintaining the
accuracy on the target label. We also show that our method reduces the
communication cost and incurs only a small computational overhead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samragh_M/0/1/0/all/0/1"&gt;Mohammad Samragh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hosseini_H/0/1/0/all/0/1"&gt;Hossein Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Triastcyn_A/0/1/0/all/0/1"&gt;Aleksei Triastcyn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azarian_K/0/1/0/all/0/1"&gt;Kambiz Azarian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soriaga_J/0/1/0/all/0/1"&gt;Joseph Soriaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koushanfar_F/0/1/0/all/0/1"&gt;Farinaz Koushanfar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep ReLU Networks Preserve Expected Length. (arXiv:2102.10492v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10492</id>
        <link href="http://arxiv.org/abs/2102.10492"/>
        <updated>2021-06-24T01:51:44.505Z</updated>
        <summary type="html"><![CDATA[Assessing the complexity of functions computed by a neural network helps us
understand how the network will learn and generalize. One natural measure of
complexity is how the network distorts length - if the network takes a
unit-length curve as input, what is the length of the resulting curve of
outputs? It has been widely believed that this length grows exponentially in
network depth. We prove that in fact this is not the case: the expected length
distortion does not grow with depth, and indeed shrinks slightly, for ReLU
networks with standard random initialization. We also generalize this result by
proving upper bounds both for higher moments of the length distortion and for
the distortion of higher-dimensional volumes. These theoretical results are
corroborated by our experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hanin_B/0/1/0/all/0/1"&gt;Boris Hanin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jeong_R/0/1/0/all/0/1"&gt;Ryan Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rolnick_D/0/1/0/all/0/1"&gt;David Rolnick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TetraPackNet: Four-Corner-Based Object Detection in Logistics Use-Cases. (arXiv:2104.09123v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09123</id>
        <link href="http://arxiv.org/abs/2104.09123"/>
        <updated>2021-06-24T01:51:44.491Z</updated>
        <summary type="html"><![CDATA[While common image object detection tasks focus on bounding boxes or
segmentation masks as object representations, we consider the problem of
finding objects based on four arbitrary vertices. We propose a novel model,
named TetraPackNet, to tackle this problem. TetraPackNet is based on CornerNet
and uses similar algorithms and ideas. It is designated for applications
requiring high-accuracy detection of regularly shaped objects, which is the
case in the logistics use-case of packaging structure recognition. We evaluate
our model on our specific real-world dataset for this use-case. Baselined
against a previous solution, consisting of a Mask R-CNN model and suitable
post-processing steps, TetraPackNet achieves superior results (9% higher in
accuracy) in the sub-task of four-corner based transport unit side detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dorr_L/0/1/0/all/0/1"&gt;Laura D&amp;#xf6;rr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brandt_F/0/1/0/all/0/1"&gt;Felix Brandt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naumann_A/0/1/0/all/0/1"&gt;Alexander Naumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pouls_M/0/1/0/all/0/1"&gt;Martin Pouls&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning in weakly nonlinear systems: A Case study on Significant wave heights. (arXiv:2105.08583v2 [physics.ao-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08583</id>
        <link href="http://arxiv.org/abs/2105.08583"/>
        <updated>2021-06-24T01:51:44.477Z</updated>
        <summary type="html"><![CDATA[This paper proposes a machine learning method based on the Extra Trees (ET)
algorithm for forecasting Significant Wave Heights in oceanic waters. To derive
multiple features from the CDIP buoys, which make point measurements, we first
nowcast various parameters and then forecast them at 30-min intervals. The
proposed algorithm has Scatter Index (SI), Bias, Correlation Coefficient, Root
Mean Squared Error (RMSE) of 0.130, -0.002, 0.97, and 0.14, respectively, for
one day ahead prediction and 0.110, -0.001, 0.98, and 0.122, respectively, for
14-day ahead prediction on the testing dataset. While other state-of-the-art
methods can only forecast up to 120 hours ahead, we extend it further to 14
days. Our proposed setup includes spectral features, hv-block cross-validation,
and stringent QC criteria. The proposed algorithm performs significantly better
than the state-of-the-art methods commonly used for significant wave height
forecasting for one-day ahead prediction. Moreover, the improved performance of
the proposed machine learning method compared to the numerical methods shows
that this performance can be extended to even longer periods allowing for early
prediction of significant wave heights in oceanic waters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Pokhrel_P/0/1/0/all/0/1"&gt;Pujan Pokhrel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Hoque_M/0/1/0/all/0/1"&gt;Md Tamjidul Hoque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Abdelguerfi_M/0/1/0/all/0/1"&gt;Mahdi Abdelguerfi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Perceiver: General Perception with Iterative Attention. (arXiv:2103.03206v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03206</id>
        <link href="http://arxiv.org/abs/2103.03206"/>
        <updated>2021-06-24T01:51:44.471Z</updated>
        <summary type="html"><![CDATA[Biological systems perceive the world by simultaneously processing
high-dimensional inputs from modalities as diverse as vision, audition, touch,
proprioception, etc. The perception models used in deep learning on the other
hand are designed for individual modalities, often relying on domain-specific
assumptions such as the local grid structures exploited by virtually all
existing vision models. These priors introduce helpful inductive biases, but
also lock models to individual modalities. In this paper we introduce the
Perceiver - a model that builds upon Transformers and hence makes few
architectural assumptions about the relationship between its inputs, but that
also scales to hundreds of thousands of inputs, like ConvNets. The model
leverages an asymmetric attention mechanism to iteratively distill inputs into
a tight latent bottleneck, allowing it to scale to handle very large inputs. We
show that this architecture is competitive with or outperforms strong,
specialized models on classification tasks across various modalities: images,
point clouds, audio, video, and video+audio. The Perceiver obtains performance
comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly
attending to 50,000 pixels. It is also competitive in all modalities in
AudioSet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1"&gt;Andrew Jaegle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gimeno_F/0/1/0/all/0/1"&gt;Felix Gimeno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brock_A/0/1/0/all/0/1"&gt;Andrew Brock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1"&gt;Andrew Zisserman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1"&gt;Oriol Vinyals&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1"&gt;Joao Carreira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Private Query Release Through Adaptive Projection. (arXiv:2103.06641v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06641</id>
        <link href="http://arxiv.org/abs/2103.06641"/>
        <updated>2021-06-24T01:51:44.466Z</updated>
        <summary type="html"><![CDATA[We propose, implement, and evaluate a new algorithm for releasing answers to
very large numbers of statistical queries like $k$-way marginals, subject to
differential privacy. Our algorithm makes adaptive use of a continuous
relaxation of the Projection Mechanism, which answers queries on the private
dataset using simple perturbation, and then attempts to find the synthetic
dataset that most closely matches the noisy answers. We use a continuous
relaxation of the synthetic dataset domain which makes the projection loss
differentiable, and allows us to use efficient ML optimization techniques and
tooling. Rather than answering all queries up front, we make judicious use of
our privacy budget by iteratively and adaptively finding queries for which our
(relaxed) synthetic data has high error, and then repeating the projection. We
perform extensive experimental evaluations across a range of parameters and
datasets, and find that our method outperforms existing algorithms in many
cases, especially when the privacy budget is small or the query class is large.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aydore_S/0/1/0/all/0/1"&gt;Sergul Aydore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_W/0/1/0/all/0/1"&gt;William Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kearns_M/0/1/0/all/0/1"&gt;Michael Kearns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kenthapadi_K/0/1/0/all/0/1"&gt;Krishnaram Kenthapadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melis_L/0/1/0/all/0/1"&gt;Luca Melis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1"&gt;Aaron Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siva_A/0/1/0/all/0/1"&gt;Ankit Siva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards an efficient approach for the nonconvex $\ell_p$-ball projection: algorithm and analysis. (arXiv:2101.01350v4 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.01350</id>
        <link href="http://arxiv.org/abs/2101.01350"/>
        <updated>2021-06-24T01:51:44.448Z</updated>
        <summary type="html"><![CDATA[This paper primarily focuses on computing the Euclidean projection of a
vector onto the $\ell_{p}$ ball in which $p\in(0,1)$. Such a problem emerges as
the core building block in statistical machine learning and signal processing
tasks because of its ability to promote sparsity. However, efficient numerical
algorithms for finding the projections are still not available, particularly in
large-scale optimization. To meet this challenge, we first derive the
first-order necessary optimality conditions of this problem using Fr\'echet
normal cone. Based on this characterization, we develop a novel numerical
approach for computing the stationary point through solving a sequence of
projections onto the reweighted $\ell_{1}$-balls. This method is practically
simple to implement and computationally efficient. Moreover, the proposed
algorithm is shown to converge uniquely under mild conditions and has a
worst-case $O(1/\sqrt{k})$ convergence rate. Numerical experiments demonstrate
the efficiency of our proposed algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiangyu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiashan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probing Model Signal-Awareness via Prediction-Preserving Input Minimization. (arXiv:2011.14934v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14934</id>
        <link href="http://arxiv.org/abs/2011.14934"/>
        <updated>2021-06-24T01:51:44.442Z</updated>
        <summary type="html"><![CDATA[This work explores the signal awareness of AI models for source code
understanding. Using a software vulnerability detection use case, we evaluate
the models' ability to capture the correct vulnerability signals to produce
their predictions. Our prediction-preserving input minimization (P2IM) approach
systematically reduces the original source code to a minimal snippet which a
model needs to maintain its prediction. The model's reliance on incorrect
signals is then uncovered when the vulnerability in the original code is
missing in the minimal snippet, both of which the model however predicts as
being vulnerable. We measure the signal awareness of models using a new metric
we propose- Signal-aware Recall (SAR). We apply P2IM on three different neural
network architectures across multiple datasets. The results show a sharp drop
in the model's Recall from the high 90s to sub-60s with the new metric,
highlighting that the models are presumably picking up a lot of noise or
dataset nuances while learning their vulnerability detection logic. Although
the drop in model performance may be perceived as an adversarial attack, but
this isn't P2IM's objective. The idea is rather to uncover the signal-awareness
of a black-box model in a data-driven manner via controlled queries. SAR's
purpose is to measure the impact of task-agnostic model training, and not to
suggest a shortcoming in the Recall metric. The expectation, in fact, is for
SAR to match Recall in the ideal scenario where the model truly captures
task-specific signals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suneja_S/0/1/0/all/0/1"&gt;Sahil Suneja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yunhui Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1"&gt;Yufan Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laredo_J/0/1/0/all/0/1"&gt;Jim Laredo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morari_A/0/1/0/all/0/1"&gt;Alessandro Morari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Calibrating the Lee-Carter and the Poisson Lee-Carter models via Neural Networks. (arXiv:2106.12312v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.12312</id>
        <link href="http://arxiv.org/abs/2106.12312"/>
        <updated>2021-06-24T01:51:44.436Z</updated>
        <summary type="html"><![CDATA[This paper introduces a neural network approach for fitting the Lee-Carter
and the Poisson Lee-Carter model on multiple populations. We develop some
neural networks that replicate the structure of the individual LC models and
allow their joint fitting by analysing the mortality data of all the considered
populations simultaneously. The neural network architecture is specifically
designed to calibrate each individual model using all available information
instead of using a population-specific subset of data as in the traditional
estimation schemes. A large set of numerical experiments performed on all the
countries of the Human Mortality Database (HMD) shows the effectiveness of our
approach. In particular, the resulting parameter estimates appear smooth and
less sensitive to the random fluctuations often present in the mortality rates'
data, especially for low-population countries. In addition, the forecasting
performance results significantly improved as well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Scognamiglio_S/0/1/0/all/0/1"&gt;Salvatore Scognamiglio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Post-hoc Uncertainty Calibration for Domain Drift Scenarios. (arXiv:2012.10988v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10988</id>
        <link href="http://arxiv.org/abs/2012.10988"/>
        <updated>2021-06-24T01:51:44.429Z</updated>
        <summary type="html"><![CDATA[We address the problem of uncertainty calibration. While standard deep neural
networks typically yield uncalibrated predictions, calibrated confidence scores
that are representative of the true likelihood of a prediction can be achieved
using post-hoc calibration methods. However, to date the focus of these
approaches has been on in-domain calibration. Our contribution is two-fold.
First, we show that existing post-hoc calibration methods yield highly
over-confident predictions under domain shift. Second, we introduce a simple
strategy where perturbations are applied to samples in the validation set
before performing the post-hoc calibration step. In extensive experiments, we
demonstrate that this perturbation step results in substantially better
calibration under domain shift on a wide range of architectures and modelling
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tomani_C/0/1/0/all/0/1"&gt;Christian Tomani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gruber_S/0/1/0/all/0/1"&gt;Sebastian Gruber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdem_M/0/1/0/all/0/1"&gt;Muhammed Ebrar Erdem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1"&gt;Daniel Cremers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buettner_F/0/1/0/all/0/1"&gt;Florian Buettner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regret-optimal measurement-feedback control. (arXiv:2011.12785v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12785</id>
        <link href="http://arxiv.org/abs/2011.12785"/>
        <updated>2021-06-24T01:51:44.423Z</updated>
        <summary type="html"><![CDATA[We consider measurement-feedback control in linear dynamical systems from the
perspective of regret minimization. Unlike most prior work in this area, we
focus on the problem of designing an online controller which competes with the
optimal dynamic sequence of control actions selected in hindsight, instead of
the best controller in some specific class of controllers. This formulation of
regret is attractive when the environment changes over time and no single
controller achieves good performance over the entire time horizon. We show that
in the measurement-feedback setting, unlike in the full-information setting,
there is no single offline controller which outperforms every other offline
controller on every disturbance, and propose a new $H_2$-optimal offline
controller as a benchmark for the online controller to compete against. We show
that the corresponding regret-optimal online controller can be found via a
novel reduction to the classical Nehari problem from robust control and present
a tight data-dependent bound on its regret.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Goel_G/0/1/0/all/0/1"&gt;Gautam Goel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hassibi_B/0/1/0/all/0/1"&gt;Babak Hassibi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequential Model Adaptation Using Domain Agnostic Internal Distributions. (arXiv:2007.00197v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.00197</id>
        <link href="http://arxiv.org/abs/2007.00197"/>
        <updated>2021-06-24T01:51:44.407Z</updated>
        <summary type="html"><![CDATA[We develop an algorithm for sequential adaptation of a classifier that is
trained for a source domain to generalize in an unannotated target domain. We
consider that the model has been trained on the source domain annotated data
and then it needs to be adapted using the target domain unannotated data when
the source domain data is not accessible. We align the distributions of the
source and the target domains in a discriminative embedding space via an
intermediate internal distribution. This distribution is estimated using the
source data representations in the embedding. We conduct experiments on four
benchmarks to demonstrate the method is effective and compares favorably
against existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1"&gt;Mohammad Rostami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1"&gt;Aram Galstyan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Adversarial Neural Architecture Search. (arXiv:2105.09356v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09356</id>
        <link href="http://arxiv.org/abs/2105.09356"/>
        <updated>2021-06-24T01:51:44.402Z</updated>
        <summary type="html"><![CDATA[Despite the empirical success of neural architecture search (NAS) in deep
learning applications, the optimality, reproducibility and cost of NAS schemes
remain hard to assess. In this paper, we propose Generative Adversarial NAS
(GA-NAS) with theoretically provable convergence guarantees, promoting
stability and reproducibility in neural architecture search. Inspired by
importance sampling, GA-NAS iteratively fits a generator to previously
discovered top architectures, thus increasingly focusing on important parts of
a large search space. Furthermore, we propose an efficient adversarial learning
approach, where the generator is trained by reinforcement learning based on
rewards provided by a discriminator, thus being able to explore the search
space without evaluating a large number of architectures. Extensive experiments
show that GA-NAS beats the best published results under several cases on three
public NAS benchmarks. In the meantime, GA-NAS can handle ad-hoc search
constraints and search spaces. We show that GA-NAS can be used to improve
already optimized baselines found by other NAS methods, including EfficientNet
and ProxylessNAS, in terms of ImageNet accuracy or the number of parameters, in
their original search space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rezaei_S/0/1/0/all/0/1"&gt;Seyed Saeed Changiz Rezaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1"&gt;Fred X. Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1"&gt;Di Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salameh_M/0/1/0/all/0/1"&gt;Mohammad Salameh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1"&gt;Keith Mills&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1"&gt;Shuo Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wei Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1"&gt;Shangling Jui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time Neural Radiance Caching for Path Tracing. (arXiv:2106.12372v1 [cs.GR])]]></title>
        <id>http://arxiv.org/abs/2106.12372</id>
        <link href="http://arxiv.org/abs/2106.12372"/>
        <updated>2021-06-24T01:51:44.397Z</updated>
        <summary type="html"><![CDATA[We present a real-time neural radiance caching method for path-traced global
illumination. Our system is designed to handle fully dynamic scenes, and makes
no assumptions about the lighting, geometry, and materials. The data-driven
nature of our approach sidesteps many difficulties of caching algorithms, such
as locating, interpolating, and updating cache points. Since pretraining neural
networks to handle novel, dynamic scenes is a formidable generalization
challenge, we do away with pretraining and instead achieve generalization via
adaptation, i.e. we opt for training the radiance cache while rendering. We
employ self-training to provide low-noise training targets and simulate
infinite-bounce transport by merely iterating few-bounce training updates. The
updates and cache queries incur a mild overhead -- about 2.6ms on full HD
resolution -- thanks to a streaming implementation of the neural network that
fully exploits modern hardware. We demonstrate significant noise reduction at
the cost of little induced bias, and report state-of-the-art, real-time
performance on a number of challenging scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1"&gt;Thomas M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rousselle_F/0/1/0/all/0/1"&gt;Fabrice Rousselle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Novak_J/0/1/0/all/0/1"&gt;Jan Nov&amp;#xe1;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keller_A/0/1/0/all/0/1"&gt;Alexander Keller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Posterior Meta-Replay for Continual Learning. (arXiv:2103.01133v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01133</id>
        <link href="http://arxiv.org/abs/2103.01133"/>
        <updated>2021-06-24T01:51:44.391Z</updated>
        <summary type="html"><![CDATA[Learning a sequence of tasks without access to i.i.d. observations is a
widely studied form of continual learning (CL) that remains challenging. In
principle, Bayesian learning directly applies to this setting, since recursive
and one-off Bayesian updates yield the same result. In practice, however,
recursive updating often leads to poor trade-off solutions across tasks because
approximate inference is necessary for most models of interest. Here, we
describe an alternative Bayesian approach where task-conditioned parameter
distributions are continually inferred from data. We offer a practical deep
learning implementation of our framework based on probabilistic
task-conditioned hypernetworks, an approach we term "posterior meta-replay".
Experiments on standard benchmarks show that our probabilistic hypernetworks
compress sequences of posterior parameter distributions with virtually no
forgetting. We obtain considerable performance gains compared to existing
Bayesian CL methods, and identify task inference as our major limiting factor.
This limitation has several causes that are independent of the considered
sequential setting, opening up new avenues for progress in CL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henning_C/0/1/0/all/0/1"&gt;Christian Henning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cervera_M/0/1/0/all/0/1"&gt;Maria R. Cervera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DAngelo_F/0/1/0/all/0/1"&gt;Francesco D&amp;#x27;Angelo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oswald_J/0/1/0/all/0/1"&gt;Johannes von Oswald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Traber_R/0/1/0/all/0/1"&gt;Regina Traber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ehret_B/0/1/0/all/0/1"&gt;Benjamin Ehret&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kobayashi_S/0/1/0/all/0/1"&gt;Seijin Kobayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sacramento_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Sacramento&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grewe_B/0/1/0/all/0/1"&gt;Benjamin F. Grewe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Legal Proceedings Status: Approaches Based on Sequential Text Data. (arXiv:2003.11561v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.11561</id>
        <link href="http://arxiv.org/abs/2003.11561"/>
        <updated>2021-06-24T01:51:44.385Z</updated>
        <summary type="html"><![CDATA[The objective of this paper is to develop predictive models to classify
Brazilian legal proceedings in three possible classes of status: (i) archived
proceedings, (ii) active proceedings, and (iii) suspended proceedings. This
problem's resolution is intended to assist public and private institutions in
managing large portfolios of legal proceedings, providing gains in scale and
efficiency. In this paper, legal proceedings are made up of sequences of short
texts called "motions." We combined several natural language processing (NLP)
and machine learning techniques to solve the problem. Although working with
Portuguese NLP, which can be challenging due to lack of resources, our
approaches performed remarkably well in the classification task, achieving
maximum accuracy of .93 and top average F1 Scores of .89 (macro) and .93
(weighted). Furthermore, we could extract and interpret the patterns learned by
one of our models besides quantifying how those patterns relate to the
classification task. The interpretability step is important among machine
learning legal applications and gives us an exciting insight into how black-box
models make decisions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Polo_F/0/1/0/all/0/1"&gt;Felipe Maia Polo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciochetti_I/0/1/0/all/0/1"&gt;Itamar Ciochetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bertolo_E/0/1/0/all/0/1"&gt;Emerson Bertolo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Cal: Well-controlled Post-hoc Calibration by Ranking. (arXiv:2105.04290v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04290</id>
        <link href="http://arxiv.org/abs/2105.04290"/>
        <updated>2021-06-24T01:51:44.369Z</updated>
        <summary type="html"><![CDATA[In many applications, it is desirable that a classifier not only makes
accurate predictions, but also outputs calibrated posterior probabilities.
However, many existing classifiers, especially deep neural network classifiers,
tend to be uncalibrated. Post-hoc calibration is a technique to recalibrate a
model by learning a calibration map. Existing approaches mostly focus on
constructing calibration maps with low calibration errors, however, this
quality is inadequate for a calibrator being useful. In this paper, we
introduce two constraints that are worth consideration in designing a
calibration map for post-hoc calibration. Then we present Meta-Cal, which is
built from a base calibrator and a ranking model. Under some mild assumptions,
two high-probability bounds are given with respect to these constraints.
Empirical results on CIFAR-10, CIFAR-100 and ImageNet and a range of popular
network architectures show our proposed method significantly outperforms the
current state of the art for post-hoc multi-class classification calibration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xingchen Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Blaschko_M/0/1/0/all/0/1"&gt;Matthew B. Blaschko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MG-DVD: A Real-time Framework for Malware Variant Detection Based on Dynamic Heterogeneous Graph Learning. (arXiv:2106.12288v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.12288</id>
        <link href="http://arxiv.org/abs/2106.12288"/>
        <updated>2021-06-24T01:51:44.363Z</updated>
        <summary type="html"><![CDATA[Detecting the newly emerging malware variants in real time is crucial for
mitigating cyber risks and proactively blocking intrusions. In this paper, we
propose MG-DVD, a novel detection framework based on dynamic heterogeneous
graph learning, to detect malware variants in real time. Particularly, MG-DVD
first models the fine-grained execution event streams of malware variants into
dynamic heterogeneous graphs and investigates real-world meta-graphs between
malware objects, which can effectively characterize more discriminative
malicious evolutionary patterns between malware and their variants. Then,
MG-DVD presents two dynamic walk-based heterogeneous graph learning methods to
learn more comprehensive representations of malware variants, which
significantly reduces the cost of the entire graph retraining. As a result,
MG-DVD is equipped with the ability to detect malware variants in real time,
and it presents better interpretability by introducing meaningful meta-graphs.
Comprehensive experiments on large-scale samples prove that our proposed MG-DVD
outperforms state-of-the-art methods in detecting malware variants in terms of
effectiveness and efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jun Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_M/0/1/0/all/0/1"&gt;Ming Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xu-Dong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Behavior Mimics Distribution: Combining Individual and Group Behaviors for Federated Learning. (arXiv:2106.12300v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12300</id>
        <link href="http://arxiv.org/abs/2106.12300"/>
        <updated>2021-06-24T01:51:44.358Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) has become an active and promising distributed
machine learning paradigm. As a result of statistical heterogeneity, recent
studies clearly show that the performance of popular FL methods (e.g., FedAvg)
deteriorates dramatically due to the client drift caused by local updates. This
paper proposes a novel Federated Learning algorithm (called IGFL), which
leverages both Individual and Group behaviors to mimic distribution, thereby
improving the ability to deal with heterogeneity. Unlike existing FL methods,
our IGFL can be applied to both client and server optimization. As a
by-product, we propose a new attention-based federated learning in the server
optimization of IGFL. To the best of our knowledge, this is the first time to
incorporate attention mechanisms into federated optimization. We conduct
extensive experiments and show that IGFL can significantly improve the
performance of existing federated learning methods. Especially when the
distributions of data among individuals are diverse, IGFL can improve the
classification accuracy by about 13% compared with prior baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hua Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1"&gt;Fanhua Shang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuanyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hongying Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-resolution Outlier Pooling for Sorghum Classification. (arXiv:2106.05748v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05748</id>
        <link href="http://arxiv.org/abs/2106.05748"/>
        <updated>2021-06-24T01:51:44.353Z</updated>
        <summary type="html"><![CDATA[Automated high throughput plant phenotyping involves leveraging sensors, such
as RGB, thermal and hyperspectral cameras (among others), to make large scale
and rapid measurements of the physical properties of plants for the purpose of
better understanding the difference between crops and facilitating rapid plant
breeding programs. One of the most basic phenotyping tasks is to determine the
cultivar, or species, in a particular sensor product. This simple phenotype can
be used to detect errors in planting and to learn the most differentiating
features between cultivars. It is also a challenging visual recognition task,
as a large number of highly related crops are grown simultaneously, leading to
a classification problem with low inter-class variance. In this paper, we
introduce the Sorghum-100 dataset, a large dataset of RGB imagery of sorghum
captured by a state-of-the-art gantry system, a multi-resolution network
architecture that learns both global and fine-grained features on the crops,
and a new global pooling strategy called Dynamic Outlier Pooling which
outperforms standard global pooling strategies on this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_C/0/1/0/all/0/1"&gt;Chao Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dulay_J/0/1/0/all/0/1"&gt;Justin Dulay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rolwes_G/0/1/0/all/0/1"&gt;Gregory Rolwes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pauli_D/0/1/0/all/0/1"&gt;Duke Pauli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shakoor_N/0/1/0/all/0/1"&gt;Nadia Shakoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stylianou_A/0/1/0/all/0/1"&gt;Abby Stylianou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stiff Neural Ordinary Differential Equations. (arXiv:2103.15341v2 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15341</id>
        <link href="http://arxiv.org/abs/2103.15341"/>
        <updated>2021-06-24T01:51:44.347Z</updated>
        <summary type="html"><![CDATA[Neural Ordinary Differential Equations (ODE) are a promising approach to
learn dynamic models from time-series data in science and engineering
applications. This work aims at learning Neural ODE for stiff systems, which
are usually raised from chemical kinetic modeling in chemical and biological
systems. We first show the challenges of learning neural ODE in the classical
stiff ODE systems of Robertson's problem and propose techniques to mitigate the
challenges associated with scale separations in stiff systems. We then present
successful demonstrations in stiff systems of Robertson's problem and an air
pollution problem. The demonstrations show that the usage of deep networks with
rectified activations, proper scaling of the network outputs as well as loss
functions, and stabilized gradient calculations are the key techniques enabling
the learning of stiff neural ODE. The success of learning stiff neural ODE
opens up possibilities of using neural ODEs in applications with widely varying
time-scales, like chemical dynamics in energy conversion, environmental
engineering, and the life sciences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Kim_S/0/1/0/all/0/1"&gt;Suyong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ji_W/0/1/0/all/0/1"&gt;Weiqi Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Deng_S/0/1/0/all/0/1"&gt;Sili Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yingbo Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Rackauckas_C/0/1/0/all/0/1"&gt;Christopher Rackauckas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Under Delayed Feedback: Implicitly Adapting to Gradient Delays. (arXiv:2106.12261v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12261</id>
        <link href="http://arxiv.org/abs/2106.12261"/>
        <updated>2021-06-24T01:51:44.341Z</updated>
        <summary type="html"><![CDATA[We consider stochastic convex optimization problems, where several machines
act asynchronously in parallel while sharing a common memory. We propose a
robust training method for the constrained setting and derive non asymptotic
convergence guarantees that do not depend on prior knowledge of update delays,
objective smoothness, and gradient variance. Conversely, existing methods for
this setting crucially rely on this prior knowledge, which render them
unsuitable for essentially all shared-resources computational environments,
such as clouds and data centers. Concretely, existing approaches are unable to
accommodate changes in the delays which result from dynamic allocation of the
machines, while our method implicitly adapts to such changes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aviv_R/0/1/0/all/0/1"&gt;Rotem Zamir Aviv&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Hakimi_I/0/1/0/all/0/1"&gt;Ido Hakimi&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Schuster_A/0/1/0/all/0/1"&gt;Assaf Schuster&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Levy_K/0/1/0/all/0/1"&gt;Kfir Y. Levy&lt;/a&gt; (1 and 3) ((1) Department of Electrical and Computer Engineering, Technion, (2) Department of Computer Science, Technion, (3) A Viterbi Fellow)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Symmetry between Arms and Knapsacks: A Primal-Dual Approach for Bandits with Knapsacks. (arXiv:2102.06385v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06385</id>
        <link href="http://arxiv.org/abs/2102.06385"/>
        <updated>2021-06-24T01:51:44.326Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the bandits with knapsacks (BwK) problem and develop
a primal-dual based algorithm that achieves a problem-dependent logarithmic
regret bound. The BwK problem extends the multi-arm bandit (MAB) problem to
model the resource consumption associated with playing each arm, and the
existing BwK literature has been mainly focused on deriving asymptotically
optimal distribution-free regret bounds. We first study the primal and dual
linear programs underlying the BwK problem. From this primal-dual perspective,
we discover symmetry between arms and knapsacks, and then propose a new notion
of sub-optimality measure for the BwK problem. The sub-optimality measure
highlights the important role of knapsacks in determining algorithm regret and
inspires the design of our two-phase algorithm. In the first phase, the
algorithm identifies the optimal arms and the binding knapsacks, and in the
second phase, it exhausts the binding knapsacks via playing the optimal arms
through an adaptive procedure. Our regret upper bound involves the proposed
sub-optimality measure and it has a logarithmic dependence on length of horizon
$T$ and a polynomial dependence on $m$ (the numbers of arms) and $d$ (the
number of knapsacks). To the best of our knowledge, this is the first
problem-dependent logarithmic regret bound for solving the general BwK problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaocheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Chunlin Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yinyu Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Some Hoeffding- and Bernstein-type Concentration Inequalities. (arXiv:2102.06304v4 [math.PR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06304</id>
        <link href="http://arxiv.org/abs/2102.06304"/>
        <updated>2021-06-24T01:51:44.319Z</updated>
        <summary type="html"><![CDATA[We prove concentration inequalities for functions of independent random
variables {under} sub-gaussian and sub-exponential conditions. The utility of
the inequalities is demonstrated by an extension of the now classical method of
Rademacher complexities to Lipschitz function classes and unbounded
sub-exponential distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Maurer_A/0/1/0/all/0/1"&gt;Andreas Maurer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Pontil_M/0/1/0/all/0/1"&gt;Massimiliano Pontil&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Learning Divergences of Variational Inference. (arXiv:2007.02912v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.02912</id>
        <link href="http://arxiv.org/abs/2007.02912"/>
        <updated>2021-06-24T01:51:44.314Z</updated>
        <summary type="html"><![CDATA[Variational inference (VI) plays an essential role in approximate Bayesian
inference due to its computational efficiency and broad applicability. Crucial
to the performance of VI is the selection of the associated divergence measure,
as VI approximates the intractable distribution by minimizing this divergence.
In this paper we propose a meta-learning algorithm to learn the divergence
metric suited for the task of interest, automating the design of VI methods. In
addition, we learn the initialization of the variational parameters without
additional cost when our method is deployed in the few-shot learning scenarios.
We demonstrate our approach outperforms standard VI on Gaussian mixture
distribution approximation, Bayesian neural network regression, image
generation with variational autoencoders and recommender systems with a partial
variational autoencoder.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yingzhen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1"&gt;Christopher De Sa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Devlin_S/0/1/0/all/0/1"&gt;Sam Devlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Cheng Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explaining Black-Box Algorithms Using Probabilistic Contrastive Counterfactuals. (arXiv:2103.11972v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11972</id>
        <link href="http://arxiv.org/abs/2103.11972"/>
        <updated>2021-06-24T01:51:44.308Z</updated>
        <summary type="html"><![CDATA[There has been a recent resurgence of interest in explainable artificial
intelligence (XAI) that aims to reduce the opaqueness of AI-based
decision-making systems, allowing humans to scrutinize and trust them. Prior
work in this context has focused on the attribution of responsibility for an
algorithm's decisions to its inputs wherein responsibility is typically
approached as a purely associational concept. In this paper, we propose a
principled causality-based approach for explaining black-box decision-making
systems that addresses limitations of existing methods in XAI. At the core of
our framework lies probabilistic contrastive counterfactuals, a concept that
can be traced back to philosophical, cognitive, and social foundations of
theories on how humans generate and select explanations. We show how such
counterfactuals can quantify the direct and indirect influences of a variable
on decisions made by an algorithm, and provide actionable recourse for
individuals negatively affected by the algorithm's decision. Unlike prior work,
our system, LEWIS: (1)can compute provably effective explanations and recourse
at local, global and contextual levels (2)is designed to work with users with
varying levels of background knowledge of the underlying causal model and
(3)makes no assumptions about the internals of an algorithmic system except for
the availability of its input-output data. We empirically evaluate LEWIS on
three real-world datasets and show that it generates human-understandable
explanations that improve upon state-of-the-art approaches in XAI, including
the popular LIME and SHAP. Experiments on synthetic data further demonstrate
the correctness of LEWIS's explanations and the scalability of its recourse
algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Galhotra_S/0/1/0/all/0/1"&gt;Sainyam Galhotra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pradhan_R/0/1/0/all/0/1"&gt;Romila Pradhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salimi_B/0/1/0/all/0/1"&gt;Babak Salimi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Acyclicity Reasoning for Bayesian Network Structure Learning with Constraint Programming. (arXiv:2106.12269v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.12269</id>
        <link href="http://arxiv.org/abs/2106.12269"/>
        <updated>2021-06-24T01:51:44.302Z</updated>
        <summary type="html"><![CDATA[Bayesian networks are probabilistic graphical models with a wide range of
application areas including gene regulatory networks inference, risk analysis
and image processing. Learning the structure of a Bayesian network (BNSL) from
discrete data is known to be an NP-hard task with a superexponential search
space of directed acyclic graphs. In this work, we propose a new polynomial
time algorithm for discovering a subset of all possible cluster cuts, a greedy
algorithm for approximately solving the resulting linear program, and a
generalised arc consistency algorithm for the acyclicity constraint. We embed
these in the constraint programmingbased branch-and-bound solver CPBayes and
show that, despite being suboptimal, they improve performance by orders of
magnitude. The resulting solver also compares favourably with GOBNILP, a
state-of-the-art solver for the BNSL problem which solves an NP-hard problem to
discover each cut and solves the linear program exactly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trosser_F/0/1/0/all/0/1"&gt;Fulya Tr&amp;#xf6;sser&lt;/a&gt; (MIAT INRA), &lt;a href="http://arxiv.org/find/cs/1/au:+Givry_S/0/1/0/all/0/1"&gt;Simon de Givry&lt;/a&gt; (MIAT INRA), &lt;a href="http://arxiv.org/find/cs/1/au:+Katsirelos_G/0/1/0/all/0/1"&gt;George Katsirelos&lt;/a&gt; (MIA-Paris)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tilting the playing field: Dynamical loss functions for machine learning. (arXiv:2102.03793v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03793</id>
        <link href="http://arxiv.org/abs/2102.03793"/>
        <updated>2021-06-24T01:51:44.286Z</updated>
        <summary type="html"><![CDATA[We show that learning can be improved by using loss functions that evolve
cyclically during training to emphasize one class at a time. In
underparameterized networks, such dynamical loss functions can lead to
successful training for networks that fail to find a deep minima of the
standard cross-entropy loss. In overparameterized networks, dynamical loss
functions can lead to better generalization. Improvement arises from the
interplay of the changing loss landscape with the dynamics of the system as it
evolves to minimize the loss. In particular, as the loss function oscillates,
instabilities develop in the form of bifurcation cascades, which we study using
the Hessian and Neural Tangent Kernel. Valleys in the landscape widen and
deepen, and then narrow and rise as the loss landscape changes during a cycle.
As the landscape narrows, the learning rate becomes too large and the network
becomes unstable and bounces around the valley. This process ultimately pushes
the system into deeper and wider regions of the loss landscape and is
characterized by decreasing eigenvalues of the Hessian. This results in better
regularized models with improved generalization performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruiz_Garcia_M/0/1/0/all/0/1"&gt;Miguel Ruiz-Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Ge Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schoenholz_S/0/1/0/all/0/1"&gt;Samuel S. Schoenholz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Andrea J. Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emergent Properties of Foveated Perceptual Systems. (arXiv:2006.07991v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07991</id>
        <link href="http://arxiv.org/abs/2006.07991"/>
        <updated>2021-06-24T01:51:44.281Z</updated>
        <summary type="html"><![CDATA[The goal of this work is to characterize the representational impact that
foveation operations have for machine vision systems, inspired by the foveated
human visual system, which has higher acuity at the center of gaze and
texture-like encoding in the periphery. To do so, we introduce models
consisting of a first-stage \textit{fixed} image transform followed by a
second-stage \textit{learnable} convolutional neural network, and we varied the
first stage component. The primary model has a foveated-textural input stage,
which we compare to a model with foveated-blurred input and a model with
spatially-uniform blurred input (both matched for perceptual compression), and
a final reference model with minimal input-based compression. We find that: 1)
the foveated-texture model shows similar scene classification accuracy as the
reference model despite its compressed input, with greater i.i.d.
generalization than the other models; 2) the foveated-texture model has greater
sensitivity to high-spatial frequency information and greater robustness to
occlusion, w.r.t the comparison models; 3) both the foveated systems, show a
stronger center image-bias relative to the spatially-uniform systems even with
a weight sharing constraint. Critically, these results are preserved over
different classical CNN architectures throughout their learning dynamics.
Altogether, this suggests that foveation with peripheral texture-based
computations yields an efficient, distinct, and robust representational format
of scene information, and provides symbiotic computational insight into the
representational consequences that texture-based peripheral encoding may have
for processing in the human visual system, while also potentially inspiring the
next generation of computer vision models via spatially-adaptive computation.
Code + Data available here: https://github.com/ArturoDeza/EmergentProperties]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deza_A/0/1/0/all/0/1"&gt;Arturo Deza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konkle_T/0/1/0/all/0/1"&gt;Talia Konkle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Forecasting Health and Wellbeing for Shift Workers Using Job-role Based Deep Neural Network. (arXiv:2106.12081v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12081</id>
        <link href="http://arxiv.org/abs/2106.12081"/>
        <updated>2021-06-24T01:51:44.275Z</updated>
        <summary type="html"><![CDATA[Shift workers who are essential contributors to our society, face high risks
of poor health and wellbeing. To help with their problems, we collected and
analyzed physiological and behavioral wearable sensor data from shift working
nurses and doctors, as well as their behavioral questionnaire data and their
self-reported daily health and wellbeing labels, including alertness,
happiness, energy, health, and stress. We found the similarities and
differences between the responses of nurses and doctors. According to the
differences in self-reported health and wellbeing labels between nurses and
doctors, and the correlations among their labels, we proposed a job-role based
multitask and multilabel deep learning model, where we modeled physiological
and behavioral data for nurses and doctors simultaneously to predict
participants' next day's multidimensional self-reported health and wellbeing
status. Our model showed significantly better performances than baseline models
and previous state-of-the-art models in the evaluations of binary/3-class
classification and regression prediction tasks. We also found features related
to heart rate, sleep, and work shift contributed to shift workers' health and
wellbeing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Han Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Itoh_A/0/1/0/all/0/1"&gt;Asami Itoh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sakamoto_R/0/1/0/all/0/1"&gt;Ryota Sakamoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shimaoka_M/0/1/0/all/0/1"&gt;Motomu Shimaoka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sano_A/0/1/0/all/0/1"&gt;Akane Sano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring the Representational Power of Graph Autoencoder. (arXiv:2106.12005v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12005</id>
        <link href="http://arxiv.org/abs/2106.12005"/>
        <updated>2021-06-24T01:51:44.270Z</updated>
        <summary type="html"><![CDATA[While representation learning has yielded a great success on many graph
learning tasks, there is little understanding behind the structures that are
being captured by these embeddings. For example, we wonder if the topological
features, such as the Triangle Count, the Degree of the node and other
centrality measures are concretely encoded in the embeddings. Furthermore, we
ask if the presence of these structures in the embeddings is necessary for a
better performance on the downstream tasks, such as clustering and
classification. To address these questions, we conduct an extensive empirical
study over three classes of unsupervised graph embedding models and seven
different variants of Graph Autoencoders. Our results show that five
topological features: the Degree, the Local Clustering Score, the Betweenness
Centrality, the Eigenvector Centrality, and Triangle Count are concretely
preserved in the first layer of the graph autoencoder that employs the SUM
aggregation rule, under the condition that the model preserves the second-order
proximity. We supplement further evidence for the presence of these features by
revealing a hierarchy in the distribution of the topological features in the
embeddings of the aforementioned model. We also show that a model with such
properties can outperform other models on certain downstream tasks, especially
when the preserved features are relevant to the task at hand. Finally, we
evaluate the suitability of our findings through a test case study related to
social influence prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haddad_M/0/1/0/all/0/1"&gt;Maroun Haddad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouguessa_M/0/1/0/all/0/1"&gt;Mohamed Bouguessa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Teacher Model Fingerprinting Attacks Against Transfer Learning. (arXiv:2106.12478v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.12478</id>
        <link href="http://arxiv.org/abs/2106.12478"/>
        <updated>2021-06-24T01:51:44.264Z</updated>
        <summary type="html"><![CDATA[Transfer learning has become a common solution to address training data
scarcity in practice. It trains a specified student model by reusing or
fine-tuning early layers of a well-trained teacher model that is usually
publicly available. However, besides utility improvement, the transferred
public knowledge also brings potential threats to model confidentiality, and
even further raises other security and privacy issues.

In this paper, we present the first comprehensive investigation of the
teacher model exposure threat in the transfer learning context, aiming to gain
a deeper insight into the tension between public knowledge and model
confidentiality. To this end, we propose a teacher model fingerprinting attack
to infer the origin of a student model, i.e., the teacher model it transfers
from. Specifically, we propose a novel optimization-based method to carefully
generate queries to probe the student model to realize our attack. Unlike
existing model reverse engineering approaches, our proposed fingerprinting
method neither relies on fine-grained model outputs, e.g., posteriors, nor
auxiliary information of the model architecture or training dataset. We
systematically evaluate the effectiveness of our proposed attack. The empirical
results demonstrate that our attack can accurately identify the model origin
with few probing queries. Moreover, we show that the proposed attack can serve
as a stepping stone to facilitating other attacks against machine learning
models, such as model stealing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yufei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chao Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Cong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yang Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Throughput Precision Phenotyping of Left Ventricular Hypertrophy with Cardiovascular Deep Learning. (arXiv:2106.12511v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12511</id>
        <link href="http://arxiv.org/abs/2106.12511"/>
        <updated>2021-06-24T01:51:44.249Z</updated>
        <summary type="html"><![CDATA[Left ventricular hypertrophy (LVH) results from chronic remodeling caused by
a broad range of systemic and cardiovascular disease including hypertension,
aortic stenosis, hypertrophic cardiomyopathy, and cardiac amyloidosis. Early
detection and characterization of LVH can significantly impact patient care but
is limited by under-recognition of hypertrophy, measurement error and
variability, and difficulty differentiating etiologies of LVH. To overcome this
challenge, we present EchoNet-LVH - a deep learning workflow that automatically
quantifies ventricular hypertrophy with precision equal to human experts and
predicts etiology of LVH. Trained on 28,201 echocardiogram videos, our model
accurately measures intraventricular wall thickness (mean absolute error [MAE]
1.4mm, 95% CI 1.2-1.5mm), left ventricular diameter (MAE 2.4mm, 95% CI
2.2-2.6mm), and posterior wall thickness (MAE 1.2mm, 95% CI 1.1-1.3mm) and
classifies cardiac amyloidosis (area under the curve of 0.83) and hypertrophic
cardiomyopathy (AUC 0.98) from other etiologies of LVH. In external datasets
from independent domestic and international healthcare systems, EchoNet-LVH
accurately quantified ventricular parameters (R2 of 0.96 and 0.90 respectively)
and detected cardiac amyloidosis (AUC 0.79) and hypertrophic cardiomyopathy
(AUC 0.89) on the domestic external validation site. Leveraging measurements
across multiple heart beats, our model can more accurately identify subtle
changes in LV geometry and its causal etiologies. Compared to human experts,
EchoNet-LVH is fully automated, allowing for reproducible, precise
measurements, and lays the foundation for precision diagnosis of cardiac
hypertrophy. As a resource to promote further innovation, we also make publicly
available a large dataset of 23,212 annotated echocardiogram videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Duffy_G/0/1/0/all/0/1"&gt;Grant Duffy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_P/0/1/0/all/0/1"&gt;Paul P Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yuan_N/0/1/0/all/0/1"&gt;Neal Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+He_B/0/1/0/all/0/1"&gt;Bryan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kwan_A/0/1/0/all/0/1"&gt;Alan C. Kwan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shun_Shin_M/0/1/0/all/0/1"&gt;Matthew J. Shun-Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alexander_K/0/1/0/all/0/1"&gt;Kevin M. Alexander&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ebinger_J/0/1/0/all/0/1"&gt;Joseph Ebinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lungren_M/0/1/0/all/0/1"&gt;Matthew P. Lungren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rader_F/0/1/0/all/0/1"&gt;Florian Rader&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1"&gt;David H. Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schnittger_I/0/1/0/all/0/1"&gt;Ingela Schnittger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ashley_E/0/1/0/all/0/1"&gt;Euan A. Ashley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zou_J/0/1/0/all/0/1"&gt;James Y. Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Patel_J/0/1/0/all/0/1"&gt;Jignesh Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Witteles_R/0/1/0/all/0/1"&gt;Ronald Witteles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_S/0/1/0/all/0/1"&gt;Susan Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ouyang_D/0/1/0/all/0/1"&gt;David Ouyang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LogME: Practical Assessment of Pre-trained Models for Transfer Learning. (arXiv:2102.11005v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11005</id>
        <link href="http://arxiv.org/abs/2102.11005"/>
        <updated>2021-06-24T01:51:44.244Z</updated>
        <summary type="html"><![CDATA[This paper studies task adaptive pre-trained model selection, an
underexplored problem of assessing pre-trained models for the target task and
select best ones from the model zoo \emph{without fine-tuning}. A few pilot
works addressed the problem in transferring supervised pre-trained models to
classification tasks, but they cannot handle emerging unsupervised pre-trained
models or regression tasks. In pursuit of a practical assessment method, we
propose to estimate the maximum value of label evidence given features
extracted by pre-trained models. Unlike the maximum likelihood, the maximum
evidence is \emph{immune to over-fitting}, while its expensive computation can
be dramatically reduced by our carefully designed algorithm. The Logarithm of
Maximum Evidence (LogME) can be used to assess pre-trained models for transfer
learning: a pre-trained model with a high LogME value is likely to have good
transfer performance. LogME is \emph{fast, accurate, and general},
characterizing itself as the first practical method for assessing pre-trained
models. Compared with brute-force fine-tuning, LogME brings at most
$3000\times$ speedup in wall-clock time and requires only $1\%$ memory
footprint. It outperforms prior methods by a large margin in their setting and
is applicable to new settings. It is general enough for diverse pre-trained
models (supervised pre-trained and unsupervised pre-trained), downstream tasks
(classification and regression), and modalities (vision and language). Code is
available at this repository:
\href{https://github.com/thuml/LogME}{https://github.com/thuml/LogME}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+You_K/0/1/0/all/0/1"&gt;Kaichao You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianmin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1"&gt;Mingsheng Long&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Eliminating Sharp Minima from SGD with Truncated Heavy-tailed Noise. (arXiv:2102.04297v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04297</id>
        <link href="http://arxiv.org/abs/2102.04297"/>
        <updated>2021-06-24T01:51:44.238Z</updated>
        <summary type="html"><![CDATA[The empirical success of deep learning is often attributed to SGD's
mysterious ability to avoid sharp local minima in the loss landscape, as sharp
minima are known to lead to poor generalization. Recently, empirical evidence
of heavy-tailed gradient noise was reported in many deep learning tasks, and it
was shown in \c{S}im\c{s}ekli (2019a,b) that SGD can escape sharp local minima
under the presence of such heavy-tailed gradient noise, providing a partial
solution to the mystery. In this work, we analyze a popular variant of SGD
where gradients are truncated above a fixed threshold. We show that it achieves
a stronger notion of avoiding sharp minima: it can effectively eliminate sharp
local minima entirely from its training trajectory. We characterize the
dynamics of truncated SGD driven by heavy-tailed noises. First, we show that
the truncation threshold and width of the attraction field dictate the order of
the first exit time from the associated local minimum. Moreover, when the
objective function satisfies appropriate structural conditions, we prove that
as the learning rate decreases, the dynamics of heavy-tailed truncated SGD
closely resemble those of a continuous-time Markov chain that never visits any
sharp minima. Real data experiments on deep learning confirm our theoretical
prediction that heavy-tailed SGD with gradient clipping finds a "flatter" local
minima and achieves better generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xingyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1"&gt;Sewoong Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rhee_C/0/1/0/all/0/1"&gt;Chang-Han Rhee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient-Based Interpretability Methods and Binarized Neural Networks. (arXiv:2106.12569v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12569</id>
        <link href="http://arxiv.org/abs/2106.12569"/>
        <updated>2021-06-24T01:51:44.232Z</updated>
        <summary type="html"><![CDATA[Binarized Neural Networks (BNNs) have the potential to revolutionize the way
that deep learning is carried out in edge computing platforms. However, the
effectiveness of interpretability methods on these networks has not been
assessed.

In this paper, we compare the performance of several widely used saliency
map-based interpretabilty techniques (Gradient, SmoothGrad and GradCAM), when
applied to Binarized or Full Precision Neural Networks (FPNNs). We found that
the basic Gradient method produces very similar-looking maps for both types of
network. However, SmoothGrad produces significantly noisier maps for BNNs.
GradCAM also produces saliency maps which differ between network types, with
some of the BNNs having seemingly nonsensical explanations. We comment on
possible reasons for these differences in explanations and present it as an
example of why interpretability techniques should be tested on a wider range of
network types.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Widdicombe_A/0/1/0/all/0/1"&gt;Amy Widdicombe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Julier_S/0/1/0/all/0/1"&gt;Simon J. Julier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Learning of Tensor Network Structures. (arXiv:2008.05437v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.05437</id>
        <link href="http://arxiv.org/abs/2008.05437"/>
        <updated>2021-06-24T01:51:44.226Z</updated>
        <summary type="html"><![CDATA[Tensor Networks (TN) offer a powerful framework to efficiently represent very
high-dimensional objects. TN have recently shown their potential for machine
learning applications and offer a unifying view of common tensor decomposition
models such as Tucker, tensor train (TT) and tensor ring (TR). However,
identifying the best tensor network structure from data for a given task is
challenging. In this work, we leverage the TN formalism to develop a generic
and efficient adaptive algorithm to jointly learn the structure and the
parameters of a TN from data. Our method is based on a simple greedy approach
starting from a rank one tensor and successively identifying the most promising
tensor network edges for small rank increments. Our algorithm can adaptively
identify TN structures with small number of parameters that effectively
optimize any differentiable objective function. Experiments on tensor
decomposition, tensor completion and model compression tasks demonstrate the
effectiveness of the proposed algorithm. In particular, our method outperforms
the state-of-the-art evolutionary topology search [Li and Sun, 2020] for tensor
decomposition of images (while being orders of magnitude faster) and finds
efficient tensor network structures to compress neural networks outperforming
popular TT based approaches [Novikov et al., 2015].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hashemizadeh_M/0/1/0/all/0/1"&gt;Meraj Hashemizadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Michelle Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1"&gt;Jacob Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabusseau_G/0/1/0/all/0/1"&gt;Guillaume Rabusseau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiband VAE: Latent Space Partitioning for Knowledge Consolidation in Continual Learning. (arXiv:2106.12196v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12196</id>
        <link href="http://arxiv.org/abs/2106.12196"/>
        <updated>2021-06-24T01:51:44.211Z</updated>
        <summary type="html"><![CDATA[We propose a new method for unsupervised continual knowledge consolidation in
generative models that relies on the partitioning of Variational Autoencoder's
latent space. Acquiring knowledge about new data samples without forgetting
previous ones is a critical problem of continual learning. Currently proposed
methods achieve this goal by extending the existing model while constraining
its behavior not to degrade on the past data, which does not exploit the full
potential of relations within the entire training dataset. In this work, we
identify this limitation and posit the goal of continual learning as a
knowledge accumulation task. We solve it by continuously re-aligning latent
space partitions that we call bands which are representations of samples seen
in different tasks, driven by the similarity of the information they contain.
In addition, we introduce a simple yet effective method for controlled
forgetting of past data that improves the quality of reconstructions encoded in
latent bands and a latent space disentanglement technique that improves
knowledge consolidation. On top of the standard continual learning evaluation
benchmarks, we evaluate our method on a new knowledge consolidation scenario
and show that the proposed approach outperforms state-of-the-art by up to
twofold across all testing scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deja_K/0/1/0/all/0/1"&gt;Kamil Deja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wawrzynski_P/0/1/0/all/0/1"&gt;Pawe&amp;#x142; Wawrzy&amp;#x144;ski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marczak_D/0/1/0/all/0/1"&gt;Daniel Marczak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masarczyk_W/0/1/0/all/0/1"&gt;Wojciech Masarczyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1"&gt;Tomasz Trzci&amp;#x144;ski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Explainable Representations of Malware Behavior. (arXiv:2106.12328v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12328</id>
        <link href="http://arxiv.org/abs/2106.12328"/>
        <updated>2021-06-24T01:51:44.205Z</updated>
        <summary type="html"><![CDATA[We address the problems of identifying malware in network telemetry logs and
providing \emph{indicators of compromise} -- comprehensible explanations of
behavioral patterns that identify the threat. In our system, an array of
specialized detectors abstracts network-flow data into comprehensible
\emph{network events} in a first step. We develop a neural network that
processes this sequence of events and identifies specific threats, malware
families and broad categories of malware. We then use the
\emph{integrated-gradients} method to highlight events that jointly constitute
the characteristic behavioral pattern of the threat. We compare network
architectures based on CNNs, LSTMs, and transformers, and explore the efficacy
of unsupervised pre-training experimentally on large-scale telemetry data. We
demonstrate how this system detects njRAT and other malware based on behavioral
patterns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Prasse_P/0/1/0/all/0/1"&gt;Paul Prasse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brabec_J/0/1/0/all/0/1"&gt;Jan Brabec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kohout_J/0/1/0/all/0/1"&gt;Jan Kohout&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kopp_M/0/1/0/all/0/1"&gt;Martin Kopp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bajer_L/0/1/0/all/0/1"&gt;Lukas Bajer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scheffer_T/0/1/0/all/0/1"&gt;Tobias Scheffer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Should You Go Deeper? Optimizing Convolutional Neural Network Architectures without Training by Receptive Field Analysis. (arXiv:2106.12307v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12307</id>
        <link href="http://arxiv.org/abs/2106.12307"/>
        <updated>2021-06-24T01:51:44.186Z</updated>
        <summary type="html"><![CDATA[Applying artificial neural networks (ANN) to specific tasks, researchers,
programmers, and other specialists usually overshot the number of convolutional
layers in their designs. By implication, these ANNs hold too many parameters,
which needed unnecessarily trained without impacting the result. The features,
a convolutional layer can process, are strictly limited by its receptive field.
By layer-wise analyzing the expansion of the receptive fields, we can reliably
predict sequences of layers that will not contribute qualitatively to the
inference in thegiven ANN architecture. Based on these analyses, we propose
design strategies to resolve these inefficiencies, optimizing the
explainability and the computational performance of ANNs. Since neither the
strategies nor the analysis requires training of the actual model, these
insights allow for a very efficient design process of ANNs architectures which
might be automated in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Richter_M/0/1/0/all/0/1"&gt;Mats L. Richter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schoning_J/0/1/0/all/0/1"&gt;Julius Sch&amp;#xf6;ning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krumnack_U/0/1/0/all/0/1"&gt;Ulf Krumnack&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Alignment for Approximated Reversibility in Neural Networks. (arXiv:2106.12562v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12562</id>
        <link href="http://arxiv.org/abs/2106.12562"/>
        <updated>2021-06-24T01:51:44.178Z</updated>
        <summary type="html"><![CDATA[We introduce feature alignment, a technique for obtaining approximate
reversibility in artificial neural networks. By means of feature extraction, we
can train a neural network to learn an estimated map for its reverse process
from outputs to inputs. Combined with variational autoencoders, we can generate
new samples from the same statistics as the training data. Improvements of the
results are obtained by using concepts from generative adversarial networks.
Finally, we show that the technique can be modified for training neural
networks locally, saving computational memory resources. Applying these
techniques, we report results for three vision generative tasks: MNIST,
CIFAR-10, and celebA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farias_T/0/1/0/all/0/1"&gt;Tiago de Souza Farias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maziero_J/0/1/0/all/0/1"&gt;Jonas Maziero&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TextSETTR: Few-Shot Text Style Extraction and Tunable Targeted Restyling. (arXiv:2010.03802v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03802</id>
        <link href="http://arxiv.org/abs/2010.03802"/>
        <updated>2021-06-24T01:51:44.123Z</updated>
        <summary type="html"><![CDATA[We present a novel approach to the problem of text style transfer. Unlike
previous approaches requiring style-labeled training data, our method makes use
of readily-available unlabeled text by relying on the implicit connection in
style between adjacent sentences, and uses labeled data only at inference time.
We adapt T5 (Raffel et al., 2020), a strong pretrained text-to-text model, to
extract a style vector from text and use it to condition the decoder to perform
style transfer. As our label-free training results in a style vector space
encoding many facets of style, we recast transfers as "targeted restyling"
vector operations that adjust specific attributes of the input while preserving
others. We demonstrate that training on unlabeled Amazon reviews data results
in a model that is competitive on sentiment transfer, even compared to models
trained fully on labeled data. Furthermore, applying our novel method to a
diverse corpus of unlabeled web text results in a single model capable of
transferring along multiple dimensions of style (dialect, emotiveness,
formality, politeness, sentiment) despite no additional training and using only
a handful of exemplars at inference time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Riley_P/0/1/0/all/0/1"&gt;Parker Riley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1"&gt;Noah Constant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Mandy Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1"&gt;Girish Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uthus_D/0/1/0/all/0/1"&gt;David Uthus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parekh_Z/0/1/0/all/0/1"&gt;Zarana Parekh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sampling with Mirrored Stein Operators. (arXiv:2106.12506v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.12506</id>
        <link href="http://arxiv.org/abs/2106.12506"/>
        <updated>2021-06-24T01:51:44.116Z</updated>
        <summary type="html"><![CDATA[We introduce a new family of particle evolution samplers suitable for
constrained domains and non-Euclidean geometries. Stein Variational Mirror
Descent and Mirrored Stein Variational Gradient Descent minimize the
Kullback-Leibler (KL) divergence to constrained target distributions by
evolving particles in a dual space defined by a mirror map. Stein Variational
Natural Gradient exploits non-Euclidean geometry to more efficiently minimize
the KL divergence to unconstrained targets. We derive these samplers from a new
class of mirrored Stein operators and adaptive kernels developed in this work.
We demonstrate that these new samplers yield accurate approximations to
distributions on the simplex, deliver valid confidence intervals in
post-selection inference, and converge more rapidly than prior methods in
large-scale unconstrained posterior inference. Finally, we establish the
convergence of our new procedures under verifiable conditions on the target
distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jiaxin Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mackey_L/0/1/0/all/0/1"&gt;Lester Mackey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finding Phish in a Haystack: A Pipeline for Phishing Classification on Certificate Transparency Logs. (arXiv:2106.12343v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.12343</id>
        <link href="http://arxiv.org/abs/2106.12343"/>
        <updated>2021-06-24T01:51:44.111Z</updated>
        <summary type="html"><![CDATA[Current popular phishing prevention techniques mainly utilize reactive
blocklists, which leave a ``window of opportunity'' for attackers during which
victims are unprotected. One possible approach to shorten this window aims to
detect phishing attacks earlier, during website preparation, by monitoring
Certificate Transparency (CT) logs. Previous attempts to work with CT log data
for phishing classification exist, however they lack evaluations on actual CT
log data. In this paper, we present a pipeline that facilitates such
evaluations by addressing a number of problems when working with CT log data.
The pipeline includes dataset creation, training, and past or live
classification of CT logs. Its modular structure makes it possible to easily
exchange classifiers or verification sources to support ground truth labeling
efforts and classifier comparisons. We test the pipeline on a number of new and
existing classifiers, and find a general potential to improve classifiers for
this scenario in the future. We publish the source code of the pipeline and the
used datasets along with this paper
(https://gitlab.com/rwth-itsec/ctl-pipeline), thus making future research in
this direction more accessible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Drichel_A/0/1/0/all/0/1"&gt;Arthur Drichel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drury_V/0/1/0/all/0/1"&gt;Vincent Drury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brandt_J/0/1/0/all/0/1"&gt;Justus von Brandt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meyer_U/0/1/0/all/0/1"&gt;Ulrike Meyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combination of Convolutional Neural Network and Gated Recurrent Unit for Energy Aware Resource Allocation. (arXiv:2106.12178v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2106.12178</id>
        <link href="http://arxiv.org/abs/2106.12178"/>
        <updated>2021-06-24T01:51:44.106Z</updated>
        <summary type="html"><![CDATA[Cloud computing service models have experienced rapid growth and inefficient
resource usage is known as one of the greatest causes of high energy
consumption in cloud data centers. Resource allocation in cloud data centers
aiming to reduce energy consumption has been conducted using live migration of
Virtual Machines (VMs) and their consolidation into the small number of
Physical Machines (PMs). However, the selection of the appropriate VM for
migration is an important challenge. To solve this issue, VMs can be classified
according to the pattern of user requests into sensitive or insensitive classes
to latency, and thereafter suitable VMs can be selected for migration. In this
paper, the combination of Convolution Neural Network (CNN) and Gated Recurrent
Unit (GRU) is utilized for the classification of VMs in the Microsoft Azure
dataset. Due to the fact the majority of VMs in this dataset are labeled as
insensitive to latency, migration of more VMs in this group not only reduces
energy consumption but also decreases the violation of Service Level Agreements
(SLA). Based on the empirical results, the proposed model obtained an accuracy
of 95.18which clearly demonstrates the superiority of our proposed model
compared to other existing models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khodaverdian_Z/0/1/0/all/0/1"&gt;Zeinab Khodaverdian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadr_H/0/1/0/all/0/1"&gt;Hossein Sadr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Edalatpanah_S/0/1/0/all/0/1"&gt;Seyed Ahmad Edalatpanah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Solimandarabi_M/0/1/0/all/0/1"&gt;Mojdeh Nazari Solimandarabi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NodePiece: Compositional and Parameter-Efficient Representations of Large Knowledge Graphs. (arXiv:2106.12144v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12144</id>
        <link href="http://arxiv.org/abs/2106.12144"/>
        <updated>2021-06-24T01:51:44.101Z</updated>
        <summary type="html"><![CDATA[Conventional representation learning algorithms for knowledge graphs (KG) map
each entity to a unique embedding vector. Such a shallow lookup results in a
linear growth of memory consumption for storing the embedding matrix and incurs
high computational costs when working with real-world KGs. Drawing parallels
with subword tokenization commonly used in NLP, we explore the landscape of
more parameter-efficient node embedding strategies with possibly sublinear
memory requirements. To this end, we propose NodePiece, an anchor-based
approach to learn a fixed-size entity vocabulary. In NodePiece, a vocabulary of
subword/sub-entity units is constructed from anchor nodes in a graph with known
relation types. Given such a fixed-size vocabulary, it is possible to bootstrap
an encoding and embedding for any entity, including those unseen during
training. Experiments show that NodePiece performs competitively in node
classification, link prediction, and relation prediction tasks while retaining
less than 10% of explicit nodes in a graph as anchors and often having 10x
fewer parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Galkin_M/0/1/0/all/0/1"&gt;Mikhail Galkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiapeng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Denis_E/0/1/0/all/0/1"&gt;Etienne Denis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1"&gt;William L. Hamilton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regret-optimal Estimation and Control. (arXiv:2106.12097v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12097</id>
        <link href="http://arxiv.org/abs/2106.12097"/>
        <updated>2021-06-24T01:51:44.085Z</updated>
        <summary type="html"><![CDATA[We consider estimation and control in linear time-varying dynamical systems
from the perspective of regret minimization. Unlike most prior work in this
area, we focus on the problem of designing causal estimators and controllers
which compete against a clairvoyant noncausal policy, instead of the best
policy selected in hindsight from some fixed parametric class. We show that the
regret-optimal estimator and regret-optimal controller can be derived in
state-space form using operator-theoretic techniques from robust control and
present tight,data-dependent bounds on the regret incurred by our algorithms in
terms of the energy of the disturbances. Our results can be viewed as extending
traditional robust estimation and control, which focuses on minimizing
worst-case cost, to minimizing worst-case regret. We propose regret-optimal
analogs of Model-Predictive Control (MPC) and the Extended KalmanFilter (EKF)
for systems with nonlinear dynamics and present numerical experiments which
show that our regret-optimal algorithms can significantly outperform standard
approaches to estimation and control.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goel_G/0/1/0/all/0/1"&gt;Gautam Goel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassibi_B/0/1/0/all/0/1"&gt;Babak Hassibi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Blur, Noise, and Compression Robust Generative Adversarial Networks. (arXiv:2003.07849v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.07849</id>
        <link href="http://arxiv.org/abs/2003.07849"/>
        <updated>2021-06-24T01:51:44.080Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) have gained considerable attention
owing to their ability to reproduce images. However, they can recreate training
images faithfully despite image degradation in the form of blur, noise, and
compression, generating similarly degraded images. To solve this problem, the
recently proposed noise robust GAN (NR-GAN) provides a partial solution by
demonstrating the ability to learn a clean image generator directly from noisy
images using a two-generator model comprising image and noise generators.
However, its application is limited to noise, which is relatively easy to
decompose owing to its additive and reversible characteristics, and its
application to irreversible image degradation, in the form of blur,
compression, and combination of all, remains a challenge. To address these
problems, we propose blur, noise, and compression robust GAN (BNCR-GAN) that
can learn a clean image generator directly from degraded images without
knowledge of degradation parameters (e.g., blur kernel types, noise amounts, or
quality factor values). Inspired by NR-GAN, BNCR-GAN uses a multiple-generator
model composed of image, blur-kernel, noise, and quality-factor generators.
However, in contrast to NR-GAN, to address irreversible characteristics, we
introduce masking architectures adjusting degradation strength values in a
data-driven manner using bypasses before and after degradation. Furthermore, to
suppress uncertainty caused by the combination of blur, noise, and compression,
we introduce adaptive consistency losses imposing consistency between
irreversible degradation processes according to the degradation strengths. We
demonstrate the effectiveness of BNCR-GAN through large-scale comparative
studies on CIFAR-10 and a generality analysis on FFHQ. In addition, we
demonstrate the applicability of BNCR-GAN in image restoration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaneko_T/0/1/0/all/0/1"&gt;Takuhiro Kaneko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1"&gt;Tatsuya Harada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coarse-to-Fine Q-attention: Efficient Learning for Visual Robotic Manipulation via Discretisation. (arXiv:2106.12534v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.12534</id>
        <link href="http://arxiv.org/abs/2106.12534"/>
        <updated>2021-06-24T01:51:44.075Z</updated>
        <summary type="html"><![CDATA[Reflecting on the last few years, the biggest breakthroughs in deep
reinforcement learning (RL) have been in the discrete action domain. Robotic
manipulation, however, is inherently a continuous control environment, but
these continuous control reinforcement learning algorithms often depend on
actor-critic methods that are sample-inefficient and inherently difficult to
train, due to the joint optimisation of the actor and critic. To that end, we
explore how we can bring the stability of discrete action RL algorithms to the
robot manipulation domain. We extend the recently released ARM algorithm, by
replacing the continuous next-best pose agent with a discrete next-best pose
agent. Discretisation of rotation is trivial given its bounded nature, while
translation is inherently unbounded, making discretisation difficult. We
formulate the translation prediction as the voxel prediction problem by
discretising the 3D space; however, voxelisation of a large workspace is memory
intensive and would not work with a high density of voxels, crucial to
obtaining the resolution needed for robotic manipulation. We therefore propose
to apply this voxel prediction in a coarse-to-fine manner by gradually
increasing the resolution. In each step, we extract the highest valued voxel as
the predicted location, which is then used as the centre of the
higher-resolution voxelisation in the next step. This coarse-to-fine prediction
is applied over several steps, giving a near-lossless prediction of the
translation. We show that our new coarse-to-fine algorithm is able to
accomplish RLBench tasks much more efficiently than the continuous control
equivalent, and even train some real-world tasks, tabular rasa, in less than 7
minutes, with only 3 demonstrations. Moreover, we show that by moving to a
voxel representation, we are able to easily incorporate observations from
multiple cameras.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1"&gt;Stephen James&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wada_K/0/1/0/all/0/1"&gt;Kentaro Wada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laidlow_T/0/1/0/all/0/1"&gt;Tristan Laidlow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1"&gt;Andrew J. Davison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Imitation Learning: Progress, Taxonomies and Opportunities. (arXiv:2106.12177v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12177</id>
        <link href="http://arxiv.org/abs/2106.12177"/>
        <updated>2021-06-24T01:51:44.069Z</updated>
        <summary type="html"><![CDATA[Imitation learning aims to extract knowledge from human experts'
demonstrations or artificially created agents in order to replicate their
behaviors. Its success has been demonstrated in areas such as video games,
autonomous driving, robotic simulations and object manipulation. However, this
replicating process could be problematic, such as the performance is highly
dependent on the demonstration quality, and most trained agents are limited to
perform well in task-specific environments. In this survey, we provide a
systematic review on imitation learning. We first introduce the background
knowledge from development history and preliminaries, followed by presenting
different taxonomies within Imitation Learning and key milestones of the field.
We then detail challenges in learning strategies and present research
opportunities with learning policy from suboptimal demonstration, voice
instructions and other associated optimization schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1"&gt;Boyuan Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1"&gt;Sunny Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jianlong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1"&gt;Ivor Tsang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Fang Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Q-Learning Lagrange Policies for Multi-Action Restless Bandits. (arXiv:2106.12024v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12024</id>
        <link href="http://arxiv.org/abs/2106.12024"/>
        <updated>2021-06-24T01:51:44.053Z</updated>
        <summary type="html"><![CDATA[Multi-action restless multi-armed bandits (RMABs) are a powerful framework
for constrained resource allocation in which $N$ independent processes are
managed. However, previous work only study the offline setting where problem
dynamics are known. We address this restrictive assumption, designing the first
algorithms for learning good policies for Multi-action RMABs online using
combinations of Lagrangian relaxation and Q-learning. Our first approach,
MAIQL, extends a method for Q-learning the Whittle index in binary-action RMABs
to the multi-action setting. We derive a generalized update rule and
convergence proof and establish that, under standard assumptions, MAIQL
converges to the asymptotically optimal multi-action RMAB policy as
$t\rightarrow{}\infty$. However, MAIQL relies on learning Q-functions and
indexes on two timescales which leads to slow convergence and requires problem
structure to perform well. Thus, we design a second algorithm, LPQL, which
learns the well-performing and more general Lagrange policy for multi-action
RMABs by learning to minimize the Lagrange bound through a variant of
Q-learning. To ensure fast convergence, we take an approximation strategy that
enables learning on a single timescale, then give a guarantee relating the
approximation's precision to an upper bound of LPQL's return as
$t\rightarrow{}\infty$. Finally, we show that our approaches always outperform
baselines across multiple settings, including one derived from real-world
medication adherence data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Killian_J/0/1/0/all/0/1"&gt;Jackson A. Killian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biswas_A/0/1/0/all/0/1"&gt;Arpita Biswas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1"&gt;Sanket Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tambe_M/0/1/0/all/0/1"&gt;Milind Tambe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual T: Reducing Estimation Error for Transition Matrix in Label-noise Learning. (arXiv:2006.07805v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07805</id>
        <link href="http://arxiv.org/abs/2006.07805"/>
        <updated>2021-06-24T01:51:44.048Z</updated>
        <summary type="html"><![CDATA[The transition matrix, denoting the transition relationship from clean labels
to noisy labels, is essential to build statistically consistent classifiers in
label-noise learning. Existing methods for estimating the transition matrix
rely heavily on estimating the noisy class posterior. However, the estimation
error for noisy class posterior could be large due to the randomness of label
noise, which would lead the transition matrix to be poorly estimated.
Therefore, in this paper, we aim to solve this problem by exploiting the
divide-and-conquer paradigm. Specifically, we introduce an intermediate class
to avoid directly estimating the noisy class posterior. By this intermediate
class, the original transition matrix can then be factorized into the product
of two easy-to-estimate transition matrices. We term the proposed method the
dual-T estimator. Both theoretical analyses and empirical results illustrate
the effectiveness of the dual-T estimator for estimating transition matrices,
leading to better classification performances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1"&gt;Mingming Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Jiankang Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learned Interpretable Residual Extragradient ISTA for Sparse Coding. (arXiv:2106.11970v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11970</id>
        <link href="http://arxiv.org/abs/2106.11970"/>
        <updated>2021-06-24T01:51:44.043Z</updated>
        <summary type="html"><![CDATA[Recently, the study on learned iterative shrinkage thresholding algorithm
(LISTA) has attracted increasing attentions. A large number of experiments as
well as some theories have proved the high efficiency of LISTA for solving
sparse coding problems. However, existing LISTA methods are all serial
connection. To address this issue, we propose a novel extragradient based LISTA
(ELISTA), which has a residual structure and theoretical guarantees. In
particular, our algorithm can also provide the interpretability for Res-Net to
a certain extent. From a theoretical perspective, we prove that our method
attains linear convergence. In practice, extensive empirical results verify the
advantages of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1"&gt;Lin Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1"&gt;Fanhua Shang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuanyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hongying Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[False perfection in machine prediction: Detecting and assessing circularity problems in machine learning. (arXiv:2106.12417v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12417</id>
        <link href="http://arxiv.org/abs/2106.12417"/>
        <updated>2021-06-24T01:51:44.037Z</updated>
        <summary type="html"><![CDATA[Machine learning algorithms train models from patterns of input data and
target outputs, with the goal of predicting correct outputs for unseen test
inputs. Here we demonstrate a problem of machine learning in vital application
areas such as medical informatics or patent law that consists of the inclusion
of measurements on which target outputs are deterministically defined in the
representations of input data. This leads to perfect, but circular predictions
based on a machine reconstruction of the known target definition, but fails on
real-world data where the defining measurements may not or only incompletely be
available. We present a circularity test that shows, for given datasets and
black-box machine learning models, whether the target functional definition can
be reconstructed and has been used in training. We argue that a transfer of
research results to real-world applications requires to avoid circularity by
separating measurements that define target outcomes from data representations
in machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hagmann_M/0/1/0/all/0/1"&gt;Michael Hagmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1"&gt;Stefan Riezler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FLOP: Federated Learning on Medical Datasets using Partial Networks. (arXiv:2102.05218v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05218</id>
        <link href="http://arxiv.org/abs/2102.05218"/>
        <updated>2021-06-24T01:51:44.032Z</updated>
        <summary type="html"><![CDATA[The outbreak of COVID-19 Disease due to the novel coronavirus has caused a
shortage of medical resources. To aid and accelerate the diagnosis process,
automatic diagnosis of COVID-19 via deep learning models has recently been
explored by researchers across the world. While different data-driven deep
learning models have been developed to mitigate the diagnosis of COVID-19, the
data itself is still scarce due to patient privacy concerns. Federated Learning
(FL) is a natural solution because it allows different organizations to
cooperatively learn an effective deep learning model without sharing raw data.
However, recent studies show that FL still lacks privacy protection and may
cause data leakage. We investigate this challenging problem by proposing a
simple yet effective algorithm, named \textbf{F}ederated \textbf{L}earning
\textbf{o}n Medical Datasets using \textbf{P}artial Networks (FLOP), that
shares only a partial model between the server and clients. Extensive
experiments on benchmark data and real-world healthcare tasks show that our
approach achieves comparable or better performance while reducing the privacy
and security risks. Of particular interest, we conduct experiments on the
COVID-19 dataset and find that our FLOP algorithm can allow different hospitals
to collaboratively and effectively train a partially shared model without
sharing local patients' data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qian Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_W/0/1/0/all/0/1"&gt;Weituo Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spell_G/0/1/0/all/0/1"&gt;Gregory Spell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1"&gt;Lawrence Carin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AC/DC: Alternating Compressed/DeCompressed Training of Deep Neural Networks. (arXiv:2106.12379v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12379</id>
        <link href="http://arxiv.org/abs/2106.12379"/>
        <updated>2021-06-24T01:51:44.026Z</updated>
        <summary type="html"><![CDATA[The increasing computational requirements of deep neural networks (DNNs) have
led to significant interest in obtaining DNN models that are sparse, yet
accurate. Recent work has investigated the even harder case of sparse training,
where the DNN weights are, for as much as possible, already sparse to reduce
computational costs during training.

Existing sparse training methods are mainly empirical and often have lower
accuracy relative to the dense baseline. In this paper, we present a general
approach called Alternating Compressed/DeCompressed (AC/DC) training of DNNs,
demonstrate convergence for a variant of the algorithm, and show that AC/DC
outperforms existing sparse training methods in accuracy at similar
computational budgets; at high sparsity levels, AC/DC even outperforms existing
methods that rely on accurate pre-trained dense models. An important property
of AC/DC is that it allows co-training of dense and sparse models, yielding
accurate sparse-dense model pairs at the end of the training process. This is
useful in practice, where compressed variants may be desirable for deployment
in resource-constrained settings without re-doing the entire training flow, and
also provides us with insights into the accuracy gap between dense and
compressed models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peste_A/0/1/0/all/0/1"&gt;Alexandra Peste&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iofinova_E/0/1/0/all/0/1"&gt;Eugenia Iofinova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vladu_A/0/1/0/all/0/1"&gt;Adrian Vladu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1"&gt;Dan Alistarh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Approach to Fair Online Learning via Blackwell Approachability. (arXiv:2106.12242v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12242</id>
        <link href="http://arxiv.org/abs/2106.12242"/>
        <updated>2021-06-24T01:51:44.010Z</updated>
        <summary type="html"><![CDATA[We provide a setting and a general approach to fair online learning with
stochastic sensitive and non-sensitive contexts. The setting is a repeated game
between the Player and Nature, where at each stage both pick actions based on
the contexts. Inspired by the notion of unawareness, we assume that the Player
can only access the non-sensitive context before making a decision, while we
discuss both cases of Nature accessing the sensitive contexts and Nature
unaware of the sensitive contexts. Adapting Blackwell's approachability theory
to handle the case of an unknown contexts' distribution, we provide a general
necessary and sufficient condition for learning objectives to be compatible
with some fairness constraints. This condition is instantiated on (group-wise)
no-regret and (group-wise) calibration objectives, and on demographic parity as
an additional constraint. When the objective is not compatible with the
constraint, the provided framework permits to characterise the optimal
trade-off between the two.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chzhen_E/0/1/0/all/0/1"&gt;Evgenii Chzhen&lt;/a&gt; (LMO, CELESTE), &lt;a href="http://arxiv.org/find/cs/1/au:+Giraud_C/0/1/0/all/0/1"&gt;Christophe Giraud&lt;/a&gt; (LMO, CELESTE), &lt;a href="http://arxiv.org/find/cs/1/au:+Stoltz_G/0/1/0/all/0/1"&gt;Gilles Stoltz&lt;/a&gt; (LMO, CELESTE)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Bayesian Multiscale Deep Learning Framework for Flows in Random Media. (arXiv:2103.09056v2 [physics.comp-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09056</id>
        <link href="http://arxiv.org/abs/2103.09056"/>
        <updated>2021-06-24T01:51:44.005Z</updated>
        <summary type="html"><![CDATA[Fine-scale simulation of complex systems governed by multiscale partial
differential equations (PDEs) is computationally expensive and various
multiscale methods have been developed for addressing such problems. In
addition, it is challenging to develop accurate surrogate and uncertainty
quantification models for high-dimensional problems governed by stochastic
multiscale PDEs using limited training data. In this work to address these
challenges, we introduce a novel hybrid deep-learning and multiscale approach
for stochastic multiscale PDEs with limited training data. For demonstration
purposes, we focus on a porous media flow problem. We use an image-to-image
supervised deep learning model to learn the mapping between the input
permeability field and the multiscale basis functions. We introduce a Bayesian
approach to this hybrid framework to allow us to perform uncertainty
quantification and propagation tasks. The performance of this hybrid approach
is evaluated with varying intrinsic dimensionality of the permeability field.
Numerical results indicate that the hybrid network can efficiently predict well
for high-dimensional inputs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Padmanabha_G/0/1/0/all/0/1"&gt;Govinda Anantha Padmanabha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zabaras_N/0/1/0/all/0/1"&gt;Nicholas Zabaras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ATOM: Robustifying Out-of-distribution Detection Using Outlier Mining. (arXiv:2006.15207v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.15207</id>
        <link href="http://arxiv.org/abs/2006.15207"/>
        <updated>2021-06-24T01:51:43.999Z</updated>
        <summary type="html"><![CDATA[Detecting out-of-distribution (OOD) inputs is critical for safely deploying
deep learning models in an open-world setting. However, existing OOD detection
solutions can be brittle in the open world, facing various types of adversarial
OOD inputs. While methods leveraging auxiliary OOD data have emerged, our
analysis on illuminative examples reveals a key insight that the majority of
auxiliary OOD examples may not meaningfully improve or even hurt the decision
boundary of the OOD detector, which is also observed in empirical results on
real data. In this paper, we provide a theoretically motivated method,
Adversarial Training with informative Outlier Mining (ATOM), which improves the
robustness of OOD detection. We show that, by mining informative auxiliary OOD
data, one can significantly improve OOD detection performance, and somewhat
surprisingly, generalize to unseen adversarial attacks. ATOM achieves
state-of-the-art performance under a broad family of classic and adversarial
OOD evaluation tasks. For example, on the CIFAR-10 in-distribution dataset,
ATOM reduces the FPR (at TPR 95%) by up to 57.99% under adversarial OOD inputs,
surpassing the previous best baseline by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiefeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yixuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yingyu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1"&gt;Somesh Jha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Deep Learning Hyperparameter Search for Robust Function Mapping to Polynomials with Noise. (arXiv:2106.12532v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12532</id>
        <link href="http://arxiv.org/abs/2106.12532"/>
        <updated>2021-06-24T01:51:43.993Z</updated>
        <summary type="html"><![CDATA[Advances in neural architecture search, as well as explainability and
interpretability of connectionist architectures, have been reported in the
recent literature. However, our understanding of how to design Bayesian Deep
Learning (BDL) hyperparameters, specifically, the depth, width and ensemble
size, for robust function mapping with uncertainty quantification, is still
emerging. This paper attempts to further our understanding by mapping Bayesian
connectionist representations to polynomials of different orders with varying
noise types and ratios. We examine the noise-contaminated polynomials to search
for the combination of hyperparameters that can extract the underlying
polynomial signals while quantifying uncertainties based on the noise
attributes. Specifically, we attempt to study the question that an appropriate
neural architecture and ensemble configuration can be found to detect a signal
of any n-th order polynomial contaminated with noise having different
distributions and signal-to-noise (SNR) ratios and varying noise attributes.
Our results suggest the possible existence of an optimal network depth as well
as an optimal number of ensembles for prediction skills and uncertainty
quantification, respectively. However, optimality is not discernible for width,
even though the performance gain reduces with increasing width at high values
of width. Our experiments and insights can be directional to understand
theoretical properties of BDL representations and to design practical
solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Harilal_N/0/1/0/all/0/1"&gt;Nidhin Harilal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhatia_U/0/1/0/all/0/1"&gt;Udit Bhatia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganguly_A/0/1/0/all/0/1"&gt;Auroop R. Ganguly&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Assistive Technologies for Activities of Daily Living of Elderly. (arXiv:2106.12183v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2106.12183</id>
        <link href="http://arxiv.org/abs/2106.12183"/>
        <updated>2021-06-24T01:51:43.979Z</updated>
        <summary type="html"><![CDATA[One of the distinct features of this century has been the population of older
adults which has been on a constant rise. Elderly people have several needs and
requirements due to physical disabilities, cognitive issues, weakened memory
and disorganized behavior, that they face with increasing age. The extent of
these limitations also differs according to the varying diversities in elderly,
which include age, gender, background, experience, skills, knowledge and so on.
These varying needs and challenges with increasing age, limits abilities of
older adults to perform Activities of Daily Living (ADLs) in an independent
manner. To add to it, the shortage of caregivers creates a looming need for
technology-based services for elderly people, to assist them in performing
their daily routine tasks to sustain their independent living and active aging.
To address these needs, this work consists of making three major contributions
in this field. First, it provides a rather comprehensive review of assisted
living technologies aimed at helping elderly people to perform ADLs. Second,
the work discusses the challenges identified through this review, that
currently exist in the context of implementation of assisted living services
for elderly care in Smart Homes and Smart Cities. Finally, the work also
outlines an approach for implementation, extension and integration of the
existing works in this field for development of a much-needed framework that
can provide personalized assistance and user-centered behavior interventions to
elderly as per their varying and ever-changing needs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1"&gt;Nirmalya Thakur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1"&gt;Chia Y. Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Energy-Based Models for End-to-End Speech Recognition. (arXiv:2103.14152v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14152</id>
        <link href="http://arxiv.org/abs/2103.14152"/>
        <updated>2021-06-24T01:51:43.973Z</updated>
        <summary type="html"><![CDATA[End-to-end models with auto-regressive decoders have shown impressive results
for automatic speech recognition (ASR). These models formulate the
sequence-level probability as a product of the conditional probabilities of all
individual tokens given their histories. However, the performance of locally
normalised models can be sub-optimal because of factors such as exposure bias.
Consequently, the model distribution differs from the underlying data
distribution. In this paper, the residual energy-based model (R-EBM) is
proposed to complement the auto-regressive ASR model to close the gap between
the two distributions. Meanwhile, R-EBMs can also be regarded as
utterance-level confidence estimators, which may benefit many downstream tasks.
Experiments on a 100hr LibriSpeech dataset show that R-EBMs can reduce the word
error rates (WERs) by 8.2%/6.7% while improving areas under precision-recall
curves of confidence scores by 12.6%/28.4% on test-clean/test-other sets.
Furthermore, on a state-of-the-art model using self-supervised learning
(wav2vec 2.0), R-EBMs still significantly improves both the WER and confidence
estimation performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qiujia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cao_L/0/1/0/all/0/1"&gt;Liangliang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Woodland_P/0/1/0/all/0/1"&gt;Philip C. Woodland&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Better Algorithms for Individually Fair $k$-Clustering. (arXiv:2106.12150v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2106.12150</id>
        <link href="http://arxiv.org/abs/2106.12150"/>
        <updated>2021-06-24T01:51:43.969Z</updated>
        <summary type="html"><![CDATA[We study data clustering problems with $\ell_p$-norm objectives (e.g.
$k$-Median and $k$-Means) in the context of individual fairness. The dataset
consists of $n$ points, and we want to find $k$ centers such that (a) the
objective is minimized, while (b) respecting the individual fairness constraint
that every point $v$ has a center within a distance at most $r(v)$, where
$r(v)$ is $v$'s distance to its $(n/k)$th nearest point. Jung, Kannan, and Lutz
[FORC 2020] introduced this concept and designed a clustering algorithm with
provable (approximate) fairness and objective guarantees for the $\ell_\infty$
or $k$-Center objective. Mahabadi and Vakilian [ICML 2020] revisited this
problem to give a local-search algorithm for all $\ell_p$-norms. Empirically,
their algorithms outperform Jung et. al.'s by a large margin in terms of cost
(for $k$-Median and $k$-Means), but they incur a reasonable loss in fairness.
In this paper, our main contribution is to use Linear Programming (LP)
techniques to obtain better algorithms for this problem, both in theory and in
practice. We prove that by modifying known LP rounding techniques, one gets a
worst-case guarantee on the objective which is much better than in MV20, and
empirically, this objective is extremely close to the optimal. Furthermore, our
theoretical fairness guarantees are comparable with MV20 in theory, and
empirically, we obtain noticeably fairer solutions. Although solving the LP
{\em exactly} might be prohibitive, we demonstrate that in practice, a simple
sparsification technique drastically improves the run-time of our algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chakrabarty_D/0/1/0/all/0/1"&gt;Deeparnab Chakrabarty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Negahbani_M/0/1/0/all/0/1"&gt;Maryam Negahbani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Compressed Sensing using Generative Models. (arXiv:2006.09461v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09461</id>
        <link href="http://arxiv.org/abs/2006.09461"/>
        <updated>2021-06-24T01:51:43.962Z</updated>
        <summary type="html"><![CDATA[The goal of compressed sensing is to estimate a high dimensional vector from
an underdetermined system of noisy linear equations. In analogy to classical
compressed sensing, here we assume a generative model as a prior, that is, we
assume the vector is represented by a deep generative model $G: \mathbb{R}^k
\rightarrow \mathbb{R}^n$. Classical recovery approaches such as empirical risk
minimization (ERM) are guaranteed to succeed when the measurement matrix is
sub-Gaussian. However, when the measurement matrix and measurements are
heavy-tailed or have outliers, recovery may fail dramatically. In this paper we
propose an algorithm inspired by the Median-of-Means (MOM). Our algorithm
guarantees recovery for heavy-tailed data, even in the presence of outliers.
Theoretically, our results show our novel MOM-based algorithm enjoys the same
sample complexity guarantees as ERM under sub-Gaussian assumptions. Our
experiments validate both aspects of our claims: other algorithms are indeed
fragile and fail under heavy-tailed and/or corrupted data, while our approach
exhibits the predicted robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Jalal_A/0/1/0/all/0/1"&gt;Ajil Jalal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Liu_L/0/1/0/all/0/1"&gt;Liu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dimakis_A/0/1/0/all/0/1"&gt;Alexandros G. Dimakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Caramanis_C/0/1/0/all/0/1"&gt;Constantine Caramanis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Oneshot Differentially Private Top-k Selection. (arXiv:2105.08233v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08233</id>
        <link href="http://arxiv.org/abs/2105.08233"/>
        <updated>2021-06-24T01:51:43.957Z</updated>
        <summary type="html"><![CDATA[Being able to efficiently and accurately select the top-$k$ elements with
differential privacy is an integral component of various private data analysis
tasks. In this paper, we present the oneshot Laplace mechanism, which
generalizes the well-known Report Noisy Max mechanism to reporting noisy
top-$k$ elements. We show that the oneshot Laplace mechanism with a noise level
of $\widetilde{O}(\sqrt{k}/\eps)$ is approximately differentially private.
Compared to the previous peeling approach of running Report Noisy Max $k$
times, the oneshot Laplace mechanism only adds noises and computes the top $k$
elements once, hence much more efficient for large $k$. In addition, our proof
of privacy relies on a novel coupling technique that bypasses the use of
composition theorems. Finally, we present a novel application of efficient
top-$k$ selection in the classical problem of ranking from pairwise
comparisons.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_G/0/1/0/all/0/1"&gt;Gang Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1"&gt;Weijie J. Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Li Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Phase Retrieval using Single-Instance Deep Generative Prior. (arXiv:2106.04812v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04812</id>
        <link href="http://arxiv.org/abs/2106.04812"/>
        <updated>2021-06-24T01:51:43.951Z</updated>
        <summary type="html"><![CDATA[Several deep learning methods for phase retrieval exist, but most of them
fail on realistic data without precise support information. We propose a novel
method based on single-instance deep generative prior that works well on
complex-valued crystal data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tayal_K/0/1/0/all/0/1"&gt;Kshitij Tayal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manekar_R/0/1/0/all/0/1"&gt;Raunak Manekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1"&gt;Zhong Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1"&gt;David Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vipin Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_F/0/1/0/all/0/1"&gt;Felix Hofmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Ju Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking supervised learning: insights from biological learning and from calling it by its name. (arXiv:2012.02526v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02526</id>
        <link href="http://arxiv.org/abs/2012.02526"/>
        <updated>2021-06-24T01:51:43.937Z</updated>
        <summary type="html"><![CDATA[The renaissance of artificial neural networks was catalysed by the success of
classification models, tagged by the community with the broader term supervised
learning. The extraordinary results gave rise to a hype loaded with ambitious
promises and overstatements. Soon the community realised that the success owed
much to the availability of thousands of labelled examples and supervised
learning went, for many, from glory to shame: Some criticised deep learning as
a whole and others proclaimed that the way forward had to be alternatives to
supervised learning: predictive, unsupervised, semi-supervised and, more
recently, self-supervised learning. However, all these seem brand names, rather
than actual categories of a theoretically grounded taxonomy. Moreover, the call
to banish supervised learning was motivated by the questionable claim that
humans learn with little or no supervision and are capable of robust
out-of-distribution generalisation. Here, we review insights about learning and
supervision in nature, revisit the notion that learning and generalisation are
not possible without supervision or inductive biases and argue that we will
make better progress if we just call it by its name.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hernandez_Garcia_A/0/1/0/all/0/1"&gt;Alex Hernandez-Garcia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BiblioDAP: The 1st Workshop on Bibliographic Data Analysis and Processing. (arXiv:2106.12320v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2106.12320</id>
        <link href="http://arxiv.org/abs/2106.12320"/>
        <updated>2021-06-24T01:51:43.932Z</updated>
        <summary type="html"><![CDATA[Automatic processing of bibliographic data becomes very important in digital
libraries, data science and machine learning due to its importance in keeping
pace with the significant increase of published papers every year from one side
and to the inherent challenges from the other side. This processing has several
aspects including but not limited to I) Automatic extraction of references from
PDF documents, II) Building an accurate citation graph, III) Author name
disambiguation, etc. Bibliographic data is heterogeneous by nature and occurs
in both structured (e.g. citation graph) and unstructured (e.g. publications)
formats. Therefore, it requires data science and machine learning techniques to
be processed and analysed. Here we introduce BiblioDAP'21: The 1st Workshop on
Bibliographic Data Analysis and Processing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boukhers_Z/0/1/0/all/0/1"&gt;Zeyd Boukhers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mayr_P/0/1/0/all/0/1"&gt;Philipp Mayr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peroni_S/0/1/0/all/0/1"&gt;Silvio Peroni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Learning with Radial Basis Function Networks. (arXiv:2103.08414v2 [cs.CE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08414</id>
        <link href="http://arxiv.org/abs/2103.08414"/>
        <updated>2021-06-24T01:51:43.926Z</updated>
        <summary type="html"><![CDATA[We investigate the benefits of feature selection, nonlinear modelling and
online learning when forecasting in financial time series. We consider the
sequential and continual learning sub-genres of online learning. The
experiments we conduct show that there is a benefit to online transfer
learning, in the form of radial basis function networks, beyond the sequential
updating of recursive least-squares models. We show that the radial basis
function networks, which make use of clustering algorithms to construct a
kernel Gram matrix, are more beneficial than treating each training vector as
separate basis functions, as occurs with kernel Ridge regression. We
demonstrate quantitative procedures to determine the very structure of the
radial basis function networks. Finally, we conduct experiments on the log
returns of financial time series and show that the online learning models,
particularly the radial basis function networks, are able to outperform a
random walk baseline, whereas the offline learning models struggle to do so.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Borrageiro_G/0/1/0/all/0/1"&gt;Gabriel Borrageiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Firoozye_N/0/1/0/all/0/1"&gt;Nick Firoozye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barucca_P/0/1/0/all/0/1"&gt;Paolo Barucca&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness for Image Generation with Uncertain Sensitive Attributes. (arXiv:2106.12182v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12182</id>
        <link href="http://arxiv.org/abs/2106.12182"/>
        <updated>2021-06-24T01:51:43.921Z</updated>
        <summary type="html"><![CDATA[This work tackles the issue of fairness in the context of generative
procedures, such as image super-resolution, which entail different definitions
from the standard classification setting. Moreover, while traditional group
fairness definitions are typically defined with respect to specified protected
groups -- camouflaging the fact that these groupings are artificial and carry
historical and political motivations -- we emphasize that there are no ground
truth identities. For instance, should South and East Asians be viewed as a
single group or separate groups? Should we consider one race as a whole or
further split by gender? Choosing which groups are valid and who belongs in
them is an impossible dilemma and being ``fair'' with respect to Asians may
require being ``unfair'' with respect to South Asians. This motivates the
introduction of definitions that allow algorithms to be \emph{oblivious} to the
relevant groupings.

We define several intuitive notions of group fairness and study their
incompatibilities and trade-offs. We show that the natural extension of
demographic parity is strongly dependent on the grouping, and \emph{impossible}
to achieve obliviously. On the other hand, the conceptually new definition we
introduce, Conditional Proportional Representation, can be achieved obliviously
through Posterior Sampling. Our experiments validate our theoretical results
and achieve fair image reconstruction using state-of-the-art generative models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jalal_A/0/1/0/all/0/1"&gt;Ajil Jalal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karmalkar_S/0/1/0/all/0/1"&gt;Sushrut Karmalkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoffmann_J/0/1/0/all/0/1"&gt;Jessica Hoffmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1"&gt;Alexandros G. Dimakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1"&gt;Eric Price&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SoftNER: Mining Knowledge Graphs From Cloud Incidents. (arXiv:2101.05961v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05961</id>
        <link href="http://arxiv.org/abs/2101.05961"/>
        <updated>2021-06-24T01:51:43.905Z</updated>
        <summary type="html"><![CDATA[The move from boxed products to services and the widespread adoption of cloud
computing has had a huge impact on the software development life cycle and
DevOps processes. Particularly, incident management has become critical for
developing and operating large-scale services. Prior work on incident
management has heavily focused on the challenges with incident triaging and
de-duplication. In this work, we address the fundamental problem of structured
knowledge extraction from service incidents. We have built SoftNER, a framework
for mining Knowledge Graphs from incident reports. First, we build a novel
multi-task learning based BiLSTM-CRF model which leverages not just the
semantic context but also the data-types for extracting factual information in
the form of named entities. Next, we present an approach to mine relations
between the named entities for automatically constructing knowledge graphs. We
have deployed SoftNER at Microsoft, a major cloud service provider and have
evaluated it on more than 2 months of cloud incidents. We show that the
unsupervised machine learning pipeline has a high precision of 0.96. Our
multi-task learning based deep learning model also outperforms the
state-of-the-art NER models. Lastly, using the knowledge extracted by SoftNER,
we are able to build accurate models for applications such as incident triaging
and recommending entities based on their relevance to incident titles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shetty_M/0/1/0/all/0/1"&gt;Manish Shetty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_C/0/1/0/all/0/1"&gt;Chetan Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sumit Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_N/0/1/0/all/0/1"&gt;Nikitha Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagappan_N/0/1/0/all/0/1"&gt;Nachiappan Nagappan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast and Feature-Complete Differentiable Physics for Articulated Rigid Bodies with Contact. (arXiv:2103.16021v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16021</id>
        <link href="http://arxiv.org/abs/2103.16021"/>
        <updated>2021-06-24T01:51:43.900Z</updated>
        <summary type="html"><![CDATA[We present a fast and feature-complete differentiable physics engine, Nimble
(nimblephysics.org), that supports Lagrangian dynamics and hard contact
constraints for articulated rigid body simulation. Our differentiable physics
engine offers a complete set of features that are typically only available in
non-differentiable physics simulators commonly used by robotics applications.
We solve contact constraints precisely using linear complementarity problems
(LCPs). We present efficient and novel analytical gradients through the LCP
formulation of inelastic contact that exploit the sparsity of the LCP solution.
We support complex contact geometry, and gradients approximating
continuous-time elastic collision. We also introduce a novel method to compute
complementarity-aware gradients that help downstream optimization tasks avoid
stalling in saddle points. We show that an implementation of this combination
in an existing physics engine (DART) is capable of a 87x single-core speedup
over finite-differencing in computing analytical Jacobians for a single
timestep, while preserving all the expressiveness of original DART.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Werling_K/0/1/0/all/0/1"&gt;Keenon Werling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Omens_D/0/1/0/all/0/1"&gt;Dalton Omens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jeongseok Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Exarchos_I/0/1/0/all/0/1"&gt;Ioannis Exarchos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;C. Karen Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Policy choice in experiments with unknown interference. (arXiv:2011.08174v4 [econ.EM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08174</id>
        <link href="http://arxiv.org/abs/2011.08174"/>
        <updated>2021-06-24T01:51:43.894Z</updated>
        <summary type="html"><![CDATA[This paper discusses experimental design to estimate welfare-maximizing
policies. We consider a setting where units are organized into large, finitely
many independent clusters and interact over unobserved dimensions within each
cluster. The contribution of this paper is two-fold. First, we construct a test
for whether a welfare-improving treatment configuration exists and hence worth
learning by conducting a larger scale experiment. Second, we introduce an
adaptive randomization procedure to estimate welfare-maximizing individual
treatment allocation rules valid under unobserved interference. We derive
asymptotic properties of the marginal effects estimators and finite-sample
regret guarantees of the policy. Finally, we illustrate the method's advantage
in simulations calibrated to an existing experiment on information diffusion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/econ/1/au:+Viviano_D/0/1/0/all/0/1"&gt;Davide Viviano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Learning Approach to Anomaly Sequence Detection for High-Resolution Monitoring of Power Systems. (arXiv:2012.05163v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05163</id>
        <link href="http://arxiv.org/abs/2012.05163"/>
        <updated>2021-06-24T01:51:43.814Z</updated>
        <summary type="html"><![CDATA[A deep learning approach is proposed to detect data and system anomalies
using high-resolution continuous point-on-wave (CPOW) or phasor measurements.
Both the anomaly and anomaly-free measurement models are assumed to have
unknown temporal dependencies and probability distributions. Historical
training samples are assumed for the anomaly-free model, while no training
samples are available for the anomaly measurements. By transforming the
anomaly-free observations into uniform independent and identically distributed
sequences via a generative adversarial network, the proposed approach deploys a
uniformity test for anomaly detection at the sensor level. A distributed
detection scheme that combines sensor level detections at the control center is
also proposed that combines local detections to form more reliable detections.
Numerical results demonstrate significant improvement over the state-of-the-art
solutions for various bad-data cases using real and synthetic CPOW and PMU data
sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mestav_K/0/1/0/all/0/1"&gt;Kursat Rasim Mestav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tong_L/0/1/0/all/0/1"&gt;Lang Tong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthetic Benchmarks for Scientific Research in Explainable Machine Learning. (arXiv:2106.12543v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12543</id>
        <link href="http://arxiv.org/abs/2106.12543"/>
        <updated>2021-06-24T01:51:43.808Z</updated>
        <summary type="html"><![CDATA[As machine learning models grow more complex and their applications become
more high-stakes, tools for explaining model predictions have become
increasingly important. Despite the widespread use of explainability
techniques, evaluating and comparing different feature attribution methods
remains challenging: evaluations ideally require human studies, and empirical
evaluation metrics are often computationally prohibitive on real-world
datasets. In this work, we address this issue by releasing XAI-Bench: a suite
of synthetic datasets along with a library for benchmarking feature attribution
algorithms. Unlike real-world datasets, synthetic datasets allow the efficient
computation of conditional expected values that are needed to evaluate
ground-truth Shapley values and other metrics. The synthetic datasets we
release offer a wide variety of parameters that can be configured to simulate
real-world data. We demonstrate the power of our library by benchmarking
popular explainability techniques across several evaluation metrics and
identifying failure modes for popular explainers. The efficiency of our library
will help bring new explainability methods from development to deployment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khandagale_S/0/1/0/all/0/1"&gt;Sujay Khandagale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1"&gt;Colin White&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neiswanger_W/0/1/0/all/0/1"&gt;Willie Neiswanger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STEP-EZ: Syntax Tree guided semantic ExPlanation for Explainable Zero-shot modeling of clinical depression symptoms from text. (arXiv:2106.10928v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10928</id>
        <link href="http://arxiv.org/abs/2106.10928"/>
        <updated>2021-06-24T01:51:43.802Z</updated>
        <summary type="html"><![CDATA[We focus on exploring various approaches of Zero-Shot Learning (ZSL) and
their explainability for a challenging yet important supervised learning task
notorious for training data scarcity, i.e. Depression Symptoms Detection (DSD)
from text. We start with a comprehensive synthesis of different components of
our ZSL modeling and analysis of our ground truth samples and Depression
symptom clues curation process with the help of a practicing clinician. We next
analyze the accuracy of various state-of-the-art ZSL models and their potential
enhancements for our task. Further, we sketch a framework for the use of ZSL
for hierarchical text-based explanation mechanism, which we call, Syntax
Tree-Guided Semantic Explanation (STEP). Finally, we summarize experiments from
which we conclude that we can use ZSL models and achieve reasonable accuracy
and explainability, measured by a proposed Explainability Index (EI). This work
is, to our knowledge, the first work to exhaustively explore the efficacy of
ZSL models for DSD task, both in terms of accuracy and explainability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farruque_N/0/1/0/all/0/1"&gt;Nawshad Farruque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1"&gt;Randy Goebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1"&gt;Osmar Zaiane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sivapalan_S/0/1/0/all/0/1"&gt;Sudhakar Sivapalan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[S$^2$-MLP: Spatial-Shift MLP Architecture for Vision. (arXiv:2106.07477v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07477</id>
        <link href="http://arxiv.org/abs/2106.07477"/>
        <updated>2021-06-24T01:51:43.786Z</updated>
        <summary type="html"><![CDATA[Recently, visual Transformer (ViT) and its following works abandon the
convolution and exploit the self-attention operation, attaining a comparable or
even higher accuracy than CNNs. More recently, MLP-Mixer abandons both the
convolution and the self-attention operation, proposing an architecture
containing only MLP layers. To achieve cross-patch communications, it devises
an additional token-mixing MLP besides the channel-mixing MLP. It achieves
promising results when training on an extremely large-scale dataset. But it
cannot achieve as outstanding performance as its CNN and ViT counterparts when
training on medium-scale datasets such as ImageNet1K and ImageNet21K. The
performance drop of MLP-Mixer motivates us to rethink the token-mixing MLP. We
discover that the token-mixing MLP is a variant of the depthwise convolution
with a global reception field and spatial-specific configuration. But the
global reception field and the spatial-specific property make token-mixing MLP
prone to over-fitting. In this paper, we propose a novel pure MLP architecture,
spatial-shift MLP (S$^2$-MLP). Different from MLP-Mixer, our S$^2$-MLP only
contains channel-mixing MLP. We utilize a spatial-shift operation for
communications between patches. It has a local reception field and is
spatial-agnostic. It is parameter-free and efficient for computation. The
proposed S$^2$-MLP attains higher recognition accuracy than MLP-Mixer when
training on ImageNet-1K dataset. Meanwhile, S$^2$-MLP accomplishes as excellent
performance as ViT on ImageNet-1K dataset with considerably simpler
architecture and fewer FLOPs and parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1"&gt;Tan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1"&gt;Yunfeng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Mingming Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Ping Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Consistent Predictive Confidence through Fitted Ensembles. (arXiv:2106.12070v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12070</id>
        <link href="http://arxiv.org/abs/2106.12070"/>
        <updated>2021-06-24T01:51:43.781Z</updated>
        <summary type="html"><![CDATA[Deep neural networks are behind many of the recent successes in machine
learning applications. However, these models can produce overconfident
decisions while encountering out-of-distribution (OOD) examples or making a
wrong prediction. This inconsistent predictive confidence limits the
integration of independently-trained learning models into a larger system. This
paper introduces separable concept learning framework to realistically measure
the performance of classifiers in presence of OOD examples. In this setup,
several instances of a classifier are trained on different parts of a partition
of the set of classes. Later, the performance of the combination of these
models is evaluated on a separate test set. Unlike current OOD detection
techniques, this framework does not require auxiliary OOD datasets and does not
separate classification from detection performance. Furthermore, we present a
new strong baseline for more consistent predictive confidence in deep models,
called fitted ensembles, where overconfident predictions are rectified by
transformed versions of the original classification task. Fitted ensembles can
naturally detect OOD examples without requiring auxiliary data by observing
contradicting predictions among its components. Experiments on MNIST, SVHN,
CIFAR-10/100, and ImageNet show fitted ensemble significantly outperform
conventional ensembles on OOD examples and are possible to scale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kardan_N/0/1/0/all/0/1"&gt;Navid Kardan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Ankit Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stanley_K/0/1/0/all/0/1"&gt;Kenneth O. Stanley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Multimodal VAEs through Mutual Supervision. (arXiv:2106.12570v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12570</id>
        <link href="http://arxiv.org/abs/2106.12570"/>
        <updated>2021-06-24T01:51:43.776Z</updated>
        <summary type="html"><![CDATA[Multimodal VAEs seek to model the joint distribution over heterogeneous data
(e.g.\ vision, language), whilst also capturing a shared representation across
such modalities. Prior work has typically combined information from the
modalities by reconciling idiosyncratic representations directly in the
recognition model through explicit products, mixtures, or other such
factorisations. Here we introduce a novel alternative, the MEME, that avoids
such explicit combinations by repurposing semi-supervised VAEs to combine
information between modalities implicitly through mutual supervision. This
formulation naturally allows learning from partially-observed data where some
modalities can be entirely missing -- something that most existing approaches
either cannot handle, or do so to a limited extent. We demonstrate that MEME
outperforms baselines on standard metrics across both partial and complete
observation schemes on the MNIST-SVHN (image--image) and CUB (image--text)
datasets. We also contrast the quality of the representations learnt by mutual
supervision against standard approaches and observe interesting trends in its
ability to capture relatedness between data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Joy_T/0/1/0/all/0/1"&gt;Tom Joy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yuge Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip H.S. Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rainforth_T/0/1/0/all/0/1"&gt;Tom Rainforth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmon_S/0/1/0/all/0/1"&gt;Sebastian M. Schmon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siddharth_N/0/1/0/all/0/1"&gt;N. Siddharth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lagrangian dual framework for conservative neural network solutions of kinetic equations. (arXiv:2106.12147v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2106.12147</id>
        <link href="http://arxiv.org/abs/2106.12147"/>
        <updated>2021-06-24T01:51:43.769Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel conservative formulation for solving
kinetic equations via neural networks. More precisely, we formulate the
learning problem as a constrained optimization problem with constraints that
represent the physical conservation laws. The constraints are relaxed toward
the residual loss function by the Lagrangian duality. By imposing physical
conservation properties of the solution as constraints of the learning problem,
we demonstrate far more accurate approximations of the solutions in terms of
errors and the conservation laws, for the kinetic Fokker-Planck equation and
the homogeneous Boltzmann equation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Hwang_H/0/1/0/all/0/1"&gt;Hyung Ju Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Son_H/0/1/0/all/0/1"&gt;Hwijae Son&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Predictions in Neural ODEs: Identification and Interventions. (arXiv:2106.12430v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12430</id>
        <link href="http://arxiv.org/abs/2106.12430"/>
        <updated>2021-06-24T01:51:43.754Z</updated>
        <summary type="html"><![CDATA[Spurred by tremendous success in pattern matching and prediction tasks,
researchers increasingly resort to machine learning to aid original scientific
discovery. Given large amounts of observational data about a system, can we
uncover the rules that govern its evolution? Solving this task holds the great
promise of fully understanding the causal interactions and being able to make
reliable predictions about the system's behavior under interventions. We take a
step towards answering this question for time-series data generated from
systems of ordinary differential equations (ODEs). While the governing ODEs
might not be identifiable from data alone, we show that combining simple
regularization schemes with flexible neural ODEs can robustly recover the
dynamics and causal structures from time-series data. Our results on a variety
of (non)-linear first and second order systems as well as real data validate
our method. We conclude by showing that we can also make accurate predictions
under interventions on variables or the system itself.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aliee_H/0/1/0/all/0/1"&gt;Hananeh Aliee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theis_F/0/1/0/all/0/1"&gt;Fabian J. Theis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kilbertus_N/0/1/0/all/0/1"&gt;Niki Kilbertus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diabetic Retinopathy Detection using Ensemble Machine Learning. (arXiv:2106.12545v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12545</id>
        <link href="http://arxiv.org/abs/2106.12545"/>
        <updated>2021-06-24T01:51:43.748Z</updated>
        <summary type="html"><![CDATA[Diabetic Retinopathy (DR) is among the worlds leading vision loss causes in
diabetic patients. DR is a microvascular disease that affects the eye retina,
which causes vessel blockage and therefore cuts the main source of nutrition
for the retina tissues. Treatment for this visual disorder is most effective
when it is detected in its earliest stages, as severe DR can result in
irreversible blindness. Nonetheless, DR identification requires the expertise
of Ophthalmologists which is often expensive and time-consuming. Therefore,
automatic detection systems were introduced aiming to facilitate the
identification process, making it available globally in a time and
cost-efficient manner. However, due to the limited reliable datasets and
medical records for this particular eye disease, the obtained predictions
accuracies were relatively unsatisfying for eye specialists to rely on them as
diagnostic systems. Thus, we explored an ensemble-based learning strategy,
merging a substantial selection of well-known classification algorithms in one
sophisticated diagnostic model. The proposed framework achieved the highest
accuracy rates among all other common classification algorithms in the area. 4
subdatasets were generated to contain the top 5 and top 10 features of the
Messidor dataset, selected by InfoGainEval. and WrapperSubsetEval., accuracies
of 70.7% and 75.1% were achieved on the InfoGainEval. top 5 and original
dataset respectively. The results imply the impressive performance of the
subdataset, which significantly conduces to a less complex classification
process]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Odeh_I/0/1/0/all/0/1"&gt;Israa Odeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alkasassbeh_M/0/1/0/all/0/1"&gt;Mouhammd Alkasassbeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alauthman_M/0/1/0/all/0/1"&gt;Mohammad Alauthman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning for improved global precipitation in numerical weather prediction systems. (arXiv:2106.12045v1 [physics.ao-ph])]]></title>
        <id>http://arxiv.org/abs/2106.12045</id>
        <link href="http://arxiv.org/abs/2106.12045"/>
        <updated>2021-06-24T01:51:43.743Z</updated>
        <summary type="html"><![CDATA[The formation of precipitation in state-of-the-art weather and climate models
is an important process. The understanding of its relationship with other
variables can lead to endless benefits, particularly for the world's monsoon
regions dependent on rainfall as a support for livelihood. Various factors play
a crucial role in the formation of rainfall, and those physical processes are
leading to significant biases in the operational weather forecasts. We use the
UNET architecture of a deep convolutional neural network with residual learning
as a proof of concept to learn global data-driven models of precipitation. The
models are trained on reanalysis datasets projected on the cubed-sphere
projection to minimize errors due to spherical distortion. The results are
compared with the operational dynamical model used by the India Meteorological
Department. The theoretical deep learning-based model shows doubling of the
grid point, as well as area averaged skill measured in Pearson correlation
coefficients relative to operational system. This study is a proof-of-concept
showing that residual learning-based UNET can unravel physical relationships to
target precipitation, and those physical constraints can be used in the
dynamical operational models towards improved precipitation forecasts. Our
results pave the way for the development of online, hybrid models in the
future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Singh_M/0/1/0/all/0/1"&gt;Manmeet Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kumar_B/0/1/0/all/0/1"&gt;Bipin Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Niyogi_D/0/1/0/all/0/1"&gt;Dev Niyogi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Rao_S/0/1/0/all/0/1"&gt;Suryachandra Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Gill_S/0/1/0/all/0/1"&gt;Sukhpal Singh Gill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Chattopadhyay_R/0/1/0/all/0/1"&gt;Rajib Chattopadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Nanjundiah_R/0/1/0/all/0/1"&gt;Ravi S Nanjundiah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BFTrainer: Low-Cost Training of Neural Networks on Unfillable Supercomputer Nodes. (arXiv:2106.12091v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2106.12091</id>
        <link href="http://arxiv.org/abs/2106.12091"/>
        <updated>2021-06-24T01:51:43.736Z</updated>
        <summary type="html"><![CDATA[Supercomputer FCFS-based scheduling policies result in many transient idle
nodes, a phenomenon that is only partially alleviated by backfill scheduling
methods that promote small jobs to run before large jobs. Here we describe how
to realize a novel use for these otherwise wasted resources, namely, deep
neural network (DNN) training. This important workload is easily organized as
many small fragments that can be configured dynamically to fit essentially any
node*time hole in a supercomputer's schedule. We describe how the task of
rescaling suitable DNN training tasks to fit dynamically changing holes can be
formulated as a deterministic mixed integer linear programming (MILP)-based
resource allocation algorithm, and show that this MILP problem can be solved
efficiently at run time. We show further how this MILP problem can be adapted
to optimize for administrator- or user-defined metrics. We validate our method
with supercomputer scheduler logs and different DNN training scenarios, and
demonstrate efficiencies of up to 93% compared with running the same training
tasks on dedicated nodes. Our method thus enables substantial supercomputer
resources to be allocated to DNN training with no impact on other applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhengchun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kettimuthu_R/0/1/0/all/0/1"&gt;Rajkumar Kettimuthu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papka_M/0/1/0/all/0/1"&gt;Michael E. Papka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foster_I/0/1/0/all/0/1"&gt;Ian Foster&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information Bottleneck Attribution for Visual Explanations of Diagnosis and Prognosis. (arXiv:2104.02869v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02869</id>
        <link href="http://arxiv.org/abs/2104.02869"/>
        <updated>2021-06-24T01:51:43.731Z</updated>
        <summary type="html"><![CDATA[Visual explanation methods have an important role in the prognosis of the
patients where the annotated data is limited or unavailable. There have been
several attempts to use gradient-based attribution methods to localize
pathology from medical scans without using segmentation labels. This research
direction has been impeded by the lack of robustness and reliability. These
methods are highly sensitive to the network parameters. In this study, we
introduce a robust visual explanation method to address this problem for
medical applications. We provide an innovative visual explanation algorithm for
general purpose and as an example application, we demonstrate its effectiveness
for quantifying lesions in the lungs caused by the Covid-19 with high accuracy
and robustness without using dense segmentation labels. This approach overcomes
the drawbacks of commonly used Grad-CAM and its extended versions. The premise
behind our proposed strategy is that the information flow is minimized while
ensuring the classifier prediction stays similar. Our findings indicate that
the bottleneck condition provides a more stable severity estimation than the
similar attribution methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Demir_U/0/1/0/all/0/1"&gt;Ugur Demir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Irmakci_I/0/1/0/all/0/1"&gt;Ismail Irmakci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Keles_E/0/1/0/all/0/1"&gt;Elif Keles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Topcu_A/0/1/0/all/0/1"&gt;Ahmet Topcu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Ziyue Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Spampinato_C/0/1/0/all/0/1"&gt;Concetto Spampinato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jambawalikar_S/0/1/0/all/0/1"&gt;Sachin Jambawalikar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Turkbey_E/0/1/0/all/0/1"&gt;Evrim Turkbey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Turkbey_B/0/1/0/all/0/1"&gt;Baris Turkbey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bagci_U/0/1/0/all/0/1"&gt;Ulas Bagci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FLOP: Federated Learning on Medical Datasets using Partial Networks. (arXiv:2102.05218v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05218</id>
        <link href="http://arxiv.org/abs/2102.05218"/>
        <updated>2021-06-24T01:51:43.725Z</updated>
        <summary type="html"><![CDATA[The outbreak of COVID-19 Disease due to the novel coronavirus has caused a
shortage of medical resources. To aid and accelerate the diagnosis process,
automatic diagnosis of COVID-19 via deep learning models has recently been
explored by researchers across the world. While different data-driven deep
learning models have been developed to mitigate the diagnosis of COVID-19, the
data itself is still scarce due to patient privacy concerns. Federated Learning
(FL) is a natural solution because it allows different organizations to
cooperatively learn an effective deep learning model without sharing raw data.
However, recent studies show that FL still lacks privacy protection and may
cause data leakage. We investigate this challenging problem by proposing a
simple yet effective algorithm, named \textbf{F}ederated \textbf{L}earning
\textbf{o}n Medical Datasets using \textbf{P}artial Networks (FLOP), that
shares only a partial model between the server and clients. Extensive
experiments on benchmark data and real-world healthcare tasks show that our
approach achieves comparable or better performance while reducing the privacy
and security risks. Of particular interest, we conduct experiments on the
COVID-19 dataset and find that our FLOP algorithm can allow different hospitals
to collaboratively and effectively train a partially shared model without
sharing local patients' data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qian Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_W/0/1/0/all/0/1"&gt;Weituo Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spell_G/0/1/0/all/0/1"&gt;Gregory Spell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1"&gt;Lawrence Carin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Self-training for Cross-domain Unsupervised Tagged-to-Cine MRI Synthesis. (arXiv:2106.12499v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12499</id>
        <link href="http://arxiv.org/abs/2106.12499"/>
        <updated>2021-06-24T01:51:43.719Z</updated>
        <summary type="html"><![CDATA[Self-training based unsupervised domain adaptation (UDA) has shown great
potential to address the problem of domain shift, when applying a trained deep
learning model in a source domain to unlabeled target domains. However, while
the self-training UDA has demonstrated its effectiveness on discriminative
tasks, such as classification and segmentation, via the reliable pseudo-label
selection based on the softmax discrete histogram, the self-training UDA for
generative tasks, such as image synthesis, is not fully investigated. In this
work, we propose a novel generative self-training (GST) UDA framework with
continuous value prediction and regression objective for cross-domain image
synthesis. Specifically, we propose to filter the pseudo-label with an
uncertainty mask, and quantify the predictive confidence of generated images
with practical variational Bayes learning. The fast test-time adaptation is
achieved by a round-based alternative optimization scheme. We validated our
framework on the tagged-to-cine magnetic resonance imaging (MRI) synthesis
problem, where datasets in the source and target domains were acquired from
different scanners or centers. Extensive validations were carried out to verify
our framework against popular adversarial training UDA methods. Results show
that our GST, with tagged MRI of test subjects in new target domains, improved
the synthesis quality by a large margin, compared with the adversarial training
UDA methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1"&gt;Fangxu Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stone_M/0/1/0/all/0/1"&gt;Maureen Stone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuo_J/0/1/0/all/0/1"&gt;Jiachen Zhuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Timothy_R/0/1/0/all/0/1"&gt;Reese Timothy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prince_J/0/1/0/all/0/1"&gt;Jerry L. Prince&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1"&gt;Georges El Fakhri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1"&gt;Jonghye Woo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Secure Domain Adaptation with Multiple Sources. (arXiv:2106.12124v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12124</id>
        <link href="http://arxiv.org/abs/2106.12124"/>
        <updated>2021-06-24T01:51:43.713Z</updated>
        <summary type="html"><![CDATA[Multi-source unsupervised domain adaptation (MUDA) is a recently explored
learning framework, where the goal is to address the challenge of labeled data
scarcity in a target domain via transferring knowledge from multiple source
domains with annotated data. Since the source data is distributed, the privacy
of source domains' data can be a natural concern. We benefit from the idea of
domain alignment in an embedding space to address the privacy concern for MUDA.
Our method is based on aligning the sources and target distributions indirectly
via internally learned distributions, without communicating data samples
between domains. We justify our approach theoretically and perform extensive
experiments to demonstrate that our method is effective and compares favorably
against existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stan_S/0/1/0/all/0/1"&gt;Serban Stan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1"&gt;Mohammad Rostami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[S$^2$-MLP: Spatial-Shift MLP Architecture for Vision. (arXiv:2106.07477v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07477</id>
        <link href="http://arxiv.org/abs/2106.07477"/>
        <updated>2021-06-24T01:51:43.676Z</updated>
        <summary type="html"><![CDATA[Recently, visual Transformer (ViT) and its following works abandon the
convolution and exploit the self-attention operation, attaining a comparable or
even higher accuracy than CNNs. More recently, MLP-Mixer abandons both the
convolution and the self-attention operation, proposing an architecture
containing only MLP layers. To achieve cross-patch communications, it devises
an additional token-mixing MLP besides the channel-mixing MLP. It achieves
promising results when training on an extremely large-scale dataset. But it
cannot achieve as outstanding performance as its CNN and ViT counterparts when
training on medium-scale datasets such as ImageNet1K and ImageNet21K. The
performance drop of MLP-Mixer motivates us to rethink the token-mixing MLP. We
discover that the token-mixing MLP is a variant of the depthwise convolution
with a global reception field and spatial-specific configuration. But the
global reception field and the spatial-specific property make token-mixing MLP
prone to over-fitting. In this paper, we propose a novel pure MLP architecture,
spatial-shift MLP (S$^2$-MLP). Different from MLP-Mixer, our S$^2$-MLP only
contains channel-mixing MLP. We utilize a spatial-shift operation for
communications between patches. It has a local reception field and is
spatial-agnostic. It is parameter-free and efficient for computation. The
proposed S$^2$-MLP attains higher recognition accuracy than MLP-Mixer when
training on ImageNet-1K dataset. Meanwhile, S$^2$-MLP accomplishes as excellent
performance as ViT on ImageNet-1K dataset with considerably simpler
architecture and fewer FLOPs and parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1"&gt;Tan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1"&gt;Yunfeng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Mingming Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Ping Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[First Step Towards EXPLAINable DGA Multiclass Classification. (arXiv:2106.12336v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.12336</id>
        <link href="http://arxiv.org/abs/2106.12336"/>
        <updated>2021-06-24T01:51:43.669Z</updated>
        <summary type="html"><![CDATA[Numerous malware families rely on domain generation algorithms (DGAs) to
establish a connection to their command and control (C2) server. Counteracting
DGAs, several machine learning classifiers have been proposed enabling the
identification of the DGA that generated a specific domain name and thus
triggering targeted remediation measures. However, the proposed
state-of-the-art classifiers are based on deep learning models. The black box
nature of these makes it difficult to evaluate their reasoning. The resulting
lack of confidence makes the utilization of such models impracticable. In this
paper, we propose EXPLAIN, a feature-based and contextless DGA multiclass
classifier. We comparatively evaluate several combinations of feature sets and
hyperparameters for our approach against several state-of-the-art classifiers
in a unified setting on the same real-world data. Our classifier achieves
competitive results, is real-time capable, and its predictions are easier to
trace back to features than the predictions made by the DGA multiclass
classifiers proposed in related work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Drichel_A/0/1/0/all/0/1"&gt;Arthur Drichel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faerber_N/0/1/0/all/0/1"&gt;Nils Faerber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meyer_U/0/1/0/all/0/1"&gt;Ulrike Meyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reducing Textural Bias Improves Robustness of Deep Segmentation Models. (arXiv:2011.15093v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.15093</id>
        <link href="http://arxiv.org/abs/2011.15093"/>
        <updated>2021-06-24T01:51:43.661Z</updated>
        <summary type="html"><![CDATA[Despite advances in deep learning, robustness under domain shift remains a
major bottleneck in medical imaging settings. Findings on natural images
suggest that deep neural models can show a strong textural bias when carrying
out image classification tasks. In this thorough empirical study, we draw
inspiration from findings on natural images and investigate ways in which
addressing the textural bias phenomenon could bring up the robustness of deep
segmentation models when applied to three-dimensional (3D) medical data. To
achieve this, publicly available MRI scans from the Developing Human Connectome
Project are used to study ways in which simulating textural noise can help
train robust models in a complex semantic segmentation task. We contribute an
extensive empirical investigation consisting of 176 experiments and illustrate
how applying specific types of simulated textural noise prior to training can
lead to texture invariant models, resulting in improved robustness when
segmenting scans corrupted by previously unseen noise types and levels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chai_S/0/1/0/all/0/1"&gt;Seoin Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1"&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fetit_A/0/1/0/all/0/1"&gt;Ahmed E. Fetit&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Identity-Preserving Transformations on Data Manifolds. (arXiv:2106.12096v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12096</id>
        <link href="http://arxiv.org/abs/2106.12096"/>
        <updated>2021-06-24T01:51:43.652Z</updated>
        <summary type="html"><![CDATA[Many machine learning techniques incorporate identity-preserving
transformations into their models to generalize their performance to previously
unseen data. These transformations are typically selected from a set of
functions that are known to maintain the identity of an input when applied
(e.g., rotation, translation, flipping, and scaling). However, there are many
natural variations that cannot be labeled for supervision or defined through
examination of the data. As suggested by the manifold hypothesis, many of these
natural variations live on or near a low-dimensional, nonlinear manifold.
Several techniques represent manifold variations through a set of learned Lie
group operators that define directions of motion on the manifold. However
theses approaches are limited because they require transformation labels when
training their models and they lack a method for determining which regions of
the manifold are appropriate for applying each specific operator. We address
these limitations by introducing a learning strategy that does not require
transformation labels and developing a method that learns the local regions
where each operator is likely to be used while preserving the identity of
inputs. Experiments on MNIST and Fashion MNIST highlight our model's ability to
learn identity-preserving transformations on multi-class datasets.
Additionally, we train on CelebA to showcase our model's ability to learn
semantically meaningful transformations on complex datasets in an unsupervised
manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Connor_M/0/1/0/all/0/1"&gt;Marissa Connor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fallah_K/0/1/0/all/0/1"&gt;Kion Fallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rozell_C/0/1/0/all/0/1"&gt;Christopher Rozell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alias-Free Generative Adversarial Networks. (arXiv:2106.12423v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12423</id>
        <link href="http://arxiv.org/abs/2106.12423"/>
        <updated>2021-06-24T01:51:43.645Z</updated>
        <summary type="html"><![CDATA[We observe that despite their hierarchical convolutional nature, the
synthesis process of typical generative adversarial networks depends on
absolute pixel coordinates in an unhealthy manner. This manifests itself as,
e.g., detail appearing to be glued to image coordinates instead of the surfaces
of depicted objects. We trace the root cause to careless signal processing that
causes aliasing in the generator network. Interpreting all signals in the
network as continuous, we derive generally applicable, small architectural
changes that guarantee that unwanted information cannot leak into the
hierarchical synthesis process. The resulting networks match the FID of
StyleGAN2 but differ dramatically in their internal representations, and they
are fully equivariant to translation and rotation even at subpixel scales. Our
results pave the way for generative models better suited for video and
animation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karras_T/0/1/0/all/0/1"&gt;Tero Karras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aittala_M/0/1/0/all/0/1"&gt;Miika Aittala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laine_S/0/1/0/all/0/1"&gt;Samuli Laine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harkonen_E/0/1/0/all/0/1"&gt;Erik H&amp;#xe4;rk&amp;#xf6;nen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hellsten_J/0/1/0/all/0/1"&gt;Janne Hellsten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehtinen_J/0/1/0/all/0/1"&gt;Jaakko Lehtinen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aila_T/0/1/0/all/0/1"&gt;Timo Aila&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Practical & Unified Notation for Information-Theoretic Quantities in ML. (arXiv:2106.12062v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12062</id>
        <link href="http://arxiv.org/abs/2106.12062"/>
        <updated>2021-06-24T01:51:43.618Z</updated>
        <summary type="html"><![CDATA[Information theory is of importance to machine learning, but the notation for
information-theoretic quantities is sometimes opaque. The right notation can
convey valuable intuitions and concisely express new ideas. We propose such a
notation for machine learning users and expand it to include
information-theoretic quantities between events (outcomes) and random
variables. We apply this notation to a popular information-theoretic
acquisition function in Bayesian active learning which selects the most
informative (unlabelled) samples to be labelled by an expert. We demonstrate
the value of our notation when extending the acquisition function to the
core-set problem, which consists of selecting the most informative samples
\emph{given} the labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kirsch_A/0/1/0/all/0/1"&gt;Andreas Kirsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Canonical Correlation Analysis to Self-supervised Graph Neural Networks. (arXiv:2106.12484v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12484</id>
        <link href="http://arxiv.org/abs/2106.12484"/>
        <updated>2021-06-24T01:51:43.607Z</updated>
        <summary type="html"><![CDATA[We introduce a conceptually simple yet effective model for self-supervised
representation learning with graph data. It follows the previous methods that
generate two views of an input graph through data augmentation. However, unlike
contrastive methods that focus on instance-level discrimination, we optimize an
innovative feature-level objective inspired by classical Canonical Correlation
Analysis. Compared with other works, our approach requires none of the
parameterized mutual information estimator, additional projector, asymmetric
structures, and most importantly, negative samples which can be costly. We show
that the new objective essentially 1) aims at discarding augmentation-variant
information by learning invariant representations, and 2) can prevent
degenerated solutions by decorrelating features in different dimensions. Our
theoretical analysis further provides an understanding for the new objective
which can be equivalently seen as an instantiation of the Information
Bottleneck Principle under the self-supervised setting. Despite its simplicity,
our method performs competitively on seven public graph datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hengrui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qitian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junchi Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wipf_D/0/1/0/all/0/1"&gt;David Wipf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip S. Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Universal Consistency of Deep Convolutional Neural Networks. (arXiv:2106.12498v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12498</id>
        <link href="http://arxiv.org/abs/2106.12498"/>
        <updated>2021-06-24T01:51:43.593Z</updated>
        <summary type="html"><![CDATA[Compared with avid research activities of deep convolutional neural networks
(DCNNs) in practice, the study of theoretical behaviors of DCNNs lags heavily
behind. In particular, the universal consistency of DCNNs remains open. In this
paper, we prove that implementing empirical risk minimization on DCNNs with
expansive convolution (with zero-padding) is strongly universally consistent.
Motivated by the universal consistency, we conduct a series of experiments to
show that without any fully connected layers, DCNNs with expansive convolution
perform not worse than the widely used deep neural networks with hybrid
structure containing contracting (without zero-padding) convolution layers and
several fully connected layers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Shao-Bo Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kaidong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Ding-Xuan Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classifying Textual Data with Pre-trained Vision Models through Transfer Learning and Data Transformations. (arXiv:2106.12479v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12479</id>
        <link href="http://arxiv.org/abs/2106.12479"/>
        <updated>2021-06-24T01:51:43.588Z</updated>
        <summary type="html"><![CDATA[Knowledge is acquired by humans through experience, and no boundary is set
between the kinds of knowledge or skill levels we can achieve on different
tasks at the same time. When it comes to Neural Networks, that is not the case,
the major breakthroughs in the field are extremely task and domain specific.
Vision and language are dealt with in separate manners, using separate methods
and different datasets. In this work, we propose to use knowledge acquired by
benchmark Vision Models which are trained on ImageNet to help a much smaller
architecture learn to classify text. After transforming the textual data
contained in the IMDB dataset to gray scale images. An analysis of different
domains and the Transfer Learning method is carried out. Despite the challenge
posed by the very different datasets, promising results are achieved. The main
contribution of this work is a novel approach which links large pretrained
models on both language and vision to achieve state-of-the-art results in
different sub-fields from the original task. Without needing high compute
capacity resources. Specifically, Sentiment Analysis is achieved after
transferring knowledge between vision and language models. BERT embeddings are
transformed into grayscale images, these images are then used as training
examples for pretrained vision models such as VGG16 and ResNet

Index Terms: Natural language, Vision, BERT, Transfer Learning, CNN, Domain
Adaptation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Benarab_C/0/1/0/all/0/1"&gt;Charaf Eddine Benarab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HAWQV3: Dyadic Neural Network Quantization. (arXiv:2011.10680v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10680</id>
        <link href="http://arxiv.org/abs/2011.10680"/>
        <updated>2021-06-24T01:51:43.582Z</updated>
        <summary type="html"><![CDATA[Current low-precision quantization algorithms often have the hidden cost of
conversion back and forth from floating point to quantized integer values. This
hidden cost limits the latency improvement realized by quantizing Neural
Networks. To address this, we present HAWQV3, a novel mixed-precision
integer-only quantization framework. The contributions of HAWQV3 are the
following: (i) An integer-only inference where the entire computational graph
is performed only with integer multiplication, addition, and bit shifting,
without any floating point operations or even integer division; (ii) A novel
hardware-aware mixed-precision quantization method where the bit-precision is
calculated by solving an integer linear programming problem that balances the
trade-off between model perturbation and other constraints, e.g., memory
footprint and latency; (iii) Direct hardware deployment and open source
contribution for 4-bit uniform/mixed-precision quantization in TVM, achieving
an average speed up of $1.45\times$ for uniform 4-bit, as compared to uniform
8-bit for ResNet50 on T4 GPUs; and (iv) extensive evaluation of the proposed
methods on ResNet18/50 and InceptionV3, for various model compression levels
with/without mixed precision. For ResNet50, our INT8 quantization achieves an
accuracy of $77.58\%$, which is $2.68\%$ higher than prior integer-only work,
and our mixed-precision INT4/8 quantization can reduce INT8 latency by $23\%$
and still achieve $76.73\%$ accuracy. Our framework and the TVM implementation
have been open sourced.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1"&gt;Zhewei Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1"&gt;Zhen Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhangcheng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1"&gt;Amir Gholami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jiali Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_E/0/1/0/all/0/1"&gt;Eric Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Leyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qijing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yida Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1"&gt;Michael W. Mahoney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1"&gt;Kurt Keutzer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[groupShapley: Efficient prediction explanation with Shapley values for feature groups. (arXiv:2106.12228v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.12228</id>
        <link href="http://arxiv.org/abs/2106.12228"/>
        <updated>2021-06-24T01:51:43.567Z</updated>
        <summary type="html"><![CDATA[Shapley values has established itself as one of the most appropriate and
theoretically sound frameworks for explaining predictions from complex machine
learning models. The popularity of Shapley values in the explanation setting is
probably due to its unique theoretical properties. The main drawback with
Shapley values, however, is that its computational complexity grows
exponentially in the number of input features, making it unfeasible in many
real world situations where there could be hundreds or thousands of features.
Furthermore, with many (dependent) features, presenting/visualizing and
interpreting the computed Shapley values also becomes challenging. The present
paper introduces groupShapley: a conceptually simple approach for dealing with
the aforementioned bottlenecks. The idea is to group the features, for example
by type or dependence, and then compute and present Shapley values for these
groups instead of for all individual features. Reducing hundreds or thousands
of features to half a dozen or so, makes precise computations practically
feasible and the presentation and knowledge extraction greatly simplified. We
prove that under certain conditions, groupShapley is equivalent to summing the
feature-wise Shapley values within each feature group. Moreover, we provide a
simulation study exemplifying the differences when these conditions are not
met. We illustrate the usability of the approach in a real world car insurance
example, where groupShapley is used to provide simple and intuitive
explanations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Jullum_M/0/1/0/all/0/1"&gt;Martin Jullum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Redelmeier_A/0/1/0/all/0/1"&gt;Annabelle Redelmeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Aas_K/0/1/0/all/0/1"&gt;Kjersti Aas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adapting Off-the-Shelf Source Segmenter for Target Medical Image Segmentation. (arXiv:2106.12497v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12497</id>
        <link href="http://arxiv.org/abs/2106.12497"/>
        <updated>2021-06-24T01:51:43.562Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from
a labeled source domain to an unlabeled and unseen target domain, which is
usually trained on data from both domains. Access to the source domain data at
the adaptation stage, however, is often limited, due to data storage or privacy
issues. To alleviate this, in this work, we target source free UDA for
segmentation, and propose to adapt an ``off-the-shelf" segmentation model
pre-trained in the source domain to the target domain, with an adaptive
batch-wise normalization statistics adaptation framework. Specifically, the
domain-specific low-order batch statistics, i.e., mean and variance, are
gradually adapted with an exponential momentum decay scheme, while the
consistency of domain shareable high-order batch statistics, i.e., scaling and
shifting parameters, is explicitly enforced by our optimization objective. The
transferability of each channel is adaptively measured first from which to
balance the contribution of each channel. Moreover, the proposed source free
UDA framework is orthogonal to unsupervised learning methods, e.g.,
self-entropy minimization, which can thus be simply added on top of our
framework. Extensive experiments on the BraTS 2018 database show that our
source free UDA framework outperformed existing source-relaxed UDA methods for
the cross-subtype UDA segmentation task and yielded comparable results for the
cross-modality UDA segmentation task, compared with a supervised UDA methods
with the source data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1"&gt;Fangxu Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1"&gt;Georges El Fakhri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1"&gt;Jonghye Woo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pure Exploration in Kernel and Neural Bandits. (arXiv:2106.12034v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.12034</id>
        <link href="http://arxiv.org/abs/2106.12034"/>
        <updated>2021-06-24T01:51:43.556Z</updated>
        <summary type="html"><![CDATA[We study pure exploration in bandits, where the dimension of the feature
representation can be much larger than the number of arms. To overcome the
curse of dimensionality, we propose to adaptively embed the feature
representation of each arm into a lower-dimensional space and carefully deal
with the induced model misspecifications. Our approach is conceptually very
different from existing works that can either only handle low-dimensional
linear bandits or passively deal with model misspecifications. We showcase the
application of our approach to two pure exploration settings that were
previously under-studied: (1) the reward function belongs to a possibly
infinite-dimensional Reproducing Kernel Hilbert Space, and (2) the reward
function is nonlinear and can be approximated by neural networks. Our main
results provide sample complexity guarantees that only depend on the effective
dimension of the feature spaces in the kernel or neural representations.
Extensive experiments conducted on both synthetic and real-world datasets
demonstrate the efficacy of our methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yinglun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Dongruo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jiang_R/0/1/0/all/0/1"&gt;Ruoxi Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gu_Q/0/1/0/all/0/1"&gt;Quanquan Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Willett_R/0/1/0/all/0/1"&gt;Rebecca Willett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nowak_R/0/1/0/all/0/1"&gt;Robert Nowak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Neural Network Based Respiratory Pathology Classification Using Cough Sounds. (arXiv:2106.12174v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12174</id>
        <link href="http://arxiv.org/abs/2106.12174"/>
        <updated>2021-06-24T01:51:43.551Z</updated>
        <summary type="html"><![CDATA[Intelligent systems are transforming the world, as well as our healthcare
system. We propose a deep learning-based cough sound classification model that
can distinguish between children with healthy versus pathological coughs such
as asthma, upper respiratory tract infection (URTI), and lower respiratory
tract infection (LRTI). In order to train a deep neural network model, we
collected a new dataset of cough sounds, labelled with clinician's diagnosis.
The chosen model is a bidirectional long-short term memory network (BiLSTM)
based on Mel Frequency Cepstral Coefficients (MFCCs) features. The resulting
trained model when trained for classifying two classes of coughs -- healthy or
pathology (in general or belonging to a specific respiratory pathology),
reaches accuracy exceeding 84\% when classifying cough to the label provided by
the physicians' diagnosis. In order to classify subject's respiratory pathology
condition, results of multiple cough epochs per subject were combined. The
resulting prediction accuracy exceeds 91\% for all three respiratory
pathologies. However, when the model is trained to classify and discriminate
among the four classes of coughs, overall accuracy dropped: one class of
pathological coughs are often misclassified as other. However, if one consider
the healthy cough classified as healthy and pathological cough classified to
have some kind of pathologies, then the overall accuracy of four class model is
above 84\%. A longitudinal study of MFCC feature space when comparing
pathological and recovered coughs collected from the same subjects revealed the
fact that pathological cough irrespective of the underlying conditions occupy
the same feature space making it harder to differentiate only using MFCC
features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+T_B/0/1/0/all/0/1"&gt;Balamurali B T&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hee_H/0/1/0/all/0/1"&gt;Hwan Ing Hee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kapoor_S/0/1/0/all/0/1"&gt;Saumitra Kapoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teoh_O/0/1/0/all/0/1"&gt;Oon Hoe Teoh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teng_S/0/1/0/all/0/1"&gt;Sung Shin Teng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Khai Pin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herremans_D/0/1/0/all/0/1"&gt;Dorien Herremans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jer Ming Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty-Aware Model-Based Reinforcement Learning with Application to Autonomous Driving. (arXiv:2106.12194v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.12194</id>
        <link href="http://arxiv.org/abs/2106.12194"/>
        <updated>2021-06-24T01:51:43.546Z</updated>
        <summary type="html"><![CDATA[To further improve the learning efficiency and performance of reinforcement
learning (RL), in this paper we propose a novel uncertainty-aware model-based
RL (UA-MBRL) framework, and then implement and validate it in autonomous
driving under various task scenarios. First, an action-conditioned ensemble
model with the ability of uncertainty assessment is established as the virtual
environment model. Then, a novel uncertainty-aware model-based RL framework is
developed based on the adaptive truncation approach, providing virtual
interactions between the agent and environment model, and improving RL's
training efficiency and performance. The developed algorithms are then
implemented in end-to-end autonomous vehicle control tasks, validated and
compared with state-of-the-art methods under various driving scenarios. The
validation results suggest that the proposed UA-MBRL method surpasses the
existing model-based and model-free RL approaches, in terms of learning
efficiency and achieved performance. The results also demonstrate the good
ability of the proposed method with respect to the adaptiveness and robustness,
under various autonomous driving scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jingda Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhiyu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1"&gt;Chen Lv&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ADAVI: Automatic Dual Amortized Variational Inference Applied To Pyramidal Bayesian Models. (arXiv:2106.12248v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.12248</id>
        <link href="http://arxiv.org/abs/2106.12248"/>
        <updated>2021-06-24T01:51:43.530Z</updated>
        <summary type="html"><![CDATA[Frequently, population studies feature pyramidally-organized data represented
using Hierarchical Bayesian Models (HBM) enriched with plates. These models can
become prohibitively large in settings such as neuroimaging, where a sample is
composed of a functional MRI signal measured on 64 thousand brain locations,
across 4 measurement sessions, and at least tens of subjects. Even a reduced
example on a specific cortical region of 300 brain locations features around 1
million parameters, hampering the usage of modern density estimation techniques
such as Simulation-Based Inference (SBI). To infer parameter posterior
distributions in this challenging class of problems, we designed a novel
methodology that automatically produces a variational family dual to a target
HBM. This variatonal family, represented as a neural network, consists in the
combination of an attention-based hierarchical encoder feeding summary
statistics to a set of normalizing flows. Our automatically-derived neural
network exploits exchangeability in the plate-enriched HBM and factorizes its
parameter space. The resulting architecture reduces by orders of magnitude its
parameterization with respect to that of a typical SBI representation, while
maintaining expressivity. Our method performs inference on the specified HBM in
an amortized setup: once trained, it can readily be applied to a new data
sample to compute the parameters' full posterior. We demonstrate the capability
of our method on simulated data, as well as a challenging high-dimensional
brain parcellation experiment. We also open up several questions that lie at
the intersection between SBI techniques and structured Variational Inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rouillard_L/0/1/0/all/0/1"&gt;Louis Rouillard&lt;/a&gt; (PARIETAL, Inria, CEA), &lt;a href="http://arxiv.org/find/cs/1/au:+Wassermann_D/0/1/0/all/0/1"&gt;Demian Wassermann&lt;/a&gt; (PARIETAL, Inria, CEA)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Adversarial Neural Architecture Search. (arXiv:2105.09356v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09356</id>
        <link href="http://arxiv.org/abs/2105.09356"/>
        <updated>2021-06-24T01:51:43.523Z</updated>
        <summary type="html"><![CDATA[Despite the empirical success of neural architecture search (NAS) in deep
learning applications, the optimality, reproducibility and cost of NAS schemes
remain hard to assess. In this paper, we propose Generative Adversarial NAS
(GA-NAS) with theoretically provable convergence guarantees, promoting
stability and reproducibility in neural architecture search. Inspired by
importance sampling, GA-NAS iteratively fits a generator to previously
discovered top architectures, thus increasingly focusing on important parts of
a large search space. Furthermore, we propose an efficient adversarial learning
approach, where the generator is trained by reinforcement learning based on
rewards provided by a discriminator, thus being able to explore the search
space without evaluating a large number of architectures. Extensive experiments
show that GA-NAS beats the best published results under several cases on three
public NAS benchmarks. In the meantime, GA-NAS can handle ad-hoc search
constraints and search spaces. We show that GA-NAS can be used to improve
already optimized baselines found by other NAS methods, including EfficientNet
and ProxylessNAS, in terms of ImageNet accuracy or the number of parameters, in
their original search space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rezaei_S/0/1/0/all/0/1"&gt;Seyed Saeed Changiz Rezaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1"&gt;Fred X. Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1"&gt;Di Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salameh_M/0/1/0/all/0/1"&gt;Mohammad Salameh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1"&gt;Keith Mills&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1"&gt;Shuo Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wei Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1"&gt;Shangling Jui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Automated Biometric Identification of Sea Turtles (Chelonia mydas). (arXiv:1909.11277v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.11277</id>
        <link href="http://arxiv.org/abs/1909.11277"/>
        <updated>2021-06-24T01:51:43.517Z</updated>
        <summary type="html"><![CDATA[Passive biometric identification enables wildlife monitoring with minimal
disturbance. Using a motion-activated camera placed at an elevated position and
facing downwards, we collected images of sea turtle carapace, each belonging to
one of sixteen Chelonia mydas juveniles. We then learned co-variant and robust
image descriptors from these images, enabling indexing and retrieval. In this
work, we presented several classification results of sea turtle carapaces using
the learned image descriptors. We found that a template-based descriptor, i.e.,
Histogram of Oriented Gradients (HOG) performed exceedingly better during
classification than keypoint-based descriptors. For our dataset, a
high-dimensional descriptor is a must due to the minimal gradient and color
information inside the carapace images. Using HOG, we obtained an average
classification accuracy of 65%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hipiny_I/0/1/0/all/0/1"&gt;Irwandi Hipiny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ujir_H/0/1/0/all/0/1"&gt;Hamimah Ujir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mujahid_A/0/1/0/all/0/1"&gt;Aazani Mujahid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yahya_N/0/1/0/all/0/1"&gt;Nurhartini Kamalia Yahya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Simple Baseline for Batch Active Learning with Stochastic Acquisition Functions. (arXiv:2106.12059v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12059</id>
        <link href="http://arxiv.org/abs/2106.12059"/>
        <updated>2021-06-24T01:51:43.512Z</updated>
        <summary type="html"><![CDATA[In active learning, new labels are commonly acquired in batches. However,
common acquisition functions are only meant for one-sample acquisition rounds
at a time, and when their scores are used naively for batch acquisition, they
result in batches lacking diversity, which deteriorates performance. On the
other hand, state-of-the-art batch acquisition functions are costly to compute.
In this paper, we present a novel class of stochastic acquisition functions
that extend one-sample acquisition functions to the batch setting by observing
how one-sample acquisition scores change as additional samples are acquired and
modelling this difference for additional batch samples. We simply acquire new
samples by sampling from the pool set using a Gibbs distribution based on the
acquisition scores. Our acquisition functions are both vastly cheaper to
compute and out-perform other batch acquisition functions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kirsch_A/0/1/0/all/0/1"&gt;Andreas Kirsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farquhar_S/0/1/0/all/0/1"&gt;Sebastian Farquhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Rate of Convergence of Variation-Constrained Deep Neural Networks. (arXiv:2106.12068v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12068</id>
        <link href="http://arxiv.org/abs/2106.12068"/>
        <updated>2021-06-24T01:51:43.507Z</updated>
        <summary type="html"><![CDATA[Multi-layer feedforward networks have been used to approximate a wide range
of nonlinear functions. An important and fundamental problem is to understand
the learnability of a network model through its statistical risk, or the
expected prediction error on future data. To the best of our knowledge, the
rate of convergence of neural networks shown by existing works is bounded by at
most the order of $n^{-1/4}$ for a sample size of $n$. In this paper, we show
that a class of variation-constrained neural networks, with arbitrary width,
can achieve near-parametric rate $n^{-1/2+\delta}$ for an arbitrarily small
positive constant $\delta$. It is equivalent to $n^{-1 +2\delta}$ under the
mean squared error. This rate is also observed by numerical experiments. The
result indicates that the neural function space needed for approximating smooth
functions may not be as large as what is often perceived. Our result also
provides insight to the phenomena that deep neural networks do not easily
suffer from overfitting when the number of neurons and learning parameters
rapidly grow with $n$ or even surpass $n$. We also discuss the rate of
convergence regarding other network parameters, including the input dimension,
network layer, and coefficient norm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Gen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yuantao Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1"&gt;Jie Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ParK: Sound and Efficient Kernel Ridge Regression by Feature Space Partitions. (arXiv:2106.12231v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.12231</id>
        <link href="http://arxiv.org/abs/2106.12231"/>
        <updated>2021-06-24T01:51:43.491Z</updated>
        <summary type="html"><![CDATA[We introduce ParK, a new large-scale solver for kernel ridge regression. Our
approach combines partitioning with random projections and iterative
optimization to reduce space and time complexity while provably maintaining the
same statistical accuracy. In particular, constructing suitable partitions
directly in the feature space rather than in the input space, we promote
orthogonality between the local estimators, thus ensuring that key quantities
such as local effective dimension and bias remain under control. We
characterize the statistical-computational tradeoff of our model, and
demonstrate the effectiveness of our method by numerical experiments on
large-scale datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Carratino_L/0/1/0/all/0/1"&gt;Luigi Carratino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vigogna_S/0/1/0/all/0/1"&gt;Stefano Vigogna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Calandriello_D/0/1/0/all/0/1"&gt;Daniele Calandriello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rosasco_L/0/1/0/all/0/1"&gt;Lorenzo Rosasco&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LPF: A Language-Prior Feedback Objective Function for De-biased Visual Question Answering. (arXiv:2105.14300v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14300</id>
        <link href="http://arxiv.org/abs/2105.14300"/>
        <updated>2021-06-24T01:51:43.486Z</updated>
        <summary type="html"><![CDATA[Most existing Visual Question Answering (VQA) systems tend to overly rely on
language bias and hence fail to reason from the visual clue. To address this
issue, we propose a novel Language-Prior Feedback (LPF) objective function, to
re-balance the proportion of each answer's loss value in the total VQA loss.
The LPF firstly calculates a modulating factor to determine the language bias
using a question-only branch. Then, the LPF assigns a self-adaptive weight to
each training sample in the training process. With this reweighting mechanism,
the LPF ensures that the total VQA loss can be reshaped to a more balanced
form. By this means, the samples that require certain visual information to
predict will be efficiently used during training. Our method is simple to
implement, model-agnostic, and end-to-end trainable. We conduct extensive
experiments and the results show that the LPF (1) brings a significant
improvement over various VQA models, (2) achieves competitive performance on
the bias-sensitive VQA-CP v2 benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1"&gt;Zujie Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Haifeng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jiaying Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Permuted AdaIN: Reducing the Bias Towards Global Statistics in Image Classification. (arXiv:2010.05785v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05785</id>
        <link href="http://arxiv.org/abs/2010.05785"/>
        <updated>2021-06-24T01:51:43.481Z</updated>
        <summary type="html"><![CDATA[Recent work has shown that convolutional neural network classifiers overly
rely on texture at the expense of shape cues. We make a similar but different
distinction between shape and local image cues, on the one hand, and global
image statistics, on the other. Our method, called Permuted Adaptive Instance
Normalization (pAdaIN), reduces the representation of global statistics in the
hidden layers of image classifiers. pAdaIN samples a random permutation $\pi$
that rearranges the samples in a given batch. Adaptive Instance Normalization
(AdaIN) is then applied between the activations of each (non-permuted) sample
$i$ and the corresponding activations of the sample $\pi(i)$, thus swapping
statistics between the samples of the batch. Since the global image statistics
are distorted, this swapping procedure causes the network to rely on cues, such
as shape or texture. By choosing the random permutation with probability $p$
and the identity permutation otherwise, one can control the effect's strength.

With the correct choice of $p$, fixed apriori for all experiments and
selected without considering test data, our method consistently outperforms
baselines in multiple settings. In image classification, our method improves on
both CIFAR100 and ImageNet using multiple architectures. In the setting of
robustness, our method improves on both ImageNet-C and Cifar-100-C for multiple
architectures. In the setting of domain adaptation and domain generalization,
our method achieves state of the art results on the transfer learning task from
GTAV to Cityscapes and on the PACS benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nuriel_O/0/1/0/all/0/1"&gt;Oren Nuriel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benaim_S/0/1/0/all/0/1"&gt;Sagie Benaim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1"&gt;Lior Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Gaussian Processes: A Survey. (arXiv:2106.12135v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12135</id>
        <link href="http://arxiv.org/abs/2106.12135"/>
        <updated>2021-06-24T01:51:43.474Z</updated>
        <summary type="html"><![CDATA[Gaussian processes are one of the dominant approaches in Bayesian learning.
Although the approach has been applied to numerous problems with great success,
it has a few fundamental limitations. Multiple methods in literature have
addressed these limitations. However, there has not been a comprehensive survey
of the topics as of yet. Most existing surveys focus on only one particular
variant of Gaussian processes and their derivatives. This survey details the
core motivations for using Gaussian processes, their mathematical formulations,
limitations, and research themes that have flourished over the years to address
said limitations. Furthermore, one particular research area is Deep Gaussian
Processes (DGPs), it has improved substantially in the past decade. The
significant publications that advanced the forefront of this research area are
outlined in their survey. Finally, a brief discussion on open problems and
research directions for future work is presented at the end.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jakkala_K/0/1/0/all/0/1"&gt;Kalvik Jakkala&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SketchEmbedNet: Learning Novel Concepts by Imitating Drawings. (arXiv:2009.04806v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04806</id>
        <link href="http://arxiv.org/abs/2009.04806"/>
        <updated>2021-06-24T01:51:43.468Z</updated>
        <summary type="html"><![CDATA[Sketch drawings capture the salient information of visual concepts. Previous
work has shown that neural networks are capable of producing sketches of
natural objects drawn from a small number of classes. While earlier approaches
focus on generation quality or retrieval, we explore properties of image
representations learned by training a model to produce sketches of images. We
show that this generative, class-agnostic model produces informative embeddings
of images from novel examples, classes, and even novel datasets in a few-shot
setting. Additionally, we find that these learned representations exhibit
interesting structure and compositionality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1"&gt;Alexander Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1"&gt;Mengye Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1"&gt;Richard S. Zemel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TetraPackNet: Four-Corner-Based Object Detection in Logistics Use-Cases. (arXiv:2104.09123v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09123</id>
        <link href="http://arxiv.org/abs/2104.09123"/>
        <updated>2021-06-24T01:51:43.462Z</updated>
        <summary type="html"><![CDATA[While common image object detection tasks focus on bounding boxes or
segmentation masks as object representations, we consider the problem of
finding objects based on four arbitrary vertices. We propose a novel model,
named TetraPackNet, to tackle this problem. TetraPackNet is based on CornerNet
and uses similar algorithms and ideas. It is designated for applications
requiring high-accuracy detection of regularly shaped objects, which is the
case in the logistics use-case of packaging structure recognition. We evaluate
our model on our specific real-world dataset for this use-case. Baselined
against a previous solution, consisting of a Mask R-CNN model and suitable
post-processing steps, TetraPackNet achieves superior results (9% higher in
accuracy) in the sub-task of four-corner based transport unit side detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dorr_L/0/1/0/all/0/1"&gt;Laura D&amp;#xf6;rr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brandt_F/0/1/0/all/0/1"&gt;Felix Brandt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naumann_A/0/1/0/all/0/1"&gt;Alexander Naumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pouls_M/0/1/0/all/0/1"&gt;Martin Pouls&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ShaRF: Shape-conditioned Radiance Fields from a Single View. (arXiv:2102.08860v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08860</id>
        <link href="http://arxiv.org/abs/2102.08860"/>
        <updated>2021-06-24T01:51:43.447Z</updated>
        <summary type="html"><![CDATA[We present a method for estimating neural scenes representations of objects
given only a single image. The core of our method is the estimation of a
geometric scaffold for the object and its use as a guide for the reconstruction
of the underlying radiance field. Our formulation is based on a generative
process that first maps a latent code to a voxelized shape, and then renders it
to an image, with the object appearance being controlled by a second latent
code. During inference, we optimize both the latent codes and the networks to
fit a test image of a new object. The explicit disentanglement of shape and
appearance allows our model to be fine-tuned given a single image. We can then
render new views in a geometrically consistent manner and they represent
faithfully the input object. Additionally, our method is able to generalize to
images outside of the training domain (more realistic renderings and even real
photographs). Finally, the inferred geometric scaffold is itself an accurate
estimate of the object's 3D shape. We demonstrate in several experiments the
effectiveness of our approach in both synthetic and real images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rematas_K/0/1/0/all/0/1"&gt;Konstantinos Rematas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_Brualla_R/0/1/0/all/0/1"&gt;Ricardo Martin-Brualla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1"&gt;Vittorio Ferrari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Near-Optimal Linear Regression under Distribution Shift. (arXiv:2106.12108v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12108</id>
        <link href="http://arxiv.org/abs/2106.12108"/>
        <updated>2021-06-24T01:51:43.442Z</updated>
        <summary type="html"><![CDATA[Transfer learning is essential when sufficient data comes from the source
domain, with scarce labeled data from the target domain. We develop estimators
that achieve minimax linear risk for linear regression problems under
distribution shift. Our algorithms cover different transfer learning settings
including covariate shift and model shift. We also consider when data are
generated from either linear or general nonlinear models. We show that linear
minimax estimators are within an absolute constant of the minimax risk even
among nonlinear estimators for various source/target distributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1"&gt;Qi Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Wei Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jason D. Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coarse-to-Fine Q-attention: Efficient Learning for Visual Robotic Manipulation via Discretisation. (arXiv:2106.12534v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.12534</id>
        <link href="http://arxiv.org/abs/2106.12534"/>
        <updated>2021-06-24T01:51:43.435Z</updated>
        <summary type="html"><![CDATA[Reflecting on the last few years, the biggest breakthroughs in deep
reinforcement learning (RL) have been in the discrete action domain. Robotic
manipulation, however, is inherently a continuous control environment, but
these continuous control reinforcement learning algorithms often depend on
actor-critic methods that are sample-inefficient and inherently difficult to
train, due to the joint optimisation of the actor and critic. To that end, we
explore how we can bring the stability of discrete action RL algorithms to the
robot manipulation domain. We extend the recently released ARM algorithm, by
replacing the continuous next-best pose agent with a discrete next-best pose
agent. Discretisation of rotation is trivial given its bounded nature, while
translation is inherently unbounded, making discretisation difficult. We
formulate the translation prediction as the voxel prediction problem by
discretising the 3D space; however, voxelisation of a large workspace is memory
intensive and would not work with a high density of voxels, crucial to
obtaining the resolution needed for robotic manipulation. We therefore propose
to apply this voxel prediction in a coarse-to-fine manner by gradually
increasing the resolution. In each step, we extract the highest valued voxel as
the predicted location, which is then used as the centre of the
higher-resolution voxelisation in the next step. This coarse-to-fine prediction
is applied over several steps, giving a near-lossless prediction of the
translation. We show that our new coarse-to-fine algorithm is able to
accomplish RLBench tasks much more efficiently than the continuous control
equivalent, and even train some real-world tasks, tabular rasa, in less than 7
minutes, with only 3 demonstrations. Moreover, we show that by moving to a
voxel representation, we are able to easily incorporate observations from
multiple cameras.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1"&gt;Stephen James&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wada_K/0/1/0/all/0/1"&gt;Kentaro Wada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laidlow_T/0/1/0/all/0/1"&gt;Tristan Laidlow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1"&gt;Andrew J. Davison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking supervised learning: insights from biological learning and from calling it by its name. (arXiv:2012.02526v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02526</id>
        <link href="http://arxiv.org/abs/2012.02526"/>
        <updated>2021-06-24T01:51:43.429Z</updated>
        <summary type="html"><![CDATA[The renaissance of artificial neural networks was catalysed by the success of
classification models, tagged by the community with the broader term supervised
learning. The extraordinary results gave rise to a hype loaded with ambitious
promises and overstatements. Soon the community realised that the success owed
much to the availability of thousands of labelled examples and supervised
learning went, for many, from glory to shame: Some criticised deep learning as
a whole and others proclaimed that the way forward had to be alternatives to
supervised learning: predictive, unsupervised, semi-supervised and, more
recently, self-supervised learning. However, all these seem brand names, rather
than actual categories of a theoretically grounded taxonomy. Moreover, the call
to banish supervised learning was motivated by the questionable claim that
humans learn with little or no supervision and are capable of robust
out-of-distribution generalisation. Here, we review insights about learning and
supervision in nature, revisit the notion that learning and generalisation are
not possible without supervision or inductive biases and argue that we will
make better progress if we just call it by its name.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hernandez_Garcia_A/0/1/0/all/0/1"&gt;Alex Hernandez-Garcia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Generalized Spatial-Temporal Deep Feature Representation for No-Reference Video Quality Assessment. (arXiv:2012.13936v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13936</id>
        <link href="http://arxiv.org/abs/2012.13936"/>
        <updated>2021-06-24T01:51:43.422Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a no-reference video quality assessment method,
aiming to achieve high-generalization capability in cross-content, -resolution
and -frame rate quality prediction. In particular, we evaluate the quality of a
video by learning effective feature representations in spatial-temporal domain.
In the spatial domain, to tackle the resolution and content variations, we
impose the Gaussian distribution constraints on the quality features. The
unified distribution can significantly reduce the domain gap between different
video samples, resulting in a more generalized quality feature representation.
Along the temporal dimension, inspired by the mechanism of visual perception,
we propose a pyramid temporal aggregation module by involving the short-term
and long-term memory to aggregate the frame-level quality. Experiments show
that our method outperforms the state-of-the-art methods on cross-dataset
settings, and achieves comparable performance on intra-dataset configurations,
demonstrating the high-generalization capability of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_B/0/1/0/all/0/1"&gt;Baoliang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lingyu Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1"&gt;Guo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fan_H/0/1/0/all/0/1"&gt;Hongfei Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shiqi Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-resolution Outlier Pooling for Sorghum Classification. (arXiv:2106.05748v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05748</id>
        <link href="http://arxiv.org/abs/2106.05748"/>
        <updated>2021-06-24T01:51:43.406Z</updated>
        <summary type="html"><![CDATA[Automated high throughput plant phenotyping involves leveraging sensors, such
as RGB, thermal and hyperspectral cameras (among others), to make large scale
and rapid measurements of the physical properties of plants for the purpose of
better understanding the difference between crops and facilitating rapid plant
breeding programs. One of the most basic phenotyping tasks is to determine the
cultivar, or species, in a particular sensor product. This simple phenotype can
be used to detect errors in planting and to learn the most differentiating
features between cultivars. It is also a challenging visual recognition task,
as a large number of highly related crops are grown simultaneously, leading to
a classification problem with low inter-class variance. In this paper, we
introduce the Sorghum-100 dataset, a large dataset of RGB imagery of sorghum
captured by a state-of-the-art gantry system, a multi-resolution network
architecture that learns both global and fine-grained features on the crops,
and a new global pooling strategy called Dynamic Outlier Pooling which
outperforms standard global pooling strategies on this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_C/0/1/0/all/0/1"&gt;Chao Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dulay_J/0/1/0/all/0/1"&gt;Justin Dulay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rolwes_G/0/1/0/all/0/1"&gt;Gregory Rolwes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pauli_D/0/1/0/all/0/1"&gt;Duke Pauli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shakoor_N/0/1/0/all/0/1"&gt;Nadia Shakoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stylianou_A/0/1/0/all/0/1"&gt;Abby Stylianou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring Human Perception to Improve Handwritten Document Transcription. (arXiv:1904.03734v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.03734</id>
        <link href="http://arxiv.org/abs/1904.03734"/>
        <updated>2021-06-24T01:51:43.401Z</updated>
        <summary type="html"><![CDATA[The subtleties of human perception, as measured by vision scientists through
the use of psychophysics, are important clues to the internal workings of
visual recognition. For instance, measured reaction time can indicate whether a
visual stimulus is easy for a subject to recognize, or whether it is hard. In
this paper, we consider how to incorporate psychophysical measurements of
visual perception into the loss function of a deep neural network being trained
for a recognition task, under the assumption that such information can enforce
consistency with human behavior. As a case study to assess the viability of
this approach, we look at the problem of handwritten document transcription.
While good progress has been made towards automatically transcribing modern
handwriting, significant challenges remain in transcribing historical
documents. Here we describe a general enhancement strategy, underpinned by the
new loss formulation, which can be applied to the training regime of any deep
learning-based document transcription system. Through experimentation, reliable
performance improvement is demonstrated for the standard IAM and RIMES datasets
for three different network architectures. Further, we go on to show
feasibility for our approach on a new dataset of digitized Latin manuscripts,
originally produced by scribes in the Cloister of St. Gall in the the 9th
century.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grieggs_S/0/1/0/all/0/1"&gt;Samuel Grieggs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1"&gt;Bingyu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rauch_G/0/1/0/all/0/1"&gt;Greta Rauch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jiaqi Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1"&gt;David Chiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Price_B/0/1/0/all/0/1"&gt;Brian Price&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scheirer_W/0/1/0/all/0/1"&gt;Walter J. Scheirer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Quanvolutional Neural Networks with enhanced image encoding. (arXiv:2106.07327v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07327</id>
        <link href="http://arxiv.org/abs/2106.07327"/>
        <updated>2021-06-24T01:51:43.394Z</updated>
        <summary type="html"><![CDATA[Image classification is an important task in various machine learning
applications. In recent years, a number of classification methods based on
quantum machine learning and different quantum image encoding techniques have
been proposed. In this paper, we study the effect of three different quantum
image encoding approaches on the performance of a convolution-inspired hybrid
quantum-classical image classification algorithm called quanvolutional neural
network (QNN). We furthermore examine the effect of variational - i.e.
trainable - quantum circuits on the classification results. Our experiments
indicate that some image encodings are better suited for variational circuits.
However, our experiments show as well that there is not one best image
encoding, but that the choice of the encoding depends on the specific
constraints of the application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mattern_D/0/1/0/all/0/1"&gt;Denny Mattern&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martyniuk_D/0/1/0/all/0/1"&gt;Darya Martyniuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Willems_H/0/1/0/all/0/1"&gt;Henri Willems&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bergmann_F/0/1/0/all/0/1"&gt;Fabian Bergmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paschke_A/0/1/0/all/0/1"&gt;Adrian Paschke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Flows with Invertible Attentions. (arXiv:2106.03959v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03959</id>
        <link href="http://arxiv.org/abs/2106.03959"/>
        <updated>2021-06-24T01:51:43.388Z</updated>
        <summary type="html"><![CDATA[Flow-based generative models have shown excellent ability to explicitly learn
the probability density function of data via a sequence of invertible
transformations. Yet, modeling long-range dependencies over normalizing flows
remains understudied. To fill the gap, in this paper, we introduce two types of
invertible attention mechanisms for generative flow models. To be precise, we
propose map-based and scaled dot-product attention for unconditional and
conditional generative flow models. The key idea is to exploit split-based
attention mechanisms to learn the attention weights and input representations
on every two splits of flow feature maps. Our method provides invertible
attention modules with tractable Jacobian determinants, enabling seamless
integration of it at any positions of the flow-based models. The proposed
attention mechanism can model the global data dependencies, leading to more
comprehensive flow models. Evaluation on multiple generation tasks demonstrates
that the introduced attention flow idea results in efficient flow models and
compares favorably against the state-of-the-art unconditional and conditional
generative flow methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sukthanker_R/0/1/0/all/0/1"&gt;Rhea Sanjay Sukthanker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhiwu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Suryansh Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1"&gt;Radu Timofte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Taming Transformers for High-Resolution Image Synthesis. (arXiv:2012.09841v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09841</id>
        <link href="http://arxiv.org/abs/2012.09841"/>
        <updated>2021-06-24T01:51:43.383Z</updated>
        <summary type="html"><![CDATA[Designed to learn long-range interactions on sequential data, transformers
continue to show state-of-the-art results on a wide variety of tasks. In
contrast to CNNs, they contain no inductive bias that prioritizes local
interactions. This makes them expressive, but also computationally infeasible
for long sequences, such as high-resolution images. We demonstrate how
combining the effectiveness of the inductive bias of CNNs with the expressivity
of transformers enables them to model and thereby synthesize high-resolution
images. We show how to (i) use CNNs to learn a context-rich vocabulary of image
constituents, and in turn (ii) utilize transformers to efficiently model their
composition within high-resolution images. Our approach is readily applied to
conditional synthesis tasks, where both non-spatial information, such as object
classes, and spatial information, such as segmentations, can control the
generated image. In particular, we present the first results on
semantically-guided synthesis of megapixel images with transformers and obtain
the state of the art among autoregressive models on class-conditional ImageNet.
Code and pretrained models can be found at
https://github.com/CompVis/taming-transformers .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Esser_P/0/1/0/all/0/1"&gt;Patrick Esser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rombach_R/0/1/0/all/0/1"&gt;Robin Rombach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn Ommer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the Transferability of Adversarial Examples with New Iteration Framework and Input Dropout. (arXiv:2106.01617v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01617</id>
        <link href="http://arxiv.org/abs/2106.01617"/>
        <updated>2021-06-24T01:51:43.367Z</updated>
        <summary type="html"><![CDATA[Deep neural networks(DNNs) is vulnerable to be attacked by adversarial
examples. Black-box attack is the most threatening attack. At present,
black-box attack methods mainly adopt gradient-based iterative attack methods,
which usually limit the relationship between the iteration step size, the
number of iterations, and the maximum perturbation. In this paper, we propose a
new gradient iteration framework, which redefines the relationship between the
above three. Under this framework, we easily improve the attack success rate of
DI-TI-MIM. In addition, we propose a gradient iterative attack method based on
input dropout, which can be well combined with our framework. We further
propose a multi dropout rate version of this method. Experimental results show
that our best method can achieve attack success rate of 96.2\% for defense
model on average, which is higher than the state-of-the-art gradient-based
attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1"&gt;Pengfei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Linyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1"&gt;Ruoxi Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_K/0/1/0/all/0/1"&gt;Kai Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1"&gt;Shuhao Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1"&gt;Guoen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1"&gt;Bin Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large-Scale Spatio-Temporal Person Re-identification: Algorithm and Benchmark. (arXiv:2105.15076v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15076</id>
        <link href="http://arxiv.org/abs/2105.15076"/>
        <updated>2021-06-24T01:51:43.361Z</updated>
        <summary type="html"><![CDATA[Person re-identification (re-ID) in the scenario with large spatial and
temporal spans has not been fully explored. This is partially because that,
existing benchmark datasets were mainly collected with limited spatial and
temporal ranges, e.g., using videos recorded in a few days by cameras in a
specific region of the campus. Such limited spatial and temporal ranges make it
hard to simulate the difficulties of person re-ID in real scenarios. In this
work, we contribute a novel Large-scale Spatio-Temporal LaST person re-ID
dataset, including 10,862 identities with more than 228k images. Compared with
existing datasets, LaST presents more challenging and high-diversity re-ID
settings, and significantly larger spatial and temporal ranges. For instance,
each person can appear in different cities or countries, and in various time
slots from daytime to night, and in different seasons from spring to winter. To
our best knowledge, LaST is a novel person re-ID dataset with the largest
spatio-temporal ranges. Based on LaST, we verified its challenge by conducting
a comprehensive performance evaluation of 14 re-ID algorithms. We further
propose an easy-to-implement baseline that works well on such challenging re-ID
setting. We also verified that models pre-trained on LaST can generalize well
on existing datasets with short-term and cloth-changing scenarios. We expect
LaST to inspire future works toward more realistic and challenging re-ID tasks.
More information about the dataset is available at
https://github.com/shuxjweb/last.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1"&gt;Xiujun Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shiliang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xianghao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuanqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Ge Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Test-Time Adaptation for Out-of-distributed Image Inpainting. (arXiv:2102.01360v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01360</id>
        <link href="http://arxiv.org/abs/2102.01360"/>
        <updated>2021-06-24T01:51:43.356Z</updated>
        <summary type="html"><![CDATA[Deep learning-based image inpainting algorithms have shown great performance
via powerful learned prior from the numerous external natural images. However,
they show unpleasant results on the test image whose distribution is far from
the that of training images because their models are biased toward the training
images. In this paper, we propose a simple image inpainting algorithm with
test-time adaptation named AdaFill. Given a single out-of-distributed test
image, our goal is to complete hole region more naturally than the pre-trained
inpainting models. To achieve this goal, we treat remained valid regions of the
test image as another training cues because natural images have strong internal
similarities. From this test-time adaptation, our network can exploit
externally learned image priors from the pre-trained features as well as the
internal prior of the test image explicitly. Experimental results show that
AdaFill outperforms other models on the various out-of-distribution test
images. Furthermore, the model named ZeroFill, that are not pre-trained also
sometimes outperforms the pre-trained models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shin_C/0/1/0/all/0/1"&gt;Chajin Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1"&gt;Taeoh Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sangjin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sangyoun Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visualizing Missing Surfaces In Colonoscopy Videos using Shared Latent Space Representations. (arXiv:2101.07280v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.07280</id>
        <link href="http://arxiv.org/abs/2101.07280"/>
        <updated>2021-06-24T01:51:43.349Z</updated>
        <summary type="html"><![CDATA[Optical colonoscopy (OC), the most prevalent colon cancer screening tool, has
a high miss rate due to a number of factors, including the geometry of the
colon (haustral fold and sharp bends occlusions), endoscopist inexperience or
fatigue, endoscope field of view, etc. We present a framework to visualize the
missed regions per-frame during the colonoscopy, and provides a workable
clinical solution. Specifically, we make use of 3D reconstructed virtual
colonoscopy (VC) data and the insight that VC and OC share the same underlying
geometry but differ in color, texture and specular reflections, embedded in the
OC domain. A lossy unpaired image-to-image translation model is introduced with
enforced shared latent space for OC and VC. This shared latent space captures
the geometric information while deferring the color, texture, and specular
information creation to additional Gaussian noise input. This additional noise
input can be utilized to generate one-to-many mappings from VC to OC and OC to
OC. The code, data and trained models will be released via our Computational
Endoscopy Platform at https://github.com/nadeemlab/CEP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mathew_S/0/1/0/all/0/1"&gt;Shawn Mathew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nadeem_S/0/1/0/all/0/1"&gt;Saad Nadeem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kaufman_A/0/1/0/all/0/1"&gt;Arie Kaufman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stronger NAS with Weaker Predictors. (arXiv:2102.10490v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10490</id>
        <link href="http://arxiv.org/abs/2102.10490"/>
        <updated>2021-06-24T01:51:43.337Z</updated>
        <summary type="html"><![CDATA[Neural Architecture Search (NAS) often trains and evaluates a large number of
architectures. Recent predictor-based NAS approaches attempt to address such
heavy computation costs with two key steps: sampling some
architecture-performance pairs and fitting a proxy accuracy predictor. Given
limited samples, these predictors, however, are far from accurate to locate top
architectures due to the difficulty of fitting the huge search space. This
paper reflects on a simple yet crucial question: if our final goal is to find
the best architecture, do we really need to model the whole space well?. We
propose a paradigm shift from fitting the whole architecture space using one
strong predictor, to progressively fitting a search path towards the
high-performance sub-space through a set of weaker predictors. As a key
property of the proposed weak predictors, their probabilities of sampling
better architectures keep increasing. Hence we only sample a few well-performed
architectures guided by the previously learned predictor and estimate a new
better weak predictor. This embarrassingly easy framework produces
coarse-to-fine iteration to refine the ranking of sampling space gradually.
Extensive experiments demonstrate that our method costs fewer samples to find
top-performance architectures on NAS-Bench-101 and NAS-Bench-201, as well as
achieves the state-of-the-art ImageNet performance on the NASNet search space.
In particular, compared to state-of-the-art (SOTA) predictor-based NAS methods,
WeakNAS outperforms all of them with notable margins, e.g., requiring at least
7.5x less samples to find global optimal on NAS-Bench-101; and WeakNAS can also
absorb them for further performance boost. We further strike the new SOTA
result of 81.3% in the ImageNet MobileNet Search Space. The code is available
at https://github.com/VITA-Group/WeakNAS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Junru Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1"&gt;Xiyang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dongdong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yinpeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mengchen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Ye Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zicheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Lu Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VariTex: Variational Neural Face Textures. (arXiv:2104.05988v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05988</id>
        <link href="http://arxiv.org/abs/2104.05988"/>
        <updated>2021-06-24T01:51:43.321Z</updated>
        <summary type="html"><![CDATA[Deep generative models have recently demonstrated the ability to synthesize
photorealistic images of human faces with novel identities. A key challenge to
the wide applicability of such techniques is to provide independent control
over semantically meaningful parameters: appearance, head pose, face shape, and
facial expressions. In this paper, we propose VariTex - to the best of our
knowledge the first method that learns a variational latent feature space of
neural face textures, which allows sampling of novel identities. We combine
this generative model with a parametric face model and gain explicit control
over head pose and facial expressions. To generate images of complete human
heads, we propose an additive decoder that generates plausible additional
details such as hair. A novel training scheme enforces a pose independent
latent space and in consequence, allows learning of a one-to-many mapping
between latent codes and pose-conditioned exterior regions. The resulting
method can generate geometrically consistent images of novel identities
allowing fine-grained control over head pose, face shape, and facial
expressions, facilitating a broad range of downstream tasks, like sampling
novel identities, re-posing, expression transfer, and more.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Buhler_M/0/1/0/all/0/1"&gt;Marcel C. B&amp;#xfc;hler&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Meka_A/0/1/0/all/0/1"&gt;Abhimitra Meka&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Gengyan Li&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Beeler_T/0/1/0/all/0/1"&gt;Thabo Beeler&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1"&gt;Otmar Hilliges&lt;/a&gt; (1) ((1) ETH Zurich, (2) Google)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Horizontal-to-Vertical Video Conversion. (arXiv:2101.04051v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.04051</id>
        <link href="http://arxiv.org/abs/2101.04051"/>
        <updated>2021-06-24T01:51:43.316Z</updated>
        <summary type="html"><![CDATA[Alongside the prevalence of mobile videos, the general public leans towards
consuming vertical videos on hand-held devices. To revitalize the exposure of
horizontal contents, we hereby set forth the exploration of automated
horizontal-to-vertical (abbreviated as H2V) video conversion with our proposed
H2V framework, accompanied by an accurately annotated H2V-142K dataset.
Concretely, H2V framework integrates video shot boundary detection, subject
selection and multi-object tracking to facilitate the subject-preserving
conversion, wherein the key is subject selection. To achieve so, we propose a
Rank-SS module that detects human objects, then selects the subject-to-preserve
via exploiting location, appearance, and salient cues. Afterward, the framework
automatically crops the video around the subject to produce vertical contents
from horizontal sources. To build and evaluate our H2V framework, H2V-142K
dataset is densely annotated with subject bounding boxes for 125 videos with
132K frames and 9,500 video covers, upon which we demonstrate superior subject
selection performance comparing to traditional salient approaches, and exhibit
promising horizontal-to-vertical conversion performance overall. By publicizing
this dataset as well as our approach, we wish to pave the way for more valuable
endeavors on the horizontal-to-vertical video conversion task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1"&gt;Tun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Daoxin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianran Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xiaolong Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jianke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiawei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modality Attention and Sampling Enables Deep Learning with Heterogeneous Marker Combinations in Fluorescence Microscopy. (arXiv:2008.12380v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.12380</id>
        <link href="http://arxiv.org/abs/2008.12380"/>
        <updated>2021-06-24T01:51:43.310Z</updated>
        <summary type="html"><![CDATA[Fluorescence microscopy allows for a detailed inspection of cells, cellular
networks, and anatomical landmarks by staining with a variety of
carefully-selected markers visualized as color channels. Quantitative
characterization of structures in acquired images often relies on automatic
image analysis methods. Despite the success of deep learning methods in other
vision applications, their potential for fluorescence image analysis remains
underexploited. One reason lies in the considerable workload required to train
accurate models, which are normally specific for a given combination of
markers, and therefore applicable to a very restricted number of experimental
settings. We herein propose Marker Sampling and Excite, a neural network
approach with a modality sampling strategy and a novel attention module that
together enable (i) flexible training with heterogeneous datasets with
combinations of markers and (ii) successful utility of learned models on
arbitrary subsets of markers prospectively. We show that our single neural
network solution performs comparably to an upper bound scenario where an
ensemble of many networks is na\"ively trained for each possible marker
combination separately. In addition, we demonstrate the feasibility of this
framework in high-throughput biological analysis by revising a recent
quantitative characterization of bone marrow vasculature in 3D confocal
microscopy datasets and further confirm the validity of our approach on an
additional, significantly different dataset of microvessels in fetal liver
tissues. Not only can our work substantially ameliorate the use of deep
learning in fluorescence microscopy analysis, but it can also be utilized in
other fields with incomplete data acquisitions and missing modalities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gomariz_A/0/1/0/all/0/1"&gt;Alvaro Gomariz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Portenier_T/0/1/0/all/0/1"&gt;Tiziano Portenier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Helbling_P/0/1/0/all/0/1"&gt;Patrick M. Helbling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Isringhausen_S/0/1/0/all/0/1"&gt;Stephan Isringhausen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suessbier_U/0/1/0/all/0/1"&gt;Ute Suessbier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nombela_Arrieta_C/0/1/0/all/0/1"&gt;C&amp;#xe9;sar Nombela-Arrieta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goksel_O/0/1/0/all/0/1"&gt;Orcun Goksel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Alignment for Approximated Reversibility in Neural Networks. (arXiv:2106.12562v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12562</id>
        <link href="http://arxiv.org/abs/2106.12562"/>
        <updated>2021-06-24T01:51:43.304Z</updated>
        <summary type="html"><![CDATA[We introduce feature alignment, a technique for obtaining approximate
reversibility in artificial neural networks. By means of feature extraction, we
can train a neural network to learn an estimated map for its reverse process
from outputs to inputs. Combined with variational autoencoders, we can generate
new samples from the same statistics as the training data. Improvements of the
results are obtained by using concepts from generative adversarial networks.
Finally, we show that the technique can be modified for training neural
networks locally, saving computational memory resources. Applying these
techniques, we report results for three vision generative tasks: MNIST,
CIFAR-10, and celebA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farias_T/0/1/0/all/0/1"&gt;Tiago de Souza Farias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maziero_J/0/1/0/all/0/1"&gt;Jonas Maziero&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Classification of Intrusive Igneous Rock Thin Section Images using Edge Detection and Colour Analysis. (arXiv:1710.00189v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1710.00189</id>
        <link href="http://arxiv.org/abs/1710.00189"/>
        <updated>2021-06-24T01:51:43.297Z</updated>
        <summary type="html"><![CDATA[Classification of rocks is one of the fundamental tasks in a geological
study. The process requires a human expert to examine sampled thin section
images under a microscope. In this study, we propose a method that uses
microscope automation, digital image acquisition, edge detection and colour
analysis (histogram). We collected 60 digital images from 20 standard thin
sections using a digital camera mounted on a conventional microscope. Each
image is partitioned into a finite number of cells that form a grid structure.
Edge and colour profile of pixels inside each cell determine its
classification. The individual cells then determine the thin section image
classification via a majority voting scheme. Our method yielded successful
results as high as 90% to 100% precision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Joseph_S/0/1/0/all/0/1"&gt;S. Joseph&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ujir_H/0/1/0/all/0/1"&gt;H. Ujir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hipiny_I/0/1/0/all/0/1"&gt;I. Hipiny&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unbiased Mean Teacher for Cross-domain Object Detection. (arXiv:2003.00707v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.00707</id>
        <link href="http://arxiv.org/abs/2003.00707"/>
        <updated>2021-06-24T01:51:43.262Z</updated>
        <summary type="html"><![CDATA[Cross-domain object detection is challenging, because object detection model
is often vulnerable to data variance, especially to the considerable domain
shift between two distinctive domains. In this paper, we propose a new Unbiased
Mean Teacher (UMT) model for cross-domain object detection. We reveal that
there often exists a considerable model bias for the simple mean teacher (MT)
model in cross-domain scenarios, and eliminate the model bias with several
simple yet highly effective strategies. In particular, for the teacher model,
we propose a cross-domain distillation method for MT to maximally exploit the
expertise of the teacher model. Moreover, for the student model, we alleviate
its bias by augmenting training samples with pixel-level adaptation. Finally,
for the teaching process, we employ an out-of-distribution estimation strategy
to select samples that most fit the current model to further enhance the
cross-domain distillation process. By tackling the model bias issue with these
strategies, our UMT model achieves mAPs of 44.1%, 58.1%, 41.7%, and 43.1% on
benchmark datasets Clipart1k, Watercolor2k, Foggy Cityscapes, and Cityscapes,
respectively, which outperforms the existing state-of-the-art results in
notable margins. Our implementation is available at
https://github.com/kinredon/umt.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Jinhong Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuhua Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1"&gt;Lixin Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A System for Automatic Rice Disease Detection from Rice Paddy Images Serviced via a Chatbot. (arXiv:2011.10823v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10823</id>
        <link href="http://arxiv.org/abs/2011.10823"/>
        <updated>2021-06-24T01:51:43.256Z</updated>
        <summary type="html"><![CDATA[A LINE Bot System to diagnose rice diseases from actual paddy field images
was developed and presented in this paper. It was easy-to-use and automatic
system designed to help rice farmers improve the rice yield and quality. The
targeted images were taken from the actual paddy environment without special
sample preparation. We used a deep learning neural networks technique to detect
rice diseases from the images. We developed an object detection model training
and refinement process to improve the performance of our previous research on
rice leave diseases detection. The process was based on analyzing the model's
predictive results and could be repeatedly used to improve the quality of the
database in the next training of the model. The deployment model for our LINE
Bot system was created from the selected best performance technique in our
previous paper, YOLOv3, trained by refined training data set. The performance
of the deployment model was measured on 5 target classes and found that the
Average True Positive Point improved from 91.1% in the previous paper to 95.6%
in this study. Therefore, we used this deployment model for Rice Disease LINE
Bot system. Our system worked automatically real-time to suggest primary
diagnosis results to the users in the LINE group, which included rice farmers
and rice disease specialists. They could communicate freely via chat. In the
real LINE Bot deployment, the model's performance was measured by our own
defined measurement Average True Positive Point and was found to be an average
of 78.86%. The system was fast and took only 2-3 s for detection process in our
system server.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Temniranrat_P/0/1/0/all/0/1"&gt;Pitchayagan Temniranrat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kiratiratanapruk_K/0/1/0/all/0/1"&gt;Kantip Kiratiratanapruk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kitvimonrat_A/0/1/0/all/0/1"&gt;Apichon Kitvimonrat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sinthupinyo_W/0/1/0/all/0/1"&gt;Wasin Sinthupinyo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Patarapuwadol_S/0/1/0/all/0/1"&gt;Sujin Patarapuwadol&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Blur, Noise, and Compression Robust Generative Adversarial Networks. (arXiv:2003.07849v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.07849</id>
        <link href="http://arxiv.org/abs/2003.07849"/>
        <updated>2021-06-24T01:51:43.250Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) have gained considerable attention
owing to their ability to reproduce images. However, they can recreate training
images faithfully despite image degradation in the form of blur, noise, and
compression, generating similarly degraded images. To solve this problem, the
recently proposed noise robust GAN (NR-GAN) provides a partial solution by
demonstrating the ability to learn a clean image generator directly from noisy
images using a two-generator model comprising image and noise generators.
However, its application is limited to noise, which is relatively easy to
decompose owing to its additive and reversible characteristics, and its
application to irreversible image degradation, in the form of blur,
compression, and combination of all, remains a challenge. To address these
problems, we propose blur, noise, and compression robust GAN (BNCR-GAN) that
can learn a clean image generator directly from degraded images without
knowledge of degradation parameters (e.g., blur kernel types, noise amounts, or
quality factor values). Inspired by NR-GAN, BNCR-GAN uses a multiple-generator
model composed of image, blur-kernel, noise, and quality-factor generators.
However, in contrast to NR-GAN, to address irreversible characteristics, we
introduce masking architectures adjusting degradation strength values in a
data-driven manner using bypasses before and after degradation. Furthermore, to
suppress uncertainty caused by the combination of blur, noise, and compression,
we introduce adaptive consistency losses imposing consistency between
irreversible degradation processes according to the degradation strengths. We
demonstrate the effectiveness of BNCR-GAN through large-scale comparative
studies on CIFAR-10 and a generality analysis on FFHQ. In addition, we
demonstrate the applicability of BNCR-GAN in image restoration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaneko_T/0/1/0/all/0/1"&gt;Takuhiro Kaneko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1"&gt;Tatsuya Harada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Open Images V5 Text Annotation and Yet Another Mask Text Spotter. (arXiv:2106.12326v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12326</id>
        <link href="http://arxiv.org/abs/2106.12326"/>
        <updated>2021-06-24T01:51:43.244Z</updated>
        <summary type="html"><![CDATA[A large scale human-labeled dataset plays an important role in creating high
quality deep learning models. In this paper we present text annotation for Open
Images V5 dataset. To our knowledge it is the largest among publicly available
manually created text annotations. Having this annotation we trained a simple
Mask-RCNN-based network, referred as Yet Another Mask Text Spotter (YAMTS),
which achieves competitive performance or even outperforms current
state-of-the-art approaches in some cases on ICDAR2013, ICDAR2015 and
Total-Text datasets. Code for text spotting model available online at:
https://github.com/openvinotoolkit/training_extensions. The model can be
exported to OpenVINO-format and run on Intel CPUs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krylov_I/0/1/0/all/0/1"&gt;Ilya Krylov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nosov_S/0/1/0/all/0/1"&gt;Sergei Nosov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sovrasov_V/0/1/0/all/0/1"&gt;Vladislav Sovrasov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Euro-PVI: Pedestrian Vehicle Interactions in Dense Urban Centers. (arXiv:2106.12442v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12442</id>
        <link href="http://arxiv.org/abs/2106.12442"/>
        <updated>2021-06-24T01:51:43.229Z</updated>
        <summary type="html"><![CDATA[Accurate prediction of pedestrian and bicyclist paths is integral to the
development of reliable autonomous vehicles in dense urban environments. The
interactions between vehicle and pedestrian or bicyclist have a significant
impact on the trajectories of traffic participants e.g. stopping or turning to
avoid collisions. Although recent datasets and trajectory prediction approaches
have fostered the development of autonomous vehicles yet the amount of
vehicle-pedestrian (bicyclist) interactions modeled are sparse. In this work,
we propose Euro-PVI, a dataset of pedestrian and bicyclist trajectories. In
particular, our dataset caters more diverse and complex interactions in dense
urban scenarios compared to the existing datasets. To address the challenges in
predicting future trajectories with dense interactions, we develop a joint
inference model that learns an expressive multi-modal shared latent space
across agents in the urban scene. This enables our Joint-$\beta$-cVAE approach
to better model the distribution of future trajectories. We achieve state of
the art results on the nuScenes and Euro-PVI datasets demonstrating the
importance of capturing interactions between ego-vehicle and pedestrians
(bicyclists) for accurate predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1"&gt;Apratim Bhattacharyya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reino_D/0/1/0/all/0/1"&gt;Daniel Olmeda Reino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1"&gt;Mario Fritz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1"&gt;Bernt Schiele&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FoldIt: Haustral Folds Detection and Segmentation in Colonoscopy Videos. (arXiv:2106.12522v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12522</id>
        <link href="http://arxiv.org/abs/2106.12522"/>
        <updated>2021-06-24T01:51:43.220Z</updated>
        <summary type="html"><![CDATA[Haustral folds are colon wall protrusions implicated for high polyp miss rate
during optical colonoscopy procedures. If segmented accurately, haustral folds
can allow for better estimation of missed surface and can also serve as
valuable landmarks for registering pre-treatment virtual (CT) and optical
colonoscopies, to guide navigation towards the anomalies found in pre-treatment
scans. We present a novel generative adversarial network, FoldIt, for
feature-consistent image translation of optical colonoscopy videos to virtual
colonoscopy renderings with haustral fold overlays. A new transitive loss is
introduced in order to leverage ground truth information between haustral fold
annotations and virtual colonoscopy renderings. We demonstrate the
effectiveness of our model on real challenging optical colonoscopy videos as
well as on textured virtual colonoscopy videos with clinician-verified haustral
fold annotations. All code and scripts to reproduce the experiments of this
paper will be made available via our Computational Endoscopy Platform at
https://github.com/nadeemlab/CEP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mathew_S/0/1/0/all/0/1"&gt;Shawn Mathew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nadeem_S/0/1/0/all/0/1"&gt;Saad Nadeem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kaufman_A/0/1/0/all/0/1"&gt;Arie Kaufman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from Pseudo Lesion: A Self-supervised Framework for COVID-19 Diagnosis. (arXiv:2106.12313v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12313</id>
        <link href="http://arxiv.org/abs/2106.12313"/>
        <updated>2021-06-24T01:51:43.214Z</updated>
        <summary type="html"><![CDATA[The Coronavirus disease 2019 (COVID-19) has rapidly spread all over the world
since its first report in December 2019 and thoracic computed tomography (CT)
has become one of the main tools for its diagnosis. In recent years, deep
learning-based approaches have shown impressive performance in myriad image
recognition tasks. However, they usually require a large number of annotated
data for training. Inspired by Ground Glass Opacity (GGO), a common finding in
COIVD-19 patient's CT scans, we proposed in this paper a novel self-supervised
pretraining method based on pseudo lesions generation and restoration for
COVID-19 diagnosis. We used Perlin noise, a gradient noise based mathematical
model, to generate lesion-like patterns, which were then randomly pasted to the
lung regions of normal CT images to generate pseudo COVID-19 images. The pairs
of normal and pseudo COVID-19 images were then used to train an encoder-decoder
architecture based U-Net for image restoration, which does not require any
labelled data. The pretrained encoder was then fine-tuned using labelled data
for COVID-19 diagnosis task. Two public COVID-19 diagnosis datasets made up of
CT images were employed for evaluation. Comprehensive experimental results
demonstrated that the proposed self-supervised learning approach could extract
better feature representation for COVID-19 diagnosis and the accuracy of the
proposed method outperformed the supervised model pretrained on large scale
images by 6.57% and 3.03% on SARS-CoV-2 dataset and Jinan COVID-19 dataset,
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhongliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jin_Z/0/1/0/all/0/1"&gt;Zhihao Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuechen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_L/0/1/0/all/0/1"&gt;Linlin Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emergent Properties of Foveated Perceptual Systems. (arXiv:2006.07991v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07991</id>
        <link href="http://arxiv.org/abs/2006.07991"/>
        <updated>2021-06-24T01:51:43.207Z</updated>
        <summary type="html"><![CDATA[The goal of this work is to characterize the representational impact that
foveation operations have for machine vision systems, inspired by the foveated
human visual system, which has higher acuity at the center of gaze and
texture-like encoding in the periphery. To do so, we introduce models
consisting of a first-stage \textit{fixed} image transform followed by a
second-stage \textit{learnable} convolutional neural network, and we varied the
first stage component. The primary model has a foveated-textural input stage,
which we compare to a model with foveated-blurred input and a model with
spatially-uniform blurred input (both matched for perceptual compression), and
a final reference model with minimal input-based compression. We find that: 1)
the foveated-texture model shows similar scene classification accuracy as the
reference model despite its compressed input, with greater i.i.d.
generalization than the other models; 2) the foveated-texture model has greater
sensitivity to high-spatial frequency information and greater robustness to
occlusion, w.r.t the comparison models; 3) both the foveated systems, show a
stronger center image-bias relative to the spatially-uniform systems even with
a weight sharing constraint. Critically, these results are preserved over
different classical CNN architectures throughout their learning dynamics.
Altogether, this suggests that foveation with peripheral texture-based
computations yields an efficient, distinct, and robust representational format
of scene information, and provides symbiotic computational insight into the
representational consequences that texture-based peripheral encoding may have
for processing in the human visual system, while also potentially inspiring the
next generation of computer vision models via spatially-adaptive computation.
Code + Data available here: https://github.com/ArturoDeza/EmergentProperties]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deza_A/0/1/0/all/0/1"&gt;Arturo Deza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konkle_T/0/1/0/all/0/1"&gt;Talia Konkle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diabetic Retinopathy Detection using Ensemble Machine Learning. (arXiv:2106.12545v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12545</id>
        <link href="http://arxiv.org/abs/2106.12545"/>
        <updated>2021-06-24T01:51:43.201Z</updated>
        <summary type="html"><![CDATA[Diabetic Retinopathy (DR) is among the worlds leading vision loss causes in
diabetic patients. DR is a microvascular disease that affects the eye retina,
which causes vessel blockage and therefore cuts the main source of nutrition
for the retina tissues. Treatment for this visual disorder is most effective
when it is detected in its earliest stages, as severe DR can result in
irreversible blindness. Nonetheless, DR identification requires the expertise
of Ophthalmologists which is often expensive and time-consuming. Therefore,
automatic detection systems were introduced aiming to facilitate the
identification process, making it available globally in a time and
cost-efficient manner. However, due to the limited reliable datasets and
medical records for this particular eye disease, the obtained predictions
accuracies were relatively unsatisfying for eye specialists to rely on them as
diagnostic systems. Thus, we explored an ensemble-based learning strategy,
merging a substantial selection of well-known classification algorithms in one
sophisticated diagnostic model. The proposed framework achieved the highest
accuracy rates among all other common classification algorithms in the area. 4
subdatasets were generated to contain the top 5 and top 10 features of the
Messidor dataset, selected by InfoGainEval. and WrapperSubsetEval., accuracies
of 70.7% and 75.1% were achieved on the InfoGainEval. top 5 and original
dataset respectively. The results imply the impressive performance of the
subdataset, which significantly conduces to a less complex classification
process]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Odeh_I/0/1/0/all/0/1"&gt;Israa Odeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alkasassbeh_M/0/1/0/all/0/1"&gt;Mouhammd Alkasassbeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alauthman_M/0/1/0/all/0/1"&gt;Mohammad Alauthman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Co-advise: Cross Inductive Bias Distillation. (arXiv:2106.12378v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12378</id>
        <link href="http://arxiv.org/abs/2106.12378"/>
        <updated>2021-06-24T01:51:43.196Z</updated>
        <summary type="html"><![CDATA[Transformers recently are adapted from the community of natural language
processing as a promising substitute of convolution-based neural networks for
visual learning tasks. However, its supremacy degenerates given an insufficient
amount of training data (e.g., ImageNet). To make it into practical utility, we
propose a novel distillation-based method to train vision transformers. Unlike
previous works, where merely heavy convolution-based teachers are provided, we
introduce lightweight teachers with different architectural inductive biases
(e.g., convolution and involution) to co-advise the student transformer. The
key is that teachers with different inductive biases attain different knowledge
despite that they are trained on the same dataset, and such different knowledge
compounds and boosts the student's performance during distillation. Equipped
with this cross inductive bias distillation method, our vision transformers
(termed as CivT) outperform all previous transformers of the same architecture
on ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Sucheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhengqi Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_T/0/1/0/all/0/1"&gt;Tianyu Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1"&gt;Zihui Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonglong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shengfeng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hang Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FusionPainting: Multimodal Fusion with Adaptive Attention for 3D Object Detection. (arXiv:2106.12449v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12449</id>
        <link href="http://arxiv.org/abs/2106.12449"/>
        <updated>2021-06-24T01:51:43.177Z</updated>
        <summary type="html"><![CDATA[Accurate detection of obstacles in 3D is an essential task for autonomous
driving and intelligent transportation. In this work, we propose a general
multimodal fusion framework FusionPainting to fuse the 2D RGB image and 3D
point clouds at a semantic level for boosting the 3D object detection task.
Especially, the FusionPainting framework consists of three main modules: a
multi-modal semantic segmentation module, an adaptive attention-based semantic
fusion module, and a 3D object detector. First, semantic information is
obtained for 2D images and 3D Lidar point clouds based on 2D and 3D
segmentation approaches. Then the segmentation results from different sensors
are adaptively fused based on the proposed attention-based semantic fusion
module. Finally, the point clouds painted with the fused semantic label are
sent to the 3D detector for obtaining the 3D objection results. The
effectiveness of the proposed framework has been verified on the large-scale
nuScenes detection benchmark by comparing it with three different baselines.
The experimental results show that the fusion strategy can significantly
improve the detection performance compared to the methods using only point
clouds, and the methods using point clouds only painted with 2D segmentation
information. Furthermore, the proposed approach outperforms other
state-of-the-art methods on the nuScenes testing benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shaoqing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Dingfu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1"&gt;Jin Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1"&gt;Junbo Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bin_Z/0/1/0/all/0/1"&gt;Zhou Bin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liangjun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alias-Free Generative Adversarial Networks. (arXiv:2106.12423v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12423</id>
        <link href="http://arxiv.org/abs/2106.12423"/>
        <updated>2021-06-24T01:51:43.171Z</updated>
        <summary type="html"><![CDATA[We observe that despite their hierarchical convolutional nature, the
synthesis process of typical generative adversarial networks depends on
absolute pixel coordinates in an unhealthy manner. This manifests itself as,
e.g., detail appearing to be glued to image coordinates instead of the surfaces
of depicted objects. We trace the root cause to careless signal processing that
causes aliasing in the generator network. Interpreting all signals in the
network as continuous, we derive generally applicable, small architectural
changes that guarantee that unwanted information cannot leak into the
hierarchical synthesis process. The resulting networks match the FID of
StyleGAN2 but differ dramatically in their internal representations, and they
are fully equivariant to translation and rotation even at subpixel scales. Our
results pave the way for generative models better suited for video and
animation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karras_T/0/1/0/all/0/1"&gt;Tero Karras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aittala_M/0/1/0/all/0/1"&gt;Miika Aittala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laine_S/0/1/0/all/0/1"&gt;Samuli Laine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harkonen_E/0/1/0/all/0/1"&gt;Erik H&amp;#xe4;rk&amp;#xf6;nen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hellsten_J/0/1/0/all/0/1"&gt;Janne Hellsten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehtinen_J/0/1/0/all/0/1"&gt;Jaakko Lehtinen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aila_T/0/1/0/all/0/1"&gt;Timo Aila&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep unsupervised 3D human body reconstruction from a sparse set of landmarks. (arXiv:2106.12282v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12282</id>
        <link href="http://arxiv.org/abs/2106.12282"/>
        <updated>2021-06-24T01:51:43.163Z</updated>
        <summary type="html"><![CDATA[In this paper we propose the first deep unsupervised approach in human body
reconstruction to estimate body surface from a sparse set of landmarks, so
called DeepMurf. We apply a denoising autoencoder to estimate missing
landmarks. Then we apply an attention model to estimate body joints from
landmarks. Finally, a cascading network is applied to regress parameters of a
statistical generative model that reconstructs body. Our set of proposed loss
functions allows us to train the network in an unsupervised way. Results on
four public datasets show that our approach accurately reconstructs the human
body from real world mocap data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Madadi_M/0/1/0/all/0/1"&gt;Meysam Madadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bertiche_H/0/1/0/all/0/1"&gt;Hugo Bertiche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1"&gt;Sergio Escalera&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation. (arXiv:2012.07177v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07177</id>
        <link href="http://arxiv.org/abs/2012.07177"/>
        <updated>2021-06-24T01:51:43.157Z</updated>
        <summary type="html"><![CDATA[Building instance segmentation models that are data-efficient and can handle
rare object categories is an important challenge in computer vision. Leveraging
data augmentations is a promising direction towards addressing this challenge.
Here, we perform a systematic study of the Copy-Paste augmentation ([13, 12])
for instance segmentation where we randomly paste objects onto an image. Prior
studies on Copy-Paste relied on modeling the surrounding visual context for
pasting the objects. However, we find that the simple mechanism of pasting
objects randomly is good enough and can provide solid gains on top of strong
baselines. Furthermore, we show Copy-Paste is additive with semi-supervised
methods that leverage extra data through pseudo labeling (e.g. self-training).
On COCO instance segmentation, we achieve 49.1 mask AP and 57.3 box AP, an
improvement of +0.6 mask AP and +1.5 box AP over the previous state-of-the-art.
We further demonstrate that Copy-Paste can lead to significant improvements on
the LVIS benchmark. Our baseline model outperforms the LVIS 2020 Challenge
winning entry by +3.6 mask AP on rare categories.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghiasi_G/0/1/0/all/0/1"&gt;Golnaz Ghiasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1"&gt;Yin Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivas_A/0/1/0/all/0/1"&gt;Aravind Srinivas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1"&gt;Rui Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tsung-Yi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cubuk_E/0/1/0/all/0/1"&gt;Ekin D. Cubuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1"&gt;Quoc V. Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1"&gt;Barret Zoph&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Segmentation of Action Segments in Egocentric Videos using Gaze. (arXiv:1710.00187v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1710.00187</id>
        <link href="http://arxiv.org/abs/1710.00187"/>
        <updated>2021-06-24T01:51:43.141Z</updated>
        <summary type="html"><![CDATA[Unsupervised segmentation of action segments in egocentric videos is a
desirable feature in tasks such as activity recognition and content-based video
retrieval. Reducing the search space into a finite set of action segments
facilitates a faster and less noisy matching. However, there exist a
substantial gap in machine understanding of natural temporal cuts during a
continuous human activity. This work reports on a novel gaze-based approach for
segmenting action segments in videos captured using an egocentric camera. Gaze
is used to locate the region-of-interest inside a frame. By tracking two simple
motion-based parameters inside successive regions-of-interest, we discover a
finite set of temporal cuts. We present several results using combinations (of
the two parameters) on a dataset, i.e., BRISGAZE-ACTIONS. The dataset contains
egocentric videos depicting several daily-living activities. The quality of the
temporal cuts is further improved by implementing two entropy measures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hipiny_I/0/1/0/all/0/1"&gt;I. Hipiny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ujir_H/0/1/0/all/0/1"&gt;H. Ujir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Minoi_J/0/1/0/all/0/1"&gt;J.L. Minoi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juan_S/0/1/0/all/0/1"&gt;S.F. Samson Juan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khairuddin_M/0/1/0/all/0/1"&gt;M.A. Khairuddin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sunar_M/0/1/0/all/0/1"&gt;M.S. Sunar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient-Based Interpretability Methods and Binarized Neural Networks. (arXiv:2106.12569v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12569</id>
        <link href="http://arxiv.org/abs/2106.12569"/>
        <updated>2021-06-24T01:51:43.126Z</updated>
        <summary type="html"><![CDATA[Binarized Neural Networks (BNNs) have the potential to revolutionize the way
that deep learning is carried out in edge computing platforms. However, the
effectiveness of interpretability methods on these networks has not been
assessed.

In this paper, we compare the performance of several widely used saliency
map-based interpretabilty techniques (Gradient, SmoothGrad and GradCAM), when
applied to Binarized or Full Precision Neural Networks (FPNNs). We found that
the basic Gradient method produces very similar-looking maps for both types of
network. However, SmoothGrad produces significantly noisier maps for BNNs.
GradCAM also produces saliency maps which differ between network types, with
some of the BNNs having seemingly nonsensical explanations. We comment on
possible reasons for these differences in explanations and present it as an
example of why interpretability techniques should be tested on a wider range of
network types.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Widdicombe_A/0/1/0/all/0/1"&gt;Amy Widdicombe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Julier_S/0/1/0/all/0/1"&gt;Simon J. Julier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness in Cardiac MR Image Analysis: An Investigation of Bias Due to Data Imbalance in Deep Learning Based Segmentation. (arXiv:2106.12387v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12387</id>
        <link href="http://arxiv.org/abs/2106.12387"/>
        <updated>2021-06-24T01:51:43.120Z</updated>
        <summary type="html"><![CDATA[The subject of "fairness" in artificial intelligence (AI) refers to assessing
AI algorithms for potential bias based on demographic characteristics such as
race and gender, and the development of algorithms to address this bias. Most
applications to date have been in computer vision, although some work in
healthcare has started to emerge. The use of deep learning (DL) in cardiac MR
segmentation has led to impressive results in recent years, and such techniques
are starting to be translated into clinical practice. However, no work has yet
investigated the fairness of such models. In this work, we perform such an
analysis for racial/gender groups, focusing on the problem of training data
imbalance, using a nnU-Net model trained and evaluated on cine short axis
cardiac MR data from the UK Biobank dataset, consisting of 5,903 subjects from
6 different racial groups. We find statistically significant differences in
Dice performance between different racial groups. To reduce the racial bias, we
investigated three strategies: (1) stratified batch sampling, in which batch
sampling is stratified to ensure balance between racial groups; (2) fair
meta-learning for segmentation, in which a DL classifier is trained to classify
race and jointly optimized with the segmentation model; and (3) protected group
models, in which a different segmentation model is trained for each racial
group. We also compared the results to the scenario where we have a perfectly
balanced database. To assess fairness we used the standard deviation (SD) and
skewed error ratio (SER) of the average Dice values. Our results demonstrate
that the racial bias results from the use of imbalanced training data, and that
all proposed bias mitigation strategies improved fairness, with the best SD and
SER resulting from the use of protected group models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Puyol_Anton_E/0/1/0/all/0/1"&gt;Esther Puyol-Anton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruijsink_B/0/1/0/all/0/1"&gt;Bram Ruijsink&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piechnik_S/0/1/0/all/0/1"&gt;Stefan K. Piechnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neubauer_S/0/1/0/all/0/1"&gt;Stefan Neubauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petersen_S/0/1/0/all/0/1"&gt;Steffen E. Petersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Razavi_R/0/1/0/all/0/1"&gt;Reza Razavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_A/0/1/0/all/0/1"&gt;Andrew P. King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A new Video Synopsis Based Approach Using Stereo Camera. (arXiv:2106.12362v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12362</id>
        <link href="http://arxiv.org/abs/2106.12362"/>
        <updated>2021-06-24T01:51:43.114Z</updated>
        <summary type="html"><![CDATA[In today's world, the amount of data produced in every field has increased at
an unexpected level. In the face of increasing data, the importance of data
processing has increased remarkably. Our resource topic is on the processing of
video data, which has an important place in increasing data, and the production
of summary videos. Within the scope of this resource, a new method for anomaly
detection with object-based unsupervised learning has been developed while
creating a video summary. By using this method, the video data is processed as
pixels and the result is produced as a video segment. The process flow can be
briefly summarized as follows. Objects on the video are detected according to
their type, and then they are tracked. Then, the tracking history data of the
objects are processed, and the classifier is trained with the object type.
Thanks to this classifier, anomaly behavior of objects is detected. Video
segments are determined by processing video moments containing anomaly
behaviors. The video summary is created by extracting the detected video
segments from the original video and combining them. The model we developed has
been tested and verified separately for single camera and dual camera systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dilber_T/0/1/0/all/0/1"&gt;Talha Dilber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guzel_M/0/1/0/all/0/1"&gt;Mehmet Serdar Guzel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bostanci_E/0/1/0/all/0/1"&gt;Erkan Bostanci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimating the Robustness of Classification Models by the Structure of the Learned Feature-Space. (arXiv:2106.12303v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12303</id>
        <link href="http://arxiv.org/abs/2106.12303"/>
        <updated>2021-06-24T01:51:43.109Z</updated>
        <summary type="html"><![CDATA[Over the last decade, the development of deep image classification networks
has mostly been driven by the search for the best performance in terms of
classification accuracy on standardized benchmarks like ImageNet. More
recently, this focus has been expanded by the notion of model robustness, i.e.
the generalization abilities of models towards previously unseen changes in the
data distribution. While new benchmarks, like ImageNet-C, have been introduced
to measure robustness properties, we argue that fixed testsets are only able to
capture a small portion of possible data variations and are thus limited and
prone to generate new overfitted solutions. To overcome these drawbacks, we
suggest to estimate the robustness of a model directly from the structure of
its learned feature-space. We introduce robustness indicators which are
obtained via unsupervised clustering of latent representations inside a trained
classifier and show very high correlations to the model performance on
corrupted test data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ho_K/0/1/0/all/0/1"&gt;Kalun Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfreundt_F/0/1/0/all/0/1"&gt;Franz-Josef Pfreundt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1"&gt;Janis Keuper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1"&gt;Margret Keuper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Label Management Mechanism for Retinal Fundus Image Classification of Diabetic Retinopathy. (arXiv:2106.12284v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12284</id>
        <link href="http://arxiv.org/abs/2106.12284"/>
        <updated>2021-06-24T01:51:43.104Z</updated>
        <summary type="html"><![CDATA[Diabetic retinopathy (DR) remains the most prevalent cause of vision
impairment and irreversible blindness in the working-age adults. Due to the
renaissance of deep learning (DL), DL-based DR diagnosis has become a promising
tool for the early screening and severity grading of DR. However, training deep
neural networks (DNNs) requires an enormous amount of carefully labeled data.
Noisy label data may be introduced when labeling plenty of data, degrading the
performance of models. In this work, we propose a novel label management
mechanism (LMM) for the DNN to overcome overfitting on the noisy data. LMM
utilizes maximum posteriori probability (MAP) in the Bayesian statistic and
time-weighted technique to selectively correct the labels of unclean data,
which gradually purify the training data and improve classification
performance. Comprehensive experiments on both synthetic noise data (Messidor
\& our collected DR dataset) and real-world noise data (ANIMAL-10N)
demonstrated that LMM could boost performance of models and is superior to
three state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1"&gt;Mengdi Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1"&gt;Ximeng Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_M/0/1/0/all/0/1"&gt;Mufeng Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhe Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1"&gt;Xiangxi Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chuanqing Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Q/0/1/0/all/0/1"&gt;Qiushi Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yanye Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition. (arXiv:2106.12368v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12368</id>
        <link href="http://arxiv.org/abs/2106.12368"/>
        <updated>2021-06-24T01:51:43.087Z</updated>
        <summary type="html"><![CDATA[In this paper, we present Vision Permutator, a conceptually simple and data
efficient MLP-like architecture for visual recognition. By realizing the
importance of the positional information carried by 2D feature representations,
unlike recent MLP-like models that encode the spatial information along the
flattened spatial dimensions, Vision Permutator separately encodes the feature
representations along the height and width dimensions with linear projections.
This allows Vision Permutator to capture long-range dependencies along one
spatial direction and meanwhile preserve precise positional information along
the other direction. The resulting position-sensitive outputs are then
aggregated in a mutually complementing manner to form expressive
representations of the objects of interest. We show that our Vision Permutators
are formidable competitors to convolutional neural networks (CNNs) and vision
transformers. Without the dependence on spatial convolutions or attention
mechanisms, Vision Permutator achieves 81.5% top-1 accuracy on ImageNet without
extra large-scale training data (e.g., ImageNet-22k) using only 25M learnable
parameters, which is much better than most CNNs and vision transformers under
the same model size constraint. When scaling up to 88M, it attains 83.2% top-1
accuracy. We hope this work could encourage research on rethinking the way of
encoding spatial information and facilitate the development of MLP-like models.
Code is available at https://github.com/Andrew-Qibin/VisionPermutator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1"&gt;Qibin Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zihang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Li Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1"&gt;Ming-Ming Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1"&gt;Shuicheng Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiashi Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sentinel-1 and Sentinel-2 Spatio-Temporal Data Fusion for Clouds Removal. (arXiv:2106.12226v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12226</id>
        <link href="http://arxiv.org/abs/2106.12226"/>
        <updated>2021-06-24T01:51:43.082Z</updated>
        <summary type="html"><![CDATA[The abundance of clouds, located both spatially and temporally, often makes
remote sensing applications with optical images difficult or even impossible.
In this manuscript, a novel method for clouds-corrupted optical image
restoration has been presented and developed, based on a joint data fusion
paradigm, where three deep neural networks have been combined in order to fuse
spatio-temporal features extracted from Sentinel-1 and Sentinel-2 time-series
of data. It is worth highlighting that both the code and the dataset have been
implemented from scratch and made available to interested research for further
analysis and investigation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sebastianelli_A/0/1/0/all/0/1"&gt;Alessandro Sebastianelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nowakowski_A/0/1/0/all/0/1"&gt;Artur Nowakowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Puglisi_E/0/1/0/all/0/1"&gt;Erika Puglisi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosso_M/0/1/0/all/0/1"&gt;Maria Pia Del Rosso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mifdal_J/0/1/0/all/0/1"&gt;Jamila Mifdal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pirri_F/0/1/0/all/0/1"&gt;Fiora Pirri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathieu_P/0/1/0/all/0/1"&gt;Pierre Philippe Mathieu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullo_S/0/1/0/all/0/1"&gt;Silvia Liberata Ullo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Team PyKale (xy9) Submission to the EPIC-Kitchens 2021 Unsupervised Domain Adaptation Challenge for Action Recognition. (arXiv:2106.12023v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12023</id>
        <link href="http://arxiv.org/abs/2106.12023"/>
        <updated>2021-06-24T01:51:43.076Z</updated>
        <summary type="html"><![CDATA[This report describes the technical details of our submission to the
EPIC-Kitchens 2021 Unsupervised Domain Adaptation Challenge for Action
Recognition. The EPIC-Kitchens dataset is more difficult than other video
domain adaptation datasets due to multi-tasks with more modalities. Firstly, to
participate in the challenge, we employ a transformer to capture the spatial
information from each modality. Secondly, we employ a temporal attention module
to model temporal-wise inter-dependency. Thirdly, we employ the adversarial
domain adaptation network to learn the general features between labeled source
and unlabeled target domain. Finally, we incorporate multiple modalities to
improve the performance by a three-stream network with late fusion. Our network
achieves the comparable performance with the state-of-the-art baseline T$A^3$N
and outperforms the baseline on top-1 accuracy for verb class and top-5
accuracies for all three tasks which are verb, noun and action. Under the team
name xy9, our submission achieved 5th place in terms of top-1 accuracy for verb
class and all top-5 accuracies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xianyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koot_R/0/1/0/all/0/1"&gt;Raivo Koot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shuo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_T/0/1/0/all/0/1"&gt;Tao Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Haiping Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Well do Feature Visualizations Support Causal Understanding of CNN Activations?. (arXiv:2106.12447v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12447</id>
        <link href="http://arxiv.org/abs/2106.12447"/>
        <updated>2021-06-24T01:51:43.062Z</updated>
        <summary type="html"><![CDATA[One widely used approach towards understanding the inner workings of deep
convolutional neural networks is to visualize unit responses via activation
maximization. Feature visualizations via activation maximization are thought to
provide humans with precise information about the image features that cause a
unit to be activated. If this is indeed true, these synthetic images should
enable humans to predict the effect of an intervention, such as whether
occluding a certain patch of the image (say, a dog's head) changes a unit's
activation. Here, we test this hypothesis by asking humans to predict which of
two square occlusions causes a larger change to a unit's activation. Both a
large-scale crowdsourced experiment and measurements with experts show that on
average, the extremely activating feature visualizations by Olah et al. (2017)
indeed help humans on this task ($67 \pm 4\%$ accuracy; baseline performance
without any visualizations is $60 \pm 3\%$). However, they do not provide any
significant advantage over other visualizations (such as e.g. dataset samples),
which yield similar performance ($66 \pm 3\%$ to $67 \pm 3\%$ accuracy). Taken
together, we propose an objective psychophysical task to quantify the benefit
of unit-level interpretability methods for humans, and find no evidence that
feature visualizations provide humans with better "causal understanding" than
simple alternative visualizations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1"&gt;Roland S. Zimmermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borowski_J/0/1/0/all/0/1"&gt;Judy Borowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1"&gt;Robert Geirhos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1"&gt;Matthias Bethge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallis_T/0/1/0/all/0/1"&gt;Thomas S. A. Wallis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1"&gt;Wieland Brendel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Self-training for Cross-domain Unsupervised Tagged-to-Cine MRI Synthesis. (arXiv:2106.12499v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12499</id>
        <link href="http://arxiv.org/abs/2106.12499"/>
        <updated>2021-06-24T01:51:43.057Z</updated>
        <summary type="html"><![CDATA[Self-training based unsupervised domain adaptation (UDA) has shown great
potential to address the problem of domain shift, when applying a trained deep
learning model in a source domain to unlabeled target domains. However, while
the self-training UDA has demonstrated its effectiveness on discriminative
tasks, such as classification and segmentation, via the reliable pseudo-label
selection based on the softmax discrete histogram, the self-training UDA for
generative tasks, such as image synthesis, is not fully investigated. In this
work, we propose a novel generative self-training (GST) UDA framework with
continuous value prediction and regression objective for cross-domain image
synthesis. Specifically, we propose to filter the pseudo-label with an
uncertainty mask, and quantify the predictive confidence of generated images
with practical variational Bayes learning. The fast test-time adaptation is
achieved by a round-based alternative optimization scheme. We validated our
framework on the tagged-to-cine magnetic resonance imaging (MRI) synthesis
problem, where datasets in the source and target domains were acquired from
different scanners or centers. Extensive validations were carried out to verify
our framework against popular adversarial training UDA methods. Results show
that our GST, with tagged MRI of test subjects in new target domains, improved
the synthesis quality by a large margin, compared with the adversarial training
UDA methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1"&gt;Fangxu Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stone_M/0/1/0/all/0/1"&gt;Maureen Stone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuo_J/0/1/0/all/0/1"&gt;Jiachen Zhuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Timothy_R/0/1/0/all/0/1"&gt;Reese Timothy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prince_J/0/1/0/all/0/1"&gt;Jerry L. Prince&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1"&gt;Georges El Fakhri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1"&gt;Jonghye Woo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Multi-Task Model for Sarcasm Detection and Sentiment Analysis in Arabic Language. (arXiv:2106.12488v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12488</id>
        <link href="http://arxiv.org/abs/2106.12488"/>
        <updated>2021-06-24T01:51:43.048Z</updated>
        <summary type="html"><![CDATA[The prominence of figurative language devices, such as sarcasm and irony,
poses serious challenges for Arabic Sentiment Analysis (SA). While previous
research works tackle SA and sarcasm detection separately, this paper
introduces an end-to-end deep Multi-Task Learning (MTL) model, allowing
knowledge interaction between the two tasks. Our MTL model's architecture
consists of a Bidirectional Encoder Representation from Transformers (BERT)
model, a multi-task attention interaction module, and two task classifiers. The
overall obtained results show that our proposed model outperforms its
single-task counterparts on both SA and sarcasm detection sub-tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahdaouy_A/0/1/0/all/0/1"&gt;Abdelkader El Mahdaouy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mekki_A/0/1/0/all/0/1"&gt;Abdellah El Mekki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Essefar_K/0/1/0/all/0/1"&gt;Kabil Essefar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mamoun_N/0/1/0/all/0/1"&gt;Nabil El Mamoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berrada_I/0/1/0/all/0/1"&gt;Ismail Berrada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khoumsi_A/0/1/0/all/0/1"&gt;Ahmed Khoumsi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-Tuning StyleGAN2 For Cartoon Face Generation. (arXiv:2106.12445v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12445</id>
        <link href="http://arxiv.org/abs/2106.12445"/>
        <updated>2021-06-24T01:51:43.043Z</updated>
        <summary type="html"><![CDATA[Recent studies have shown remarkable success in the unsupervised image to
image (I2I) translation. However, due to the imbalance in the data, learning
joint distribution for various domains is still very challenging. Although
existing models can generate realistic target images, it's difficult to
maintain the structure of the source image. In addition, training a generative
model on large data in multiple domains requires a lot of time and computer
resources. To address these limitations, we propose a novel image-to-image
translation method that generates images of the target domain by finetuning a
stylegan2 pretrained model. The stylegan2 model is suitable for unsupervised
I2I translation on unbalanced datasets; it is highly stable, produces realistic
images, and even learns properly from limited data when applied with simple
fine-tuning techniques. Thus, in this paper, we propose new methods to preserve
the structure of the source images and generate realistic images in the target
domain. The code and results are available at
https://github.com/happy-jihye/Cartoon-StyleGan2]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Back_J/0/1/0/all/0/1"&gt;Jihye Back&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Class Classification of Blood Cells - End to End Computer Vision based diagnosis case study. (arXiv:2106.12548v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12548</id>
        <link href="http://arxiv.org/abs/2106.12548"/>
        <updated>2021-06-24T01:51:43.037Z</updated>
        <summary type="html"><![CDATA[The diagnosis of blood-based diseases often involves identifying and
characterizing patient blood samples. Automated methods to detect and classify
blood cell subtypes have important medical applications. Automated medical
image processing and analysis offers a powerful tool for medical diagnosis. In
this work we tackle the problem of white blood cell classification based on the
morphological characteristics of their outer contour, color. The work we would
explore a set of preprocessing and segmentation (Color-based segmentation,
Morphological processing, contouring) algorithms along with a set of features
extraction methods (Corner detection algorithms and Histogram of
Gradients(HOG)), dimensionality reduction algorithms (Principal Component
Analysis(PCA)) that are able to recognize and classify through various
Unsupervised(k-nearest neighbors) and Supervised (Support Vector Machine,
Decision Trees, Linear Discriminant Analysis, Quadratic Discriminant Analysis,
Naive Bayes) algorithms different categories of white blood cells to
Eosinophil, Lymphocyte, Monocyte, and Neutrophil. We even take a step forwards
to explore various Deep Convolutional Neural network architecture (Sqeezent,
MobilenetV1,MobilenetV2, InceptionNet etc.) without preprocessing/segmentation
and with preprocessing. We would like to explore many algorithms to identify
the robust algorithm with least time complexity and low resource requirement.
The outcome of this work can be a cue to selection of algorithms as per
requirement for automated blood cell classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bezugam_S/0/1/0/all/0/1"&gt;Sai Sukruth Bezugam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Throughput Precision Phenotyping of Left Ventricular Hypertrophy with Cardiovascular Deep Learning. (arXiv:2106.12511v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12511</id>
        <link href="http://arxiv.org/abs/2106.12511"/>
        <updated>2021-06-24T01:51:43.031Z</updated>
        <summary type="html"><![CDATA[Left ventricular hypertrophy (LVH) results from chronic remodeling caused by
a broad range of systemic and cardiovascular disease including hypertension,
aortic stenosis, hypertrophic cardiomyopathy, and cardiac amyloidosis. Early
detection and characterization of LVH can significantly impact patient care but
is limited by under-recognition of hypertrophy, measurement error and
variability, and difficulty differentiating etiologies of LVH. To overcome this
challenge, we present EchoNet-LVH - a deep learning workflow that automatically
quantifies ventricular hypertrophy with precision equal to human experts and
predicts etiology of LVH. Trained on 28,201 echocardiogram videos, our model
accurately measures intraventricular wall thickness (mean absolute error [MAE]
1.4mm, 95% CI 1.2-1.5mm), left ventricular diameter (MAE 2.4mm, 95% CI
2.2-2.6mm), and posterior wall thickness (MAE 1.2mm, 95% CI 1.1-1.3mm) and
classifies cardiac amyloidosis (area under the curve of 0.83) and hypertrophic
cardiomyopathy (AUC 0.98) from other etiologies of LVH. In external datasets
from independent domestic and international healthcare systems, EchoNet-LVH
accurately quantified ventricular parameters (R2 of 0.96 and 0.90 respectively)
and detected cardiac amyloidosis (AUC 0.79) and hypertrophic cardiomyopathy
(AUC 0.89) on the domestic external validation site. Leveraging measurements
across multiple heart beats, our model can more accurately identify subtle
changes in LV geometry and its causal etiologies. Compared to human experts,
EchoNet-LVH is fully automated, allowing for reproducible, precise
measurements, and lays the foundation for precision diagnosis of cardiac
hypertrophy. As a resource to promote further innovation, we also make publicly
available a large dataset of 23,212 annotated echocardiogram videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Duffy_G/0/1/0/all/0/1"&gt;Grant Duffy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_P/0/1/0/all/0/1"&gt;Paul P Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yuan_N/0/1/0/all/0/1"&gt;Neal Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+He_B/0/1/0/all/0/1"&gt;Bryan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kwan_A/0/1/0/all/0/1"&gt;Alan C. Kwan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shun_Shin_M/0/1/0/all/0/1"&gt;Matthew J. Shun-Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alexander_K/0/1/0/all/0/1"&gt;Kevin M. Alexander&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ebinger_J/0/1/0/all/0/1"&gt;Joseph Ebinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lungren_M/0/1/0/all/0/1"&gt;Matthew P. Lungren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rader_F/0/1/0/all/0/1"&gt;Florian Rader&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1"&gt;David H. Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schnittger_I/0/1/0/all/0/1"&gt;Ingela Schnittger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ashley_E/0/1/0/all/0/1"&gt;Euan A. Ashley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zou_J/0/1/0/all/0/1"&gt;James Y. Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Patel_J/0/1/0/all/0/1"&gt;Jignesh Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Witteles_R/0/1/0/all/0/1"&gt;Ronald Witteles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_S/0/1/0/all/0/1"&gt;Susan Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ouyang_D/0/1/0/all/0/1"&gt;David Ouyang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer Meets Convolution: A Bilateral Awareness Net-work for Semantic Segmentation of Very Fine Resolution Ur-ban Scene Images. (arXiv:2106.12413v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12413</id>
        <link href="http://arxiv.org/abs/2106.12413"/>
        <updated>2021-06-24T01:51:43.026Z</updated>
        <summary type="html"><![CDATA[Semantic segmentation from very fine resolution (VFR) urban scene images
plays a significant role in several application scenarios including autonomous
driving, land cover classification, and urban planning, etc. However, the
tremendous details contained in the VFR image severely limit the potential of
the existing deep learning approaches. More seriously, the considerable
variations in scale and appearance of objects further deteriorate the
representational capacity of those se-mantic segmentation methods, leading to
the confusion of adjacent objects. Addressing such is-sues represents a
promising research field in the remote sensing community, which paves the way
for scene-level landscape pattern analysis and decision making. In this
manuscript, we pro-pose a bilateral awareness network (BANet) which contains a
dependency path and a texture path to fully capture the long-range
relationships and fine-grained details in VFR images. Specif-ically, the
dependency path is conducted based on the ResT, a novel Transformer backbone
with memory-efficient multi-head self-attention, while the texture path is
built on the stacked convo-lution operation. Besides, using the linear
attention mechanism, a feature aggregation module (FAM) is designed to
effectively fuse the dependency features and texture features. Extensive
experiments conducted on the three large-scale urban scene image segmentation
datasets, i.e., ISPRS Vaihingen dataset, ISPRS Potsdam dataset, and UAVid
dataset, demonstrate the effective-ness of our BANet. Specifically, a 64.6%
mIoU is achieved on the UAVid dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Libo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Rui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dongzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1"&gt;Chenxi Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Teng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1"&gt;Xiaoliang Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-modal and frequency-weighted tensor nuclear norm for hyperspectral image denoising. (arXiv:2106.12489v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12489</id>
        <link href="http://arxiv.org/abs/2106.12489"/>
        <updated>2021-06-24T01:51:43.020Z</updated>
        <summary type="html"><![CDATA[Low-rankness is important in the hyperspectral image (HSI) denoising tasks.
The tensor nuclear norm (TNN), defined based on the tensor singular value
decomposition, is a state-of-the-art method to describe the low-rankness of
HSI. However, TNN ignores some of the physical meanings of HSI in tackling the
denoising tasks, leading to suboptimal denoising performance. In this paper, we
propose the multi-modal and frequency-weighted tensor nuclear norm (MFWTNN) and
the non-convex MFWTNN for HSI denoising tasks. Firstly, we investigate the
physical meaning of frequency components and reconsider their weights to
improve the low-rank representation ability of TNN. Meanwhile, we also consider
the correlation among two spatial dimensions and the spectral dimension of HSI
and combine the above improvements to TNN to propose MFWTNN. Secondly, we use
non-convex functions to approximate the rank function of the frequency tensor
and propose the NonMFWTNN to relax the MFWTNN better. Besides, we adaptively
choose bigger weights for slices mainly containing noise information and
smaller weights for slices containing profile information. Finally, we develop
the efficient alternating direction method of multiplier (ADMM) based algorithm
to solve the proposed models, and the effectiveness of our models are
substantiated in simulated and real HSI datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xiaozhen Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kong_W/0/1/0/all/0/1"&gt;Wenfeng Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ning_J/0/1/0/all/0/1"&gt;Jifeng Ning&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collaborative Visual Inertial SLAM for Multiple Smart Phones. (arXiv:2106.12186v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.12186</id>
        <link href="http://arxiv.org/abs/2106.12186"/>
        <updated>2021-06-24T01:51:43.001Z</updated>
        <summary type="html"><![CDATA[The efficiency and accuracy of mapping are crucial in a large scene and
long-term AR applications. Multi-agent cooperative SLAM is the precondition of
multi-user AR interaction. The cooperation of multiple smart phones has the
potential to improve efficiency and robustness of task completion and can
complete tasks that a single agent cannot do. However, it depends on robust
communication, efficient location detection, robust mapping, and efficient
information sharing among agents. We propose a multi-intelligence collaborative
monocular visual-inertial SLAM deployed on multiple ios mobile devices with a
centralized architecture. Each agent can independently explore the environment,
run a visual-inertial odometry module online, and then send all the measurement
information to a central server with higher computing resources. The server
manages all the information received, detects overlapping areas, merges and
optimizes the map, and shares information with the agents when needed. We have
verified the performance of the system in public datasets and real
environments. The accuracy of mapping and fusion of the proposed system is
comparable to VINS-Mono which requires higher computing resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jialing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Ruyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kaiqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianhua Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1"&gt;Dongyan Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adapting Off-the-Shelf Source Segmenter for Target Medical Image Segmentation. (arXiv:2106.12497v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12497</id>
        <link href="http://arxiv.org/abs/2106.12497"/>
        <updated>2021-06-24T01:51:42.987Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from
a labeled source domain to an unlabeled and unseen target domain, which is
usually trained on data from both domains. Access to the source domain data at
the adaptation stage, however, is often limited, due to data storage or privacy
issues. To alleviate this, in this work, we target source free UDA for
segmentation, and propose to adapt an ``off-the-shelf" segmentation model
pre-trained in the source domain to the target domain, with an adaptive
batch-wise normalization statistics adaptation framework. Specifically, the
domain-specific low-order batch statistics, i.e., mean and variance, are
gradually adapted with an exponential momentum decay scheme, while the
consistency of domain shareable high-order batch statistics, i.e., scaling and
shifting parameters, is explicitly enforced by our optimization objective. The
transferability of each channel is adaptively measured first from which to
balance the contribution of each channel. Moreover, the proposed source free
UDA framework is orthogonal to unsupervised learning methods, e.g.,
self-entropy minimization, which can thus be simply added on top of our
framework. Extensive experiments on the BraTS 2018 database show that our
source free UDA framework outperformed existing source-relaxed UDA methods for
the cross-subtype UDA segmentation task and yielded comparable results for the
cross-modality UDA segmentation task, compared with a supervised UDA methods
with the source data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1"&gt;Fangxu Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1"&gt;Georges El Fakhri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1"&gt;Jonghye Woo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CharacterChat: Supporting the Creation of Fictional Characters through Conversation and Progressive Manifestation with a Chatbot. (arXiv:2106.12314v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2106.12314</id>
        <link href="http://arxiv.org/abs/2106.12314"/>
        <updated>2021-06-24T01:51:42.964Z</updated>
        <summary type="html"><![CDATA[We present CharacterChat, a concept and chatbot to support writers in
creating fictional characters. Concretely, writers progressively turn the bot
into their imagined character through conversation. We iteratively developed
CharacterChat in a user-centred approach, starting with a survey on character
creation with writers (N=30), followed by two qualitative user studies (N=7 and
N=8). Our prototype combines two modes: (1) Guided prompts help writers define
character attributes (e.g. User: "Your name is Jane."), including suggestions
for attributes (e.g. Bot: "What is my main motivation?") and values, realised
as a rule-based system with a concept network. (2) Open conversation with the
chatbot helps writers explore their character and get inspiration, realised
with a language model that takes into account the defined character attributes.
Our user studies reveal benefits particularly for early stages of character
creation, and challenges due to limited conversational capabilities. We
conclude with lessons learned and ideas for future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schmitt_O/0/1/0/all/0/1"&gt;Oliver Schmitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buschek_D/0/1/0/all/0/1"&gt;Daniel Buschek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Great Service! Fine-grained Parsing of Implicit Arguments. (arXiv:2106.02561v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02561</id>
        <link href="http://arxiv.org/abs/2106.02561"/>
        <updated>2021-06-24T01:51:42.958Z</updated>
        <summary type="html"><![CDATA[Broad-coverage meaning representations in NLP mostly focus on explicitly
expressed content. More importantly, the scarcity of datasets annotating
diverse implicit roles limits empirical studies into their linguistic nuances.
For example, in the web review "Great service!", the provider and consumer are
implicit arguments of different types. We examine an annotated corpus of
fine-grained implicit arguments (Cui and Hershcovich, 2020) by carefully
re-annotating it, resolving several inconsistencies. Subsequently, we present
the first transition-based neural parser that can handle implicit arguments
dynamically, and experiment with two different transition systems on the
improved dataset. We find that certain types of implicit arguments are more
difficult to parse than others and that the simpler system is more accurate in
recovering implicit arguments, despite having a lower overall parsing score,
attesting current reasoning limitations of NLP models. This work will
facilitate a better understanding of implicit and underspecified language, by
incorporating it holistically into meaning representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1"&gt;Ruixiang Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1"&gt;Daniel Hershcovich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D human tongue reconstruction from single "in-the-wild" images. (arXiv:2106.12302v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12302</id>
        <link href="http://arxiv.org/abs/2106.12302"/>
        <updated>2021-06-24T01:51:42.953Z</updated>
        <summary type="html"><![CDATA[3D face reconstruction from a single image is a task that has garnered
increased interest in the Computer Vision community, especially due to its
broad use in a number of applications such as realistic 3D avatar creation,
pose invariant face recognition and face hallucination. Since the introduction
of the 3D Morphable Model in the late 90's, we witnessed an explosion of
research aiming at particularly tackling this task. Nevertheless, despite the
increasing level of detail in the 3D face reconstructions from single images
mainly attributed to deep learning advances, finer and highly deformable
components of the face such as the tongue are still absent from all 3D face
models in the literature, although being very important for the realness of the
3D avatar representations. In this work we present the first, to the best of
our knowledge, end-to-end trainable pipeline that accurately reconstructs the
3D face together with the tongue. Moreover, we make this pipeline robust in
"in-the-wild" images by introducing a novel GAN method tailored for 3D tongue
surface generation. Finally, we make publicly available to the community the
first diverse tongue dataset, consisting of 1,800 raw scans of 700 individuals
varying in gender, age, and ethnicity backgrounds. As we demonstrate in an
extensive series of quantitative as well as qualitative experiments, our model
proves to be robust and realistically captures the 3D tongue structure, even in
adverse "in-the-wild" conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ploumpis_S/0/1/0/all/0/1"&gt;Stylianos Ploumpis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moschoglou_S/0/1/0/all/0/1"&gt;Stylianos Moschoglou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Triantafyllou_V/0/1/0/all/0/1"&gt;Vasileios Triantafyllou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1"&gt;Stefanos Zafeiriou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image-to-Image Translation of Synthetic Samples for Rare Classes. (arXiv:2106.12212v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12212</id>
        <link href="http://arxiv.org/abs/2106.12212"/>
        <updated>2021-06-24T01:51:42.937Z</updated>
        <summary type="html"><![CDATA[The natural world is long-tailed: rare classes are observed orders of
magnitudes less frequently than common ones, leading to highly-imbalanced data
where rare classes can have only handfuls of examples. Learning from few
examples is a known challenge for deep learning based classification
algorithms, and is the focus of the field of low-shot learning. One potential
approach to increase the training data for these rare classes is to augment the
limited real data with synthetic samples. This has been shown to help, but the
domain shift between real and synthetic hinders the approaches' efficacy when
tested on real data.

We explore the use of image-to-image translation methods to close the domain
gap between synthetic and real imagery for animal species classification in
data collected from camera traps: motion-activated static cameras used to
monitor wildlife. We use low-level feature alignment between source and target
domains to make synthetic data for a rare species generated using a graphics
engine more "realistic". Compared against a system augmented with unaligned
synthetic data, our experiments show a considerable decrease in classification
error rates on a rare species.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lanzini_E/0/1/0/all/0/1"&gt;Edoardo Lanzini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beery_S/0/1/0/all/0/1"&gt;Sara Beery&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Circular-Structured Representation for Visual Emotion Distribution Learning. (arXiv:2106.12450v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12450</id>
        <link href="http://arxiv.org/abs/2106.12450"/>
        <updated>2021-06-24T01:51:42.925Z</updated>
        <summary type="html"><![CDATA[Visual Emotion Analysis (VEA) has attracted increasing attention recently
with the prevalence of sharing images on social networks. Since human emotions
are ambiguous and subjective, it is more reasonable to address VEA in a label
distribution learning (LDL) paradigm rather than a single-label classification
task. Different from other LDL tasks, there exist intrinsic relationships
between emotions and unique characteristics within them, as demonstrated in
psychological theories. Inspired by this, we propose a well-grounded
circular-structured representation to utilize the prior knowledge for visual
emotion distribution learning. To be specific, we first construct an Emotion
Circle to unify any emotional state within it. On the proposed Emotion Circle,
each emotion distribution is represented with an emotion vector, which is
defined with three attributes (i.e., emotion polarity, emotion type, emotion
intensity) as well as two properties (i.e., similarity, additivity). Besides,
we design a novel Progressive Circular (PC) loss to penalize the
dissimilarities between predicted emotion vector and labeled one in a
coarse-to-fine manner, which further boosts the learning process in an
emotion-specific way. Extensive experiments and comparisons are conducted on
public visual emotion distribution datasets, and the results demonstrate that
the proposed method outperforms the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jingyuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lie_J/0/1/0/all/0/1"&gt;Ji Lie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Leida Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiumei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xinbo Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STRESS: Super-Resolution for Dynamic Fetal MRI using Self-Supervised Learning. (arXiv:2106.12407v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12407</id>
        <link href="http://arxiv.org/abs/2106.12407"/>
        <updated>2021-06-24T01:51:42.915Z</updated>
        <summary type="html"><![CDATA[Fetal motion is unpredictable and rapid on the scale of conventional MR scan
times. Therefore, dynamic fetal MRI, which aims at capturing fetal motion and
dynamics of fetal function, is limited to fast imaging techniques with
compromises in image quality and resolution. Super-resolution for dynamic fetal
MRI is still a challenge, especially when multi-oriented stacks of image slices
for oversampling are not available and high temporal resolution for recording
the dynamics of the fetus or placenta is desired. Further, fetal motion makes
it difficult to acquire high-resolution images for supervised learning methods.
To address this problem, in this work, we propose STRESS (Spatio-Temporal
Resolution Enhancement with Simulated Scans), a self-supervised
super-resolution framework for dynamic fetal MRI with interleaved slice
acquisitions. Our proposed method simulates an interleaved slice acquisition
along the high-resolution axis on the originally acquired data to generate
pairs of low- and high-resolution images. Then, it trains a super-resolution
network by exploiting both spatial and temporal correlations in the MR time
series, which is used to enhance the resolution of the original data.
Evaluations on both simulated and in utero data show that our proposed method
outperforms other self-supervised super-resolution methods and improves image
quality, which is beneficial to other downstream tasks and evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1"&gt;Junshen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Turk_E/0/1/0/all/0/1"&gt;Esra Abaci Turk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Grant_P/0/1/0/all/0/1"&gt;P. Ellen Grant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Golland_P/0/1/0/all/0/1"&gt;Polina Golland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Adalsteinsson_E/0/1/0/all/0/1"&gt;Elfar Adalsteinsson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NodePiece: Compositional and Parameter-Efficient Representations of Large Knowledge Graphs. (arXiv:2106.12144v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12144</id>
        <link href="http://arxiv.org/abs/2106.12144"/>
        <updated>2021-06-24T01:51:42.903Z</updated>
        <summary type="html"><![CDATA[Conventional representation learning algorithms for knowledge graphs (KG) map
each entity to a unique embedding vector. Such a shallow lookup results in a
linear growth of memory consumption for storing the embedding matrix and incurs
high computational costs when working with real-world KGs. Drawing parallels
with subword tokenization commonly used in NLP, we explore the landscape of
more parameter-efficient node embedding strategies with possibly sublinear
memory requirements. To this end, we propose NodePiece, an anchor-based
approach to learn a fixed-size entity vocabulary. In NodePiece, a vocabulary of
subword/sub-entity units is constructed from anchor nodes in a graph with known
relation types. Given such a fixed-size vocabulary, it is possible to bootstrap
an encoding and embedding for any entity, including those unseen during
training. Experiments show that NodePiece performs competitively in node
classification, link prediction, and relation prediction tasks while retaining
less than 10% of explicit nodes in a graph as anchors and often having 10x
fewer parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Galkin_M/0/1/0/all/0/1"&gt;Mikhail Galkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiapeng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Denis_E/0/1/0/all/0/1"&gt;Etienne Denis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1"&gt;William L. Hamilton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time Instance Segmentation with Discriminative Orientation Maps. (arXiv:2106.12204v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12204</id>
        <link href="http://arxiv.org/abs/2106.12204"/>
        <updated>2021-06-24T01:51:42.879Z</updated>
        <summary type="html"><![CDATA[Although instance segmentation has made considerable advancement over recent
years, it's still a challenge to design high accuracy algorithms with real-time
performance. In this paper, we propose a real-time instance segmentation
framework termed OrienMask. Upon the one-stage object detector YOLOv3, a mask
head is added to predict some discriminative orientation maps, which are
explicitly defined as spatial offset vectors for both foreground and background
pixels. Thanks to the discrimination ability of orientation maps, masks can be
recovered without the need for extra foreground segmentation. All instances
that match with the same anchor size share a common orientation map. This
special sharing strategy reduces the amortized memory utilization for mask
predictions but without loss of mask granularity. Given the surviving box
predictions after NMS, instance masks can be concurrently constructed from the
corresponding orientation maps with low complexity. Owing to the concise design
for mask representation and its effective integration with the anchor-based
object detector, our method is qualified under real-time conditions while
maintaining competitive accuracy. Experiments on COCO benchmark show that
OrienMask achieves 34.8 mask AP at the speed of 42.7 fps evaluated with a
single RTX 2080 Ti. The code is available at https://github.com/duwt/OrienMask.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1"&gt;Wentao Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1"&gt;Zhiyu Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shuya Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_C/0/1/0/all/0/1"&gt;Chengyu Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiman Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1"&gt;Tingming Bai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixtures of Deep Neural Experts for Automated Speech Scoring. (arXiv:2106.12475v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12475</id>
        <link href="http://arxiv.org/abs/2106.12475"/>
        <updated>2021-06-24T01:51:42.862Z</updated>
        <summary type="html"><![CDATA[The paper copes with the task of automatic assessment of second language
proficiency from the language learners' spoken responses to test prompts. The
task has significant relevance to the field of computer assisted language
learning. The approach presented in the paper relies on two separate modules:
(1) an automatic speech recognition system that yields text transcripts of the
spoken interactions involved, and (2) a multiple classifier system based on
deep learners that ranks the transcripts into proficiency classes. Different
deep neural network architectures (both feed-forward and recurrent) are
specialized over diverse representations of the texts in terms of: a reference
grammar, the outcome of probabilistic language models, several word embeddings,
and two bag-of-word models. Combination of the individual classifiers is
realized either via a probabilistic pseudo-joint model, or via a neural mixture
of experts. Using the data of the third Spoken CALL Shared Task challenge, the
highest values to date were obtained in terms of three popular evaluation
metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1"&gt;Sara Papi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trentin_E/0/1/0/all/0/1"&gt;Edmondo Trentin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gretter_R/0/1/0/all/0/1"&gt;Roberto Gretter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matassoni_M/0/1/0/all/0/1"&gt;Marco Matassoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Falavigna_D/0/1/0/all/0/1"&gt;Daniele Falavigna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PALRACE: Reading Comprehension Dataset with Human Data and Labeled Rationales. (arXiv:2106.12373v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12373</id>
        <link href="http://arxiv.org/abs/2106.12373"/>
        <updated>2021-06-24T01:51:42.857Z</updated>
        <summary type="html"><![CDATA[Pre-trained language models achieves high performance on machine reading
comprehension (MRC) tasks but the results are hard to explain. An appealing
approach to make models explainable is to provide rationales for its decision.
To facilitate supervised learning of human rationales, here we present PALRACE
(Pruned And Labeled RACE), a new MRC dataset with human labeled rationales for
800 passages selected from the RACE dataset. We further classified the question
to each passage into 6 types. Each passage was read by at least 26
participants, who labeled their rationales to answer the question. Besides, we
conducted a rationale evaluation session in which participants were asked to
answering the question solely based on labeled rationales, confirming that the
labeled rationales were of high quality and can sufficiently support question
answering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1"&gt;Jiajie Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuran Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1"&gt;Peiqing Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1"&gt;Cheng Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xunyi Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1"&gt;Nai Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Behavior Mimics Distribution: Combining Individual and Group Behaviors for Federated Learning. (arXiv:2106.12300v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12300</id>
        <link href="http://arxiv.org/abs/2106.12300"/>
        <updated>2021-06-24T01:51:42.818Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) has become an active and promising distributed
machine learning paradigm. As a result of statistical heterogeneity, recent
studies clearly show that the performance of popular FL methods (e.g., FedAvg)
deteriorates dramatically due to the client drift caused by local updates. This
paper proposes a novel Federated Learning algorithm (called IGFL), which
leverages both Individual and Group behaviors to mimic distribution, thereby
improving the ability to deal with heterogeneity. Unlike existing FL methods,
our IGFL can be applied to both client and server optimization. As a
by-product, we propose a new attention-based federated learning in the server
optimization of IGFL. To the best of our knowledge, this is the first time to
incorporate attention mechanisms into federated optimization. We conduct
extensive experiments and show that IGFL can significantly improve the
performance of existing federated learning methods. Especially when the
distributions of data among individuals are diverse, IGFL can improve the
classification accuracy by about 13% compared with prior baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hua Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1"&gt;Fanhua Shang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuanyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hongying Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Instance-based Vision Transformer for Subtyping of Papillary Renal Cell Carcinoma in Histopathological Image. (arXiv:2106.12265v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12265</id>
        <link href="http://arxiv.org/abs/2106.12265"/>
        <updated>2021-06-24T01:51:42.810Z</updated>
        <summary type="html"><![CDATA[Histological subtype of papillary (p) renal cell carcinoma (RCC), type 1 vs.
type 2, is an essential prognostic factor. The two subtypes of pRCC have a
similar pattern, i.e., the papillary architecture, yet some subtle differences,
including cellular and cell-layer level patterns. However, the cellular and
cell-layer level patterns almost cannot be captured by existing CNN-based
models in large-size histopathological images, which brings obstacles to
directly applying these models to such a fine-grained classification task. This
paper proposes a novel instance-based Vision Transformer (i-ViT) to learn
robust representations of histopathological images for the pRCC subtyping task
by extracting finer features from instance patches (by cropping around
segmented nuclei and assigning predicted grades). The proposed i-ViT takes
top-K instances as input and aggregates them for capturing both the cellular
and cell-layer level patterns by a position-embedding layer, a grade-embedding
layer, and a multi-head multi-layer self-attention module. To evaluate the
performance of the proposed framework, experienced pathologists are invited to
selected 1162 regions of interest from 171 whole slide images of type 1 and
type 2 pRCC. Experimental results show that the proposed method achieves better
performance than existing CNN-based models with a significant margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zeyu Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_B/0/1/0/all/0/1"&gt;Bangyang Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xianli Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1"&gt;Chang Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jialun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chunbao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1"&gt;Deyu Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mutual-Information Based Few-Shot Classification. (arXiv:2106.12252v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12252</id>
        <link href="http://arxiv.org/abs/2106.12252"/>
        <updated>2021-06-24T01:51:42.804Z</updated>
        <summary type="html"><![CDATA[We introduce Transductive Infomation Maximization (TIM) for few-shot
learning. Our method maximizes the mutual information between the query
features and their label predictions for a given few-shot task, in conjunction
with a supervision loss based on the support set. We motivate our transductive
loss by deriving a formal relation between the classification accuracy and
mutual-information maximization. Furthermore, we propose a new
alternating-direction solver, which substantially speeds up transductive
inference over gradient-based optimization, while yielding competitive
accuracy. We also provide a convergence analysis of our solver based on
Zangwill's theory and bound-optimization arguments. TIM inference is modular:
it can be used on top of any base-training feature extractor. Following
standard transductive few-shot settings, our comprehensive experiments
demonstrate that TIM outperforms state-of-the-art methods significantly across
various datasets and networks, while used on top of a fixed feature extractor
trained with simple cross-entropy on the base classes, without resorting to
complex meta-learning schemes. It consistently brings between 2 % and 5 %
improvement in accuracy over the best performing method, not only on all the
well-established few-shot benchmarks but also on more challenging scenarios,
with random tasks, domain shift and larger numbers of classes, as in the
recently introduced META-DATASET. Our code is publicly available at
https://github.com/mboudiaf/TIM. We also publicly release a standalone PyTorch
implementation of META-DATASET, along with additional benchmarking results, at
https://github.com/mboudiaf/pytorch-meta-dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boudiaf_M/0/1/0/all/0/1"&gt;Malik Boudiaf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masud_Z/0/1/0/all/0/1"&gt;Ziko Imtiaz Masud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rony_J/0/1/0/all/0/1"&gt;J&amp;#xe9;r&amp;#xf4;me Rony&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1"&gt;Jose Dolz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1"&gt;Ismail Ben Ayed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1"&gt;Pablo Piantanida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deformed2Self: Self-Supervised Denoising for Dynamic Medical Imaging. (arXiv:2106.12175v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12175</id>
        <link href="http://arxiv.org/abs/2106.12175"/>
        <updated>2021-06-24T01:51:42.786Z</updated>
        <summary type="html"><![CDATA[Image denoising is of great importance for medical imaging system, since it
can improve image quality for disease diagnosis and downstream image analyses.
In a variety of applications, dynamic imaging techniques are utilized to
capture the time-varying features of the subject, where multiple images are
acquired for the same subject at different time points. Although
signal-to-noise ratio of each time frame is usually limited by the short
acquisition time, the correlation among different time frames can be exploited
to improve denoising results with shared information across time frames. With
the success of neural networks in computer vision, supervised deep learning
methods show prominent performance in single-image denoising, which rely on
large datasets with clean-vs-noisy image pairs. Recently, several
self-supervised deep denoising models have been proposed, achieving promising
results without needing the pairwise ground truth of clean images. In the field
of multi-image denoising, however, very few works have been done on extracting
correlated information from multiple slices for denoising using self-supervised
deep learning methods. In this work, we propose Deformed2Self, an end-to-end
self-supervised deep learning framework for dynamic imaging denoising. It
combines single-image and multi-image denoising to improve image quality and
use a spatial transformer network to model motion between different slices.
Further, it only requires a single noisy image with a few auxiliary
observations at different time frames for training and inference. Evaluations
on phantom and in vivo data with different noise statistics show that our
method has comparable performance to other state-of-the-art unsupervised or
self-supervised denoising methods and outperforms under high noise levels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1"&gt;Junshen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Adalsteinsson_E/0/1/0/all/0/1"&gt;Elfar Adalsteinsson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[APNN-TC: Accelerating Arbitrary Precision Neural Networks on Ampere GPU Tensor Cores. (arXiv:2106.12169v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2106.12169</id>
        <link href="http://arxiv.org/abs/2106.12169"/>
        <updated>2021-06-24T01:51:42.780Z</updated>
        <summary type="html"><![CDATA[Over the years, accelerating neural networks with quantization has been
widely studied. Unfortunately, prior efforts with diverse precisions (e.g.,
1-bit weights and 2-bit activations) are usually restricted by limited
precision support on GPUs (e.g., int1 and int4). To break such restrictions, we
introduce the first Arbitrary Precision Neural Network framework (APNN-TC) to
fully exploit quantization benefits on Ampere GPU Tensor Cores. Specifically,
APNN-TC first incorporates a novel emulation algorithm to support arbitrary
short bit-width computation with int1 compute primitives and XOR/AND Boolean
operations. Second, APNN-TC integrates arbitrary precision layer designs to
efficiently map our emulation algorithm to Tensor Cores with novel batching
strategies and specialized memory organization. Third, APNN-TC embodies a novel
arbitrary precision NN design to minimize memory access across layers and
further improve performance. Extensive evaluations show that APNN-TC can
achieve significant speedup over CUTLASS kernels and various NN models, such as
ResNet and VGG.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1"&gt;Boyuan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuke Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_T/0/1/0/all/0/1"&gt;Tong Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1"&gt;Ang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yufei Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Assistive Technologies for Activities of Daily Living of Elderly. (arXiv:2106.12183v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2106.12183</id>
        <link href="http://arxiv.org/abs/2106.12183"/>
        <updated>2021-06-24T01:51:42.774Z</updated>
        <summary type="html"><![CDATA[One of the distinct features of this century has been the population of older
adults which has been on a constant rise. Elderly people have several needs and
requirements due to physical disabilities, cognitive issues, weakened memory
and disorganized behavior, that they face with increasing age. The extent of
these limitations also differs according to the varying diversities in elderly,
which include age, gender, background, experience, skills, knowledge and so on.
These varying needs and challenges with increasing age, limits abilities of
older adults to perform Activities of Daily Living (ADLs) in an independent
manner. To add to it, the shortage of caregivers creates a looming need for
technology-based services for elderly people, to assist them in performing
their daily routine tasks to sustain their independent living and active aging.
To address these needs, this work consists of making three major contributions
in this field. First, it provides a rather comprehensive review of assisted
living technologies aimed at helping elderly people to perform ADLs. Second,
the work discusses the challenges identified through this review, that
currently exist in the context of implementation of assisted living services
for elderly care in Smart Homes and Smart Cities. Finally, the work also
outlines an approach for implementation, extension and integration of the
existing works in this field for development of a much-needed framework that
can provide personalized assistance and user-centered behavior interventions to
elderly as per their varying and ever-changing needs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1"&gt;Nirmalya Thakur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1"&gt;Chia Y. Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Region-Aware Network: Model Human's Top-Down Visual Perception Mechanism for Crowd Counting. (arXiv:2106.12163v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12163</id>
        <link href="http://arxiv.org/abs/2106.12163"/>
        <updated>2021-06-24T01:51:42.768Z</updated>
        <summary type="html"><![CDATA[Background noise and scale variation are common problems that have been long
recognized in crowd counting. Humans glance at a crowd image and instantly know
the approximate number of human and where they are through attention the crowd
regions and the congestion degree of crowd regions with a global receptive
filed. Hence, in this paper, we propose a novel feedback network with
Region-Aware block called RANet by modeling human's Top-Down visual perception
mechanism. Firstly, we introduce a feedback architecture to generate priority
maps that provide prior about candidate crowd regions in input images. The
prior enables the RANet pay more attention to crowd regions. Then we design
Region-Aware block that could adaptively encode the contextual information into
input images through global receptive field. More specifically, we scan the
whole input images and its priority maps in the form of column vector to obtain
a relevance matrix estimating their similarity. The relevance matrix obtained
would be utilized to build global relationships between pixels. Our method
outperforms state-of-the-art crowd counting methods on several public datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuehai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Badong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1"&gt;Shaoyi Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PatentNet: A Large-Scale Incomplete Multiview, Multimodal, Multilabel Industrial Goods Image Database. (arXiv:2106.12139v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12139</id>
        <link href="http://arxiv.org/abs/2106.12139"/>
        <updated>2021-06-24T01:51:42.762Z</updated>
        <summary type="html"><![CDATA[In deep learning area, large-scale image datasets bring a breakthrough in the
success of object recognition and retrieval. Nowadays, as the embodiment of
innovation, the diversity of the industrial goods is significantly larger, in
which the incomplete multiview, multimodal and multilabel are different from
the traditional dataset. In this paper, we introduce an industrial goods
dataset, namely PatentNet, with numerous highly diverse, accurate and detailed
annotations of industrial goods images, and corresponding texts. In PatentNet,
the images and texts are sourced from design patent. Within over 6M images and
corresponding texts of industrial goods labeled manually checked by
professionals, PatentNet is the first ongoing industrial goods image database
whose varieties are wider than industrial goods datasets used previously for
benchmarking. PatentNet organizes millions of images into 32 classes and 219
subclasses based on the Locarno Classification Agreement. Through extensive
experiments on image classification, image retrieval and incomplete multiview
clustering, we demonstrate that our PatentNet is much more diverse, complex,
and challenging, enjoying higher potentials than existing industrial image
datasets. Furthermore, the characteristics of incomplete multiview, multimodal
and multilabel in PatentNet are able to offer unparalleled opportunities in the
artificial intelligence community and beyond.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lei_F/0/1/0/all/0/1"&gt;Fangyuan Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1"&gt;Da Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jianjian Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1"&gt;Ruijun Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Senhong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jiangzhong Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yusen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1"&gt;Qingyun Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vision-based Behavioral Recognition of Novelty Preference in Pigs. (arXiv:2106.12181v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12181</id>
        <link href="http://arxiv.org/abs/2106.12181"/>
        <updated>2021-06-24T01:51:42.746Z</updated>
        <summary type="html"><![CDATA[Behavioral scoring of research data is crucial for extracting domain-specific
metrics but is bottlenecked on the ability to analyze enormous volumes of
information using human labor. Deep learning is widely viewed as a key
advancement to relieve this bottleneck. We identify one such domain, where deep
learning can be leveraged to alleviate the process of manual scoring. Novelty
preference paradigms have been widely used to study recognition memory in pigs,
but analysis of these videos requires human intervention. We introduce a subset
of such videos in the form of the 'Pig Novelty Preference Behavior' (PNPB)
dataset that is fully annotated with pig actions and keypoints. In order to
demonstrate the application of state-of-the-art action recognition models on
this dataset, we compare LRCN, C3D, and TSM on the basis of various analytical
metrics and discuss common pitfalls of the models. Our methods achieve an
accuracy of 93% and a mean Average Precision of 96% in estimating piglet
behavior.

We open-source our code and annotated dataset at
https://github.com/AIFARMS/NOR-behavior-recognition]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shirke_A/0/1/0/all/0/1"&gt;Aniket Shirke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golden_R/0/1/0/all/0/1"&gt;Rebecca Golden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gautam_M/0/1/0/all/0/1"&gt;Mrinal Gautam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Green_Miller_A/0/1/0/all/0/1"&gt;Angela Green-Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caesar_M/0/1/0/all/0/1"&gt;Matthew Caesar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dilger_R/0/1/0/all/0/1"&gt;Ryan N. Dilger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CxSE: Chest X-ray Slow Encoding CNN forCOVID-19 Diagnosis. (arXiv:2106.12157v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12157</id>
        <link href="http://arxiv.org/abs/2106.12157"/>
        <updated>2021-06-24T01:51:42.741Z</updated>
        <summary type="html"><![CDATA[The coronavirus continues to disrupt our everyday lives as it spreads at an
exponential rate. It needs to be detected quickly in order to quarantine
positive patients so as to avoid further spread. This work proposes a new
convolutional neural network (CNN) architecture called 'slow Encoding CNN. The
proposed model's best performance wrt Sensitivity, Positive Predictive Value
(PPV) found to be SP=0.67, PP=0.98, SN=0.96, and PN=0.52 on AI AGAINST COVID19
- Screening X-ray images for COVID-19 Infections competition's test data
samples. SP and PP stand for the Sensitivity and PPV of the COVID-19 positive
class, while PN and SN stand for the Sensitivity and PPV of the COVID-19
negative class.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Akilan_T/0/1/0/all/0/1"&gt;Thangarajah Akilan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-trained Models for Natural Language Processing: A Survey. (arXiv:2003.08271v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.08271</id>
        <link href="http://arxiv.org/abs/2003.08271"/>
        <updated>2021-06-24T01:51:42.735Z</updated>
        <summary type="html"><![CDATA[Recently, the emergence of pre-trained models (PTMs) has brought natural
language processing (NLP) to a new era. In this survey, we provide a
comprehensive review of PTMs for NLP. We first briefly introduce language
representation learning and its research progress. Then we systematically
categorize existing PTMs based on a taxonomy with four perspectives. Next, we
describe how to adapt the knowledge of PTMs to the downstream tasks. Finally,
we outline some potential directions of PTMs for future research. This survey
is purposed to be a hands-on guide for understanding, using, and developing
PTMs for various NLP tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1"&gt;Xipeng Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1"&gt;Tianxiang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yige Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yunfan Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_N/0/1/0/all/0/1"&gt;Ning Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xuanjing Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Negative Learning for Implicit Pseudo Label Rectification in Source-Free Domain Adaptive Semantic Segmentation. (arXiv:2106.12123v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12123</id>
        <link href="http://arxiv.org/abs/2106.12123"/>
        <updated>2021-06-24T01:51:42.728Z</updated>
        <summary type="html"><![CDATA[It is desirable to transfer the knowledge stored in a well-trained source
model onto non-annotated target domain in the absence of source data. However,
state-of-the-art methods for source free domain adaptation (SFDA) are subject
to strict limits: 1) access to internal specifications of source models is a
must; and 2) pseudo labels should be clean during self-training, making
critical tasks relying on semantic segmentation unreliable. Aiming at these
pitfalls, this study develops a domain adaptive solution to semantic
segmentation with pseudo label rectification (namely \textit{PR-SFDA}), which
operates in two phases: 1) \textit{Confidence-regularized unsupervised
learning}: Maximum squares loss applies to regularize the target model to
ensure the confidence in prediction; and 2) \textit{Noise-aware pseudo label
learning}: Negative learning enables tolerance to noisy pseudo labels in
training, meanwhile positive learning achieves fast convergence. Extensive
experiments have been performed on domain adaptive semantic segmentation
benchmark, \textit{GTA5 $\to$ Cityscapes}. Overall, \textit{PR-SFDA} achieves a
performance of 49.0 mIoU, which is very close to that of the state-of-the-art
counterparts. Note that the latter demand accesses to the source model's
internal specifications, whereas the \textit{PR-SFDA} solution needs none as a
sharp contrast.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xin Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1"&gt;Yusong Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yulin He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xiaogang Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reachability Analysis of Convolutional Neural Networks. (arXiv:2106.12074v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12074</id>
        <link href="http://arxiv.org/abs/2106.12074"/>
        <updated>2021-06-24T01:51:42.722Z</updated>
        <summary type="html"><![CDATA[Deep convolutional neural networks have been widely employed as an effective
technique to handle complex and practical problems. However, one of the
fundamental problems is the lack of formal methods to analyze their behavior.
To address this challenge, we propose an approach to compute the exact
reachable sets of a network given an input domain, where the reachable set is
represented by the face lattice structure. Besides the computation of reachable
sets, our approach is also capable of backtracking to the input domain given an
output reachable set. Therefore, a full analysis of a network's behavior can be
realized. In addition, an approach for fast analysis is also introduced, which
conducts fast computation of reachable sets by considering selected sensitive
neurons in each layer. The exact pixel-level reachability analysis method is
evaluated on a CNN for the CIFAR10 dataset and compared to related works. The
fast analysis method is evaluated over a CNN CIFAR10 dataset and VGG16
architecture for the ImageNet dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaodong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamaguchi_T/0/1/0/all/0/1"&gt;Tomoya Yamaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1"&gt;Hoang-Dung Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoxha_B/0/1/0/all/0/1"&gt;Bardh Hoxha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johnson_T/0/1/0/all/0/1"&gt;Taylor T Johnson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prokhorov_D/0/1/0/all/0/1"&gt;Danil Prokhorov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Societal Biases in Language Generation: Progress and Challenges. (arXiv:2105.04054v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04054</id>
        <link href="http://arxiv.org/abs/2105.04054"/>
        <updated>2021-06-24T01:51:42.706Z</updated>
        <summary type="html"><![CDATA[Technology for language generation has advanced rapidly, spurred by
advancements in pre-training large models on massive amounts of data and the
need for intelligent agents to communicate in a natural manner. While
techniques can effectively generate fluent text, they can also produce
undesirable societal biases that can have a disproportionately negative impact
on marginalized populations. Language generation presents unique challenges for
biases in terms of direct user interaction and the structure of decoding
techniques. To better understand these challenges, we present a survey on
societal biases in language generation, focusing on how data and techniques
contribute to biases and progress towards reducing biases. Motivated by a lack
of studies on biases from decoding techniques, we also conduct experiments to
quantify the effects of these techniques. By further discussing general trends
and open challenges, we call to attention promising directions for research and
the importance of fairness and inclusivity considerations for language
generation applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_E/0/1/0/all/0/1"&gt;Emily Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1"&gt;Premkumar Natarajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1"&gt;Nanyun Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Volume Rendering of Neural Implicit Surfaces. (arXiv:2106.12052v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12052</id>
        <link href="http://arxiv.org/abs/2106.12052"/>
        <updated>2021-06-24T01:51:42.700Z</updated>
        <summary type="html"><![CDATA[Neural volume rendering became increasingly popular recently due to its
success in synthesizing novel views of a scene from a sparse set of input
images. So far, the geometry learned by neural volume rendering techniques was
modeled using a generic density function. Furthermore, the geometry itself was
extracted using an arbitrary level set of the density function leading to a
noisy, often low fidelity reconstruction. The goal of this paper is to improve
geometry representation and reconstruction in neural volume rendering. We
achieve that by modeling the volume density as a function of the geometry. This
is in contrast to previous work modeling the geometry as a function of the
volume density. In more detail, we define the volume density function as
Laplace's cumulative distribution function (CDF) applied to a signed distance
function (SDF) representation. This simple density representation has three
benefits: (i) it provides a useful inductive bias to the geometry learned in
the neural volume rendering process; (ii) it facilitates a bound on the opacity
approximation error, leading to an accurate sampling of the viewing ray.
Accurate sampling is important to provide a precise coupling of geometry and
radiance; and (iii) it allows efficient unsupervised disentanglement of shape
and appearance in volume rendering. Applying this new density representation to
challenging scene multiview datasets produced high quality geometry
reconstructions, outperforming relevant baselines. Furthermore, switching shape
and appearance between scenes is possible due to the disentanglement of the
two.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yariv_L/0/1/0/all/0/1"&gt;Lior Yariv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jiatao Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasten_Y/0/1/0/all/0/1"&gt;Yoni Kasten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipman_Y/0/1/0/all/0/1"&gt;Yaron Lipman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Fashion Image Captioning : Accounting for Data Diversity. (arXiv:2106.12154v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12154</id>
        <link href="http://arxiv.org/abs/2106.12154"/>
        <updated>2021-06-24T01:51:42.695Z</updated>
        <summary type="html"><![CDATA[Image captioning has increasingly large domains of application, and fashion
is not an exception. Having automatic item descriptions is of great interest
for fashion web platforms hosting sometimes hundreds of thousands of images.
This paper is one of the first tackling image captioning for fashion images. To
contribute addressing dataset diversity issues, we introduced the InFashAIv1
dataset containing almost 16.000 African fashion item images with their titles,
prices and general descriptions. We also used the well known DeepFashion
dataset in addition to InFashAIv1. Captions are generated using the
\textit{Show and Tell} model made of CNN encoder and RNN Decoder. We showed
that jointly training the model on both datasets improves captions quality for
African style fashion images, suggesting a transfer learning from Western style
data. The InFashAIv1 dataset is released on
\href{https://github.com/hgilles06/infashai}{Github} to encourage works with
more diversity inclusion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hacheme_G/0/1/0/all/0/1"&gt;Gilles Hacheme&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sayouti_N/0/1/0/all/0/1"&gt;Noureini Sayouti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Head Overcoat Thickness Measure with NASNet-Large-Decoder Net. (arXiv:2106.12054v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12054</id>
        <link href="http://arxiv.org/abs/2106.12054"/>
        <updated>2021-06-24T01:51:42.690Z</updated>
        <summary type="html"><![CDATA[Transmission electron microscopy (TEM) is one of the primary tools to show
microstructural characterization of materials as well as film thickness.
However, manual determination of film thickness from TEM images is
time-consuming as well as subjective, especially when the films in question are
very thin and the need for measurement precision is very high. Such is the case
for head overcoat (HOC) thickness measurements in the magnetic hard disk drive
industry. It is therefore necessary to develop software to automatically
measure HOC thickness. In this paper, for the first time, we propose a HOC
layer segmentation method using NASNet-Large as an encoder and then followed by
a decoder architecture, which is one of the most commonly used architectures in
deep learning for image segmentation. To further improve segmentation results,
we are the first to propose a post-processing layer to remove irrelevant
portions in the segmentation result. To measure the thickness of the segmented
HOC layer, we propose a regressive convolutional neural network (RCNN) model as
well as orthogonal thickness calculation methods. Experimental results
demonstrate a higher dice score for our model which has lower mean squared
error and outperforms current state-of-the-art manual measurement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Youshan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davison_B/0/1/0/all/0/1"&gt;Brian D. Davison&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talghader_V/0/1/0/all/0/1"&gt;Vivien W. Talghader&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhiyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1"&gt;Zhiyong Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kunkel_G/0/1/0/all/0/1"&gt;Gary J. Kunkel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Matrix Factorizations in Subspace Clustering. (arXiv:2106.12016v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12016</id>
        <link href="http://arxiv.org/abs/2106.12016"/>
        <updated>2021-06-24T01:51:42.684Z</updated>
        <summary type="html"><![CDATA[This article explores subspace clustering algorithms using CUR
decompositions, and examines the effect of various hyperparameters in these
algorithms on clustering performance on two real-world benchmark datasets, the
Hopkins155 motion segmentation dataset and the Yale face dataset. Extensive
experiments are done for a variety of sampling methods and oversampling
parameters for these datasets, and some guidelines for parameter choices are
given for practical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arian_R/0/1/0/all/0/1"&gt;Reeshad Arian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamm_K/0/1/0/all/0/1"&gt;Keaton Hamm&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Consistent Predictive Confidence through Fitted Ensembles. (arXiv:2106.12070v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12070</id>
        <link href="http://arxiv.org/abs/2106.12070"/>
        <updated>2021-06-24T01:51:42.668Z</updated>
        <summary type="html"><![CDATA[Deep neural networks are behind many of the recent successes in machine
learning applications. However, these models can produce overconfident
decisions while encountering out-of-distribution (OOD) examples or making a
wrong prediction. This inconsistent predictive confidence limits the
integration of independently-trained learning models into a larger system. This
paper introduces separable concept learning framework to realistically measure
the performance of classifiers in presence of OOD examples. In this setup,
several instances of a classifier are trained on different parts of a partition
of the set of classes. Later, the performance of the combination of these
models is evaluated on a separate test set. Unlike current OOD detection
techniques, this framework does not require auxiliary OOD datasets and does not
separate classification from detection performance. Furthermore, we present a
new strong baseline for more consistent predictive confidence in deep models,
called fitted ensembles, where overconfident predictions are rectified by
transformed versions of the original classification task. Fitted ensembles can
naturally detect OOD examples without requiring auxiliary data by observing
contradicting predictions among its components. Experiments on MNIST, SVHN,
CIFAR-10/100, and ImageNet show fitted ensemble significantly outperform
conventional ensembles on OOD examples and are possible to scale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kardan_N/0/1/0/all/0/1"&gt;Navid Kardan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Ankit Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stanley_K/0/1/0/all/0/1"&gt;Kenneth O. Stanley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Neurally-Guided Shape Parser: A Monte Carlo Method for Hierarchical Labeling of Over-segmented 3D Shapes. (arXiv:2106.12026v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12026</id>
        <link href="http://arxiv.org/abs/2106.12026"/>
        <updated>2021-06-24T01:51:42.663Z</updated>
        <summary type="html"><![CDATA[Many learning-based 3D shape semantic segmentation methods assign labels to
shape atoms (e.g. points in a point cloud or faces in a mesh) with a
single-pass approach trained in an end-to-end fashion. Such methods achieve
impressive performance but require large amounts of labeled training data. This
paradigm entangles two separable subproblems: (1) decomposing a shape into
regions and (2) assigning semantic labels to these regions. We claim that
disentangling these subproblems reduces the labeled data burden: (1) region
decomposition requires no semantic labels and could be performed in an
unsupervised fashion, and (2) labeling shape regions instead of atoms results
in a smaller search space and should be learnable with less labeled training
data. In this paper, we investigate this second claim by presenting the
Neurally-Guided Shape Parser (NGSP), a method that learns how to assign
semantic labels to regions of an over-segmented 3D shape. We solve this problem
via MAP inference, modeling the posterior probability of a labeling assignment
conditioned on an input shape. We employ a Monte Carlo importance sampling
approach guided by a neural proposal network, a search-based approach made
feasible by assuming the input shape is decomposed into discrete regions. We
evaluate NGSP on the task of hierarchical semantic segmentation on manufactured
3D shapes from PartNet. We find that NGSP delivers significant performance
improvements over baselines that learn to label shape atoms and then aggregate
predictions for each shape region, especially in low-data regimes. Finally, we
demonstrate that NGSP is robust to region granularity, as it maintains strong
segmentation performance even as the regions undergo significant corruption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1"&gt;R. Kenny Jones&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanocka_R/0/1/0/all/0/1"&gt;Rana Hanocka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1"&gt;Daniel Ritchie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LegoFormer: Transformers for Block-by-Block Multi-view 3D Reconstruction. (arXiv:2106.12102v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12102</id>
        <link href="http://arxiv.org/abs/2106.12102"/>
        <updated>2021-06-24T01:51:42.657Z</updated>
        <summary type="html"><![CDATA[Most modern deep learning-based multi-view 3D reconstruction techniques use
RNNs or fusion modules to combine information from multiple images after
encoding them. These two separate steps have loose connections and do not
consider all available information while encoding each view. We propose
LegoFormer, a transformer-based model that unifies object reconstruction under
a single framework and parametrizes the reconstructed occupancy grid by its
decomposition factors. This reformulation allows the prediction of an object as
a set of independent structures then aggregated to obtain the final
reconstruction. Experiments conducted on ShapeNet display the competitive
performance of our network with respect to the state-of-the-art methods. We
also demonstrate how the use of self-attention leads to increased
interpretability of the model output.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yagubbayli_F/0/1/0/all/0/1"&gt;Farid Yagubbayli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tonioni_A/0/1/0/all/0/1"&gt;Alessio Tonioni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1"&gt;Federico Tombari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bootstrap Representation Learning for Segmentation on Medical Volumes and Sequences. (arXiv:2106.12153v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12153</id>
        <link href="http://arxiv.org/abs/2106.12153"/>
        <updated>2021-06-24T01:51:42.649Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a novel straightforward method for medical volume
and sequence segmentation with limited annotations. To avert laborious
annotating, the recent success of self-supervised learning(SSL) motivates the
pre-training on unlabeled data. Despite its success, it is still challenging to
adapt typical SSL methods to volume/sequence segmentation, due to their lack of
mining on local semantic discrimination and rare exploitation on volume and
sequence structures. Based on the continuity between slices/frames and the
common spatial layout of organs across volumes/sequences, we introduced a novel
bootstrap self-supervised representation learning method by leveraging the
predictable possibility of neighboring slices. At the core of our method is a
simple and straightforward dense self-supervision on the predictions of local
representations and a strategy of predicting locals based on global context,
which enables stable and reliable supervision for both global and local
representation mining among volumes. Specifically, we first proposed an
asymmetric network with an attention-guided predictor to enforce
distance-specific prediction and supervision on slices within and across
volumes/sequences. Secondly, we introduced a novel prototype-based
foreground-background calibration module to enhance representation consistency.
The two parts are trained jointly on labeled and unlabeled data. When evaluated
on three benchmark datasets of medical volumes and sequences, our model
outperforms existing methods with a large margin of 4.5\% DSC on ACDC, 1.7\% on
Prostate, and 2.3\% on CAMUS. Intensive evaluations reveals the effectiveness
and superiority of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zejian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuo_W/0/1/0/all/0/1"&gt;Wei Zhuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianfu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1"&gt;Wufeng Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1"&gt;Dong Ni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maria: A Visual Experience Powered Conversational Agent. (arXiv:2105.13073v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13073</id>
        <link href="http://arxiv.org/abs/2105.13073"/>
        <updated>2021-06-24T01:51:42.643Z</updated>
        <summary type="html"><![CDATA[Arguably, the visual perception of conversational agents to the physical
world is a key way for them to exhibit the human-like intelligence.
Image-grounded conversation is thus proposed to address this challenge.
Existing works focus on exploring the multimodal dialog models that ground the
conversation on a given image. In this paper, we take a step further to study
image-grounded conversation under a fully open-ended setting where no paired
dialog and image are assumed available. Specifically, we present Maria, a
neural conversation agent powered by the visual world experiences which are
retrieved from a large-scale image index. Maria consists of three flexible
components, i.e., text-to-image retriever, visual concept detector and
visual-knowledge-grounded response generator. The retriever aims to retrieve a
correlated image to the dialog from an image index, while the visual concept
detector extracts rich visual knowledge from the image. Then, the response
generator is grounded on the extracted visual knowledge and dialog context to
generate the target response. Extensive experiments demonstrate Maria
outperforms previous state-of-the-art methods on automatic metrics and human
evaluation, and can generate informative responses that have some visual
commonsense of the physical world.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1"&gt;Zujie Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Huang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Can Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1"&gt;Chongyang Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1"&gt;Xiubo Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yining Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_F/0/1/0/all/0/1"&gt;Fan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1"&gt;Daxin Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classifying Textual Data with Pre-trained Vision Models through Transfer Learning and Data Transformations. (arXiv:2106.12479v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12479</id>
        <link href="http://arxiv.org/abs/2106.12479"/>
        <updated>2021-06-24T01:51:42.624Z</updated>
        <summary type="html"><![CDATA[Knowledge is acquired by humans through experience, and no boundary is set
between the kinds of knowledge or skill levels we can achieve on different
tasks at the same time. When it comes to Neural Networks, that is not the case,
the major breakthroughs in the field are extremely task and domain specific.
Vision and language are dealt with in separate manners, using separate methods
and different datasets. In this work, we propose to use knowledge acquired by
benchmark Vision Models which are trained on ImageNet to help a much smaller
architecture learn to classify text. After transforming the textual data
contained in the IMDB dataset to gray scale images. An analysis of different
domains and the Transfer Learning method is carried out. Despite the challenge
posed by the very different datasets, promising results are achieved. The main
contribution of this work is a novel approach which links large pretrained
models on both language and vision to achieve state-of-the-art results in
different sub-fields from the original task. Without needing high compute
capacity resources. Specifically, Sentiment Analysis is achieved after
transferring knowledge between vision and language models. BERT embeddings are
transformed into grayscale images, these images are then used as training
examples for pretrained vision models such as VGG16 and ResNet

Index Terms: Natural language, Vision, BERT, Transfer Learning, CNN, Domain
Adaptation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Benarab_C/0/1/0/all/0/1"&gt;Charaf Eddine Benarab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-Based Keyword Localisation in Speech using Visual Grounding. (arXiv:2106.08859v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08859</id>
        <link href="http://arxiv.org/abs/2106.08859"/>
        <updated>2021-06-24T01:51:42.618Z</updated>
        <summary type="html"><![CDATA[Visually grounded speech models learn from images paired with spoken
captions. By tagging images with soft text labels using a trained visual
classifier with a fixed vocabulary, previous work has shown that it is possible
to train a model that can detect whether a particular text keyword occurs in
speech utterances or not. Here we investigate whether visually grounded speech
models can also do keyword localisation: predicting where, within an utterance,
a given textual keyword occurs without any explicit text-based or alignment
supervision. We specifically consider whether incorporating attention into a
convolutional model is beneficial for localisation. Although absolute
localisation performance with visually supervised models is still modest
(compared to using unordered bag-of-word text labels for supervision), we show
that attention provides a large gain in performance over previous visually
grounded models. As in many other speech-image studies, we find that many of
the incorrect localisations are due to semantic confusions, e.g. locating the
word 'backstroke' for the query keyword 'swimming'.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Olaleye_K/0/1/0/all/0/1"&gt;Kayode Olaleye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamper_H/0/1/0/all/0/1"&gt;Herman Kamper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transfer Learning of Deep Spatiotemporal Networks to Model Arbitrarily Long Videos of Seizures. (arXiv:2106.12014v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12014</id>
        <link href="http://arxiv.org/abs/2106.12014"/>
        <updated>2021-06-24T01:51:42.611Z</updated>
        <summary type="html"><![CDATA[Detailed analysis of seizure semiology, the symptoms and signs which occur
during a seizure, is critical for management of epilepsy patients. Inter-rater
reliability using qualitative visual analysis is often poor for semiological
features. Therefore, automatic and quantitative analysis of video-recorded
seizures is needed for objective assessment.

We present GESTURES, a novel architecture combining convolutional neural
networks (CNNs) and recurrent neural networks (RNNs) to learn deep
representations of arbitrarily long videos of epileptic seizures.

We use a spatiotemporal CNN (STCNN) pre-trained on large human action
recognition (HAR) datasets to extract features from short snippets (approx. 0.5
s) sampled from seizure videos. We then train an RNN to learn seizure-level
representations from the sequence of features.

We curated a dataset of seizure videos from 68 patients and evaluated
GESTURES on its ability to classify seizures into focal onset seizures (FOSs)
(N = 106) vs. focal to bilateral tonic-clonic seizures (TCSs) (N = 77),
obtaining an accuracy of 98.9% using bidirectional long short-term memory
(BLSTM) units.

We demonstrate that an STCNN trained on a HAR dataset can be used in
combination with an RNN to accurately represent arbitrarily long videos of
seizures. GESTURES can provide accurate seizure classification by modeling
sequences of semiologies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Garcia_F/0/1/0/all/0/1"&gt;Fernando P&amp;#xe9;rez-Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scott_C/0/1/0/all/0/1"&gt;Catherine Scott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sparks_R/0/1/0/all/0/1"&gt;Rachel Sparks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diehl_B/0/1/0/all/0/1"&gt;Beate Diehl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Ourselin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Listen to Your Favorite Melodies with img2Mxml, Producing MusicXML from Sheet Music Image by Measure-based Multimodal Deep Learning-driven Assembly. (arXiv:2106.12037v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12037</id>
        <link href="http://arxiv.org/abs/2106.12037"/>
        <updated>2021-06-24T01:51:42.604Z</updated>
        <summary type="html"><![CDATA[Deep learning has recently been applied to optical music recognition (OMR).
However, currently OMR processing from various sheet music images still lacks
precision to be widely applicable. Here, we present an MMdA (Measure-based
Multimodal deep learning (DL)-driven Assembly) method allowing for end-to-end
OMR processing from various images including inclined photo images. Using this
method, measures are extracted by a deep learning model, aligned, and resized
to be used for inference of given musical symbol components by using multiple
deep learning models in sequence or in parallel. Use of each standardized
measure enables efficient training of the models and accurate adjustment of
five staff lines in each measure. Multiple musical symbol component category
models with a small number of feature types can represent a diverse set of
notes and other musical symbols including chords. This MMdA method provides a
solution to end-to-end OMR processing with precision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shishido_T/0/1/0/all/0/1"&gt;Tomoyuki Shishido&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fati_F/0/1/0/all/0/1"&gt;Fehmiju Fati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tokushige_D/0/1/0/all/0/1"&gt;Daisuke Tokushige&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ono_Y/0/1/0/all/0/1"&gt;Yasuhiro Ono&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERT-based Multi-Task Model for Country and Province Level Modern Standard Arabic and Dialectal Arabic Identification. (arXiv:2106.12495v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12495</id>
        <link href="http://arxiv.org/abs/2106.12495"/>
        <updated>2021-06-24T01:51:42.599Z</updated>
        <summary type="html"><![CDATA[Dialect and standard language identification are crucial tasks for many
Arabic natural language processing applications. In this paper, we present our
deep learning-based system, submitted to the second NADI shared task for
country-level and province-level identification of Modern Standard Arabic (MSA)
and Dialectal Arabic (DA). The system is based on an end-to-end deep Multi-Task
Learning (MTL) model to tackle both country-level and province-level MSA/DA
identification. The latter MTL model consists of a shared Bidirectional Encoder
Representation Transformers (BERT) encoder, two task-specific attention layers,
and two classifiers. Our key idea is to leverage both the task-discriminative
and the inter-task shared features for country and province MSA/DA
identification. The obtained results show that our MTL model outperforms
single-task models on most subtasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mekki_A/0/1/0/all/0/1"&gt;Abdellah El Mekki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahdaouy_A/0/1/0/all/0/1"&gt;Abdelkader El Mahdaouy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Essefar_K/0/1/0/all/0/1"&gt;Kabil Essefar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mamoun_N/0/1/0/all/0/1"&gt;Nabil El Mamoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berrada_I/0/1/0/all/0/1"&gt;Ismail Berrada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khoumsi_A/0/1/0/all/0/1"&gt;Ahmed Khoumsi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Legal Proceedings Status: Approaches Based on Sequential Text Data. (arXiv:2003.11561v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.11561</id>
        <link href="http://arxiv.org/abs/2003.11561"/>
        <updated>2021-06-24T01:51:42.584Z</updated>
        <summary type="html"><![CDATA[The objective of this paper is to develop predictive models to classify
Brazilian legal proceedings in three possible classes of status: (i) archived
proceedings, (ii) active proceedings, and (iii) suspended proceedings. This
problem's resolution is intended to assist public and private institutions in
managing large portfolios of legal proceedings, providing gains in scale and
efficiency. In this paper, legal proceedings are made up of sequences of short
texts called "motions." We combined several natural language processing (NLP)
and machine learning techniques to solve the problem. Although working with
Portuguese NLP, which can be challenging due to lack of resources, our
approaches performed remarkably well in the classification task, achieving
maximum accuracy of .93 and top average F1 Scores of .89 (macro) and .93
(weighted). Furthermore, we could extract and interpret the patterns learned by
one of our models besides quantifying how those patterns relate to the
classification task. The interpretability step is important among machine
learning legal applications and gives us an exciting insight into how black-box
models make decisions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Polo_F/0/1/0/all/0/1"&gt;Felipe Maia Polo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciochetti_I/0/1/0/all/0/1"&gt;Itamar Ciochetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bertolo_E/0/1/0/all/0/1"&gt;Emerson Bertolo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[P2T: Pyramid Pooling Transformer for Scene Understanding. (arXiv:2106.12011v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12011</id>
        <link href="http://arxiv.org/abs/2106.12011"/>
        <updated>2021-06-24T01:51:42.577Z</updated>
        <summary type="html"><![CDATA[This paper jointly resolves two problems in vision transformer: i) the
computation of Multi-Head Self-Attention (MHSA) has high computational/space
complexity; ii) recent vision transformer networks are overly tuned for image
classification, ignoring the difference between image classification (simple
scenarios, more similar to NLP) and downstream scene understanding tasks
(complicated scenarios, rich structural and contextual information). To this
end, we note that pyramid pooling has been demonstrated to be effective in
various vision tasks owing to its powerful context abstraction, and its natural
property of spatial invariance is suitable to address the loss of structural
information (problem ii)). Hence, we propose to adapt pyramid pooling to MHSA
for alleviating its high requirement on computational resources (problem i)).
In this way, this pooling-based MHSA can well address the above two problems
and is thus flexible and powerful for downstream scene understanding tasks.
Plugged with our pooling-based MHSA, we build a downstream-task-oriented
transformer network, dubbed Pyramid Pooling Transformer (P2T). Extensive
experiments demonstrate that, when applied P2T as the backbone network, it
shows substantial superiority in various downstream scene understanding tasks
such as semantic segmentation, object detection, instance segmentation, and
visual saliency detection, compared to previous CNN- and transformer-based
networks. The code will be released at https://github.com/yuhuan-wu/P2T. Note
that this technical report will keep updating.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yu-Huan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1"&gt;Xin Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1"&gt;Ming-Ming Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LPF: A Language-Prior Feedback Objective Function for De-biased Visual Question Answering. (arXiv:2105.14300v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14300</id>
        <link href="http://arxiv.org/abs/2105.14300"/>
        <updated>2021-06-24T01:51:42.572Z</updated>
        <summary type="html"><![CDATA[Most existing Visual Question Answering (VQA) systems tend to overly rely on
language bias and hence fail to reason from the visual clue. To address this
issue, we propose a novel Language-Prior Feedback (LPF) objective function, to
re-balance the proportion of each answer's loss value in the total VQA loss.
The LPF firstly calculates a modulating factor to determine the language bias
using a question-only branch. Then, the LPF assigns a self-adaptive weight to
each training sample in the training process. With this reweighting mechanism,
the LPF ensures that the total VQA loss can be reshaped to a more balanced
form. By this means, the samples that require certain visual information to
predict will be efficiently used during training. Our method is simple to
implement, model-agnostic, and end-to-end trainable. We conduct extensive
experiments and the results show that the LPF (1) brings a significant
improvement over various VQA models, (2) achieves competitive performance on
the bias-sensitive VQA-CP v2 benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1"&gt;Zujie Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Haifeng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jiaying Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bringing Structure into Summaries: a Faceted Summarization Dataset for Long Scientific Documents. (arXiv:2106.00130v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00130</id>
        <link href="http://arxiv.org/abs/2106.00130"/>
        <updated>2021-06-24T01:51:42.566Z</updated>
        <summary type="html"><![CDATA[Faceted summarization provides briefings of a document from different
perspectives. Readers can quickly comprehend the main points of a long document
with the help of a structured outline. However, little research has been
conducted on this subject, partially due to the lack of large-scale faceted
summarization datasets. In this study, we present FacetSum, a faceted
summarization benchmark built on Emerald journal articles, covering a diverse
range of domains. Different from traditional document-summary pairs, FacetSum
provides multiple summaries, each targeted at specific sections of a long
document, including the purpose, method, findings, and value. Analyses and
empirical results on our dataset reveal the importance of bringing structure
into summaries. We believe FacetSum will spur further advances in summarization
research and foster the development of NLP systems that can leverage the
structured information in both long texts and summaries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_R/0/1/0/all/0/1"&gt;Rui Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thaker_K/0/1/0/all/0/1"&gt;Khushboo Thaker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yue Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1"&gt;Xingdi Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Daqing He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Lexically Constrained Machine Translation for Morphologically Rich Languages. (arXiv:2106.12398v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12398</id>
        <link href="http://arxiv.org/abs/2106.12398"/>
        <updated>2021-06-24T01:51:42.503Z</updated>
        <summary type="html"><![CDATA[Lexically constrained machine translation allows the user to manipulate the
output sentence by enforcing the presence or absence of certain words and
phrases. Although current approaches can enforce terms to appear in the
translation, they often struggle to make the constraint word form agree with
the rest of the generated output. Our manual analysis shows that 46% of the
errors in the output of a baseline constrained model for English to Czech
translation are related to agreement. We investigate mechanisms to allow neural
machine translation to infer the correct word inflection given lemmatized
constraints. In particular, we focus on methods based on training the model
with constraints provided as part of the input sequence. Our experiments on the
English-Czech language pair show that this approach improves the translation of
constrained terms in both automatic and manual evaluation by reducing errors in
agreement. Our approach thus eliminates inflection errors, without introducing
new errors or decreasing the overall quality of the translation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jon_J/0/1/0/all/0/1"&gt;Josef Jon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aires_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Paulo Aires&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varis_D/0/1/0/all/0/1"&gt;Du&amp;#x161;an Vari&amp;#x161;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1"&gt;Ond&amp;#x159;ej Bojar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERT Goes Shopping: Comparing Distributional Models for Product Representations. (arXiv:2012.09807v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09807</id>
        <link href="http://arxiv.org/abs/2012.09807"/>
        <updated>2021-06-24T01:51:42.485Z</updated>
        <summary type="html"><![CDATA[Word embeddings (e.g., word2vec) have been applied successfully to eCommerce
products through~\textit{prod2vec}. Inspired by the recent performance
improvements on several NLP tasks brought by contextualized embeddings, we
propose to transfer BERT-like architectures to eCommerce: our model --
~\textit{Prod2BERT} -- is trained to generate representations of products
through masked session modeling. Through extensive experiments over multiple
shops, different tasks, and a range of design choices, we systematically
compare the accuracy of~\textit{Prod2BERT} and~\textit{prod2vec} embeddings:
while~\textit{Prod2BERT} is found to be superior in several scenarios, we
highlight the importance of resources and hyperparameters in the best
performing models. Finally, we provide guidelines to practitioners for training
embeddings under a variety of computational and data constraints.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1"&gt;Federico Bianchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1"&gt;Bingqing Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tagliabue_J/0/1/0/all/0/1"&gt;Jacopo Tagliabue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TextSETTR: Few-Shot Text Style Extraction and Tunable Targeted Restyling. (arXiv:2010.03802v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03802</id>
        <link href="http://arxiv.org/abs/2010.03802"/>
        <updated>2021-06-24T01:51:42.479Z</updated>
        <summary type="html"><![CDATA[We present a novel approach to the problem of text style transfer. Unlike
previous approaches requiring style-labeled training data, our method makes use
of readily-available unlabeled text by relying on the implicit connection in
style between adjacent sentences, and uses labeled data only at inference time.
We adapt T5 (Raffel et al., 2020), a strong pretrained text-to-text model, to
extract a style vector from text and use it to condition the decoder to perform
style transfer. As our label-free training results in a style vector space
encoding many facets of style, we recast transfers as "targeted restyling"
vector operations that adjust specific attributes of the input while preserving
others. We demonstrate that training on unlabeled Amazon reviews data results
in a model that is competitive on sentiment transfer, even compared to models
trained fully on labeled data. Furthermore, applying our novel method to a
diverse corpus of unlabeled web text results in a single model capable of
transferring along multiple dimensions of style (dialect, emotiveness,
formality, politeness, sentiment) despite no additional training and using only
a handful of exemplars at inference time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Riley_P/0/1/0/all/0/1"&gt;Parker Riley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1"&gt;Noah Constant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Mandy Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1"&gt;Girish Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uthus_D/0/1/0/all/0/1"&gt;David Uthus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parekh_Z/0/1/0/all/0/1"&gt;Zarana Parekh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STEP-EZ: Syntax Tree guided semantic ExPlanation for Explainable Zero-shot modeling of clinical depression symptoms from text. (arXiv:2106.10928v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10928</id>
        <link href="http://arxiv.org/abs/2106.10928"/>
        <updated>2021-06-24T01:51:42.471Z</updated>
        <summary type="html"><![CDATA[We focus on exploring various approaches of Zero-Shot Learning (ZSL) and
their explainability for a challenging yet important supervised learning task
notorious for training data scarcity, i.e. Depression Symptoms Detection (DSD)
from text. We start with a comprehensive synthesis of different components of
our ZSL modeling and analysis of our ground truth samples and Depression
symptom clues curation process with the help of a practicing clinician. We next
analyze the accuracy of various state-of-the-art ZSL models and their potential
enhancements for our task. Further, we sketch a framework for the use of ZSL
for hierarchical text-based explanation mechanism, which we call, Syntax
Tree-Guided Semantic Explanation (STEP). Finally, we summarize experiments from
which we conclude that we can use ZSL models and achieve reasonable accuracy
and explainability, measured by a proposed Explainability Index (EI). This work
is, to our knowledge, the first work to exhaustively explore the efficacy of
ZSL models for DSD task, both in terms of accuracy and explainability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farruque_N/0/1/0/all/0/1"&gt;Nawshad Farruque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1"&gt;Randy Goebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1"&gt;Osmar Zaiane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sivapalan_S/0/1/0/all/0/1"&gt;Sudhakar Sivapalan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dialectal Layers in West Iranian: a Hierarchical Dirichlet Process Approach to Linguistic Relationships. (arXiv:2001.05297v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.05297</id>
        <link href="http://arxiv.org/abs/2001.05297"/>
        <updated>2021-06-24T01:51:42.453Z</updated>
        <summary type="html"><![CDATA[This paper addresses a series of complex and unresolved issues in the
historical phonology of West Iranian languages. The West Iranian languages
(Persian, Kurdish, Balochi, and other languages) display a high degree of
non-Lautgesetzlich behavior. Most of this irregularity is undoubtedly due to
language contact; we argue, however, that an oversimplified view of the
processes at work has prevailed in the literature on West Iranian dialectology,
with specialists assuming that deviations from an expected outcome in a given
non-Persian language are due to lexical borrowing from some chronological stage
of Persian. It is demonstrated that this qualitative approach yields at times
problematic conclusions stemming from the lack of explicit probabilistic
inferences regarding the distribution of the data: Persian may not be the sole
donor language; additionally, borrowing at the lexical level is not always the
mechanism that introduces irregularity. In many cases, the possibility that
West Iranian languages show different reflexes in different conditioning
environments remains under-explored. We employ a novel Bayesian approach
designed to overcome these problems and tease apart the different determinants
of irregularity in patterns of West Iranian sound change. Our methodology
allows us to provisionally resolve a number of outstanding questions in the
literature on West Iranian dialectology concerning the dialectal affiliation of
certain sound changes. We outline future directions for work of this sort.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cathcart_C/0/1/0/all/0/1"&gt;Chundra Aroor Cathcart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Positivity Bias in Negative Reviews. (arXiv:2106.12056v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12056</id>
        <link href="http://arxiv.org/abs/2106.12056"/>
        <updated>2021-06-24T01:51:42.447Z</updated>
        <summary type="html"><![CDATA[Prior work has revealed that positive words occur more frequently than
negative words in human expressions, which is typically attributed to
positivity bias, a tendency for people to report positive views of reality. But
what about the language used in negative reviews? Consistent with prior work,
we show that English negative reviews tend to contain more positive words than
negative words, using a variety of datasets. We reconcile this observation with
prior findings on the pragmatics of negation, and show that negations are
commonly associated with positive words in negative reviews. Furthermore, in
negative reviews, the majority of sentences with positive words express
negative opinions based on sentiment classifiers, indicating some form of
negation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aithal_M/0/1/0/all/0/1"&gt;Madhusudhan Aithal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1"&gt;Chenhao Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[It's All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning. (arXiv:2106.12066v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12066</id>
        <link href="http://arxiv.org/abs/2106.12066"/>
        <updated>2021-06-24T01:51:42.424Z</updated>
        <summary type="html"><![CDATA[Commonsense reasoning is one of the key problems in natural language
processing, but the relative scarcity of labeled data holds back the progress
for languages other than English. Pretrained cross-lingual models are a source
of powerful language-agnostic representations, yet their inherent reasoning
capabilities are still actively studied. In this work, we design a simple
approach to commonsense reasoning which trains a linear classifier with weights
of multi-head attention as features. To evaluate this approach, we create a
multilingual Winograd Schema corpus by processing several datasets from prior
work within a standardized pipeline and measure cross-lingual generalization
ability in terms of out-of-sample performance. The method performs
competitively with recent supervised and unsupervised approaches for
commonsense reasoning, even when applied to other languages in a zero-shot
manner. Also, we demonstrate that most of the performance is given by the same
small subset of attention heads for all studied languages, which provides
evidence of universal reasoning capabilities in multilingual encoders.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1"&gt;Alexey Tikhonov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ryabinin_M/0/1/0/all/0/1"&gt;Max Ryabinin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reducing Spelling Inconsistencies in Code-Switching ASR using Contextualized CTC Loss. (arXiv:2005.07920v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.07920</id>
        <link href="http://arxiv.org/abs/2005.07920"/>
        <updated>2021-06-24T01:51:42.416Z</updated>
        <summary type="html"><![CDATA[Code-Switching (CS) remains a challenge for Automatic Speech Recognition
(ASR), especially character-based models. With the combined choice of
characters from multiple languages, the outcome from character-based models
suffers from phoneme duplication, resulting in language-inconsistent spellings.
We propose Contextualized Connectionist Temporal Classification (CCTC) loss to
encourage spelling consistencies of a character-based non-autoregressive ASR
which allows for faster inference. The CCTC loss conditions the main prediction
on the predicted contexts to ensure language consistency in the spellings. In
contrast to existing CTC-based approaches, CCTC loss does not require
frame-level alignments, since the context ground truth is obtained from the
model's estimated path. Compared to the same model trained with regular CTC
loss, our method consistently improved the ASR performance on both CS and
monolingual corpora.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Naowarat_B/0/1/0/all/0/1"&gt;Burin Naowarat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kongthaworn_T/0/1/0/all/0/1"&gt;Thananchai Kongthaworn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Karunratanakul_K/0/1/0/all/0/1"&gt;Korrawe Karunratanakul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1"&gt;Sheng Hui Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chuangsuwanich_E/0/1/0/all/0/1"&gt;Ekapol Chuangsuwanich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning-based Dialogue Guided Event Extraction to Exploit Argument Relations. (arXiv:2106.12384v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12384</id>
        <link href="http://arxiv.org/abs/2106.12384"/>
        <updated>2021-06-24T01:51:42.409Z</updated>
        <summary type="html"><![CDATA[Event extraction is a fundamental task for natural language processing.
Finding the roles of event arguments like event participants is essential for
event extraction. However, doing so for real-life event descriptions is
challenging because an argument's role often varies in different contexts.
While the relationship and interactions between multiple arguments are useful
for settling the argument roles, such information is largely ignored by
existing approaches. This paper presents a better approach for event extraction
by explicitly utilizing the relationships of event arguments. We achieve this
through a carefully designed task-oriented dialogue system. To model the
argument relation, we employ reinforcement learning and incremental learning to
extract multiple arguments via a multi-turned, iterative process. Our approach
leverages knowledge of the already extracted arguments of the same sentence to
determine the role of arguments that would be difficult to decide individually.
It then uses the newly obtained information to improve the decisions of
previously extracted arguments. This two-way feedback process allows us to
exploit the argument relations to effectively settle argument roles, leading to
better sentence understanding and event extraction. Experimental results show
that our approach consistently outperforms seven state-of-the-art event
extraction methods for the classification of events and argument role and
argument identification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Hao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1"&gt;Yuanxing Ning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lihong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip S. Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zheng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding. (arXiv:2106.12566v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12566</id>
        <link href="http://arxiv.org/abs/2106.12566"/>
        <updated>2021-06-24T01:51:42.400Z</updated>
        <summary type="html"><![CDATA[The attention module, which is a crucial component in Transformer, cannot
scale efficiently to long sequences due to its quadratic complexity. Many works
focus on approximating the dot-then-exponentiate softmax function in the
original attention, leading to sub-quadratic or even linear-complexity
Transformer architectures. However, we show that these methods cannot be
applied to more powerful attention modules that go beyond the
dot-then-exponentiate style, e.g., Transformers with relative positional
encoding (RPE). Since in many state-of-the-art models, relative positional
encoding is used as default, designing efficient Transformers that can
incorporate RPE is appealing. In this paper, we propose a novel way to
accelerate attention calculation for Transformers with RPE on top of the
kernelized attention. Based upon the observation that relative positional
encoding forms a Toeplitz matrix, we mathematically show that kernelized
attention with RPE can be calculated efficiently using Fast Fourier Transform
(FFT). With FFT, our method achieves $\mathcal{O}(n\log n)$ time complexity.
Interestingly, we further demonstrate that properly using relative positional
encoding can mitigate the training instability problem of vanilla kernelized
attention. On a wide range of tasks, we empirically show that our models can be
trained from scratch without any optimization issues. The learned model
performs better than many efficient Transformer variants and is faster than
standard Transformer in the long-sequence regime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1"&gt;Shengjie Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shanda Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1"&gt;Tianle Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Di He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1"&gt;Dinglan Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Shuxin Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ke_G/0/1/0/all/0/1"&gt;Guolin Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Simple and Practical Approach to Improve Misspellings in OCR Text. (arXiv:2106.12030v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12030</id>
        <link href="http://arxiv.org/abs/2106.12030"/>
        <updated>2021-06-24T01:51:42.391Z</updated>
        <summary type="html"><![CDATA[The focus of our paper is the identification and correction of non-word
errors in OCR text. Such errors may be the result of incorrect insertion,
deletion, or substitution of a character, or the transposition of two adjacent
characters within a single word. Or, it can be the result of word boundary
problems that lead to run-on errors and incorrect-split errors. The traditional
N-gram correction methods can handle single-word errors effectively. However,
they show limitations when dealing with split and merge errors. In this paper,
we develop an unsupervised method that can handle both errors. The method we
develop leads to a sizable improvement in the correction rates. This tutorial
paper addresses very difficult word correction problems - namely incorrect
run-on and split errors - and illustrates what needs to be considered when
addressing such problems. We outline a possible approach and assess its success
on a limited study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Junxia Lin&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Ledolter_J/0/1/0/all/0/1"&gt;Johannes Ledolter&lt;/a&gt; (2) ((1) Georgetown University Medical Center, Georgetown University, (2) Tippie College of Business, University of Iowa)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ad Text Classification with Transformer-Based Natural Language Processing Methods. (arXiv:2106.10899v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10899</id>
        <link href="http://arxiv.org/abs/2106.10899"/>
        <updated>2021-06-24T01:51:42.366Z</updated>
        <summary type="html"><![CDATA[In this study, a natural language processing-based (NLP-based) method is
proposed for the sector-wise automatic classification of ad texts created on
online advertising platforms. Our data set consists of approximately 21,000
labeled advertising texts from 12 different sectors. In the study, the
Bidirectional Encoder Representations from Transformers (BERT) model, which is
a transformer-based language model that is recently used in fields such as text
classification in the natural language processing literature, was used. The
classification efficiencies obtained using a pre-trained BERT model for the
Turkish language are shown in detail.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozdil_U/0/1/0/all/0/1"&gt;Umut &amp;#xd6;zdil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arslan_B/0/1/0/all/0/1"&gt;B&amp;#xfc;&amp;#x15f;ra Arslan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tasar_D/0/1/0/all/0/1"&gt;D. Emre Ta&amp;#x15f;ar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polat_G/0/1/0/all/0/1"&gt;G&amp;#xf6;k&amp;#xe7;e Polat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozan_S/0/1/0/all/0/1"&gt;&amp;#x15e;&amp;#xfc;kr&amp;#xfc; Ozan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ABCD: A Graph Framework to Convert Complex Sentences to a Covering Set of Simple Sentences. (arXiv:2106.12027v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12027</id>
        <link href="http://arxiv.org/abs/2106.12027"/>
        <updated>2021-06-24T01:51:42.350Z</updated>
        <summary type="html"><![CDATA[Atomic clauses are fundamental text units for understanding complex
sentences. Identifying the atomic sentences within complex sentences is
important for applications such as summarization, argument mining, discourse
analysis, discourse parsing, and question answering. Previous work mainly
relies on rule-based methods dependent on parsing. We propose a new task to
decompose each complex sentence into simple sentences derived from the tensed
clauses in the source, and a novel problem formulation as a graph edit task.
Our neural model learns to Accept, Break, Copy or Drop elements of a graph that
combines word adjacency and grammatical dependencies. The full processing
pipeline includes modules for graph construction, graph editing, and sentence
generation from the output graph. We introduce DeSSE, a new dataset designed to
train and evaluate complex sentence decomposition, and MinWiki, a subset of
MinWikiSplit. ABCD achieves comparable performance as two parsing baselines on
MinWiki. On DeSSE, which has a more even balance of complex sentence types, our
model achieves higher accuracy on the number of atomic sentences than an
encoder-decoder baseline. Results include a detailed error analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yanjun Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ting-hao/0/1/0/all/0/1"&gt;Ting-hao&lt;/a&gt; (Kenneth) &lt;a href="http://arxiv.org/find/cs/1/au:+Huang/0/1/0/all/0/1"&gt;Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Passonneau_R/0/1/0/all/0/1"&gt;Rebecca J. Passonneau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recognising Biomedical Names: Challenges and Solutions. (arXiv:2106.12230v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12230</id>
        <link href="http://arxiv.org/abs/2106.12230"/>
        <updated>2021-06-24T01:51:42.344Z</updated>
        <summary type="html"><![CDATA[The growth rate in the amount of biomedical documents is staggering.
Unlocking information trapped in these documents can enable researchers and
practitioners to operate confidently in the information world. Biomedical NER,
the task of recognising biomedical names, is usually employed as the first step
of the NLP pipeline. Standard NER models, based on sequence tagging technique,
are good at recognising short entity mentions in the generic domain. However,
there are several open challenges of applying these models to recognise
biomedical names: 1) Biomedical names may contain complex inner structure
(discontinuity and overlapping) which cannot be recognised using standard
sequence tagging technique; 2) The training of NER models usually requires
large amount of labelled data, which are difficult to obtain in the biomedical
domain; and, 3) Commonly used language representation models are pre-trained on
generic data; a domain shift therefore exists between these models and target
biomedical data. To deal with these challenges, we explore several research
directions and make the following contributions: 1) we propose a
transition-based NER model which can recognise discontinuous mentions; 2) We
develop a cost-effective approach that nominates the suitable pre-training
data; and, 3) We design several data augmentation methods for NER. Our
contributions have obvious practical implications, especially when new
biomedical applications are needed. Our proposed data augmentation methods can
help the NER model achieve decent performance, requiring only a small amount of
labelled data. Our investigation regarding selecting pre-training data can
improve the model by incorporating language representation models, which are
pre-trained using in-domain data. Finally, our proposed transition-based NER
model can further improve the performance by recognising discontinuous
mentions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1"&gt;Xiang Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PO-EMO: Conceptualization, Annotation, and Modeling of Aesthetic Emotions in German and English Poetry. (arXiv:2003.07723v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.07723</id>
        <link href="http://arxiv.org/abs/2003.07723"/>
        <updated>2021-06-24T01:51:42.337Z</updated>
        <summary type="html"><![CDATA[Most approaches to emotion analysis of social media, literature, news, and
other domains focus exclusively on basic emotion categories as defined by Ekman
or Plutchik. However, art (such as literature) enables engagement in a broader
range of more complex and subtle emotions. These have been shown to also
include mixed emotional responses. We consider emotions in poetry as they are
elicited in the reader, rather than what is expressed in the text or intended
by the author. Thus, we conceptualize a set of aesthetic emotions that are
predictive of aesthetic appreciation in the reader, and allow the annotation of
multiple labels per line to capture mixed emotions within their context. We
evaluate this novel setting in an annotation experiment both with carefully
trained experts and via crowdsourcing. Our annotation with experts leads to an
acceptable agreement of kappa = .70, resulting in a consistent dataset for
future large scale analysis. Finally, we conduct first emotion classification
experiments based on BERT, showing that identifying aesthetic emotions is
challenging in our data, with up to .52 F1-micro on the German subset. Data and
resources are available at https://github.com/tnhaider/poetry-emotion]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haider_T/0/1/0/all/0/1"&gt;Thomas Haider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1"&gt;Steffen Eger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1"&gt;Evgeny Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1"&gt;Roman Klinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menninghaus_W/0/1/0/all/0/1"&gt;Winfried Menninghaus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Diversity and Limits of Human Explanations. (arXiv:2106.11988v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11988</id>
        <link href="http://arxiv.org/abs/2106.11988"/>
        <updated>2021-06-24T01:51:42.329Z</updated>
        <summary type="html"><![CDATA[A growing effort in NLP aims to build datasets of human explanations.
However, the term explanation encompasses a broad range of notions, each with
different properties and ramifications. Our goal is to provide an overview of
diverse types of explanations and human limitations, and discuss implications
for collecting and using explanations in NLP. Inspired by prior work in
psychology and cognitive sciences, we group existing human explanations in NLP
into three categories: proximal mechanism, evidence, and procedure. These three
types differ in nature and have implications for the resultant explanations.
For instance, procedure is not considered explanations in psychology and
connects with a rich body of work on learning from instructions. The diversity
of explanations is further evidenced by proxy questions that are needed for
annotators to interpret and answer open-ended why questions. Finally,
explanations may require different, often deeper, understandings than
predictions, which casts doubt on whether humans can provide useful
explanations in some tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1"&gt;Chenhao Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BanditMF: Multi-Armed Bandit Based Matrix Factorization Recommender System. (arXiv:2106.10898v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10898</id>
        <link href="http://arxiv.org/abs/2106.10898"/>
        <updated>2021-06-24T01:51:42.312Z</updated>
        <summary type="html"><![CDATA[Multi-armed bandits (MAB) provide a principled online learning approach to
attain the balance between exploration and exploitation. Due to the superior
performance and low feedback learning without the learning to act in multiple
situations, Multi-armed Bandits drawing widespread attention in applications
ranging such as recommender systems. Likewise, within the recommender system,
collaborative filtering (CF) is arguably the earliest and most influential
method in the recommender system. Crucially, new users and an ever-changing
pool of recommended items are the challenges that recommender systems need to
address. For collaborative filtering, the classical method is training the
model offline, then perform the online testing, but this approach can no longer
handle the dynamic changes in user preferences which is the so-called cold
start. So how to effectively recommend items to users in the absence of
effective information? To address the aforementioned problems, a multi-armed
bandit based collaborative filtering recommender system has been proposed,
named BanditMF. BanditMF is designed to address two challenges in the
multi-armed bandits algorithm and collaborative filtering: (1) how to solve the
cold start problem for collaborative filtering under the condition of scarcity
of valid information, (2) how to solve the sub-optimal problem of bandit
algorithms in strong social relations domains caused by independently
estimating unknown parameters associated with each user and ignoring
correlations between users.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shenghao Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Shot Joint Modeling of Multiple Spoken-Text-Style Conversion Tasks using Switching Tokens. (arXiv:2106.12131v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12131</id>
        <link href="http://arxiv.org/abs/2106.12131"/>
        <updated>2021-06-24T01:51:42.304Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel spoken-text-style conversion method that
can simultaneously execute multiple style conversion modules such as
punctuation restoration and disfluency deletion without preparing matched
datasets. In practice, transcriptions generated by automatic speech recognition
systems are not highly readable because they often include many disfluencies
and do not include punctuation marks. To improve their readability, multiple
spoken-text-style conversion modules that individually model a single
conversion task are cascaded because matched datasets that simultaneously
handle multiple conversion tasks are often unavailable. However, the cascading
is unstable against the order of tasks because of the chain of conversion
errors. Besides, the computation cost of the cascading must be higher than the
single conversion. To execute multiple conversion tasks simultaneously without
preparing matched datasets, our key idea is to distinguish individual
conversion tasks using the on-off switch. In our proposed zero-shot joint
modeling, we switch the individual tasks using multiple switching tokens,
enabling us to utilize a zero-shot learning approach to executing simultaneous
conversions. Our experiments on joint modeling of disfluency deletion and
punctuation restoration demonstrate the effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ihori_M/0/1/0/all/0/1"&gt;Mana Ihori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makishima_N/0/1/0/all/0/1"&gt;Naoki Makishima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1"&gt;Tomohiro Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takashima_A/0/1/0/all/0/1"&gt;Akihiko Takashima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orihashi_S/0/1/0/all/0/1"&gt;Shota Orihashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masumura_R/0/1/0/all/0/1"&gt;Ryo Masumura&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Energy-Based Models for End-to-End Speech Recognition. (arXiv:2103.14152v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14152</id>
        <link href="http://arxiv.org/abs/2103.14152"/>
        <updated>2021-06-24T01:51:42.294Z</updated>
        <summary type="html"><![CDATA[End-to-end models with auto-regressive decoders have shown impressive results
for automatic speech recognition (ASR). These models formulate the
sequence-level probability as a product of the conditional probabilities of all
individual tokens given their histories. However, the performance of locally
normalised models can be sub-optimal because of factors such as exposure bias.
Consequently, the model distribution differs from the underlying data
distribution. In this paper, the residual energy-based model (R-EBM) is
proposed to complement the auto-regressive ASR model to close the gap between
the two distributions. Meanwhile, R-EBMs can also be regarded as
utterance-level confidence estimators, which may benefit many downstream tasks.
Experiments on a 100hr LibriSpeech dataset show that R-EBMs can reduce the word
error rates (WERs) by 8.2%/6.7% while improving areas under precision-recall
curves of confidence scores by 12.6%/28.4% on test-clean/test-other sets.
Furthermore, on a state-of-the-art model using self-supervised learning
(wav2vec 2.0), R-EBMs still significantly improves both the WER and confidence
estimation performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qiujia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cao_L/0/1/0/all/0/1"&gt;Liangliang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Woodland_P/0/1/0/all/0/1"&gt;Philip C. Woodland&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STEP-EZ: Syntax Tree guided semantic ExPlanation for Explainable Zero-shot modeling of clinical depression symptoms from text. (arXiv:2106.10928v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10928</id>
        <link href="http://arxiv.org/abs/2106.10928"/>
        <updated>2021-06-24T01:51:42.283Z</updated>
        <summary type="html"><![CDATA[We focus on exploring various approaches of Zero-Shot Learning (ZSL) and
their explainability for a challenging yet important supervised learning task
notorious for training data scarcity, i.e. Depression Symptoms Detection (DSD)
from text. We start with a comprehensive synthesis of different components of
our ZSL modeling and analysis of our ground truth samples and Depression
symptom clues curation process with the help of a practicing clinician. We next
analyze the accuracy of various state-of-the-art ZSL models and their potential
enhancements for our task. Further, we sketch a framework for the use of ZSL
for hierarchical text-based explanation mechanism, which we call, Syntax
Tree-Guided Semantic Explanation (STEP). Finally, we summarize experiments from
which we conclude that we can use ZSL models and achieve reasonable accuracy
and explainability, measured by a proposed Explainability Index (EI). This work
is, to our knowledge, the first work to exhaustively explore the efficacy of
ZSL models for DSD task, both in terms of accuracy and explainability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farruque_N/0/1/0/all/0/1"&gt;Nawshad Farruque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1"&gt;Randy Goebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1"&gt;Osmar Zaiane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sivapalan_S/0/1/0/all/0/1"&gt;Sudhakar Sivapalan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Transformer-based Sequential Recommenders through Preference Editing. (arXiv:2106.12120v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.12120</id>
        <link href="http://arxiv.org/abs/2106.12120"/>
        <updated>2021-06-24T01:51:42.106Z</updated>
        <summary type="html"><![CDATA[One of the key challenges in Sequential Recommendation (SR) is how to extract
and represent user preferences. Traditional SR methods rely on the next item as
the supervision signal to guide preference extraction and representation. We
propose a novel learning strategy, named preference editing. The idea is to
force the SR model to discriminate the common and unique preferences in
different sequences of interactions between users and the recommender system.
By doing so, the SR model is able to learn how to identify common and unique
user preferences, and thereby do better user preference extraction and
representation. We propose a transformer based SR model, named MrTransformer
(Multi-preference Transformer), that concatenates some special tokens in front
of the sequence to represent multiple user preferences and makes sure they
capture different aspects through a preference coverage mechanism. Then, we
devise a preference editing-based self-supervised learning mechanism for
training MrTransformer which contains two main operations: preference
separation and preference recombination. The former separates the common and
unique user preferences for a given pair of sequences. The latter swaps the
common preferences to obtain recombined user preferences for each sequence.
Based on the preference separation and preference recombination operations, we
define two types of SSL loss that require that the recombined preferences are
similar to the original ones, and the common preferences are close to each
other.

We carry out extensive experiments on two benchmark datasets. MrTransformer
with preference editing significantly outperforms state-of-the-art SR methods
in terms of Recall, MRR and NDCG. We find that long sequences whose user
preferences are harder to extract and represent benefit most from preference
editing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1"&gt;Muyang Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1"&gt;Pengjie Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhumin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1"&gt;Zhaochun Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1"&gt;Huasheng Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jun Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1"&gt;Maarten de Rijke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Graph-based Method for Session-based Recommendations. (arXiv:2106.12085v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.12085</id>
        <link href="http://arxiv.org/abs/2106.12085"/>
        <updated>2021-06-24T01:51:42.093Z</updated>
        <summary type="html"><![CDATA[We present a graph-based approach for the data management tasks and the
efficient operation of a system for session-based next-item recommendations.
The proposed method can collect data continuously and incrementally from an
ecommerce web site, thus seemingly prepare the necessary data infrastructure
for the recommendation algorithm to operate without any excessive training
phase. Our work aims at developing a recommender method that represents a
balance between data processing and management efficiency requirements and the
effectiveness of the recommendations produced. We use the Neo4j graph database
to implement a prototype of such a system. Furthermore, we use an industry
dataset corresponding to a typical e-commerce session-based scenario, and we
report on experiments using our graph-based approach and other state-of-the-art
machine learning and deep learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Delianidi_M/0/1/0/all/0/1"&gt;Marina Delianidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salampasis_M/0/1/0/all/0/1"&gt;Michail Salampasis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diamantaras_K/0/1/0/all/0/1"&gt;Konstantinos Diamantaras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siomos_T/0/1/0/all/0/1"&gt;Theodosios Siomos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katsalis_A/0/1/0/all/0/1"&gt;Alkiviadis Katsalis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karaveli_I/0/1/0/all/0/1"&gt;Iphigenia Karaveli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diversity-Robust Acoustic Feature Signatures Based on Multiscale Fractal Dimension for Similarity Search of Environmental Sounds. (arXiv:2102.02964v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02964</id>
        <link href="http://arxiv.org/abs/2102.02964"/>
        <updated>2021-06-24T01:51:42.072Z</updated>
        <summary type="html"><![CDATA[This paper proposes new acoustic feature signatures based on the multiscale
fractal dimension (MFD), which are robust against the diversity of
environmental sounds, for the content-based similarity search. The diversity of
sound sources and acoustic compositions is a typical feature of environmental
sounds. Several acoustic features have been proposed for environmental sounds.
Among them is the widely-used Mel-Frequency Cepstral Coefficients (MFCCs),
which describes frequency-domain features. However, in addition to these
features in the frequency domain, environmental sounds have other important
features in the time domain with various time scales. In our previous paper, we
proposed enhanced multiscale fractal dimension signature (EMFD) for
environmental sounds. This paper extends EMFD by using the kernel density
estimation method, which results in better performance of the similarity search
tasks. Furthermore, it newly proposes another acoustic feature signature based
on MFD, namely very-long-range multiscale fractal dimension signature (MFD-VL).
The MFD-VL signature describes several features of the time-varying envelope
for long periods of time. The MFD-VL signature has stability and robustness
against background noise and small fluctuations in the parameters of sound
sources, which are produced in field recordings. We discuss the effectiveness
of these signatures in the similarity sound search by comparing with acoustic
features proposed in the DCASE 2018 challenges. Due to the unique
descriptiveness of our proposed signatures, we confirmed the signatures are
effective when they are used with other acoustic features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sunouchi_M/0/1/0/all/0/1"&gt;Motohiro Sunouchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoshioka_M/0/1/0/all/0/1"&gt;Masaharu Yoshioka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Approach to Detect Redundant Activity Labels For More Representative Event Logs. (arXiv:2103.16061v2 [cs.DB] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16061</id>
        <link href="http://arxiv.org/abs/2103.16061"/>
        <updated>2021-06-24T01:51:42.012Z</updated>
        <summary type="html"><![CDATA[The insights revealed from process mining heavily rely on the quality of
event logs. Activities extracted from healthcare information systems with the
free-text nature may lead to inconsistent labels. Such inconsistency would then
lead to redundancy of activity labels, which refer to labels that have
different syntax but share the same behaviours. The identifications of these
labels from data-driven process discovery are difficult and rely heavily on
resource-intensive human review. Existing work achieves low accuracy either
redundant activity labels are in low occurrence frequency or the existence of
numerical data values as attributes in event logs. However, these phenomena are
commonly observed in healthcare information systems. In this paper, we propose
an approach to detect redundant activity labels using control-flow relations
and numerical data values from event logs. Natural Language Processing is also
integrated into our method to assess semantic similarity between labels, which
provides users with additional insights. We have evaluated our approach through
synthetic logs generated from the real-life Sepsis log and a case study using
the MIMIC-III data set. The results demonstrate that our approach can
successfully detect redundant activity labels. This approach can add value to
the preprocessing step to generate more representative event logs for process
mining tasks in the healthcare domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qifan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tam_C/0/1/0/all/0/1"&gt;Charmaine Tam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poon_S/0/1/0/all/0/1"&gt;Simon Poon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERT Goes Shopping: Comparing Distributional Models for Product Representations. (arXiv:2012.09807v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09807</id>
        <link href="http://arxiv.org/abs/2012.09807"/>
        <updated>2021-06-24T01:51:41.975Z</updated>
        <summary type="html"><![CDATA[Word embeddings (e.g., word2vec) have been applied successfully to eCommerce
products through~\textit{prod2vec}. Inspired by the recent performance
improvements on several NLP tasks brought by contextualized embeddings, we
propose to transfer BERT-like architectures to eCommerce: our model --
~\textit{Prod2BERT} -- is trained to generate representations of products
through masked session modeling. Through extensive experiments over multiple
shops, different tasks, and a range of design choices, we systematically
compare the accuracy of~\textit{Prod2BERT} and~\textit{prod2vec} embeddings:
while~\textit{Prod2BERT} is found to be superior in several scenarios, we
highlight the importance of resources and hyperparameters in the best
performing models. Finally, we provide guidelines to practitioners for training
embeddings under a variety of computational and data constraints.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1"&gt;Federico Bianchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1"&gt;Bingqing Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tagliabue_J/0/1/0/all/0/1"&gt;Jacopo Tagliabue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraphConfRec: A Graph Neural Network-Based Conference Recommender System. (arXiv:2106.12340v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.12340</id>
        <link href="http://arxiv.org/abs/2106.12340"/>
        <updated>2021-06-24T01:51:41.960Z</updated>
        <summary type="html"><![CDATA[In today's academic publishing model, especially in Computer Science,
conferences commonly constitute the main platforms for releasing the latest
peer-reviewed advancements in their respective fields. However, choosing a
suitable academic venue for publishing one's research can represent a
challenging task considering the plethora of available conferences,
particularly for those at the start of their academic careers, or for those
seeking to publish outside of their usual domain. In this paper, we propose
GraphConfRec, a conference recommender system which combines SciGraph and graph
neural networks, to infer suggestions based not only on title and abstract, but
also on co-authorship and citation relationships. GraphConfRec achieves a
recall@10 of up to 0.580 and a MAP of up to 0.336 with a graph attention
network-based recommendation model. A user study with 25 subjects supports the
positive results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Iana_A/0/1/0/all/0/1"&gt;Andreea Iana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1"&gt;Heiko Paulheim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BiblioDAP: The 1st Workshop on Bibliographic Data Analysis and Processing. (arXiv:2106.12320v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2106.12320</id>
        <link href="http://arxiv.org/abs/2106.12320"/>
        <updated>2021-06-24T01:51:41.930Z</updated>
        <summary type="html"><![CDATA[Automatic processing of bibliographic data becomes very important in digital
libraries, data science and machine learning due to its importance in keeping
pace with the significant increase of published papers every year from one side
and to the inherent challenges from the other side. This processing has several
aspects including but not limited to I) Automatic extraction of references from
PDF documents, II) Building an accurate citation graph, III) Author name
disambiguation, etc. Bibliographic data is heterogeneous by nature and occurs
in both structured (e.g. citation graph) and unstructured (e.g. publications)
formats. Therefore, it requires data science and machine learning techniques to
be processed and analysed. Here we introduce BiblioDAP'21: The 1st Workshop on
Bibliographic Data Analysis and Processing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boukhers_Z/0/1/0/all/0/1"&gt;Zeyd Boukhers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mayr_P/0/1/0/all/0/1"&gt;Philipp Mayr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peroni_S/0/1/0/all/0/1"&gt;Silvio Peroni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learnt Sparsity for Effective and Interpretable Document Ranking. (arXiv:2106.12460v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.12460</id>
        <link href="http://arxiv.org/abs/2106.12460"/>
        <updated>2021-06-24T01:51:41.910Z</updated>
        <summary type="html"><![CDATA[Machine learning models for the ad-hoc retrieval of documents and passages
have recently shown impressive improvements due to better language
understanding using large pre-trained language models. However, these
over-parameterized models are inherently non-interpretable and do not provide
any information on the parts of the documents that were used to arrive at a
certain prediction.

In this paper we introduce the select and rank paradigm for document ranking,
where interpretability is explicitly ensured when scoring longer documents.
Specifically, we first select sentences in a document based on the input query
and then predict the query-document score based only on the selected sentences,
acting as an explanation. We treat sentence selection as a latent variable
trained jointly with the ranker from the final output. We conduct extensive
experiments to demonstrate that our inherently interpretable select-and-rank
approach is competitive in comparison to other state-of-the-art methods and
sometimes even outperforms them. This is due to our novel end-to-end training
approach based on weighted reservoir sampling that manages to train the
selector despite the stochastic sentence selection. We also show that our
sentence selection approach can be used to provide explanations for models that
operate on only parts of the document, such as BERT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leonhardt_J/0/1/0/all/0/1"&gt;Jurek Leonhardt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudra_K/0/1/0/all/0/1"&gt;Koustav Rudra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1"&gt;Avishek Anand&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Neural Network Based Respiratory Pathology Classification Using Cough Sounds. (arXiv:2106.12174v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12174</id>
        <link href="http://arxiv.org/abs/2106.12174"/>
        <updated>2021-06-24T01:51:41.860Z</updated>
        <summary type="html"><![CDATA[Intelligent systems are transforming the world, as well as our healthcare
system. We propose a deep learning-based cough sound classification model that
can distinguish between children with healthy versus pathological coughs such
as asthma, upper respiratory tract infection (URTI), and lower respiratory
tract infection (LRTI). In order to train a deep neural network model, we
collected a new dataset of cough sounds, labelled with clinician's diagnosis.
The chosen model is a bidirectional long-short term memory network (BiLSTM)
based on Mel Frequency Cepstral Coefficients (MFCCs) features. The resulting
trained model when trained for classifying two classes of coughs -- healthy or
pathology (in general or belonging to a specific respiratory pathology),
reaches accuracy exceeding 84\% when classifying cough to the label provided by
the physicians' diagnosis. In order to classify subject's respiratory pathology
condition, results of multiple cough epochs per subject were combined. The
resulting prediction accuracy exceeds 91\% for all three respiratory
pathologies. However, when the model is trained to classify and discriminate
among the four classes of coughs, overall accuracy dropped: one class of
pathological coughs are often misclassified as other. However, if one consider
the healthy cough classified as healthy and pathological cough classified to
have some kind of pathologies, then the overall accuracy of four class model is
above 84\%. A longitudinal study of MFCC feature space when comparing
pathological and recovered coughs collected from the same subjects revealed the
fact that pathological cough irrespective of the underlying conditions occupy
the same feature space making it harder to differentiate only using MFCC
features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+T_B/0/1/0/all/0/1"&gt;Balamurali B T&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hee_H/0/1/0/all/0/1"&gt;Hwan Ing Hee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kapoor_S/0/1/0/all/0/1"&gt;Saumitra Kapoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teoh_O/0/1/0/all/0/1"&gt;Oon Hoe Teoh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teng_S/0/1/0/all/0/1"&gt;Sung Shin Teng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Khai Pin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herremans_D/0/1/0/all/0/1"&gt;Dorien Herremans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jer Ming Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rank-one matrix estimation with groupwise heteroskedasticity. (arXiv:2106.11950v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.11950</id>
        <link href="http://arxiv.org/abs/2106.11950"/>
        <updated>2021-06-23T01:48:42.476Z</updated>
        <summary type="html"><![CDATA[We study the problem of estimating a rank-one matrix from Gaussian
observations where different blocks of the matrix are observed under different
noise levels. This problem is motivated by applications in clustering and
community detection where latent variables can be partitioned into a fixed
number of known groups (e.g., users and items) and the blocks of the matrix
correspond to different types of pairwise interactions (e.g., user-user,
user-item, or item-item interactions). In the setting where the number of
blocks is fixed while the number of variables tends to infinity, we prove
asymptotically exact formulas for the minimum mean-squared error in estimating
both the matrix and the latent variables. These formulas describe the weak
recovery thresholds for the problem and reveal invariance properties with
respect to certain scalings of the noise variance. We also derive an
approximate message passing algorithm and a gradient descent algorithm and show
empirically that these algorithms achieve the information-theoretic limits in
certain regimes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Behne_J/0/1/0/all/0/1"&gt;Joshua K. Behne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Reeves_G/0/1/0/all/0/1"&gt;Galen Reeves&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis of Optimization Algorithms via Sum-of-Squares. (arXiv:1906.04648v4 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.04648</id>
        <link href="http://arxiv.org/abs/1906.04648"/>
        <updated>2021-06-23T01:48:42.470Z</updated>
        <summary type="html"><![CDATA[We introduce a new framework for unifying and systematizing the performance
analysis of first-order black-box optimization algorithms for unconstrained
convex minimization. The low-cost iteration complexity enjoyed by first-order
algorithms renders them particularly relevant for applications in machine
learning and large-scale data analysis. Relying on sum-of-squares (SOS)
optimization, we introduce a hierarchy of semidefinite programs that give
increasingly better convergence bounds for higher levels of the hierarchy.
Alluding to the power of the SOS hierarchy, we show that the (dual of the)
first level corresponds to the Performance Estimation Problem (PEP) introduced
by Drori and Teboulle [Math. Program., 145(1):451--482, 2014], a powerful
framework for determining convergence rates of first-order optimization
algorithms. Consequently, many results obtained within the PEP framework can be
reinterpreted as degree-1 SOS proofs, and thus, the SOS framework provides a
promising new approach for certifying improved rates of convergence by means of
higher-order SOS certificates. To determine analytical rate bounds, in this
work we use the first level of the SOS hierarchy and derive new result{s} for
noisy gradient descent with inexact line search methods (Armijo, Wolfe, and
Goldstein).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Tan_S/0/1/0/all/0/1"&gt;Sandra S. Y. Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Varvitsiotis_A/0/1/0/all/0/1"&gt;Antonios Varvitsiotis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Tan_V/0/1/0/all/0/1"&gt;Vincent Y. F. Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aligned Contrastive Predictive Coding. (arXiv:2104.11946v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11946</id>
        <link href="http://arxiv.org/abs/2104.11946"/>
        <updated>2021-06-23T01:48:42.386Z</updated>
        <summary type="html"><![CDATA[We investigate the possibility of forcing a self-supervised model trained
using a contrastive predictive loss to extract slowly varying latent
representations. Rather than producing individual predictions for each of the
future representations, the model emits a sequence of predictions shorter than
that of the upcoming representations to which they will be aligned. In this
way, the prediction network solves a simpler task of predicting the next
symbols, but not their exact timing, while the encoding network is trained to
produce piece-wise constant latent codes. We evaluate the model on a speech
coding task and demonstrate that the proposed Aligned Contrastive Predictive
Coding (ACPC) leads to higher linear phone prediction accuracy and lower ABX
error rates, while being slightly faster to train due to the reduced number of
prediction heads.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chorowski_J/0/1/0/all/0/1"&gt;Jan Chorowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciesielski_G/0/1/0/all/0/1"&gt;Grzegorz Ciesielski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dzikowski_J/0/1/0/all/0/1"&gt;Jaros&amp;#x142;aw Dzikowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lancucki_A/0/1/0/all/0/1"&gt;Adrian &amp;#x141;a&amp;#x144;cucki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marxer_R/0/1/0/all/0/1"&gt;Ricard Marxer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Opala_M/0/1/0/all/0/1"&gt;Mateusz Opala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pusz_P/0/1/0/all/0/1"&gt;Piotr Pusz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rychlikowski_P/0/1/0/all/0/1"&gt;Pawe&amp;#x142; Rychlikowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stypulkowski_M/0/1/0/all/0/1"&gt;Micha&amp;#x142; Stypu&amp;#x142;kowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum Optimisation of Complex Systems with a Quantum Annealer. (arXiv:2105.13945v3 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13945</id>
        <link href="http://arxiv.org/abs/2105.13945"/>
        <updated>2021-06-23T01:48:42.369Z</updated>
        <summary type="html"><![CDATA[We perform an in-depth comparison of quantum annealing with several classical
optimisation techniques, namely thermal annealing, Nelder-Mead, and gradient
descent. We begin with a direct study of the 2D Ising model on a quantum
annealer, and compare its properties directly with those of the thermal 2D
Ising model. These properties include an Ising-like phase transition that can
be induced by either a change in 'quantum-ness' of the theory, or by a scaling
the Ising couplings up or down. This behaviour is in accord with what is
expected from the physical understanding of the quantum system. We then go on
to demonstrate the efficacy of the quantum annealer at minimising several
increasingly hard two dimensional potentials. For all the potentials we find
the general behaviour that Nelder-Mead and gradient descent methods are very
susceptible to becoming trapped in false minima, while the thermal anneal
method is somewhat better at discovering the true minimum. However, and despite
current limitations on its size, the quantum annealer performs a minimisation
very markedly better than any of these classical techniques. A quantum anneal
can be designed so that the system almost never gets trapped in a false
minimum, and rapidly and successfully minimises the potentials.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Abel_S/0/1/0/all/0/1"&gt;Steve Abel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Blance_A/0/1/0/all/0/1"&gt;Andrew Blance&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Spannowsky_M/0/1/0/all/0/1"&gt;Michael Spannowsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Copyright in Generative Deep Learning. (arXiv:2105.09266v2 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09266</id>
        <link href="http://arxiv.org/abs/2105.09266"/>
        <updated>2021-06-23T01:48:42.362Z</updated>
        <summary type="html"><![CDATA[Machine-generated artworks are now part of the contemporary art scene: they
are attracting significant investments and they are presented in exhibitions
together with those created by human artists. These artworks are mainly based
on generative deep learning techniques. Also given their success, several legal
problems arise when working with these techniques.

In this article we consider a set of key questions in the area of generative
deep learning for the arts. Is it possible to use copyrighted works as training
set for generative models? How do we legally store their copies in order to
perform the training process? And then, who (if someone) will own the copyright
on the generated data? We try to answer these questions considering the law in
force in both US and EU and the future alternatives, trying to define a set of
guidelines for artists and developers working on deep learning generated art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Franceschelli_G/0/1/0/all/0/1"&gt;Giorgio Franceschelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musolesi_M/0/1/0/all/0/1"&gt;Mirco Musolesi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhance Multimodal Model Performance with Data Augmentation: Facebook Hateful Meme Challenge Solution. (arXiv:2105.13132v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13132</id>
        <link href="http://arxiv.org/abs/2105.13132"/>
        <updated>2021-06-23T01:48:42.356Z</updated>
        <summary type="html"><![CDATA[Hateful content detection is one of the areas where deep learning can and
should make a significant difference. The Hateful Memes Challenge from Facebook
helps fulfill such potential by challenging the contestants to detect hateful
speech in multi-modal memes using deep learning algorithms. In this paper, we
utilize multi-modal, pre-trained models VilBERT and Visual BERT. We improved
models' performance by adding training datasets generated from data
augmentation. Enlarging the training data set helped us get a more than 2%
boost in terms of AUROC with the Visual BERT model. Our approach achieved
0.7439 AUROC along with an accuracy of 0.7037 on the challenge's test set,
which revealed remarkable progress.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zinc Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hutchin Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer-based Spatial-Temporal Feature Learning for EEG Decoding. (arXiv:2106.11170v1 [eess.SP] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2106.11170</id>
        <link href="http://arxiv.org/abs/2106.11170"/>
        <updated>2021-06-23T01:48:42.350Z</updated>
        <summary type="html"><![CDATA[At present, people usually use some methods based on convolutional neural
networks (CNNs) for Electroencephalograph (EEG) decoding. However, CNNs have
limitations in perceiving global dependencies, which is not adequate for common
EEG paradigms with a strong overall relationship. Regarding this issue, we
propose a novel EEG decoding method that mainly relies on the attention
mechanism. The EEG data is firstly preprocessed and spatially filtered. And
then, we apply attention transforming on the feature-channel dimension so that
the model can enhance more relevant spatial features. The most crucial step is
to slice the data in the time dimension for attention transforming, and finally
obtain a highly distinguishable representation. At this time, global averaging
pooling and a simple fully-connected layer are used to classify different
categories of EEG data. Experiments on two public datasets indicate that the
strategy of attention transforming effectively utilizes spatial and temporal
features. And we have reached the level of the state-of-the-art in
multi-classification of EEG, with fewer parameters. As far as we know, it is
the first time that a detailed and complete method based on the transformer
idea has been proposed in this field. It has good potential to promote the
practicality of brain-computer interface (BCI). The source code can be found
at: \textit{https://github.com/anranknight/EEG-Transformer}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yonghao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xueyu Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lie Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xie_L/0/1/0/all/0/1"&gt;Longhan Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network insensitivity to parameter noise via adversarial regularization. (arXiv:2106.05009v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05009</id>
        <link href="http://arxiv.org/abs/2106.05009"/>
        <updated>2021-06-23T01:48:42.334Z</updated>
        <summary type="html"><![CDATA[Neuromorphic neural network processors, in the form of compute-in-memory
crossbar arrays of memristors, or in the form of subthreshold analog and
mixed-signal ASICs, promise enormous advantages in compute density and energy
efficiency for NN-based ML tasks. However, these technologies are prone to
computational non-idealities, due to process variation and intrinsic device
physics. This degrades the task performance of networks deployed to the
processor, by introducing parameter noise into the deployed model. While it is
possible to calibrate each device, or train networks individually for each
processor, these approaches are expensive and impractical for commercial
deployment. Alternative methods are therefore needed to train networks that are
inherently robust against parameter variation, as a consequence of network
architecture and parameters. We present a new adversarial network optimisation
algorithm that attacks network parameters during training, and promotes robust
performance during inference in the face of parameter variation. Our approach
introduces a regularization term penalising the susceptibility of a network to
weight perturbation. We compare against previous approaches for producing
parameter insensitivity such as dropout, weight smoothing and introducing
parameter noise during training. We show that our approach produces models that
are more robust to targeted parameter variation, and equally robust to random
parameter variation. Our approach finds minima in flatter locations in the
weight-loss landscape compared with other approaches, highlighting that the
networks found by our technique are less sensitive to parameter perturbation.
Our work provides an approach to deploy neural network architectures to
inference devices that suffer from computational non-idealities, with minimal
loss of performance. ...]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Buchel_J/0/1/0/all/0/1"&gt;Julian B&amp;#xfc;chel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faber_F/0/1/0/all/0/1"&gt;Fynn Faber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muir_D/0/1/0/all/0/1"&gt;Dylan R. Muir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Conditional Gaussian Mixture Model for Constrained Clustering. (arXiv:2106.06385v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06385</id>
        <link href="http://arxiv.org/abs/2106.06385"/>
        <updated>2021-06-23T01:48:42.318Z</updated>
        <summary type="html"><![CDATA[Constrained clustering has gained significant attention in the field of
machine learning as it can leverage prior information on a growing amount of
only partially labeled data. Following recent advances in deep generative
models, we propose a novel framework for constrained clustering that is
intuitive, interpretable, and can be trained efficiently in the framework of
stochastic gradient variational inference. By explicitly integrating domain
knowledge in the form of probabilistic relations, our proposed model (DC-GMM)
uncovers the underlying distribution of data conditioned on prior clustering
preferences, expressed as pairwise constraints. These constraints guide the
clustering process towards a desirable partition of the data by indicating
which samples should or should not belong to the same cluster. We provide
extensive experiments to demonstrate that DC-GMM shows superior clustering
performances and robustness compared to state-of-the-art deep constrained
clustering methods on a wide range of data sets. We further demonstrate the
usefulness of our approach on two challenging real-world applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Manduchi_L/0/1/0/all/0/1"&gt;Laura Manduchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chin_Cheong_K/0/1/0/all/0/1"&gt;Kieran Chin-Cheong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michel_H/0/1/0/all/0/1"&gt;Holger Michel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wellmann_S/0/1/0/all/0/1"&gt;Sven Wellmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogt_J/0/1/0/all/0/1"&gt;Julia E. Vogt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Focus U-Net: A novel dual attention-gated CNN for polyp segmentation during colonoscopy. (arXiv:2105.07467v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07467</id>
        <link href="http://arxiv.org/abs/2105.07467"/>
        <updated>2021-06-23T01:48:42.310Z</updated>
        <summary type="html"><![CDATA[Background: Colonoscopy remains the gold-standard screening for colorectal
cancer. However, significant miss rates for polyps have been reported,
particularly when there are multiple small adenomas. This presents an
opportunity to leverage computer-aided systems to support clinicians and reduce
the number of polyps missed.

Method: In this work we introduce the Focus U-Net, a novel dual
attention-gated deep neural network, which combines efficient spatial and
channel-based attention into a single Focus Gate module to encourage selective
learning of polyp features. The Focus U-Net further incorporates short-range
skip connections and deep supervision. Furthermore, we introduce the Hybrid
Focal loss, a new compound loss function based on the Focal loss and Focal
Tversky loss, to handle class-imbalanced image segmentation. For our
experiments, we selected five public datasets containing images of polyps
obtained during optical colonoscopy: CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB,
ETIS-Larib PolypDB and EndoScene test set. To evaluate model performance, we
use the Dice similarity coefficient (DSC) and Intersection over Union (IoU)
metrics.

Results: Our model achieves state-of-the-art results for both CVC-ClinicDB
and Kvasir-SEG, with a mean DSC of 0.941 and 0.910, respectively. When
evaluated on a combination of five public polyp datasets, our model similarly
achieves state-of-the-art results with a mean DSC of 0.878 and mean IoU of
0.809, a 14% and 15% improvement over the previous state-of-the-art results of
0.768 and 0.702, respectively.

Conclusions: This study shows the potential for deep learning to provide fast
and accurate polyp segmentation results for use during colonoscopy. The Focus
U-Net may be adapted for future use in newer non-invasive screening and more
broadly to other biomedical image segmentation tasks involving class imbalance
and requiring efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yeung_M/0/1/0/all/0/1"&gt;Michael Yeung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sala_E/0/1/0/all/0/1"&gt;Evis Sala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1"&gt;Carola-Bibiane Sch&amp;#xf6;nlieb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rundo_L/0/1/0/all/0/1"&gt;Leonardo Rundo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pruning of Deep Spiking Neural Networks through Gradient Rewiring. (arXiv:2105.04916v3 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04916</id>
        <link href="http://arxiv.org/abs/2105.04916"/>
        <updated>2021-06-23T01:48:42.302Z</updated>
        <summary type="html"><![CDATA[Spiking Neural Networks (SNNs) have been attached great importance due to
their biological plausibility and high energy-efficiency on neuromorphic chips.
As these chips are usually resource-constrained, the compression of SNNs is
thus crucial along the road of practical use of SNNs. Most existing methods
directly apply pruning approaches in artificial neural networks (ANNs) to SNNs,
which ignore the difference between ANNs and SNNs, thus limiting the
performance of the pruned SNNs. Besides, these methods are only suitable for
shallow SNNs. In this paper, inspired by synaptogenesis and synapse elimination
in the neural system, we propose gradient rewiring (Grad R), a joint learning
algorithm of connectivity and weight for SNNs, that enables us to seamlessly
optimize network structure without retraining. Our key innovation is to
redefine the gradient to a new synaptic parameter, allowing better exploration
of network structures by taking full advantage of the competition between
pruning and regrowth of connections. The experimental results show that the
proposed method achieves minimal loss of SNNs' performance on MNIST and
CIFAR-10 dataset so far. Moreover, it reaches a $\sim$3.5% accuracy loss under
unprecedented 0.73% connectivity, which reveals remarkable structure refining
capability in SNNs. Our work suggests that there exists extremely high
redundancy in deep SNNs. Our codes are available at
https://github.com/Yanqi-Chen/Gradient-Rewiring.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yanqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhaofei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1"&gt;Wei Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1"&gt;Tiejun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Policy Gradient Bayesian Robust Optimization for Imitation Learning. (arXiv:2106.06499v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06499</id>
        <link href="http://arxiv.org/abs/2106.06499"/>
        <updated>2021-06-23T01:48:42.292Z</updated>
        <summary type="html"><![CDATA[The difficulty in specifying rewards for many real-world problems has led to
an increased focus on learning rewards from human feedback, such as
demonstrations. However, there are often many different reward functions that
explain the human feedback, leaving agents with uncertainty over what the true
reward function is. While most policy optimization approaches handle this
uncertainty by optimizing for expected performance, many applications demand
risk-averse behavior. We derive a novel policy gradient-style robust
optimization approach, PG-BROIL, that optimizes a soft-robust objective that
balances expected performance and risk. To the best of our knowledge, PG-BROIL
is the first policy optimization algorithm robust to a distribution of reward
hypotheses which can scale to continuous MDPs. Results suggest that PG-BROIL
can produce a family of behaviors ranging from risk-neutral to risk-averse and
outperforms state-of-the-art imitation learning algorithms when learning from
ambiguous demonstrations by hedging against uncertainty, rather than seeking to
uniquely identify the demonstrator's reward function.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Javed_Z/0/1/0/all/0/1"&gt;Zaynah Javed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1"&gt;Daniel S. Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1"&gt;Satvik Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jerry Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balakrishna_A/0/1/0/all/0/1"&gt;Ashwin Balakrishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petrik_M/0/1/0/all/0/1"&gt;Marek Petrik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1"&gt;Anca D. Dragan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1"&gt;Ken Goldberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Polyak Stepsize with a Moving Target. (arXiv:2106.11851v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11851</id>
        <link href="http://arxiv.org/abs/2106.11851"/>
        <updated>2021-06-23T01:48:42.250Z</updated>
        <summary type="html"><![CDATA[We propose a new stochastic gradient method that uses recorded past loss
values to reduce the variance. Our method can be interpreted as a new
stochastic variant of the Polyak Stepsize that converges globally without
assuming interpolation. Our method introduces auxiliary variables, one for each
data point, that track the loss value for each data point. We provide a global
convergence theory for our method by showing that it can be interpreted as a
special variant of online SGD. The new method only stores a single scalar per
data point, opening up new applications for variance reduction where memory is
the bottleneck.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gower_R/0/1/0/all/0/1"&gt;Robert M. Gower&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Defazio_A/0/1/0/all/0/1"&gt;Aaron Defazio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1"&gt;Michael Rabbat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SISA: Securing Images by Selective Alteration. (arXiv:2106.11770v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.11770</id>
        <link href="http://arxiv.org/abs/2106.11770"/>
        <updated>2021-06-23T01:48:42.233Z</updated>
        <summary type="html"><![CDATA[With an increase in mobile and camera devices' popularity, digital content in
the form of images has increased drastically. As personal life is being
continuously documented in pictures, the risk of losing it to eavesdroppers is
a matter of grave concern. Secondary storage is the most preferred medium for
the storage of personal and other images. Our work is concerned with the
security of such images. While encryption is the best way to ensure image
security, full encryption and decryption is a computationally-intensive
process. Moreover, as cameras are getting better every day, image quality, and
thus, the pixel density has increased considerably. The increased pixel density
makes encryption and decryption more expensive. We, therefore, delve into
selective encryption and selective blurring based on the region of interest.
Instead of encrypting or blurring the entire photograph, we only encode
selected regions of the image. We present a comparative analysis of the partial
and full encryption of the photos. This kind of encoding will help us lower the
encryption overhead without compromising security. The applications utilizing
this technique will become more usable due to the reduction in the decryption
time. Additionally, blurred images being more readable than encrypted ones,
allowed us to define the level of security. We leverage the machine learning
algorithms like Mask-RCNN (Region-based convolutional neural network) and YOLO
(You Only Look Once) to select the region of interest. These algorithms have
set new benchmarks for object recognition. We develop an end to end system to
demonstrate our idea of selective encryption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gaherwar_P/0/1/0/all/0/1"&gt;Prutha Gaherwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1"&gt;Shraddha Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1"&gt;Raviraj Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khengare_R/0/1/0/all/0/1"&gt;Rahul Khengare&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Dynamical Systems from Noisy Sensor Measurements using Multiple Shooting. (arXiv:2106.11712v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11712</id>
        <link href="http://arxiv.org/abs/2106.11712"/>
        <updated>2021-06-23T01:48:42.226Z</updated>
        <summary type="html"><![CDATA[Modeling dynamical systems plays a crucial role in capturing and
understanding complex physical phenomena. When physical models are not
sufficiently accurate or hardly describable by analytical formulas, one can use
generic function approximators such as neural networks to capture the system
dynamics directly from sensor measurements. As for now, current methods to
learn the parameters of these neural networks are highly sensitive to the
inherent instability of most dynamical systems of interest, which in turn
prevents the study of very long sequences. In this work, we introduce a generic
and scalable method based on multiple shooting to learn latent representations
of indirectly observed dynamical systems. We achieve state-of-the-art
performances on systems observed directly from raw images. Further, we
demonstrate that our method is robust to noisy measurements and can handle
complex dynamical systems, such as chaotic ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jordana_A/0/1/0/all/0/1"&gt;Armand Jordana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carpentier_J/0/1/0/all/0/1"&gt;Justin Carpentier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Righetti_L/0/1/0/all/0/1"&gt;Ludovic Righetti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Constrained Optimization in Differentiable Neural Architecture Search. (arXiv:2106.11655v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11655</id>
        <link href="http://arxiv.org/abs/2106.11655"/>
        <updated>2021-06-23T01:48:42.219Z</updated>
        <summary type="html"><![CDATA[Differentiable Architecture Search (DARTS) is a recently proposed neural
architecture search (NAS) method based on a differentiable relaxation. Due to
its success, numerous variants analyzing and improving parts of the DARTS
framework have recently been proposed. By considering the problem as a
constrained bilevel optimization, we propose and analyze three improvements to
architectural weight competition, update scheduling, and regularization towards
discretization. First, we introduce a new approach to the activation of
architecture weights, which prevents confounding competition within an edge and
allows for fair comparison across edges to aid in discretization. Next, we
propose a dynamic schedule based on per-minibatch network information to make
architecture updates more informed. Finally, we consider two regularizations,
based on proximity to discretization and the Alternating Directions Method of
Multipliers (ADMM) algorithm, to promote early discretization. Our results show
that this new activation scheme reduces final architecture size and the
regularizations improve reliability in search results while maintaining
comparable performance to state-of-the-art in NAS, especially when used with
our new dynamic informed schedule.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maile_K/0/1/0/all/0/1"&gt;Kaitlin Maile&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lecarpentier_E/0/1/0/all/0/1"&gt;Erwan Lecarpentier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luga_H/0/1/0/all/0/1"&gt;Herv&amp;#xe9; Luga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_D/0/1/0/all/0/1"&gt;Dennis G. Wilson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CausalNLP: A Practical Toolkit for Causal Inference with Text. (arXiv:2106.08043v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08043</id>
        <link href="http://arxiv.org/abs/2106.08043"/>
        <updated>2021-06-23T01:48:41.861Z</updated>
        <summary type="html"><![CDATA[The vast majority of existing methods and systems for causal inference assume
that all variables under consideration are categorical or numerical (e.g.,
gender, price, blood pressure, enrollment). In this paper, we present
CausalNLP, a toolkit for inferring causality from observational data that
includes text in addition to traditional numerical and categorical variables.
CausalNLP employs the use of meta-learners for treatment effect estimation and
supports using raw text and its linguistic properties as both a treatment and a
"controlled-for" variable (e.g., confounder). The library is open-source and
available at: https://github.com/amaiya/causalnlp.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maiya_A/0/1/0/all/0/1"&gt;Arun S. Maiya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Including Sparse Production Knowledge into Variational Autoencoders to Increase Anomaly Detection Reliability. (arXiv:2103.12998v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12998</id>
        <link href="http://arxiv.org/abs/2103.12998"/>
        <updated>2021-06-23T01:48:41.855Z</updated>
        <summary type="html"><![CDATA[Digitalization leads to data transparency for production systems that we can
benefit from with data-driven analysis methods like neural networks. For
example, automated anomaly detection enables saving resources and optimizing
the production. We study using rarely occurring information about labeled
anomalies into Variational Autoencoder neural network structures to overcome
information deficits of supervised and unsupervised approaches. This method
outperforms all other models in terms of accuracy, precision, and recall. We
evaluate the following methods: Principal Component Analysis, Isolation Forest,
Classifying Neural Networks, and Variational Autoencoders on seven time series
datasets to find the best performing detection methods. We extend this idea to
include more infrequently occurring meta information about production
processes. This use of sparse labels, both of anomalies or production data,
allows to harness any additional information available for increasing anomaly
detection performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hammerbacher_T/0/1/0/all/0/1"&gt;Tom Hammerbacher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lange_Hegermann_M/0/1/0/all/0/1"&gt;Markus Lange-Hegermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Platz_G/0/1/0/all/0/1"&gt;Gorden Platz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Augmentation for Meta-Learning. (arXiv:2010.07092v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07092</id>
        <link href="http://arxiv.org/abs/2010.07092"/>
        <updated>2021-06-23T01:48:41.849Z</updated>
        <summary type="html"><![CDATA[Conventional image classifiers are trained by randomly sampling mini-batches
of images. To achieve state-of-the-art performance, practitioners use
sophisticated data augmentation schemes to expand the amount of training data
available for sampling. In contrast, meta-learning algorithms sample support
data, query data, and tasks on each training step. In this complex sampling
scenario, data augmentation can be used not only to expand the number of images
available per class, but also to generate entirely new classes/tasks. We
systematically dissect the meta-learning pipeline and investigate the distinct
ways in which data augmentation can be integrated at both the image and class
levels. Our proposed meta-specific data augmentation significantly improves the
performance of meta-learners on few-shot classification benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ni_R/0/1/0/all/0/1"&gt;Renkun Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1"&gt;Micah Goldblum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharaf_A/0/1/0/all/0/1"&gt;Amr Sharaf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_K/0/1/0/all/0/1"&gt;Kezhi Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1"&gt;Tom Goldstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Entropy-based Logic Explanations of Neural Networks. (arXiv:2106.06804v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06804</id>
        <link href="http://arxiv.org/abs/2106.06804"/>
        <updated>2021-06-23T01:48:41.843Z</updated>
        <summary type="html"><![CDATA[Explainable artificial intelligence has rapidly emerged since lawmakers have
started requiring interpretable models for safety-critical domains.
Concept-based neural networks have arisen as explainable-by-design methods as
they leverage human-understandable symbols (i.e. concepts) to predict class
memberships. However, most of these approaches focus on the identification of
the most relevant concepts but do not provide concise, formal explanations of
how such concepts are leveraged by the classifier to make predictions. In this
paper, we propose a novel end-to-end differentiable approach enabling the
extraction of logic explanations from neural networks using the formalism of
First-Order Logic. The method relies on an entropy-based criterion which
automatically identifies the most relevant concepts. We consider four different
case studies to demonstrate that: (i) this entropy-based criterion enables the
distillation of concise logic explanations in safety-critical domains from
clinical data to computer vision; (ii) the proposed approach outperforms
state-of-the-art white-box models in terms of classification accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1"&gt;Pietro Barbiero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciravegna_G/0/1/0/all/0/1"&gt;Gabriele Ciravegna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giannini_F/0/1/0/all/0/1"&gt;Francesco Giannini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1"&gt;Pietro Li&amp;#xf3;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gori_M/0/1/0/all/0/1"&gt;Marco Gori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melacci_S/0/1/0/all/0/1"&gt;Stefano Melacci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for Suicide and Depression Identification with Unsupervised Label Correction. (arXiv:2102.09427v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09427</id>
        <link href="http://arxiv.org/abs/2102.09427"/>
        <updated>2021-06-23T01:48:41.837Z</updated>
        <summary type="html"><![CDATA[Early detection of suicidal ideation in depressed individuals can allow for
adequate medical attention and support, which in many cases is life-saving.
Recent NLP research focuses on classifying, from a given piece of text, if an
individual is suicidal or clinically healthy. However, there have been no major
attempts to differentiate between depression and suicidal ideation, which is an
important clinical challenge. Due to the scarce availability of EHR data,
suicide notes, or other similar verified sources, web query data has emerged as
a promising alternative. Online sources, such as Reddit, allow for anonymity
that prompts honest disclosure of symptoms, making it a plausible source even
in a clinical setting. However, these online datasets also result in lower
performance, which can be attributed to the inherent noise in web-scraped
labels, which necessitates a noise-removal process. Thus, we propose SDCNL, a
suicide versus depression classification method through a deep learning
approach. We utilize online content from Reddit to train our algorithm, and to
verify and correct noisy labels, we propose a novel unsupervised label
correction method which, unlike previous work, does not require prior noise
distribution information. Our extensive experimentation with multiple deep word
embedding models and classifiers display the strong performance of the method
in anew, challenging classification application. We make our code and dataset
available at https://github.com/ayaanzhaque/SDCNL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haque_A/0/1/0/all/0/1"&gt;Ayaan Haque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddi_V/0/1/0/all/0/1"&gt;Viraaj Reddi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giallanza_T/0/1/0/all/0/1"&gt;Tyler Giallanza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Solving Inefficiency of Self-supervised Representation Learning. (arXiv:2104.08760v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08760</id>
        <link href="http://arxiv.org/abs/2104.08760"/>
        <updated>2021-06-23T01:48:41.830Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning (especially contrastive learning) has attracted
great interest due to its tremendous potentials in learning discriminative
representations in an unsupervised manner. Despite the acknowledged successes,
existing contrastive learning methods suffer from very low learning efficiency,
e.g., taking about ten times more training epochs than supervised learning for
comparable recognition accuracy. In this paper, we discover two contradictory
phenomena in contrastive learning that we call under-clustering and
over-clustering problems, which are major obstacles to learning efficiency.
Under-clustering means that the model cannot efficiently learn to discover the
dissimilarity between inter-class samples when the negative sample pairs for
contrastive learning are insufficient to differentiate all the actual object
categories. Over-clustering implies that the model cannot efficiently learn the
feature representation from excessive negative sample pairs, which enforces the
model to over-cluster samples of the same actual categories into different
clusters. To simultaneously overcome these two problems, we propose a novel
self-supervised learning framework using a median triplet loss. Precisely, we
employ a triplet loss tending to maximize the relative distance between the
positive pair and negative pairs to address the under-clustering problem; and
we construct the negative pair by selecting the negative sample of a median
similarity score from all negative samples to avoid the over-clustering
problem, guaranteed by the Bernoulli Distribution model. We extensively
evaluate our proposed framework in several large-scale benchmarks (e.g.,
ImageNet, SYSU-30k, and COCO). The results demonstrate the superior performance
(e.g., the learning efficiency) of our model over the latest state-of-the-art
methods by a clear margin. Codes available at:
https://github.com/wanggrun/triplet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guangrun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Keze Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guangcong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip H.S. Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1"&gt;Liang Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Hard Optimization Problems: A Data Generation Perspective. (arXiv:2106.02601v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02601</id>
        <link href="http://arxiv.org/abs/2106.02601"/>
        <updated>2021-06-23T01:48:41.824Z</updated>
        <summary type="html"><![CDATA[Optimization problems are ubiquitous in our societies and are present in
almost every segment of the economy. Most of these optimization problems are
NP-hard and computationally demanding, often requiring approximate solutions
for large-scale instances. Machine learning frameworks that learn to
approximate solutions to such hard optimization problems are a potentially
promising avenue to address these difficulties, particularly when many closely
related problem instances must be solved repeatedly. Supervised learning
frameworks can train a model using the outputs of pre-solved instances.
However, when the outputs are themselves approximations, when the optimization
problem has symmetric solutions, and/or when the solver uses randomization,
solutions to closely related instances may exhibit large differences and the
learning task can become inherently more difficult. This paper demonstrates
this critical challenge, connects the volatility of the training data to the
ability of a model to approximate it, and proposes a method for producing
(exact or approximate) solutions to optimization problems that are more
amenable to supervised learning tasks. The effectiveness of the method is
tested on hard non-linear nonconvex and discrete combinatorial problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Kotary_J/0/1/0/all/0/1"&gt;James Kotary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Fioretto_F/0/1/0/all/0/1"&gt;Ferdinando Fioretto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hentenryck_P/0/1/0/all/0/1"&gt;Pascal Van Hentenryck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RtFPS: An Interactive Map that Visualizes and Predicts Wildfires in the US. (arXiv:2105.10880v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10880</id>
        <link href="http://arxiv.org/abs/2105.10880"/>
        <updated>2021-06-23T01:48:41.816Z</updated>
        <summary type="html"><![CDATA[Climate change has largely impacted our daily lives. As one of its
consequences, we are experiencing more wildfires. In the year 2020, wildfires
burned a record number of 8,888,297 acres in the US. To awaken people's
attention to climate change, and to visualize the current risk of wildfires, We
developed RtFPS, "Real-Time Fire Prediction System". It provides a real-time
prediction visualization of wildfire risk at specific locations base on a
Machine Learning model. It also provides interactive map features that show the
historical wildfire events with environmental info.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mulyono_H/0/1/0/all/0/1"&gt;Hermawan Mulyono&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Ying Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zhiyin Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_D/0/1/0/all/0/1"&gt;Desmond Chan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The impact of using biased performance metrics on software defect prediction research. (arXiv:2103.10201v4 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10201</id>
        <link href="http://arxiv.org/abs/2103.10201"/>
        <updated>2021-06-23T01:48:41.809Z</updated>
        <summary type="html"><![CDATA[Context: Software engineering researchers have undertaken many experiments
investigating the potential of software defect prediction algorithms.
Unfortunately, some widely used performance metrics are known to be
problematic, most notably F1, but nevertheless F1 is widely used.

Objective: To investigate the potential impact of using F1 on the validity of
this large body of research.

Method: We undertook a systematic review to locate relevant experiments and
then extract all pairwise comparisons of defect prediction performance using F1
and the un-biased Matthews correlation coefficient (MCC).

Results: We found a total of 38 primary studies. These contain 12,471 pairs
of results. Of these, 21.95% changed direction when the MCC metric is used
instead of the biased F1 metric. Unfortunately, we also found evidence
suggesting that F1 remains widely used in software defect prediction research.

Conclusions: We reiterate the concerns of statisticians that the F1 is a
problematic metric outside of an information retrieval context, since we are
concerned about both classes (defect-prone and not defect-prone units). This
inappropriate usage has led to a substantial number (more than one fifth) of
erroneous (in terms of direction) results. Therefore we urge researchers to (i)
use an unbiased metric and (ii) publish detailed results including confusion
matrices such that alternative analyses become possible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1"&gt;Jingxiu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shepperd_M/0/1/0/all/0/1"&gt;Martin Shepperd&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Guaranteed Fixed-Confidence Best Arm Identification in Multi-Armed Bandits: Simple Sequential Elimination Algorithms. (arXiv:2106.06848v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06848</id>
        <link href="http://arxiv.org/abs/2106.06848"/>
        <updated>2021-06-23T01:48:41.767Z</updated>
        <summary type="html"><![CDATA[We consider the problem of finding, through adaptive sampling, which of $n$
options (arms) has the largest mean. Our objective is to determine a rule which
identifies the best arm with a fixed minimum confidence using as few
observations as possible, i.e. this is a fixed-confidence (FC) best arm
identification (BAI) in multi-armed bandits. We study such problems under the
Bayesian setting with both Bernoulli and Gaussian arms. We propose to use the
classical "vector at a time" (VT) rule, which samples each remaining arm once
in each round. We show how VT can be implemented and analyzed in our Bayesian
setting and be improved by early elimination. Our analysis show that these
algorithms guarantee an optimal strategy under the prior. We also propose and
analyze a variant of the classical "play the winner" (PW) algorithm. Numerical
results show that these rules compare favorably with state-of-art algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Azizi_M/0/1/0/all/0/1"&gt;MohammadJavad Azizi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ross_S/0/1/0/all/0/1"&gt;Sheldon M Ross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhengyu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarially-Trained Nonnegative Matrix Factorization. (arXiv:2104.04757v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04757</id>
        <link href="http://arxiv.org/abs/2104.04757"/>
        <updated>2021-06-23T01:48:41.757Z</updated>
        <summary type="html"><![CDATA[We consider an adversarially-trained version of the nonnegative matrix
factorization, a popular latent dimensionality reduction technique. In our
formulation, an attacker adds an arbitrary matrix of bounded norm to the given
data matrix. We design efficient algorithms inspired by adversarial training to
optimize for dictionary and coefficient matrices with enhanced generalization
abilities. Extensive simulations on synthetic and benchmark datasets
demonstrate the superior predictive performance on matrix completion tasks of
our proposed method compared to state-of-the-art competitors, including other
variants of adversarial nonnegative matrix factorization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1"&gt;Ting Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1"&gt;Vincent Y. F. Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fevotte_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric F&amp;#xe9;votte&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Routine Clustering of Mobile Sensor Data Facilitates Psychotic Relapse Prediction in Schizophrenia Patients. (arXiv:2106.11487v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11487</id>
        <link href="http://arxiv.org/abs/2106.11487"/>
        <updated>2021-06-23T01:48:41.748Z</updated>
        <summary type="html"><![CDATA[We aim to develop clustering models to obtain behavioral representations from
continuous multimodal mobile sensing data towards relapse prediction tasks. The
identified clusters could represent different routine behavioral trends related
to daily living of patients as well as atypical behavioral trends associated
with impending relapse.

We used the mobile sensing data obtained in the CrossCheck project for our
analysis. Continuous data from six different mobile sensing-based modalities
(e.g. ambient light, sound/conversation, acceleration etc.) obtained from a
total of 63 schizophrenia patients, each monitored for up to a year, were used
for the clustering models and relapse prediction evaluation. Two clustering
models, Gaussian Mixture Model (GMM) and Partition Around Medoids (PAM), were
used to obtain behavioral representations from the mobile sensing data. The
features obtained from the clustering models were used to train and evaluate a
personalized relapse prediction model using Balanced Random Forest. The
personalization was done by identifying optimal features for a given patient
based on a personalization subset consisting of other patients who are of
similar age.

The clusters identified using the GMM and PAM models were found to represent
different behavioral patterns (such as clusters representing sedentary days,
active but with low communications days, etc.). Significant changes near the
relapse periods were seen in the obtained behavioral representation features
from the clustering models. The clustering model based features, together with
other features characterizing the mobile sensing data, resulted in an F2 score
of 0.24 for the relapse prediction task in a leave-one-patient-out evaluation
setting. This obtained F2 score is significantly higher than a random
classification baseline with an average F2 score of 0.042.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Joanne Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lamichhane_B/0/1/0/all/0/1"&gt;Bishal Lamichhane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Zeev_D/0/1/0/all/0/1"&gt;Dror Ben-Zeev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Campbell_A/0/1/0/all/0/1"&gt;Andrew Campbell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sano_A/0/1/0/all/0/1"&gt;Akane Sano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Quality as Predictor of Voice Anti-Spoofing Generalization. (arXiv:2103.14602v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14602</id>
        <link href="http://arxiv.org/abs/2103.14602"/>
        <updated>2021-06-23T01:48:41.733Z</updated>
        <summary type="html"><![CDATA[Voice anti-spoofing aims at classifying a given utterance either as a
bonafide human sample, or a spoofing attack (e.g. synthetic or replayed
sample). Many anti-spoofing methods have been proposed but most of them fail to
generalize across domains (corpora) -- and we do not know \emph{why}. We
outline a novel interpretative framework for gauging the impact of data quality
upon anti-spoofing performance. Our within- and between-domain experiments pool
data from seven public corpora and three anti-spoofing methods based on
Gaussian mixture and convolutive neural network models. We assess the impacts
of long-term spectral information, speaker population (through x-vector speaker
embeddings), signal-to-noise ratio, and selected voice quality features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chettri_B/0/1/0/all/0/1"&gt;Bhusan Chettri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hautamaki_R/0/1/0/all/0/1"&gt;Rosa Gonz&amp;#xe1;lez Hautam&amp;#xe4;ki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sahidullah_M/0/1/0/all/0/1"&gt;Md Sahidullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kinnunen_T/0/1/0/all/0/1"&gt;Tomi Kinnunen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uniform-PAC Bounds for Reinforcement Learning with Linear Function Approximation. (arXiv:2106.11612v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11612</id>
        <link href="http://arxiv.org/abs/2106.11612"/>
        <updated>2021-06-23T01:48:41.718Z</updated>
        <summary type="html"><![CDATA[We study reinforcement learning (RL) with linear function approximation.
Existing algorithms for this problem only have high-probability regret and/or
Probably Approximately Correct (PAC) sample complexity guarantees, which cannot
guarantee the convergence to the optimal policy. In this paper, in order to
overcome the limitation of existing algorithms, we propose a new algorithm
called FLUTE, which enjoys uniform-PAC convergence to the optimal policy with
high probability. The uniform-PAC guarantee is the strongest possible guarantee
for reinforcement learning in the literature, which can directly imply both PAC
and high probability regret bounds, making our algorithm superior to all
existing algorithms with linear function approximation. At the core of our
algorithm is a novel minimax value function estimator and a multi-level
partition scheme to select the training samples from historical observations.
Both of these techniques are new and of independent interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jiafan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Dongruo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1"&gt;Quanquan Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incremental Deep Neural Network Learning using Classification Confidence Thresholding. (arXiv:2106.11437v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11437</id>
        <link href="http://arxiv.org/abs/2106.11437"/>
        <updated>2021-06-23T01:48:41.688Z</updated>
        <summary type="html"><![CDATA[Most modern neural networks for classification fail to take into account the
concept of the unknown. Trained neural networks are usually tested in an
unrealistic scenario with only examples from a closed set of known classes. In
an attempt to develop a more realistic model, the concept of working in an open
set environment has been introduced. This in turn leads to the concept of
incremental learning where a model with its own architecture and initial
trained set of data can identify unknown classes during the testing phase and
autonomously update itself if evidence of a new class is detected. Some
problems that arise in incremental learning are inefficient use of resources to
retrain the classifier repeatedly and the decrease of classification accuracy
as multiple classes are added over time. This process of instantiating new
classes is repeated as many times as necessary, accruing errors. To address
these problems, this paper proposes the Classification Confidence Threshold
approach to prime neural networks for incremental learning to keep accuracies
high by limiting forgetting. A lean method is also used to reduce resources
used in the retraining of the neural network. The proposed method is based on
the idea that a network is able to incrementally learn a new class even when
exposed to a limited number samples associated with the new class. This method
can be applied to most existing neural networks with minimal changes to network
architecture.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leo_J/0/1/0/all/0/1"&gt;Justin Leo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1"&gt;Jugal Kalita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Agnostic Reinforcement Learning with Low-Rank MDPs and Rich Observations. (arXiv:2106.11519v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11519</id>
        <link href="http://arxiv.org/abs/2106.11519"/>
        <updated>2021-06-23T01:48:41.679Z</updated>
        <summary type="html"><![CDATA[There have been many recent advances on provably efficient Reinforcement
Learning (RL) in problems with rich observation spaces. However, all these
works share a strong realizability assumption about the optimal value function
of the true MDP. Such realizability assumptions are often too strong to hold in
practice. In this work, we consider the more realistic setting of agnostic RL
with rich observation spaces and a fixed class of policies $\Pi$ that may not
contain any near-optimal policy. We provide an algorithm for this setting whose
error is bounded in terms of the rank $d$ of the underlying MDP. Specifically,
our algorithm enjoys a sample complexity bound of $\widetilde{O}\left((H^{4d}
K^{3d} \log |\Pi|)/\epsilon^2\right)$ where $H$ is the length of episodes, $K$
is the number of actions and $\epsilon>0$ is the desired sub-optimality. We
also provide a nearly matching lower bound for this agnostic setting that shows
that the exponential dependence on rank is unavoidable, without further
assumptions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dann_C/0/1/0/all/0/1"&gt;Christoph Dann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansour_Y/0/1/0/all/0/1"&gt;Yishay Mansour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohri_M/0/1/0/all/0/1"&gt;Mehryar Mohri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sekhari_A/0/1/0/all/0/1"&gt;Ayush Sekhari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sridharan_K/0/1/0/all/0/1"&gt;Karthik Sridharan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking Invertible Architectures on Inverse Problems. (arXiv:2101.10763v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10763</id>
        <link href="http://arxiv.org/abs/2101.10763"/>
        <updated>2021-06-23T01:48:41.672Z</updated>
        <summary type="html"><![CDATA[Recent work demonstrated that flow-based invertible neural networks are
promising tools for solving ambiguous inverse problems. Following up on this,
we investigate how ten invertible architectures and related models fare on two
intuitive, low-dimensional benchmark problems, obtaining the best results with
coupling layers and simple autoencoders. We hope that our initial efforts
inspire other researchers to evaluate their invertible architectures in the
same setting and put forth additional benchmarks, so our evaluation may
eventually grow into an official community challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kruse_J/0/1/0/all/0/1"&gt;Jakob Kruse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ardizzone_L/0/1/0/all/0/1"&gt;Lynton Ardizzone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rother_C/0/1/0/all/0/1"&gt;Carsten Rother&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kothe_U/0/1/0/all/0/1"&gt;Ullrich K&amp;#xf6;the&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[User Identification across Social Networking Sites using User Profiles and Posting Patterns. (arXiv:2106.11815v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11815</id>
        <link href="http://arxiv.org/abs/2106.11815"/>
        <updated>2021-06-23T01:48:41.665Z</updated>
        <summary type="html"><![CDATA[With the prevalence of online social networking sites (OSNs) and mobile
devices, people are increasingly reliant on a variety of OSNs for keeping in
touch with family and friends, and using it as a source of information. For
example, a user might utilise multiple OSNs for different purposes, such as
using Flickr to share holiday pictures with family and friends, and Twitter to
post short messages about their thoughts. Identifying the same user across
multiple OSNs is an important task as this allows us to understand the usage
patterns of users among different OSNs, make recommendations when a user
registers for a new OSN, and various other useful applications. To address this
problem, we proposed an algorithm based on the multilayer perceptron using
various types of features, namely: (i) user profile, such as name, location,
description; (ii) temporal distribution of user generated content; and (iii)
embedding based on user name, real name and description. Using a Twitter and
Flickr dataset of users and their posting activities, we perform an empirical
study on how these features affect the performance of user identification
across the two OSNs and discuss our main findings based on the different
features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Solanki_P/0/1/0/all/0/1"&gt;Prashant Solanki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1"&gt;Kwan Hui Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harwood_A/0/1/0/all/0/1"&gt;Aaron Harwood&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Augmentation for Opcode Sequence Based Malware Detection. (arXiv:2106.11821v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.11821</id>
        <link href="http://arxiv.org/abs/2106.11821"/>
        <updated>2021-06-23T01:48:41.659Z</updated>
        <summary type="html"><![CDATA[Data augmentation has been successfully used in many areas of deep-learning
to significantly improve model performance. Typically data augmentation
simulates realistic variations in data in order to increase the apparent
diversity of the training-set. However, for opcode-based malware analysis,
where deep learning methods are already achieving state of the art performance,
it is not immediately clear how to apply data augmentation. In this paper we
study different methods of data augmentation starting with basic methods using
fixed transformations and moving to methods that adapt to the data. We propose
a novel data augmentation method based on using an opcode embedding layer
within the network and its corresponding opcode embedding matrix to perform
adaptive data augmentation during training. To the best of our knowledge this
is the first paper to carry out a systematic study of different augmentation
methods applied to opcode sequence based malware classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McLaughlin_N/0/1/0/all/0/1"&gt;Niall McLaughlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rincon_J/0/1/0/all/0/1"&gt;Jesus Martinez del Rincon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimax Regret for Stochastic Shortest Path with Adversarial Costs and Known Transition. (arXiv:2012.04053v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04053</id>
        <link href="http://arxiv.org/abs/2012.04053"/>
        <updated>2021-06-23T01:48:41.640Z</updated>
        <summary type="html"><![CDATA[We study the stochastic shortest path problem with adversarial costs and
known transition, and show that the minimax regret is
$\widetilde{O}(\sqrt{DT^\star K})$ and $\widetilde{O}(\sqrt{DT^\star SA K})$
for the full-information setting and the bandit feedback setting respectively,
where $D$ is the diameter, $T^\star$ is the expected hitting time of the
optimal policy, $S$ is the number of states, $A$ is the number of actions, and
$K$ is the number of episodes. Our results significantly improve upon the
existing work of (Rosenberg and Mansour, 2020) which only considers the
full-information setting and achieves suboptimal regret. Our work is also the
first to consider bandit feedback with adversarial costs.

Our algorithms are built on top of the Online Mirror Descent framework with a
variety of new techniques that might be of independent interest, including an
improved multi-scale expert algorithm, a reduction from general stochastic
shortest path to a special loop-free case, a skewed occupancy measure space,
and a novel correction term added to the cost estimators. Interestingly, the
last two elements reduce the variance of the learner via positive bias and the
variance of the optimal policy via negative bias respectively, and having them
simultaneously is critical for obtaining the optimal high-probability bound in
the bandit feedback setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Haipeng Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1"&gt;Chen-Yu Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Learning under Pool Set Distribution Shift and Noisy Data. (arXiv:2106.11719v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11719</id>
        <link href="http://arxiv.org/abs/2106.11719"/>
        <updated>2021-06-23T01:48:41.632Z</updated>
        <summary type="html"><![CDATA[Active Learning is essential for more label-efficient deep learning. Bayesian
Active Learning has focused on BALD, which reduces model parameter uncertainty.
However, we show that BALD gets stuck on out-of-distribution or junk data that
is not relevant for the task. We examine a novel *Expected Predictive
Information Gain (EPIG)* to deal with distribution shifts of the pool set. EPIG
reduces the uncertainty of *predictions* on an unlabelled *evaluation set*
sampled from the test data distribution whose distribution might be different
to the pool set distribution. Based on this, our new EPIG-BALD acquisition
function for Bayesian Neural Networks selects samples to improve the
performance on the test data distribution instead of selecting samples that
reduce model uncertainty everywhere, including for out-of-distribution regions
with low density in the test data distribution. Our method outperforms
state-of-the-art Bayesian active learning methods on high-dimensional datasets
and avoids out-of-distribution junk data in cases where current
state-of-the-art methods fail.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kirsch_A/0/1/0/all/0/1"&gt;Andreas Kirsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rainforth_T/0/1/0/all/0/1"&gt;Tom Rainforth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Routing between Capsules. (arXiv:2106.11531v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11531</id>
        <link href="http://arxiv.org/abs/2106.11531"/>
        <updated>2021-06-23T01:48:41.626Z</updated>
        <summary type="html"><![CDATA[Routing methods in capsule networks often learn a hierarchical relationship
for capsules in successive layers, but the intra-relation between capsules in
the same layer is less studied, while this intra-relation is a key factor for
the semantic understanding in text data. Therefore, in this paper, we introduce
a new capsule network with graph routing to learn both relationships, where
capsules in each layer are treated as the nodes of a graph. We investigate
strategies to yield adjacency and degree matrix with three different distances
from a layer of capsules, and propose the graph routing mechanism between those
capsules. We validate our approach on five text classification datasets, and
our findings suggest that the approach combining bottom-up routing and top-down
attention performs the best. Such an approach demonstrates generalization
capability across datasets. Compared to the state-of-the-art routing methods,
the improvements in accuracy in the five datasets we used were 0.82, 0.39,
0.07, 1.01, and 0.02, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wei Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1"&gt;Erik Cambria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Suhang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1"&gt;Steffen Eger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bandit Learning in Decentralized Matching Markets. (arXiv:2012.07348v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07348</id>
        <link href="http://arxiv.org/abs/2012.07348"/>
        <updated>2021-06-23T01:48:41.619Z</updated>
        <summary type="html"><![CDATA[We study two-sided matching markets in which one side of the market (the
players) does not have a priori knowledge about its preferences for the other
side (the arms) and is required to learn its preferences from experience. Also,
we assume the players have no direct means of communication. This model extends
the standard stochastic multi-armed bandit framework to a decentralized
multiple player setting with competition. We introduce a new algorithm for this
setting that, over a time horizon $T$, attains $\mathcal{O}(\log(T))$ stable
regret when preferences of the arms over players are shared, and
$\mathcal{O}(\log(T)^2)$ regret when there are no assumptions on the
preferences on either side. Moreover, in the setting where a single player may
deviate, we show that the algorithm is incentive compatible whenever the arms'
preferences are shared, but not necessarily so when preferences are fully
general.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lydia T. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruan_F/0/1/0/all/0/1"&gt;Feng Ruan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mania_H/0/1/0/all/0/1"&gt;Horia Mania&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Impossible Tuning Made Possible: A New Expert Algorithm and Its Applications. (arXiv:2102.01046v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01046</id>
        <link href="http://arxiv.org/abs/2102.01046"/>
        <updated>2021-06-23T01:48:41.612Z</updated>
        <summary type="html"><![CDATA[We resolve the long-standing "impossible tuning" issue for the classic expert
problem and show that, it is in fact possible to achieve regret
$O\left(\sqrt{(\ln d)\sum_t \ell_{t,i}^2}\right)$ simultaneously for all expert
$i$ in a $T$-round $d$-expert problem where $\ell_{t,i}$ is the loss for expert
$i$ in round $t$. Our algorithm is based on the Mirror Descent framework with a
correction term and a weighted entropy regularizer. While natural, the
algorithm has not been studied before and requires a careful analysis. We also
generalize the bound to $O\left(\sqrt{(\ln d)\sum_t
(\ell_{t,i}-m_{t,i})^2}\right)$ for any prediction vector $m_t$ that the
learner receives, and recover or improve many existing results by choosing
different $m_t$. Furthermore, we use the same framework to create a master
algorithm that combines a set of base algorithms and learns the best one with
little overhead. The new guarantee of our master allows us to derive many new
results for both the expert problem and more generally Online Linear
Optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Haipeng Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1"&gt;Chen-Yu Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generate High Resolution Images With Generative Variational Autoencoder. (arXiv:2008.10399v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.10399</id>
        <link href="http://arxiv.org/abs/2008.10399"/>
        <updated>2021-06-23T01:48:41.592Z</updated>
        <summary type="html"><![CDATA[In this work, we present a novel neural network to generate high resolution
images. We replace the decoder of VAE with a discriminator while using the
encoder as it is. The encoder is fed data from a normal distribution while the
generator is fed from a gaussian distribution. The combination from both is
given to a discriminator which tells whether the generated image is correct or
not. We evaluate our network on 3 different datasets: MNIST, LSUN and CelebA
dataset. Our network beats the previous state of the art using MMD, SSIM, log
likelihood, reconstruction error, ELBO and KL divergence as the evaluation
metrics while generating much sharper images. This work is potentially very
exciting as we are able to combine the advantages of generative models and
inference models in a principled bayesian manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sagar_A/0/1/0/all/0/1"&gt;Abhinav Sagar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hi-BEHRT: Hierarchical Transformer-based model for accurate prediction of clinical events using multimodal longitudinal electronic health records. (arXiv:2106.11360v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11360</id>
        <link href="http://arxiv.org/abs/2106.11360"/>
        <updated>2021-06-23T01:48:41.584Z</updated>
        <summary type="html"><![CDATA[Electronic health records represent a holistic overview of patients'
trajectories. Their increasing availability has fueled new hopes to leverage
them and develop accurate risk prediction models for a wide range of diseases.
Given the complex interrelationships of medical records and patient outcomes,
deep learning models have shown clear merits in achieving this goal. However, a
key limitation of these models remains their capacity in processing long
sequences. Capturing the whole history of medical encounters is expected to
lead to more accurate predictions, but the inclusion of records collected for
decades and from multiple resources can inevitably exceed the receptive field
of the existing deep learning architectures. This can result in missing
crucial, long-term dependencies. To address this gap, we present Hi-BEHRT, a
hierarchical Transformer-based model that can significantly expand the
receptive field of Transformers and extract associations from much longer
sequences. Using a multimodal large-scale linked longitudinal electronic health
records, the Hi-BEHRT exceeds the state-of-the-art BEHRT 1% to 5% for area
under the receiver operating characteristic (AUROC) curve and 3% to 6% for area
under the precision recall (AUPRC) curve on average, and 3% to 6% (AUROC) and
3% to 11% (AUPRC) for patients with long medical history for 5-year heart
failure, diabetes, chronic kidney disease, and stroke risk prediction.
Additionally, because pretraining for hierarchical Transformer is not
well-established, we provide an effective end-to-end contrastive pre-training
strategy for Hi-BEHRT using EHR, improving its transferability on predicting
clinical events with relatively small training dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yikuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mamouei_M/0/1/0/all/0/1"&gt;Mohammad Mamouei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salimi_Khorshidi_G/0/1/0/all/0/1"&gt;Gholamreza Salimi-Khorshidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_S/0/1/0/all/0/1"&gt;Shishir Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassaine_A/0/1/0/all/0/1"&gt;Abdelaali Hassaine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Canoy_D/0/1/0/all/0/1"&gt;Dexter Canoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1"&gt;Thomas Lukasiewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahimi_K/0/1/0/all/0/1"&gt;Kazem Rahimi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Neural Ordinary Differential Equations. (arXiv:1911.07532v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.07532</id>
        <link href="http://arxiv.org/abs/1911.07532"/>
        <updated>2021-06-23T01:48:41.576Z</updated>
        <summary type="html"><![CDATA[We introduce the framework of continuous--depth graph neural networks (GNNs).
Graph neural ordinary differential equations (GDEs) are formalized as the
counterpart to GNNs where the input-output relationship is determined by a
continuum of GNN layers, blending discrete topological structures and
differential equations. The proposed framework is shown to be compatible with
various static and autoregressive GNN models. Results prove general
effectiveness of GDEs: in static settings they offer computational advantages
by incorporating numerical methods in their forward pass; in dynamic settings,
on the other hand, they are shown to improve performance by exploiting the
geometry of the underlying dynamics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Poli_M/0/1/0/all/0/1"&gt;Michael Poli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Massaroli_S/0/1/0/all/0/1"&gt;Stefano Massaroli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Junyoung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamashita_A/0/1/0/all/0/1"&gt;Atsushi Yamashita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asama_H/0/1/0/all/0/1"&gt;Hajime Asama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jinkyoo Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta Adversarial Training against Universal Patches. (arXiv:2101.11453v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11453</id>
        <link href="http://arxiv.org/abs/2101.11453"/>
        <updated>2021-06-23T01:48:41.568Z</updated>
        <summary type="html"><![CDATA[Recently demonstrated physical-world adversarial attacks have exposed
vulnerabilities in perception systems that pose severe risks for
safety-critical applications such as autonomous driving. These attacks place
adversarial artifacts in the physical world that indirectly cause the addition
of a universal patch to inputs of a model that can fool it in a variety of
contexts. Adversarial training is the most effective defense against
image-dependent adversarial attacks. However, tailoring adversarial training to
universal patches is computationally expensive since the optimal universal
patch depends on the model weights which change during training. We propose
meta adversarial training (MAT), a novel combination of adversarial training
with meta-learning, which overcomes this challenge by meta-learning universal
patches along with model training. MAT requires little extra computation while
continuously adapting a large set of patches to the current model. MAT
considerably increases robustness against universal patch attacks on image
classification and traffic-light detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Metzen_J/0/1/0/all/0/1"&gt;Jan Hendrik Metzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finnie_N/0/1/0/all/0/1"&gt;Nicole Finnie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hutmacher_R/0/1/0/all/0/1"&gt;Robin Hutmacher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Photozilla: A Large-Scale Photography Dataset and Visual Embedding for 20 Photography Styles. (arXiv:2106.11359v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11359</id>
        <link href="http://arxiv.org/abs/2106.11359"/>
        <updated>2021-06-23T01:48:41.559Z</updated>
        <summary type="html"><![CDATA[The advent of social media platforms has been a catalyst for the development
of digital photography that engendered a boom in vision applications. With this
motivation, we introduce a large-scale dataset termed 'Photozilla', which
includes over 990k images belonging to 10 different photographic styles. The
dataset is then used to train 3 classification models to automatically classify
the images into the relevant style which resulted in an accuracy of ~96%. With
the rapid evolution of digital photography, we have seen new types of
photography styles emerging at an exponential rate. On that account, we present
a novel Siamese-based network that uses the trained classification models as
the base architecture to adapt and classify unseen styles with only 25 training
samples. We report an accuracy of over 68% for identifying 10 other distinct
types of photography styles. This dataset can be found at
https://trisha025.github.io/Photozilla/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singhal_T/0/1/0/all/0/1"&gt;Trisha Singhal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Junhua Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blessing_L/0/1/0/all/0/1"&gt;Lucienne T. M. Blessing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1"&gt;Kwan Hui Lim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Over-Air Subspace Tracking from Incomplete and Corrupted Data. (arXiv:2002.12873v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.12873</id>
        <link href="http://arxiv.org/abs/2002.12873"/>
        <updated>2021-06-23T01:48:41.549Z</updated>
        <summary type="html"><![CDATA[Subspace tracking (ST) with missing data (ST-miss) or outliers (Robust ST) or
both (Robust ST-miss) has been extensively studied in the last many years. This
work provides a new simple algorithm and guarantee for both ST with missing
data (ST-miss) and RST-miss. Unlike past work on this topic, the algorithm is
much simpler (uses fewer parameters) and the guarantee does not make the
artificial assumption of piecewise constant subspace change, although it still
handles that setting. Secondly, we extend our approach and its analysis to
provably solving these problems when the raw data is federated and when the
over-air data communication modality is used for information exchange between
the $K$ peer nodes and the center.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Narayanamurthy_P/0/1/0/all/0/1"&gt;Praneeth Narayanamurthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vaswani_N/0/1/0/all/0/1"&gt;Namrata Vaswani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramamoorthy_A/0/1/0/all/0/1"&gt;Aditya Ramamoorthy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimating Smooth GLM in Non-interactive Local Differential Privacy Model with Public Unlabeled Data. (arXiv:1910.00482v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.00482</id>
        <link href="http://arxiv.org/abs/1910.00482"/>
        <updated>2021-06-23T01:48:41.527Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the problem of estimating smooth Generalized Linear
Models (GLM) in the Non-interactive Local Differential Privacy (NLDP) model.
Different from its classical setting, our model allows the server to access
some additional public but unlabeled data. By using Stein's lemma and its
variants, we first show that there is an $(\epsilon, \delta)$-NLDP algorithm
for GLM (under some mild assumptions), if each data record is i.i.d sampled
from some sub-Gaussian distribution with bounded $\ell_1$-norm. Then with high
probability, the sample complexity of the public and private data, for the
algorithm to achieve an $\alpha$ estimation error (in $\ell_\infty$-norm), is
$O(p^2\alpha^{-2})$ and ${O}(p^2\alpha^{-2}\epsilon^{-2})$, respectively, if
$\alpha$ is not too small ({\em i.e.,} $\alpha\geq
\Omega(\frac{1}{\sqrt{p}})$), where $p$ is the dimensionality of the data. This
is a significant improvement over the previously known quasi-polynomial (in
$\alpha$) or exponential (in $p$) complexity of GLM with no public data. Also,
our algorithm can answer multiple (at most $\exp(O(p))$) GLM queries with the
same sample complexities as in the one GLM query case with at least constant
probability. We then extend our idea to the non-linear regression problem and
show a similar phenomenon for it. Finally, we demonstrate the effectiveness of
our algorithms through experiments on both synthetic and real world datasets.
To our best knowledge, this is the first paper showing the existence of
efficient and effective algorithms for GLM and non-linear regression in the
NLDP model with public unlabeled data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Di Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1"&gt;Lijie Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huanyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaboardi_M/0/1/0/all/0/1"&gt;Marco Gaboardi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jinhui Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Preconditioned Riemannian Optimization on the Generalized Stiefel Manifold. (arXiv:1902.01635v3 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1902.01635</id>
        <link href="http://arxiv.org/abs/1902.01635"/>
        <updated>2021-06-23T01:48:41.520Z</updated>
        <summary type="html"><![CDATA[Optimization problems on the generalized Stiefel manifold (and products of
it) are prevalent across science and engineering. For example, in computational
science they arise in the symmetric (generalized) eigenvalue problem, in
nonlinear eigenvalue problems, and in electronic structures computations, to
name a few problems. In statistics and machine learning, they arise, for
example, in various dimensionality reduction techniques such as canonical
correlation analysis. In deep learning, regularization and improved stability
can be obtained by constraining some layers to have parameter matrices that
belong to the Stiefel manifold. Solving problems on the generalized Stiefel
manifold can be approached via the tools of Riemannian optimization. However,
using the standard geometric components for the generalized Stiefel manifold
has two possible shortcoming: computing some of the geometric components can be
too expensive and converge can be rather slow in certain cases. Both
shortcomings can be addressed using a technique called Riemannian
preconditioning, which amounts to using geometric components derived using a
precoditioner that defines a Riemannian metric on the constraint manifold. In
this paper we develop the geometric components required to perform Riemannian
optimization on the generalized Stiefel manifold equipped with a non-standard
metric, and illustrate theoretically and numerically the use of those
components and the effect of Riemannian preconditioning for solving
optimization problems on the generalized Stiefel manifold.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Shustin_B/0/1/0/all/0/1"&gt;Boris Shustin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Avron_H/0/1/0/all/0/1"&gt;Haim Avron&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting Deep Learning Models for Tabular Data. (arXiv:2106.11959v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11959</id>
        <link href="http://arxiv.org/abs/2106.11959"/>
        <updated>2021-06-23T01:48:41.510Z</updated>
        <summary type="html"><![CDATA[The necessity of deep learning for tabular data is still an unanswered
question addressed by a large number of research efforts. The recent literature
on tabular DL proposes several deep architectures reported to be superior to
traditional "shallow" models like Gradient Boosted Decision Trees. However,
since existing works often use different benchmarks and tuning protocols, it is
unclear if the proposed models universally outperform GBDT. Moreover, the
models are often not compared to each other, therefore, it is challenging to
identify the best deep model for practitioners.

In this work, we start from a thorough review of the main families of DL
models recently developed for tabular data. We carefully tune and evaluate them
on a wide range of datasets and reveal two significant findings. First, we show
that the choice between GBDT and DL models highly depends on data and there is
still no universally superior solution. Second, we demonstrate that a simple
ResNet-like architecture is a surprisingly effective baseline, which
outperforms most of the sophisticated models from the DL literature. Finally,
we design a simple adaptation of the Transformer architecture for tabular data
that becomes a new strong DL baseline and reduces the gap between GBDT and DL
models on datasets where GBDT dominates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gorishniy_Y/0/1/0/all/0/1"&gt;Yury Gorishniy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubachev_I/0/1/0/all/0/1"&gt;Ivan Rubachev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khrulkov_V/0/1/0/all/0/1"&gt;Valentin Khrulkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babenko_A/0/1/0/all/0/1"&gt;Artem Babenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Making Invisible Visible: Data-Driven Seismic Inversion with Physics-Informed Data Augmentation. (arXiv:2106.11892v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11892</id>
        <link href="http://arxiv.org/abs/2106.11892"/>
        <updated>2021-06-23T01:48:41.503Z</updated>
        <summary type="html"><![CDATA[Deep learning and data-driven approaches have shown great potential in
scientific domains. The promise of data-driven techniques relies on the
availability of a large volume of high-quality training datasets. Due to the
high cost of obtaining data through expensive physical experiments,
instruments, and simulations, data augmentation techniques for scientific
applications have emerged as a new direction for obtaining scientific data
recently. However, existing data augmentation techniques originating from
computer vision, yield physically unacceptable data samples that are not
helpful for the domain problems that we are interested in. In this paper, we
develop new physics-informed data augmentation techniques based on
convolutional neural networks. Specifically, our generative models leverage
different physics knowledge (such as governing equations, observable
perception, and physics phenomena) to improve the quality of the synthetic
data. To validate the effectiveness of our data augmentation techniques, we
apply them to solve a subsurface seismic full-waveform inversion using
simulated CO$_2$ leakage data. Our interest is to invert for subsurface
velocity models associated with very small CO$_2$ leakage. We validate the
performance of our methods using comprehensive numerical tests. Via comparison
and analysis, we show that data-driven seismic imaging can be significantly
enhanced by using our physics-informed data augmentation techniques.
Particularly, the imaging quality has been improved by 15% in test scenarios of
general-sized leakage and 17% in small-sized leakage when using an augmented
training set obtained with our techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yuxin Yang&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xitong Zhang&lt;/a&gt; (1 and 3), &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_Q/0/1/0/all/0/1"&gt;Qiang Guan&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Youzuo Lin&lt;/a&gt; (1) ((1) Earth and Environmental Sciences Division, Los Alamos National Laboratory, (2) Department of Computer Science, Kent State University, (3) Department of Computational Mathematics, Science and Engineering, Michigan State University)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deciding What to Learn: A Rate-Distortion Approach. (arXiv:2101.06197v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06197</id>
        <link href="http://arxiv.org/abs/2101.06197"/>
        <updated>2021-06-23T01:48:41.494Z</updated>
        <summary type="html"><![CDATA[Agents that learn to select optimal actions represent a prominent focus of
the sequential decision-making literature. In the face of a complex environment
or constraints on time and resources, however, aiming to synthesize such an
optimal policy can become infeasible. These scenarios give rise to an important
trade-off between the information an agent must acquire to learn and the
sub-optimality of the resulting policy. While an agent designer has a
preference for how this trade-off is resolved, existing approaches further
require that the designer translate these preferences into a fixed learning
target for the agent. In this work, leveraging rate-distortion theory, we
automate this process such that the designer need only express their
preferences via a single hyperparameter and the agent is endowed with the
ability to compute its own learning targets that best achieve the desired
trade-off. We establish a general bound on expected discounted regret for an
agent that decides what to learn in this manner along with computational
experiments that illustrate the expressiveness of designer preferences and even
show improvements over Thompson sampling in identifying an optimal policy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arumugam_D/0/1/0/all/0/1"&gt;Dilip Arumugam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1"&gt;Benjamin Van Roy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Supervised Momentum Contrastive Learning for Few-Shot Classification. (arXiv:2101.11058v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11058</id>
        <link href="http://arxiv.org/abs/2101.11058"/>
        <updated>2021-06-23T01:48:41.468Z</updated>
        <summary type="html"><![CDATA[Few-shot learning aims to transfer information from one task to enable
generalization on novel tasks given a few examples. This information is present
both in the domain and the class labels. In this work we investigate the
complementary roles of these two sources of information by combining
instance-discriminative contrastive learning and supervised learning in a
single framework called Supervised Momentum Contrastive learning (SUPMOCO). Our
approach avoids a problem observed in supervised learning where information in
images not relevant to the task is discarded, which hampers their
generalization to novel tasks. We show that (self-supervised) contrastive
learning and supervised learning are mutually beneficial, leading to a new
state-of-the-art on the META-DATASET - a recently introduced benchmark for
few-shot learning. Our method is based on a simple modification of MOCO and
scales better than prior work on combining supervised and self-supervised
learning. This allows us to easily combine data from multiple domains leading
to further improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Majumder_O/0/1/0/all/0/1"&gt;Orchid Majumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1"&gt;Avinash Ravichandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1"&gt;Subhransu Maji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1"&gt;Alessandro Achille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polito_M/0/1/0/all/0/1"&gt;Marzia Polito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Covariance Matrix Estimation in Stochastic Gradient Descent. (arXiv:2002.03979v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.03979</id>
        <link href="http://arxiv.org/abs/2002.03979"/>
        <updated>2021-06-23T01:48:41.459Z</updated>
        <summary type="html"><![CDATA[The stochastic gradient descent (SGD) algorithm is widely used for parameter
estimation, especially for huge data sets and online learning. While this
recursive algorithm is popular for computation and memory efficiency,
quantifying variability and randomness of the solutions has been rarely
studied. This paper aims at conducting statistical inference of SGD-based
estimates in an online setting. In particular, we propose a fully online
estimator for the covariance matrix of averaged SGD iterates (ASGD) only using
the iterates from SGD. We formally establish our online estimator's consistency
and show that the convergence rate is comparable to offline counterparts. Based
on the classic asymptotic normality results of ASGD, we construct
asymptotically valid confidence intervals for model parameters. Upon receiving
new observations, we can quickly update the covariance matrix estimate and the
confidence intervals. This approach fits in an online setting and takes full
advantage of SGD: efficiency in computation and memory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhu_W/0/1/0/all/0/1"&gt;Wanrong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wei Biao Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cogment: Open Source Framework For Distributed Multi-actor Training, Deployment & Operations. (arXiv:2106.11345v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.11345</id>
        <link href="http://arxiv.org/abs/2106.11345"/>
        <updated>2021-06-23T01:48:41.451Z</updated>
        <summary type="html"><![CDATA[Involving humans directly for the benefit of AI agents' training is getting
traction thanks to several advances in reinforcement learning and
human-in-the-loop learning. Humans can provide rewards to the agent,
demonstrate tasks, design a curriculum, or act in the environment, but these
benefits also come with architectural, functional design and engineering
complexities. We present Cogment, a unifying open-source framework that
introduces an actor formalism to support a variety of humans-agents
collaboration typologies and training approaches. It is also scalable out of
the box thanks to a distributed micro service architecture, and offers
solutions to the aforementioned complexities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Redefined_A/0/1/0/all/0/1"&gt;AI Redefined&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gottipati_S/0/1/0/all/0/1"&gt;Sai Krishna Gottipati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurandwad_S/0/1/0/all/0/1"&gt;Sagar Kurandwad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mars_C/0/1/0/all/0/1"&gt;Clod&amp;#xe9;ric Mars&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szriftgiser_G/0/1/0/all/0/1"&gt;Gregory Szriftgiser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chabot_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Chabot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Robustness vs Model Compression, or Both?. (arXiv:1903.12561v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1903.12561</id>
        <link href="http://arxiv.org/abs/1903.12561"/>
        <updated>2021-06-23T01:48:41.444Z</updated>
        <summary type="html"><![CDATA[It is well known that deep neural networks (DNNs) are vulnerable to
adversarial attacks, which are implemented by adding crafted perturbations onto
benign examples. Min-max robust optimization based adversarial training can
provide a notion of security against adversarial attacks. However, adversarial
robustness requires a significantly larger capacity of the network than that
for the natural training with only benign examples. This paper proposes a
framework of concurrent adversarial training and weight pruning that enables
model compression while still preserving the adversarial robustness and
essentially tackles the dilemma of adversarial training. Furthermore, this work
studies two hypotheses about weight pruning in the conventional setting and
finds that weight pruning is essential for reducing the network model size in
the adversarial setting, training a small model from scratch even with
inherited initialization from the large model cannot achieve both adversarial
robustness and high standard accuracy. Code is available at
https://github.com/yeshaokai/Robustness-Aware-Pruning-ADMM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1"&gt;Shaokai Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kaidi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sijia Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lambrechts_J/0/1/0/all/0/1"&gt;Jan-Henrik Lambrechts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1"&gt;Aojun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1"&gt;Kaisheng Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xue Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reusing Combinatorial Structure: Faster Iterative Projections over Submodular Base Polytopes. (arXiv:2106.11943v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11943</id>
        <link href="http://arxiv.org/abs/2106.11943"/>
        <updated>2021-06-23T01:48:41.436Z</updated>
        <summary type="html"><![CDATA[Optimization algorithms such as projected Newton's method, FISTA, mirror
descent and its variants enjoy near-optimal regret bounds and convergence
rates, but suffer from a computational bottleneck of computing "projections''
in potentially each iteration (e.g., $O(T^{1/2})$ regret of online mirror
descent). On the other hand, conditional gradient variants solve a linear
optimization in each iteration, but result in suboptimal rates (e.g.,
$O(T^{3/4})$ regret of online Frank-Wolfe). Motivated by this trade-off in
runtime v/s convergence rates, we consider iterative projections of close-by
points over widely-prevalent submodular base polytopes $B(f)$. We develop a
toolkit to speed up the computation of projections using both discrete and
continuous perspectives. We subsequently adapt the away-step Frank-Wolfe
algorithm to use this information and enable early termination. For the special
case of cardinality based submodular polytopes, we improve the runtime of
computing certain Bregman projections by a factor of $\Omega(n/\log(n))$. Our
theoretical results show orders of magnitude reduction in runtime in
preliminary computational experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moondra_J/0/1/0/all/0/1"&gt;Jai Moondra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mortagy_H/0/1/0/all/0/1"&gt;Hassan Mortagy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Swati Gupta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effective Semi-Supervised Node Classification on Few-Labeled Graph Data. (arXiv:1910.02684v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.02684</id>
        <link href="http://arxiv.org/abs/1910.02684"/>
        <updated>2021-06-23T01:48:41.416Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) are designed for semi-supervised node
classification on graphs where only a small subset of nodes have class labels.
However, under extreme cases when very few labels are available (e.g., 1
labeled node per class), GNNs suffer from severe result quality degradation.
Several existing studies make an initial effort to ease this situation, but are
still far from satisfactory.

In this paper, on few-labeled graph data, we propose an effective framework
ABN that is readily applicable to both shallow and deep GNN architectures and
significantly boosts classification accuracy. In particular, on a benchmark
dataset Cora with only 1 labeled node per class, while the classic graph
convolutional network (GCN) only has 44.6% accuracy, an immediate instantiation
of ABN over GCN achieves 62.5% accuracy; when applied to a deep architecture
DAGNN, ABN improves accuracy from 59.8% to 66.4%, which is state of the art.

ABN obtains superior performance through three main algorithmic designs.
First, it selects high-quality unlabeled nodes via an adaptive pseudo labeling
technique, so as to adaptively enhance the training process of GNNs. Second,
ABN balances the labels of the selected nodes on real-world skewed graph data
by pseudo label balancing. Finally, a negative sampling regularizer is designed
for ABN to further utilize the unlabeled nodes. The effectiveness of the three
techniques in ABN is well-validated by both theoretical and empirical analysis.
Extensive experiments, comparing 12 existing approaches on 4 benchmark
datasets, demonstrate that ABN achieves state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Ziang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jieming Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shengzhong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zengfeng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qing Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latency-Aware Neural Architecture Search with Multi-Objective Bayesian Optimization. (arXiv:2106.11890v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11890</id>
        <link href="http://arxiv.org/abs/2106.11890"/>
        <updated>2021-06-23T01:48:41.408Z</updated>
        <summary type="html"><![CDATA[When tuning the architecture and hyperparameters of large machine learning
models for on-device deployment, it is desirable to understand the optimal
trade-offs between on-device latency and model accuracy. In this work, we
leverage recent methodological advances in Bayesian optimization over
high-dimensional search spaces and multi-objective Bayesian optimization to
efficiently explore these trade-offs for a production-scale on-device natural
language understanding model at Facebook.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eriksson_D/0/1/0/all/0/1"&gt;David Eriksson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chuang_P/0/1/0/all/0/1"&gt;Pierce I-Jen Chuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daulton_S/0/1/0/all/0/1"&gt;Sam Daulton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aly_A/0/1/0/all/0/1"&gt;Ahmed Aly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babu_A/0/1/0/all/0/1"&gt;Arun Babu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Akshat Shrivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_P/0/1/0/all/0/1"&gt;Peng Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Shicong Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venkatesh_G/0/1/0/all/0/1"&gt;Ganesh Venkatesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balandat_M/0/1/0/all/0/1"&gt;Maximilian Balandat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Phasor Networks: Connecting Conventional and Spiking Neural Networks. (arXiv:2106.11908v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.11908</id>
        <link href="http://arxiv.org/abs/2106.11908"/>
        <updated>2021-06-23T01:48:41.402Z</updated>
        <summary type="html"><![CDATA[In this work, we extend standard neural networks by building upon an
assumption that neuronal activations correspond to the angle of a complex
number lying on the unit circle, or 'phasor.' Each layer in such a network
produces new activations by taking a weighted superposition of the previous
layer's phases and calculating the new phase value. This generalized
architecture allows models to reach high accuracy and carries the singular
advantage that mathematically equivalent versions of the network can be
executed with or without regard to a temporal variable. Importantly, the value
of a phase angle in the temporal domain can be sparsely represented by a
periodically repeating series of delta functions or 'spikes'. We demonstrate
the atemporal training of a phasor network on standard deep learning tasks and
show that these networks can then be executed in either the traditional
atemporal domain or spiking temporal domain with no conversion step needed.
This provides a novel basis for constructing deep networkswhich operate via
temporal, spike-based calculations suitable for neuromorphic computing
hardware.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Olin_Ammentorp_W/0/1/0/all/0/1"&gt;Wilkie Olin-Ammentorp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bazhenov_M/0/1/0/all/0/1"&gt;Maxim Bazhenov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NetFense: Adversarial Defenses against Privacy Attacks on Neural Networks for Graph Data. (arXiv:2106.11865v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11865</id>
        <link href="http://arxiv.org/abs/2106.11865"/>
        <updated>2021-06-23T01:48:41.396Z</updated>
        <summary type="html"><![CDATA[Recent advances in protecting node privacy on graph data and attacking graph
neural networks (GNNs) gain much attention. The eye does not bring these two
essential tasks together yet. Imagine an adversary can utilize the powerful
GNNs to infer users' private labels in a social network. How can we
adversarially defend against such privacy attacks while maintaining the utility
of perturbed graphs? In this work, we propose a novel research task,
adversarial defenses against GNN-based privacy attacks, and present a graph
perturbation-based approach, NetFense, to achieve the goal. NetFense can
simultaneously keep graph data unnoticeability (i.e., having limited changes on
the graph structure), maintain the prediction confidence of targeted label
classification (i.e., preserving data utility), and reduce the prediction
confidence of private label classification (i.e., protecting the privacy of
nodes). Experiments conducted on single- and multiple-target perturbations
using three real graph data exhibit that the perturbed graphs by NetFense can
effectively maintain data utility (i.e., model unnoticeability) on targeted
label classification and significantly decrease the prediction confidence of
private label classification (i.e., privacy protection). Extensive studies also
bring several insights, such as the flexibility of NetFense, preserving local
neighborhoods in data unnoticeability, and better privacy protection for
high-degree nodes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_I/0/1/0/all/0/1"&gt;I-Chung Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Cheng-Te Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dangers of Bayesian Model Averaging under Covariate Shift. (arXiv:2106.11905v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11905</id>
        <link href="http://arxiv.org/abs/2106.11905"/>
        <updated>2021-06-23T01:48:41.388Z</updated>
        <summary type="html"><![CDATA[Approximate Bayesian inference for neural networks is considered a robust
alternative to standard training, often providing good performance on
out-of-distribution data. However, Bayesian neural networks (BNNs) with
high-fidelity approximate inference via full-batch Hamiltonian Monte Carlo
achieve poor generalization under covariate shift, even underperforming
classical estimation. We explain this surprising result, showing how a Bayesian
model average can in fact be problematic under covariate shift, particularly in
cases where linear dependencies in the input features cause a lack of posterior
contraction. We additionally show why the same issue does not affect many
approximate inference procedures, or classical maximum a-posteriori (MAP)
training. Finally, we propose novel priors that improve the robustness of BNNs
to many sources of covariate shift.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Izmailov_P/0/1/0/all/0/1"&gt;Pavel Izmailov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nicholson_P/0/1/0/all/0/1"&gt;Patrick Nicholson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lotfi_S/0/1/0/all/0/1"&gt;Sanae Lotfi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1"&gt;Andrew Gordon Wilson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-Informed Deep Reversible Regression Model for Temperature Field Reconstruction of Heat-Source Systems. (arXiv:2106.11929v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11929</id>
        <link href="http://arxiv.org/abs/2106.11929"/>
        <updated>2021-06-23T01:48:41.368Z</updated>
        <summary type="html"><![CDATA[Temperature monitoring during the life time of heat-source components in
engineering systems becomes essential to ensure the normal work and even the
long working life of the heat sources. However, prior methods, which mainly use
the interpolate estimation, require large amounts of temperature tensors for an
accurate estimation. To solve this problem, this work develops a novel
physics-informed deep surrogate models for temperature field reconstruction.
First, we defines the temperature field reconstruction task of heat-source
systems. Then, this work develops the deep surrogate model mapping for the
proposed task. Finally, considering the physical properties of heat transfer,
this work proposes four different losses and joint learns the deep surrogate
model with these losses. Experimental studies have conducted over typical
two-dimensional heat-source systems to demonstrate the effectiveness and
efficiency of the proposed physics-informed deep surrogate models for
temperature field reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1"&gt;Zhiqiang Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Weien Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1"&gt;Wei Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1"&gt;Wen Yao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph coarsening: From scientific computing to machine learning. (arXiv:2106.11863v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11863</id>
        <link href="http://arxiv.org/abs/2106.11863"/>
        <updated>2021-06-23T01:48:41.361Z</updated>
        <summary type="html"><![CDATA[The general method of graph coarsening or graph reduction has been a
remarkably useful and ubiquitous tool in scientific computing and it is now
just starting to have a similar impact in machine learning. The goal of this
paper is to take a broad look into coarsening techniques that have been
successfully deployed in scientific computing and see how similar principles
are finding their way in more recent applications related to machine learning.
In scientific computing, coarsening plays a central role in algebraic multigrid
methods as well as the related class of multilevel incomplete LU
factorizations. In machine learning, graph coarsening goes under various names,
e.g., graph downsampling or graph reduction. Its goal in most cases is to
replace some original graph by one which has fewer nodes, but whose structure
and characteristics are similar to those of the original graph. As will be
seen, a common strategy in these methods is to rely on spectral properties to
define the coarse graph.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saad_Y/0/1/0/all/0/1"&gt;Yousef Saad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zechen Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compressive Statistical Learning with Random Feature Moments. (arXiv:1706.07180v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1706.07180</id>
        <link href="http://arxiv.org/abs/1706.07180"/>
        <updated>2021-06-23T01:48:41.355Z</updated>
        <summary type="html"><![CDATA[We describe a general framework -- compressive statistical learning -- for
resource-efficient large-scale learning: the training collection is compressed
in one pass into a low-dimensional sketch (a vector of random empirical
generalized moments) that captures the information relevant to the considered
learning task. A near-minimizer of the risk is computed from the sketch through
the solution of a nonlinear least squares problem. We investigate sufficient
sketch sizes to control the generalization error of this procedure. The
framework is illustrated on compressive PCA, compressive clustering, and
compressive Gaussian mixture Modeling with fixed known variance. The latter two
are further developed in a companion paper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Gribonval_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Gribonval&lt;/a&gt; (PANAMA, DANTE), &lt;a href="http://arxiv.org/find/stat/1/au:+Blanchard_G/0/1/0/all/0/1"&gt;Gilles Blanchard&lt;/a&gt; (DATASHAPE, LMO), &lt;a href="http://arxiv.org/find/stat/1/au:+Keriven_N/0/1/0/all/0/1"&gt;Nicolas Keriven&lt;/a&gt; (PANAMA, GIPSA-GAIA), &lt;a href="http://arxiv.org/find/stat/1/au:+Traonmilin_Y/0/1/0/all/0/1"&gt;Yann Traonmilin&lt;/a&gt; (PANAMA, IMB)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From SIR to SEAIRD: a novel data-driven modeling approach based on the Grey-box System Theory to predict the dynamics of COVID-19. (arXiv:2106.11918v1 [stat.AP])]]></title>
        <id>http://arxiv.org/abs/2106.11918</id>
        <link href="http://arxiv.org/abs/2106.11918"/>
        <updated>2021-06-23T01:48:41.347Z</updated>
        <summary type="html"><![CDATA[Common compartmental modeling for COVID-19 is based on a priori knowledge and
numerous assumptions. Additionally, they do not systematically incorporate
asymptomatic cases. Our study aimed at providing a framework for data-driven
approaches, by leveraging the strengths of the grey-box system theory or
grey-box identification, known for its robustness in problem solving under
partial, incomplete, or uncertain data. Empirical data on confirmed cases and
deaths, extracted from an open source repository were used to develop the
SEAIRD compartment model. Adjustments were made to fit current knowledge on the
COVID-19 behavior. The model was implemented and solved using an Ordinary
Differential Equation solver and an optimization tool. A cross-validation
technique was applied, and the coefficient of determination $R^2$ was computed
in order to evaluate the goodness-of-fit of the model. %to the data. Key
epidemiological parameters were finally estimated and we provided the rationale
for the construction of SEAIRD model. When applied to Brazil's cases, SEAIRD
produced an excellent agreement to the data, with an %coefficient of
determination $R^2$ $\geq 90\%$. The probability of COVID-19 transmission was
generally high ($\geq 95\%$). On the basis of a 20-day modeling data, the
incidence rate of COVID-19 was as low as 3 infected cases per 100,000 exposed
persons in Brazil and France. Within the same time frame, the fatality rate of
COVID-19 was the highest in France (16.4\%) followed by Brazil (6.9\%), and the
lowest in Russia ($\leq 1\%$). SEAIRD represents an asset for modeling
infectious diseases in their dynamical stable phase, especially for new viruses
when pathophysiology knowledge is very limited.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Pekpe_K/0/1/0/all/0/1"&gt;Komi Midzodzi P&amp;#xe9;kp&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zitouni_D/0/1/0/all/0/1"&gt;Djamel Zitouni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gasso_G/0/1/0/all/0/1"&gt;Gilles Gasso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dhifli_W/0/1/0/all/0/1"&gt;Wajdi Dhifli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Guinhouya_B/0/1/0/all/0/1"&gt;Benjamin C. Guinhouya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Asynchronous Stochastic Optimization Robust to Arbitrary Delays. (arXiv:2106.11879v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.11879</id>
        <link href="http://arxiv.org/abs/2106.11879"/>
        <updated>2021-06-23T01:48:41.338Z</updated>
        <summary type="html"><![CDATA[We consider stochastic optimization with delayed gradients where, at each
time step $t$, the algorithm makes an update using a stale stochastic gradient
from step $t - d_t$ for some arbitrary delay $d_t$. This setting abstracts
asynchronous distributed optimization where a central server receives gradient
updates computed by worker machines. These machines can experience computation
and communication loads that might vary significantly over time. In the general
non-convex smooth optimization setting, we give a simple and efficient
algorithm that requires $O( \sigma^2/\epsilon^4 + \tau/\epsilon^2 )$ steps for
finding an $\epsilon$-stationary point $x$, where $\tau$ is the \emph{average}
delay $\smash{\frac{1}{T}\sum_{t=1}^T d_t}$ and $\sigma^2$ is the variance of
the stochastic gradients. This improves over previous work, which showed that
stochastic gradient decent achieves the same rate but with respect to the
\emph{maximal} delay $\max_{t} d_t$, that can be significantly larger than the
average delay especially in heterogeneous distributed systems. Our experiments
demonstrate the efficacy and robustness of our algorithm in cases where the
delay distribution is skewed or heavy-tailed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Cohen_A/0/1/0/all/0/1"&gt;Alon Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Daniely_A/0/1/0/all/0/1"&gt;Amit Daniely&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Drori_Y/0/1/0/all/0/1"&gt;Yoel Drori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Koren_T/0/1/0/all/0/1"&gt;Tomer Koren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Schain_M/0/1/0/all/0/1"&gt;Mariano Schain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RootPainter3D: Interactive-machine-learning enables rapid and accurate contouring for radiotherapy. (arXiv:2106.11942v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11942</id>
        <link href="http://arxiv.org/abs/2106.11942"/>
        <updated>2021-06-23T01:48:41.317Z</updated>
        <summary type="html"><![CDATA[Organ-at-risk contouring is still a bottleneck in radiotherapy, with many
deep learning methods falling short of promised results when evaluated on
clinical data. We investigate the accuracy and time-savings resulting from the
use of an interactive-machine-learning method for an organ-at-risk contouring
task. We compare the method to the Eclipse contouring software and find strong
agreement with manual delineations, with a dice score of 0.95. The annotations
created using corrective-annotation also take less time to create as more
images are annotated, resulting in substantial time savings compared to manual
methods, with hearts that take 2 minutes and 2 seconds to delineate on average,
after 923 images have been delineated, compared to 7 minutes and 1 seconds when
delineating manually. Our experiment demonstrates that
interactive-machine-learning with corrective-annotation provides a fast and
accessible way for non computer-scientists to train deep-learning models to
segment their own structures of interest as part of routine clinical workflows.

Source code is available at
\href{https://github.com/Abe404/RootPainter3D}{this HTTPS URL}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smith_A/0/1/0/all/0/1"&gt;Abraham George Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petersen_J/0/1/0/all/0/1"&gt;Jens Petersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Terrones_Campos_C/0/1/0/all/0/1"&gt;Cynthia Terrones-Campos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berthelsen_A/0/1/0/all/0/1"&gt;Anne Kiil Berthelsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forbes_N/0/1/0/all/0/1"&gt;Nora Jarrett Forbes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darkner_S/0/1/0/all/0/1"&gt;Sune Darkner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Specht_L/0/1/0/all/0/1"&gt;Lena Specht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogelius_I/0/1/0/all/0/1"&gt;Ivan Richter Vogelius&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably Efficient Representation Learning in Low-rank Markov Decision Processes. (arXiv:2106.11935v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11935</id>
        <link href="http://arxiv.org/abs/2106.11935"/>
        <updated>2021-06-23T01:48:41.309Z</updated>
        <summary type="html"><![CDATA[The success of deep reinforcement learning (DRL) is due to the power of
learning a representation that is suitable for the underlying exploration and
exploitation task. However, existing provable reinforcement learning algorithms
with linear function approximation often assume the feature representation is
known and fixed. In order to understand how representation learning can improve
the efficiency of RL, we study representation learning for a class of low-rank
Markov Decision Processes (MDPs) where the transition kernel can be represented
in a bilinear form. We propose a provably efficient algorithm called ReLEX that
can simultaneously learn the representation and perform exploration. We show
that ReLEX always performs no worse than a state-of-the-art algorithm without
representation learning, and will be strictly better in terms of sample
efficiency if the function class of representations enjoys a certain mild
"coverage'' property over the whole state-action space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weitong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jiafan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Dongruo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Amy Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1"&gt;Quanquan Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MONCAE: Multi-Objective Neuroevolution of Convolutional Autoencoders. (arXiv:2106.11914v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.11914</id>
        <link href="http://arxiv.org/abs/2106.11914"/>
        <updated>2021-06-23T01:48:41.303Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a novel neuroevolutionary method to identify the
architecture and hyperparameters of convolutional autoencoders. Remarkably, we
used a hypervolume indicator in the context of neural architecture search for
autoencoders, for the first time to our current knowledge. Results show that
images were compressed by a factor of more than 10, while still retaining
enough information to achieve image classification for the majority of the
tasks. Thus, this new approach can be used to speed up the AutoML pipeline for
image compression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dimanov_D/0/1/0/all/0/1"&gt;Daniel Dimanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balaguer_Ballester_E/0/1/0/all/0/1"&gt;Emili Balaguer-Ballester&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singleton_C/0/1/0/all/0/1"&gt;Colin Singleton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rostami_S/0/1/0/all/0/1"&gt;Shahin Rostami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notes on the H-measure of classifier performance. (arXiv:2106.11888v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11888</id>
        <link href="http://arxiv.org/abs/2106.11888"/>
        <updated>2021-06-23T01:48:41.293Z</updated>
        <summary type="html"><![CDATA[The H-measure is a classifier performance measure which takes into account
the context of application without requiring a rigid value of relative
misclassification costs to be set. Since its introduction in 2009 it has become
widely adopted. This paper answers various queries which users have raised
since its introduction, including questions about its interpretation, the
choice of a weighting function, whether it is strictly proper, and its
coherence, and relates the measure to other work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hand_D/0/1/0/all/0/1"&gt;D. J. Hand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anagnostopoulos_C/0/1/0/all/0/1"&gt;C. Anagnostopoulos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local policy search with Bayesian optimization. (arXiv:2106.11899v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11899</id>
        <link href="http://arxiv.org/abs/2106.11899"/>
        <updated>2021-06-23T01:48:41.286Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning (RL) aims to find an optimal policy by interaction
with an environment. Consequently, learning complex behavior requires a vast
number of samples, which can be prohibitive in practice. Nevertheless, instead
of systematically reasoning and actively choosing informative samples, policy
gradients for local search are often obtained from random perturbations. These
random samples yield high variance estimates and hence are sub-optimal in terms
of sample complexity. Actively selecting informative samples is at the core of
Bayesian optimization, which constructs a probabilistic surrogate of the
objective from past samples to reason about informative subsequent ones. In
this paper, we propose to join both worlds. We develop an algorithm utilizing a
probabilistic model of the objective function and its gradient. Based on the
model, the algorithm decides where to query a noisy zeroth-order oracle to
improve the gradient estimates. The resulting algorithm is a novel type of
policy search method, which we compare to existing black-box algorithms. The
comparison reveals improved sample complexity and reduced variance in extensive
empirical evaluations on synthetic objectives. Further, we highlight the
benefits of active sampling on popular RL benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Muller_S/0/1/0/all/0/1"&gt;Sarah M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rohr_A/0/1/0/all/0/1"&gt;Alexander von Rohr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trimpe_S/0/1/0/all/0/1"&gt;Sebastian Trimpe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Off-Policy Reinforcement Learning with Delayed Rewards. (arXiv:2106.11854v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11854</id>
        <link href="http://arxiv.org/abs/2106.11854"/>
        <updated>2021-06-23T01:48:41.278Z</updated>
        <summary type="html"><![CDATA[We study deep reinforcement learning (RL) algorithms with delayed rewards. In
many real-world tasks, instant rewards are often not readily accessible or even
defined immediately after the agent performs actions. In this work, we first
formally define the environment with delayed rewards and discuss the challenges
raised due to the non-Markovian nature of such environments. Then, we introduce
a general off-policy RL framework with a new Q-function formulation that can
handle the delayed rewards with theoretical convergence guarantees. For
practical tasks with high dimensional state spaces, we further introduce the
HC-decomposition rule of the Q-function in our framework which naturally leads
to an approximation scheme that helps boost the training efficiency and
stability. We finally conduct extensive experiments to demonstrate the superior
performance of our algorithms over the existing work and their variants.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Beining Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1"&gt;Zhizhou Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zuofan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jian Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple Organ Failure Prediction with Classifier-Guided Generative Adversarial Imputation Networks. (arXiv:2106.11878v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11878</id>
        <link href="http://arxiv.org/abs/2106.11878"/>
        <updated>2021-06-23T01:48:41.247Z</updated>
        <summary type="html"><![CDATA[Multiple organ failure (MOF) is a severe syndrome with a high mortality rate
among Intensive Care Unit (ICU) patients. Early and precise detection is
critical for clinicians to make timely decisions. An essential challenge in
applying machine learning models to electronic health records (EHRs) is the
pervasiveness of missing values. Most existing imputation methods are involved
in the data preprocessing phase, failing to capture the relationship between
data and outcome for downstream predictions. In this paper, we propose
classifier-guided generative adversarial imputation networks Classifier-GAIN)
for MOF prediction to bridge this gap, by incorporating both observed data and
label information. Specifically, the classifier takes imputed values from the
generator(imputer) to predict task outcomes and provides additional supervision
signals to the generator by joint training. The classifier-guide generator
imputes missing values with label-awareness during training, improving the
classifier's performance during inference. We conduct extensive experiments
showing that our approach consistently outperforms classical and state-of-art
neural baselines across a range of missing data scenarios and evaluation
metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xinlu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yun Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Callcut_R/0/1/0/all/0/1"&gt;Rachael Callcut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petzold_L/0/1/0/all/0/1"&gt;Linda Petzold&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Algorithmic Recourse in Partially and Fully Confounded Settings Through Bounding Counterfactual Effects. (arXiv:2106.11849v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.11849</id>
        <link href="http://arxiv.org/abs/2106.11849"/>
        <updated>2021-06-23T01:48:41.236Z</updated>
        <summary type="html"><![CDATA[Algorithmic recourse aims to provide actionable recommendations to
individuals to obtain a more favourable outcome from an automated
decision-making system. As it involves reasoning about interventions performed
in the physical world, recourse is fundamentally a causal problem. Existing
methods compute the effect of recourse actions using a causal model learnt from
data under the assumption of no hidden confounding and modelling assumptions
such as additive noise. Building on the seminal work of Balke and Pearl (1994),
we propose an alternative approach for discrete random variables which relaxes
these assumptions and allows for unobserved confounding and arbitrary
structural equations. The proposed approach only requires specification of the
causal graph and confounding structure and bounds the expected counterfactual
effect of recourse actions. If the lower bound is above a certain threshold,
i.e., on the other side of the decision boundary, recourse is guaranteed in
expectation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kugelgen_J/0/1/0/all/0/1"&gt;Julius von K&amp;#xfc;gelgen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Agarwal_N/0/1/0/all/0/1"&gt;Nikita Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zeitler_J/0/1/0/all/0/1"&gt;Jakob Zeitler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mastouri_A/0/1/0/all/0/1"&gt;Afsaneh Mastouri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Reducing Labeling Cost in Deep Object Detection. (arXiv:2106.11921v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11921</id>
        <link href="http://arxiv.org/abs/2106.11921"/>
        <updated>2021-06-23T01:48:41.227Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have reached very high accuracy on object detection but
their success hinges on large amounts of labeled data. To reduce the dependency
on labels, various active-learning strategies have been proposed, typically
based on the confidence of the detector. However, these methods are biased
towards best-performing classes and can lead to acquired datasets that are not
good representatives of the data in the testing set. In this work, we propose a
unified framework for active learning, that considers both the uncertainty and
the robustness of the detector, ensuring that the network performs accurately
in all classes. Furthermore, our method is able to pseudo-label the very
confident predictions, suppressing a potential distribution drift while further
boosting the performance of the model. Experiments show that our method
comprehensively outperforms a wide range of active-learning methods on PASCAL
VOC07+12 and MS-COCO, having up to a 7.7% relative improvement, or up to 82%
reduction in labeling cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elezi_I/0/1/0/all/0/1"&gt;Ismail Elezi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhiding Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1"&gt;Laura Leal-Taixe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1"&gt;Jose M. Alvarez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Failing with Grace: Learning Neural Network Controllers that are Boundedly Unsafe. (arXiv:2106.11881v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2106.11881</id>
        <link href="http://arxiv.org/abs/2106.11881"/>
        <updated>2021-06-23T01:48:41.219Z</updated>
        <summary type="html"><![CDATA[In this work, we consider the problem of learning a feed-forward neural
network (NN) controller to safely steer an arbitrarily shaped planar robot in a
compact and obstacle-occluded workspace. Unlike existing methods that depend
strongly on the density of data points close to the boundary of the safe state
space to train NN controllers with closed-loop safety guarantees, we propose an
approach that lifts such assumptions on the data that are hard to satisfy in
practice and instead allows for graceful safety violations, i.e., of a bounded
magnitude that can be spatially controlled. To do so, we employ reachability
analysis methods to encapsulate safety constraints in the training process.
Specifically, to obtain a computationally efficient over-approximation of the
forward reachable set of the closed-loop system, we partition the robot's state
space into cells and adaptively subdivide the cells that contain states which
may escape the safe set under the trained control law. To do so, we first
design appropriate under- and over-approximations of the robot's footprint to
adaptively subdivide the configuration space into cells. Then, using the
overlap between each cell's forward reachable set and the set of infeasible
robot configurations as a measure for safety violations, we introduce penalty
terms into the loss function that penalize this overlap in the training
process. As a result, our method can learn a safe vector field for the
closed-loop system and, at the same time, provide numerical worst-case bounds
on safety violation over the whole configuration space, defined by the overlap
between the over-approximation of the forward reachable set of the closed-loop
system and the set of unsafe states. Moreover, it can control the tradeoff
between computational complexity and tightness of these bounds. Finally, we
provide a simulation study that verifies the efficacy of the proposed scheme.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Vlantis_P/0/1/0/all/0/1"&gt;Panagiotis Vlantis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zavlanos_M/0/1/0/all/0/1"&gt;Michael M. Zavlanos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine learning for risk assessment in gender-based crime. (arXiv:2106.11847v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2106.11847</id>
        <link href="http://arxiv.org/abs/2106.11847"/>
        <updated>2021-06-23T01:48:41.212Z</updated>
        <summary type="html"><![CDATA[Gender-based crime is one of the most concerning scourges of contemporary
society. Governments worldwide have invested lots of economic and human
resources to radically eliminate this threat. Despite these efforts, providing
accurate predictions of the risk that a victim of gender violence has of being
attacked again is still a very hard open problem. The development of new
methods for issuing accurate, fair and quick predictions would allow police
forces to select the most appropriate measures to prevent recidivism. In this
work, we propose to apply Machine Learning (ML) techniques to create models
that accurately predict the recidivism risk of a gender-violence offender. The
relevance of the contribution of this work is threefold: (i) the proposed ML
method outperforms the preexisting risk assessment algorithm based on classical
statistical techniques, (ii) the study has been conducted through an official
specific-purpose database with more than 40,000 reports of gender violence, and
(iii) two new quality measures are proposed for assessing the effective police
protection that a model supplies and the overload in the invested resources
that it generates. Additionally, we propose a hybrid model that combines the
statistical prediction methods with the ML method, permitting authorities to
implement a smooth transition from the preexisting model to the ML-based model.
This hybrid nature enables a decision-making process to optimally balance
between the efficiency of the police system and aggressiveness of the
protection measures taken.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Prieto_A/0/1/0/all/0/1"&gt;&amp;#xc1;ngel Gonz&amp;#xe1;lez-Prieto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bru_A/0/1/0/all/0/1"&gt;Antonio Br&amp;#xfa;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nuno_J/0/1/0/all/0/1"&gt;Juan Carlos Nu&amp;#xf1;o&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Alvarez_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Luis Gonz&amp;#xe1;lez-&amp;#xc1;lvarez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MEAL: Manifold Embedding-based Active Learning. (arXiv:2106.11858v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11858</id>
        <link href="http://arxiv.org/abs/2106.11858"/>
        <updated>2021-06-23T01:48:41.189Z</updated>
        <summary type="html"><![CDATA[Image segmentation is a common and challenging task in autonomous driving.
Availability of sufficient pixel-level annotations for the training data is a
hurdle. Active learning helps learning from small amounts of data by suggesting
the most promising samples for labeling. In this work, we propose a new
pool-based method for active learning, which proposes promising image regions,
in each acquisition step. The problem is framed in an exploration-exploitation
framework by combining an embedding based on Uniform Manifold Approximation to
model representativeness with entropy as uncertainty measure to model
informativeness. We applied our proposed method to the challenging autonomous
driving data sets CamVid and Cityscapes and performed a quantitative comparison
with state-of-the-art methods. We find that our active learning method achieves
better performance on CamVid compared to other methods, while on Cityscapes,
the performance lift was negligible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sreenivasaiah_D/0/1/0/all/0/1"&gt;Deepthi Sreenivasaiah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wollmann_T/0/1/0/all/0/1"&gt;Thomas Wollmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Any equation is a forest: Symbolic genetic algorithm for discovering open-form partial differential equations (SGA-PDE). (arXiv:2106.11927v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.11927</id>
        <link href="http://arxiv.org/abs/2106.11927"/>
        <updated>2021-06-23T01:48:41.183Z</updated>
        <summary type="html"><![CDATA[Partial differential equations (PDEs) are concise and understandable
representations of domain knowledge, which are essential for deepening our
understanding of physical processes and predicting future responses. However,
the PDEs of many real-world problems are uncertain, which calls for PDE
discovery. We propose the symbolic genetic algorithm (SGA-PDE) to discover
open-form PDEs directly from data without prior knowledge about the equation
structure. SGA-PDE focuses on the representation and optimization of PDE.
Firstly, SGA-PDE uses symbolic mathematics to realize the flexible
representation of any given PDE, transforms a PDE into a forest, and converts
each function term into a binary tree. Secondly, SGA-PDE adopts a specially
designed genetic algorithm to efficiently optimize the binary trees by
iteratively updating the tree topology and node attributes. The SGA-PDE is
gradient-free, which is a desirable characteristic in PDE discovery since it is
difficult to obtain the gradient between the PDE loss and the PDE structure. In
the experiment, SGA-PDE not only successfully discovered nonlinear Burgers'
equation, Korteweg-de Vries (KdV) equation, and Chafee-Infante equation, but
also handled PDEs with fractional structure and compound functions that cannot
be solved by conventional PDE discovery methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuntian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yingtao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongxiao Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Customer Embeddings for Financial Service Applications. (arXiv:2106.11880v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11880</id>
        <link href="http://arxiv.org/abs/2106.11880"/>
        <updated>2021-06-23T01:48:41.175Z</updated>
        <summary type="html"><![CDATA[As financial services (FS) companies have experienced drastic technology
driven changes, the availability of new data streams provides the opportunity
for more comprehensive customer understanding. We propose Dynamic Customer
Embeddings (DCE), a framework that leverages customers' digital activity and a
wide range of financial context to learn dense representations of customers in
the FS industry. Our method examines customer actions and pageviews within a
mobile or web digital session, the sequencing of the sessions themselves, and
snapshots of common financial features across our organization at the time of
login. We test our customer embeddings using real world data in three
prediction problems: 1) the intent of a customer in their next digital session,
2) the probability of a customer calling the call centers after a session, and
3) the probability of a digital session to be fraudulent. DCE showed
performance lift in all three downstream problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chitsazan_N/0/1/0/all/0/1"&gt;Nima Chitsazan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharpe_S/0/1/0/all/0/1"&gt;Samuel Sharpe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katariya_D/0/1/0/all/0/1"&gt;Dwipam Katariya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1"&gt;Qianyu Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajasethupathy_K/0/1/0/all/0/1"&gt;Karthik Rajasethupathy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy Amplification via Iteration for Shuffled and Online PNSGD. (arXiv:2106.11767v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.11767</id>
        <link href="http://arxiv.org/abs/2106.11767"/>
        <updated>2021-06-23T01:48:41.169Z</updated>
        <summary type="html"><![CDATA[In this paper, we consider the framework of privacy amplification via
iteration, which is originally proposed by Feldman et al. and subsequently
simplified by Asoodeh et al. in their analysis via the contraction coefficient.
This line of work focuses on the study of the privacy guarantees obtained by
the projected noisy stochastic gradient descent (PNSGD) algorithm with hidden
intermediate updates. A limitation in the existing literature is that only the
early stopped PNSGD has been studied, while no result has been proved on the
more widely-used PNSGD applied on a shuffled dataset. Moreover, no scheme has
been yet proposed regarding how to decrease the injected noise when new data
are received in an online fashion. In this work, we first prove a privacy
guarantee for shuffled PNSGD, which is investigated asymptotically when the
noise is fixed for each sample size $n$ but reduced at a predetermined rate
when $n$ increases, in order to achieve the convergence of privacy loss. We
then analyze the online setting and provide a faster decaying scheme for the
magnitude of the injected noise that also guarantees the convergence of privacy
loss.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sordello_M/0/1/0/all/0/1"&gt;Matteo Sordello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1"&gt;Zhiqi Bu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Jinshuo Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lower and Upper Bounds on the VC-Dimension of Tensor Network Models. (arXiv:2106.11827v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11827</id>
        <link href="http://arxiv.org/abs/2106.11827"/>
        <updated>2021-06-23T01:48:41.161Z</updated>
        <summary type="html"><![CDATA[Tensor network methods have been a key ingredient of advances in condensed
matter physics and have recently sparked interest in the machine learning
community for their ability to compactly represent very high-dimensional
objects. Tensor network methods can for example be used to efficiently learn
linear models in exponentially large feature spaces [Stoudenmire and Schwab,
2016]. In this work, we derive upper and lower bounds on the VC dimension and
pseudo-dimension of a large class of tensor network models for classification,
regression and completion. Our upper bounds hold for linear models
parameterized by arbitrary tensor network structures, and we derive lower
bounds for common tensor decomposition models~(CP, Tensor Train, Tensor Ring
and Tucker) showing the tightness of our general upper bound. These results are
used to derive a generalization bound which can be applied to classification
with low rank matrices as well as linear classifiers based on any of the
commonly used tensor decomposition models. As a corollary of our results, we
obtain a bound on the VC dimension of the matrix product state classifier
introduced in [Stoudenmire and Schwab, 2016] as a function of the so-called
bond dimension~(i.e. tensor train rank), which answers an open problem listed
by Cirac, Garre-Rubio and P\'erez-Garc\'ia in [Cirac et al., 2019].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khavari_B/0/1/0/all/0/1"&gt;Behnoush Khavari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabusseau_G/0/1/0/all/0/1"&gt;Guillaume Rabusseau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enabling Long-Term Cooperation in Cross-Silo Federated Learning: A Repeated Game Perspective. (arXiv:2106.11814v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11814</id>
        <link href="http://arxiv.org/abs/2106.11814"/>
        <updated>2021-06-23T01:48:41.141Z</updated>
        <summary type="html"><![CDATA[Cross-silo federated learning (FL) is a distributed learning approach where
clients train a global model cooperatively while keeping their local data
private. Different from cross-device FL, clients in cross-silo FL are usually
organizations or companies which may execute multiple cross-silo FL processes
repeatedly due to their time-varying local data sets, and aim to optimize their
long-term benefits by selfishly choosing their participation levels. While
there has been some work on incentivizing clients to join FL, the analysis of
the long-term selfish participation behaviors of clients in cross-silo FL
remains largely unexplored. In this paper, we analyze the selfish participation
behaviors of heterogeneous clients in cross-silo FL. Specifically, we model the
long-term selfish participation behaviors of clients as an infinitely repeated
game, with the stage game being a selfish participation game in one cross-silo
FL process (SPFL). For the stage game SPFL, we derive the unique Nash
equilibrium (NE), and propose a distributed algorithm for each client to
calculate its equilibrium participation strategy. For the long-term
interactions among clients, we derive a cooperative strategy for clients which
minimizes the number of free riders while increasing the amount of local data
for model training. We show that enforced by a punishment strategy, such a
cooperative strategy is a SPNE of the infinitely repeated game, under which
some clients who are free riders at the NE of the stage game choose to be
(partial) contributors. We further propose an algorithm to calculate the
optimal SPNE which minimizes the number of free riders while maximizing the
amount of local data for model training. Simulation results show that our
proposed cooperative strategy at the optimal SPNE can effectively reduce the
number of free riders and increase the amount of local data for model training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Ning Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1"&gt;Qian Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evo* 2021 -- Late-Breaking Abstracts Volume. (arXiv:2106.11804v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.11804</id>
        <link href="http://arxiv.org/abs/2106.11804"/>
        <updated>2021-06-23T01:48:41.134Z</updated>
        <summary type="html"><![CDATA[Volumen with the Late-Breaking Abstracts submitted to the Evo* 2021
Conference, held online from 7 to 9 of April 2021. These papers present ongoing
research and preliminary results investigating on the application of different
approaches of Bioinspired Methods (mainly Evolutionary Computation) to
different problems, most of them real world ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mora_A/0/1/0/all/0/1"&gt;A.M. Mora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Esparcia_Alcazar_A/0/1/0/all/0/1"&gt;A.I. Esparcia-Alc&amp;#xe1;zar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emphatic Algorithms for Deep Reinforcement Learning. (arXiv:2106.11779v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11779</id>
        <link href="http://arxiv.org/abs/2106.11779"/>
        <updated>2021-06-23T01:48:41.128Z</updated>
        <summary type="html"><![CDATA[Off-policy learning allows us to learn about possible policies of behavior
from experience generated by a different behavior policy. Temporal difference
(TD) learning algorithms can become unstable when combined with function
approximation and off-policy sampling - this is known as the ''deadly triad''.
Emphatic temporal difference (ETD($\lambda$)) algorithm ensures convergence in
the linear case by appropriately weighting the TD($\lambda$) updates. In this
paper, we extend the use of emphatic methods to deep reinforcement learning
agents. We show that naively adapting ETD($\lambda$) to popular deep
reinforcement learning algorithms, which use forward view multi-step returns,
results in poor performance. We then derive new emphatic algorithms for use in
the context of such algorithms, and we demonstrate that they provide noticeable
benefits in small problems designed to highlight the instability of TD methods.
Finally, we observed improved performance when applying these algorithms at
scale on classic Atari games from the Arcade Learning Environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1"&gt;Ray Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zahavy_T/0/1/0/all/0/1"&gt;Tom Zahavy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhongwen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1"&gt;Adam White&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hessel_M/0/1/0/all/0/1"&gt;Matteo Hessel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blundell_C/0/1/0/all/0/1"&gt;Charles Blundell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasselt_H/0/1/0/all/0/1"&gt;Hado van Hasselt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Credal Self-Supervised Learning. (arXiv:2106.11853v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.11853</id>
        <link href="http://arxiv.org/abs/2106.11853"/>
        <updated>2021-06-23T01:48:41.121Z</updated>
        <summary type="html"><![CDATA[Self-training is an effective approach to semi-supervised learning. The key
idea is to let the learner itself iteratively generate "pseudo-supervision" for
unlabeled instances based on its current hypothesis. In combination with
consistency regularization, pseudo-labeling has shown promising performance in
various domains, for example in computer vision. To account for the
hypothetical nature of the pseudo-labels, these are commonly provided in the
form of probability distributions. Still, one may argue that even a probability
distribution represents an excessive level of informedness, as it suggests that
the learner precisely knows the ground-truth conditional probabilities. In our
approach, we therefore allow the learner to label instances in the form of
credal sets, that is, sets of (candidate) probability distributions. Thanks to
this increased expressiveness, the learner is able to represent uncertainty and
a lack of knowledge in a more flexible and more faithful manner. To learn from
weakly labeled data of that kind, we leverage methods that have recently been
proposed in the realm of so-called superset learning. In an exhaustive
empirical evaluation, we compare our methodology to state-of-the-art
self-supervision approaches, showing competitive to superior performance
especially in low-label scenarios incorporating a high degree of uncertainty.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lienen_J/0/1/0/all/0/1"&gt;Julian Lienen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hullermeier_E/0/1/0/all/0/1"&gt;Eyke H&amp;#xfc;llermeier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speeding Up OPFython with Numba. (arXiv:2106.11828v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11828</id>
        <link href="http://arxiv.org/abs/2106.11828"/>
        <updated>2021-06-23T01:48:41.114Z</updated>
        <summary type="html"><![CDATA[A graph-inspired classifier, known as Optimum-Path Forest (OPF), has proven
to be a state-of-the-art algorithm comparable to Logistic Regressors, Support
Vector Machines in a wide variety of tasks. Recently, its Python-based version,
denoted as OPFython, has been proposed to provide a more friendly framework and
a faster prototyping environment. Nevertheless, Python-based algorithms are
slower than their counterpart C-based algorithms, impacting their performance
when confronted with large amounts of data. Therefore, this paper proposed a
simple yet highly efficient speed up using the Numba package, which accelerates
Numpy-based calculations and attempts to increase the algorithm's overall
performance. Experimental results showed that the proposed approach achieved
better results than the na\"ive Python-based OPF and speeded up its distance
measurement calculation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosa_G/0/1/0/all/0/1"&gt;Gustavo H. de Rosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papa_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Paulo Papa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the importance of cross-task features for class-incremental learning. (arXiv:2106.11930v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11930</id>
        <link href="http://arxiv.org/abs/2106.11930"/>
        <updated>2021-06-23T01:48:41.095Z</updated>
        <summary type="html"><![CDATA[In class-incremental learning, an agent with limited resources needs to learn
a sequence of classification tasks, forming an ever growing classification
problem, with the constraint of not being able to access data from previous
tasks. The main difference with task-incremental learning, where a task-ID is
available at inference time, is that the learner also needs to perform
cross-task discrimination, i.e. distinguish between classes that have not been
seen together. Approaches to tackle this problem are numerous and mostly make
use of an external memory (buffer) of non-negligible size. In this paper, we
ablate the learning of cross-task features and study its influence on the
performance of basic replay strategies used for class-IL. We also define a new
forgetting measure for class-incremental learning, and see that forgetting is
not the principal cause of low performance. Our experimental results show that
future algorithms for class-incremental learning should not only prevent
forgetting, but also aim to improve the quality of the cross-task features.
This is especially important when the number of classes per task is small.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soutif__Cormerais_A/0/1/0/all/0/1"&gt;Albin Soutif--Cormerais&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masana_M/0/1/0/all/0/1"&gt;Marc Masana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1"&gt;Joost Van de Weijer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Twardowski_B/0/1/0/all/0/1"&gt;Bart&amp;#x142;omiej Twardowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis and Tuning of a Voice Assistant System for Dysfluent Speech. (arXiv:2106.11759v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.11759</id>
        <link href="http://arxiv.org/abs/2106.11759"/>
        <updated>2021-06-23T01:48:41.087Z</updated>
        <summary type="html"><![CDATA[Dysfluencies and variations in speech pronunciation can severely degrade
speech recognition performance, and for many individuals with
moderate-to-severe speech disorders, voice operated systems do not work.
Current speech recognition systems are trained primarily with data from fluent
speakers and as a consequence do not generalize well to speech with
dysfluencies such as sound or word repetitions, sound prolongations, or audible
blocks. The focus of this work is on quantitative analysis of a consumer speech
recognition system on individuals who stutter and production-oriented
approaches for improving performance for common voice assistant tasks (i.e.,
"what is the weather?"). At baseline, this system introduces a significant
number of insertion and substitution errors resulting in intended speech Word
Error Rates (isWER) that are 13.64\% worse (absolute) for individuals with
fluency disorders. We show that by simply tuning the decoding parameters in an
existing hybrid speech recognition system one can improve isWER by 24\%
(relative) for individuals with fluency disorders. Tuning these parameters
translates to 3.6\% better domain recognition and 1.7\% better intent
recognition relative to the default setup for the 18 study participants across
all stuttering severities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mitra_V/0/1/0/all/0/1"&gt;Vikramjit Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zifang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lea_C/0/1/0/all/0/1"&gt;Colin Lea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tooley_L/0/1/0/all/0/1"&gt;Lauren Tooley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1"&gt;Sarah Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Botten_D/0/1/0/all/0/1"&gt;Darren Botten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Palekar_A/0/1/0/all/0/1"&gt;Ashwini Palekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thelapurath_S/0/1/0/all/0/1"&gt;Shrinath Thelapurath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Georgiou_P/0/1/0/all/0/1"&gt;Panayiotis Georgiou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kajarekar_S/0/1/0/all/0/1"&gt;Sachin Kajarekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bigham_J/0/1/0/all/0/1"&gt;Jefferey Bigham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Clustering-based Framework for Classifying Data Streams. (arXiv:2106.11823v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11823</id>
        <link href="http://arxiv.org/abs/2106.11823"/>
        <updated>2021-06-23T01:48:41.080Z</updated>
        <summary type="html"><![CDATA[The non-stationary nature of data streams strongly challenges traditional
machine learning techniques. Although some solutions have been proposed to
extend traditional machine learning techniques for handling data streams, these
approaches either require an initial label set or rely on specialized design
parameters. The overlap among classes and the labeling of data streams
constitute other major challenges for classifying data streams. In this paper,
we proposed a clustering-based data stream classification framework to handle
non-stationary data streams without utilizing an initial label set. A
density-based stream clustering procedure is used to capture novel concepts
with a dynamic threshold and an effective active label querying strategy is
introduced to continuously learn the new concepts from the data streams. The
sub-cluster structure of each cluster is explored to handle the overlap among
classes. Experimental results and quantitative comparison studies reveal that
the proposed method provides statistically better or comparable performance
than the existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1"&gt;Xuyang Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Homaifar_A/0/1/0/all/0/1"&gt;Abdollah Homaifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_M/0/1/0/all/0/1"&gt;Mrinmoy Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Girma_A/0/1/0/all/0/1"&gt;Abenezer Girma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tunstel_E/0/1/0/all/0/1"&gt;Edward Tunstel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Automated Evaluation of Explanations in Graph Neural Networks. (arXiv:2106.11864v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.11864</id>
        <link href="http://arxiv.org/abs/2106.11864"/>
        <updated>2021-06-23T01:48:41.073Z</updated>
        <summary type="html"><![CDATA[Explaining Graph Neural Networks predictions to end users of AI applications
in easily understandable terms remains an unsolved problem. In particular, we
do not have well developed methods for automatically evaluating explanations,
in ways that are closer to how users consume those explanations. Based on
recent application trends and our own experiences in real world problems, we
propose automatic evaluation approaches for GNN Explanations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+BK_V/0/1/0/all/0/1"&gt;Vanya BK&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganesan_B/0/1/0/all/0/1"&gt;Balaji Ganesan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saxena_A/0/1/0/all/0/1"&gt;Aniket Saxena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1"&gt;Devbrat Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1"&gt;Arvind Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparsistent Model Discovery. (arXiv:2106.11936v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.11936</id>
        <link href="http://arxiv.org/abs/2106.11936"/>
        <updated>2021-06-23T01:48:41.066Z</updated>
        <summary type="html"><![CDATA[Discovering the partial differential equations underlying a spatio-temporal
datasets from very limited observations is of paramount interest in many
scientific fields. However, it remains an open question to know when model
discovery algorithms based on sparse regression can actually recover the
underlying physical processes. We trace back the poor of performance of Lasso
based model discovery algorithms to its potential variable selection
inconsistency: meaning that even if the true model is present in the library,
it might not be selected. By first revisiting the irrepresentability condition
(IRC) of the Lasso, we gain some insights of when this might occur. We then
show that the adaptive Lasso will have more chances of verifying the IRC than
the Lasso and propose to integrate it within a deep learning model discovery
framework with stability selection and error control. Experimental results show
we can recover several nonlinear and chaotic canonical PDEs with a single set
of hyperparameters from a very limited number of samples at high noise levels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tod_G/0/1/0/all/0/1"&gt;Georges Tod&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Both_G/0/1/0/all/0/1"&gt;Gert-Jan Both&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kusters_R/0/1/0/all/0/1"&gt;Remy Kusters&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FLEA: Provably Fair Multisource Learning from Unreliable Training Data. (arXiv:2106.11732v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11732</id>
        <link href="http://arxiv.org/abs/2106.11732"/>
        <updated>2021-06-23T01:48:41.060Z</updated>
        <summary type="html"><![CDATA[Fairness-aware learning aims at constructing classifiers that not only make
accurate predictions, but do not discriminate against specific groups. It is a
fast-growing area of machine learning with far-reaching societal impact.
However, existing fair learning methods are vulnerable to accidental or
malicious artifacts in the training data, which can cause them to unknowingly
produce unfair classifiers. In this work we address the problem of fair
learning from unreliable training data in the robust multisource setting, where
the available training data comes from multiple sources, a fraction of which
might be not representative of the true data distribution. We introduce FLEA, a
filtering-based algorithm that allows the learning system to identify and
suppress those data sources that would have a negative impact on fairness or
accuracy if they were used for training. We show the effectiveness of our
approach by a diverse range of experiments on multiple datasets. Additionally
we prove formally that, given enough data, FLEA protects the learner against
unreliable data as long as the fraction of affected data sources is less than
half.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Iofinova_E/0/1/0/all/0/1"&gt;Eugenia Iofinova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konstantinov_N/0/1/0/all/0/1"&gt;Nikola Konstantinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lampert_C/0/1/0/all/0/1"&gt;Christoph H. Lampert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lifted Model Checking for Relational MDPs. (arXiv:2106.11735v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11735</id>
        <link href="http://arxiv.org/abs/2106.11735"/>
        <updated>2021-06-23T01:48:41.041Z</updated>
        <summary type="html"><![CDATA[Model checking has been developed for verifying the behaviour of systems with
stochastic and non-deterministic behavior. It is used to provide guarantees
about such systems. While most model checking methods focus on propositional
models, various probabilistic planning and reinforcement frameworks deal with
relational domains, for instance, STRIPS planning and relational Markov
Decision Processes. Using propositional model checking in relational settings
requires one to ground the model, which leads to the well known state explosion
problem and intractability. We present pCTL-REBEL, a lifted model checking
approach for verifying pCTL properties on relational MDPs. It extends REBEL,
the relational Bellman update operator, which is a lifted value iteration
approach for model-based relational reinforcement learning, toward relational
model-checking. PCTL-REBEL is lifted, which means that rather than grounding,
the model exploits symmetries and reasons at an abstract relational level.
Theoretically, we show that the pCTL model checking approach is decidable for
relational MDPs even for possibly infinite domains provided that the states
have a bounded size. Practically, we contribute algorithms and an
implementation of lifted relational model checking, and we show that the lifted
approach improves the scalability of the model checking approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wen-Chi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raskin_J/0/1/0/all/0/1"&gt;Jean-Fran&amp;#xe7;ois Raskin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raedt_L/0/1/0/all/0/1"&gt;Luc De Raedt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting Anomalous User Behavior in Remote Patient Monitoring. (arXiv:2106.11844v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11844</id>
        <link href="http://arxiv.org/abs/2106.11844"/>
        <updated>2021-06-23T01:48:41.032Z</updated>
        <summary type="html"><![CDATA[The growth in Remote Patient Monitoring (RPM) services using wearable and
non-wearable Internet of Medical Things (IoMT) promises to improve the quality
of diagnosis and facilitate timely treatment for a gamut of medical conditions.
At the same time, the proliferation of IoMT devices increases the potential for
malicious activities that can lead to catastrophic results including theft of
personal information, data breach, and compromised medical devices, putting
human lives at risk. IoMT devices generate tremendous amount of data that
reflect user behavior patterns including both personal and day-to-day social
activities along with daily routine health monitoring. In this context, there
are possibilities of anomalies generated due to various reasons including
unexpected user behavior, faulty sensor, or abnormal values from
malicious/compromised devices. To address this problem, there is an imminent
need to develop a framework for securing the smart health care infrastructure
to identify and mitigate anomalies. In this paper, we present an anomaly
detection model for RPM utilizing IoMT and smart home devices. We propose
Hidden Markov Model (HMM) based anomaly detection that analyzes normal user
behavior in the context of RPM comprising both smart home and smart health
devices, and identifies anomalous user behavior. We design a testbed with
multiple IoMT devices and home sensors to collect data and use the HMM model to
train using network and user behavioral data. Proposed HMM based anomaly
detection model achieved over 98% accuracy in identifying the anomalies in the
context of RPM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1"&gt;Deepti Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1"&gt;Maanak Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhatt_S/0/1/0/all/0/1"&gt;Smriti Bhatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tosun_A/0/1/0/all/0/1"&gt;Ali Saman Tosun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Hitchhiker's Guide to Prior-Shift Adaptation. (arXiv:2106.11695v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11695</id>
        <link href="http://arxiv.org/abs/2106.11695"/>
        <updated>2021-06-23T01:48:41.023Z</updated>
        <summary type="html"><![CDATA[In many computer vision classification tasks, class priors at test time often
differ from priors on the training set. In the case of such prior shift,
classifiers must be adapted correspondingly to maintain close to optimal
performance. This paper analyzes methods for adaptation of probabilistic
classifiers to new priors and for estimating new priors on an unlabeled test
set. We propose a novel method to address a known issue of prior estimation
methods based on confusion matrices, where inconsistent estimates of decision
probabilities and confusion matrices lead to negative values in the estimated
priors. Experiments on fine-grained image classification datasets provide
insight into the best practice of prior shift estimation and classifier
adaptation and show that the proposed method achieves state-of-the-art results
in prior adaptation. Applying the best practice to two tasks with naturally
imbalanced priors, learning from web-crawled images and plant species
classification, increased the recognition accuracy by 1.1% and 3.4%
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sipka_T/0/1/0/all/0/1"&gt;Tomas Sipka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sulc_M/0/1/0/all/0/1"&gt;Milan Sulc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1"&gt;Jiri Matas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LV-BERT: Exploiting Layer Variety for BERT. (arXiv:2106.11740v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11740</id>
        <link href="http://arxiv.org/abs/2106.11740"/>
        <updated>2021-06-23T01:48:41.016Z</updated>
        <summary type="html"><![CDATA[Modern pre-trained language models are mostly built upon backbones stacking
self-attention and feed-forward layers in an interleaved order. In this paper,
beyond this stereotyped layer pattern, we aim to improve pre-trained models by
exploiting layer variety from two aspects: the layer type set and the layer
order. Specifically, besides the original self-attention and feed-forward
layers, we introduce convolution into the layer type set, which is
experimentally found beneficial to pre-trained models. Furthermore, beyond the
original interleaved order, we explore more layer orders to discover more
powerful architectures. However, the introduced layer variety leads to a large
architecture space of more than billions of candidates, while training a single
candidate model from scratch already requires huge computation cost, making it
not affordable to search such a space by directly training large amounts of
candidate models. To solve this problem, we first pre-train a supernet from
which the weights of all candidate models can be inherited, and then adopt an
evolutionary algorithm guided by pre-training accuracy to find the optimal
architecture. Extensive experiments show that LV-BERT model obtained by our
method outperforms BERT and its variants on various downstream tasks. For
example, LV-BERT-small achieves 78.8 on the GLUE testing set, 1.8 higher than
the strong baseline ELECTRA-small.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1"&gt;Weihao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zihang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Fei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1"&gt;Qibin Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiashi Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Ultrasound Tongue Image Reconstruction from Lip Images Using Self-supervised Learning and Attention Mechanism. (arXiv:2106.11769v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.11769</id>
        <link href="http://arxiv.org/abs/2106.11769"/>
        <updated>2021-06-23T01:48:40.998Z</updated>
        <summary type="html"><![CDATA[Speech production is a dynamic procedure, which involved multi human organs
including the tongue, jaw and lips. Modeling the dynamics of the vocal tract
deformation is a fundamental problem to understand the speech, which is the
most common way for human daily communication. Researchers employ several
sensory streams to describe the process simultaneously, which are
incontrovertibly statistically related to other streams. In this paper, we
address the following question: given an observable image sequences of lips,
can we picture the corresponding tongue motion. We formulated this problem as
the self-supervised learning problem, and employ the two-stream convolutional
network and long-short memory network for the learning task, with the attention
mechanism. We evaluate the performance of the proposed method by leveraging the
unlabeled lip videos to predict an upcoming ultrasound tongue image sequence.
The results show that our model is able to generate images that close to the
real ultrasound tongue images, and results in the matching between two imaging
modalities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1"&gt;Haiyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jihan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Deep Learning for the Remote Characterisation of Ambulation in Multiple Sclerosis using Smartphones. (arXiv:2103.09171v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09171</id>
        <link href="http://arxiv.org/abs/2103.09171"/>
        <updated>2021-06-23T01:48:40.991Z</updated>
        <summary type="html"><![CDATA[The emergence of digital technologies such as smartphones in healthcare
applications have demonstrated the possibility of developing rich, continuous,
and objective measures of multiple sclerosis (MS) disability that can be
administered remotely and out-of-clinic. In this work, deep convolutional
neural networks (DCNN) applied to smartphone inertial sensor data were shown to
better distinguish healthy from MS participant ambulation, compared to standard
Support Vector Machine (SVM) feature-based methodologies. To overcome the
typical limitations associated with remotely generated health data, such as low
subject numbers, sparsity, and heterogeneous data, a transfer learning (TL)
model from similar large open-source datasets was proposed. Our TL framework
utilised the ambulatory information learned on Human Activity Recognition (HAR)
tasks collected from similar smartphone-based sensor data. A lack of
transparency of "black-box" deep networks remains one of the largest stumbling
blocks to the wider acceptance of deep learning for clinical applications.
Ensuing work therefore aimed to visualise DCNN decisions attributed by
relevance heatmaps using Layer-Wise Relevance Propagation (LRP). Through the
LRP framework, the patterns captured from smartphone-based inertial sensor data
that were reflective of those who are healthy versus persons with MS (PwMS)
could begin to be established and understood. Interpretations suggested that
cadence-based measures, gait speed, and ambulation-related signal perturbations
were distinct characteristics that distinguished MS disability from healthy
participants. Robust and interpretable outcomes, generated from high-frequency
out-of-clinic assessments, could greatly augment the current in-clinic
assessment picture for PwMS, to inform better disease management techniques,
and enable the development of better therapeutic interventions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Creagh_A/0/1/0/all/0/1"&gt;Andrew P. Creagh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipsmeier_F/0/1/0/all/0/1"&gt;Florian Lipsmeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lindemann_M/0/1/0/all/0/1"&gt;Michael Lindemann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vos_M/0/1/0/all/0/1"&gt;Maarten De Vos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variance-Aware Off-Policy Evaluation with Linear Function Approximation. (arXiv:2106.11960v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11960</id>
        <link href="http://arxiv.org/abs/2106.11960"/>
        <updated>2021-06-23T01:48:40.958Z</updated>
        <summary type="html"><![CDATA[We study the off-policy evaluation (OPE) problem in reinforcement learning
with linear function approximation, which aims to estimate the value function
of a target policy based on the offline data collected by a behavior policy. We
propose to incorporate the variance information of the value function to
improve the sample efficiency of OPE. More specifically, for time-inhomogeneous
episodic linear Markov decision processes (MDPs), we propose an algorithm,
VA-OPE, which uses the estimated variance of the value function to reweight the
Bellman residual in Fitted Q-Iteration. We show that our algorithm achieves a
tighter error bound than the best-known result. We also provide a fine-grained
characterization of the distribution shift between the behavior policy and the
target policy. Extensive numerical experiments corroborate our theory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1"&gt;Yifei Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianhao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Dongruo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1"&gt;Quanquan Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MIMIR: Deep Regression for Automated Analysis of UK Biobank Body MRI. (arXiv:2106.11731v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.11731</id>
        <link href="http://arxiv.org/abs/2106.11731"/>
        <updated>2021-06-23T01:48:40.951Z</updated>
        <summary type="html"><![CDATA[UK Biobank (UKB) is conducting a large-scale study of more than half a
million volunteers, collecting health-related information on genetics,
lifestyle, blood biochemistry, and more. Medical imaging furthermore targets
100,000 subjects, with 70,000 follow-up sessions, enabling measurements of
organs, muscle, and body composition. With up to 170,000 mounting MR images,
various methodologies are accordingly engaged in large-scale image analysis.
This work presents an experimental inference engine that can automatically
predict a comprehensive profile of subject metadata from UKB neck-to-knee body
MRI. In cross-validation, it accurately inferred baseline characteristics such
as age, height, weight, and sex, but also emulated measurements of body
composition by DXA, organ volumes, and abstract properties like grip strength,
pulse rate, and type 2 diabetic status (AUC: 0.866). The proposed system can
automatically analyze thousands of subjects within hours and provide individual
confidence intervals. The underlying methodology is based on convolutional
neural networks for image-based mean-variance regression on two-dimensional
representations of the MRI data. This work aims to make the proposed system
available for free to researchers, who can use it to obtain fast and
fully-automated estimates of 72 different measurements immediately upon release
of new UK Biobank image data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Langner_T/0/1/0/all/0/1"&gt;Taro Langner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mora_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9;s Mart&amp;#xed;nez Mora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Strand_R/0/1/0/all/0/1"&gt;Robin Strand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ahlstrom_H/0/1/0/all/0/1"&gt;H&amp;#xe5;kan Ahlstr&amp;#xf6;m&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kullberg_J/0/1/0/all/0/1"&gt;Joel Kullberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smooth Sequential Optimisation with Delayed Feedback. (arXiv:2106.11294v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11294</id>
        <link href="http://arxiv.org/abs/2106.11294"/>
        <updated>2021-06-23T01:48:40.944Z</updated>
        <summary type="html"><![CDATA[Stochastic delays in feedback lead to unstable sequential learning using
multi-armed bandits. Recently, empirical Bayesian shrinkage has been shown to
improve reward estimation in bandit learning. Here, we propose a novel
adaptation to shrinkage that estimates smoothed reward estimates from windowed
cumulative inputs, to deal with incomplete knowledge from delayed feedback and
non-stationary rewards. Using numerical simulations, we show that this
adaptation retains the benefits of shrinkage, and improves the stability of
reward estimation by more than 50%. Our proposal reduces variability in
treatment allocations to the best arm by up to 3.8x, and improves statistical
accuracy - with up to 8% improvement in true positive rates and 37% reduction
in false positive rates. Together, these advantages enable control of the
trade-off between speed and stability of adaptation, and facilitate
human-in-the-loop sequential optimisation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chennu_S/0/1/0/all/0/1"&gt;Srivas Chennu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1"&gt;Jamie Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liyanagama_P/0/1/0/all/0/1"&gt;Puli Liyanagama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohr_P/0/1/0/all/0/1"&gt;Phil Mohr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample Complexity of Offline Reinforcement Learning with Deep ReLU Networks. (arXiv:2103.06671v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06671</id>
        <link href="http://arxiv.org/abs/2103.06671"/>
        <updated>2021-06-23T01:48:40.933Z</updated>
        <summary type="html"><![CDATA[We study the statistical theory of offline reinforcement learning (RL) with
deep ReLU network function approximation. We analyze a variant of fitted-Q
iteration (FQI) algorithm under a new dynamic condition that we call Besov
dynamic closure, which encompasses the conditions from prior analyses for deep
neural network function approximation. Under Besov dynamic closure, we prove
that the FQI-type algorithm enjoys the sample complexity of
$\tilde{\mathcal{O}}\left( \kappa^{1 + d/\alpha} \cdot \epsilon^{-2 -
2d/\alpha} \right)$ where $\kappa$ is a distribution shift measure, $d$ is the
dimensionality of the state-action space, $\alpha$ is the (possibly fractional)
smoothness parameter of the underlying MDP, and $\epsilon$ is a user-specified
precision. This is an improvement over the sample complexity of
$\tilde{\mathcal{O}}\left( K \cdot \kappa^{2 + d/\alpha} \cdot \epsilon^{-2 -
d/\alpha} \right)$ in the prior result [Yang et al., 2019] where $K$ is an
algorithmic iteration number which is arbitrarily large in practice.
Importantly, our sample complexity is obtained under the new general dynamic
condition and a data-dependent structure where the latter is either ignored in
prior algorithms or improperly handled by prior analyses. This is the first
comprehensive analysis for offline RL with deep ReLU network function
approximation under a general setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Nguyen_Tang_T/0/1/0/all/0/1"&gt;Thanh Nguyen-Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Sunil Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tran_The_H/0/1/0/all/0/1"&gt;Hung Tran-The&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Venkatesh_S/0/1/0/all/0/1"&gt;Svetha Venkatesh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating abstractive summaries of Lithuanian news articles using a transformer model. (arXiv:2105.03279v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03279</id>
        <link href="http://arxiv.org/abs/2105.03279"/>
        <updated>2021-06-23T01:48:40.910Z</updated>
        <summary type="html"><![CDATA[In this work, we train the first monolingual Lithuanian transformer model on
a relatively large corpus of Lithuanian news articles and compare various
output decoding algorithms for abstractive news summarization. We achieve an
average ROUGE-2 score 0.163, generated summaries are coherent and look
impressive at first glance. However, some of them contain misleading
information that is not so easy to spot. We describe all the technical details
and share our trained model and accompanying code in an online open-source
repository, as well as some characteristic samples of the generated summaries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stankevicius_L/0/1/0/all/0/1"&gt;Lukas Stankevi&amp;#x10d;ius&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukosevicius_M/0/1/0/all/0/1"&gt;Mantas Luko&amp;#x161;evi&amp;#x10d;ius&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HDMI: High-order Deep Multiplex Infomax. (arXiv:2102.07810v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07810</id>
        <link href="http://arxiv.org/abs/2102.07810"/>
        <updated>2021-06-23T01:48:40.903Z</updated>
        <summary type="html"><![CDATA[Networks have been widely used to represent the relations between objects
such as academic networks and social networks, and learning embedding for
networks has thus garnered plenty of research attention. Self-supervised
network representation learning aims at extracting node embedding without
external supervision. Recently, maximizing the mutual information between the
local node embedding and the global summary (e.g. Deep Graph Infomax, or DGI
for short) has shown promising results on many downstream tasks such as node
classification. However, there are two major limitations of DGI. Firstly, DGI
merely considers the extrinsic supervision signal (i.e., the mutual information
between node embedding and global summary) while ignores the intrinsic signal
(i.e., the mutual dependence between node embedding and node attributes).
Secondly, nodes in a real-world network are usually connected by multiple edges
with different relations, while DGI does not fully explore the various
relations among nodes. To address the above-mentioned problems, we propose a
novel framework, called High-order Deep Multiplex Infomax (HDMI), for learning
node embedding on multiplex networks in a self-supervised way. To be more
specific, we first design a joint supervision signal containing both extrinsic
and intrinsic mutual information by high-order mutual information, and we
propose a High-order Deep Infomax (HDI) to optimize the proposed supervision
signal. Then we propose an attention based fusion module to combine node
embedding from different layers of the multiplex network. Finally, we evaluate
the proposed HDMI on various downstream tasks such as unsupervised clustering
and supervised classification. The experimental results show that HDMI achieves
state-of-the-art performance on these tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jing_B/0/1/0/all/0/1"&gt;Baoyu Jing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1"&gt;Chanyoung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1"&gt;Hanghang Tong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Regression Revisited: Acceleration and Improved Estimation Rates. (arXiv:2106.11938v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2106.11938</id>
        <link href="http://arxiv.org/abs/2106.11938"/>
        <updated>2021-06-23T01:48:40.883Z</updated>
        <summary type="html"><![CDATA[We study fast algorithms for statistical regression problems under the strong
contamination model, where the goal is to approximately optimize a generalized
linear model (GLM) given adversarially corrupted samples. Prior works in this
line of research were based on the robust gradient descent framework of Prasad
et. al., a first-order method using biased gradient queries, or the Sever
framework of Diakonikolas et. al., an iterative outlier-removal method calling
a stationary point finder.

We present nearly-linear time algorithms for robust regression problems with
improved runtime or estimation guarantees compared to the state-of-the-art. For
the general case of smooth GLMs (e.g. logistic regression), we show that the
robust gradient descent framework of Prasad et. al. can be accelerated, and
show our algorithm extends to optimizing the Moreau envelopes of Lipschitz GLMs
(e.g. support vector machines), answering several open questions in the
literature.

For the well-studied case of robust linear regression, we present an
alternative approach obtaining improved estimation rates over prior
nearly-linear time algorithms. Interestingly, our method starts with an
identifiability proof introduced in the context of the sum-of-squares algorithm
of Bakshi and Prasad, which achieved optimal error rates while requiring large
polynomial runtime and sample complexity. We reinterpret their proof within the
Sever framework and obtain a dramatically faster and more sample-efficient
algorithm under fewer distributional assumptions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jambulapati_A/0/1/0/all/0/1"&gt;Arun Jambulapati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jerry Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schramm_T/0/1/0/all/0/1"&gt;Tselil Schramm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_K/0/1/0/all/0/1"&gt;Kevin Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global inducing point variational posteriors for Bayesian neural networks and deep Gaussian processes. (arXiv:2005.08140v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.08140</id>
        <link href="http://arxiv.org/abs/2005.08140"/>
        <updated>2021-06-23T01:48:40.877Z</updated>
        <summary type="html"><![CDATA[We consider the optimal approximate posterior over the top-layer weights in a
Bayesian neural network for regression, and show that it exhibits strong
dependencies on the lower-layer weights. We adapt this result to develop a
correlated approximate posterior over the weights at all layers in a Bayesian
neural network. We extend this approach to deep Gaussian processes, unifying
inference in the two model classes. Our approximate posterior uses learned
"global" inducing points, which are defined only at the input layer and
propagated through the network to obtain inducing inputs at subsequent layers.
By contrast, standard, "local", inducing point methods from the deep Gaussian
process literature optimise a separate set of inducing inputs at every layer,
and thus do not model correlations across layers. Our method gives
state-of-the-art performance for a variational Bayesian method, without data
augmentation or tempering, on CIFAR-10 of 86.7%, which is comparable to SGMCMC
without tempering but with data augmentation (88% in Wenzel et al. 2020).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ober_S/0/1/0/all/0/1"&gt;Sebastian W. Ober&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Aitchison_L/0/1/0/all/0/1"&gt;Laurence Aitchison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Stealthy and Robust Fingerprinting Scheme for Generative Models. (arXiv:2106.11760v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.11760</id>
        <link href="http://arxiv.org/abs/2106.11760"/>
        <updated>2021-06-23T01:48:40.870Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel fingerprinting methodology for the Intellectual
Property protection of generative models. Prior solutions for discriminative
models usually adopt adversarial examples as the fingerprints, which give
anomalous inference behaviors and prediction results. Hence, these methods are
not stealthy and can be easily recognized by the adversary. Our approach
leverages the invisible backdoor technique to overcome the above limitation.
Specifically, we design verification samples, whose model outputs look normal
but can trigger a backdoor classifier to make abnormal predictions. We propose
a new backdoor embedding approach with Unique-Triplet Loss and fine-grained
categorization to enhance the effectiveness of our fingerprints. Extensive
evaluations show that this solution can outperform other strategies with higher
robustness, uniqueness and stealthiness for various GAN models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guanlin_L/0/1/0/all/0/1"&gt;Li Guanlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shangwei_G/0/1/0/all/0/1"&gt;Guo Shangwei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Run_W/0/1/0/all/0/1"&gt;Wang Run&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guowen_X/0/1/0/all/0/1"&gt;Xu Guowen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tianwei_Z/0/1/0/all/0/1"&gt;Zhang Tianwei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sphynx: ReLU-Efficient Network Design for Private Inference. (arXiv:2106.11755v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.11755</id>
        <link href="http://arxiv.org/abs/2106.11755"/>
        <updated>2021-06-23T01:48:40.863Z</updated>
        <summary type="html"><![CDATA[The emergence of deep learning has been accompanied by privacy concerns
surrounding users' data and service providers' models. We focus on private
inference (PI), where the goal is to perform inference on a user's data sample
using a service provider's model. Existing PI methods for deep networks enable
cryptographically secure inference with little drop in functionality; however,
they incur severe latency costs, primarily caused by non-linear network
operations (such as ReLUs). This paper presents Sphynx, a ReLU-efficient
network design method based on micro-search strategies for convolutional cell
design. Sphynx achieves Pareto dominance over all existing private inference
methods on CIFAR-100. We also design large-scale networks that support
cryptographically private inference on Tiny-ImageNet and ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1"&gt;Minsu Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghodsi_Z/0/1/0/all/0/1"&gt;Zahra Ghodsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reagen_B/0/1/0/all/0/1"&gt;Brandon Reagen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1"&gt;Siddharth Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1"&gt;Chinmay Hegde&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Symplectic Learning for Hamiltonian Neural Networks. (arXiv:2106.11753v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11753</id>
        <link href="http://arxiv.org/abs/2106.11753"/>
        <updated>2021-06-23T01:48:40.856Z</updated>
        <summary type="html"><![CDATA[Machine learning methods are widely used in the natural sciences to model and
predict physical systems from observation data. Yet, they are often used as
poorly understood "black boxes," disregarding existing mathematical structure
and invariants of the problem. Recently, the proposal of Hamiltonian Neural
Networks (HNNs) took a first step towards a unified "gray box" approach, using
physical insight to improve performance for Hamiltonian systems. In this paper,
we explore a significantly improved training method for HNNs, exploiting the
symplectic structure of Hamiltonian systems with a different loss function.
This frees the loss from an artificial lower bound. We mathematically guarantee
the existence of an exact Hamiltonian function which the HNN can learn. This
allows us to prove and numerically analyze the errors made by HNNs which, in
turn, renders them fully explainable. Finally, we present a novel post-training
correction to obtain the true Hamiltonian only from discretized observation
data, up to an arbitrary order.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+David_M/0/1/0/all/0/1"&gt;Marco David&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehats_F/0/1/0/all/0/1"&gt;Florian M&amp;#xe9;hats&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs. (arXiv:2103.09430v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09430</id>
        <link href="http://arxiv.org/abs/2103.09430"/>
        <updated>2021-06-23T01:48:40.849Z</updated>
        <summary type="html"><![CDATA[Enabling effective and efficient machine learning (ML) over large-scale graph
data (e.g., graphs with billions of edges) can have a huge impact on both
industrial and scientific applications. However, community efforts to advance
large-scale graph ML have been severely limited by the lack of a suitable
public benchmark. For KDD Cup 2021, we present OGB Large-Scale Challenge
(OGB-LSC), a collection of three real-world datasets for advancing the
state-of-the-art in large-scale graph ML. OGB-LSC provides graph datasets that
are orders of magnitude larger than existing ones and covers three core graph
learning tasks -- link prediction, graph regression, and node classification.
Furthermore, OGB-LSC provides dedicated baseline experiments, scaling up
expressive graph ML models to the massive datasets. We show that the expressive
models significantly outperform simple scalable baselines, indicating an
opportunity for dedicated efforts to further improve graph ML at scale. Our
datasets and baseline code are released and maintained as part of our OGB
initiative (Hu et al., 2020). We hope OGB-LSC at KDD Cup 2021 can empower the
community to discover innovative solutions for large-scale graph ML.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Weihua Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fey_M/0/1/0/all/0/1"&gt;Matthias Fey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1"&gt;Hongyu Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakata_M/0/1/0/all/0/1"&gt;Maho Nakata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yuxiao Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1"&gt;Jure Leskovec&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Surrogate-based variational data assimilation for tidal modelling. (arXiv:2106.11926v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.11926</id>
        <link href="http://arxiv.org/abs/2106.11926"/>
        <updated>2021-06-23T01:48:40.842Z</updated>
        <summary type="html"><![CDATA[Data assimilation (DA) is widely used to combine physical knowledge and
observations. It is nowadays commonly used in geosciences to perform parametric
calibration. In a context of climate change, old calibrations can not
necessarily be used for new scenarios. This raises the question of DA
computational cost, as costly physics-based numerical models need to be
reanalyzed. Reduction and metamodelling represent therefore interesting
perspectives, for example proposed in recent contributions as hybridization
between ensemble and variational methods, to combine their advantages
(efficiency, non-linear framework). They are however often based on Monte Carlo
(MC) type sampling, which often requires considerable increase of the ensemble
size for better efficiency, therefore representing a computational burden in
ensemble-based methods as well. To address these issues, two methods to replace
the complex model by a surrogate are proposed and confronted : (i) PODEn3DVAR
directly inspired from PODEn4DVAR, relies on an ensemble-based joint
parameter-state Proper Orthogonal Decomposition (POD), which provides a linear
metamodel ; (ii) POD-PCE-3DVAR, where the model states are POD reduced then
learned using Polynomial Chaos Expansion (PCE), resulting in a non-linear
metamodel. Both metamodels allow to write an approximate cost function whose
minimum can be analytically computed, or deduced by a gradient descent at
negligible cost. Furthermore, adapted metamodelling error covariance matrix is
given for POD-PCE-3DVAR, allowing to substantially improve the metamodel-based
DA analysis. Proposed methods are confronted on a twin experiment, and compared
to classical 3DVAR on a measurement-based problem. Results are promising, in
particular superior with POD-PCE-3DVAR, showing good convergence to classical
3DVAR and robustness to noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Mouradi_R/0/1/0/all/0/1"&gt;Rem-Sophia Mouradi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Goeury_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric Goeury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Thual_O/0/1/0/all/0/1"&gt;Olivier Thual&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zaoui_F/0/1/0/all/0/1"&gt;Fabrice Zaoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tassi_P/0/1/0/all/0/1"&gt;Pablo Tassi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Break-It-Fix-It: Unsupervised Learning for Program Repair. (arXiv:2106.06600v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06600</id>
        <link href="http://arxiv.org/abs/2106.06600"/>
        <updated>2021-06-23T01:48:40.835Z</updated>
        <summary type="html"><![CDATA[We consider repair tasks: given a critic (e.g., compiler) that assesses the
quality of an input, the goal is to train a fixer that converts a bad example
(e.g., code with syntax errors) into a good one (e.g., code with no syntax
errors). Existing works create training data consisting of (bad, good) pairs by
corrupting good examples using heuristics (e.g., dropping tokens). However,
fixers trained on this synthetically-generated data do not extrapolate well to
the real distribution of bad inputs. To bridge this gap, we propose a new
training approach, Break-It-Fix-It (BIFI), which has two key ideas: (i) we use
the critic to check a fixer's output on real bad inputs and add good (fixed)
outputs to the training data, and (ii) we train a breaker to generate realistic
bad code from good code. Based on these ideas, we iteratively update the
breaker and the fixer while using them in conjunction to generate more paired
data. We evaluate BIFI on two code repair datasets: GitHub-Python, a new
dataset we introduce where the goal is to repair Python code with AST parse
errors; and DeepFix, where the goal is to repair C code with compiler errors.
BIFI outperforms existing methods, obtaining 90.5% repair accuracy on
GitHub-Python (+28.5%) and 71.7% on DeepFix (+5.6%). Notably, BIFI does not
require any labeled data; we hope it will be a strong starting point for
unsupervised learning of various repair tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1"&gt;Michihiro Yasunaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Percy Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speed Benchmarking of Genetic Programming Frameworks. (arXiv:2106.11919v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.11919</id>
        <link href="http://arxiv.org/abs/2106.11919"/>
        <updated>2021-06-23T01:48:40.815Z</updated>
        <summary type="html"><![CDATA[Genetic Programming (GP) is known to suffer from the burden of being
computationally expensive by design. While, over the years, many techniques
have been developed to mitigate this issue, data vectorization, in particular,
is arguably still the most attractive strategy due to the parallel nature of
GP. In this work, we employ a series of benchmarks meant to compare both the
performance and evolution capabilities of different vectorized and iterative
implementation approaches across several existing frameworks. Namely, TensorGP,
a novel open-source engine written in Python, is shown to greatly benefit from
the TensorFlow library to accelerate the domain evaluation phase in GP. The
presented performance benchmarks demonstrate that the TensorGP engine manages
to pull ahead, with relative speedups above two orders of magnitude for
problems with a higher number of fitness cases. Additionally, as a consequence
of being able to compute larger domains, we argue that TensorGP performance
gains aid the discovery of more accurate candidate solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baeta_F/0/1/0/all/0/1"&gt;Francisco Baeta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Correia_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Correia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martins_T/0/1/0/all/0/1"&gt;Tiago Martins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Machado_P/0/1/0/all/0/1"&gt;Penousal Machado&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Query-Driven Topic Model. (arXiv:2106.07346v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07346</id>
        <link href="http://arxiv.org/abs/2106.07346"/>
        <updated>2021-06-23T01:48:40.805Z</updated>
        <summary type="html"><![CDATA[Topic modeling is an unsupervised method for revealing the hidden semantic
structure of a corpus. It has been increasingly widely adopted as a tool in the
social sciences, including political science, digital humanities and
sociological research in general. One desirable property of topic models is to
allow users to find topics describing a specific aspect of the corpus. A
possible solution is to incorporate domain-specific knowledge into topic
modeling, but this requires a specification from domain experts. We propose a
novel query-driven topic model that allows users to specify a simple query in
words or phrases and return query-related topics, thus avoiding tedious work
from domain experts. Our proposed approach is particularly attractive when the
user-specified query has a low occurrence in a text corpus, making it difficult
for traditional topic models built on word cooccurrence patterns to identify
relevant topics. Experimental results demonstrate the effectiveness of our
model in comparison with both classical topic models and neural topic models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1"&gt;Zheng Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yulan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Procter_R/0/1/0/all/0/1"&gt;Rob Procter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Randomness In Neural Network Training: Characterizing The Impact of Tooling. (arXiv:2106.11872v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11872</id>
        <link href="http://arxiv.org/abs/2106.11872"/>
        <updated>2021-06-23T01:48:40.795Z</updated>
        <summary type="html"><![CDATA[The quest for determinism in machine learning has disproportionately focused
on characterizing the impact of noise introduced by algorithmic design choices.
In this work, we address a less well understood and studied question: how does
our choice of tooling introduce randomness to deep neural network training. We
conduct large scale experiments across different types of hardware,
accelerators, state of art networks, and open-source datasets, to characterize
how tooling choices contribute to the level of non-determinism in a system, the
impact of said non-determinism, and the cost of eliminating different sources
of noise.

Our findings are surprising, and suggest that the impact of non-determinism
in nuanced. While top-line metrics such as top-1 accuracy are not noticeably
impacted, model performance on certain parts of the data distribution is far
more sensitive to the introduction of randomness. Our results suggest that
deterministic tooling is critical for AI safety. However, we also find that the
cost of ensuring determinism varies dramatically between neural network
architectures and hardware types, e.g., with overhead up to $746\%$, $241\%$,
and $196\%$ on a spectrum of widely used GPU accelerator architectures,
relative to non-deterministic training. The source code used in this paper is
available at https://github.com/usyd-fsalab/NeuralNetworkRandomness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_D/0/1/0/all/0/1"&gt;Donglin Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xingyao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Shuaiwen Leon Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1"&gt;Sara Hooker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Empirically explaining SGD from a line search perspective. (arXiv:2103.17132v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.17132</id>
        <link href="http://arxiv.org/abs/2103.17132"/>
        <updated>2021-06-23T01:48:40.786Z</updated>
        <summary type="html"><![CDATA[Optimization in Deep Learning is mainly guided by vague intuitions and strong
assumptions, with a limited understanding how and why these work in practice.
To shed more light on this, our work provides some deeper understandings of how
SGD behaves by empirically analyzing the trajectory taken by SGD from a line
search perspective. Specifically, a costly quantitative analysis of the
full-batch loss along SGD trajectories from common used models trained on a
subset of CIFAR-10 is performed. Our core results include that the full-batch
loss along lines in update step direction is highly parabolically. Further on,
we show that there exists a learning rate with which SGD always performs almost
exact line searches on the full-batch loss. Finally, we provide a different
perspective why increasing the batch size has almost the same effect as
decreasing the learning rate by the same factor.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mutschler_M/0/1/0/all/0/1"&gt;Maximus Mutschler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1"&gt;Andreas Zell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Update of a Progressively Expanded Database for Automated Lung Sound Analysis. (arXiv:2102.04062v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04062</id>
        <link href="http://arxiv.org/abs/2102.04062"/>
        <updated>2021-06-23T01:48:40.774Z</updated>
        <summary type="html"><![CDATA[A continuous real-time respiratory sound automated analysis system is needed
in clinical practice. Previously, we established an open access lung sound
database, HF_Lung_V1, and automated lung sound analysis algorithms capable of
detecting inhalation, exhalation, continuous adventitious sounds (CASs) and
discontinuous adventitious sounds (DASs). In this study, HF-Lung-V1 has been
further expanded to HF-Lung-V2 with 1.45 times of increase in audio files. The
convolutional neural network (CNN)-bidirectional gated recurrent unit (BiGRU)
model was separately trained with training datasets of HF_Lung_V1 (V1_Train)
and HF_Lung_V2 (V2_Train), and then were used for the performance comparisons
of segment detection and event detection on both test datasets of HF_Lung_V1
(V1_Test) and HF_Lung_V2 (V2_Test). The performance of segment detection was
measured by accuracy, predictive positive value (PPV), sensitivity,
specificity, F1 score, receiver operating characteristic (ROC) curve and area
under the curve (AUC), whereas that of event detection was evaluated with PPV,
sensitivity, and F1 score. Results indicate that the model performance trained
by V2_Train showed improvement on both V1_Test and V2_Test in inhalation, CASs
and DASs, particularly in CASs, as well as on V1_Test in exhalation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_F/0/1/0/all/0/1"&gt;Fu-Shun Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shang-Ran Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chien-Wen Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yuan-Ren Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chun-Chieh Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsiao_J/0/1/0/all/0/1"&gt;Jack Hsiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chung-Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_F/0/1/0/all/0/1"&gt;Feipei Lai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepReDuce: ReLU Reduction for Fast Private Inference. (arXiv:2103.01396v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01396</id>
        <link href="http://arxiv.org/abs/2103.01396"/>
        <updated>2021-06-23T01:48:40.746Z</updated>
        <summary type="html"><![CDATA[The recent rise of privacy concerns has led researchers to devise methods for
private neural inference -- where inferences are made directly on encrypted
data, never seeing inputs. The primary challenge facing private inference is
that computing on encrypted data levies an impractically-high latency penalty,
stemming mostly from non-linear operators like ReLU. Enabling practical and
private inference requires new optimization methods that minimize network ReLU
counts while preserving accuracy. This paper proposes DeepReDuce: a set of
optimizations for the judicious removal of ReLUs to reduce private inference
latency. The key insight is that not all ReLUs contribute equally to accuracy.
We leverage this insight to drop, or remove, ReLUs from classic networks to
significantly reduce inference latency and maintain high accuracy. Given a
target network, DeepReDuce outputs a Pareto frontier of networks that tradeoff
the number of ReLUs and accuracy. Compared to the state-of-the-art for private
inference DeepReDuce improves accuracy and reduces ReLU count by up to 3.5%
(iso-ReLU count) and 3.5$\times$ (iso-accuracy), respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jha_N/0/1/0/all/0/1"&gt;Nandan Kumar Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghodsi_Z/0/1/0/all/0/1"&gt;Zahra Ghodsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1"&gt;Siddharth Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reagen_B/0/1/0/all/0/1"&gt;Brandon Reagen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constrained Ensemble Langevin Monte Carlo. (arXiv:2102.04279v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04279</id>
        <link href="http://arxiv.org/abs/2102.04279"/>
        <updated>2021-06-23T01:48:40.728Z</updated>
        <summary type="html"><![CDATA[The classical Langevin Monte Carlo method looks for i.i.d. samples from a
target distribution by descending along the gradient of the target
distribution. It is popular partially due to its fast convergence rate.
However, the numerical cost is sometimes high because the gradient can be hard
to obtain. One approach to eliminate the gradient computation is to employ the
concept of "ensemble", where a large number of particles are evolved together
so that the neighboring particles provide gradient information to each other.
In this article, we discuss two algorithms that integrate the ensemble feature
into LMC, and the associated properties. There are two sides of our discovery:

1. By directly surrogating the gradient using the ensemble approximation, we
develop Ensemble Langevin Monte Carlo. We show that this method is unstable due
to a potentially small denominator that induces high variance. We provide a
counterexample to explicitly show this instability.

2. We then change the strategy and enact the ensemble approximation to the
gradient only in a constrained manner, to eliminate the unstable points. The
algorithm is termed Constrained Ensemble Langevin Monte Carlo. We show that,
with a proper tuning, the surrogation takes place often enough to bring the
reasonable numerical saving, while the induced error is still low enough for us
to maintain the fast convergence rate, up to a controllable discretization and
ensemble error.

Such combination of ensemble method and LMC shed light on inventing
gradient-free algorithms that produce i.i.d. samples almost exponentially fast.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ding_Z/0/1/0/all/0/1"&gt;Zhiyan Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qin Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Learning Under Triage. (arXiv:2103.08902v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08902</id>
        <link href="http://arxiv.org/abs/2103.08902"/>
        <updated>2021-06-23T01:48:40.708Z</updated>
        <summary type="html"><![CDATA[Multiple lines of evidence suggest that predictive models may benefit from
algorithmic triage. Under algorithmic triage, a predictive model does not
predict all instances but instead defers some of them to human experts.
However, the interplay between the prediction accuracy of the model and the
human experts under algorithmic triage is not well understood. In this work, we
start by formally characterizing under which circumstances a predictive model
may benefit from algorithmic triage. In doing so, we also demonstrate that
models trained for full automation may be suboptimal under triage. Then, given
any model and desired level of triage, we show that the optimal triage policy
is a deterministic threshold rule in which triage decisions are derived
deterministically by thresholding the difference between the model and human
errors on a per-instance level. Building upon these results, we introduce a
practical gradient-based algorithm that is guaranteed to find a sequence of
triage policies and predictive models of increasing performance. Experiments on
a wide variety of supervised learning tasks using synthetic and real data from
two important applications -- content moderation and scientific discovery --
illustrate our theoretical results and show that the models and triage policies
provided by our gradient-based algorithm outperform those provided by several
competitive baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Okati_N/0/1/0/all/0/1"&gt;Nastaran Okati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+De_A/0/1/0/all/0/1"&gt;Abir De&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gomez_Rodriguez_M/0/1/0/all/0/1"&gt;Manuel Gomez-Rodriguez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Long Range Memory Effects in Deep Neural Networks. (arXiv:2105.02062v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02062</id>
        <link href="http://arxiv.org/abs/2105.02062"/>
        <updated>2021-06-23T01:48:40.701Z</updated>
        <summary type="html"><![CDATA[\textit{Stochastic gradient descent} (SGD) is of fundamental importance in
deep learning. Despite its simplicity, elucidating its efficacy remains
challenging. Conventionally, the success of SGD is attributed to the
\textit{stochastic gradient noise} (SGN) incurred in the training process.
Based on this general consensus, SGD is frequently treated and analyzed as the
Euler-Maruyama discretization of a \textit{stochastic differential equation}
(SDE) driven by either Brownian or L\'evy stable motion. In this study, we
argue that SGN is neither Gaussian nor stable. Instead, inspired by the
long-time correlation emerging in SGN series, we propose that SGD can be viewed
as a discretization of an SDE driven by \textit{fractional Brownian motion}
(FBM). Accordingly, the different convergence behavior of SGD dynamics is well
grounded. Moreover, the first passage time of an SDE driven by FBM is
approximately derived. This indicates a lower escaping rate for a larger Hurst
parameter, and thus SGD stays longer in flat minima. This happens to coincide
with the well-known phenomenon that SGD favors flat minima that generalize
well. Four groups of experiments are conducted to validate our conjecture, and
it is demonstrated that long-range memory effects persist across various model
architectures, datasets, and training strategies. Our study opens up a new
perspective and may contribute to a better understanding of SGD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1"&gt;Chengli Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiangshe Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Junmin Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EC-GAN: Low-Sample Classification using Semi-Supervised Algorithms and GANs. (arXiv:2012.15864v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15864</id>
        <link href="http://arxiv.org/abs/2012.15864"/>
        <updated>2021-06-23T01:48:40.682Z</updated>
        <summary type="html"><![CDATA[Semi-supervised learning has been gaining attention as it allows for
performing image analysis tasks such as classification with limited labeled
data. Some popular algorithms using Generative Adversarial Networks (GANs) for
semi-supervised classification share a single architecture for classification
and discrimination. However, this may require a model to converge to a separate
data distribution for each task, which may reduce overall performance. While
progress in semi-supervised learning has been made, less addressed are
small-scale, fully-supervised tasks where even unlabeled data is unavailable
and unattainable. We therefore, propose a novel GAN model namely External
Classifier GAN (EC-GAN), that utilizes GANs and semi-supervised algorithms to
improve classification in fully-supervised regimes. Our method leverages a GAN
to generate artificial data used to supplement supervised classification. More
specifically, we attach an external classifier, hence the name EC-GAN, to the
GAN's generator, as opposed to sharing an architecture with the discriminator.
Our experiments demonstrate that EC-GAN's performance is comparable to the
shared architecture method, far superior to the standard data augmentation and
regularization-based approach, and effective on a small, realistic dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haque_A/0/1/0/all/0/1"&gt;Ayaan Haque&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distilled Replay: Overcoming Forgetting through Synthetic Samples. (arXiv:2103.15851v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15851</id>
        <link href="http://arxiv.org/abs/2103.15851"/>
        <updated>2021-06-23T01:48:40.675Z</updated>
        <summary type="html"><![CDATA[Replay strategies are Continual Learning techniques which mitigate
catastrophic forgetting by keeping a buffer of patterns from previous
experiences, which are interleaved with new data during training. The amount of
patterns stored in the buffer is a critical parameter which largely influences
the final performance and the memory footprint of the approach. This work
introduces Distilled Replay, a novel replay strategy for Continual Learning
which is able to mitigate forgetting by keeping a very small buffer (1 pattern
per class) of highly informative samples. Distilled Replay builds the buffer
through a distillation process which compresses a large dataset into a tiny set
of informative examples. We show the effectiveness of our Distilled Replay
against popular replay-based strategies on four Continual Learning benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosasco_A/0/1/0/all/0/1"&gt;Andrea Rosasco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carta_A/0/1/0/all/0/1"&gt;Antonio Carta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cossu_A/0/1/0/all/0/1"&gt;Andrea Cossu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1"&gt;Vincenzo Lomonaco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1"&gt;Davide Bacciu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nutrition5k: Towards Automatic Nutritional Understanding of Generic Food. (arXiv:2103.03375v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03375</id>
        <link href="http://arxiv.org/abs/2103.03375"/>
        <updated>2021-06-23T01:48:40.666Z</updated>
        <summary type="html"><![CDATA[Understanding the nutritional content of food from visual data is a
challenging computer vision problem, with the potential to have a positive and
widespread impact on public health. Studies in this area are limited to
existing datasets in the field that lack sufficient diversity or labels
required for training models with nutritional understanding capability. We
introduce Nutrition5k, a novel dataset of 5k diverse, real world food dishes
with corresponding video streams, depth images, component weights, and high
accuracy nutritional content annotation. We demonstrate the potential of this
dataset by training a computer vision algorithm capable of predicting the
caloric and macronutrient values of a complex, real world dish at an accuracy
that outperforms professional nutritionists. Further we present a baseline for
incorporating depth sensor data to improve nutrition predictions. We will
publicly release Nutrition5k in the hope that it will accelerate innovation in
the space of nutritional understanding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thames_Q/0/1/0/all/0/1"&gt;Quin Thames&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karpur_A/0/1/0/all/0/1"&gt;Arjun Karpur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Norris_W/0/1/0/all/0/1"&gt;Wade Norris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1"&gt;Fangting Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panait_L/0/1/0/all/0/1"&gt;Liviu Panait&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weyand_T/0/1/0/all/0/1"&gt;Tobias Weyand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sim_J/0/1/0/all/0/1"&gt;Jack Sim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latent-CF: A Simple Baseline for Reverse Counterfactual Explanations. (arXiv:2012.09301v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09301</id>
        <link href="http://arxiv.org/abs/2012.09301"/>
        <updated>2021-06-23T01:48:40.659Z</updated>
        <summary type="html"><![CDATA[In the environment of fair lending laws and the General Data Protection
Regulation (GDPR), the ability to explain a model's prediction is of paramount
importance. High quality explanations are the first step in assessing fairness.
Counterfactuals are valuable tools for explainability. They provide actionable,
comprehensible explanations for the individual who is subject to decisions made
from the prediction. It is important to find a baseline for producing them. We
propose a simple method for generating counterfactuals by using gradient
descent to search in the latent space of an autoencoder and benchmark our
method against approaches that search for counterfactuals in feature space.
Additionally, we implement metrics to concretely evaluate the quality of the
counterfactuals. We show that latent space counterfactual generation strikes a
balance between the speed of basic feature gradient descent methods and the
sparseness and authenticity of counterfactuals generated by more complex
feature space oriented techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_R/0/1/0/all/0/1"&gt;Rachana Balasubramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharpe_S/0/1/0/all/0/1"&gt;Samuel Sharpe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barr_B/0/1/0/all/0/1"&gt;Brian Barr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wittenbach_J/0/1/0/all/0/1"&gt;Jason Wittenbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruss_C/0/1/0/all/0/1"&gt;C. Bayan Bruss&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Best-Arm Identification Methods for Tail-Risk Measures. (arXiv:2008.07606v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.07606</id>
        <link href="http://arxiv.org/abs/2008.07606"/>
        <updated>2021-06-23T01:48:40.648Z</updated>
        <summary type="html"><![CDATA[Conditional value-at-risk (CVaR) and value-at-risk (VaR) are popular
tail-risk measures in finance and insurance industries as well as in highly
reliable, safety-critical uncertain environments where often the underlying
probability distributions are heavy-tailed. We use the multi-armed bandit
best-arm identification framework and consider the problem of identifying the
arm from amongst finitely many that has the smallest CVaR, VaR, or weighted sum
of CVaR and mean. The latter captures the risk-return trade-off common in
finance. Our main contribution is an optimal $\delta$-correct algorithm that
acts on general arms, including heavy-tailed distributions, and matches the
lower bound on the expected number of samples needed, asymptotically (as
$\delta$ approaches $0$). The algorithm requires solving a non-convex
optimization problem in the space of probability measures, that requires
delicate analysis. En-route, we develop new non-asymptotic empirical
likelihood-based concentration inequalities for tail-risk measures which are
tighter than those for popular truncation-based empirical estimators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1"&gt;Shubhada Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koolen_W/0/1/0/all/0/1"&gt;Wouter M. Koolen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juneja_S/0/1/0/all/0/1"&gt;Sandeep Juneja&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RUHSNet: 3D Object Detection Using Lidar Data in Real Time. (arXiv:2006.01250v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.01250</id>
        <link href="http://arxiv.org/abs/2006.01250"/>
        <updated>2021-06-23T01:48:40.634Z</updated>
        <summary type="html"><![CDATA[In this work, we address the problem of 3D object detection from point cloud
data in real time. For autonomous vehicles to work, it is very important for
the perception component to detect the real world objects with both high
accuracy and fast inference. We propose a novel neural network architecture
along with the training and optimization details for detecting 3D objects in
point cloud data. We compare the results with different backbone architectures
including the standard ones like VGG, ResNet, Inception with our backbone. Also
we present the optimization and ablation studies including designing an
efficient anchor. We use the Kitti 3D Birds Eye View dataset for benchmarking
and validating our results. Our work surpasses the state of the art in this
domain both in terms of average precision and speed running at > 30 FPS. This
makes it a feasible option to be deployed in real time applications including
self driving cars.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1"&gt;Abhinav Sagar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fair Algorithms for Hierarchical Agglomerative Clustering. (arXiv:2005.03197v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.03197</id>
        <link href="http://arxiv.org/abs/2005.03197"/>
        <updated>2021-06-23T01:48:40.615Z</updated>
        <summary type="html"><![CDATA[Hierarchical Agglomerative Clustering (HAC) algorithms are extensively
utilized in modern data science, and seek to partition the dataset into
clusters while generating a hierarchical relationship between the data samples.
HAC algorithms are employed in many applications, such as biology, natural
language processing, and recommender systems. Thus, it is imperative to ensure
that these algorithms are fair -- even if the dataset contains biases against
certain protected groups, the cluster outputs generated should not discriminate
against samples from any of these groups. However, recent work in clustering
fairness has mostly focused on center-based clustering algorithms, such as
k-median and k-means clustering. In this paper, we propose fair algorithms for
performing HAC that enforce fairness constraints 1) irrespective of the
distance linkage criteria used, 2) generalize to any natural measures of
clustering fairness for HAC, 3) work for multiple protected groups, and 4) have
competitive running times to vanilla HAC. Through extensive experiments on
multiple real-world UCI datasets, we show that our proposed algorithm finds
fairer clusterings compared to vanilla HAC as well as other state-of-the-art
fair clustering approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chhabra_A/0/1/0/all/0/1"&gt;Anshuman Chhabra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vashishth_V/0/1/0/all/0/1"&gt;Vidushi Vashishth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohapatra_P/0/1/0/all/0/1"&gt;Prasant Mohapatra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SDCOR: Scalable Density-based Clustering for Local Outlier Detection in Massive-Scale Datasets. (arXiv:2006.07616v10 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07616</id>
        <link href="http://arxiv.org/abs/2006.07616"/>
        <updated>2021-06-23T01:48:40.608Z</updated>
        <summary type="html"><![CDATA[This paper presents a batch-wise density-based clustering approach for local
outlier detection in massive-scale datasets. Unlike the well-known traditional
algorithms, which assume that all the data is memory-resident, our proposed
method is scalable and processes the input data chunk-by-chunk within the
confines of a limited memory buffer. A temporary clustering model is built at
the first phase; then, it is gradually updated by analyzing consecutive memory
loads of points. Subsequently, at the end of scalable clustering, the
approximate structure of the original clusters is obtained. Finally, by another
scan of the entire dataset and using a suitable criterion, an outlying score is
assigned to each object called SDCOR (Scalable Density-based Clustering
Outlierness Ratio). Evaluations on real-life and synthetic datasets demonstrate
that the proposed method has a low linear time complexity and is more effective
and efficient compared to best-known conventional density-based methods, which
need to load all data into the memory; and also, to some fast distance-based
methods, which can perform on data resident in the disk.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nozad_S/0/1/0/all/0/1"&gt;Sayyed Ahmad Naghavi Nozad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haeri_M/0/1/0/all/0/1"&gt;Maryam Amir Haeri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Folino_G/0/1/0/all/0/1"&gt;Gianluigi Folino&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Iterative Contextual Smoothing for Efficient Adversarial Defense against Gray- and Black-Box Attack. (arXiv:2106.11644v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11644</id>
        <link href="http://arxiv.org/abs/2106.11644"/>
        <updated>2021-06-23T01:48:40.601Z</updated>
        <summary type="html"><![CDATA[We propose a novel and effective input transformation based adversarial
defense method against gray- and black-box attack, which is computationally
efficient and does not require any adversarial training or retraining of a
classification model. We first show that a very simple iterative Gaussian
smoothing can effectively wash out adversarial noise and achieve substantially
high robust accuracy. Based on the observation, we propose Self-Supervised
Iterative Contextual Smoothing (SSICS), which aims to reconstruct the original
discriminative features from the Gaussian-smoothed image in context-adaptive
manner, while still smoothing out the adversarial noise. From the experiments
on ImageNet, we show that our SSICS achieves both high standard accuracy and
very competitive robust accuracy for the gray- and black-box attacks; e.g.,
transfer-based PGD-attack and score-based attack. A note-worthy point to stress
is that our defense is free of computationally expensive adversarial training,
yet, can approach its robust accuracy via input transformation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1"&gt;Sungmin Cha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_N/0/1/0/all/0/1"&gt;Naeun Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1"&gt;Youngjoon Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1"&gt;Taesup Moon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Bayesian Neural Networks. (arXiv:2008.07587v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.07587</id>
        <link href="http://arxiv.org/abs/2008.07587"/>
        <updated>2021-06-23T01:48:40.594Z</updated>
        <summary type="html"><![CDATA[Bayesian neural networks perform variational inference over the weights
however calculation of the posterior distribution remains a challenge. Our work
builds on variational inference techniques for bayesian neural networks using
the original Evidence Lower Bound. In this paper, we present a stochastic
bayesian neural network in which we maximize Evidence Lower Bound using a new
objective function which we name as Stochastic Evidence Lower Bound. We
evaluate our network on 5 publicly available UCI datasets using test RMSE and
log likelihood as the evaluation metrics. We demonstrate that our work not only
beats the previous state of the art algorithms but is also scalable to larger
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1"&gt;Abhinav Sagar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Neural Network via Stochastic Gradient Descent. (arXiv:2006.08453v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08453</id>
        <link href="http://arxiv.org/abs/2006.08453"/>
        <updated>2021-06-23T01:48:40.575Z</updated>
        <summary type="html"><![CDATA[The goal of bayesian approach used in variational inference is to minimize
the KL divergence between variational distribution and unknown posterior
distribution. This is done by maximizing the Evidence Lower Bound (ELBO). A
neural network is used to parametrize these distributions using Stochastic
Gradient Descent. This work extends the work done by others by deriving the
variational inference models. We show how SGD can be applied on bayesian neural
networks by gradient estimation techniques. For validation, we have tested our
model on 5 UCI datasets and the metrics chosen for evaluation are Root Mean
Square Error (RMSE) error and negative log likelihood. Our work considerably
beats the previous state of the art approaches for regression using bayesian
neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1"&gt;Abhinav Sagar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Stereo Image Compression with Decoder Side Information using Wyner Common Information. (arXiv:2106.11723v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.11723</id>
        <link href="http://arxiv.org/abs/2106.11723"/>
        <updated>2021-06-23T01:48:40.568Z</updated>
        <summary type="html"><![CDATA[We present a novel deep neural network (DNN) architecture for compressing an
image when a correlated image is available as side information only at the
decoder. This problem is known as distributed source coding (DSC) in
information theory. In particular, we consider a pair of stereo images, which
generally have high correlation with each other due to overlapping fields of
view, and assume that one image of the pair is to be compressed and
transmitted, while the other image is available only at the decoder. In the
proposed architecture, the encoder maps the input image to a latent space,
quantizes the latent representation, and compresses it using entropy coding.
The decoder is trained to extract the Wyner's common information between the
input image and the correlated image from the latter. The received latent
representation and the locally generated common information are passed through
a decoder network to obtain an enhanced reconstruction of the input image. The
common information provides a succinct representation of the relevant
information at the receiver. We train and demonstrate the effectiveness of the
proposed approach on the KITTI dataset of stereo image pairs. Our results show
that the proposed architecture is capable of exploiting the decoder-only side
information, and outperforms previous work on stereo image compression with
decoder side information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mital_N/0/1/0/all/0/1"&gt;Nitish Mital&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ozyilkan_E/0/1/0/all/0/1"&gt;Ezgi Ozyilkan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Garjani_A/0/1/0/all/0/1"&gt;Ali Garjani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gunduz_D/0/1/0/all/0/1"&gt;Deniz Gunduz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Learning Rate and Momentum for Training Deep Neural Networks. (arXiv:2106.11548v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11548</id>
        <link href="http://arxiv.org/abs/2106.11548"/>
        <updated>2021-06-23T01:48:40.560Z</updated>
        <summary type="html"><![CDATA[Recent progress on deep learning relies heavily on the quality and efficiency
of training algorithms. In this paper, we develop a fast training method
motivated by the nonlinear Conjugate Gradient (CG) framework. We propose the
Conjugate Gradient with Quadratic line-search (CGQ) method. On the one hand, a
quadratic line-search determines the step size according to current loss
landscape. On the other hand, the momentum factor is dynamically updated in
computing the conjugate gradient parameter (like Polak-Ribiere). Theoretical
results to ensure the convergence of our method in strong convex settings is
developed. And experiments in image classification datasets show that our
method yields faster convergence than other local solvers and has better
generalization capability (test set accuracy). One major advantage of the paper
method is that tedious hand tuning of hyperparameters like the learning rate
and momentum is avoided.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1"&gt;Zhiyong Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yixuan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Huihua Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiang_H/0/1/0/all/0/1"&gt;Hsiao-Dong Chiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving stochastic optimal control problem via stochastic maximum principle with deep learning method. (arXiv:2007.02227v5 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.02227</id>
        <link href="http://arxiv.org/abs/2007.02227"/>
        <updated>2021-06-23T01:48:40.553Z</updated>
        <summary type="html"><![CDATA[In this paper, we aim to solve the high dimensional stochastic optimal
control problem from the view of the stochastic maximum principle via deep
learning. By introducing the extended Hamiltonian system which is essentially
an FBSDE with a maximum condition, we reformulate the original control problem
as a new one. Three algorithms are proposed to solve the new control problem.
Numerical results for different examples demonstrate the effectiveness of our
proposed algorithms, especially in high dimensional cases. And an important
application of this method is to calculate the sub-linear expectations, which
correspond to a kind of fully nonlinear PDEs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Ji_S/0/1/0/all/0/1"&gt;Shaolin Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Peng_S/0/1/0/all/0/1"&gt;Shige Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Peng_Y/0/1/0/all/0/1"&gt;Ying Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xichuan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Categorising Fine-to-Coarse Grained Misinformation: An Empirical Study of COVID-19 Infodemic. (arXiv:2106.11702v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.11702</id>
        <link href="http://arxiv.org/abs/2106.11702"/>
        <updated>2021-06-23T01:48:40.547Z</updated>
        <summary type="html"><![CDATA[The spreading COVID-19 misinformation over social media already draws the
attention of many researchers. According to Google Scholar, about 26000
COVID-19 related misinformation studies have been published to date. Most of
these studies focusing on 1) detect and/or 2) analysing the characteristics of
COVID-19 related misinformation. However, the study of the social behaviours
related to misinformation is often neglected. In this paper, we introduce a
fine-grained annotated misinformation tweets dataset including social
behaviours annotation (e.g. comment or question to the misinformation). The
dataset not only allows social behaviours analysis but also suitable for both
evidence-based or non-evidence-based misinformation classification task. In
addition, we introduce leave claim out validation in our experiments and
demonstrate the misinformation classification performance could be
significantly different when applying to real-world unseen misinformation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Ye Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xingyi Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1"&gt;Carolina Scarton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aker_A/0/1/0/all/0/1"&gt;Ahmet Aker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bontcheva_K/0/1/0/all/0/1"&gt;Kalina Bontcheva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Adversarial Robustness of Synthetic Code Generation. (arXiv:2106.11629v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11629</id>
        <link href="http://arxiv.org/abs/2106.11629"/>
        <updated>2021-06-23T01:48:40.539Z</updated>
        <summary type="html"><![CDATA[Automatic code synthesis from natural language descriptions is a challenging
task. We witness massive progress in developing code generation systems for
domain-specific languages (DSLs) employing sequence-to-sequence deep learning
techniques in the recent past. In this paper, we specifically experiment with
\textsc{AlgoLisp} DSL-based generative models and showcase the existence of
significant dataset bias through different classes of adversarial examples. We
also experiment with two variants of Transformer-based models that outperform
all existing \textsc{AlgoLisp} DSL-based code generation baselines. Consistent
with the current state-of-the-art systems, our proposed models, too, achieve
poor performance under adversarial settings. Therefore, we propose several
dataset augmentation techniques to reduce bias and showcase their efficacy
using robust experimentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anand_M/0/1/0/all/0/1"&gt;Mrinal Anand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kayal_P/0/1/0/all/0/1"&gt;Pratik Kayal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1"&gt;Mayank Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Repulsive Deep Ensembles are Bayesian. (arXiv:2106.11642v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11642</id>
        <link href="http://arxiv.org/abs/2106.11642"/>
        <updated>2021-06-23T01:48:40.521Z</updated>
        <summary type="html"><![CDATA[Deep ensembles have recently gained popularity in the deep learning community
for their conceptual simplicity and efficiency. However, maintaining functional
diversity between ensemble members that are independently trained with gradient
descent is challenging. This can lead to pathologies when adding more ensemble
members, such as a saturation of the ensemble performance, which converges to
the performance of a single model. Moreover, this does not only affect the
quality of its predictions, but even more so the uncertainty estimates of the
ensemble, and thus its performance on out-of-distribution data. We hypothesize
that this limitation can be overcome by discouraging different ensemble members
from collapsing to the same function. To this end, we introduce a kernelized
repulsive term in the update rule of the deep ensembles. We show that this
simple modification not only enforces and maintains diversity among the members
but, even more importantly, transforms the maximum a posteriori inference into
proper Bayesian inference. Namely, we show that the training dynamics of our
proposed repulsive ensembles follow a Wasserstein gradient flow of the KL
divergence with the true posterior. We study repulsive terms in weight and
function space and empirically compare their performance to standard ensembles
and Bayesian baselines on synthetic and real-world prediction tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+DAngelo_F/0/1/0/all/0/1"&gt;Francesco D&amp;#x27;Angelo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1"&gt;Vincent Fortuin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Vertical Federated Learning Framework for Graph Convolutional Network. (arXiv:2106.11593v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11593</id>
        <link href="http://arxiv.org/abs/2106.11593"/>
        <updated>2021-06-23T01:48:40.504Z</updated>
        <summary type="html"><![CDATA[Recently, Graph Neural Network (GNN) has achieved remarkable success in
various real-world problems on graph data. However in most industries, data
exists in the form of isolated islands and the data privacy and security is
also an important issue. In this paper, we propose FedVGCN, a federated GCN
learning paradigm for privacy-preserving node classification task under data
vertically partitioned setting, which can be generalized to existing GCN
models. Specifically, we split the computation graph data into two parts. For
each iteration of the training process, the two parties transfer intermediate
results to each other under homomorphic encryption. We conduct experiments on
benchmark data and the results demonstrate the effectiveness of FedVGCN in the
case of GraphSage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ni_X/0/1/0/all/0/1"&gt;Xiang Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiaolong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1"&gt;Lingjuan Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1"&gt;Changhua Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weiqiang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuous-Depth Neural Models for Dynamic Graph Prediction. (arXiv:2106.11581v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11581</id>
        <link href="http://arxiv.org/abs/2106.11581"/>
        <updated>2021-06-23T01:48:40.497Z</updated>
        <summary type="html"><![CDATA[We introduce the framework of continuous-depth graph neural networks (GNNs).
Neural graph differential equations (Neural GDEs) are formalized as the
counterpart to GNNs where the input-output relationship is determined by a
continuum of GNN layers, blending discrete topological structures and
differential equations. The proposed framework is shown to be compatible with
static GNN models and is extended to dynamic and stochastic settings through
hybrid dynamical system theory. Here, Neural GDEs improve performance by
exploiting the underlying dynamics geometry, further introducing the ability to
accommodate irregularly sampled data. Results prove the effectiveness of the
proposed models across applications, such as traffic forecasting or prediction
in genetic regulatory networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Poli_M/0/1/0/all/0/1"&gt;Michael Poli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Massaroli_S/0/1/0/all/0/1"&gt;Stefano Massaroli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabideau_C/0/1/0/all/0/1"&gt;Clayton M. Rabideau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Junyoung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamashita_A/0/1/0/all/0/1"&gt;Atsushi Yamashita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asama_H/0/1/0/all/0/1"&gt;Hajime Asama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jinkyoo Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FLRA: A Reference Architecture for Federated Learning Systems. (arXiv:2106.11570v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11570</id>
        <link href="http://arxiv.org/abs/2106.11570"/>
        <updated>2021-06-23T01:48:40.489Z</updated>
        <summary type="html"><![CDATA[Federated learning is an emerging machine learning paradigm that enables
multiple devices to train models locally and formulate a global model, without
sharing the clients' local data. A federated learning system can be viewed as a
large-scale distributed system, involving different components and stakeholders
with diverse requirements and constraints. Hence, developing a federated
learning system requires both software system design thinking and machine
learning knowledge. Although much effort has been put into federated learning
from the machine learning perspectives, our previous systematic literature
review on the area shows that there is a distinct lack of considerations for
software architecture design for federated learning. In this paper, we propose
FLRA, a reference architecture for federated learning systems, which provides a
template design for federated learning-based solutions. The proposed FLRA
reference architecture is based on an extensive review of existing patterns of
federated learning systems found in the literature and existing industrial
implementation. The FLRA reference architecture consists of a pool of
architectural patterns that could address the frequently recurring design
problems in federated learning architectures. The FLRA reference architecture
can serve as a design guideline to assist architects and developers with
practical solutions for their problems, which can be further customised.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1"&gt;Sin Kit Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1"&gt;Qinghua Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paik_H/0/1/0/all/0/1"&gt;Hye-Young Paik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Liming Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Framework for Conservative Exploration. (arXiv:2106.11692v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11692</id>
        <link href="http://arxiv.org/abs/2106.11692"/>
        <updated>2021-06-23T01:48:40.480Z</updated>
        <summary type="html"><![CDATA[We study bandits and reinforcement learning (RL) subject to a conservative
constraint where the agent is asked to perform at least as well as a given
baseline policy. This setting is particular relevant in real-world domains
including digital marketing, healthcare, production, finance, etc. For
multi-armed bandits, linear bandits and tabular RL, specialized algorithms and
theoretical analyses were proposed in previous work. In this paper, we present
a unified framework for conservative bandits and RL, in which our core
technique is to calculate the necessary and sufficient budget obtained from
running the baseline policy. For lower bounds, our framework gives a black-box
reduction that turns a certain lower bound in the nonconservative setting into
a new lower bound in the conservative setting. We strengthen the existing lower
bound for conservative multi-armed bandits and obtain new lower bounds for
conservative linear bandits, tabular RL and low-rank MDP. For upper bounds, our
framework turns a certain nonconservative upper-confidence-bound (UCB)
algorithm into a conservative algorithm with a simple analysis. For multi-armed
bandits, linear bandits and tabular RL, our new upper bounds tighten or match
existing ones with significantly simpler analyses. We also obtain a new upper
bound for conservative low-rank MDP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yunchang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tianhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1"&gt;Han Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcelon_E/0/1/0/all/0/1"&gt;Evrard Garcelon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pirotta_M/0/1/0/all/0/1"&gt;Matteo Pirotta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lazaric_A/0/1/0/all/0/1"&gt;Alessandro Lazaric&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1"&gt;Simon S. Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Latent Space Model for Graph Representation Learning. (arXiv:2106.11721v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11721</id>
        <link href="http://arxiv.org/abs/2106.11721"/>
        <updated>2021-06-23T01:48:40.460Z</updated>
        <summary type="html"><![CDATA[Graph representation learning is a fundamental problem for modeling
relational data and benefits a number of downstream applications. Traditional
Bayesian-based graph models and recent deep learning based GNN either suffer
from impracticability or lack interpretability, thus combined models for
undirected graphs have been proposed to overcome the weaknesses. As a large
portion of real-world graphs are directed graphs (of which undirected graphs
are special cases), in this paper, we propose a Deep Latent Space Model (DLSM)
for directed graphs to incorporate the traditional latent variable based
generative model into deep learning frameworks. Our proposed model consists of
a graph convolutional network (GCN) encoder and a stochastic decoder, which are
layer-wise connected by a hierarchical variational auto-encoder architecture.
By specifically modeling the degree heterogeneity using node random factors,
our model possesses better interpretability in both community structure and
degree heterogeneity. For fast inference, the stochastic gradient variational
Bayes (SGVB) is adopted using a non-iterative recognition model, which is much
more scalable than traditional MCMC-based methods. The experiments on
real-world datasets show that the proposed model achieves the state-of-the-art
performances on both link prediction and community detection tasks while
learning interpretable node embeddings. The source code is available at
https://github.com/upperr/DLSM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hanxuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1"&gt;Qingchao Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1"&gt;Wenji Mao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Equivalence Between Private Classification and Online Prediction. (arXiv:2003.00563v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.00563</id>
        <link href="http://arxiv.org/abs/2003.00563"/>
        <updated>2021-06-23T01:48:40.452Z</updated>
        <summary type="html"><![CDATA[We prove that every concept class with finite Littlestone dimension can be
learned by an (approximate) differentially-private algorithm. This answers an
open question of Alon et al. (STOC 2019) who proved the converse statement
(this question was also asked by Neel et al.~(FOCS 2019)). Together these two
results yield an equivalence between online learnability and private PAC
learnability.

We introduce a new notion of algorithmic stability called "global stability"
which is essential to our proof and may be of independent interest. We also
discuss an application of our results to boosting the privacy and accuracy
parameters of differentially-private learners.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bun_M/0/1/0/all/0/1"&gt;Mark Bun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Livni_R/0/1/0/all/0/1"&gt;Roi Livni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1"&gt;Shay Moran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MMD-MIX: Value Function Factorisation with Maximum Mean Discrepancy for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2106.11652v1 [cs.MA])]]></title>
        <id>http://arxiv.org/abs/2106.11652</id>
        <link href="http://arxiv.org/abs/2106.11652"/>
        <updated>2021-06-23T01:48:40.445Z</updated>
        <summary type="html"><![CDATA[In the real world, many tasks require multiple agents to cooperate with each
other under the condition of local observations. To solve such problems, many
multi-agent reinforcement learning methods based on Centralized Training with
Decentralized Execution have been proposed. One representative class of work is
value decomposition, which decomposes the global joint Q-value $Q_\text{jt}$
into individual Q-values $Q_a$ to guide individuals' behaviors, e.g. VDN
(Value-Decomposition Networks) and QMIX. However, these baselines often ignore
the randomness in the situation. We propose MMD-MIX, a method that combines
distributional reinforcement learning and value decomposition to alleviate the
above weaknesses. Besides, to improve data sampling efficiency, we were
inspired by REM (Random Ensemble Mixture) which is a robust RL algorithm to
explicitly introduce randomness into the MMD-MIX. The experiments demonstrate
that MMD-MIX outperforms prior baselines in the StarCraft Multi-Agent Challenge
(SMAC) environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhiwei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dapeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yunpeng Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_G/0/1/0/all/0/1"&gt;Guoliang Fan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finding Valid Adjustments under Non-ignorability with Minimal DAG Knowledge. (arXiv:2106.11560v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11560</id>
        <link href="http://arxiv.org/abs/2106.11560"/>
        <updated>2021-06-23T01:48:40.438Z</updated>
        <summary type="html"><![CDATA[Treatment effect estimation from observational data is a fundamental problem
in causal inference. There are two very different schools of thought that have
tackled this problem. On the one hand, the Pearlian framework commonly assumes
structural knowledge (provided by an expert) in the form of Directed Acyclic
Graphs (DAGs) and provides graphical criteria such as the back-door criterion
to identify the valid adjustment sets. On the other hand, the potential
outcomes (PO) framework commonly assumes that all the observed features satisfy
ignorability (i.e., no hidden confounding), which in general is untestable. In
this work, we take steps to bridge these two frameworks. We show that even if
we know only one parent of the treatment variable (provided by an expert), then
quite remarkably it suffices to test a broad class of (but not all) back-door
criteria. Importantly, we also cover the non-trivial case where the entire set
of observed features is not ignorable (generalizing the PO framework) without
requiring all the parents of the treatment variable to be observed. Our key
technical idea involves a more general result -- Given a synthetic sub-sampling
(or environment) variable that is a function of the parent variable, we show
that an invariance test involving this sub-sampling variable is equivalent to
testing a broad class of back-door criteria. We demonstrate our approach on
synthetic data as well as real causal effect estimation benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1"&gt;Abhin Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1"&gt;Karthikeyan Shanmugam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1"&gt;Kartik Ahuja&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient-based Label Binning in Multi-label Classification. (arXiv:2106.11690v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11690</id>
        <link href="http://arxiv.org/abs/2106.11690"/>
        <updated>2021-06-23T01:48:40.431Z</updated>
        <summary type="html"><![CDATA[In multi-label classification, where a single example may be associated with
several class labels at the same time, the ability to model dependencies
between labels is considered crucial to effectively optimize non-decomposable
evaluation measures, such as the Subset 0/1 loss. The gradient boosting
framework provides a well-studied foundation for learning models that are
specifically tailored to such a loss function and recent research attests the
ability to achieve high predictive accuracy in the multi-label setting. The
utilization of second-order derivatives, as used by many recent boosting
approaches, helps to guide the minimization of non-decomposable losses, due to
the information about pairs of labels it incorporates into the optimization
process. On the downside, this comes with high computational costs, even if the
number of labels is small. In this work, we address the computational
bottleneck of such approach -- the need to solve a system of linear equations
-- by integrating a novel approximation technique into the boosting procedure.
Based on the derivatives computed during training, we dynamically group the
labels into a predefined number of bins to impose an upper bound on the
dimensionality of the linear system. Our experiments, using an existing
rule-based algorithm, suggest that this may boost the speed of training,
without any significant loss in predictive performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rapp_M/0/1/0/all/0/1"&gt;Michael Rapp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mencia_E/0/1/0/all/0/1"&gt;Eneldo Loza Menc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Furnkranz_J/0/1/0/all/0/1"&gt;Johannes F&amp;#xfc;rnkranz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1"&gt;Eyke H&amp;#xfc;llermeier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adapting Stepsizes by Momentumized Gradients Improves Optimization and Generalization. (arXiv:2106.11514v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11514</id>
        <link href="http://arxiv.org/abs/2106.11514"/>
        <updated>2021-06-23T01:48:40.412Z</updated>
        <summary type="html"><![CDATA[Adaptive gradient methods, such as \textsc{Adam}, have achieved tremendous
success in machine learning. Scaling gradients by square roots of the running
averages of squared past gradients, such methods are able to attain rapid
training of modern deep neural networks. Nevertheless, they are observed to
generalize worse than stochastic gradient descent (\textsc{SGD}) and tend to be
trapped in local minima at an early stage during training. Intriguingly, we
discover that substituting the gradient in the preconditioner term with the
momentumized version in \textsc{Adam} can well solve the issues. The intuition
is that gradient with momentum contains more accurate directional information
and therefore its second moment estimation is a better choice for scaling than
raw gradient's. Thereby we propose \textsc{AdaMomentum} as a new optimizer
reaching the goal of training faster while generalizing better. We further
develop a theory to back up the improvement in optimization and generalization
and provide convergence guarantee under both convex and nonconvex settings.
Extensive experiments on various models and tasks demonstrate that
\textsc{AdaMomentum} exhibits comparable performance to \textsc{SGD} on vision
tasks, and achieves state-of-the-art results consistently on other tasks
including language processing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yizhou Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1"&gt;Yue Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1"&gt;Can Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yulun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yun Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Particle Cloud Generation with Message Passing Generative Adversarial Networks. (arXiv:2106.11535v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11535</id>
        <link href="http://arxiv.org/abs/2106.11535"/>
        <updated>2021-06-23T01:48:40.405Z</updated>
        <summary type="html"><![CDATA[In high energy physics (HEP), jets are collections of correlated particles
produced ubiquitously in particle collisions such as those at the CERN Large
Hadron Collider (LHC). Machine-learning-based generative models, such as
generative adversarial networks (GANs), have the potential to significantly
accelerate LHC jet simulations. However, despite jets having a natural
representation as a set of particles in momentum-space, a.k.a. a particle
cloud, to our knowledge there exist no generative models applied to such a
dataset. We introduce a new particle cloud dataset (JetNet), and, due to
similarities between particle and point clouds, apply to it existing point
cloud GANs. Results are evaluated using (1) the 1-Wasserstein distance between
high- and low-level feature distributions, (2) a newly developed Fr\'{e}chet
ParticleNet Distance, and (3) the coverage and (4) minimum matching distance
metrics. Existing GANs are found to be inadequate for physics applications,
hence we develop a new message passing GAN (MPGAN), which outperforms existing
point cloud GANs on virtually every metric and shows promise for use in HEP. We
propose JetNet as a novel point-cloud-style dataset for the machine learning
community to experiment with, and set MPGAN as a benchmark to improve upon for
future generative models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kansal_R/0/1/0/all/0/1"&gt;Raghav Kansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duarte_J/0/1/0/all/0/1"&gt;Javier Duarte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hao Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orzari_B/0/1/0/all/0/1"&gt;Breno Orzari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomei_T/0/1/0/all/0/1"&gt;Thiago Tomei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pierini_M/0/1/0/all/0/1"&gt;Maurizio Pierini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Touranakou_M/0/1/0/all/0/1"&gt;Mary Touranakou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vlimant_J/0/1/0/all/0/1"&gt;Jean-Roch Vlimant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunopulos_D/0/1/0/all/0/1"&gt;Dimitrios Gunopulos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning for Model Order Selection in MIMO OFDM Systems. (arXiv:2106.11633v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.11633</id>
        <link href="http://arxiv.org/abs/2106.11633"/>
        <updated>2021-06-23T01:48:40.398Z</updated>
        <summary type="html"><![CDATA[A variety of wireless channel estimation methods, e.g., MUSIC and ESPRIT,
rely on prior knowledge of the model order. Therefore, it is important to
correctly estimate the number of multipath components (MPCs) which compose such
channels. However, environments with many scatterers may generate MPCs which
are closely spaced. This clustering of MPCs in addition to noise makes the
model order selection task difficult in practice to currently known algorithms.
In this paper, we exploit the multidimensional characteristics of MIMO
orthogonal frequency division multiplexing (OFDM) systems and propose a machine
learning (ML) method capable of determining the number of MPCs with a higher
accuracy than state of the art methods in almost coherent scenarios. Moreover,
our results show that our proposed ML method has an enhanced reliability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Boas_B/0/1/0/all/0/1"&gt;Brenda Vilas Boas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zirwas_W/0/1/0/all/0/1"&gt;Wolfgang Zirwas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Haardt_M/0/1/0/all/0/1"&gt;Martin Haardt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information Retrieval for ZeroSpeech 2021: The Submission by University of Wroclaw. (arXiv:2106.11603v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11603</id>
        <link href="http://arxiv.org/abs/2106.11603"/>
        <updated>2021-06-23T01:48:40.391Z</updated>
        <summary type="html"><![CDATA[We present a number of low-resource approaches to the tasks of the Zero
Resource Speech Challenge 2021. We build on the unsupervised representations of
speech proposed by the organizers as a baseline, derived from CPC and clustered
with the k-means algorithm. We demonstrate that simple methods of refining
those representations can narrow the gap, or even improve upon the solutions
which use a high computational budget. The results lead to the conclusion that
the CPC-derived representations are still too noisy for training language
models, but stable enough for simpler forms of pattern matching and retrieval.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chorowski_J/0/1/0/all/0/1"&gt;Jan Chorowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciesielski_G/0/1/0/all/0/1"&gt;Grzegorz Ciesielski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dzikowski_J/0/1/0/all/0/1"&gt;Jaros&amp;#x142;aw Dzikowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lancucki_A/0/1/0/all/0/1"&gt;Adrian &amp;#x141;a&amp;#x144;cucki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marxer_R/0/1/0/all/0/1"&gt;Ricard Marxer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Opala_M/0/1/0/all/0/1"&gt;Mateusz Opala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pusz_P/0/1/0/all/0/1"&gt;Piotr Pusz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rychlikowski_P/0/1/0/all/0/1"&gt;Pawe&amp;#x142; Rychlikowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stypulkowski_M/0/1/0/all/0/1"&gt;Micha&amp;#x142; Stypu&amp;#x142;kowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hardness of Samples Is All You Need: Protecting Deep Learning Models Using Hardness of Samples. (arXiv:2106.11424v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11424</id>
        <link href="http://arxiv.org/abs/2106.11424"/>
        <updated>2021-06-23T01:48:40.383Z</updated>
        <summary type="html"><![CDATA[Several recent studies have shown that Deep Neural Network (DNN)-based
classifiers are vulnerable against model extraction attacks. In model
extraction attacks, an adversary exploits the target classifier to create a
surrogate classifier imitating the target classifier with respect to some
criteria. In this paper, we investigate the hardness degree of samples and
demonstrate that the hardness degree histogram of model extraction attacks
samples is distinguishable from the hardness degree histogram of normal
samples. Normal samples come from the target classifier's training data
distribution. As the training process of DNN-based classifiers is done in
several epochs, we can consider this process as a sequence of subclassifiers so
that each subclassifier is created at the end of an epoch. We use the sequence
of subclassifiers to calculate the hardness degree of samples. We investigate
the relation between hardness degree of samples and the trust in the classifier
outputs. We propose Hardness-Oriented Detection Approach (HODA) to detect the
sample sequences of model extraction attacks. The results demonstrate that HODA
can detect the sample sequences of model extraction attacks with a high success
rate by only watching 100 attack samples. We also investigate the hardness
degree of adversarial examples and indicate that the hardness degree histogram
of adversarial examples is distinct from the hardness degree histogram of
normal samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sadeghzadeh_A/0/1/0/all/0/1"&gt;Amir Mahdi Sadeghzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dehghan_F/0/1/0/all/0/1"&gt;Faezeh Dehghan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sobhanian_A/0/1/0/all/0/1"&gt;Amir Mohammad Sobhanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jalili_R/0/1/0/all/0/1"&gt;Rasool Jalili&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement learning for PHY layer communications. (arXiv:2106.11595v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.11595</id>
        <link href="http://arxiv.org/abs/2106.11595"/>
        <updated>2021-06-23T01:48:40.362Z</updated>
        <summary type="html"><![CDATA[In this chapter, we will give comprehensive examples of applying RL in
optimizing the physical layer of wireless communications by defining different
class of problems and the possible solutions to handle them. In Section 9.2, we
present all the basic theory needed to address a RL problem, i.e. Markov
decision process (MDP), Partially observable Markov decision process (POMDP),
but also two very important and widely used algorithms for RL, i.e. the
Q-learning and SARSA algorithms. We also introduce the deep reinforcement
learning (DRL) paradigm and the section ends with an introduction to the
multi-armed bandits (MAB) framework. Section 9.3 focuses on some toy examples
to illustrate how the basic concepts of RL are employed in communication
systems. We present applications extracted from literature with simplified
system models using similar notation as in Section 9.2 of this Chapter. In
Section 9.3, we also focus on modeling RL problems, i.e. how action and state
spaces and rewards are chosen. The Chapter is concluded in Section 9.4 with a
prospective thought on RL trends and it ends with a review of a broader state
of the art in Section 9.5.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mary_P/0/1/0/all/0/1"&gt;Philippe Mary&lt;/a&gt; (IETR), &lt;a href="http://arxiv.org/find/cs/1/au:+Koivunen_V/0/1/0/all/0/1"&gt;Visa Koivunen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moy_C/0/1/0/all/0/1"&gt;Christophe Moy&lt;/a&gt; (IETR)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Accurate Non-accelerometer-based PPG Motion Artifact Removal Technique using CycleGAN. (arXiv:2106.11512v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11512</id>
        <link href="http://arxiv.org/abs/2106.11512"/>
        <updated>2021-06-23T01:48:40.355Z</updated>
        <summary type="html"><![CDATA[A photoplethysmography (PPG) is an uncomplicated and inexpensive optical
technique widely used in the healthcare domain to extract valuable
health-related information, e.g., heart rate variability, blood pressure, and
respiration rate. PPG signals can easily be collected continuously and remotely
using portable wearable devices. However, these measuring devices are
vulnerable to motion artifacts caused by daily life activities. The most common
ways to eliminate motion artifacts use extra accelerometer sensors, which
suffer from two limitations: i) high power consumption and ii) the need to
integrate an accelerometer sensor in a wearable device (which is not required
in certain wearables). This paper proposes a low-power non-accelerometer-based
PPG motion artifacts removal method outperforming the accuracy of the existing
methods. We use Cycle Generative Adversarial Network to reconstruct clean PPG
signals from noisy PPG signals. Our novel machine-learning-based technique
achieves 9.5 times improvement in motion artifact removal compared to the
state-of-the-art without using extra sensors such as an accelerometer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zargari_A/0/1/0/all/0/1"&gt;Amir Hosein Afandizadeh Zargari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aqajari_S/0/1/0/all/0/1"&gt;Seyed Amir Hossein Aqajari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khodabandeh_H/0/1/0/all/0/1"&gt;Hadi Khodabandeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahmani_A/0/1/0/all/0/1"&gt;Amir M. Rahmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurdahi_F/0/1/0/all/0/1"&gt;Fadi Kurdahi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SSUL: Semantic Segmentation with Unknown Label for Exemplar-based Class-Incremental Learning. (arXiv:2106.11562v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11562</id>
        <link href="http://arxiv.org/abs/2106.11562"/>
        <updated>2021-06-23T01:48:40.347Z</updated>
        <summary type="html"><![CDATA[We consider a class-incremental semantic segmentation (CISS) problem. While
some recently proposed algorithms utilized variants of knowledge distillation
(KD) technique to tackle the problem, they only partially addressed the key
additional challenges in CISS that causes the catastrophic forgetting; i.e.,
the semantic drift of the background class and multi-label prediction issue. To
better address these challenges, we propose a new method, dubbed as SSUL-M
(Semantic Segmentation with Unknown Label with Memory), by carefully combining
several techniques tailored for semantic segmentation. More specifically, we
make three main contributions; (1) modeling unknown class within the background
class to help learning future classes (help plasticity), (2) freezing backbone
network and past classifiers with binary cross-entropy loss and pseudo-labeling
to overcome catastrophic forgetting (help stability), and (3) utilizing tiny
exemplar memory for the first time in CISS to improve both plasticity and
stability. As a result, we show our method achieves significantly better
performance than the recent state-of-the-art baselines on the standard
benchmark datasets. Furthermore, we justify our contributions with thorough and
extensive ablation analyses and discuss different natures of the CISS problem
compared to the standard class-incremental learning for classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sungmin Cha. Beomyoung Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1"&gt;Youngjoon Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1"&gt;Taesup Moon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BiAdam: Fast Adaptive Bilevel Optimization Methods. (arXiv:2106.11396v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.11396</id>
        <link href="http://arxiv.org/abs/2106.11396"/>
        <updated>2021-06-23T01:48:40.340Z</updated>
        <summary type="html"><![CDATA[Bilevel optimization recently has attracted increased interest in machine
learning due to its many applications such as hyper-parameter optimization and
policy optimization. Although some methods recently have been proposed to solve
the bilevel problems, these methods do not consider using adaptive learning
rates. To fill this gap, in the paper, we propose a class of fast and effective
adaptive methods for solving bilevel optimization problems that the outer
problem is possibly nonconvex and the inner problem is strongly-convex.
Specifically, we propose a fast single-loop BiAdam algorithm based on the basic
momentum technique, which achieves a sample complexity of
$\tilde{O}(\epsilon^{-4})$ for finding an $\epsilon$-stationary point. At the
same time, we propose an accelerated version of BiAdam algorithm (VR-BiAdam) by
using variance reduced technique, which reaches the best known sample
complexity of $\tilde{O}(\epsilon^{-3})$. To further reduce computation in
estimating derivatives, we propose a fast single-loop stochastic approximated
BiAdam algorithm (saBiAdam) by avoiding the Hessian inverse, which still
achieves a sample complexity of $\tilde{O}(\epsilon^{-4})$ without large
batches. We further present an accelerated version of saBiAdam algorithm
(VR-saBiAdam), which also reaches the best known sample complexity of
$\tilde{O}(\epsilon^{-3})$. We apply the unified adaptive matrices to our
methods as the SUPER-ADAM \citep{huang2021super}, which including many types of
adaptive learning rates. Moreover, our framework can flexibly use the momentum
and variance reduced techniques. In particular, we provide a useful convergence
analysis framework for both the constrained and unconstrained bilevel
optimization. To the best of our knowledge, we first study the adaptive bilevel
optimization methods with adaptive learning rates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feihu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learn Like The Pro: Norms from Theory to Size Neural Computation. (arXiv:2106.11409v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11409</id>
        <link href="http://arxiv.org/abs/2106.11409"/>
        <updated>2021-06-23T01:48:40.332Z</updated>
        <summary type="html"><![CDATA[The optimal design of neural networks is a critical problem in many
applications. Here, we investigate how dynamical systems with polynomial
nonlinearities can inform the design of neural systems that seek to emulate
them. We propose a Learnability metric and its associated features to quantify
the near-equilibrium behavior of learning dynamics. Equating the Learnability
of neural systems with equivalent parameter estimation metric of the reference
system establishes bounds on network structure. In this way, norms from theory
provide a good first guess for neural structure, which may then further adapt
with data. The proposed approach neither requires training nor training data.
It reveals exact sizing for a class of neural networks with multiplicative
nodes that mimic continuous- or discrete-time polynomial dynamics. It also
provides relatively tight lower size bounds for classical feed-forward networks
that is consistent with simulated assessments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trautner_M/0/1/0/all/0/1"&gt;Margaret Trautner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Ziwei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravela_S/0/1/0/all/0/1"&gt;Sai Ravela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Logical Neural Network Structure With More Direct Mapping From Logical Relations. (arXiv:2106.11463v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.11463</id>
        <link href="http://arxiv.org/abs/2106.11463"/>
        <updated>2021-06-23T01:48:40.312Z</updated>
        <summary type="html"><![CDATA[Logical relations widely exist in human activities. Human use them for making
judgement and decision according to various conditions, which are embodied in
the form of \emph{if-then} rules. As an important kind of cognitive
intelligence, it is prerequisite of representing and storing logical relations
rightly into computer systems so as to make automatic judgement and decision,
especially for high-risk domains like medical diagnosis. However, current
numeric ANN (Artificial Neural Network) models are good at perceptual
intelligence such as image recognition while they are not good at cognitive
intelligence such as logical representation, blocking the further application
of ANN. To solve it, researchers have tried to design logical ANN models to
represent and store logical relations. Although there are some advances in this
research area, recent works still have disadvantages because the structures of
these logical ANN models still don't map more directly with logical relations
which will cause the corresponding logical relations cannot be read out from
their network structures. Therefore, in order to represent logical relations
more clearly by the neural network structure and to read out logical relations
from it, this paper proposes a novel logical ANN model by designing the new
logical neurons and links in demand of logical representation. Compared with
the recent works on logical ANN models, this logical ANN model has more clear
corresponding with logical relations using the more direct mapping method
herein, thus logical relations can be read out following the connection
patterns of the network structure. Additionally, less neurons are used.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Gang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequential Late Fusion Technique for Multi-modal Sentiment Analysis. (arXiv:2106.11473v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11473</id>
        <link href="http://arxiv.org/abs/2106.11473"/>
        <updated>2021-06-23T01:48:40.305Z</updated>
        <summary type="html"><![CDATA[Multi-modal sentiment analysis plays an important role for providing better
interactive experiences to users. Each modality in multi-modal data can provide
different viewpoints or reveal unique aspects of a user's emotional state. In
this work, we use text, audio and visual modalities from MOSI dataset and we
propose a novel fusion technique using a multi-head attention LSTM network.
Finally, we perform a classification task and evaluate its performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_D/0/1/0/all/0/1"&gt;Debapriya Banerjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lygerakis_F/0/1/0/all/0/1"&gt;Fotios Lygerakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makedon_F/0/1/0/all/0/1"&gt;Fillia Makedon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ConvDySAT: Deep Neural Representation Learning on Dynamic Graphs via Self-Attention and Convolutional Neural Networks. (arXiv:2106.11430v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11430</id>
        <link href="http://arxiv.org/abs/2106.11430"/>
        <updated>2021-06-23T01:48:40.299Z</updated>
        <summary type="html"><![CDATA[Learning node representations on temporal graphs is a fundamental step to
learn real-word dynamic graphs efficiently. Real-world graphs have the nature
of continuously evolving over time, such as changing edges weights, removing
and adding nodes and appearing and disappearing of edges, while previous graph
representation learning methods focused generally on static graphs. We present
ConvDySAT as an enhancement of DySAT, one of the state-of-the-art dynamic
methods, by augmenting convolution neural networks with the self-attention
mechanism, the employed method in DySAT to express the structural and temporal
evolution. We conducted single-step link prediction on a communication network
and rating network, Experimental results show significant performance gains for
ConvDySAT over various state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hafez_A/0/1/0/all/0/1"&gt;Ahmad Hafez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Praphul_A/0/1/0/all/0/1"&gt;Atulya Praphul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaradt_Y/0/1/0/all/0/1"&gt;Yousef Jaradt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Godwin_E/0/1/0/all/0/1"&gt;Ezani Godwin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SeqNetVLAD vs PointNetVLAD: Image Sequence vs 3D Point Clouds for Day-Night Place Recognition. (arXiv:2106.11481v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11481</id>
        <link href="http://arxiv.org/abs/2106.11481"/>
        <updated>2021-06-23T01:48:40.291Z</updated>
        <summary type="html"><![CDATA[Place Recognition is a crucial capability for mobile robot localization and
navigation. Image-based or Visual Place Recognition (VPR) is a challenging
problem as scene appearance and camera viewpoint can change significantly when
places are revisited. Recent VPR methods based on ``sequential
representations'' have shown promising results as compared to traditional
sequence score aggregation or single image based techniques. In parallel to
these endeavors, 3D point clouds based place recognition is also being explored
following the advances in deep learning based point cloud processing. However,
a key question remains: is an explicit 3D structure based place representation
always superior to an implicit ``spatial'' representation based on sequence of
RGB images which can inherently learn scene structure. In this extended
abstract, we attempt to compare these two types of methods by considering a
similar ``metric span'' to represent places. We compare a 3D point cloud based
method (PointNetVLAD) with image sequence based methods (SeqNet and others) and
showcase that image sequence based techniques approach, and can even surpass,
the performance achieved by point cloud based methods for a given metric span.
These performance variations can be attributed to differences in data richness
of input sensors as well as data accumulation strategies for a mobile robot.
While a perfect apple-to-apple comparison may not be feasible for these two
different modalities, the presented comparison takes a step in the direction of
answering deeper questions regarding spatial representations, relevant to
several applications like Autonomous Driving and Augmented/Virtual Reality.
Source code available publicly https://github.com/oravus/seqNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1"&gt;Sourav Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1"&gt;Michael Milford&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Instance-Optimal Compressed Sensing via Posterior Sampling. (arXiv:2106.11438v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11438</id>
        <link href="http://arxiv.org/abs/2106.11438"/>
        <updated>2021-06-23T01:48:40.285Z</updated>
        <summary type="html"><![CDATA[We characterize the measurement complexity of compressed sensing of signals
drawn from a known prior distribution, even when the support of the prior is
the entire space (rather than, say, sparse vectors). We show for Gaussian
measurements and \emph{any} prior distribution on the signal, that the
posterior sampling estimator achieves near-optimal recovery guarantees.
Moreover, this result is robust to model mismatch, as long as the distribution
estimate (e.g., from an invertible generative model) is close to the true
distribution in Wasserstein distance. We implement the posterior sampling
estimator for deep generative priors using Langevin dynamics, and empirically
find that it produces accurate estimates with more diversity than MAP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jalal_A/0/1/0/all/0/1"&gt;Ajil Jalal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karmalkar_S/0/1/0/all/0/1"&gt;Sushrut Karmalkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1"&gt;Alexandros G. Dimakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1"&gt;Eric Price&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributional Gradient Matching for Learning Uncertain Neural Dynamics Models. (arXiv:2106.11609v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11609</id>
        <link href="http://arxiv.org/abs/2106.11609"/>
        <updated>2021-06-23T01:48:40.263Z</updated>
        <summary type="html"><![CDATA[Differential equations in general and neural ODEs in particular are an
essential technique in continuous-time system identification. While many
deterministic learning algorithms have been designed based on numerical
integration via the adjoint method, many downstream tasks such as active
learning, exploration in reinforcement learning, robust control, or filtering
require accurate estimates of predictive uncertainties. In this work, we
propose a novel approach towards estimating epistemically uncertain neural
ODEs, avoiding the numerical integration bottleneck. Instead of modeling
uncertainty in the ODE parameters, we directly model uncertainties in the state
space. Our algorithm - distributional gradient matching (DGM) - jointly trains
a smoother and a dynamics model and matches their gradients via minimizing a
Wasserstein loss. Our experiments show that, compared to traditional
approximate inference methods based on numerical integration, our approach is
faster to train, faster at predicting previously unseen trajectories, and in
the context of neural ODEs, significantly more accurate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Treven_L/0/1/0/all/0/1"&gt;Lenart Treven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wenk_P/0/1/0/all/0/1"&gt;Philippe Wenk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dorfler_F/0/1/0/all/0/1"&gt;Florian D&amp;#xf6;rfler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1"&gt;Andreas Krause&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel Clustering with Sigmoid-based Regularization for Efficient Segmentation of Sequential Data. (arXiv:2106.11541v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11541</id>
        <link href="http://arxiv.org/abs/2106.11541"/>
        <updated>2021-06-23T01:48:40.253Z</updated>
        <summary type="html"><![CDATA[Kernel segmentation aims at partitioning a data sequence into several
non-overlapping segments that may have nonlinear and complex structures. In
general, it is formulated as a discrete optimization problem with combinatorial
constraints. A popular algorithm for optimally solving this problem is dynamic
programming (DP), which has quadratic computation and memory requirements.
Given that sequences in practice are too long, this algorithm is not a
practical approach. Although many heuristic algorithms have been proposed to
approximate the optimal segmentation, they have no guarantee on the quality of
their solutions. In this paper, we take a differentiable approach to alleviate
the aforementioned issues. First, we introduce a novel sigmoid-based
regularization to smoothly approximate the combinatorial constraints. Combining
it with objective of the balanced kernel clustering, we formulate a
differentiable model termed Kernel clustering with sigmoid-based regularization
(KCSR), where the gradient-based algorithm can be exploited to obtain the
optimal segmentation. Second, we develop a stochastic variant of the proposed
model. By using the stochastic gradient descent algorithm, which has much lower
time and space complexities, for optimization, the second model can perform
segmentation on overlong data sequences. Finally, for simultaneously segmenting
multiple data sequences, we slightly modify the sigmoid-based regularization to
further introduce an extended variant of the proposed model. Through extensive
experiments on various types of data sequences performances of our models are
evaluated and compared with those of the existing methods. The experimental
results validate advantages of the proposed models. Our Matlab source code is
available on github.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Doan_T/0/1/0/all/0/1"&gt;Tung Doan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takasu_A/0/1/0/all/0/1"&gt;Atsuhiro Takasu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-constrained deep neural network method for estimating parameters in a redox flow battery. (arXiv:2106.11451v1 [physics.chem-ph])]]></title>
        <id>http://arxiv.org/abs/2106.11451</id>
        <link href="http://arxiv.org/abs/2106.11451"/>
        <updated>2021-06-23T01:48:40.247Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a physics-constrained deep neural network (PCDNN)
method for parameter estimation in the zero-dimensional (0D) model of the
vanadium redox flow battery (VRFB). In this approach, we use deep neural
networks (DNNs) to approximate the model parameters as functions of the
operating conditions. This method allows the integration of the VRFB
computational models as the physical constraints in the parameter learning
process, leading to enhanced accuracy of parameter estimation and cell voltage
prediction. Using an experimental dataset, we demonstrate that the PCDNN method
can estimate model parameters for a range of operating conditions and improve
the 0D model prediction of voltage compared to the 0D model prediction with
constant operation-condition-independent parameters estimated with traditional
inverse methods. We also demonstrate that the PCDNN approach has an improved
generalization ability for estimating parameter values for operating conditions
not used in the DNN training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+He_Q/0/1/0/all/0/1"&gt;QiZhi He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Stinis_P/0/1/0/all/0/1"&gt;Panos Stinis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Tartakovsky_A/0/1/0/all/0/1"&gt;Alexandre Tartakovsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Architecture Search Without Training Nor Labels: A Pruning Perspective. (arXiv:2106.11542v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11542</id>
        <link href="http://arxiv.org/abs/2106.11542"/>
        <updated>2021-06-23T01:48:40.240Z</updated>
        <summary type="html"><![CDATA[With leveraging the weight-sharing and continuous relaxation to enable
gradient-descent to alternately optimize the supernet weights and the
architecture parameters through a bi-level optimization paradigm,
\textit{Differentiable ARchiTecture Search} (DARTS) has become the mainstream
method in Neural Architecture Search (NAS) due to its simplicity and
efficiency. However, more recent works found that the performance of the
searched architecture barely increases with the optimization proceeding in
DARTS. In addition, several concurrent works show that the NAS could find more
competitive architectures without labels. The above observations reveal that
the supervision signal in DARTS may be a poor indicator for architecture
optimization, inspiring a foundational question: instead of using the
supervision signal to perform bi-level optimization, \textit{can we find
high-quality architectures \textbf{without any training nor labels}}? We
provide an affirmative answer by customizing the NAS as a network pruning at
initialization problem. By leveraging recent techniques on the network pruning
at initialization, we designed a FreeFlow proxy to score the importance of
candidate operations in NAS without any training nor labels, and proposed a
novel framework called \textit{training and label free neural architecture
search} (\textbf{FreeNAS}) accordingly. We show that, without any training nor
labels, FreeNAS with the proposed FreeFlow proxy can outperform most NAS
baselines. More importantly, our framework is extremely efficient, which
completes the architecture search within only \textbf{3.6s} and \textbf{79s} on
a single GPU for the NAS-Bench-201 and DARTS search space, respectively. We
hope our work inspires more attempts in solving NAS from the perspective of
pruning at initialization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Miao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1"&gt;Steven Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Shirui Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1"&gt;Xiaojun Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1"&gt;Gholamreza Haffari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recent Deep Semi-supervised Learning Approaches and Related Works. (arXiv:2106.11528v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11528</id>
        <link href="http://arxiv.org/abs/2106.11528"/>
        <updated>2021-06-23T01:48:40.232Z</updated>
        <summary type="html"><![CDATA[The author of this work proposes an overview of the recent semi-supervised
learning approaches and related works. Despite the remarkable success of neural
networks in various applications, there exist few formidable constraints
including the need for a large amount of labeled data. Therefore,
semi-supervised learning, which is a learning scheme in which the scarce labels
and a larger amount of unlabeled data are utilized to train models (e.g., deep
neural networks) is getting more important. Based on the key assumptions of
semi-supervised learning, which are the manifold assumption, cluster
assumption, and continuity assumption, the work reviews the recent
semi-supervised learning approaches. In particular, the methods in regard to
using deep neural networks in a semi-supervised learning setting are primarily
discussed. In addition, the existing works are first classified based on the
underlying idea and explained, and then the holistic approaches that unify the
aforementioned ideas are detailed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1"&gt;Gyeongho Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Encoder-Decoder Architectures for Clinically Relevant Coronary Artery Segmentation. (arXiv:2106.11447v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.11447</id>
        <link href="http://arxiv.org/abs/2106.11447"/>
        <updated>2021-06-23T01:48:40.214Z</updated>
        <summary type="html"><![CDATA[Coronary X-ray angiography is a crucial clinical procedure for the diagnosis
and treatment of coronary artery disease, which accounts for roughly 16% of
global deaths every year. However, the images acquired in these procedures have
low resolution and poor contrast, making lesion detection and assessment
challenging. Accurate coronary artery segmentation not only helps mitigate
these problems, but also allows the extraction of relevant anatomical features
for further analysis by quantitative methods. Although automated segmentation
of coronary arteries has been proposed before, previous approaches have used
non-optimal segmentation criteria, leading to less useful results. Most methods
either segment only the major vessel, discarding important information from the
remaining ones, or segment the whole coronary tree based mostly on contrast
information, producing a noisy output that includes vessels that are not
relevant for diagnosis. We adopt a better-suited clinical criterion and segment
vessels according to their clinical relevance. Additionally, we simultaneously
perform catheter segmentation, which may be useful for diagnosis due to the
scale factor provided by the catheter's known diameter, and is a task that has
not yet been performed with good results. To derive the optimal approach, we
conducted an extensive comparative study of encoder-decoder architectures
trained on a combination of focal loss and a variant of generalized dice loss.
Based on the EfficientNet and the UNet++ architectures, we propose a line of
efficient and high-performance segmentation models using a new decoder
architecture, the EfficientUNet++, whose best-performing version achieved
average dice scores of 0.8904 and 0.7526 for the artery and catheter classes,
respectively, and an average generalized dice score of 0.9234.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Silva_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Louren&amp;#xe7;o Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Menezes_M/0/1/0/all/0/1"&gt;Miguel Nobre Menezes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rodrigues_T/0/1/0/all/0/1"&gt;Tiago Rodrigues&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Silva_B/0/1/0/all/0/1"&gt;Beatriz Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pinto_F/0/1/0/all/0/1"&gt;Fausto J. Pinto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oliveira_A/0/1/0/all/0/1"&gt;Arlindo L. Oliveira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Model-based Hierarchical Reinforcement Learning using Inductive Logic Programming. (arXiv:2106.11417v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11417</id>
        <link href="http://arxiv.org/abs/2106.11417"/>
        <updated>2021-06-23T01:48:40.204Z</updated>
        <summary type="html"><![CDATA[Recently deep reinforcement learning has achieved tremendous success in wide
ranges of applications. However, it notoriously lacks data-efficiency and
interpretability. Data-efficiency is important as interacting with the
environment is expensive. Further, interpretability can increase the
transparency of the black-box-style deep RL models and hence gain trust from
the users. In this work, we propose a new hierarchical framework via symbolic
RL, leveraging a symbolic transition model to improve the data-efficiency and
introduce the interpretability for learned policy. This framework consists of a
high-level agent, a subtask solver and a symbolic transition model. Without
assuming any prior knowledge on the state transition, we adopt inductive logic
programming (ILP) to learn the rules of symbolic state transitions, introducing
interpretability and making the learned behavior understandable to users. In
empirical experiments, we confirmed that the proposed framework offers
approximately between 30\% to 40\% more data efficiency over previous methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Duo Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fekri_F/0/1/0/all/0/1"&gt;Faramarz Fekri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding top-down attention using task-oriented ablation design. (arXiv:2106.11339v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11339</id>
        <link href="http://arxiv.org/abs/2106.11339"/>
        <updated>2021-06-23T01:48:40.114Z</updated>
        <summary type="html"><![CDATA[Top-down attention allows neural networks, both artificial and biological, to
focus on the information most relevant for a given task. This is known to
enhance performance in visual perception. But it remains unclear how attention
brings about its perceptual boost, especially when it comes to naturalistic
settings like recognising an object in an everyday scene. What aspects of a
visual task does attention help to deal with? We aim to answer this with a
computational experiment based on a general framework called task-oriented
ablation design. First we define a broad range of visual tasks and identify six
factors that underlie task variability. Then on each task we compare the
performance of two neural networks, one with top-down attention and one
without. These comparisons reveal the task-dependence of attention's perceptual
boost, giving a clearer idea of the role attention plays. Whereas many existing
cognitive accounts link attention to stimulus-level variables, such as visual
clutter and object scale, we find greater explanatory power in system-level
variables that capture the interaction between the model, the distribution of
training data and the task format. This finding suggests a shift in how
attention is studied could be fruitful. We make publicly available our code and
results, along with statistics relevant to ImageNet-based experiments beyond
this one. Our contribution serves to support the development of more human-like
vision models and the design of more informative machine-learning experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smith_F/0/1/0/all/0/1"&gt;Freddie Bickford Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roads_B/0/1/0/all/0/1"&gt;Brett D Roads&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiaoliang Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Love_B/0/1/0/all/0/1"&gt;Bradley C Love&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feedback Shaping: A Modeling Approach to Nurture Content Creation. (arXiv:2106.11312v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2106.11312</id>
        <link href="http://arxiv.org/abs/2106.11312"/>
        <updated>2021-06-23T01:48:40.105Z</updated>
        <summary type="html"><![CDATA[Social media platforms bring together content creators and content consumers
through recommender systems like newsfeed. The focus of such recommender
systems has thus far been primarily on modeling the content consumer
preferences and optimizing for their experience. However, it is equally
critical to nurture content creation by prioritizing the creators' interests,
as quality content forms the seed for sustainable engagement and conversations,
bringing in new consumers while retaining existing ones. In this work, we
propose a modeling approach to predict how feedback from content consumers
incentivizes creators. We then leverage this model to optimize the newsfeed
experience for content creators by reshaping the feedback distribution, leading
to a more active content ecosystem. Practically, we discuss how we balance the
user experience for both consumers and creators, and how we carry out online
A/B tests with strong network effects. We present a deployed use case on the
LinkedIn newsfeed, where we used this approach to improve content creation
significantly without compromising the consumers' experience.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Y/0/1/0/all/0/1"&gt;Ye Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lo_C/0/1/0/all/0/1"&gt;Chun Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1"&gt;Yiping Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1"&gt;Shaunak Chatterjee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatio-Temporal Multi-Task Learning Transformer for Joint Moving Object Detection and Segmentation. (arXiv:2106.11401v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11401</id>
        <link href="http://arxiv.org/abs/2106.11401"/>
        <updated>2021-06-23T01:48:40.099Z</updated>
        <summary type="html"><![CDATA[Moving objects have special importance for Autonomous Driving tasks.
Detecting moving objects can be posed as Moving Object Segmentation, by
segmenting the object pixels, or Moving Object Detection, by generating a
bounding box for the moving targets. In this paper, we present a Multi-Task
Learning architecture, based on Transformers, to jointly perform both tasks
through one network. Due to the importance of the motion features to the task,
the whole setup is based on a Spatio-Temporal aggregation. We evaluate the
performance of the individual tasks architecture versus the MTL setup, both
with early shared encoders, and late shared encoder-decoder transformers. For
the latter, we present a novel joint tasks query decoder transformer, that
enables us to have tasks dedicated heads out of the shared model. To evaluate
our approach, we use the KITTI MOD [29] data set. Results show1.5% mAP
improvement for Moving Object Detection, and 2%IoU improvement for Moving
Object Segmentation, over the individual tasks networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1"&gt;Eslam Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1"&gt;Ahmed El-Sallab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tensor Learning-based Precoder Codebooks for FD-MIMO Systems. (arXiv:2106.11374v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.11374</id>
        <link href="http://arxiv.org/abs/2106.11374"/>
        <updated>2021-06-23T01:48:40.082Z</updated>
        <summary type="html"><![CDATA[This paper develops an efficient procedure for designing low-complexity
codebooks for precoding in a full-dimension (FD) multiple-input multiple-output
(MIMO) system with a uniform planar array (UPA) antenna at the transmitter (Tx)
using tensor learning. In particular, instead of using statistical channel
models, we utilize a model-free data-driven approach with foundations in
machine learning to generate codebooks that adapt to the surrounding
propagation conditions. We use a tensor representation of the FD-MIMO channel
and exploit its properties to design quantized version of the channel
precoders. We find the best representation of the optimal precoder as a
function of Kronecker Product (KP) of two low-dimensional precoders,
respectively corresponding to the horizontal and vertical dimensions of the
UPA, obtained from the tensor decomposition of the channel. We then quantize
this precoder to design product codebooks such that an average loss in mutual
information due to quantization of channel state information (CSI) is
minimized. The key technical contribution lies in exploiting the constraints on
the precoders to reduce the product codebook design problem to an unsupervised
clustering problem on a Cartesian Product Grassmann manifold (CPM), where the
cluster centroids form a finite-sized precoder codebook. This codebook can be
found efficiently by running a $K$-means clustering on the CPM. With a suitable
induced distance metric on the CPM, we show that the construction of product
codebooks is equivalent to finding the optimal set of centroids on the factor
manifolds corresponding to the horizontal and vertical dimensions. Simulation
results are presented to demonstrate the capability of the proposed design
criterion in learning the codebooks and the attractive performance of the
designed codebooks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bhogi_K/0/1/0/all/0/1"&gt;Keerthana Bhogi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Saha_C/0/1/0/all/0/1"&gt;Chiranjib Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dhillon_H/0/1/0/all/0/1"&gt;Harpreet S. Dhillon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MODETR: Moving Object Detection with Transformers. (arXiv:2106.11422v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11422</id>
        <link href="http://arxiv.org/abs/2106.11422"/>
        <updated>2021-06-23T01:48:40.074Z</updated>
        <summary type="html"><![CDATA[Moving Object Detection (MOD) is a crucial task for the Autonomous Driving
pipeline. MOD is usually handled via 2-stream convolutional architectures that
incorporates both appearance and motion cues, without considering the
inter-relations between the spatial or motion features. In this paper, we
tackle this problem through multi-head attention mechanisms, both across the
spatial and motion streams. We propose MODETR; a Moving Object DEtection
TRansformer network, comprised of multi-stream transformer encoders for both
spatial and motion modalities, and an object transformer decoder that produces
the moving objects bounding boxes using set predictions. The whole architecture
is trained end-to-end using bi-partite loss. Several methods of incorporating
motion cues with the Transformer model are explored, including two-stream RGB
and Optical Flow (OF) methods, and multi-stream architectures that take
advantage of sequence information. To incorporate the temporal information, we
propose a new Temporal Positional Encoding (TPE) approach to extend the Spatial
Positional Encoding(SPE) in DETR. We explore two architectural choices for
that, balancing between speed and time. To evaluate the our network, we perform
the MOD task on the KITTI MOD [6] data set. Results show significant 5% mAP of
the Transformer network for MOD over the state-of-the art methods. Moreover,
the proposed TPE encoding provides 10% mAP improvement over the SPE baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1"&gt;Eslam Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1"&gt;Ahmad El-Sallab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Membership Inference on Word Embedding and Beyond. (arXiv:2106.11384v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11384</id>
        <link href="http://arxiv.org/abs/2106.11384"/>
        <updated>2021-06-23T01:48:40.067Z</updated>
        <summary type="html"><![CDATA[In the text processing context, most ML models are built on word embeddings.
These embeddings are themselves trained on some datasets, potentially
containing sensitive data. In some cases this training is done independently,
in other cases, it occurs as part of training a larger, task-specific model. In
either case, it is of interest to consider membership inference attacks based
on the embedding layer as a way of understanding sensitive information leakage.
But, somewhat surprisingly, membership inference attacks on word embeddings and
their effect in other natural language processing (NLP) tasks that use these
embeddings, have remained relatively unexplored.

In this work, we show that word embeddings are vulnerable to black-box
membership inference attacks under realistic assumptions. Furthermore, we show
that this leakage persists through two other major NLP applications:
classification and text-generation, even when the embedding layer is not
exposed to the attacker. We show that our MI attack achieves high attack
accuracy against a classifier model and an LSTM-based language model. Indeed,
our attack is a cheaper membership inference attack on text-generative models,
which does not require the knowledge of the target model or any expensive
training of text-generative models as shadow models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahloujifar_S/0/1/0/all/0/1"&gt;Saeed Mahloujifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Inan_H/0/1/0/all/0/1"&gt;Huseyin A. Inan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chase_M/0/1/0/all/0/1"&gt;Melissa Chase&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_E/0/1/0/all/0/1"&gt;Esha Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasegawa_M/0/1/0/all/0/1"&gt;Marcello Hasegawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Inference via Universal LSH Kernel. (arXiv:2106.11426v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11426</id>
        <link href="http://arxiv.org/abs/2106.11426"/>
        <updated>2021-06-23T01:48:40.061Z</updated>
        <summary type="html"><![CDATA[Large machine learning models achieve unprecedented performance on various
tasks and have evolved as the go-to technique. However, deploying these compute
and memory hungry models on resource constraint environments poses new
challenges. In this work, we propose mathematically provable Representer
Sketch, a concise set of count arrays that can approximate the inference
procedure with simple hashing computations and aggregations. Representer Sketch
builds upon the popular Representer Theorem from kernel literature, hence the
name, providing a generic fundamental alternative to the problem of efficient
inference that goes beyond the popular approach such as quantization, iterative
pruning and knowledge distillation. A neural network function is transformed to
its weighted kernel density representation, which can be very efficiently
estimated with our sketching algorithm. Empirically, we show that Representer
Sketch achieves up to 114x reduction in storage requirement and 59x reduction
in computation complexity without any drop in accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zichang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coleman_B/0/1/0/all/0/1"&gt;Benjamin Coleman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Anshumali Shrivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local convexity of the TAP free energy and AMP convergence for Z2-synchronization. (arXiv:2106.11428v1 [math.ST])]]></title>
        <id>http://arxiv.org/abs/2106.11428</id>
        <link href="http://arxiv.org/abs/2106.11428"/>
        <updated>2021-06-23T01:48:40.054Z</updated>
        <summary type="html"><![CDATA[We study mean-field variational Bayesian inference using the TAP approach,
for Z2-synchronization as a prototypical example of a high-dimensional Bayesian
model. We show that for any signal strength $\lambda > 1$ (the weak-recovery
threshold), there exists a unique local minimizer of the TAP free energy
functional near the mean of the Bayes posterior law. Furthermore, the TAP free
energy in a local neighborhood of this minimizer is strongly convex.
Consequently, a natural-gradient/mirror-descent algorithm achieves linear
convergence to this minimizer from a local initialization, which may be
obtained by a finite number of iterates of Approximate Message Passing (AMP).
This provides a rigorous foundation for variational inference in high
dimensions via minimization of the TAP free energy.

We also analyze the finite-sample convergence of AMP, showing that AMP is
asymptotically stable at the TAP minimizer for any $\lambda > 1$, and is
linearly convergent to this minimizer from a spectral initialization for
sufficiently large $\lambda$. Such a guarantee is stronger than results
obtainable by state evolution analyses, which only describe a fixed number of
AMP iterations in the infinite-sample limit.

Our proofs combine the Kac-Rice formula and Sudakov-Fernique Gaussian
comparison inequality to analyze the complexity of critical points that satisfy
strong convexity and stability conditions within their local neighborhoods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Celentano_M/0/1/0/all/0/1"&gt;Michael Celentano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Fan_Z/0/1/0/all/0/1"&gt;Zhou Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Mei_S/0/1/0/all/0/1"&gt;Song Mei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Quality as Predictor of Voice Anti-Spoofing Generalization. (arXiv:2103.14602v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14602</id>
        <link href="http://arxiv.org/abs/2103.14602"/>
        <updated>2021-06-23T01:48:40.033Z</updated>
        <summary type="html"><![CDATA[Voice anti-spoofing aims at classifying a given utterance either as a
bonafide human sample, or a spoofing attack (e.g. synthetic or replayed
sample). Many anti-spoofing methods have been proposed but most of them fail to
generalize across domains (corpora) -- and we do not know \emph{why}. We
outline a novel interpretative framework for gauging the impact of data quality
upon anti-spoofing performance. Our within- and between-domain experiments pool
data from seven public corpora and three anti-spoofing methods based on
Gaussian mixture and convolutive neural network models. We assess the impacts
of long-term spectral information, speaker population (through x-vector speaker
embeddings), signal-to-noise ratio, and selected voice quality features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chettri_B/0/1/0/all/0/1"&gt;Bhusan Chettri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hautamaki_R/0/1/0/all/0/1"&gt;Rosa Gonz&amp;#xe1;lez Hautam&amp;#xe4;ki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sahidullah_M/0/1/0/all/0/1"&gt;Md Sahidullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kinnunen_T/0/1/0/all/0/1"&gt;Tomi Kinnunen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dive into Deep Learning. (arXiv:2106.11342v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11342</id>
        <link href="http://arxiv.org/abs/2106.11342"/>
        <updated>2021-06-23T01:48:40.025Z</updated>
        <summary type="html"><![CDATA[This open-source book represents our attempt to make deep learning
approachable, teaching readers the concepts, the context, and the code. The
entire book is drafted in Jupyter notebooks, seamlessly integrating exposition
figures, math, and interactive examples with self-contained code. Our goal is
to offer a resource that could (i) be freely available for everyone; (ii) offer
sufficient technical depth to provide a starting point on the path to actually
becoming an applied machine learning scientist; (iii) include runnable code,
showing readers how to solve problems in practice; (iv) allow for rapid
updates, both by us and also by the community at large; (v) be complemented by
a forum for interactive discussion of technical details and to answer
questions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Aston Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1"&gt;Zachary C. Lipton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1"&gt;Alexander J. Smola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Consistent Video Depth Estimation. (arXiv:2012.05901v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05901</id>
        <link href="http://arxiv.org/abs/2012.05901"/>
        <updated>2021-06-23T01:48:40.017Z</updated>
        <summary type="html"><![CDATA[We present an algorithm for estimating consistent dense depth maps and camera
poses from a monocular video. We integrate a learning-based depth prior, in the
form of a convolutional neural network trained for single-image depth
estimation, with geometric optimization, to estimate a smooth camera trajectory
as well as detailed and stable depth reconstruction. Our algorithm combines two
complementary techniques: (1) flexible deformation-splines for low-frequency
large-scale alignment and (2) geometry-aware depth filtering for high-frequency
alignment of fine depth details. In contrast to prior approaches, our method
does not require camera poses as input and achieves robust reconstruction for
challenging hand-held cell phone captures containing a significant amount of
noise, shake, motion blur, and rolling shutter deformations. Our method
quantitatively outperforms state-of-the-arts on the Sintel benchmark for both
depth and pose estimations and attains favorable qualitative results across
diverse wild datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kopf_J/0/1/0/all/0/1"&gt;Johannes Kopf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rong_X/0/1/0/all/0/1"&gt;Xuejian Rong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jia-Bin Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Policy Smoothing for Provably Robust Reinforcement Learning. (arXiv:2106.11420v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11420</id>
        <link href="http://arxiv.org/abs/2106.11420"/>
        <updated>2021-06-23T01:48:40.010Z</updated>
        <summary type="html"><![CDATA[The study of provable adversarial robustness for deep neural network (DNN)
models has mainly focused on static supervised learning tasks such as image
classification. However, DNNs have been used extensively in real-world adaptive
tasks such as reinforcement learning (RL), making RL systems vulnerable to
adversarial attacks. The key challenge in adversarial RL is that the attacker
can adapt itself to the defense strategy used by the agent in previous
time-steps to strengthen its attack in future steps. In this work, we study the
provable robustness of RL against norm-bounded adversarial perturbations of the
inputs. We focus on smoothing-based provable defenses and propose policy
smoothing where the agent adds a Gaussian noise to its observation at each
time-step before applying the policy network to make itself less sensitive to
adversarial perturbations of its inputs. Our main theoretical contribution is
to prove an adaptive version of the Neyman-Pearson Lemma where the adversarial
perturbation at a particular time can be a stochastic function of current and
previous observations and states as well as previously observed actions. Using
this lemma, we adapt the robustness certificates produced by randomized
smoothing in the static setting of image classification to the dynamic setting
of RL. We generate certificates that guarantee that the total reward obtained
by the smoothed policy will not fall below a certain threshold under a
norm-bounded adversarial perturbation of the input. We show that our
certificates are tight by constructing a worst-case setting that achieves the
bounds derived in our analysis. In our experiments, we show that this method
can yield meaningful certificates in complex environments demonstrating its
effectiveness against adversarial attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Aounon Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_A/0/1/0/all/0/1"&gt;Alexander Levine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1"&gt;Soheil Feizi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How well do you know your summarization datasets?. (arXiv:2106.11388v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11388</id>
        <link href="http://arxiv.org/abs/2106.11388"/>
        <updated>2021-06-23T01:48:40.003Z</updated>
        <summary type="html"><![CDATA[State-of-the-art summarization systems are trained and evaluated on massive
datasets scraped from the web. Despite their prevalence, we know very little
about the underlying characteristics (data noise, summarization complexity,
etc.) of these datasets, and how these affect system performance and the
reliability of automatic metrics like ROUGE. In this study, we manually analyze
600 samples from three popular summarization datasets. Our study is driven by a
six-class typology which captures different noise types (missing facts,
entities) and degrees of summarization difficulty (extractive, abstractive). We
follow with a thorough analysis of 27 state-of-the-art summarization models and
5 popular metrics, and report our key insights: (1) Datasets have distinct data
quality and complexity distributions, which can be traced back to their
collection process. (2) The performance of models and reliability of metrics is
dependent on sample complexity. (3) Faithful summaries often receive low scores
because of the poor diversity of references. We release the code, annotated
data and model outputs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tejaswin_P/0/1/0/all/0/1"&gt;Priyam Tejaswin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naik_D/0/1/0/all/0/1"&gt;Dhruv Naik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Pengfei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wallpaper Texture Generation and Style Transfer Based on Multi-label Semantics. (arXiv:2106.11482v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11482</id>
        <link href="http://arxiv.org/abs/2106.11482"/>
        <updated>2021-06-23T01:48:39.981Z</updated>
        <summary type="html"><![CDATA[Textures contain a wealth of image information and are widely used in various
fields such as computer graphics and computer vision. With the development of
machine learning, the texture synthesis and generation have been greatly
improved. As a very common element in everyday life, wallpapers contain a
wealth of texture information, making it difficult to annotate with a simple
single label. Moreover, wallpaper designers spend significant time to create
different styles of wallpaper. For this purpose, this paper proposes to
describe wallpaper texture images by using multi-label semantics. Based on
these labels and generative adversarial networks, we present a framework for
perception driven wallpaper texture generation and style transfer. In this
framework, a perceptual model is trained to recognize whether the wallpapers
produced by the generator network are sufficiently realistic and have the
attribute designated by given perceptual description; these multi-label
semantic attributes are treated as condition variables to generate wallpaper
images. The generated wallpaper images can be converted to those with
well-known artist styles using CycleGAN. Finally, using the aesthetic
evaluation method, the generated wallpaper images are quantitatively measured.
The experimental results demonstrate that the proposed method can generate
wallpaper textures conforming to human aesthetics and have artistic
characteristics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Ying Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1"&gt;Xiaohan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tiange Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rigall_E/0/1/0/all/0/1"&gt;Eric Rigall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Huiyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1"&gt;Lin Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Junyu Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-Smoothing Network for Zero-Shot Sketch-Based Image Retrieval. (arXiv:2106.11841v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11841</id>
        <link href="http://arxiv.org/abs/2106.11841"/>
        <updated>2021-06-23T01:48:39.974Z</updated>
        <summary type="html"><![CDATA[Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR) is a novel cross-modal
retrieval task, where abstract sketches are used as queries to retrieve natural
images under zero-shot scenario. Most existing methods regard ZS-SBIR as a
traditional classification problem and employ a cross-entropy or triplet-based
loss to achieve retrieval, which neglect the problems of the domain gap between
sketches and natural images and the large intra-class diversity in sketches.
Toward this end, we propose a novel Domain-Smoothing Network (DSN) for ZS-SBIR.
Specifically, a cross-modal contrastive method is proposed to learn generalized
representations to smooth the domain gap by mining relations with additional
augmented samples. Furthermore, a category-specific memory bank with sketch
features is explored to reduce intra-class diversity in the sketch domain.
Extensive experiments demonstrate that our approach notably outperforms the
state-of-the-art methods in both Sketchy and TU-Berlin datasets. Our source
code is publicly available at https://github.com/haowang1992/DSN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhipeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Jiexi Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1"&gt;Aming Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1"&gt;Cheng Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[f-Domain-Adversarial Learning: Theory and Algorithms. (arXiv:2106.11344v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11344</id>
        <link href="http://arxiv.org/abs/2106.11344"/>
        <updated>2021-06-23T01:48:39.968Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation is used in many machine learning applications
where, during training, a model has access to unlabeled data in the target
domain, and a related labeled dataset. In this paper, we introduce a novel and
general domain-adversarial framework. Specifically, we derive a novel
generalization bound for domain adaptation that exploits a new measure of
discrepancy between distributions based on a variational characterization of
f-divergences. It recovers the theoretical results from Ben-David et al.
(2010a) as a special case and supports divergences used in practice. Based on
this bound, we derive a new algorithmic framework that introduces a key
correction in the original adversarial training method of Ganin et al. (2016).
We show that many regularizers and ad-hoc objectives introduced over the last
years in this framework are then not required to achieve performance comparable
to (if not better than) state-of-the-art domain-adversarial methods.
Experimental analysis conducted on real-world natural language and computer
vision datasets show that our framework outperforms existing baselines, and
obtains the best results for f-divergences that were not considered previously
in domain-adversarial learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Acuna_D/0/1/0/all/0/1"&gt;David Acuna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guojun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1"&gt;Marc T. Law&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1"&gt;Sanja Fidler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Augmentation for Opcode Sequence Based Malware Detection. (arXiv:2106.11821v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.11821</id>
        <link href="http://arxiv.org/abs/2106.11821"/>
        <updated>2021-06-23T01:48:39.962Z</updated>
        <summary type="html"><![CDATA[Data augmentation has been successfully used in many areas of deep-learning
to significantly improve model performance. Typically data augmentation
simulates realistic variations in data in order to increase the apparent
diversity of the training-set. However, for opcode-based malware analysis,
where deep learning methods are already achieving state of the art performance,
it is not immediately clear how to apply data augmentation. In this paper we
study different methods of data augmentation starting with basic methods using
fixed transformations and moving to methods that adapt to the data. We propose
a novel data augmentation method based on using an opcode embedding layer
within the network and its corresponding opcode embedding matrix to perform
adaptive data augmentation during training. To the best of our knowledge this
is the first paper to carry out a systematic study of different augmentation
methods applied to opcode sequence based malware classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McLaughlin_N/0/1/0/all/0/1"&gt;Niall McLaughlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rincon_J/0/1/0/all/0/1"&gt;Jesus Martinez del Rincon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SA-LOAM: Semantic-aided LiDAR SLAM with Loop Closure. (arXiv:2106.11516v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.11516</id>
        <link href="http://arxiv.org/abs/2106.11516"/>
        <updated>2021-06-23T01:48:39.955Z</updated>
        <summary type="html"><![CDATA[LiDAR-based SLAM system is admittedly more accurate and stable than others,
while its loop closure detection is still an open issue. With the development
of 3D semantic segmentation for point cloud, semantic information can be
obtained conveniently and steadily, essential for high-level intelligence and
conductive to SLAM. In this paper, we present a novel semantic-aided LiDAR SLAM
with loop closure based on LOAM, named SA-LOAM, which leverages semantics in
odometry as well as loop closure detection. Specifically, we propose a
semantic-assisted ICP, including semantically matching, downsampling and plane
constraint, and integrates a semantic graph-based place recognition method in
our loop closure detection module. Benefitting from semantics, we can improve
the localization accuracy, detect loop closures effectively, and construct a
global consistent semantic map even in large-scale scenes. Extensive
experiments on KITTI and Ford Campus dataset show that our system significantly
improves baseline performance, has generalization ability to unseen data and
achieves competitive results compared with state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1"&gt;Xin Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xiangrui Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wanlong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1"&gt;Feng Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongbo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proposal Relation Network for Temporal Action Detection. (arXiv:2106.11812v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11812</id>
        <link href="http://arxiv.org/abs/2106.11812"/>
        <updated>2021-06-23T01:48:39.948Z</updated>
        <summary type="html"><![CDATA[This technical report presents our solution for temporal action detection
task in AcitivityNet Challenge 2021. The purpose of this task is to locate and
identify actions of interest in long untrimmed videos. The crucial challenge of
the task comes from that the temporal duration of action varies dramatically,
and the target actions are typically embedded in a background of irrelevant
activities. Our solution builds on BMN, and mainly contains three steps: 1)
action classification and feature encoding by Slowfast, CSN and ViViT; 2)
proposal generation. We improve BMN by embedding the proposed Proposal Relation
Network (PRN), by which we can generate proposals of high quality; 3) action
detection. We calculate the detection results by assigning the proposals with
corresponding classification results. Finally, we ensemble the results under
different settings and achieve 44.7% on the test set, which improves the
champion result in ActivityNet 2020 by 1.9% in terms of average mAP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1"&gt;Zhiwu Qing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Ziyuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yutong Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shiwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jianwen Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1"&gt;Mingqian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1"&gt;Changxin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1"&gt;Nong Sang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HybVIO: Pushing the Limits of Real-time Visual-inertial Odometry. (arXiv:2106.11857v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11857</id>
        <link href="http://arxiv.org/abs/2106.11857"/>
        <updated>2021-06-23T01:48:39.928Z</updated>
        <summary type="html"><![CDATA[We present HybVIO, a novel hybrid approach for combining filtering-based
visual-inertial odometry (VIO) with optimization-based SLAM. The core of our
method is highly robust, independent VIO with improved IMU bias modeling,
outlier rejection, stationarity detection, and feature track selection, which
is adjustable to run on embedded hardware. Long-term consistency is achieved
with a loosely-coupled SLAM module. In academic benchmarks, our solution yields
excellent performance in all categories, especially in the real-time use case,
where we outperform the current state-of-the-art. We also demonstrate the
feasibility of VIO for vehicular tracking on consumer-grade hardware using a
custom dataset, and show good performance in comparison to current commercial
VISLAM alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seiskari_O/0/1/0/all/0/1"&gt;Otto Seiskari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rantalankila_P/0/1/0/all/0/1"&gt;Pekka Rantalankila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kannala_J/0/1/0/all/0/1"&gt;Juho Kannala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ylilammi_J/0/1/0/all/0/1"&gt;Jerry Ylilammi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1"&gt;Esa Rahtu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Solin_A/0/1/0/all/0/1"&gt;Arno Solin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Skeleton-based Action Recognition via Spatial and Temporal Transformer Networks. (arXiv:2008.07404v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.07404</id>
        <link href="http://arxiv.org/abs/2008.07404"/>
        <updated>2021-06-23T01:48:39.922Z</updated>
        <summary type="html"><![CDATA[Skeleton-based Human Activity Recognition has achieved great interest in
recent years as skeleton data has demonstrated being robust to illumination
changes, body scales, dynamic camera views, and complex background. In
particular, Spatial-Temporal Graph Convolutional Networks (ST-GCN) demonstrated
to be effective in learning both spatial and temporal dependencies on
non-Euclidean data such as skeleton graphs. Nevertheless, an effective encoding
of the latent information underlying the 3D skeleton is still an open problem,
especially when it comes to extracting effective information from joint motion
patterns and their correlations. In this work, we propose a novel
Spatial-Temporal Transformer network (ST-TR) which models dependencies between
joints using the Transformer self-attention operator. In our ST-TR model, a
Spatial Self-Attention module (SSA) is used to understand intra-frame
interactions between different body parts, and a Temporal Self-Attention module
(TSA) to model inter-frame correlations. The two are combined in a two-stream
network, whose performance is evaluated on three large-scale datasets,
NTU-RGB+D 60, NTU-RGB+D 120, and Kinetics Skeleton 400, consistently improving
backbone results. Compared with methods that use the same input data, the
proposed ST-TR achieves state-of-the-art performance on all datasets when using
joints' coordinates as input, and results on-par with state-of-the-art when
adding bones information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Plizzari_C/0/1/0/all/0/1"&gt;Chiara Plizzari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cannici_M/0/1/0/all/0/1"&gt;Marco Cannici&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matteucci_M/0/1/0/all/0/1"&gt;Matteo Matteucci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RootPainter3D: Interactive-machine-learning enables rapid and accurate contouring for radiotherapy. (arXiv:2106.11942v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11942</id>
        <link href="http://arxiv.org/abs/2106.11942"/>
        <updated>2021-06-23T01:48:39.914Z</updated>
        <summary type="html"><![CDATA[Organ-at-risk contouring is still a bottleneck in radiotherapy, with many
deep learning methods falling short of promised results when evaluated on
clinical data. We investigate the accuracy and time-savings resulting from the
use of an interactive-machine-learning method for an organ-at-risk contouring
task. We compare the method to the Eclipse contouring software and find strong
agreement with manual delineations, with a dice score of 0.95. The annotations
created using corrective-annotation also take less time to create as more
images are annotated, resulting in substantial time savings compared to manual
methods, with hearts that take 2 minutes and 2 seconds to delineate on average,
after 923 images have been delineated, compared to 7 minutes and 1 seconds when
delineating manually. Our experiment demonstrates that
interactive-machine-learning with corrective-annotation provides a fast and
accessible way for non computer-scientists to train deep-learning models to
segment their own structures of interest as part of routine clinical workflows.

Source code is available at
\href{https://github.com/Abe404/RootPainter3D}{this HTTPS URL}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smith_A/0/1/0/all/0/1"&gt;Abraham George Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petersen_J/0/1/0/all/0/1"&gt;Jens Petersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Terrones_Campos_C/0/1/0/all/0/1"&gt;Cynthia Terrones-Campos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berthelsen_A/0/1/0/all/0/1"&gt;Anne Kiil Berthelsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forbes_N/0/1/0/all/0/1"&gt;Nora Jarrett Forbes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darkner_S/0/1/0/all/0/1"&gt;Sune Darkner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Specht_L/0/1/0/all/0/1"&gt;Lena Specht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogelius_I/0/1/0/all/0/1"&gt;Ivan Richter Vogelius&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nutrition5k: Towards Automatic Nutritional Understanding of Generic Food. (arXiv:2103.03375v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03375</id>
        <link href="http://arxiv.org/abs/2103.03375"/>
        <updated>2021-06-23T01:48:39.888Z</updated>
        <summary type="html"><![CDATA[Understanding the nutritional content of food from visual data is a
challenging computer vision problem, with the potential to have a positive and
widespread impact on public health. Studies in this area are limited to
existing datasets in the field that lack sufficient diversity or labels
required for training models with nutritional understanding capability. We
introduce Nutrition5k, a novel dataset of 5k diverse, real world food dishes
with corresponding video streams, depth images, component weights, and high
accuracy nutritional content annotation. We demonstrate the potential of this
dataset by training a computer vision algorithm capable of predicting the
caloric and macronutrient values of a complex, real world dish at an accuracy
that outperforms professional nutritionists. Further we present a baseline for
incorporating depth sensor data to improve nutrition predictions. We will
publicly release Nutrition5k in the hope that it will accelerate innovation in
the space of nutritional understanding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thames_Q/0/1/0/all/0/1"&gt;Quin Thames&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karpur_A/0/1/0/all/0/1"&gt;Arjun Karpur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Norris_W/0/1/0/all/0/1"&gt;Wade Norris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1"&gt;Fangting Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panait_L/0/1/0/all/0/1"&gt;Liviu Panait&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weyand_T/0/1/0/all/0/1"&gt;Tobias Weyand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sim_J/0/1/0/all/0/1"&gt;Jack Sim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Give Me Your Trained Model: Domain Adaptive Semantic Segmentation without Source Data. (arXiv:2106.11653v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11653</id>
        <link href="http://arxiv.org/abs/2106.11653"/>
        <updated>2021-06-23T01:48:39.881Z</updated>
        <summary type="html"><![CDATA[Benefited from considerable pixel-level annotations collected from a specific
situation (source), the trained semantic segmentation model performs quite
well, but fails in a new situation (target) due to the large domain shift. To
mitigate the domain gap, previous cross-domain semantic segmentation methods
always assume the co-existence of source data and target data during
distribution alignment. However, the access to source data in the real scenario
may raise privacy concerns and violate intellectual property. To tackle this
problem, we focus on an interesting and challenging cross-domain semantic
segmentation task where only the trained source model is provided to the target
domain, and further propose a unified framework called Domain Adaptive Semantic
Segmentation without Source data (DAS$^3$ for short). Specifically, DAS$^3$
consists of three schemes, i.e., feature alignment, self-training, and
information propagation. First, we mainly develop a focal entropic loss on the
network outputs to implicitly align the target features with unseen source
features via the provided source model. Second, besides positive pseudo labels
in vanilla self-training, we first introduce negative pseudo labels to the
field and develop a bi-directional self-training strategy to enhance the
representation learning in the target domain. Finally, the information
propagation scheme further reduces the intra-domain discrepancy within the
target domain via pseudo semi-supervised learning. Extensive results on
synthesis-to-real and cross-city driving datasets validate DAS$^3$ yields
state-of-the-art performance, even on par with methods that need access to
source data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuxi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jian Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhaoxiang Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Iterative Contextual Smoothing for Efficient Adversarial Defense against Gray- and Black-Box Attack. (arXiv:2106.11644v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11644</id>
        <link href="http://arxiv.org/abs/2106.11644"/>
        <updated>2021-06-23T01:48:39.873Z</updated>
        <summary type="html"><![CDATA[We propose a novel and effective input transformation based adversarial
defense method against gray- and black-box attack, which is computationally
efficient and does not require any adversarial training or retraining of a
classification model. We first show that a very simple iterative Gaussian
smoothing can effectively wash out adversarial noise and achieve substantially
high robust accuracy. Based on the observation, we propose Self-Supervised
Iterative Contextual Smoothing (SSICS), which aims to reconstruct the original
discriminative features from the Gaussian-smoothed image in context-adaptive
manner, while still smoothing out the adversarial noise. From the experiments
on ImageNet, we show that our SSICS achieves both high standard accuracy and
very competitive robust accuracy for the gray- and black-box attacks; e.g.,
transfer-based PGD-attack and score-based attack. A note-worthy point to stress
is that our defense is free of computationally expensive adversarial training,
yet, can approach its robust accuracy via input transformation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1"&gt;Sungmin Cha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_N/0/1/0/all/0/1"&gt;Naeun Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1"&gt;Youngjoon Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1"&gt;Taesup Moon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VoxelEmbed: 3D Instance Segmentation and Tracking with Voxel Embedding based Deep Learning. (arXiv:2106.11480v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11480</id>
        <link href="http://arxiv.org/abs/2106.11480"/>
        <updated>2021-06-23T01:48:39.865Z</updated>
        <summary type="html"><![CDATA[Recent advances in bioimaging have provided scientists a superior high
spatial-temporal resolution to observe dynamics of living cells as 3D
volumetric videos. Unfortunately, the 3D biomedical video analysis is lagging,
impeded by resource insensitive human curation using off-the-shelf 3D analytic
tools. Herein, biologists often need to discard a considerable amount of rich
3D spatial information by compromising on 2D analysis via maximum intensity
projection. Recently, pixel embedding-based cell instance segmentation and
tracking provided a neat and generalizable computing paradigm for understanding
cellular dynamics. In this work, we propose a novel spatial-temporal
voxel-embedding (VoxelEmbed) based learning method to perform simultaneous cell
instance segmenting and tracking on 3D volumetric video sequences. Our
contribution is in four-fold: (1) The proposed voxel embedding generalizes the
pixel embedding with 3D context information; (2) Present a simple multi-stream
learning approach that allows effective spatial-temporal embedding; (3)
Accomplished an end-to-end framework for one-stage 3D cell instance
segmentation and tracking without heavy parameter tuning; (4) The proposed 3D
quantification is memory efficient via a single GPU with 12 GB memory. We
evaluate our VoxelEmbed method on four 3D datasets (with different cell types)
from the ISBI Cell Tracking Challenge. The proposed VoxelEmbed method achieved
consistent superior overall performance (OP) on two densely annotated datasets.
The performance is also competitive on two sparsely annotated cohorts with
20.6% and 2% of data-set having segmentation annotations. The results
demonstrate that the VoxelEmbed method is a generalizable and memory-efficient
solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1"&gt;Mengyang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Quan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1"&gt;Aadarsh Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1"&gt;Ruining Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1"&gt;Tianyuan Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahadevan_Jansen_A/0/1/0/all/0/1"&gt;Anita Mahadevan-Jansen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tyska_M/0/1/0/all/0/1"&gt;Matthew J.Tyska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Millis_B/0/1/0/all/0/1"&gt;Bryan A. Millis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1"&gt;Yuankai Huo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Robustness vs Model Compression, or Both?. (arXiv:1903.12561v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1903.12561</id>
        <link href="http://arxiv.org/abs/1903.12561"/>
        <updated>2021-06-23T01:48:39.858Z</updated>
        <summary type="html"><![CDATA[It is well known that deep neural networks (DNNs) are vulnerable to
adversarial attacks, which are implemented by adding crafted perturbations onto
benign examples. Min-max robust optimization based adversarial training can
provide a notion of security against adversarial attacks. However, adversarial
robustness requires a significantly larger capacity of the network than that
for the natural training with only benign examples. This paper proposes a
framework of concurrent adversarial training and weight pruning that enables
model compression while still preserving the adversarial robustness and
essentially tackles the dilemma of adversarial training. Furthermore, this work
studies two hypotheses about weight pruning in the conventional setting and
finds that weight pruning is essential for reducing the network model size in
the adversarial setting, training a small model from scratch even with
inherited initialization from the large model cannot achieve both adversarial
robustness and high standard accuracy. Code is available at
https://github.com/yeshaokai/Robustness-Aware-Pruning-ADMM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1"&gt;Shaokai Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kaidi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sijia Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lambrechts_J/0/1/0/all/0/1"&gt;Jan-Henrik Lambrechts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1"&gt;Aojun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1"&gt;Kaisheng Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xue Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Human-aware Robot Navigation. (arXiv:2106.11650v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.11650</id>
        <link href="http://arxiv.org/abs/2106.11650"/>
        <updated>2021-06-23T01:48:39.850Z</updated>
        <summary type="html"><![CDATA[Intelligent systems are increasingly part of our everyday lives and have been
integrated seamlessly to the point where it is difficult to imagine a world
without them. Physical manifestations of those systems on the other hand, in
the form of embodied agents or robots, have so far been used only for specific
applications and are often limited to functional roles (e.g. in the industry,
entertainment and military fields). Given the current growth and innovation in
the research communities concerned with the topics of robot navigation,
human-robot-interaction and human activity recognition, it seems like this
might soon change. Robots are increasingly easy to obtain and use and the
acceptance of them in general is growing. However, the design of a socially
compliant robot that can function as a companion needs to take various areas of
research into account. This paper is concerned with the navigation aspect of a
socially-compliant robot and provides a survey of existing solutions for the
relevant areas of research as well as an outlook on possible future directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moller_R/0/1/0/all/0/1"&gt;Ronja M&amp;#xf6;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Furnari_A/0/1/0/all/0/1"&gt;Antonino Furnari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Battiato_S/0/1/0/all/0/1"&gt;Sebastiano Battiato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harma_A/0/1/0/all/0/1"&gt;Aki H&amp;#xe4;rm&amp;#xe4;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farinella_G/0/1/0/all/0/1"&gt;Giovanni Maria Farinella&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple Object Tracking with Mixture Density Networks for Trajectory Estimation. (arXiv:2106.10950v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10950</id>
        <link href="http://arxiv.org/abs/2106.10950"/>
        <updated>2021-06-23T01:48:39.843Z</updated>
        <summary type="html"><![CDATA[Multiple object tracking faces several challenges that may be alleviated with
trajectory information. Knowing the posterior locations of an object helps
disambiguating and solving situations such as occlusions, re-identification,
and identity switching. In this work, we show that trajectory estimation can
become a key factor for tracking, and present TrajE, a trajectory estimator
based on recurrent mixture density networks, as a generic module that can be
added to existing object trackers. To provide several trajectory hypotheses,
our method uses beam search. Also, relying on the same estimated trajectory, we
propose to reconstruct a track after an occlusion occurs. We integrate TrajE
into two state of the art tracking algorithms, CenterTrack [63] and Tracktor
[3]. Their respective performances in the MOTChallenge 2017 test set are
boosted 6.3 and 0.3 points in MOTA score, and 1.8 and 3.1 in IDF1, setting a
new state of the art for the CenterTrack+TrajE configuration]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Girbau_A/0/1/0/all/0/1"&gt;Andreu Girbau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giro_i_Nieto_X/0/1/0/all/0/1"&gt;Xavier Gir&amp;#xf3;-i-Nieto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rius_I/0/1/0/all/0/1"&gt;Ignasi Rius&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marques_F/0/1/0/all/0/1"&gt;Ferran Marqu&amp;#xe9;s&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Alternative Auxiliary Task for Enhancing Image Classification. (arXiv:2106.11478v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11478</id>
        <link href="http://arxiv.org/abs/2106.11478"/>
        <updated>2021-06-23T01:48:39.835Z</updated>
        <summary type="html"><![CDATA[Image reconstruction is likely the most predominant auxiliary task for image
classification. In this paper, we investigate ``estimating the Fourier
Transform of the input image" as a potential alternative auxiliary task, in the
hope that it may further boost the performances on the primary task or
introduce novel constraints not well covered by image reconstruction. We
experimented with five popular classification architectures on the CIFAR-10
dataset, and the empirical results indicated that our proposed auxiliary task
generally improves the classification accuracy. More notably, the results
showed that in certain cases our proposed auxiliary task may enhance the
classifiers' resistance to adversarial attacks generated using the fast
gradient sign method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chen Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Entropy-based Logic Explanations of Neural Networks. (arXiv:2106.06804v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06804</id>
        <link href="http://arxiv.org/abs/2106.06804"/>
        <updated>2021-06-23T01:48:39.808Z</updated>
        <summary type="html"><![CDATA[Explainable artificial intelligence has rapidly emerged since lawmakers have
started requiring interpretable models for safety-critical domains.
Concept-based neural networks have arisen as explainable-by-design methods as
they leverage human-understandable symbols (i.e. concepts) to predict class
memberships. However, most of these approaches focus on the identification of
the most relevant concepts but do not provide concise, formal explanations of
how such concepts are leveraged by the classifier to make predictions. In this
paper, we propose a novel end-to-end differentiable approach enabling the
extraction of logic explanations from neural networks using the formalism of
First-Order Logic. The method relies on an entropy-based criterion which
automatically identifies the most relevant concepts. We consider four different
case studies to demonstrate that: (i) this entropy-based criterion enables the
distillation of concise logic explanations in safety-critical domains from
clinical data to computer vision; (ii) the proposed approach outperforms
state-of-the-art white-box models in terms of classification accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1"&gt;Pietro Barbiero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciravegna_G/0/1/0/all/0/1"&gt;Gabriele Ciravegna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giannini_F/0/1/0/all/0/1"&gt;Francesco Giannini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1"&gt;Pietro Li&amp;#xf3;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gori_M/0/1/0/all/0/1"&gt;Marco Gori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melacci_S/0/1/0/all/0/1"&gt;Stefano Melacci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Dataset Collaborative Learning for Semantic Segmentation. (arXiv:2103.11351v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11351</id>
        <link href="http://arxiv.org/abs/2103.11351"/>
        <updated>2021-06-23T01:48:39.799Z</updated>
        <summary type="html"><![CDATA[Recent work attempts to improve semantic segmentation performance by
exploring well-designed architectures on a target dataset. However, it remains
challenging to build a unified system that simultaneously learns from various
datasets due to the inherent distribution shift across different datasets. In
this paper, we present a simple, flexible, and general method for semantic
segmentation, termed Cross-Dataset Collaborative Learning (CDCL). Given
multiple labeled datasets, we aim to improve the generalization and
discrimination of feature representations on each dataset. Specifically, we
first introduce a family of Dataset-Aware Blocks (DAB) as the fundamental
computing units of the network, which help capture homogeneous representations
and heterogeneous statistics across different datasets. Second, we propose a
Dataset Alternation Training (DAT) mechanism to efficiently facilitate the
optimization procedure. We conduct extensive evaluations on four diverse
datasets, i.e., Cityscapes, BDD100K, CamVid, and COCO Stuff, with
single-dataset and cross-dataset settings. Experimental results demonstrate our
method consistently achieves notable improvements over prior single-dataset and
cross-dataset training methods without introducing extra FLOPs. Particularly,
with the same architecture of PSPNet (ResNet-18), our method outperforms the
single-dataset baseline by 5.65\%, 6.57\%, and 5.79\% of mIoU on the validation
sets of Cityscapes, BDD100K, CamVid, respectively. Code and models will be
released.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Li Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yousong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1"&gt;Lu Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1"&gt;Yi Shan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images. (arXiv:2106.11944v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11944</id>
        <link href="http://arxiv.org/abs/2106.11944"/>
        <updated>2021-06-23T01:48:39.769Z</updated>
        <summary type="html"><![CDATA[In this paper, we aim to create generalizable and controllable neural signed
distance fields (SDFs) that represent clothed humans from monocular depth
observations. Recent advances in deep learning, especially neural implicit
representations, have enabled human shape reconstruction and controllable
avatar generation from different sensor inputs. However, to generate realistic
cloth deformations from novel input poses, watertight meshes or dense full-body
scans are usually needed as inputs. Furthermore, due to the difficulty of
effectively modeling pose-dependent cloth deformations for diverse body shapes
and cloth types, existing approaches resort to per-subject/cloth-type
optimization from scratch, which is computationally expensive. In contrast, we
propose an approach that can quickly generate realistic clothed human avatars,
represented as controllable neural SDFs, given only monocular depth images. We
achieve this by using meta-learning to learn an initialization of a
hypernetwork that predicts the parameters of neural SDFs. The hypernetwork is
conditioned on human poses and represents a clothed neural avatar that deforms
non-rigidly according to the input poses. Meanwhile, it is meta-learned to
effectively incorporate priors of diverse body shapes and cloth types and thus
can be much faster to fine-tune, compared to models trained from scratch. We
qualitatively and quantitatively show that our approach outperforms
state-of-the-art approaches that require complete meshes as inputs while our
approach requires only depth frames as inputs and runs orders of magnitudes
faster. Furthermore, we demonstrate that our meta-learned hypernetwork is very
robust, being the first to generate avatars with realistic dynamic cloth
deformations given as few as 8 monocular depth frames.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shaofei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mihajlovic_M/0/1/0/all/0/1"&gt;Marko Mihajlovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1"&gt;Qianli Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1"&gt;Andreas Geiger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1"&gt;Siyu Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Obstacle Detection for BVLOS Drones. (arXiv:2106.11098v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11098</id>
        <link href="http://arxiv.org/abs/2106.11098"/>
        <updated>2021-06-23T01:48:39.756Z</updated>
        <summary type="html"><![CDATA[With the introduction of new regulations in the European Union, the future of
Beyond Visual Line Of Sight (BVLOS) drones is set to bloom. This led to the
creation of the theBEAST project, which aims to create an autonomous security
drone, with focus on those regulations and on safety. This technical paper
describes the first steps of a module within this project, which revolves
around detecting obstacles so they can be avoided in a fail-safe landing. A
deep learning powered object detection method is the subject of our research,
and various experiments are held to maximize its performance, such as comparing
various data augmentation techniques or YOLOv3 and YOLOv5. According to the
results of the experiments, we conclude that although object detection is a
promising approach to resolve this problem, more volume of data is required for
potential usage in a real-life application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Esteban_J/0/1/0/all/0/1"&gt;Jan Moros Esteban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loosdrecht_J/0/1/0/all/0/1"&gt;Jaap van de Loosdrecht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aghaei_M/0/1/0/all/0/1"&gt;Maya Aghaei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Object-Level Representation Learning from Scene Images. (arXiv:2106.11952v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11952</id>
        <link href="http://arxiv.org/abs/2106.11952"/>
        <updated>2021-06-23T01:48:39.734Z</updated>
        <summary type="html"><![CDATA[Contrastive self-supervised learning has largely narrowed the gap to
supervised pre-training on ImageNet. However, its success highly relies on the
object-centric priors of ImageNet, i.e., different augmented views of the same
image correspond to the same object. Such a heavily curated constraint becomes
immediately infeasible when pre-trained on more complex scene images with many
objects. To overcome this limitation, we introduce Object-level Representation
Learning (ORL), a new self-supervised learning framework towards scene images.
Our key insight is to leverage image-level self-supervised pre-training as the
prior to discover object-level semantic correspondence, thus realizing
object-level representation learning from scene images. Extensive experiments
on COCO show that ORL significantly improves the performance of self-supervised
learning on scene images, even surpassing supervised ImageNet pre-training on
several downstream tasks. Furthermore, ORL improves the downstream performance
when more unlabeled scene images are available, demonstrating its great
potential of harnessing unlabeled data in the wild. We hope our approach can
motivate future research on more general-purpose unsupervised representation
learning from scene data. Project page: https://www.mmlab-ntu.com/project/orl/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiahao Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1"&gt;Xiaohang Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ong_Y/0/1/0/all/0/1"&gt;Yew Soon Ong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1"&gt;Chen Change Loy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey of Quantization Methods for Efficient Neural Network Inference. (arXiv:2103.13630v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13630</id>
        <link href="http://arxiv.org/abs/2103.13630"/>
        <updated>2021-06-23T01:48:39.712Z</updated>
        <summary type="html"><![CDATA[As soon as abstract mathematical computations were adapted to computation on
digital computers, the problem of efficient representation, manipulation, and
communication of the numerical values in those computations arose. Strongly
related to the problem of numerical representation is the problem of
quantization: in what manner should a set of continuous real-valued numbers be
distributed over a fixed discrete set of numbers to minimize the number of bits
required and also to maximize the accuracy of the attendant computations? This
perennial problem of quantization is particularly relevant whenever memory
and/or computational resources are severely restricted, and it has come to the
forefront in recent years due to the remarkable performance of Neural Network
models in computer vision, natural language processing, and related areas.
Moving from floating-point representations to low-precision fixed integer
values represented in four bits or less holds the potential to reduce the
memory footprint and latency by a factor of 16x; and, in fact, reductions of 4x
to 8x are often realized in practice in these applications. Thus, it is not
surprising that quantization has emerged recently as an important and very
active sub-area of research in the efficient implementation of computations
associated with Neural Networks. In this article, we survey approaches to the
problem of quantizing the numerical values in deep Neural Network computations,
covering the advantages/disadvantages of current methods. With this survey and
its organization, we hope to have presented a useful snapshot of the current
research in quantization for Neural Networks and to have given an intelligent
organization to ease the evaluation of future research in this area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1"&gt;Amir Gholami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sehoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1"&gt;Zhen Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1"&gt;Zhewei Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1"&gt;Michael W. Mahoney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1"&gt;Kurt Keutzer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generation and frame characteristics of predefined evenly-distributed class centroids for pattern classification. (arXiv:2105.00401v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00401</id>
        <link href="http://arxiv.org/abs/2105.00401"/>
        <updated>2021-06-23T01:48:39.704Z</updated>
        <summary type="html"><![CDATA[Predefined evenly-distributed class centroids (PEDCC) can be widely used in
models and algorithms of pattern classification, such as CNN classifiers,
classification autoencoders, clustering, and semi-supervised learning, etc. Its
basic idea is to predefine the class centers, which are evenly-distributed on
the unit hypersphere in feature space, to maximize the inter-class distance.
The previous method of generating PEDCC uses an iterative algorithm based on a
charge model, that is, the initial values of various centers (charge positions)
are randomly set from the normal distribution, and the charge positions are
updated iteratively with the help of the repulsive force between charges of the
same polarity. The class centers generated by the algorithm will produce some
errors with the theoretically evenly-distributed points, and the generation
time will be longer. This paper takes advantage of regular polyhedron in
high-dimensional space and the evenly distribution of points on the n
dimensional hypersphere to generate PEDCC mathematically. Then, we discussed
the basic and extensive characteristics of the frames formed by PEDCC. Finally,
experiments show that new algorithm is not only faster than the iterative
method, but also more accurate in position. The mathematical analysis and
experimental results of this paper can provide a theoretical tool for using
PEDCC to solve the key problems in the field of pattern recognition, such as
interpretable supervised/unsupervised learning, incremental learning,
uncertainty analysis and so on.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Haiping Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yingying Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Qiuyu Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1"&gt;Guohui Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the importance of cross-task features for class-incremental learning. (arXiv:2106.11930v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11930</id>
        <link href="http://arxiv.org/abs/2106.11930"/>
        <updated>2021-06-23T01:48:39.685Z</updated>
        <summary type="html"><![CDATA[In class-incremental learning, an agent with limited resources needs to learn
a sequence of classification tasks, forming an ever growing classification
problem, with the constraint of not being able to access data from previous
tasks. The main difference with task-incremental learning, where a task-ID is
available at inference time, is that the learner also needs to perform
cross-task discrimination, i.e. distinguish between classes that have not been
seen together. Approaches to tackle this problem are numerous and mostly make
use of an external memory (buffer) of non-negligible size. In this paper, we
ablate the learning of cross-task features and study its influence on the
performance of basic replay strategies used for class-IL. We also define a new
forgetting measure for class-incremental learning, and see that forgetting is
not the principal cause of low performance. Our experimental results show that
future algorithms for class-incremental learning should not only prevent
forgetting, but also aim to improve the quality of the cross-task features.
This is especially important when the number of classes per task is small.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soutif__Cormerais_A/0/1/0/all/0/1"&gt;Albin Soutif--Cormerais&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masana_M/0/1/0/all/0/1"&gt;Marc Masana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1"&gt;Joost Van de Weijer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Twardowski_B/0/1/0/all/0/1"&gt;Bart&amp;#x142;omiej Twardowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prototypical Cross-Attention Networks for Multiple Object Tracking and Segmentation. (arXiv:2106.11958v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11958</id>
        <link href="http://arxiv.org/abs/2106.11958"/>
        <updated>2021-06-23T01:48:39.676Z</updated>
        <summary type="html"><![CDATA[Multiple object tracking and segmentation requires detecting, tracking, and
segmenting objects belonging to a set of given classes. Most approaches only
exploit the temporal dimension to address the association problem, while
relying on single frame predictions for the segmentation mask itself. We
propose Prototypical Cross-Attention Network (PCAN), capable of leveraging rich
spatio-temporal information for online multiple object tracking and
segmentation. PCAN first distills a space-time memory into a set of prototypes
and then employs cross-attention to retrieve rich information from the past
frames. To segment each object, PCAN adopts a prototypical appearance module to
learn a set of contrastive foreground and background prototypes, which are then
propagated over time. Extensive experiments demonstrate that PCAN outperforms
current video instance tracking and segmentation competition winners on both
Youtube-VIS and BDD100K datasets, and shows efficacy to both one-stage and
two-stage segmentation frameworks. Code will be available at
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1"&gt;Lei Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1"&gt;Martin Danelljan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1"&gt;Yu-Wing Tai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1"&gt;Chi-Keung Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1"&gt;Fisher Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RGB2Hands: Real-Time Tracking of 3D Hand Interactions from Monocular RGB Video. (arXiv:2106.11725v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11725</id>
        <link href="http://arxiv.org/abs/2106.11725"/>
        <updated>2021-06-23T01:48:39.669Z</updated>
        <summary type="html"><![CDATA[Tracking and reconstructing the 3D pose and geometry of two hands in
interaction is a challenging problem that has a high relevance for several
human-computer interaction applications, including AR/VR, robotics, or sign
language recognition. Existing works are either limited to simpler tracking
settings (e.g., considering only a single hand or two spatially separated
hands), or rely on less ubiquitous sensors, such as depth cameras. In contrast,
in this work we present the first real-time method for motion capture of
skeletal pose and 3D surface geometry of hands from a single RGB camera that
explicitly considers close interactions. In order to address the inherent depth
ambiguities in RGB data, we propose a novel multi-task CNN that regresses
multiple complementary pieces of information, including segmentation, dense
matchings to a 3D hand model, and 2D keypoint positions, together with newly
proposed intra-hand relative depth and inter-hand distance maps. These
predictions are subsequently used in a generative model fitting framework in
order to estimate pose and shape parameters of a 3D hand model for both hands.
We experimentally verify the individual components of our RGB two-hand tracking
and 3D reconstruction pipeline through an extensive ablation study. Moreover,
we demonstrate that our approach offers previously unseen two-hand tracking
performance from RGB, and quantitatively and qualitatively outperforms existing
RGB-based methods that were not explicitly designed for two-hand interactions.
Moreover, our method even performs on-par with depth-based real-time methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiayi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mueller_F/0/1/0/all/0/1"&gt;Franziska Mueller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bernard_F/0/1/0/all/0/1"&gt;Florian Bernard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sorli_S/0/1/0/all/0/1"&gt;Suzanne Sorli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sotnychenko_O/0/1/0/all/0/1"&gt;Oleksandr Sotnychenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_N/0/1/0/all/0/1"&gt;Neng Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Otaduy_M/0/1/0/all/0/1"&gt;Miguel A. Otaduy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casas_D/0/1/0/all/0/1"&gt;Dan Casas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Focus U-Net: A novel dual attention-gated CNN for polyp segmentation during colonoscopy. (arXiv:2105.07467v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07467</id>
        <link href="http://arxiv.org/abs/2105.07467"/>
        <updated>2021-06-23T01:48:39.662Z</updated>
        <summary type="html"><![CDATA[Background: Colonoscopy remains the gold-standard screening for colorectal
cancer. However, significant miss rates for polyps have been reported,
particularly when there are multiple small adenomas. This presents an
opportunity to leverage computer-aided systems to support clinicians and reduce
the number of polyps missed.

Method: In this work we introduce the Focus U-Net, a novel dual
attention-gated deep neural network, which combines efficient spatial and
channel-based attention into a single Focus Gate module to encourage selective
learning of polyp features. The Focus U-Net further incorporates short-range
skip connections and deep supervision. Furthermore, we introduce the Hybrid
Focal loss, a new compound loss function based on the Focal loss and Focal
Tversky loss, to handle class-imbalanced image segmentation. For our
experiments, we selected five public datasets containing images of polyps
obtained during optical colonoscopy: CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB,
ETIS-Larib PolypDB and EndoScene test set. To evaluate model performance, we
use the Dice similarity coefficient (DSC) and Intersection over Union (IoU)
metrics.

Results: Our model achieves state-of-the-art results for both CVC-ClinicDB
and Kvasir-SEG, with a mean DSC of 0.941 and 0.910, respectively. When
evaluated on a combination of five public polyp datasets, our model similarly
achieves state-of-the-art results with a mean DSC of 0.878 and mean IoU of
0.809, a 14% and 15% improvement over the previous state-of-the-art results of
0.768 and 0.702, respectively.

Conclusions: This study shows the potential for deep learning to provide fast
and accurate polyp segmentation results for use during colonoscopy. The Focus
U-Net may be adapted for future use in newer non-invasive screening and more
broadly to other biomedical image segmentation tasks involving class imbalance
and requiring efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yeung_M/0/1/0/all/0/1"&gt;Michael Yeung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sala_E/0/1/0/all/0/1"&gt;Evis Sala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1"&gt;Carola-Bibiane Sch&amp;#xf6;nlieb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rundo_L/0/1/0/all/0/1"&gt;Leonardo Rundo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unified Shape and SVBRDF Recovery using Differentiable Monte Carlo Rendering. (arXiv:2103.15208v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15208</id>
        <link href="http://arxiv.org/abs/2103.15208"/>
        <updated>2021-06-23T01:48:39.655Z</updated>
        <summary type="html"><![CDATA[Reconstructing the shape and appearance of real-world objects using measured
2D images has been a long-standing problem in computer vision. In this paper,
we introduce a new analysis-by-synthesis technique capable of producing
high-quality reconstructions through robust coarse-to-fine optimization and
physics-based differentiable rendering.

Unlike most previous methods that handle geometry and reflectance largely
separately, our method unifies the optimization of both by leveraging image
gradients with respect to both object reflectance and geometry. To obtain
physically accurate gradient estimates, we develop a new GPU-based Monte Carlo
differentiable renderer leveraging recent advances in differentiable rendering
theory to offer unbiased gradients while enjoying better performance than
existing tools like PyTorch3D and redner. To further improve robustness, we
utilize several shape and material priors as well as a coarse-to-fine
optimization strategy to reconstruct geometry. We demonstrate that our
technique can produce reconstructions with higher quality than previous methods
such as COLMAP and Kinect Fusion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luan_F/0/1/0/all/0/1"&gt;Fujun Luan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Shuang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bala_K/0/1/0/all/0/1"&gt;Kavita Bala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1"&gt;Zhao Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lasry-Lions Envelopes and Nonconvex Optimization: A Homotopy Approach. (arXiv:2103.08533v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08533</id>
        <link href="http://arxiv.org/abs/2103.08533"/>
        <updated>2021-06-23T01:48:39.633Z</updated>
        <summary type="html"><![CDATA[In large-scale optimization, the presence of nonsmooth and nonconvex terms in
a given problem typically makes it hard to solve. A popular approach to address
nonsmooth terms in convex optimization is to approximate them with their
respective Moreau envelopes. In this work, we study the use of Lasry-Lions
double envelopes to approximate nonsmooth terms that are also not convex. These
envelopes are an extension of the Moreau ones but exhibit an additional
smoothness property that makes them amenable to fast optimization algorithms.
Lasry-Lions envelopes can also be seen as an "intermediate" between a given
function and its convex envelope, and we make use of this property to develop a
method that builds a sequence of approximate subproblems that are easier to
solve than the original problem. We discuss convergence properties of this
method when used to address composite minimization problems; additionally,
based on a number of experiments, we discuss settings where it may be more
useful than classical alternatives in two domains: signal decoding and spectral
unmixing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Simoes_M/0/1/0/all/0/1"&gt;Miguel Sim&amp;#xf5;es&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Themelis_A/0/1/0/all/0/1"&gt;Andreas Themelis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Patrinos_P/0/1/0/all/0/1"&gt;Panagiotis Patrinos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Solving Inefficiency of Self-supervised Representation Learning. (arXiv:2104.08760v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08760</id>
        <link href="http://arxiv.org/abs/2104.08760"/>
        <updated>2021-06-23T01:48:39.626Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning (especially contrastive learning) has attracted
great interest due to its tremendous potentials in learning discriminative
representations in an unsupervised manner. Despite the acknowledged successes,
existing contrastive learning methods suffer from very low learning efficiency,
e.g., taking about ten times more training epochs than supervised learning for
comparable recognition accuracy. In this paper, we discover two contradictory
phenomena in contrastive learning that we call under-clustering and
over-clustering problems, which are major obstacles to learning efficiency.
Under-clustering means that the model cannot efficiently learn to discover the
dissimilarity between inter-class samples when the negative sample pairs for
contrastive learning are insufficient to differentiate all the actual object
categories. Over-clustering implies that the model cannot efficiently learn the
feature representation from excessive negative sample pairs, which enforces the
model to over-cluster samples of the same actual categories into different
clusters. To simultaneously overcome these two problems, we propose a novel
self-supervised learning framework using a median triplet loss. Precisely, we
employ a triplet loss tending to maximize the relative distance between the
positive pair and negative pairs to address the under-clustering problem; and
we construct the negative pair by selecting the negative sample of a median
similarity score from all negative samples to avoid the over-clustering
problem, guaranteed by the Bernoulli Distribution model. We extensively
evaluate our proposed framework in several large-scale benchmarks (e.g.,
ImageNet, SYSU-30k, and COCO). The results demonstrate the superior performance
(e.g., the learning efficiency) of our model over the latest state-of-the-art
methods by a clear margin. Codes available at:
https://github.com/wanggrun/triplet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guangrun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Keze Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guangcong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip H.S. Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1"&gt;Liang Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Networks as Flows of Velocity Fields for Diffeomorphic Time Series Alignment. (arXiv:2106.11911v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11911</id>
        <link href="http://arxiv.org/abs/2106.11911"/>
        <updated>2021-06-23T01:48:39.619Z</updated>
        <summary type="html"><![CDATA[Non-linear (large) time warping is a challenging source of nuisance in
time-series analysis. In this paper, we propose a novel diffeomorphic temporal
transformer network for both pairwise and joint time-series alignment. Our
ResNet-TW (Deep Residual Network for Time Warping) tackles the alignment
problem by compositing a flow of incremental diffeomorphic mappings. Governed
by the flow equation, our Residual Network (ResNet) builds smooth, fluid and
regular flows of velocity fields and consequently generates smooth and
invertible transformations (i.e. diffeomorphic warping functions). Inspired by
the elegant Large Deformation Diffeomorphic Metric Mapping (LDDMM) framework,
the final transformation is built by the flow of time-dependent vector fields
which are none other than the building blocks of our Residual Network. The
latter is naturally viewed as an Eulerian discretization schema of the flow
equation (an ODE). Once trained, our ResNet-TW aligns unseen data by a single
inexpensive forward pass. As we show in experiments on both univariate (84
datasets from UCR archive) and multivariate time-series (MSR Action-3D,
Florence-3D and MSR Daily Activity), ResNet-TW achieves competitive performance
in joint alignment and classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amor_B/0/1/0/all/0/1"&gt;Boulbaba Ben Amor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xichan Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1"&gt;Fan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yi Fang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tracking Instances as Queries. (arXiv:2106.11963v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11963</id>
        <link href="http://arxiv.org/abs/2106.11963"/>
        <updated>2021-06-23T01:48:39.612Z</updated>
        <summary type="html"><![CDATA[Recently, query based deep networks catch lots of attention owing to their
end-to-end pipeline and competitive results on several fundamental computer
vision tasks, such as object detection, semantic segmentation, and instance
segmentation. However, how to establish a query based video instance
segmentation (VIS) framework with elegant architecture and strong performance
remains to be settled. In this paper, we present \textbf{QueryTrack} (i.e.,
tracking instances as queries), a unified query based VIS framework fully
leveraging the intrinsic one-to-one correspondence between instances and
queries in QueryInst. The proposed method obtains 52.7 / 52.3 AP on
YouTube-VIS-2019 / 2021 datasets, which wins the 2-nd place in the YouTube-VIS
Challenge at CVPR 2021 \textbf{with a single online end-to-end model, single
scale testing \& modest amount of training data}. We also provide
QueryTrack-ResNet-50 baseline results on YouTube-VIS-2021 dataset as references
for the VIS community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shusheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yuxin Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinggang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1"&gt;Ying Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1"&gt;Bin Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wenyu Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis and Applications of Class-wise Robustness in Adversarial Training. (arXiv:2105.14240v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14240</id>
        <link href="http://arxiv.org/abs/2105.14240"/>
        <updated>2021-06-23T01:48:39.593Z</updated>
        <summary type="html"><![CDATA[Adversarial training is one of the most effective approaches to improve model
robustness against adversarial examples. However, previous works mainly focus
on the overall robustness of the model, and the in-depth analysis on the role
of each class involved in adversarial training is still missing. In this paper,
we propose to analyze the class-wise robustness in adversarial training. First,
we provide a detailed diagnosis of adversarial training on six benchmark
datasets, i.e., MNIST, CIFAR-10, CIFAR-100, SVHN, STL-10 and ImageNet.
Surprisingly, we find that there are remarkable robustness discrepancies among
classes, leading to unbalance/unfair class-wise robustness in the robust
models. Furthermore, we keep investigating the relations between classes and
find that the unbalanced class-wise robustness is pretty consistent among
different attack and defense methods. Moreover, we observe that the stronger
attack methods in adversarial learning achieve performance improvement mainly
from a more successful attack on the vulnerable classes (i.e., classes with
less robustness). Inspired by these interesting findings, we design a simple
but effective attack method based on the traditional PGD attack, named
Temperature-PGD attack, which proposes to enlarge the robustness disparity
among classes with a temperature factor on the confidence distribution of each
image. Experiments demonstrate our method can achieve a higher attack rate than
the PGD attack. Furthermore, from the defense perspective, we also make some
modifications in the training and inference phases to improve the robustness of
the most vulnerable class, so as to mitigate the large difference in class-wise
robustness. We believe our work can contribute to a more comprehensive
understanding of adversarial training as well as rethinking the class-wise
properties in robust models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1"&gt;Kun Kuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1"&gt;Kelu Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yisen Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GEM: Glare or Gloom, I Can Still See You -- End-to-End Multimodal Object Detection. (arXiv:2102.12319v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12319</id>
        <link href="http://arxiv.org/abs/2102.12319"/>
        <updated>2021-06-23T01:48:39.584Z</updated>
        <summary type="html"><![CDATA[Deep neural networks designed for vision tasks are often prone to failure
when they encounter environmental conditions not covered by the training data.
Single-modal strategies are insufficient when the sensor fails to acquire
information due to malfunction or its design limitations. Multi-sensor
configurations are known to provide redundancy, increase reliability, and are
crucial in achieving robustness against asymmetric sensor failures. To address
the issue of changing lighting conditions and asymmetric sensor degradation in
object detection, we develop a multi-modal 2D object detector, and propose
deterministic and stochastic sensor-aware feature fusion strategies. The
proposed fusion mechanisms are driven by the estimated sensor measurement
reliability values/weights. Reliable object detection in harsh lighting
conditions is essential for applications such as self-driving vehicles and
human-robot interaction. We also propose a new "r-blended" hybrid depth
modality for RGB-D sensors. Through extensive experimentation, we show that the
proposed strategies outperform the existing state-of-the-art methods on the
FLIR-Thermal dataset, and obtain promising results on the SUNRGB-D dataset. We
additionally record a new RGB-Infra indoor dataset, namely L515-Indoors, and
demonstrate that the proposed object detection methodologies are highly
effective for a variety of lighting conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mazhar_O/0/1/0/all/0/1"&gt;Osama Mazhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babuska_R/0/1/0/all/0/1"&gt;Robert Babuska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kober_J/0/1/0/all/0/1"&gt;Jens Kober&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Part-Aware Measurement for Robust Multi-View Multi-Human 3D Pose Estimation and Tracking. (arXiv:2106.11589v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11589</id>
        <link href="http://arxiv.org/abs/2106.11589"/>
        <updated>2021-06-23T01:48:39.577Z</updated>
        <summary type="html"><![CDATA[This paper introduces an approach for multi-human 3D pose estimation and
tracking based on calibrated multi-view. The main challenge lies in finding the
cross-view and temporal correspondences correctly even when several human pose
estimations are noisy. Compare to previous solutions that construct 3D poses
from multiple views, our approach takes advantage of temporal consistency to
match the 2D poses estimated with previously constructed 3D skeletons in every
view. Therefore cross-view and temporal associations are accomplished
simultaneously. Since the performance suffers from mistaken association and
noisy predictions, we design two strategies for aiming better correspondences
and 3D reconstruction. Specifically, we propose a part-aware measurement for
2D-3D association and a filter that can cope with 2D outliers during
reconstruction. Our approach is efficient and effective comparing to
state-of-the-art methods; it achieves competitive results on two benchmarks:
96.8% on Campus and 97.4% on Shelf. Moreover, we extends the length of Campus
evaluation frames to be more challenging and our proposal also reach
well-performed result.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chu_H/0/1/0/all/0/1"&gt;Hau Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jia-Hong Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Yao-Chih Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1"&gt;Ching-Hsien Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jia-Da Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chu-Song Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Ultrasound Tongue Image Reconstruction from Lip Images Using Self-supervised Learning and Attention Mechanism. (arXiv:2106.11769v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.11769</id>
        <link href="http://arxiv.org/abs/2106.11769"/>
        <updated>2021-06-23T01:48:39.570Z</updated>
        <summary type="html"><![CDATA[Speech production is a dynamic procedure, which involved multi human organs
including the tongue, jaw and lips. Modeling the dynamics of the vocal tract
deformation is a fundamental problem to understand the speech, which is the
most common way for human daily communication. Researchers employ several
sensory streams to describe the process simultaneously, which are
incontrovertibly statistically related to other streams. In this paper, we
address the following question: given an observable image sequences of lips,
can we picture the corresponding tongue motion. We formulated this problem as
the self-supervised learning problem, and employ the two-stream convolutional
network and long-short memory network for the learning task, with the attention
mechanism. We evaluate the performance of the proposed method by leveraging the
unlabeled lip videos to predict an upcoming ultrasound tongue image sequence.
The results show that our model is able to generate images that close to the
real ultrasound tongue images, and results in the matching between two imaging
modalities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1"&gt;Haiyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jihan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MIMIR: Deep Regression for Automated Analysis of UK Biobank Body MRI. (arXiv:2106.11731v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.11731</id>
        <link href="http://arxiv.org/abs/2106.11731"/>
        <updated>2021-06-23T01:48:39.561Z</updated>
        <summary type="html"><![CDATA[UK Biobank (UKB) is conducting a large-scale study of more than half a
million volunteers, collecting health-related information on genetics,
lifestyle, blood biochemistry, and more. Medical imaging furthermore targets
100,000 subjects, with 70,000 follow-up sessions, enabling measurements of
organs, muscle, and body composition. With up to 170,000 mounting MR images,
various methodologies are accordingly engaged in large-scale image analysis.
This work presents an experimental inference engine that can automatically
predict a comprehensive profile of subject metadata from UKB neck-to-knee body
MRI. In cross-validation, it accurately inferred baseline characteristics such
as age, height, weight, and sex, but also emulated measurements of body
composition by DXA, organ volumes, and abstract properties like grip strength,
pulse rate, and type 2 diabetic status (AUC: 0.866). The proposed system can
automatically analyze thousands of subjects within hours and provide individual
confidence intervals. The underlying methodology is based on convolutional
neural networks for image-based mean-variance regression on two-dimensional
representations of the MRI data. This work aims to make the proposed system
available for free to researchers, who can use it to obtain fast and
fully-automated estimates of 72 different measurements immediately upon release
of new UK Biobank image data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Langner_T/0/1/0/all/0/1"&gt;Taro Langner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mora_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9;s Mart&amp;#xed;nez Mora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Strand_R/0/1/0/all/0/1"&gt;Robin Strand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ahlstrom_H/0/1/0/all/0/1"&gt;H&amp;#xe5;kan Ahlstr&amp;#xf6;m&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kullberg_J/0/1/0/all/0/1"&gt;Joel Kullberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pruning of Deep Spiking Neural Networks through Gradient Rewiring. (arXiv:2105.04916v3 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04916</id>
        <link href="http://arxiv.org/abs/2105.04916"/>
        <updated>2021-06-23T01:48:39.554Z</updated>
        <summary type="html"><![CDATA[Spiking Neural Networks (SNNs) have been attached great importance due to
their biological plausibility and high energy-efficiency on neuromorphic chips.
As these chips are usually resource-constrained, the compression of SNNs is
thus crucial along the road of practical use of SNNs. Most existing methods
directly apply pruning approaches in artificial neural networks (ANNs) to SNNs,
which ignore the difference between ANNs and SNNs, thus limiting the
performance of the pruned SNNs. Besides, these methods are only suitable for
shallow SNNs. In this paper, inspired by synaptogenesis and synapse elimination
in the neural system, we propose gradient rewiring (Grad R), a joint learning
algorithm of connectivity and weight for SNNs, that enables us to seamlessly
optimize network structure without retraining. Our key innovation is to
redefine the gradient to a new synaptic parameter, allowing better exploration
of network structures by taking full advantage of the competition between
pruning and regrowth of connections. The experimental results show that the
proposed method achieves minimal loss of SNNs' performance on MNIST and
CIFAR-10 dataset so far. Moreover, it reaches a $\sim$3.5% accuracy loss under
unprecedented 0.73% connectivity, which reveals remarkable structure refining
capability in SNNs. Our work suggests that there exists extremely high
redundancy in deep SNNs. Our codes are available at
https://github.com/Yanqi-Chen/Gradient-Rewiring.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yanqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhaofei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1"&gt;Wei Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1"&gt;Tiejun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting Backdoor in Deep Neural Networks via Intentional Adversarial Perturbations. (arXiv:2105.14259v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14259</id>
        <link href="http://arxiv.org/abs/2105.14259"/>
        <updated>2021-06-23T01:48:39.531Z</updated>
        <summary type="html"><![CDATA[Recent researches show that deep learning model is susceptible to backdoor
attacks. Many defenses against backdoor attacks have been proposed. However,
existing defense works require high computational overhead or backdoor attack
information such as the trigger size, which is difficult to satisfy in
realistic scenarios. In this paper, a novel backdoor detection method based on
adversarial examples is proposed. The proposed method leverages intentional
adversarial perturbations to detect whether an image contains a trigger, which
can be applied in both the training stage and the inference stage (sanitize the
training set in training stage and detect the backdoor instances in inference
stage). Specifically, given an untrusted image, the adversarial perturbation is
added to the image intentionally. If the prediction of the model on the
perturbed image is consistent with that on the unperturbed image, the input
image will be considered as a backdoor instance. Compared with most existing
defense works, the proposed adversarial perturbation based method requires low
computational resources and maintains the visual quality of the images.
Experimental results show that, the backdoor detection rate of the proposed
defense method is 99.63%, 99.76% and 99.91% on Fashion-MNIST, CIFAR-10 and
GTSRB datasets, respectively. Besides, the proposed method maintains the visual
quality of the image as the l2 norm of the added perturbation are as low as
2.8715, 3.0513 and 2.4362 on Fashion-MNIST, CIFAR-10 and GTSRB datasets,
respectively. In addition, it is also demonstrated that the proposed method can
achieve high defense performance against backdoor attacks under different
attack settings (trigger transparency, trigger size and trigger pattern).
Compared with the existing defense work (STRIP), the proposed method has better
detection performance on all the three datasets, and is more efficient than
STRIP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1"&gt;Mingfu Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yinghao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhiyu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yushu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weiqiang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GOO: A Dataset for Gaze Object Prediction in Retail Environments. (arXiv:2105.10793v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10793</id>
        <link href="http://arxiv.org/abs/2105.10793"/>
        <updated>2021-06-23T01:48:39.522Z</updated>
        <summary type="html"><![CDATA[One of the most fundamental and information-laden actions humans do is to
look at objects. However, a survey of current works reveals that existing
gaze-related datasets annotate only the pixel being looked at, and not the
boundaries of a specific object of interest. This lack of object annotation
presents an opportunity for further advancing gaze estimation research. To this
end, we present a challenging new task called gaze object prediction, where the
goal is to predict a bounding box for a person's gazed-at object. To train and
evaluate gaze networks on this task, we present the Gaze On Objects (GOO)
dataset. GOO is composed of a large set of synthetic images (GOO Synth)
supplemented by a smaller subset of real images (GOO-Real) of people looking at
objects in a retail environment. Our work establishes extensive baselines on
GOO by re-implementing and evaluating selected state-of-the art models on the
task of gaze following and domain adaptation. Code is available on github.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tomas_H/0/1/0/all/0/1"&gt;Henri Tomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reyes_M/0/1/0/all/0/1"&gt;Marcus Reyes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dionido_R/0/1/0/all/0/1"&gt;Raimarc Dionido&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ty_M/0/1/0/all/0/1"&gt;Mark Ty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mirando_J/0/1/0/all/0/1"&gt;Jonric Mirando&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casimiro_J/0/1/0/all/0/1"&gt;Joel Casimiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atienza_R/0/1/0/all/0/1"&gt;Rowel Atienza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guinto_R/0/1/0/all/0/1"&gt;Richard Guinto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta Adversarial Training against Universal Patches. (arXiv:2101.11453v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11453</id>
        <link href="http://arxiv.org/abs/2101.11453"/>
        <updated>2021-06-23T01:48:39.515Z</updated>
        <summary type="html"><![CDATA[Recently demonstrated physical-world adversarial attacks have exposed
vulnerabilities in perception systems that pose severe risks for
safety-critical applications such as autonomous driving. These attacks place
adversarial artifacts in the physical world that indirectly cause the addition
of a universal patch to inputs of a model that can fool it in a variety of
contexts. Adversarial training is the most effective defense against
image-dependent adversarial attacks. However, tailoring adversarial training to
universal patches is computationally expensive since the optimal universal
patch depends on the model weights which change during training. We propose
meta adversarial training (MAT), a novel combination of adversarial training
with meta-learning, which overcomes this challenge by meta-learning universal
patches along with model training. MAT requires little extra computation while
continuously adapting a large set of patches to the current model. MAT
considerably increases robustness against universal patch attacks on image
classification and traffic-light detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Metzen_J/0/1/0/all/0/1"&gt;Jan Hendrik Metzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finnie_N/0/1/0/all/0/1"&gt;Nicole Finnie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hutmacher_R/0/1/0/all/0/1"&gt;Robin Hutmacher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast and Reliable Probabilistic Face Embeddings in the Wild. (arXiv:2102.04075v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04075</id>
        <link href="http://arxiv.org/abs/2102.04075"/>
        <updated>2021-06-23T01:48:39.508Z</updated>
        <summary type="html"><![CDATA[Probabilistic Face Embeddings (PFE) can improve face recognition performance
in unconstrained scenarios by integrating data uncertainty into the feature
representation. However, existing PFE methods tend to be over-confident in
estimating uncertainty and is too slow to apply to large-scale face matching.
This paper proposes a regularized probabilistic face embedding method to
improve the robustness and speed of PFE. Specifically, the mutual likelihood
score (MLS) metric used in PFE is simplified to speedup the matching of face
feature pairs. Then, an output-constraint loss is proposed to penalize the
variance of the uncertainty output, which can regularize the output of the
neural network. In addition, an identification preserving loss is proposed to
improve the discriminative of the MLS metric, and a multi-layer feature fusion
module is proposed to improve the neural network's uncertainty estimation
ability. Comprehensive experiments show that the proposed method can achieve
comparable or better results in 9 benchmarks than the state-of-the-art methods,
and can improve the performance of risk-controlled face recognition. The code
of our work is publicly available in GitHub
(https://github.com/KaenChan/ProbFace).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_Q/0/1/0/all/0/1"&gt;Qi Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_T/0/1/0/all/0/1"&gt;Taihe Yi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Points to Multi-Object 3D Reconstruction. (arXiv:2012.11575v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11575</id>
        <link href="http://arxiv.org/abs/2012.11575"/>
        <updated>2021-06-23T01:48:39.499Z</updated>
        <summary type="html"><![CDATA[We propose a method to detect and reconstruct multiple 3D objects from a
single RGB image. The key idea is to optimize for detection, alignment and
shape jointly over all objects in the RGB image, while focusing on realistic
and physically plausible reconstructions. To this end, we propose a keypoint
detector that localizes objects as center points and directly predicts all
object properties, including 9-DoF bounding boxes and 3D shapes -- all in a
single forward pass. The proposed method formulates 3D shape reconstruction as
a shape selection problem, i.e. it selects among exemplar shapes from a given
database. This makes it agnostic to shape representations, which enables a
lightweight reconstruction of realistic and visually-pleasing shapes based on
CAD-models, while the training objective is formulated around point clouds and
voxel representations. A collision-loss promotes non-intersecting objects,
further increasing the reconstruction realism. Given the RGB image, the
presented approach performs lightweight reconstruction in a single-stage, it is
real-time capable, fully differentiable and end-to-end trainable. Our
experiments compare multiple approaches for 9-DoF bounding box estimation,
evaluate the novel shape-selection mechanism and compare to recent methods in
terms of 3D bounding box estimation and 3D shape reconstruction quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Engelmann_F/0/1/0/all/0/1"&gt;Francis Engelmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rematas_K/0/1/0/all/0/1"&gt;Konstantinos Rematas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leibe_B/0/1/0/all/0/1"&gt;Bastian Leibe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1"&gt;Vittorio Ferrari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confidence-Guided Radiology Report Generation. (arXiv:2106.10887v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10887</id>
        <link href="http://arxiv.org/abs/2106.10887"/>
        <updated>2021-06-23T01:48:39.477Z</updated>
        <summary type="html"><![CDATA[Medical imaging plays a pivotal role in diagnosis and treatment in clinical
practice. Inspired by the significant progress in automatic image captioning,
various deep learning (DL)-based architectures have been proposed for
generating radiology reports for medical images. However, model uncertainty
(i.e., model reliability/confidence on report generation) is still an
under-explored problem. In this paper, we propose a novel method to explicitly
quantify both the visual uncertainty and the textual uncertainty for the task
of radiology report generation. Such multi-modal uncertainties can sufficiently
capture the model confidence scores at both the report-level and the
sentence-level, and thus they are further leveraged to weight the losses for
achieving more comprehensive model optimization. Our experimental results have
demonstrated that our proposed method for model uncertainty characterization
and estimation can provide more reliable confidence scores for radiology report
generation, and our proposed uncertainty-weighted losses can achieve more
comprehensive model optimization and result in state-of-the-art performance on
a public radiology report dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yixin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zihao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1"&gt;Jiang Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1"&gt;Zhongchao Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1"&gt;Jianping Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zhiqiang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Associating Objects with Transformers for Video Object Segmentation. (arXiv:2106.02638v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02638</id>
        <link href="http://arxiv.org/abs/2106.02638"/>
        <updated>2021-06-23T01:48:39.469Z</updated>
        <summary type="html"><![CDATA[This paper investigates how to realize better and more efficient embedding
learning to tackle the semi-supervised video object segmentation under
challenging multi-object scenarios. The state-of-the-art methods learn to
decode features with a single positive object and thus have to match and
segment each target separately under multi-object scenarios, consuming multiple
times computing resources. To solve the problem, we propose an Associating
Objects with Transformers (AOT) approach to match and decode multiple objects
uniformly. In detail, AOT employs an identification mechanism to associate
multiple targets into the same high-dimensional embedding space. Thus, we can
simultaneously process the matching and segmentation decoding of multiple
objects as efficiently as processing a single object. For sufficiently modeling
multi-object association, a Long Short-Term Transformer is designed for
constructing hierarchical matching and propagation. We conduct extensive
experiments on both multi-object and single-object benchmarks to examine AOT
variant networks with different complexities. Particularly, our AOT-L
outperforms all the state-of-the-art competitors on three popular benchmarks,
i.e., YouTube-VOS (83.7% J&F), DAVIS 2017 (83.0%), and DAVIS 2016 (91.0%),
while keeping more than 3X faster multi-object run-time. Meanwhile, our AOT-T
can maintain real-time multi-object speed on the above benchmarks. We ranked
1st in the 3rd Large-scale Video Object Segmentation Challenge. The code will
be publicly available at https://github.com/z-x-yang/AOT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zongxin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yunchao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Reducing Labeling Cost in Deep Object Detection. (arXiv:2106.11921v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11921</id>
        <link href="http://arxiv.org/abs/2106.11921"/>
        <updated>2021-06-23T01:48:39.461Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have reached very high accuracy on object detection but
their success hinges on large amounts of labeled data. To reduce the dependency
on labels, various active-learning strategies have been proposed, typically
based on the confidence of the detector. However, these methods are biased
towards best-performing classes and can lead to acquired datasets that are not
good representatives of the data in the testing set. In this work, we propose a
unified framework for active learning, that considers both the uncertainty and
the robustness of the detector, ensuring that the network performs accurately
in all classes. Furthermore, our method is able to pseudo-label the very
confident predictions, suppressing a potential distribution drift while further
boosting the performance of the model. Experiments show that our method
comprehensively outperforms a wide range of active-learning methods on PASCAL
VOC07+12 and MS-COCO, having up to a 7.7% relative improvement, or up to 82%
reduction in labeling cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elezi_I/0/1/0/all/0/1"&gt;Ismail Elezi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhiding Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1"&gt;Laura Leal-Taixe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1"&gt;Jose M. Alvarez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lightweight Image Super-Resolution with Multi-scale Feature Interaction Network. (arXiv:2103.13028v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13028</id>
        <link href="http://arxiv.org/abs/2103.13028"/>
        <updated>2021-06-23T01:48:39.454Z</updated>
        <summary type="html"><![CDATA[Recently, the single image super-resolution (SISR) approaches with deep and
complex convolutional neural network structures have achieved promising
performance. However, those methods improve the performance at the cost of
higher memory consumption, which is difficult to be applied for some mobile
devices with limited storage and computing resources. To solve this problem, we
present a lightweight multi-scale feature interaction network (MSFIN). For
lightweight SISR, MSFIN expands the receptive field and adequately exploits the
informative features of the low-resolution observed images from various scales
and interactive connections. In addition, we design a lightweight recurrent
residual channel attention block (RRCAB) so that the network can benefit from
the channel attention mechanism while being sufficiently lightweight. Extensive
experiments on some benchmarks have confirmed that our proposed MSFIN can
achieve comparable performance against the state-of-the-arts with a more
lightweight model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhengxue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gao_G/0/1/0/all/0/1"&gt;Guangwei Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Juncheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yi Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_H/0/1/0/all/0/1"&gt;Huimin Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly-Supervised Temporal Action Localization Through Local-Global Background Modeling. (arXiv:2106.11811v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11811</id>
        <link href="http://arxiv.org/abs/2106.11811"/>
        <updated>2021-06-23T01:48:39.447Z</updated>
        <summary type="html"><![CDATA[Weakly-Supervised Temporal Action Localization (WS-TAL) task aims to
recognize and localize temporal starts and ends of action instances in an
untrimmed video with only video-level label supervision. Due to lack of
negative samples of background category, it is difficult for the network to
separate foreground and background, resulting in poor detection performance. In
this report, we present our 2021 HACS Challenge - Weakly-supervised Learning
Track solution that based on BaSNet to address above problem. Specifically, we
first adopt pre-trained CSN, Slowfast, TDN, and ViViT as feature extractors to
get feature sequences. Then our proposed Local-Global Background Modeling
Network (LGBM-Net) is trained to localize instances by using only video-level
labels based on Multi-Instance Learning (MIL). Finally, we ensemble multiple
models to get the final detection results and reach 22.45% mAP on the test set]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1"&gt;Zhiwu Qing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Ziyuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yutong Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shiwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jianwen Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1"&gt;Mingqian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yuanjie Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1"&gt;Nong Sang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-grained Angular Contrastive Learning with Coarse Labels. (arXiv:2012.03515v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03515</id>
        <link href="http://arxiv.org/abs/2012.03515"/>
        <updated>2021-06-23T01:48:39.424Z</updated>
        <summary type="html"><![CDATA[Few-shot learning methods offer pre-training techniques optimized for easier
later adaptation of the model to new classes (unseen during training) using one
or a few examples. This adaptivity to unseen classes is especially important
for many practical applications where the pre-trained label space cannot remain
fixed for effective use and the model needs to be "specialized" to support new
categories on the fly. One particularly interesting scenario, essentially
overlooked by the few-shot literature, is Coarse-to-Fine Few-Shot (C2FS), where
the training classes (e.g. animals) are of much `coarser granularity' than the
target (test) classes (e.g. breeds). A very practical example of C2FS is when
the target classes are sub-classes of the training classes. Intuitively, it is
especially challenging as (both regular and few-shot) supervised pre-training
tends to learn to ignore intra-class variability which is essential for
separating sub-classes. In this paper, we introduce a novel 'Angular
normalization' module that allows to effectively combine supervised and
self-supervised contrastive pre-training to approach the proposed C2FS task,
demonstrating significant gains in a broad study over multiple baselines and
datasets. We hope that this work will help to pave the way for future research
on this new, challenging, and very practical topic of C2FS classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bukchin_G/0/1/0/all/0/1"&gt;Guy Bukchin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_E/0/1/0/all/0/1"&gt;Eli Schwartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1"&gt;Kate Saenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahar_O/0/1/0/all/0/1"&gt;Ori Shahar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1"&gt;Rogerio Feris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1"&gt;Raja Giryes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlinsky_L/0/1/0/all/0/1"&gt;Leonid Karlinsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PALMAR: Towards Adaptive Multi-inhabitant Activity Recognition in Point-Cloud Technology. (arXiv:2106.11902v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11902</id>
        <link href="http://arxiv.org/abs/2106.11902"/>
        <updated>2021-06-23T01:48:39.416Z</updated>
        <summary type="html"><![CDATA[With the advancement of deep neural networks and computer vision-based Human
Activity Recognition, employment of Point-Cloud Data technologies (LiDAR,
mmWave) has seen a lot interests due to its privacy preserving nature. Given
the high promise of accurate PCD technologies, we develop, PALMAR, a
multiple-inhabitant activity recognition system by employing efficient signal
processing and novel machine learning techniques to track individual person
towards developing an adaptive multi-inhabitant tracking and HAR system. More
specifically, we propose (i) a voxelized feature representation-based real-time
PCD fine-tuning method, (ii) efficient clustering (DBSCAN and BIRCH), Adaptive
Order Hidden Markov Model based multi-person tracking and crossover ambiguity
reduction techniques and (iii) novel adaptive deep learning-based domain
adaptation technique to improve the accuracy of HAR in presence of data
scarcity and diversity (device, location and population diversity). We
experimentally evaluate our framework and systems using (i) a real-time PCD
collected by three devices (3D LiDAR and 79 GHz mmWave) from 6 participants,
(ii) one publicly available 3D LiDAR activity data (28 participants) and (iii)
an embedded hardware prototype system which provided promising HAR performances
in multi-inhabitants (96%) scenario with a 63% improvement of multi-person
tracking than state-of-art framework without losing significant system
performances in the edge computing device.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1"&gt;Mohammad Arif Ul Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1"&gt;Md Mahmudur Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Widberg_J/0/1/0/all/0/1"&gt;Jared Q Widberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Resizing by Reconstruction from Deep Features. (arXiv:1904.08475v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.08475</id>
        <link href="http://arxiv.org/abs/1904.08475"/>
        <updated>2021-06-23T01:48:39.408Z</updated>
        <summary type="html"><![CDATA[Traditional image resizing methods usually work in pixel space and use
various saliency measures. The challenge is to adjust the image shape while
trying to preserve important content. In this paper we perform image resizing
in feature space where the deep layers of a neural network contain rich
important semantic information. We directly adjust the image feature maps,
extracted from a pre-trained classification network, and reconstruct the
resized image using a neural-network based optimization. This novel approach
leverages the hierarchical encoding of the network, and in particular, the
high-level discriminative power of its deeper layers, that recognizes semantic
objects and regions and allows maintaining their aspect ratio. Our use of
reconstruction from deep features diminishes the artifacts introduced by
image-space resizing operators. We evaluate our method on benchmarks, compare
to alternative approaches, and demonstrate its strength on challenging images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arar_M/0/1/0/all/0/1"&gt;Moab Arar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Danon_D/0/1/0/all/0/1"&gt;Dov Danon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1"&gt;Daniel Cohen-Or&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1"&gt;Ariel Shamir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RUHSNet: 3D Object Detection Using Lidar Data in Real Time. (arXiv:2006.01250v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.01250</id>
        <link href="http://arxiv.org/abs/2006.01250"/>
        <updated>2021-06-23T01:48:39.400Z</updated>
        <summary type="html"><![CDATA[In this work, we address the problem of 3D object detection from point cloud
data in real time. For autonomous vehicles to work, it is very important for
the perception component to detect the real world objects with both high
accuracy and fast inference. We propose a novel neural network architecture
along with the training and optimization details for detecting 3D objects in
point cloud data. We compare the results with different backbone architectures
including the standard ones like VGG, ResNet, Inception with our backbone. Also
we present the optimization and ablation studies including designing an
efficient anchor. We use the Kitti 3D Birds Eye View dataset for benchmarking
and validating our results. Our work surpasses the state of the art in this
domain both in terms of average precision and speed running at > 30 FPS. This
makes it a feasible option to be deployed in real time applications including
self driving cars.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1"&gt;Abhinav Sagar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SODA10M: Towards Large-Scale Object Detection Benchmark for Autonomous Driving. (arXiv:2106.11118v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11118</id>
        <link href="http://arxiv.org/abs/2106.11118"/>
        <updated>2021-06-23T01:48:39.391Z</updated>
        <summary type="html"><![CDATA[Aiming at facilitating a real-world, ever-evolving and scalable autonomous
driving system, we present a large-scale benchmark for standardizing the
evaluation of different self-supervised and semi-supervised approaches by
learning from raw data, which is the first and largest benchmark to date.
Existing autonomous driving systems heavily rely on `perfect' visual perception
models (e.g., detection) trained using extensive annotated data to ensure the
safety. However, it is unrealistic to elaborately label instances of all
scenarios and circumstances (e.g., night, extreme weather, cities) when
deploying a robust autonomous driving system. Motivated by recent powerful
advances of self-supervised and semi-supervised learning, a promising direction
is to learn a robust detection model by collaboratively exploiting large-scale
unlabeled data and few labeled data. Existing dataset (e.g., KITTI, Waymo)
either provides only a small amount of data or covers limited domains with full
annotation, hindering the exploration of large-scale pre-trained models. Here,
we release a Large-Scale Object Detection benchmark for Autonomous driving,
named as SODA10M, containing 10 million unlabeled images and 20K images labeled
with 6 representative object categories. To improve diversity, the images are
collected every ten seconds per frame within 32 different cities under
different weather conditions, periods and location scenes. We provide extensive
experiments and deep analyses of existing supervised state-of-the-art detection
models, popular self-supervised and semi-supervised approaches, and some
insights about how to develop future models. The data and more up-to-date
information have been released at https://soda-2d.github.io.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jianhua Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiwen Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1"&gt;Lanqing Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1"&gt;Chaoqiang Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chunjing Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confidence-Aware Learning for Camouflaged Object Detection. (arXiv:2106.11641v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11641</id>
        <link href="http://arxiv.org/abs/2106.11641"/>
        <updated>2021-06-23T01:48:39.371Z</updated>
        <summary type="html"><![CDATA[Confidence-aware learning is proven as an effective solution to prevent
networks becoming overconfident. We present a confidence-aware camouflaged
object detection framework using dynamic supervision to produce both accurate
camouflage map and meaningful "confidence" representing model awareness about
the current prediction. A camouflaged object detection network is designed to
produce our camouflage prediction. Then, we concatenate it with the input image
and feed it to the confidence estimation network to produce an one channel
confidence map.We generate dynamic supervision for the confidence estimation
network, representing the agreement of camouflage prediction with the ground
truth camouflage map. With the produced confidence map, we introduce
confidence-aware learning with the confidence map as guidance to pay more
attention to the hard/low-confidence pixels in the loss function. We claim
that, once trained, our confidence estimation network can evaluate pixel-wise
accuracy of the prediction without relying on the ground truth camouflage map.
Extensive results on four camouflaged object detection testing datasets
illustrate the superior performance of the proposed model in explaining the
camouflage prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiawei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1"&gt;Nick Barnes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Latent Transformer for Disentangled and Identity-Preserving Face Editing. (arXiv:2106.11895v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11895</id>
        <link href="http://arxiv.org/abs/2106.11895"/>
        <updated>2021-06-23T01:48:39.364Z</updated>
        <summary type="html"><![CDATA[High quality facial image editing is a challenging problem in the movie
post-production industry, requiring a high degree of control and identity
preservation. Previous works that attempt to tackle this problem may suffer
from the entanglement of facial attributes and the loss of the person's
identity. Furthermore, many algorithms are limited to a certain task. To tackle
these limitations, we propose to edit facial attributes via the latent space of
a StyleGAN generator, by training a dedicated latent transformation network and
incorporating explicit disentanglement and identity preservation terms in the
loss function. We further introduce a pipeline to generalize our face editing
to videos. Our model achieves a disentangled, controllable, and
identity-preserving facial attribute editing, even in the challenging case of
real (i.e., non-synthetic) images and videos. We conduct extensive experiments
on image and video datasets and show that our model outperforms other
state-of-the-art methods in visual quality and quantitative evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1"&gt;Xu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Newson_A/0/1/0/all/0/1"&gt;Alasdair Newson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gousseau_Y/0/1/0/all/0/1"&gt;Yann Gousseau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hellier_P/0/1/0/all/0/1"&gt;Pierre Hellier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comparison for Patch-level Classification of Deep Learning Methods on Transparent Images: from Convolutional Neural Networks to Visual Transformers. (arXiv:2106.11582v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11582</id>
        <link href="http://arxiv.org/abs/2106.11582"/>
        <updated>2021-06-23T01:48:39.357Z</updated>
        <summary type="html"><![CDATA[Nowadays, analysis of transparent images in the field of computer vision has
gradually become a hot spot. In this paper, we compare the classification
performance of different deep learning for the problem that transparent images
are difficult to analyze. We crop the transparent images into 8 * 8 and 224 *
224 pixels patches in the same proportion, and then divide the two different
pixels patches into foreground and background according to groundtruch. We also
use 4 types of convolutional neural networks and a novel ViT network model to
compare the foreground and background classification experiments. We conclude
that ViT performs the worst in classifying 8 * 8 pixels patches, but it
outperforms most convolutional neural networks in classifying 224 * 224.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hechen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Peng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1"&gt;Ao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1"&gt;Marcin Grzegorzek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MEAL: Manifold Embedding-based Active Learning. (arXiv:2106.11858v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11858</id>
        <link href="http://arxiv.org/abs/2106.11858"/>
        <updated>2021-06-23T01:48:39.349Z</updated>
        <summary type="html"><![CDATA[Image segmentation is a common and challenging task in autonomous driving.
Availability of sufficient pixel-level annotations for the training data is a
hurdle. Active learning helps learning from small amounts of data by suggesting
the most promising samples for labeling. In this work, we propose a new
pool-based method for active learning, which proposes promising image regions,
in each acquisition step. The problem is framed in an exploration-exploitation
framework by combining an embedding based on Uniform Manifold Approximation to
model representativeness with entropy as uncertainty measure to model
informativeness. We applied our proposed method to the challenging autonomous
driving data sets CamVid and Cityscapes and performed a quantitative comparison
with state-of-the-art methods. We find that our active learning method achieves
better performance on CamVid compared to other methods, while on Cityscapes,
the performance lift was negligible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sreenivasaiah_D/0/1/0/all/0/1"&gt;Deepthi Sreenivasaiah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wollmann_T/0/1/0/all/0/1"&gt;Thomas Wollmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Supervised Momentum Contrastive Learning for Few-Shot Classification. (arXiv:2101.11058v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11058</id>
        <link href="http://arxiv.org/abs/2101.11058"/>
        <updated>2021-06-23T01:48:39.341Z</updated>
        <summary type="html"><![CDATA[Few-shot learning aims to transfer information from one task to enable
generalization on novel tasks given a few examples. This information is present
both in the domain and the class labels. In this work we investigate the
complementary roles of these two sources of information by combining
instance-discriminative contrastive learning and supervised learning in a
single framework called Supervised Momentum Contrastive learning (SUPMOCO). Our
approach avoids a problem observed in supervised learning where information in
images not relevant to the task is discarded, which hampers their
generalization to novel tasks. We show that (self-supervised) contrastive
learning and supervised learning are mutually beneficial, leading to a new
state-of-the-art on the META-DATASET - a recently introduced benchmark for
few-shot learning. Our method is based on a simple modification of MOCO and
scales better than prior work on combining supervised and self-supervised
learning. This allows us to easily combine data from multiple domains leading
to further improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Majumder_O/0/1/0/all/0/1"&gt;Orchid Majumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1"&gt;Avinash Ravichandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1"&gt;Subhransu Maji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1"&gt;Alessandro Achille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polito_M/0/1/0/all/0/1"&gt;Marzia Polito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generate High Resolution Images With Generative Variational Autoencoder. (arXiv:2008.10399v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.10399</id>
        <link href="http://arxiv.org/abs/2008.10399"/>
        <updated>2021-06-23T01:48:39.320Z</updated>
        <summary type="html"><![CDATA[In this work, we present a novel neural network to generate high resolution
images. We replace the decoder of VAE with a discriminator while using the
encoder as it is. The encoder is fed data from a normal distribution while the
generator is fed from a gaussian distribution. The combination from both is
given to a discriminator which tells whether the generated image is correct or
not. We evaluate our network on 3 different datasets: MNIST, LSUN and CelebA
dataset. Our network beats the previous state of the art using MMD, SSIM, log
likelihood, reconstruction error, ELBO and KL divergence as the evaluation
metrics while generating much sharper images. This work is potentially very
exciting as we are able to combine the advantages of generative models and
inference models in a principled bayesian manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sagar_A/0/1/0/all/0/1"&gt;Abhinav Sagar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepMesh: Differentiable Iso-Surface Extraction. (arXiv:2106.11795v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11795</id>
        <link href="http://arxiv.org/abs/2106.11795"/>
        <updated>2021-06-23T01:48:39.313Z</updated>
        <summary type="html"><![CDATA[Geometric Deep Learning has recently made striking progress with the advent
of continuous Deep Implicit Fields. They allow for detailed modeling of
watertight surfaces of arbitrary topology while not relying on a 3D Euclidean
grid, resulting in a learnable parameterization that is unlimited in
resolution. Unfortunately, these methods are often unsuitable for applications
that require an explicit mesh-based surface representation because converting
an implicit field to such a representation relies on the Marching Cubes
algorithm, which cannot be differentiated with respect to the underlying
implicit field. In this work, we remove this limitation and introduce a
differentiable way to produce explicit surface mesh representations from Deep
Implicit Fields. Our key insight is that by reasoning on how implicit field
perturbations impact local surface geometry, one can ultimately differentiate
the 3D location of surface samples with respect to the underlying deep implicit
field. We exploit this to define DeepMesh -- end-to-end differentiable mesh
representation that can vary its topology. We use two different applications to
validate our theoretical insight: Single view 3D Reconstruction via
Differentiable Rendering and Physically-Driven Shape Optimization. In both
cases our end-to-end differentiable parameterization gives us an edge over
state-of-the-art algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guillard_B/0/1/0/all/0/1"&gt;Benoit Guillard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Remelli_E/0/1/0/all/0/1"&gt;Edoardo Remelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukoianov_A/0/1/0/all/0/1"&gt;Artem Lukoianov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richter_S/0/1/0/all/0/1"&gt;Stephan Richter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1"&gt;Timur Bagautdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baque_P/0/1/0/all/0/1"&gt;Pierre Baque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1"&gt;Pascal Fua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hessian-Aware Pruning and Optimal Neural Implant. (arXiv:2101.08940v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08940</id>
        <link href="http://arxiv.org/abs/2101.08940"/>
        <updated>2021-06-23T01:48:39.305Z</updated>
        <summary type="html"><![CDATA[Pruning is an effective method to reduce the memory footprint and FLOPs
associated with neural network models. However, existing structured-pruning
methods often result in significant accuracy degradation for moderate pruning
levels. To address this problem, we introduce a new Hessian Aware Pruning (HAP)
method coupled with a Neural Implant approach that uses second-order
sensitivity as a metric for structured pruning. The basic idea is to prune
insensitive components and to use a Neural Implant for moderately sensitive
components, instead of completely pruning them. For the latter approach, the
moderately sensitive components are replaced with with a low rank implant that
is smaller and less computationally expensive than the original component. We
use the relative Hessian trace to measure sensitivity, as opposed to the
magnitude based sensitivity metric commonly used in the literature. We test HAP
for both computer vision tasks and natural language tasks, and we achieve new
state-of-the-art results. Specifically, HAP achieves less than $0.1\%$/$0.5\%$
degradation on PreResNet29/ResNet50 (CIFAR-10/ImageNet) with more than
70\%/50\% of parameters pruned. Meanwhile, HAP also achieves significantly
better performance (up to 0.8\% with 60\% of parameters pruned) as compared to
gradient based method for head pruning on transformer-based models. The
framework has been open sourced and available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shixing Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1"&gt;Zhewei Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1"&gt;Amir Gholami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1"&gt;Zhen Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sehoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1"&gt;Michael W Mahoney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1"&gt;Kurt Keutzer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trinity: A No-Code AI platform for complex spatial datasets. (arXiv:2106.11756v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2106.11756</id>
        <link href="http://arxiv.org/abs/2106.11756"/>
        <updated>2021-06-23T01:48:39.295Z</updated>
        <summary type="html"><![CDATA[We present a no-code Artificial Intelligence (AI) platform called Trinity
with the main design goal of enabling both machine learning researchers and
non-technical geospatial domain experts to experiment with domain-specific
signals and datasets for solving a variety of complex problems on their own.
This versatility to solve diverse problems is achieved by transforming complex
Spatio-temporal datasets to make them consumable by standard deep learning
models, in this case, Convolutional Neural Networks (CNNs), and giving the
ability to formulate disparate problems in a standard way, eg. semantic
segmentation. With an intuitive user interface, a feature store that hosts
derivatives of complex feature engineering, a deep learning kernel, and a
scalable data processing mechanism, Trinity provides a powerful platform for
domain experts to share the stage with scientists and engineers in solving
business-critical problems. It enables quick prototyping, rapid experimentation
and reduces the time to production by standardizing model building and
deployment. In this paper, we present our motivation behind Trinity and its
design along with showcasing sample applications to motivate the idea of
lowering the bar to using AI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Iyer_C/0/1/0/all/0/1"&gt;C.V.Krishnakumar Iyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_F/0/1/0/all/0/1"&gt;Feili Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Henry Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yonghong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_K/0/1/0/all/0/1"&gt;Kay Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1"&gt;Swetava Ganguli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_V/0/1/0/all/0/1"&gt;Vipul Pandey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of a Region Proposal Architecture for Multi-task Document Layout Analysis. (arXiv:2106.11797v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11797</id>
        <link href="http://arxiv.org/abs/2106.11797"/>
        <updated>2021-06-23T01:48:39.287Z</updated>
        <summary type="html"><![CDATA[Automatically recognizing the layout of handwritten documents is an important
step towards useful extraction of information from those documents. The most
common application is to feed downstream applications such as automatic text
recognition and keyword spotting; however, the recognition of the layout also
helps to establish relationships between elements in the document which allows
to enrich the information that can be extracted. Most of the modern document
layout analysis systems are designed to address only one part of the document
layout problem, namely: baseline detection or region segmentation. In contrast,
we evaluate the effectiveness of the Mask-RCNN architecture to address the
problem of baseline detection and region segmentation in an integrated manner.
We present experimental results on two handwritten text datasets and one
handwritten music dataset. The analyzed architecture yields promising results,
outperforming state-of-the-art techniques in all three datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Quiros_L/0/1/0/all/0/1"&gt;Lorenzo Quir&amp;#xf3;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vidal_E/0/1/0/all/0/1"&gt;Enrique Vidal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Untrained networks for compressive lensless photography. (arXiv:2103.07609v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07609</id>
        <link href="http://arxiv.org/abs/2103.07609"/>
        <updated>2021-06-23T01:48:39.264Z</updated>
        <summary type="html"><![CDATA[Compressive lensless imagers enable novel applications in an extremely
compact device, requiring only a phase or amplitude mask placed close to the
sensor. They have been demonstrated for 2D and 3D microscopy, single-shot
video, and single-shot hyperspectral imaging; in each of these cases, a
compressive-sensing-based inverse problem is solved in order to recover a 3D
data-cube from a 2D measurement. Typically, this is accomplished using convex
optimization and hand-picked priors. Alternatively, deep learning-based
reconstruction methods offer the promise of better priors, but require many
thousands of ground truth training pairs, which can be difficult or impossible
to acquire. In this work, we propose the use of untrained networks for
compressive image recovery. Our approach does not require any labeled training
data, but instead uses the measurement itself to update the network weights. We
demonstrate our untrained approach on lensless compressive 2D imaging as well
as single-shot high-speed video recovery using the camera's rolling shutter,
and single-shot hyperspectral imaging. We provide simulation and experimental
verification, showing that our method results in improved image quality over
existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Monakhova_K/0/1/0/all/0/1"&gt;Kristina Monakhova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tran_V/0/1/0/all/0/1"&gt;Vi Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kuo_G/0/1/0/all/0/1"&gt;Grace Kuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Waller_L/0/1/0/all/0/1"&gt;Laura Waller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhanced Separable Disentanglement for Unsupervised Domain Adaptation. (arXiv:2106.11915v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11915</id>
        <link href="http://arxiv.org/abs/2106.11915"/>
        <updated>2021-06-23T01:48:39.256Z</updated>
        <summary type="html"><![CDATA[Domain adaptation aims to mitigate the domain gap when transferring knowledge
from an existing labeled domain to a new domain. However, existing
disentanglement-based methods do not fully consider separation between
domain-invariant and domain-specific features, which means the domain-invariant
features are not discriminative. The reconstructed features are also not
sufficiently used during training. In this paper, we propose a novel enhanced
separable disentanglement (ESD) model. We first employ a disentangler to
distill domain-invariant and domain-specific features. Then, we apply feature
separation enhancement processes to minimize contamination between
domain-invariant and domain-specific features. Finally, our model reconstructs
complete feature vectors, which are used for further disentanglement during the
training phase. Extensive experiments from three benchmark datasets outperform
state-of-the-art methods, especially on challenging cross-domain tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Youshan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davison_B/0/1/0/all/0/1"&gt;Brian D. Davison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[G-VAE, a Geometric Convolutional VAE for ProteinStructure Generation. (arXiv:2106.11920v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11920</id>
        <link href="http://arxiv.org/abs/2106.11920"/>
        <updated>2021-06-23T01:48:39.242Z</updated>
        <summary type="html"><![CDATA[Analyzing the structure of proteins is a key part of understanding their
functions and thus their role in biology at the molecular level. In addition,
design new proteins in a methodical way is a major engineering challenge. In
this work, we introduce a joint geometric-neural networks approach for
comparing, deforming and generating 3D protein structures. Viewing protein
structures as 3D open curves, we adopt the Square Root Velocity Function (SRVF)
representation and leverage its suitable geometric properties along with Deep
Residual Networks (ResNets) for a joint registration and comparison. Our
ResNets handle better large protein deformations while being more
computationally efficient. On top of the mathematical framework, we further
design a Geometric Variational Auto-Encoder (G-VAE), that once trained, maps
original, previously unseen structures, into a low-dimensional (latent)
hyper-sphere. Motivated by the spherical structure of the pre-shape space, we
naturally adopt the von Mises-Fisher (vMF) distribution to model our hidden
variables. We test the effectiveness of our models by generating novel protein
structures and predicting completions of corrupted protein structures.
Experimental results show that our method is able to generate plausible
structures, different from the structures in the training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amor_B/0/1/0/all/0/1"&gt;Boulbaba Ben Amor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xichan Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1"&gt;Fan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yi Fang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis and Tuning of a Voice Assistant System for Dysfluent Speech. (arXiv:2106.11759v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.11759</id>
        <link href="http://arxiv.org/abs/2106.11759"/>
        <updated>2021-06-23T01:48:39.232Z</updated>
        <summary type="html"><![CDATA[Dysfluencies and variations in speech pronunciation can severely degrade
speech recognition performance, and for many individuals with
moderate-to-severe speech disorders, voice operated systems do not work.
Current speech recognition systems are trained primarily with data from fluent
speakers and as a consequence do not generalize well to speech with
dysfluencies such as sound or word repetitions, sound prolongations, or audible
blocks. The focus of this work is on quantitative analysis of a consumer speech
recognition system on individuals who stutter and production-oriented
approaches for improving performance for common voice assistant tasks (i.e.,
"what is the weather?"). At baseline, this system introduces a significant
number of insertion and substitution errors resulting in intended speech Word
Error Rates (isWER) that are 13.64\% worse (absolute) for individuals with
fluency disorders. We show that by simply tuning the decoding parameters in an
existing hybrid speech recognition system one can improve isWER by 24\%
(relative) for individuals with fluency disorders. Tuning these parameters
translates to 3.6\% better domain recognition and 1.7\% better intent
recognition relative to the default setup for the 18 study participants across
all stuttering severities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mitra_V/0/1/0/all/0/1"&gt;Vikramjit Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zifang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lea_C/0/1/0/all/0/1"&gt;Colin Lea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tooley_L/0/1/0/all/0/1"&gt;Lauren Tooley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1"&gt;Sarah Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Botten_D/0/1/0/all/0/1"&gt;Darren Botten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Palekar_A/0/1/0/all/0/1"&gt;Ashwini Palekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thelapurath_S/0/1/0/all/0/1"&gt;Shrinath Thelapurath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Georgiou_P/0/1/0/all/0/1"&gt;Panayiotis Georgiou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kajarekar_S/0/1/0/all/0/1"&gt;Sachin Kajarekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bigham_J/0/1/0/all/0/1"&gt;Jefferey Bigham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Augmentation for Meta-Learning. (arXiv:2010.07092v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07092</id>
        <link href="http://arxiv.org/abs/2010.07092"/>
        <updated>2021-06-23T01:48:39.223Z</updated>
        <summary type="html"><![CDATA[Conventional image classifiers are trained by randomly sampling mini-batches
of images. To achieve state-of-the-art performance, practitioners use
sophisticated data augmentation schemes to expand the amount of training data
available for sampling. In contrast, meta-learning algorithms sample support
data, query data, and tasks on each training step. In this complex sampling
scenario, data augmentation can be used not only to expand the number of images
available per class, but also to generate entirely new classes/tasks. We
systematically dissect the meta-learning pipeline and investigate the distinct
ways in which data augmentation can be integrated at both the image and class
levels. Our proposed meta-specific data augmentation significantly improves the
performance of meta-learners on few-shot classification benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ni_R/0/1/0/all/0/1"&gt;Renkun Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1"&gt;Micah Goldblum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharaf_A/0/1/0/all/0/1"&gt;Amr Sharaf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_K/0/1/0/all/0/1"&gt;Kezhi Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1"&gt;Tom Goldstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EC-GAN: Low-Sample Classification using Semi-Supervised Algorithms and GANs. (arXiv:2012.15864v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15864</id>
        <link href="http://arxiv.org/abs/2012.15864"/>
        <updated>2021-06-23T01:48:39.202Z</updated>
        <summary type="html"><![CDATA[Semi-supervised learning has been gaining attention as it allows for
performing image analysis tasks such as classification with limited labeled
data. Some popular algorithms using Generative Adversarial Networks (GANs) for
semi-supervised classification share a single architecture for classification
and discrimination. However, this may require a model to converge to a separate
data distribution for each task, which may reduce overall performance. While
progress in semi-supervised learning has been made, less addressed are
small-scale, fully-supervised tasks where even unlabeled data is unavailable
and unattainable. We therefore, propose a novel GAN model namely External
Classifier GAN (EC-GAN), that utilizes GANs and semi-supervised algorithms to
improve classification in fully-supervised regimes. Our method leverages a GAN
to generate artificial data used to supplement supervised classification. More
specifically, we attach an external classifier, hence the name EC-GAN, to the
GAN's generator, as opposed to sharing an architecture with the discriminator.
Our experiments demonstrate that EC-GAN's performance is comparable to the
shared architecture method, far superior to the standard data augmentation and
regularization-based approach, and effective on a small, realistic dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haque_A/0/1/0/all/0/1"&gt;Ayaan Haque&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Shot Chinese Character Recognition with Stroke-Level Decomposition. (arXiv:2106.11613v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11613</id>
        <link href="http://arxiv.org/abs/2106.11613"/>
        <updated>2021-06-23T01:48:39.190Z</updated>
        <summary type="html"><![CDATA[Chinese character recognition has attracted much research interest due to its
wide applications. Although it has been studied for many years, some issues in
this field have not been completely resolved yet, e.g. the zero-shot problem.
Previous character-based and radical-based methods have not fundamentally
addressed the zero-shot problem since some characters or radicals in test sets
may not appear in training sets under a data-hungry condition. Inspired by the
fact that humans can generalize to know how to write characters unseen before
if they have learned stroke orders of some characters, we propose a
stroke-based method by decomposing each character into a sequence of strokes,
which are the most basic units of Chinese characters. However, we observe that
there is a one-to-many relationship between stroke sequences and Chinese
characters. To tackle this challenge, we employ a matching-based strategy to
transform the predicted stroke sequence to a specific character. We evaluate
the proposed method on handwritten characters, printed artistic characters, and
scene characters. The experimental results validate that the proposed method
outperforms existing methods on both character zero-shot and radical zero-shot
tasks. Moreover, the proposed method can be easily generalized to other
languages whose characters can be decomposed into strokes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jingye Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1"&gt;Xiangyang Xue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Hierarchy Preserving Deep Hashing for Large-scale Image Retrieval. (arXiv:1901.11259v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1901.11259</id>
        <link href="http://arxiv.org/abs/1901.11259"/>
        <updated>2021-06-23T01:48:39.176Z</updated>
        <summary type="html"><![CDATA[Deep hashing models have been proposed as an efficient method for large-scale
similarity search. However, most existing deep hashing methods only utilize
fine-level labels for training while ignoring the natural semantic hierarchy
structure. This paper presents an effective method that preserves the classwise
similarity of full-level semantic hierarchy for large-scale image retrieval.
Experiments on two benchmark datasets show that our method helps improve the
fine-level retrieval performance. Moreover, with the help of the semantic
hierarchy, it can produce significantly better binary codes for hierarchical
retrieval, which indicates its potential of providing more user-desired
retrieval results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Ming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhe_X/0/1/0/all/0/1"&gt;Xuefei Zhe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ou_Yang_L/0/1/0/all/0/1"&gt;Le Ou-Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shifeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1"&gt;Hong Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Hitchhiker's Guide to Prior-Shift Adaptation. (arXiv:2106.11695v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11695</id>
        <link href="http://arxiv.org/abs/2106.11695"/>
        <updated>2021-06-23T01:48:39.170Z</updated>
        <summary type="html"><![CDATA[In many computer vision classification tasks, class priors at test time often
differ from priors on the training set. In the case of such prior shift,
classifiers must be adapted correspondingly to maintain close to optimal
performance. This paper analyzes methods for adaptation of probabilistic
classifiers to new priors and for estimating new priors on an unlabeled test
set. We propose a novel method to address a known issue of prior estimation
methods based on confusion matrices, where inconsistent estimates of decision
probabilities and confusion matrices lead to negative values in the estimated
priors. Experiments on fine-grained image classification datasets provide
insight into the best practice of prior shift estimation and classifier
adaptation and show that the proposed method achieves state-of-the-art results
in prior adaptation. Applying the best practice to two tasks with naturally
imbalanced priors, learning from web-crawled images and plant species
classification, increased the recognition accuracy by 1.1% and 3.4%
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sipka_T/0/1/0/all/0/1"&gt;Tomas Sipka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sulc_M/0/1/0/all/0/1"&gt;Milan Sulc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1"&gt;Jiri Matas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Stealthy and Robust Fingerprinting Scheme for Generative Models. (arXiv:2106.11760v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.11760</id>
        <link href="http://arxiv.org/abs/2106.11760"/>
        <updated>2021-06-23T01:48:39.163Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel fingerprinting methodology for the Intellectual
Property protection of generative models. Prior solutions for discriminative
models usually adopt adversarial examples as the fingerprints, which give
anomalous inference behaviors and prediction results. Hence, these methods are
not stealthy and can be easily recognized by the adversary. Our approach
leverages the invisible backdoor technique to overcome the above limitation.
Specifically, we design verification samples, whose model outputs look normal
but can trigger a backdoor classifier to make abnormal predictions. We propose
a new backdoor embedding approach with Unique-Triplet Loss and fine-grained
categorization to enhance the effectiveness of our fingerprints. Extensive
evaluations show that this solution can outperform other strategies with higher
robustness, uniqueness and stealthiness for various GAN models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guanlin_L/0/1/0/all/0/1"&gt;Li Guanlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shangwei_G/0/1/0/all/0/1"&gt;Guo Shangwei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Run_W/0/1/0/all/0/1"&gt;Wang Run&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guowen_X/0/1/0/all/0/1"&gt;Xu Guowen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tianwei_Z/0/1/0/all/0/1"&gt;Zhang Tianwei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Normalized Avatar Synthesis Using StyleGAN and Perceptual Refinement. (arXiv:2106.11423v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11423</id>
        <link href="http://arxiv.org/abs/2106.11423"/>
        <updated>2021-06-23T01:48:39.155Z</updated>
        <summary type="html"><![CDATA[We introduce a highly robust GAN-based framework for digitizing a normalized
3D avatar of a person from a single unconstrained photo. While the input image
can be of a smiling person or taken in extreme lighting conditions, our method
can reliably produce a high-quality textured model of a person's face in
neutral expression and skin textures under diffuse lighting condition.
Cutting-edge 3D face reconstruction methods use non-linear morphable face
models combined with GAN-based decoders to capture the likeness and details of
a person but fail to produce neutral head models with unshaded albedo textures
which is critical for creating relightable and animation-friendly avatars for
integration in virtual environments. The key challenges for existing methods to
work is the lack of training and ground truth data containing normalized 3D
faces. We propose a two-stage approach to address this problem. First, we adopt
a highly robust normalized 3D face generator by embedding a non-linear
morphable face model into a StyleGAN2 network. This allows us to generate
detailed but normalized facial assets. This inference is then followed by a
perceptual refinement step that uses the generated assets as regularization to
cope with the limited available training samples of normalized faces. We
further introduce a Normalized Face Dataset, which consists of a combination
photogrammetry scans, carefully selected photographs, and generated fake people
with neutral expressions in diffuse lighting conditions. While our prepared
dataset contains two orders of magnitude less subjects than cutting edge
GAN-based 3D facial reconstruction methods, we show that it is possible to
produce high-quality normalized face models for very challenging unconstrained
input images, and demonstrate superior performance to the current
state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Huiwen Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagano_K/0/1/0/all/0/1"&gt;Koki Nagano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kung_H/0/1/0/all/0/1"&gt;Han-Wei Kung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldwhite_M/0/1/0/all/0/1"&gt;Mclean Goldwhite&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1"&gt;Qingguo Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zejian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1"&gt;Lingyu Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1"&gt;Liwen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[nuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles. (arXiv:2106.11810v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11810</id>
        <link href="http://arxiv.org/abs/2106.11810"/>
        <updated>2021-06-23T01:48:39.135Z</updated>
        <summary type="html"><![CDATA[In this work, we propose the world's first closed-loop ML-based planning
benchmark for autonomous driving. While there is a growing body of ML-based
motion planners, the lack of established datasets and metrics has limited the
progress in this area. Existing benchmarks for autonomous vehicle motion
prediction have focused on short-term motion forecasting, rather than long-term
planning. This has led previous works to use open-loop evaluation with L2-based
metrics, which are not suitable for fairly evaluating long-term planning. Our
benchmark overcomes these limitations by introducing a large-scale driving
dataset, lightweight closed-loop simulator, and motion-planning-specific
metrics. We provide a high-quality dataset with 1500h of human driving data
from 4 cities across the US and Asia with widely varying traffic patterns
(Boston, Pittsburgh, Las Vegas and Singapore). We will provide a closed-loop
simulation framework with reactive agents and provide a large set of both
general and scenario-specific planning metrics. We plan to release the dataset
at NeurIPS 2021 and organize benchmark challenges starting in early 2022.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Caesar_H/0/1/0/all/0/1"&gt;Holger Caesar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kabzan_J/0/1/0/all/0/1"&gt;Juraj Kabzan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1"&gt;Kok Seang Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fong_W/0/1/0/all/0/1"&gt;Whye Kit Fong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolff_E/0/1/0/all/0/1"&gt;Eric Wolff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lang_A/0/1/0/all/0/1"&gt;Alex Lang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fletcher_L/0/1/0/all/0/1"&gt;Luke Fletcher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beijbom_O/0/1/0/all/0/1"&gt;Oscar Beijbom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Omari_S/0/1/0/all/0/1"&gt;Sammy Omari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep3DPose: Realtime Reconstruction of Arbitrarily Posed Human Bodies from Single RGB Images. (arXiv:2106.11536v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11536</id>
        <link href="http://arxiv.org/abs/2106.11536"/>
        <updated>2021-06-23T01:48:39.127Z</updated>
        <summary type="html"><![CDATA[We introduce an approach that accurately reconstructs 3D human poses and
detailed 3D full-body geometric models from single images in realtime. The key
idea of our approach is a novel end-to-end multi-task deep learning framework
that uses single images to predict five outputs simultaneously: foreground
segmentation mask, 2D joints positions, semantic body partitions, 3D part
orientations and uv coordinates (uv map). The multi-task network architecture
not only generates more visual cues for reconstruction, but also makes each
individual prediction more accurate. The CNN regressor is further combined with
an optimization based algorithm for accurate kinematic pose reconstruction and
full-body shape modeling. We show that the realtime reconstruction reaches
accurate fitting that has not been seen before, especially for wild images. We
demonstrate the results of our realtime 3D pose and human body reconstruction
system on various challenging in-the-wild videos. We show the system advances
the frontier of 3D human body and pose reconstruction from single images by
quantitative evaluations and comparisons with state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Liguo Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Miaopeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianjie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Congyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Juntao Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xinguo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1"&gt;Jinxiang Chai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of the Vision-based Approaches for Dietary Assessment. (arXiv:2106.11776v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11776</id>
        <link href="http://arxiv.org/abs/2106.11776"/>
        <updated>2021-06-23T01:48:39.118Z</updated>
        <summary type="html"><![CDATA[Dietary-related problems such as obesity are a growing concern in todays
modern world. If the current trend continues, it is most likely that the
quality of life, in general, is significantly affected since obesity is
associated with other chronic diseases such as hypertension, irregular blood
sugar levels, and increased risk of heart attacks. The primary cause of these
problems is poor lifestyle choices and unhealthy dietary habits, with emphasis
on a select few food groups such as sugars, fats, and carbohydrates. In this
regard, computer-based food recognition offers automatic visual-based methods
to assess dietary intake and help people make healthier choices. Thus, the
following paper presents a brief review of visual-based methods for food
recognition, including their accuracy, performance, and the use of popular food
databases to evaluate existing models. The work further aims to highlight
future challenges in this area. New high-quality studies for developing
standard benchmarks and using continual learning methods for food recognition
are recommended.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tahir_G/0/1/0/all/0/1"&gt;Ghalib Tahir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loo_C/0/1/0/all/0/1"&gt;Chu Kiong Loo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning-Based Practical Light Field Image Compression Using A Disparity-Aware Model. (arXiv:2106.11558v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.11558</id>
        <link href="http://arxiv.org/abs/2106.11558"/>
        <updated>2021-06-23T01:48:39.105Z</updated>
        <summary type="html"><![CDATA[Light field technology has increasingly attracted the attention of the
research community with its many possible applications. The lenslet array in
commercial plenoptic cameras helps capture both the spatial and angular
information of light rays in a single exposure. While the resulting high
dimensionality of light field data enables its superior capabilities, it also
impedes its extensive adoption. Hence, there is a compelling need for efficient
compression of light field images. Existing solutions are commonly composed of
several separate modules, some of which may not have been designed for the
specific structure and quality of light field data. This increases the
complexity of the codec and results in impractical decoding runtimes. We
propose a new learning-based, disparity-aided model for compression of 4D light
field images capable of parallel decoding. The model is end-to-end trainable,
eliminating the need for hand-tuning separate modules and allowing joint
learning of rate and distortion. The disparity-aided approach ensures the
structural integrity of the reconstructed light fields. Comparisons with the
state of the art show encouraging performance in terms of PSNR and MS-SSIM
metrics. Also, there is a notable gain in the encoding and decoding runtimes.
Source code is available at https://moha23.github.io/LFDAAE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Singh_M/0/1/0/all/0/1"&gt;Mohana Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rameshan_R/0/1/0/all/0/1"&gt;Renu M. Rameshan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Universal Domain Adaptation in Ordinal Regression. (arXiv:2106.11576v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11576</id>
        <link href="http://arxiv.org/abs/2106.11576"/>
        <updated>2021-06-23T01:48:39.087Z</updated>
        <summary type="html"><![CDATA[We address the problem of universal domain adaptation (UDA) in ordinal
regression (OR), which attempts to solve classification problems in which
labels are not independent, but follow a natural order. We show that the UDA
techniques developed for classification and based on the clustering assumption,
under-perform in OR settings. We propose a method that complements the OR
classifier with an auxiliary task of order learning, which plays the double
role of discriminating between common and private instances, and expanding
class labels to the private target images via ranking. Combined with
adversarial domain discrimination, our model is able to address the closed set,
partial and open set configurations. We evaluate our method on three face age
estimation datasets, and show that it outperforms the baseline methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boris_C/0/1/0/all/0/1"&gt;Chidlovskii Boris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadek_A/0/1/0/all/0/1"&gt;Assem Sadek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_C/0/1/0/all/0/1"&gt;Christian Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hand-Drawn Electrical Circuit Recognition using Object Detection and Node Recognition. (arXiv:2106.11559v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11559</id>
        <link href="http://arxiv.org/abs/2106.11559"/>
        <updated>2021-06-23T01:48:39.079Z</updated>
        <summary type="html"><![CDATA[With the recent developments in neural networks, there has been a resurgence
in algorithms for the automatic generation of simulation ready electronic
circuits from hand-drawn circuits. However, most of the approaches in
literature were confined to classify different types of electrical components
and only a few of those methods have shown a way to rebuild the circuit
schematic from the scanned image, which is extremely important for further
automation of netlist generation. This paper proposes a real-time algorithm for
the automatic recognition of hand-drawn electrical circuits based on object
detection and circuit node recognition. The proposed approach employs You Only
Look Once version 5 (YOLOv5) for detection of circuit components and a novel
Hough transform based approach for node recognition. Using YOLOv5 object
detection algorithm, a mean average precision (mAP0.5) of 98.2% is achieved in
detecting the components. The proposed method is also able to rebuild the
circuit schematic with 80% accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_R/0/1/0/all/0/1"&gt;Rachala Rohith Reddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panicker_M/0/1/0/all/0/1"&gt;Mahesh Raveendranatha Panicker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-layered Semantic Representation Network for Multi-label Image Classification. (arXiv:2106.11596v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11596</id>
        <link href="http://arxiv.org/abs/2106.11596"/>
        <updated>2021-06-23T01:48:39.072Z</updated>
        <summary type="html"><![CDATA[Multi-label image classification (MLIC) is a fundamental and practical task,
which aims to assign multiple possible labels to an image. In recent years,
many deep convolutional neural network (CNN) based approaches have been
proposed which model label correlations to discover semantics of labels and
learn semantic representations of images. This paper advances this research
direction by improving both the modeling of label correlations and the learning
of semantic representations. On the one hand, besides the local semantics of
each label, we propose to further explore global semantics shared by multiple
labels. On the other hand, existing approaches mainly learn the semantic
representations at the last convolutional layer of a CNN. But it has been noted
that the image representations of different layers of CNN capture different
levels or scales of features and have different discriminative abilities. We
thus propose to learn semantic representations at multiple convolutional
layers. To this end, this paper designs a Multi-layered Semantic Representation
Network (MSRN) which discovers both local and global semantics of labels
through modeling label correlations and utilizes the label semantics to guide
the semantic representations learning at multiple layers through an attention
mechanism. Extensive experiments on four benchmark datasets including VOC 2007,
COCO, NUS-WIDE, and Apparel show a competitive performance of the proposed MSRN
against state-of-the-art models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1"&gt;Xiwen Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Che_H/0/1/0/all/0/1"&gt;Hao Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Linchuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1"&gt;Xiao Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatial-Temporal Super-Resolution of Satellite Imagery via Conditional Pixel Synthesis. (arXiv:2106.11485v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11485</id>
        <link href="http://arxiv.org/abs/2106.11485"/>
        <updated>2021-06-23T01:48:39.060Z</updated>
        <summary type="html"><![CDATA[High-resolution satellite imagery has proven useful for a broad range of
tasks, including measurement of global human population, local economic
livelihoods, and biodiversity, among many others. Unfortunately,
high-resolution imagery is both infrequently collected and expensive to
purchase, making it hard to efficiently and effectively scale these downstream
tasks over both time and space. We propose a new conditional pixel synthesis
model that uses abundant, low-cost, low-resolution imagery to generate accurate
high-resolution imagery at locations and times in which it is unavailable. We
show that our model attains photo-realistic sample quality and outperforms
competing baselines on a key downstream task -- object counting -- particularly
in geographic locations where conditions on the ground are changing rapidly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yutong He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dingjie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_N/0/1/0/all/0/1"&gt;Nicholas Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;William Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1"&gt;Chenlin Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burke_M/0/1/0/all/0/1"&gt;Marshall Burke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lobell_D/0/1/0/all/0/1"&gt;David B. Lobell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1"&gt;Stefano Ermon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recent Deep Semi-supervised Learning Approaches and Related Works. (arXiv:2106.11528v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11528</id>
        <link href="http://arxiv.org/abs/2106.11528"/>
        <updated>2021-06-23T01:48:39.053Z</updated>
        <summary type="html"><![CDATA[The author of this work proposes an overview of the recent semi-supervised
learning approaches and related works. Despite the remarkable success of neural
networks in various applications, there exist few formidable constraints
including the need for a large amount of labeled data. Therefore,
semi-supervised learning, which is a learning scheme in which the scarce labels
and a larger amount of unlabeled data are utilized to train models (e.g., deep
neural networks) is getting more important. Based on the key assumptions of
semi-supervised learning, which are the manifold assumption, cluster
assumption, and continuity assumption, the work reviews the recent
semi-supervised learning approaches. In particular, the methods in regard to
using deep neural networks in a semi-supervised learning setting are primarily
discussed. In addition, the existing works are first classified based on the
underlying idea and explained, and then the holistic approaches that unify the
aforementioned ideas are detailed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1"&gt;Gyeongho Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incremental Deep Neural Network Learning using Classification Confidence Thresholding. (arXiv:2106.11437v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11437</id>
        <link href="http://arxiv.org/abs/2106.11437"/>
        <updated>2021-06-23T01:48:39.046Z</updated>
        <summary type="html"><![CDATA[Most modern neural networks for classification fail to take into account the
concept of the unknown. Trained neural networks are usually tested in an
unrealistic scenario with only examples from a closed set of known classes. In
an attempt to develop a more realistic model, the concept of working in an open
set environment has been introduced. This in turn leads to the concept of
incremental learning where a model with its own architecture and initial
trained set of data can identify unknown classes during the testing phase and
autonomously update itself if evidence of a new class is detected. Some
problems that arise in incremental learning are inefficient use of resources to
retrain the classifier repeatedly and the decrease of classification accuracy
as multiple classes are added over time. This process of instantiating new
classes is repeated as many times as necessary, accruing errors. To address
these problems, this paper proposes the Classification Confidence Threshold
approach to prime neural networks for incremental learning to keep accuracies
high by limiting forgetting. A lean method is also used to reduce resources
used in the retraining of the neural network. The proposed method is based on
the idea that a network is able to incrementally learn a new class even when
exposed to a limited number samples associated with the new class. This method
can be applied to most existing neural networks with minimal changes to network
architecture.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leo_J/0/1/0/all/0/1"&gt;Justin Leo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1"&gt;Jugal Kalita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel Clustering with Sigmoid-based Regularization for Efficient Segmentation of Sequential Data. (arXiv:2106.11541v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11541</id>
        <link href="http://arxiv.org/abs/2106.11541"/>
        <updated>2021-06-23T01:48:39.039Z</updated>
        <summary type="html"><![CDATA[Kernel segmentation aims at partitioning a data sequence into several
non-overlapping segments that may have nonlinear and complex structures. In
general, it is formulated as a discrete optimization problem with combinatorial
constraints. A popular algorithm for optimally solving this problem is dynamic
programming (DP), which has quadratic computation and memory requirements.
Given that sequences in practice are too long, this algorithm is not a
practical approach. Although many heuristic algorithms have been proposed to
approximate the optimal segmentation, they have no guarantee on the quality of
their solutions. In this paper, we take a differentiable approach to alleviate
the aforementioned issues. First, we introduce a novel sigmoid-based
regularization to smoothly approximate the combinatorial constraints. Combining
it with objective of the balanced kernel clustering, we formulate a
differentiable model termed Kernel clustering with sigmoid-based regularization
(KCSR), where the gradient-based algorithm can be exploited to obtain the
optimal segmentation. Second, we develop a stochastic variant of the proposed
model. By using the stochastic gradient descent algorithm, which has much lower
time and space complexities, for optimization, the second model can perform
segmentation on overlong data sequences. Finally, for simultaneously segmenting
multiple data sequences, we slightly modify the sigmoid-based regularization to
further introduce an extended variant of the proposed model. Through extensive
experiments on various types of data sequences performances of our models are
evaluated and compared with those of the existing methods. The experimental
results validate advantages of the proposed models. Our Matlab source code is
available on github.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Doan_T/0/1/0/all/0/1"&gt;Tung Doan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takasu_A/0/1/0/all/0/1"&gt;Atsuhiro Takasu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Architecture Search Without Training Nor Labels: A Pruning Perspective. (arXiv:2106.11542v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11542</id>
        <link href="http://arxiv.org/abs/2106.11542"/>
        <updated>2021-06-23T01:48:39.016Z</updated>
        <summary type="html"><![CDATA[With leveraging the weight-sharing and continuous relaxation to enable
gradient-descent to alternately optimize the supernet weights and the
architecture parameters through a bi-level optimization paradigm,
\textit{Differentiable ARchiTecture Search} (DARTS) has become the mainstream
method in Neural Architecture Search (NAS) due to its simplicity and
efficiency. However, more recent works found that the performance of the
searched architecture barely increases with the optimization proceeding in
DARTS. In addition, several concurrent works show that the NAS could find more
competitive architectures without labels. The above observations reveal that
the supervision signal in DARTS may be a poor indicator for architecture
optimization, inspiring a foundational question: instead of using the
supervision signal to perform bi-level optimization, \textit{can we find
high-quality architectures \textbf{without any training nor labels}}? We
provide an affirmative answer by customizing the NAS as a network pruning at
initialization problem. By leveraging recent techniques on the network pruning
at initialization, we designed a FreeFlow proxy to score the importance of
candidate operations in NAS without any training nor labels, and proposed a
novel framework called \textit{training and label free neural architecture
search} (\textbf{FreeNAS}) accordingly. We show that, without any training nor
labels, FreeNAS with the proposed FreeFlow proxy can outperform most NAS
baselines. More importantly, our framework is extremely efficient, which
completes the architecture search within only \textbf{3.6s} and \textbf{79s} on
a single GPU for the NAS-Bench-201 and DARTS search space, respectively. We
hope our work inspires more attempts in solving NAS from the perspective of
pruning at initialization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Miao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1"&gt;Steven Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Shirui Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1"&gt;Xiaojun Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1"&gt;Gholamreza Haffari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SSUL: Semantic Segmentation with Unknown Label for Exemplar-based Class-Incremental Learning. (arXiv:2106.11562v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11562</id>
        <link href="http://arxiv.org/abs/2106.11562"/>
        <updated>2021-06-23T01:48:38.963Z</updated>
        <summary type="html"><![CDATA[We consider a class-incremental semantic segmentation (CISS) problem. While
some recently proposed algorithms utilized variants of knowledge distillation
(KD) technique to tackle the problem, they only partially addressed the key
additional challenges in CISS that causes the catastrophic forgetting; i.e.,
the semantic drift of the background class and multi-label prediction issue. To
better address these challenges, we propose a new method, dubbed as SSUL-M
(Semantic Segmentation with Unknown Label with Memory), by carefully combining
several techniques tailored for semantic segmentation. More specifically, we
make three main contributions; (1) modeling unknown class within the background
class to help learning future classes (help plasticity), (2) freezing backbone
network and past classifiers with binary cross-entropy loss and pseudo-labeling
to overcome catastrophic forgetting (help stability), and (3) utilizing tiny
exemplar memory for the first time in CISS to improve both plasticity and
stability. As a result, we show our method achieves significantly better
performance than the recent state-of-the-art baselines on the standard
benchmark datasets. Furthermore, we justify our contributions with thorough and
extensive ablation analyses and discuss different natures of the CISS problem
compared to the standard class-incremental learning for classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sungmin Cha. Beomyoung Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1"&gt;Youngjoon Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1"&gt;Taesup Moon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Creating A New Color Space utilizing PSO and FCM to Perform Skin Detection by using Neural Network and ANFIS. (arXiv:2106.11563v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11563</id>
        <link href="http://arxiv.org/abs/2106.11563"/>
        <updated>2021-06-23T01:48:38.946Z</updated>
        <summary type="html"><![CDATA[Skin color detection is an essential required step in various applications
related to computer vision. These applications will include face detection,
finding pornographic images in movies and photos, finding ethnicity, age,
diagnosis, and so on. Therefore, proposing a proper skin detection method can
provide solution to several problems. In this study, first a new color space is
created using FCM and PSO algorithms. Then, skin classification has been
performed in the new color space utilizing linear and nonlinear modes.
Additionally, it has been done in RGB and LAB color spaces by using ANFIS and
neural network. Skin detection in RBG color space has been performed using
Mahalanobis distance and Euclidean distance algorithms. In comparison, this
method has 18.38% higher accuracy than the most accurate method on the same
database. Additionally, this method has achieved 90.05% in equal error rate
(1-EER) in testing COMPAQ dataset and 92.93% accuracy in testing Pratheepan
dataset, which compared to the previous method on COMPAQ database, 1-EER has
increased by %0.87.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nazaria_K/0/1/0/all/0/1"&gt;Kobra Nazaria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazaheri_S/0/1/0/all/0/1"&gt;Samaneh Mazaheri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bigham_B/0/1/0/all/0/1"&gt;Bahram Sadeghi Bigham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DocFormer: End-to-End Transformer for Document Understanding. (arXiv:2106.11539v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11539</id>
        <link href="http://arxiv.org/abs/2106.11539"/>
        <updated>2021-06-23T01:48:38.940Z</updated>
        <summary type="html"><![CDATA[We present DocFormer -- a multi-modal transformer based architecture for the
task of Visual Document Understanding (VDU). VDU is a challenging problem which
aims to understand documents in their varied formats (forms, receipts etc.) and
layouts. In addition, DocFormer is pre-trained in an unsupervised fashion using
carefully designed tasks which encourage multi-modal interaction. DocFormer
uses text, vision and spatial features and combines them using a novel
multi-modal self-attention layer. DocFormer also shares learned spatial
embeddings across modalities which makes it easy for the model to correlate
text to visual tokens and vice versa. DocFormer is evaluated on 4 different
datasets each with strong baselines. DocFormer achieves state-of-the-art
results on all of them, sometimes beating models 4x its size (in no. of
parameters).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Appalaraju_S/0/1/0/all/0/1"&gt;Srikar Appalaraju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jasani_B/0/1/0/all/0/1"&gt;Bhavan Jasani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kota_B/0/1/0/all/0/1"&gt;Bhargava Urala Kota&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yusheng Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manmatha_R/0/1/0/all/0/1"&gt;R. Manmatha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gait analysis with curvature maps: A simulation study. (arXiv:2106.11466v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11466</id>
        <link href="http://arxiv.org/abs/2106.11466"/>
        <updated>2021-06-23T01:48:38.933Z</updated>
        <summary type="html"><![CDATA[Gait analysis is an important aspect of clinical investigation for detecting
neurological and musculoskeletal disorders and assessing the global health of a
patient. In this paper we propose to focus our attention on extracting relevant
curvature information from the body surface provided by a depth camera. We
assumed that the 3D mesh was made available in a previous step and demonstrated
how curvature maps could be useful to assess asymmetric anomalies with two
simple simulated abnormal gaits compared with a normal one. This research set
the grounds for the future development of a curvature-based gait analysis
system for healthcare professionals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1"&gt;Khac Chinh Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daniel_M/0/1/0/all/0/1"&gt;Marc Daniel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meunier_J/0/1/0/all/0/1"&gt;Jean Meunier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal trajectory forecasting based on discrete heat map. (arXiv:2106.11467v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11467</id>
        <link href="http://arxiv.org/abs/2106.11467"/>
        <updated>2021-06-23T01:48:38.925Z</updated>
        <summary type="html"><![CDATA[In Argoverse motion forecasting competition, the task is to predict the
probabilistic future trajectory distribution for the interested targets in the
traffic scene. We use vectorized lane map and 2 s targets' history trajectories
as input. Then the model outputs 6 forecasted trajectories with probability for
each target.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Jingni Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jianyun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yushi Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Break-It-Fix-It: Unsupervised Learning for Program Repair. (arXiv:2106.06600v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06600</id>
        <link href="http://arxiv.org/abs/2106.06600"/>
        <updated>2021-06-23T01:48:38.902Z</updated>
        <summary type="html"><![CDATA[We consider repair tasks: given a critic (e.g., compiler) that assesses the
quality of an input, the goal is to train a fixer that converts a bad example
(e.g., code with syntax errors) into a good one (e.g., code with no syntax
errors). Existing works create training data consisting of (bad, good) pairs by
corrupting good examples using heuristics (e.g., dropping tokens). However,
fixers trained on this synthetically-generated data do not extrapolate well to
the real distribution of bad inputs. To bridge this gap, we propose a new
training approach, Break-It-Fix-It (BIFI), which has two key ideas: (i) we use
the critic to check a fixer's output on real bad inputs and add good (fixed)
outputs to the training data, and (ii) we train a breaker to generate realistic
bad code from good code. Based on these ideas, we iteratively update the
breaker and the fixer while using them in conjunction to generate more paired
data. We evaluate BIFI on two code repair datasets: GitHub-Python, a new
dataset we introduce where the goal is to repair Python code with AST parse
errors; and DeepFix, where the goal is to repair C code with compiler errors.
BIFI outperforms existing methods, obtaining 90.5% repair accuracy on
GitHub-Python (+28.5%) and 71.7% on DeepFix (+5.6%). Notably, BIFI does not
require any labeled data; we hope it will be a strong starting point for
unsupervised learning of various repair tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1"&gt;Michihiro Yasunaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Percy Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatio-Temporal Multi-Task Learning Transformer for Joint Moving Object Detection and Segmentation. (arXiv:2106.11401v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11401</id>
        <link href="http://arxiv.org/abs/2106.11401"/>
        <updated>2021-06-23T01:48:38.895Z</updated>
        <summary type="html"><![CDATA[Moving objects have special importance for Autonomous Driving tasks.
Detecting moving objects can be posed as Moving Object Segmentation, by
segmenting the object pixels, or Moving Object Detection, by generating a
bounding box for the moving targets. In this paper, we present a Multi-Task
Learning architecture, based on Transformers, to jointly perform both tasks
through one network. Due to the importance of the motion features to the task,
the whole setup is based on a Spatio-Temporal aggregation. We evaluate the
performance of the individual tasks architecture versus the MTL setup, both
with early shared encoders, and late shared encoder-decoder transformers. For
the latter, we present a novel joint tasks query decoder transformer, that
enables us to have tasks dedicated heads out of the shared model. To evaluate
our approach, we use the KITTI MOD [29] data set. Results show1.5% mAP
improvement for Moving Object Detection, and 2%IoU improvement for Moving
Object Segmentation, over the individual tasks networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1"&gt;Eslam Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1"&gt;Ahmed El-Sallab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FDeblur-GAN: Fingerprint Deblurring using Generative Adversarial Network. (arXiv:2106.11354v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11354</id>
        <link href="http://arxiv.org/abs/2106.11354"/>
        <updated>2021-06-23T01:48:38.887Z</updated>
        <summary type="html"><![CDATA[While working with fingerprint images acquired from crime scenes, mobile
cameras, or low-quality sensors, it becomes difficult for automated
identification systems to verify the identity due to image blur and distortion.
We propose a fingerprint deblurring model FDeblur-GAN, based on the conditional
Generative Adversarial Networks (cGANs) and multi-stage framework of the stack
GAN. Additionally, we integrate two auxiliary sub-networks into the model for
the deblurring task. The first sub-network is a ridge extractor model. It is
added to generate ridge maps to ensure that fingerprint information and
minutiae are preserved in the deblurring process and prevent the model from
generating erroneous minutiae. The second sub-network is a verifier that helps
the generator to preserve the ID information during the generation process.
Using a database of blurred fingerprints and corresponding ridge maps, the deep
network learns to deblur from the input blurry samples. We evaluate the
proposed method in combination with two different fingerprint matching
algorithms. We achieved an accuracy of 95.18% on our fingerprint database for
the task of matching deblurred and ground truth fingerprints.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1"&gt;Amol S. Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dabouei_A/0/1/0/all/0/1"&gt;Ali Dabouei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1"&gt;Jeremy Dawson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1"&gt;Nasser M. Nasrabadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mapping Slums with Medium Resolution Satellite Imagery: a Comparative Analysis of Multi-Spectral Data and Grey-level Co-occurrence Matrix Techniques. (arXiv:2106.11395v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11395</id>
        <link href="http://arxiv.org/abs/2106.11395"/>
        <updated>2021-06-23T01:48:38.880Z</updated>
        <summary type="html"><![CDATA[The UN-Habitat estimates that over one billion people live in slums around
the world. However, state-of-the-art techniques to detect the location of slum
areas employ high-resolution satellite imagery, which is costly to obtain and
process. As a result, researchers have started to look at utilising free and
open-access medium resolution satellite imagery. Yet, there is no clear
consensus on which data preparation and machine learning approaches are the
most appropriate to use with such imagery data. In this paper, we evaluate two
techniques (multi-spectral data and grey-level co-occurrence matrix feature
extraction) on an open-access dataset consisting of labelled Sentinel-2 images
with a spatial resolution of 10 meters. Both techniques were paired with a
canonical correlation forests classifier. The results show that the grey-level
co-occurrence matrix performed better than multi-spectral data for all four
cities. It had an average accuracy for the slum class of 97% and a mean
intersection over union of 94%, while multi-spectral data had 75% and 64% for
the respective metrics. These results indicate that open-access satellite
imagery with a resolution of at least 10 meters may be suitable for keeping
track of development goals such as the detection of slums in cities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mattos_A/0/1/0/all/0/1"&gt;Agatha C. H. de Mattos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McArdle_G/0/1/0/all/0/1"&gt;Gavin McArdle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bertolotto_M/0/1/0/all/0/1"&gt;Michela Bertolotto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Embedding Adaptation via Early-Stage Feature Reconstruction for Few-Shot Classification. (arXiv:2106.11486v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11486</id>
        <link href="http://arxiv.org/abs/2106.11486"/>
        <updated>2021-06-23T01:48:38.872Z</updated>
        <summary type="html"><![CDATA[We propose unsupervised embedding adaptation for the downstream few-shot
classification task. Based on findings that deep neural networks learn to
generalize before memorizing, we develop Early-Stage Feature Reconstruction
(ESFR) -- a novel adaptation scheme with feature reconstruction and
dimensionality-driven early stopping that finds generalizable features.
Incorporating ESFR consistently improves the performance of baseline methods on
all standard settings, including the recently proposed transductive method.
ESFR used in conjunction with the transductive method further achieves
state-of-the-art performance on mini-ImageNet, tiered-ImageNet, and CUB;
especially with 1.2%~2.0% improvements in accuracy over the previous best
performing method on 1-shot setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dong Hoon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1"&gt;Sae-Young Chung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[f-Domain-Adversarial Learning: Theory and Algorithms. (arXiv:2106.11344v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11344</id>
        <link href="http://arxiv.org/abs/2106.11344"/>
        <updated>2021-06-23T01:48:38.801Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation is used in many machine learning applications
where, during training, a model has access to unlabeled data in the target
domain, and a related labeled dataset. In this paper, we introduce a novel and
general domain-adversarial framework. Specifically, we derive a novel
generalization bound for domain adaptation that exploits a new measure of
discrepancy between distributions based on a variational characterization of
f-divergences. It recovers the theoretical results from Ben-David et al.
(2010a) as a special case and supports divergences used in practice. Based on
this bound, we derive a new algorithmic framework that introduces a key
correction in the original adversarial training method of Ganin et al. (2016).
We show that many regularizers and ad-hoc objectives introduced over the last
years in this framework are then not required to achieve performance comparable
to (if not better than) state-of-the-art domain-adversarial methods.
Experimental analysis conducted on real-world natural language and computer
vision datasets show that our framework outperforms existing baselines, and
obtains the best results for f-divergences that were not considered previously
in domain-adversarial learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Acuna_D/0/1/0/all/0/1"&gt;David Acuna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guojun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1"&gt;Marc T. Law&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1"&gt;Sanja Fidler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dive into Deep Learning. (arXiv:2106.11342v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11342</id>
        <link href="http://arxiv.org/abs/2106.11342"/>
        <updated>2021-06-23T01:48:38.772Z</updated>
        <summary type="html"><![CDATA[This open-source book represents our attempt to make deep learning
approachable, teaching readers the concepts, the context, and the code. The
entire book is drafted in Jupyter notebooks, seamlessly integrating exposition
figures, math, and interactive examples with self-contained code. Our goal is
to offer a resource that could (i) be freely available for everyone; (ii) offer
sufficient technical depth to provide a starting point on the path to actually
becoming an applied machine learning scientist; (iii) include runnable code,
showing readers how to solve problems in practice; (iv) allow for rapid
updates, both by us and also by the community at large; (v) be complemented by
a forum for interactive discussion of technical details and to answer
questions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Aston Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1"&gt;Zachary C. Lipton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1"&gt;Alexander J. Smola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models. (arXiv:2106.06087v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06087</id>
        <link href="http://arxiv.org/abs/2106.06087"/>
        <updated>2021-06-23T01:48:38.764Z</updated>
        <summary type="html"><![CDATA[Targeted syntactic evaluations have demonstrated the ability of language
models to perform subject-verb agreement given difficult contexts. To elucidate
the mechanisms by which the models accomplish this behavior, this study applies
causal mediation analysis to pre-trained neural language models. We investigate
the magnitude of models' preferences for grammatical inflections, as well as
whether neurons process subject-verb agreement similarly across sentences with
different syntactic structures. We uncover similarities and differences across
architectures and model sizes -- notably, that larger models do not necessarily
learn stronger preferences. We also observe two distinct mechanisms for
producing subject-verb agreement depending on the syntactic structure of the
input sentence. Finally, we find that language models rely on similar sets of
neurons when given sentences with similar syntactic structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Finlayson_M/0/1/0/all/0/1"&gt;Matthew Finlayson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mueller_A/0/1/0/all/0/1"&gt;Aaron Mueller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1"&gt;Sebastian Gehrmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shieber_S/0/1/0/all/0/1"&gt;Stuart Shieber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1"&gt;Tal Linzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1"&gt;Yonatan Belinkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding top-down attention using task-oriented ablation design. (arXiv:2106.11339v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11339</id>
        <link href="http://arxiv.org/abs/2106.11339"/>
        <updated>2021-06-23T01:48:38.755Z</updated>
        <summary type="html"><![CDATA[Top-down attention allows neural networks, both artificial and biological, to
focus on the information most relevant for a given task. This is known to
enhance performance in visual perception. But it remains unclear how attention
brings about its perceptual boost, especially when it comes to naturalistic
settings like recognising an object in an everyday scene. What aspects of a
visual task does attention help to deal with? We aim to answer this with a
computational experiment based on a general framework called task-oriented
ablation design. First we define a broad range of visual tasks and identify six
factors that underlie task variability. Then on each task we compare the
performance of two neural networks, one with top-down attention and one
without. These comparisons reveal the task-dependence of attention's perceptual
boost, giving a clearer idea of the role attention plays. Whereas many existing
cognitive accounts link attention to stimulus-level variables, such as visual
clutter and object scale, we find greater explanatory power in system-level
variables that capture the interaction between the model, the distribution of
training data and the task format. This finding suggests a shift in how
attention is studied could be fruitful. We make publicly available our code and
results, along with statistics relevant to ImageNet-based experiments beyond
this one. Our contribution serves to support the development of more human-like
vision models and the design of more informative machine-learning experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smith_F/0/1/0/all/0/1"&gt;Freddie Bickford Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roads_B/0/1/0/all/0/1"&gt;Brett D Roads&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiaoliang Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Love_B/0/1/0/all/0/1"&gt;Bradley C Love&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ProphetNet-X: Large-Scale Pre-training Models for English, Chinese, Multi-lingual, Dialog, and Code Generation. (arXiv:2104.08006v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08006</id>
        <link href="http://arxiv.org/abs/2104.08006"/>
        <updated>2021-06-23T01:48:38.731Z</updated>
        <summary type="html"><![CDATA[Now, the pre-training technique is ubiquitous in natural language processing
field. ProphetNet is a pre-training based natural language generation method
which shows powerful performance on English text summarization and question
generation tasks. In this paper, we extend ProphetNet into other domains and
languages, and present the ProphetNet family pre-training models, named
ProphetNet-X, where X can be English, Chinese, Multi-lingual, and so on. We
pre-train a cross-lingual generation model ProphetNet-Multi, a Chinese
generation model ProphetNet-Zh, two open-domain dialog generation models
ProphetNet-Dialog-En and ProphetNet-Dialog-Zh. And also, we provide a PLG
(Programming Language Generation) model ProphetNet-Code to show the generation
performance besides NLG (Natural Language Generation) tasks. In our
experiments, ProphetNet-X models achieve new state-of-the-art performance on 10
benchmarks. All the models of ProphetNet-X share the same model structure,
which allows users to easily switch between different models. We make the code
and models publicly available, and we will keep updating more pre-training
models and finetuning scripts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_W/0/1/0/all/0/1"&gt;Weizhen Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1"&gt;Yeyun Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Can Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1"&gt;Bolun Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bartuer Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1"&gt;Biao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1"&gt;Daxin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiusheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruofei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Houqiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1"&gt;Nan Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SeqNetVLAD vs PointNetVLAD: Image Sequence vs 3D Point Clouds for Day-Night Place Recognition. (arXiv:2106.11481v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11481</id>
        <link href="http://arxiv.org/abs/2106.11481"/>
        <updated>2021-06-23T01:48:38.724Z</updated>
        <summary type="html"><![CDATA[Place Recognition is a crucial capability for mobile robot localization and
navigation. Image-based or Visual Place Recognition (VPR) is a challenging
problem as scene appearance and camera viewpoint can change significantly when
places are revisited. Recent VPR methods based on ``sequential
representations'' have shown promising results as compared to traditional
sequence score aggregation or single image based techniques. In parallel to
these endeavors, 3D point clouds based place recognition is also being explored
following the advances in deep learning based point cloud processing. However,
a key question remains: is an explicit 3D structure based place representation
always superior to an implicit ``spatial'' representation based on sequence of
RGB images which can inherently learn scene structure. In this extended
abstract, we attempt to compare these two types of methods by considering a
similar ``metric span'' to represent places. We compare a 3D point cloud based
method (PointNetVLAD) with image sequence based methods (SeqNet and others) and
showcase that image sequence based techniques approach, and can even surpass,
the performance achieved by point cloud based methods for a given metric span.
These performance variations can be attributed to differences in data richness
of input sensors as well as data accumulation strategies for a mobile robot.
While a perfect apple-to-apple comparison may not be feasible for these two
different modalities, the presented comparison takes a step in the direction of
answering deeper questions regarding spatial representations, relevant to
several applications like Autonomous Driving and Augmented/Virtual Reality.
Source code available publicly https://github.com/oravus/seqNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1"&gt;Sourav Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1"&gt;Michael Milford&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilingual Speech Translation with Unified Transformer: Huawei Noah's Ark Lab at IWSLT 2021. (arXiv:2106.00197v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00197</id>
        <link href="http://arxiv.org/abs/2106.00197"/>
        <updated>2021-06-23T01:48:38.717Z</updated>
        <summary type="html"><![CDATA[This paper describes the system submitted to the IWSLT 2021 Multilingual
Speech Translation (MultiST) task from Huawei Noah's Ark Lab. We use a unified
transformer architecture for our MultiST model, so that the data from different
modalities (i.e., speech and text) and different tasks (i.e., Speech
Recognition, Machine Translation, and Speech Translation) can be exploited to
enhance the model's ability. Specifically, speech and text inputs are firstly
fed to different feature extractors to extract acoustic and textual features,
respectively. Then, these features are processed by a shared encoder--decoder
architecture. We apply several training techniques to improve the performance,
including multi-task learning, task-level curriculum learning, data
augmentation, etc. Our final system achieves significantly better results than
bilingual baselines on supervised language pairs and yields reasonable results
on zero-shot language pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xingshan Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Liangyou Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qun Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Encoder-Decoder Architectures for Clinically Relevant Coronary Artery Segmentation. (arXiv:2106.11447v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.11447</id>
        <link href="http://arxiv.org/abs/2106.11447"/>
        <updated>2021-06-23T01:48:38.709Z</updated>
        <summary type="html"><![CDATA[Coronary X-ray angiography is a crucial clinical procedure for the diagnosis
and treatment of coronary artery disease, which accounts for roughly 16% of
global deaths every year. However, the images acquired in these procedures have
low resolution and poor contrast, making lesion detection and assessment
challenging. Accurate coronary artery segmentation not only helps mitigate
these problems, but also allows the extraction of relevant anatomical features
for further analysis by quantitative methods. Although automated segmentation
of coronary arteries has been proposed before, previous approaches have used
non-optimal segmentation criteria, leading to less useful results. Most methods
either segment only the major vessel, discarding important information from the
remaining ones, or segment the whole coronary tree based mostly on contrast
information, producing a noisy output that includes vessels that are not
relevant for diagnosis. We adopt a better-suited clinical criterion and segment
vessels according to their clinical relevance. Additionally, we simultaneously
perform catheter segmentation, which may be useful for diagnosis due to the
scale factor provided by the catheter's known diameter, and is a task that has
not yet been performed with good results. To derive the optimal approach, we
conducted an extensive comparative study of encoder-decoder architectures
trained on a combination of focal loss and a variant of generalized dice loss.
Based on the EfficientNet and the UNet++ architectures, we propose a line of
efficient and high-performance segmentation models using a new decoder
architecture, the EfficientUNet++, whose best-performing version achieved
average dice scores of 0.8904 and 0.7526 for the artery and catheter classes,
respectively, and an average generalized dice score of 0.9234.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Silva_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Louren&amp;#xe7;o Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Menezes_M/0/1/0/all/0/1"&gt;Miguel Nobre Menezes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rodrigues_T/0/1/0/all/0/1"&gt;Tiago Rodrigues&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Silva_B/0/1/0/all/0/1"&gt;Beatriz Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pinto_F/0/1/0/all/0/1"&gt;Fausto J. Pinto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oliveira_A/0/1/0/all/0/1"&gt;Arlindo L. Oliveira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MODETR: Moving Object Detection with Transformers. (arXiv:2106.11422v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11422</id>
        <link href="http://arxiv.org/abs/2106.11422"/>
        <updated>2021-06-23T01:48:38.699Z</updated>
        <summary type="html"><![CDATA[Moving Object Detection (MOD) is a crucial task for the Autonomous Driving
pipeline. MOD is usually handled via 2-stream convolutional architectures that
incorporates both appearance and motion cues, without considering the
inter-relations between the spatial or motion features. In this paper, we
tackle this problem through multi-head attention mechanisms, both across the
spatial and motion streams. We propose MODETR; a Moving Object DEtection
TRansformer network, comprised of multi-stream transformer encoders for both
spatial and motion modalities, and an object transformer decoder that produces
the moving objects bounding boxes using set predictions. The whole architecture
is trained end-to-end using bi-partite loss. Several methods of incorporating
motion cues with the Transformer model are explored, including two-stream RGB
and Optical Flow (OF) methods, and multi-stream architectures that take
advantage of sequence information. To incorporate the temporal information, we
propose a new Temporal Positional Encoding (TPE) approach to extend the Spatial
Positional Encoding(SPE) in DETR. We explore two architectural choices for
that, balancing between speed and time. To evaluate the our network, we perform
the MOD task on the KITTI MOD [6] data set. Results show significant 5% mAP of
the Transformer network for MOD over the state-of-the art methods. Moreover,
the proposed TPE encoding provides 10% mAP improvement over the SPE baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1"&gt;Eslam Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1"&gt;Ahmad El-Sallab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Winning the CVPR'2021 Kinetics-GEBD Challenge: Contrastive Learning Approach. (arXiv:2106.11549v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11549</id>
        <link href="http://arxiv.org/abs/2106.11549"/>
        <updated>2021-06-23T01:48:38.671Z</updated>
        <summary type="html"><![CDATA[Generic Event Boundary Detection (GEBD) is a newly introduced task that aims
to detect "general" event boundaries that correspond to natural human
perception. In this paper, we introduce a novel contrastive learning based
approach to deal with the GEBD. Our intuition is that the feature similarity of
the video snippet would significantly vary near the event boundaries, while
remaining relatively the same in the remaining part of the video. In our model,
Temporal Self-similarity Matrix (TSM) is utilized as an intermediate
representation which takes on a role as an information bottleneck. With our
model, we achieved significant performance boost compared to the given
baselines. Our code is available at
https://github.com/hello-jinwoo/LOVEU-CVPR2021.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1"&gt;Hyolim Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jinwoo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kyungmin Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1"&gt;Taehyun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seon Joo Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BEyond observation: an approach for ObjectNav. (arXiv:2106.11379v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11379</id>
        <link href="http://arxiv.org/abs/2106.11379"/>
        <updated>2021-06-23T01:48:38.662Z</updated>
        <summary type="html"><![CDATA[With the rise of automation, unmanned vehicles became a hot topic both as
commercial products and as a scientific research topic. It composes a
multi-disciplinary field of robotics that encompasses embedded systems, control
theory, path planning, Simultaneous Localization and Mapping (SLAM), scene
reconstruction, and pattern recognition. In this work, we present our
exploratory research of how sensor data fusion and state-of-the-art machine
learning algorithms can perform the Embodied Artificial Intelligence (E-AI)
task called Visual Semantic Navigation. This task, a.k.a Object-Goal Navigation
(ObjectNav) consists of autonomous navigation using egocentric visual
observations to reach an object belonging to the target semantic class without
prior knowledge of the environment. Our method reached fourth place on the
Habitat Challenge 2021 ObjectNav on the Minival phase and the Test-Standard
Phase.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruiz_D/0/1/0/all/0/1"&gt;Daniel V. Ruiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Todt_E/0/1/0/all/0/1"&gt;Eduardo Todt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Photozilla: A Large-Scale Photography Dataset and Visual Embedding for 20 Photography Styles. (arXiv:2106.11359v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11359</id>
        <link href="http://arxiv.org/abs/2106.11359"/>
        <updated>2021-06-23T01:48:38.651Z</updated>
        <summary type="html"><![CDATA[The advent of social media platforms has been a catalyst for the development
of digital photography that engendered a boom in vision applications. With this
motivation, we introduce a large-scale dataset termed 'Photozilla', which
includes over 990k images belonging to 10 different photographic styles. The
dataset is then used to train 3 classification models to automatically classify
the images into the relevant style which resulted in an accuracy of ~96%. With
the rapid evolution of digital photography, we have seen new types of
photography styles emerging at an exponential rate. On that account, we present
a novel Siamese-based network that uses the trained classification models as
the base architecture to adapt and classify unseen styles with only 25 training
samples. We report an accuracy of over 68% for identifying 10 other distinct
types of photography styles. This dataset can be found at
https://trisha025.github.io/Photozilla/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singhal_T/0/1/0/all/0/1"&gt;Trisha Singhal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Junhua Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blessing_L/0/1/0/all/0/1"&gt;Lucienne T. M. Blessing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1"&gt;Kwan Hui Lim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provable Limitations of Acquiring Meaning from Ungrounded Form: What Will Future Language Models Understand?. (arXiv:2104.10809v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10809</id>
        <link href="http://arxiv.org/abs/2104.10809"/>
        <updated>2021-06-23T01:48:38.643Z</updated>
        <summary type="html"><![CDATA[Language models trained on billions of tokens have recently led to
unprecedented results on many NLP tasks. This success raises the question of
whether, in principle, a system can ever ``understand'' raw text without access
to some form of grounding. We formally investigate the abilities of ungrounded
systems to acquire meaning. Our analysis focuses on the role of ``assertions'':
textual contexts that provide indirect clues about the underlying semantics. We
study whether assertions enable a system to emulate representations preserving
semantic relations like equivalence. We find that assertions enable semantic
emulation of languages that satisfy a strong notion of semantic transparency.
However, for classes of languages where the same expression can take different
values in different contexts, we show that emulation can become uncomputable.
Finally, we discuss differences between our formal model and natural language,
exploring how our results generalize to a modal setting and other semantic
relations. Together, our results suggest that assertions in code or language do
not provide sufficient signal to fully emulate semantic representations. We
formalize ways in which ungrounded language models appear to be fundamentally
limited in their ability to ``understand''.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1"&gt;William Merrill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1"&gt;Yoav Goldberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1"&gt;Roy Schwartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1"&gt;Noah A. Smith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CogAlign: Learning to Align Textual Neural Representations to Cognitive Language Processing Signals. (arXiv:2106.05544v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05544</id>
        <link href="http://arxiv.org/abs/2106.05544"/>
        <updated>2021-06-23T01:48:38.636Z</updated>
        <summary type="html"><![CDATA[Most previous studies integrate cognitive language processing signals (e.g.,
eye-tracking or EEG data) into neural models of natural language processing
(NLP) just by directly concatenating word embeddings with cognitive features,
ignoring the gap between the two modalities (i.e., textual vs. cognitive) and
noise in cognitive features. In this paper, we propose a CogAlign approach to
these issues, which learns to align textual neural representations to cognitive
features. In CogAlign, we use a shared encoder equipped with a modality
discriminator to alternatively encode textual and cognitive inputs to capture
their differences and commonalities. Additionally, a text-aware attention
mechanism is proposed to detect task-related information and to avoid using
noise in cognitive features. Experimental results on three NLP tasks, namely
named entity recognition, sentiment analysis and relation extraction, show that
CogAlign achieves significant improvements with multiple cognitive features
over state-of-the-art models on public datasets. Moreover, our model is able to
transfer cognitive information to other datasets that do not have any cognitive
processing signals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yuqi Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1"&gt;Deyi Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BiAdam: Fast Adaptive Bilevel Optimization Methods. (arXiv:2106.11396v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.11396</id>
        <link href="http://arxiv.org/abs/2106.11396"/>
        <updated>2021-06-23T01:48:38.628Z</updated>
        <summary type="html"><![CDATA[Bilevel optimization recently has attracted increased interest in machine
learning due to its many applications such as hyper-parameter optimization and
policy optimization. Although some methods recently have been proposed to solve
the bilevel problems, these methods do not consider using adaptive learning
rates. To fill this gap, in the paper, we propose a class of fast and effective
adaptive methods for solving bilevel optimization problems that the outer
problem is possibly nonconvex and the inner problem is strongly-convex.
Specifically, we propose a fast single-loop BiAdam algorithm based on the basic
momentum technique, which achieves a sample complexity of
$\tilde{O}(\epsilon^{-4})$ for finding an $\epsilon$-stationary point. At the
same time, we propose an accelerated version of BiAdam algorithm (VR-BiAdam) by
using variance reduced technique, which reaches the best known sample
complexity of $\tilde{O}(\epsilon^{-3})$. To further reduce computation in
estimating derivatives, we propose a fast single-loop stochastic approximated
BiAdam algorithm (saBiAdam) by avoiding the Hessian inverse, which still
achieves a sample complexity of $\tilde{O}(\epsilon^{-4})$ without large
batches. We further present an accelerated version of saBiAdam algorithm
(VR-saBiAdam), which also reaches the best known sample complexity of
$\tilde{O}(\epsilon^{-3})$. We apply the unified adaptive matrices to our
methods as the SUPER-ADAM \citep{huang2021super}, which including many types of
adaptive learning rates. Moreover, our framework can flexibly use the momentum
and variance reduced techniques. In particular, we provide a useful convergence
analysis framework for both the constrained and unconstrained bilevel
optimization. To the best of our knowledge, we first study the adaptive bilevel
optimization methods with adaptive learning rates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feihu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GAIA: A Transfer Learning System of Object Detection that Fits Your Needs. (arXiv:2106.11346v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11346</id>
        <link href="http://arxiv.org/abs/2106.11346"/>
        <updated>2021-06-23T01:48:38.606Z</updated>
        <summary type="html"><![CDATA[Transfer learning with pre-training on large-scale datasets has played an
increasingly significant role in computer vision and natural language
processing recently. However, as there exist numerous application scenarios
that have distinctive demands such as certain latency constraints and
specialized data distributions, it is prohibitively expensive to take advantage
of large-scale pre-training for per-task requirements. In this paper, we focus
on the area of object detection and present a transfer learning system named
GAIA, which could automatically and efficiently give birth to customized
solutions according to heterogeneous downstream needs. GAIA is capable of
providing powerful pre-trained weights, selecting models that conform to
downstream demands such as latency constraints and specified data domains, and
collecting relevant data for practitioners who have very few datapoints for
their tasks. With GAIA, we achieve promising results on COCO, Objects365, Open
Images, Caltech, CityPersons, and UODB which is a collection of datasets
including KITTI, VOC, WiderFace, DOTA, Clipart, Comic, and more. Taking COCO as
an example, GAIA is able to efficiently produce models covering a wide range of
latency from 16ms to 53ms, and yields AP from 38.2 to 46.5 without whistles and
bells. To benefit every practitioner in the community of object detection, GAIA
is released at https://github.com/GAIA-vision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bu_X/0/1/0/all/0/1"&gt;Xingyuan Bu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Junran Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junjie Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1"&gt;Tieniu Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhaoxiang Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating abstractive summaries of Lithuanian news articles using a transformer model. (arXiv:2105.03279v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03279</id>
        <link href="http://arxiv.org/abs/2105.03279"/>
        <updated>2021-06-23T01:48:38.597Z</updated>
        <summary type="html"><![CDATA[In this work, we train the first monolingual Lithuanian transformer model on
a relatively large corpus of Lithuanian news articles and compare various
output decoding algorithms for abstractive news summarization. We achieve an
average ROUGE-2 score 0.163, generated summaries are coherent and look
impressive at first glance. However, some of them contain misleading
information that is not so easy to spot. We describe all the technical details
and share our trained model and accompanying code in an online open-source
repository, as well as some characteristic samples of the generated summaries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stankevicius_L/0/1/0/all/0/1"&gt;Lukas Stankevi&amp;#x10d;ius&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukosevicius_M/0/1/0/all/0/1"&gt;Mantas Luko&amp;#x161;evi&amp;#x10d;ius&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IITP@COLIEE 2019: Legal Information Retrieval using BM25 and BERT. (arXiv:2104.08653v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08653</id>
        <link href="http://arxiv.org/abs/2104.08653"/>
        <updated>2021-06-23T01:48:38.589Z</updated>
        <summary type="html"><![CDATA[Natural Language Processing (NLP) and Information Retrieval (IR) in the
judicial domain is an essential task. With the advent of availability
domain-specific data in electronic form and aid of different Artificial
intelligence (AI) technologies, automated language processing becomes more
comfortable, and hence it becomes feasible for researchers and developers to
provide various automated tools to the legal community to reduce human burden.
The Competition on Legal Information Extraction/Entailment (COLIEE-2019) run in
association with the International Conference on Artificial Intelligence and
Law (ICAIL)-2019 has come up with few challenging tasks. The shared defined
four sub-tasks (i.e. Task1, Task2, Task3 and Task4), which will be able to
provide few automated systems to the judicial system. The paper presents our
working note on the experiments carried out as a part of our participation in
all the sub-tasks defined in this shared task. We make use of different
Information Retrieval(IR) and deep learning based approaches to tackle these
problems. We obtain encouraging results in all these four sub-tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gain_B/0/1/0/all/0/1"&gt;Baban Gain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bandyopadhyay_D/0/1/0/all/0/1"&gt;Dibyanayan Bandyopadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saikh_T/0/1/0/all/0/1"&gt;Tanik Saikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1"&gt;Asif Ekbal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CausalNLP: A Practical Toolkit for Causal Inference with Text. (arXiv:2106.08043v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08043</id>
        <link href="http://arxiv.org/abs/2106.08043"/>
        <updated>2021-06-23T01:48:38.581Z</updated>
        <summary type="html"><![CDATA[The vast majority of existing methods and systems for causal inference assume
that all variables under consideration are categorical or numerical (e.g.,
gender, price, blood pressure, enrollment). In this paper, we present
CausalNLP, a toolkit for inferring causality from observational data that
includes text in addition to traditional numerical and categorical variables.
CausalNLP employs the use of meta-learners for treatment effect estimation and
supports using raw text and its linguistic properties as both a treatment and a
"controlled-for" variable (e.g., confounder). The library is open-source and
available at: https://github.com/amaiya/causalnlp.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maiya_A/0/1/0/all/0/1"&gt;Arun S. Maiya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Who Blames or Endorses Whom? Entity-to-Entity Directed Sentiment Extraction in News Text. (arXiv:2106.01033v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01033</id>
        <link href="http://arxiv.org/abs/2106.01033"/>
        <updated>2021-06-23T01:48:38.573Z</updated>
        <summary type="html"><![CDATA[Understanding who blames or supports whom in news text is a critical research
question in computational social science. Traditional methods and datasets for
sentiment analysis are, however, not suitable for the domain of political text
as they do not consider the direction of sentiments expressed between entities.
In this paper, we propose a novel NLP task of identifying directed sentiment
relationship between political entities from a given news document, which we
call directed sentiment extraction. From a million-scale news corpus, we
construct a dataset of news sentences where sentiment relations of political
entities are manually annotated. We present a simple but effective approach for
utilizing a pretrained transformer, which infers the target class by predicting
multiple question-answering tasks and combining the outcomes. We demonstrate
the utility of our proposed method for social science research questions by
analyzing positive and negative opinions between political entities in two
major events: 2016 U.S. presidential election and COVID-19. The newly proposed
problem, data, and method will facilitate future studies on interdisciplinary
NLP methods and applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1"&gt;Kunwoo Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1"&gt;Zhufeng Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1"&gt;Jungseock Joo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image simulation for space applications with the SurRender software. (arXiv:2106.11322v1 [astro-ph.EP])]]></title>
        <id>http://arxiv.org/abs/2106.11322</id>
        <link href="http://arxiv.org/abs/2106.11322"/>
        <updated>2021-06-23T01:48:38.550Z</updated>
        <summary type="html"><![CDATA[Image Processing algorithms for vision-based navigation require reliable
image simulation capacities. In this paper we explain why traditional rendering
engines may present limitations that are potentially critical for space
applications. We introduce Airbus SurRender software v7 and provide details on
features that make it a very powerful space image simulator. We show how
SurRender is at the heart of the development processes of our computer vision
solutions and we provide a series of illustrations of rendered images for
various use cases ranging from Moon and Solar System exploration, to in orbit
rendezvous and planetary robotics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Lebreton_J/0/1/0/all/0/1"&gt;J&amp;#xe9;r&amp;#xe9;my Lebreton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Brochard_R/0/1/0/all/0/1"&gt;Roland Brochard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Baudry_M/0/1/0/all/0/1"&gt;Matthieu Baudry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Jonniaux_G/0/1/0/all/0/1"&gt;Gr&amp;#xe9;gory Jonniaux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Salah_A/0/1/0/all/0/1"&gt;Adrien Hadj Salah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Kanani_K/0/1/0/all/0/1"&gt;Keyvan Kanani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Goff_M/0/1/0/all/0/1"&gt;Matthieu Le Goff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Masson_A/0/1/0/all/0/1"&gt;Aurore Masson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Ollagnier_N/0/1/0/all/0/1"&gt;Nicolas Ollagnier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Panicucci_P/0/1/0/all/0/1"&gt;Paolo Panicucci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Proag_A/0/1/0/all/0/1"&gt;Amsha Proag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Robin_C/0/1/0/all/0/1"&gt;Cyril Robin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Context-aware PolyUNet for Liver and Lesion Segmentation from Abdominal CT Images. (arXiv:2106.11330v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.11330</id>
        <link href="http://arxiv.org/abs/2106.11330"/>
        <updated>2021-06-23T01:48:38.542Z</updated>
        <summary type="html"><![CDATA[Accurate liver and lesion segmentation from computed tomography (CT) images
are highly demanded in clinical practice for assisting the diagnosis and
assessment of hepatic tumor disease. However, automatic liver and lesion
segmentation from contrast-enhanced CT volumes is extremely challenging due to
the diversity in contrast, resolution, and quality of images. Previous methods
based on UNet for 2D slice-by-slice or 3D volume-by-volume segmentation either
lack sufficient spatial contexts or suffer from high GPU computational cost,
which limits the performance. To tackle these issues, we propose a novel
context-aware PolyUNet for accurate liver and lesion segmentation. It jointly
explores structural diversity and consecutive t-adjacent slices to enrich
feature expressive power and spatial contextual information while avoiding the
overload of GPU memory consumption. In addition, we utilize zoom out/in and
two-stage refinement strategy to exclude the irrelevant contexts and focus on
the specific region for the fine-grained segmentation. Our method achieved very
competitive performance at the MICCAI 2017 Liver Tumor Segmentation (LiTS)
Challenge among all tasks with a single model and ranked the $3^{rd}$,
$12^{th}$, $2^{nd}$, and $5^{th}$ places in the liver segmentation, lesion
segmentation, lesion detection, and tumor burden estimation, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yu_S/0/1/0/all/0/1"&gt;Simon Chun-Ho Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ETC-NLG: End-to-end Topic-Conditioned Natural Language Generation. (arXiv:2008.10875v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.10875</id>
        <link href="http://arxiv.org/abs/2008.10875"/>
        <updated>2021-06-23T01:48:38.422Z</updated>
        <summary type="html"><![CDATA[Plug-and-play language models (PPLMs) enable topic-conditioned natural
language generation by pairing large pre-trained generators with attribute
models used to steer the predicted token distribution towards the selected
topic. Despite their computational efficiency, PPLMs require large amounts of
labeled texts to effectively balance generation fluency and proper
conditioning, making them unsuitable for low-resource settings. We present
ETC-NLG, an approach leveraging topic modeling annotations to enable
fully-unsupervised End-to-end Topic-Conditioned Natural Language Generation
over emergent topics in unlabeled document collections. We first test the
effectiveness of our approach in a low-resource setting for Italian, evaluating
the conditioning for both topic models and gold annotations. We then perform a
comparative evaluation of ETC-NLG for Italian and English using a parallel
corpus. Finally, we propose an automatic approach to estimate the effectiveness
of conditioning on the generated utterances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carbone_G/0/1/0/all/0/1"&gt;Ginevra Carbone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarti_G/0/1/0/all/0/1"&gt;Gabriele Sarti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Evaluation of Machine Translation for Terminology Consistency. (arXiv:2106.11891v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11891</id>
        <link href="http://arxiv.org/abs/2106.11891"/>
        <updated>2021-06-23T01:48:38.396Z</updated>
        <summary type="html"><![CDATA[As neural machine translation (NMT) systems become an important part of
professional translator pipelines, a growing body of work focuses on combining
NMT with terminologies. In many scenarios and particularly in cases of domain
adaptation, one expects the MT output to adhere to the constraints provided by
a terminology. In this work, we propose metrics to measure the consistency of
MT output with regards to a domain terminology. We perform studies on the
COVID-19 domain over 5 languages, also performing terminology-targeted human
evaluation. We open-source the code for computing all proposed metrics:
https://github.com/mahfuzibnalam/terminology_evaluation]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1"&gt;Md Mahfuz ibn Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1"&gt;Antonios Anastasopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1"&gt;Laurent Besacier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cross_J/0/1/0/all/0/1"&gt;James Cross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galle_M/0/1/0/all/0/1"&gt;Matthias Gall&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1"&gt;Philipp Koehn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1"&gt;Vassilina Nikoulina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Dialogue Generation via Multi-Level Contrastive Learning. (arXiv:2009.09147v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.09147</id>
        <link href="http://arxiv.org/abs/2009.09147"/>
        <updated>2021-06-23T01:48:38.347Z</updated>
        <summary type="html"><![CDATA[Most of the existing works for dialogue generation are data-driven models
trained directly on corpora crawled from websites. They mainly focus on
improving the model architecture to produce better responses but pay little
attention to considering the quality of the training data contrastively. In
this paper, we propose a multi-level contrastive learning paradigm to model the
fine-grained quality of the responses with respect to the query. A Rank-aware
Calibration (RC) network is designed to construct the multi-level contrastive
optimization objectives. Since these objectives are calculated based on the
sentence level, which may erroneously encourage/suppress the generation of
uninformative/informative words. To tackle this incidental issue, on one hand,
we design an exquisite token-level strategy for estimating the instance loss
more accurately. On the other hand, we build a Knowledge Inference (KI)
component to capture the keyword knowledge from the reference during training
and exploit such information to encourage the generation of informative words.
We evaluate the proposed model on a carefully annotated dialogue dataset and
the results suggest that our model can generate more relevant and diverse
responses compared to the baseline models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Piji Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaojiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1"&gt;Wai Lam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Task-Oriented Dialog Modeling with Semi-Structured Knowledge Management. (arXiv:2106.11796v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11796</id>
        <link href="http://arxiv.org/abs/2106.11796"/>
        <updated>2021-06-23T01:48:38.337Z</updated>
        <summary type="html"><![CDATA[Current task-oriented dialog (TOD) systems mostly manage structured knowledge
(e.g. databases and tables) to guide the goal-oriented conversations. However,
they fall short of handling dialogs which also involve unstructured knowledge
(e.g. reviews and documents). In this paper, we formulate a task of modeling
TOD grounded on a fusion of structured and unstructured knowledge. To address
this task, we propose a TOD system with semi-structured knowledge management,
SeKnow, which extends the belief state to manage knowledge with both structured
and unstructured contents. Furthermore, we introduce two implementations of
SeKnow based on a non-pretrained sequence-to-sequence model and a pretrained
language model, respectively. Both implementations use the end-to-end manner to
jointly optimize dialog modeling grounded on structured and unstructured
knowledge. We conduct experiments on the modified version of MultiWOZ 2.1
dataset, where dialogs are processed to involve semi-structured knowledge.
Experimental results show that SeKnow has strong performances in both
end-to-end dialog and intermediate knowledge management, compared to existing
TOD systems and their extensions with pipeline knowledge management schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1"&gt;Silin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takanobu_R/0/1/0/all/0/1"&gt;Ryuichi Takanobu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Minlie Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Knowledge-Grounded Counter Narrative Generation for Hate Speech. (arXiv:2106.11783v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11783</id>
        <link href="http://arxiv.org/abs/2106.11783"/>
        <updated>2021-06-23T01:48:38.327Z</updated>
        <summary type="html"><![CDATA[Tackling online hatred using informed textual responses - called counter
narratives - has been brought under the spotlight recently. Accordingly, a
research line has emerged to automatically generate counter narratives in order
to facilitate the direct intervention in the hate discussion and to prevent
hate content from further spreading. Still, current neural approaches tend to
produce generic/repetitive responses and lack grounded and up-to-date evidence
such as facts, statistics, or examples. Moreover, these models can create
plausible but not necessarily true arguments. In this paper we present the
first complete knowledge-bound counter narrative generation pipeline, grounded
in an external knowledge repository that can provide more informative content
to fight online hatred. Together with our approach, we present a series of
experiments that show its feasibility to produce suitable and informative
counter narratives in in-domain and cross-domain settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1"&gt;Yi-Ling Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tekiroglu_S/0/1/0/all/0/1"&gt;Serra Sinem Tekiroglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guerini_M/0/1/0/all/0/1"&gt;Marco Guerini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How well do you know your summarization datasets?. (arXiv:2106.11388v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11388</id>
        <link href="http://arxiv.org/abs/2106.11388"/>
        <updated>2021-06-23T01:48:38.283Z</updated>
        <summary type="html"><![CDATA[State-of-the-art summarization systems are trained and evaluated on massive
datasets scraped from the web. Despite their prevalence, we know very little
about the underlying characteristics (data noise, summarization complexity,
etc.) of these datasets, and how these affect system performance and the
reliability of automatic metrics like ROUGE. In this study, we manually analyze
600 samples from three popular summarization datasets. Our study is driven by a
six-class typology which captures different noise types (missing facts,
entities) and degrees of summarization difficulty (extractive, abstractive). We
follow with a thorough analysis of 27 state-of-the-art summarization models and
5 popular metrics, and report our key insights: (1) Datasets have distinct data
quality and complexity distributions, which can be traced back to their
collection process. (2) The performance of models and reliability of metrics is
dependent on sample complexity. (3) Faithful summaries often receive low scores
because of the poor diversity of references. We release the code, annotated
data and model outputs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tejaswin_P/0/1/0/all/0/1"&gt;Priyam Tejaswin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naik_D/0/1/0/all/0/1"&gt;Dhruv Naik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Pengfei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Error-Aware Interactive Semantic Parsing of OpenStreetMap. (arXiv:2106.11739v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11739</id>
        <link href="http://arxiv.org/abs/2106.11739"/>
        <updated>2021-06-23T01:48:38.270Z</updated>
        <summary type="html"><![CDATA[In semantic parsing of geographical queries against real-world databases such
as OpenStreetMap (OSM), unique correct answers do not necessarily exist.
Instead, the truth might be lying in the eye of the user, who needs to enter an
interactive setup where ambiguities can be resolved and parsing mistakes can be
corrected. Our work presents an approach to interactive semantic parsing where
an explicit error detection is performed, and a clarification question is
generated that pinpoints the suspected source of ambiguity or error and
communicates it to the human user. Our experimental results show that a
combination of entropy-based uncertainty detection and beam search, together
with multi-source training on clarification question, initial parse, and user
answer, results in improvements of 1.2% F1 score on a parser that already
performs at 90.26% on the NLMaps dataset for OSM semantic parsing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Staniek_M/0/1/0/all/0/1"&gt;Michael Staniek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1"&gt;Stefan Riezler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Language Models Perform Generalizable Commonsense Inference?. (arXiv:2106.11533v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11533</id>
        <link href="http://arxiv.org/abs/2106.11533"/>
        <updated>2021-06-23T01:48:38.262Z</updated>
        <summary type="html"><![CDATA[Inspired by evidence that pretrained language models (LMs) encode commonsense
knowledge, recent work has applied LMs to automatically populate commonsense
knowledge graphs (CKGs). However, there is a lack of understanding on their
generalization to multiple CKGs, unseen relations, and novel entities. This
paper analyzes the ability of LMs to perform generalizable commonsense
inference, in terms of knowledge capacity, transferability, and induction. Our
experiments with these three aspects show that: (1) LMs can adapt to different
schemas defined by multiple CKGs but fail to reuse the knowledge to generalize
to new relations. (2) Adapted LMs generalize well to unseen subjects, but less
so on novel objects. Future work should investigate how to improve the
transferability and induction of commonsense mining from LMs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Peifeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1"&gt;Filip Ilievski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Muhao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SENT: Sentence-level Distant Relation Extraction via Negative Training. (arXiv:2106.11566v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11566</id>
        <link href="http://arxiv.org/abs/2106.11566"/>
        <updated>2021-06-23T01:48:38.253Z</updated>
        <summary type="html"><![CDATA[Distant supervision for relation extraction provides uniform bag labels for
each sentence inside the bag, while accurate sentence labels are important for
downstream applications that need the exact relation type. Directly using bag
labels for sentence-level training will introduce much noise, thus severely
degrading performance. In this work, we propose the use of negative training
(NT), in which a model is trained using complementary labels regarding that
``the instance does not belong to these complementary labels". Since the
probability of selecting a true label as a complementary label is low, NT
provides less noisy information. Furthermore, the model trained with NT is able
to separate the noisy data from the training data. Based on NT, we propose a
sentence-level framework, SENT, for distant relation extraction. SENT not only
filters the noisy data to construct a cleaner dataset, but also performs a
re-labeling process to transform the noisy data into useful training data, thus
further benefiting the model's performance. Experimental results show the
significant improvement of the proposed method over previous methods on
sentence-level evaluation and de-noise effect.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1"&gt;Ruotian Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1"&gt;Tao Gui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Linyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yaqian Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xuanjing Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comprehensive Exploration of Pre-training Language Models. (arXiv:2106.11483v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11483</id>
        <link href="http://arxiv.org/abs/2106.11483"/>
        <updated>2021-06-23T01:48:38.245Z</updated>
        <summary type="html"><![CDATA[Recently, the development of pre-trained language models has brought natural
language processing (NLP) tasks to the new state-of-the-art. In this paper we
explore the efficiency of various pre-trained language models. We pre-train a
list of transformer-based models with the same amount of text and the same
training steps. The experimental results shows that the most improvement upon
the origin BERT is adding the RNN-layer to capture more contextual information
for the transformer-encoder layers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1"&gt;Tong Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exemplars-guided Empathetic Response Generation Controlled by the Elements of Human Communication. (arXiv:2106.11791v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11791</id>
        <link href="http://arxiv.org/abs/2106.11791"/>
        <updated>2021-06-23T01:48:38.223Z</updated>
        <summary type="html"><![CDATA[The majority of existing methods for empathetic response generation rely on
the emotion of the context to generate empathetic responses. However, empathy
is much more than generating responses with an appropriate emotion. It also
often entails subtle expressions of understanding and personal resonance with
the situation of the other interlocutor. Unfortunately, such qualities are
difficult to quantify and the datasets lack the relevant annotations. To
address this issue, in this paper we propose an approach that relies on
exemplars to cue the generative model on fine stylistic properties that signal
empathy to the interlocutor. To this end, we employ dense passage retrieval to
extract relevant exemplary responses from the training set. Three elements of
human communication -- emotional presence, interpretation, and exploration, and
sentiment are additionally introduced using synthetic labels to guide the
generation towards empathy. The human evaluation is also extended by these
elements of human communication. We empirically show that these approaches
yield significant improvements in empathetic response quality in terms of both
automated and human-evaluated metrics. The implementation is available at
https://github.com/declare-lab/exemplary-empathy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1"&gt;Navonil Majumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1"&gt;Deepanway Ghosal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hazarika_D/0/1/0/all/0/1"&gt;Devamanyu Hazarika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1"&gt;Alexander Gelbukh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1"&gt;Rada Mihalcea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1"&gt;Soujanya Poria&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Routing between Capsules. (arXiv:2106.11531v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11531</id>
        <link href="http://arxiv.org/abs/2106.11531"/>
        <updated>2021-06-23T01:48:38.211Z</updated>
        <summary type="html"><![CDATA[Routing methods in capsule networks often learn a hierarchical relationship
for capsules in successive layers, but the intra-relation between capsules in
the same layer is less studied, while this intra-relation is a key factor for
the semantic understanding in text data. Therefore, in this paper, we introduce
a new capsule network with graph routing to learn both relationships, where
capsules in each layer are treated as the nodes of a graph. We investigate
strategies to yield adjacency and degree matrix with three different distances
from a layer of capsules, and propose the graph routing mechanism between those
capsules. We validate our approach on five text classification datasets, and
our findings suggest that the approach combining bottom-up routing and top-down
attention performs the best. Such an approach demonstrates generalization
capability across datasets. Compared to the state-of-the-art routing methods,
the improvements in accuracy in the five datasets we used were 0.82, 0.39,
0.07, 1.01, and 0.02, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wei Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1"&gt;Erik Cambria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Suhang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1"&gt;Steffen Eger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KaggleDBQA: Realistic Evaluation of Text-to-SQL Parsers. (arXiv:2106.11455v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11455</id>
        <link href="http://arxiv.org/abs/2106.11455"/>
        <updated>2021-06-23T01:48:38.072Z</updated>
        <summary type="html"><![CDATA[The goal of database question answering is to enable natural language
querying of real-life relational databases in diverse application domains.
Recently, large-scale datasets such as Spider and WikiSQL facilitated novel
modeling techniques for text-to-SQL parsing, improving zero-shot generalization
to unseen databases. In this work, we examine the challenges that still prevent
these techniques from practical deployment. First, we present KaggleDBQA, a new
cross-domain evaluation dataset of real Web databases, with domain-specific
data types, original formatting, and unrestricted questions. Second, we
re-examine the choice of evaluation tasks for text-to-SQL parsers as applied in
real-life settings. Finally, we augment our in-domain evaluation task with
database documentation, a naturally occurring source of implicit domain
knowledge. We show that KaggleDBQA presents a challenge to state-of-the-art
zero-shot parsers but a more realistic evaluation setting and creative use of
associated database documentation boosts their accuracy by over 13.2%, doubling
their performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chia-Hsuan Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polozov_O/0/1/0/all/0/1"&gt;Oleksandr Polozov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richardson_M/0/1/0/all/0/1"&gt;Matthew Richardson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BARTScore: Evaluating Generated Text as Text Generation. (arXiv:2106.11520v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11520</id>
        <link href="http://arxiv.org/abs/2106.11520"/>
        <updated>2021-06-23T01:48:38.063Z</updated>
        <summary type="html"><![CDATA[A wide variety of NLP applications, such as machine translation,
summarization, and dialog, involve text generation. One major challenge for
these applications is how to evaluate whether such generated texts are actually
fluent, accurate, or effective. In this work, we conceptualize the evaluation
of generated text as a text generation problem, modeled using pre-trained
sequence-to-sequence models. The general idea is that models trained to convert
the generated text to/from a reference output or the source text will achieve
higher scores when the generated text is better. We operationalize this idea
using BART, an encoder-decoder based pre-trained model, and propose a metric
BARTScore with a number of variants that can be flexibly applied in an
unsupervised fashion to evaluation of text from different perspectives (e.g.
informativeness, fluency, or factuality). BARTScore is conceptually simple and
empirically effective. It can outperform existing top-scoring metrics in 16 of
22 test settings, covering evaluation of 16 datasets (e.g., machine
translation, text summarization) and 7 different perspectives (e.g.,
informativeness, factuality). Code to calculate BARTScore is available at
https://github.com/neulab/BARTScore, and we have released an interactive
leaderboard for meta-evaluation at
this http URL on the ExplainaBoard
platform, which allows us to interactively understand the strengths,
weaknesses, and complementarity of each metric.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1"&gt;Weizhe Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1"&gt;Graham Neubig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Pengfei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LV-BERT: Exploiting Layer Variety for BERT. (arXiv:2106.11740v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11740</id>
        <link href="http://arxiv.org/abs/2106.11740"/>
        <updated>2021-06-23T01:48:38.047Z</updated>
        <summary type="html"><![CDATA[Modern pre-trained language models are mostly built upon backbones stacking
self-attention and feed-forward layers in an interleaved order. In this paper,
beyond this stereotyped layer pattern, we aim to improve pre-trained models by
exploiting layer variety from two aspects: the layer type set and the layer
order. Specifically, besides the original self-attention and feed-forward
layers, we introduce convolution into the layer type set, which is
experimentally found beneficial to pre-trained models. Furthermore, beyond the
original interleaved order, we explore more layer orders to discover more
powerful architectures. However, the introduced layer variety leads to a large
architecture space of more than billions of candidates, while training a single
candidate model from scratch already requires huge computation cost, making it
not affordable to search such a space by directly training large amounts of
candidate models. To solve this problem, we first pre-train a supernet from
which the weights of all candidate models can be inherited, and then adopt an
evolutionary algorithm guided by pre-training accuracy to find the optimal
architecture. Extensive experiments show that LV-BERT model obtained by our
method outperforms BERT and its variants on various downstream tasks. For
example, LV-BERT-small achieves 78.8 on the GLUE testing set, 1.8 higher than
the strong baseline ELECTRA-small.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1"&gt;Weihao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zihang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Fei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1"&gt;Qibin Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiashi Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis and Tuning of a Voice Assistant System for Dysfluent Speech. (arXiv:2106.11759v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.11759</id>
        <link href="http://arxiv.org/abs/2106.11759"/>
        <updated>2021-06-23T01:48:38.037Z</updated>
        <summary type="html"><![CDATA[Dysfluencies and variations in speech pronunciation can severely degrade
speech recognition performance, and for many individuals with
moderate-to-severe speech disorders, voice operated systems do not work.
Current speech recognition systems are trained primarily with data from fluent
speakers and as a consequence do not generalize well to speech with
dysfluencies such as sound or word repetitions, sound prolongations, or audible
blocks. The focus of this work is on quantitative analysis of a consumer speech
recognition system on individuals who stutter and production-oriented
approaches for improving performance for common voice assistant tasks (i.e.,
"what is the weather?"). At baseline, this system introduces a significant
number of insertion and substitution errors resulting in intended speech Word
Error Rates (isWER) that are 13.64\% worse (absolute) for individuals with
fluency disorders. We show that by simply tuning the decoding parameters in an
existing hybrid speech recognition system one can improve isWER by 24\%
(relative) for individuals with fluency disorders. Tuning these parameters
translates to 3.6\% better domain recognition and 1.7\% better intent
recognition relative to the default setup for the 18 study participants across
all stuttering severities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mitra_V/0/1/0/all/0/1"&gt;Vikramjit Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zifang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lea_C/0/1/0/all/0/1"&gt;Colin Lea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tooley_L/0/1/0/all/0/1"&gt;Lauren Tooley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1"&gt;Sarah Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Botten_D/0/1/0/all/0/1"&gt;Darren Botten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Palekar_A/0/1/0/all/0/1"&gt;Ashwini Palekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thelapurath_S/0/1/0/all/0/1"&gt;Shrinath Thelapurath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Georgiou_P/0/1/0/all/0/1"&gt;Panayiotis Georgiou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kajarekar_S/0/1/0/all/0/1"&gt;Sachin Kajarekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bigham_J/0/1/0/all/0/1"&gt;Jefferey Bigham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incremental Deep Neural Network Learning using Classification Confidence Thresholding. (arXiv:2106.11437v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11437</id>
        <link href="http://arxiv.org/abs/2106.11437"/>
        <updated>2021-06-23T01:48:38.013Z</updated>
        <summary type="html"><![CDATA[Most modern neural networks for classification fail to take into account the
concept of the unknown. Trained neural networks are usually tested in an
unrealistic scenario with only examples from a closed set of known classes. In
an attempt to develop a more realistic model, the concept of working in an open
set environment has been introduced. This in turn leads to the concept of
incremental learning where a model with its own architecture and initial
trained set of data can identify unknown classes during the testing phase and
autonomously update itself if evidence of a new class is detected. Some
problems that arise in incremental learning are inefficient use of resources to
retrain the classifier repeatedly and the decrease of classification accuracy
as multiple classes are added over time. This process of instantiating new
classes is repeated as many times as necessary, accruing errors. To address
these problems, this paper proposes the Classification Confidence Threshold
approach to prime neural networks for incremental learning to keep accuracies
high by limiting forgetting. A lean method is also used to reduce resources
used in the retraining of the neural network. The proposed method is based on
the idea that a network is able to incrementally learn a new class even when
exposed to a limited number samples associated with the new class. This method
can be applied to most existing neural networks with minimal changes to network
architecture.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leo_J/0/1/0/all/0/1"&gt;Justin Leo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1"&gt;Jugal Kalita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Cross-lingual Adaptation for Sequence Tagging and Beyond. (arXiv:2010.12405v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12405</id>
        <link href="http://arxiv.org/abs/2010.12405"/>
        <updated>2021-06-23T01:48:38.000Z</updated>
        <summary type="html"><![CDATA[Cross-lingual adaptation with multilingual pre-trained language models
(mPTLMs) mainly consists of two lines of works: zero-shot approach and
translation-based approach, which have been studied extensively on the
sequence-level tasks. We further verify the efficacy of these cross-lingual
adaptation approaches by evaluating their performances on more fine-grained
sequence tagging tasks. After re-examining their strengths and drawbacks, we
propose a novel framework to consolidate the zero-shot approach and the
translation-based approach for better adaptation performance. Instead of simply
augmenting the source data with the machine-translated data, we tailor-make a
warm-up mechanism to quickly update the mPTLMs with the gradients estimated on
a few translated data. Then, the adaptation approach is applied to the refined
parameters and the cross-lingual transfer is performed in a warm-start way. The
experimental results on nine target languages demonstrate that our method is
beneficial to the cross-lingual adaptation of various sequence tagging tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1"&gt;Lidong Bing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wenxuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1"&gt;Wai Lam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dive into Deep Learning. (arXiv:2106.11342v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11342</id>
        <link href="http://arxiv.org/abs/2106.11342"/>
        <updated>2021-06-23T01:48:37.985Z</updated>
        <summary type="html"><![CDATA[This open-source book represents our attempt to make deep learning
approachable, teaching readers the concepts, the context, and the code. The
entire book is drafted in Jupyter notebooks, seamlessly integrating exposition
figures, math, and interactive examples with self-contained code. Our goal is
to offer a resource that could (i) be freely available for everyone; (ii) offer
sufficient technical depth to provide a starting point on the path to actually
becoming an applied machine learning scientist; (iii) include runnable code,
showing readers how to solve problems in practice; (iv) allow for rapid
updates, both by us and also by the community at large; (v) be complemented by
a forum for interactive discussion of technical details and to answer
questions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Aston Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1"&gt;Zachary C. Lipton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1"&gt;Alexander J. Smola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-tune the Entire RAG Architecture (including DPR retriever) for Question-Answering. (arXiv:2106.11517v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.11517</id>
        <link href="http://arxiv.org/abs/2106.11517"/>
        <updated>2021-06-23T01:48:37.975Z</updated>
        <summary type="html"><![CDATA[In this paper, we illustrate how to fine-tune the entire Retrieval Augment
Generation (RAG) architecture in an end-to-end manner. We highlighted the main
engineering challenges that needed to be addressed to achieve this objective.
We also compare how end-to-end RAG architecture outperforms the original RAG
architecture for the task of question answering. We have open-sourced our
implementation in the HuggingFace Transformers library.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Siriwardhana_S/0/1/0/all/0/1"&gt;Shamane Siriwardhana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weerasekera_R/0/1/0/all/0/1"&gt;Rivindu Weerasekera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_E/0/1/0/all/0/1"&gt;Elliott Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nanayakkara_S/0/1/0/all/0/1"&gt;Suranga Nanayakkara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Systematic Evaluation of Transfer Learning and Pseudo-labeling with BERT-based Ranking Models. (arXiv:2103.03335v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03335</id>
        <link href="http://arxiv.org/abs/2103.03335"/>
        <updated>2021-06-23T01:48:37.967Z</updated>
        <summary type="html"><![CDATA[Due to high annotation costs making the best use of existing human-created
training data is an important research direction. We, therefore, carry out a
systematic evaluation of transferability of BERT-based neural ranking models
across five English datasets. Previous studies focused primarily on zero-shot
and few-shot transfer from a large dataset to a dataset with a small number of
queries. In contrast, each of our collections has a substantial number of
queries, which enables a full-shot evaluation mode and improves reliability of
our results. Furthermore, since source datasets licences often prohibit
commercial use, we compare transfer learning to training on pseudo-labels
generated by a BM25 scorer. We find that training on pseudo-labels -- possibly
with subsequent fine-tuning using a modest number of annotated queries -- can
produce a competitive or better model compared to transfer learning. Yet, it is
necessary to improve the stability and/or effectiveness of the few-shot
training, which, sometimes, can degrade performance of a pretrained model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mokrii_I/0/1/0/all/0/1"&gt;Iurii Mokrii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boytsov_L/0/1/0/all/0/1"&gt;Leonid Boytsov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Braslavski_P/0/1/0/all/0/1"&gt;Pavel Braslavski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey of Race, Racism, and Anti-Racism in NLP. (arXiv:2106.11410v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11410</id>
        <link href="http://arxiv.org/abs/2106.11410"/>
        <updated>2021-06-23T01:48:37.958Z</updated>
        <summary type="html"><![CDATA[Despite inextricable ties between race and language, little work has
considered race in NLP research and development. In this work, we survey 79
papers from the ACL anthology that mention race. These papers reveal various
types of race-related bias in all stages of NLP model development, highlighting
the need for proactive consideration of how NLP systems can uphold racial
hierarchies. However, persistent gaps in research on race and NLP remain: race
has been siloed as a niche topic and remains ignored in many NLP tasks; most
work operationalizes race as a fixed single-dimensional variable with a
ground-truth label, which risks reinforcing differences produced by historical
racism; and the voices of historically marginalized people are nearly absent in
NLP literature. By identifying where and how NLP literature has and has not
considered race, especially in comparison to related fields, our work calls for
inclusion and racial justice in NLP research practices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Field_A/0/1/0/all/0/1"&gt;Anjalie Field&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blodgett_S/0/1/0/all/0/1"&gt;Su Lin Blodgett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waseem_Z/0/1/0/all/0/1"&gt;Zeerak Waseem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1"&gt;Yulia Tsvetkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Models in Detection of Dietary Supplement Adverse Event Signals from Twitter. (arXiv:2106.11403v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11403</id>
        <link href="http://arxiv.org/abs/2106.11403"/>
        <updated>2021-06-23T01:48:37.932Z</updated>
        <summary type="html"><![CDATA[Objective: The objective of this study is to develop a deep learning pipeline
to detect signals on dietary supplement-related adverse events (DS AEs) from
Twitter. Material and Methods: We obtained 247,807 tweets ranging from 2012 to
2018 that mentioned both DS and AE. We annotated biomedical entities and
relations on 2,000 randomly selected tweets. For the concept extraction task,
we compared the performance of traditional word embeddings with SVM, CRF and
LSTM-CRF classifiers to BERT models. For the relation extraction task, we
compared GloVe vectors with CNN classifiers to BERT models. We chose the best
performing models in each task to assemble an end-to-end deep learning pipeline
to detect DS AE signals and compared the results to the known DS AEs from a DS
knowledge base (i.e., iDISK). Results: In both tasks, the BERT-based models
outperformed traditional word embeddings. The best performing concept
extraction model is the BioBERT model that can identify supplement, symptom,
and body organ entities with F1-scores of 0.8646, 0.8497, and 0.7104,
respectively. The best performing relation extraction model is the BERT model
that can identify purpose and AE relations with F1-scores of 0.8335 and 0.7538,
respectively. The end-to-end pipeline was able to extract DS indication and DS
AEs with an F1-score of 0.7459 and 0,7414, respectively. Comparing to the
iDISK, we could find both known and novel DS-AEs. Conclusion: We have
demonstrated the feasibility of detecting DS AE signals from Twitter with a
BioBERT-based deep learning pipeline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yefeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yunpeng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1"&gt;Jiang Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Membership Inference on Word Embedding and Beyond. (arXiv:2106.11384v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11384</id>
        <link href="http://arxiv.org/abs/2106.11384"/>
        <updated>2021-06-23T01:48:37.921Z</updated>
        <summary type="html"><![CDATA[In the text processing context, most ML models are built on word embeddings.
These embeddings are themselves trained on some datasets, potentially
containing sensitive data. In some cases this training is done independently,
in other cases, it occurs as part of training a larger, task-specific model. In
either case, it is of interest to consider membership inference attacks based
on the embedding layer as a way of understanding sensitive information leakage.
But, somewhat surprisingly, membership inference attacks on word embeddings and
their effect in other natural language processing (NLP) tasks that use these
embeddings, have remained relatively unexplored.

In this work, we show that word embeddings are vulnerable to black-box
membership inference attacks under realistic assumptions. Furthermore, we show
that this leakage persists through two other major NLP applications:
classification and text-generation, even when the embedding layer is not
exposed to the attacker. We show that our MI attack achieves high attack
accuracy against a classifier model and an LSTM-based language model. Indeed,
our attack is a cheaper membership inference attack on text-generative models,
which does not require the knowledge of the target model or any expensive
training of text-generative models as shadow models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahloujifar_S/0/1/0/all/0/1"&gt;Saeed Mahloujifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Inan_H/0/1/0/all/0/1"&gt;Huseyin A. Inan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chase_M/0/1/0/all/0/1"&gt;Melissa Chase&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_E/0/1/0/all/0/1"&gt;Esha Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasegawa_M/0/1/0/all/0/1"&gt;Marcello Hasegawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Phrase-level Active Learning for Neural Machine Translation. (arXiv:2106.11375v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11375</id>
        <link href="http://arxiv.org/abs/2106.11375"/>
        <updated>2021-06-23T01:48:37.904Z</updated>
        <summary type="html"><![CDATA[Neural machine translation (NMT) is sensitive to domain shift. In this paper,
we address this problem in an active learning setting where we can spend a
given budget on translating in-domain data, and gradually fine-tune a
pre-trained out-of-domain NMT model on the newly translated data. Existing
active learning methods for NMT usually select sentences based on uncertainty
scores, but these methods require costly translation of full sentences even
when only one or two key phrases within the sentence are informative. To
address this limitation, we re-examine previous work from the phrase-based
machine translation (PBMT) era that selected not full sentences, but rather
individual phrases. However, while incorporating these phrases into PBMT
systems was relatively simple, it is less trivial for NMT systems, which need
to be trained on full sequences to capture larger structural properties of
sentences unique to the new domain. To overcome these hurdles, we propose to
select both full sentences and individual phrases from unlabelled data in the
new domain for routing to human translators. In a German-English translation
task, our active learning approach achieves consistent improvements over
uncertainty-based sentence selection methods, improving up to 1.2 BLEU score
over strong active learning baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Junjie Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1"&gt;Graham Neubig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learn to Resolve Conversational Dependency: A Consistency Training Framework for Conversational Question Answering. (arXiv:2106.11575v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11575</id>
        <link href="http://arxiv.org/abs/2106.11575"/>
        <updated>2021-06-23T01:48:37.891Z</updated>
        <summary type="html"><![CDATA[One of the main challenges in conversational question answering (CQA) is to
resolve the conversational dependency, such as anaphora and ellipsis. However,
existing approaches do not explicitly train QA models on how to resolve the
dependency, and thus these models are limited in understanding human dialogues.
In this paper, we propose a novel framework, ExCorD (Explicit guidance on how
to resolve Conversational Dependency) to enhance the abilities of QA models in
comprehending conversational context. ExCorD first generates self-contained
questions that can be understood without the conversation history, then trains
a QA model with the pairs of original and self-contained questions using a
consistency-based regularizer. In our experiments, we demonstrate that ExCorD
significantly improves the QA models' performance by up to 1.2 F1 on QuAC, and
5.2 F1 on CANARD, while addressing the limitations of the existing approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1"&gt;Gangwoo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyunjae Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jungsoo Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1"&gt;Jaewoo Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RtFPS: An Interactive Map that Visualizes and Predicts Wildfires in the US. (arXiv:2105.10880v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10880</id>
        <link href="http://arxiv.org/abs/2105.10880"/>
        <updated>2021-06-23T01:48:37.718Z</updated>
        <summary type="html"><![CDATA[Climate change has largely impacted our daily lives. As one of its
consequences, we are experiencing more wildfires. In the year 2020, wildfires
burned a record number of 8,888,297 acres in the US. To awaken people's
attention to climate change, and to visualize the current risk of wildfires, We
developed RtFPS, "Real-Time Fire Prediction System". It provides a real-time
prediction visualization of wildfire risk at specific locations base on a
Machine Learning model. It also provides interactive map features that show the
historical wildfire events with environmental info.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mulyono_H/0/1/0/all/0/1"&gt;Hermawan Mulyono&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Ying Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zhiyin Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_D/0/1/0/all/0/1"&gt;Desmond Chan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tracking Instances as Queries. (arXiv:2106.11963v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11963</id>
        <link href="http://arxiv.org/abs/2106.11963"/>
        <updated>2021-06-23T01:48:37.697Z</updated>
        <summary type="html"><![CDATA[Recently, query based deep networks catch lots of attention owing to their
end-to-end pipeline and competitive results on several fundamental computer
vision tasks, such as object detection, semantic segmentation, and instance
segmentation. However, how to establish a query based video instance
segmentation (VIS) framework with elegant architecture and strong performance
remains to be settled. In this paper, we present \textbf{QueryTrack} (i.e.,
tracking instances as queries), a unified query based VIS framework fully
leveraging the intrinsic one-to-one correspondence between instances and
queries in QueryInst. The proposed method obtains 52.7 / 52.3 AP on
YouTube-VIS-2019 / 2021 datasets, which wins the 2-nd place in the YouTube-VIS
Challenge at CVPR 2021 \textbf{with a single online end-to-end model, single
scale testing \& modest amount of training data}. We also provide
QueryTrack-ResNet-50 baseline results on YouTube-VIS-2021 dataset as references
for the VIS community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shusheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yuxin Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinggang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1"&gt;Ying Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1"&gt;Bin Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wenyu Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is all this new MeSH about? Exploring the semantic provenance of new descriptors in the MeSH thesaurus. (arXiv:2101.08293v2 [cs.DL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08293</id>
        <link href="http://arxiv.org/abs/2101.08293"/>
        <updated>2021-06-23T01:48:37.688Z</updated>
        <summary type="html"><![CDATA[The Medical Subject Headings (MeSH) thesaurus is a controlled vocabulary
widely used in biomedical knowledge systems, particularly for semantic indexing
of scientific literature. As the MeSH hierarchy evolves through annual version
updates, some new descriptors are introduced that were not previously
available. This paper explores the conceptual provenance of these new
descriptors. In particular, we investigate whether such new descriptors have
been previously covered by older descriptors and what is their current relation
to them. To this end, we propose a framework to categorize new descriptors
based on their current relation to older descriptors. Based on the proposed
classification scheme, we quantify, analyse and present the different types of
new descriptors introduced in MeSH during the last fifteen years. The results
show that only about 25% of new MeSH descriptors correspond to new emerging
concepts, whereas the rest were previously covered by one or more existing
descriptors, either implicitly or explicitly. Most of them were covered by a
single existing descriptor and they usually end up as descendants of it in the
current hierarchy, gradually leading towards a more fine-grained MeSH
vocabulary. These insights about the dynamics of the thesaurus are useful for
the retrospective study of scientific articles annotated with MeSH, but could
also be used to inform the policy of updating the thesaurus in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nentidis_A/0/1/0/all/0/1"&gt;Anastasios Nentidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krithara_A/0/1/0/all/0/1"&gt;Anastasia Krithara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1"&gt;Grigorios Tsoumakas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paliouras_G/0/1/0/all/0/1"&gt;Georgios Paliouras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Turing Award elites revisited: patterns of productivity, collaboration, authorship and impact. (arXiv:2106.11534v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2106.11534</id>
        <link href="http://arxiv.org/abs/2106.11534"/>
        <updated>2021-06-23T01:48:37.677Z</updated>
        <summary type="html"><![CDATA[The Turing Award is recognized as the most influential and prestigious award
in the field of computer science(CS). With the rise of the science of science
(SciSci), a large amount of bibliographic data has been analyzed in an attempt
to understand the hidden mechanism of scientific evolution. These include the
analysis of the Nobel Prize, including physics, chemistry, medicine, etc. In
this article, we extract and analyze the data of 72 Turing Award laureates from
the complete bibliographic data, fill the gap in the lack of Turing Award
analysis, and discover the development characteristics of computer science as
an independent discipline. First, we show most Turing Award laureates have
long-term and high-quality educational backgrounds, and more than 61% of them
have a degree in mathematics, which indicates that mathematics has played a
significant role in the development of computer science. Secondly, the data
shows that not all scholars have high productivity and high h-index; that is,
the number of publications and h-index is not the leading indicator for
evaluating the Turing Award. Third, the average age of awardees has increased
from 40 to around 70 in recent years. This may be because new breakthroughs
take longer, and some new technologies need time to prove their influence.
Besides, we have also found that in the past ten years, international
collaboration has experienced explosive growth, showing a new paradigm in the
form of collaboration. It is also worth noting that in recent years, the
emergence of female winners has also been eye-catching. Finally, by analyzing
the personal publication records, we find that many people are more likely to
publish high-impact articles during their high-yield periods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1"&gt;Yinyu Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1"&gt;Sha Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1"&gt;Zhou Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hall_W/0/1/0/all/0/1"&gt;Wendy Hall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jie Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Systematic Evaluation of Transfer Learning and Pseudo-labeling with BERT-based Ranking Models. (arXiv:2103.03335v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03335</id>
        <link href="http://arxiv.org/abs/2103.03335"/>
        <updated>2021-06-23T01:48:37.666Z</updated>
        <summary type="html"><![CDATA[Due to high annotation costs making the best use of existing human-created
training data is an important research direction. We, therefore, carry out a
systematic evaluation of transferability of BERT-based neural ranking models
across five English datasets. Previous studies focused primarily on zero-shot
and few-shot transfer from a large dataset to a dataset with a small number of
queries. In contrast, each of our collections has a substantial number of
queries, which enables a full-shot evaluation mode and improves reliability of
our results. Furthermore, since source datasets licences often prohibit
commercial use, we compare transfer learning to training on pseudo-labels
generated by a BM25 scorer. We find that training on pseudo-labels -- possibly
with subsequent fine-tuning using a modest number of annotated queries -- can
produce a competitive or better model compared to transfer learning. Yet, it is
necessary to improve the stability and/or effectiveness of the few-shot
training, which, sometimes, can degrade performance of a pretrained model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mokrii_I/0/1/0/all/0/1"&gt;Iurii Mokrii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boytsov_L/0/1/0/all/0/1"&gt;Leonid Boytsov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Braslavski_P/0/1/0/all/0/1"&gt;Pavel Braslavski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SeqNetVLAD vs PointNetVLAD: Image Sequence vs 3D Point Clouds for Day-Night Place Recognition. (arXiv:2106.11481v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11481</id>
        <link href="http://arxiv.org/abs/2106.11481"/>
        <updated>2021-06-23T01:48:37.598Z</updated>
        <summary type="html"><![CDATA[Place Recognition is a crucial capability for mobile robot localization and
navigation. Image-based or Visual Place Recognition (VPR) is a challenging
problem as scene appearance and camera viewpoint can change significantly when
places are revisited. Recent VPR methods based on ``sequential
representations'' have shown promising results as compared to traditional
sequence score aggregation or single image based techniques. In parallel to
these endeavors, 3D point clouds based place recognition is also being explored
following the advances in deep learning based point cloud processing. However,
a key question remains: is an explicit 3D structure based place representation
always superior to an implicit ``spatial'' representation based on sequence of
RGB images which can inherently learn scene structure. In this extended
abstract, we attempt to compare these two types of methods by considering a
similar ``metric span'' to represent places. We compare a 3D point cloud based
method (PointNetVLAD) with image sequence based methods (SeqNet and others) and
showcase that image sequence based techniques approach, and can even surpass,
the performance achieved by point cloud based methods for a given metric span.
These performance variations can be attributed to differences in data richness
of input sensors as well as data accumulation strategies for a mobile robot.
While a perfect apple-to-apple comparison may not be feasible for these two
different modalities, the presented comparison takes a step in the direction of
answering deeper questions regarding spatial representations, relevant to
several applications like Autonomous Driving and Augmented/Virtual Reality.
Source code available publicly https://github.com/oravus/seqNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1"&gt;Sourav Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1"&gt;Michael Milford&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating abstractive summaries of Lithuanian news articles using a transformer model. (arXiv:2105.03279v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03279</id>
        <link href="http://arxiv.org/abs/2105.03279"/>
        <updated>2021-06-23T01:48:37.576Z</updated>
        <summary type="html"><![CDATA[In this work, we train the first monolingual Lithuanian transformer model on
a relatively large corpus of Lithuanian news articles and compare various
output decoding algorithms for abstractive news summarization. We achieve an
average ROUGE-2 score 0.163, generated summaries are coherent and look
impressive at first glance. However, some of them contain misleading
information that is not so easy to spot. We describe all the technical details
and share our trained model and accompanying code in an online open-source
repository, as well as some characteristic samples of the generated summaries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stankevicius_L/0/1/0/all/0/1"&gt;Lukas Stankevi&amp;#x10d;ius&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukosevicius_M/0/1/0/all/0/1"&gt;Mantas Luko&amp;#x161;evi&amp;#x10d;ius&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Models in Detection of Dietary Supplement Adverse Event Signals from Twitter. (arXiv:2106.11403v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11403</id>
        <link href="http://arxiv.org/abs/2106.11403"/>
        <updated>2021-06-23T01:48:37.549Z</updated>
        <summary type="html"><![CDATA[Objective: The objective of this study is to develop a deep learning pipeline
to detect signals on dietary supplement-related adverse events (DS AEs) from
Twitter. Material and Methods: We obtained 247,807 tweets ranging from 2012 to
2018 that mentioned both DS and AE. We annotated biomedical entities and
relations on 2,000 randomly selected tweets. For the concept extraction task,
we compared the performance of traditional word embeddings with SVM, CRF and
LSTM-CRF classifiers to BERT models. For the relation extraction task, we
compared GloVe vectors with CNN classifiers to BERT models. We chose the best
performing models in each task to assemble an end-to-end deep learning pipeline
to detect DS AE signals and compared the results to the known DS AEs from a DS
knowledge base (i.e., iDISK). Results: In both tasks, the BERT-based models
outperformed traditional word embeddings. The best performing concept
extraction model is the BioBERT model that can identify supplement, symptom,
and body organ entities with F1-scores of 0.8646, 0.8497, and 0.7104,
respectively. The best performing relation extraction model is the BERT model
that can identify purpose and AE relations with F1-scores of 0.8335 and 0.7538,
respectively. The end-to-end pipeline was able to extract DS indication and DS
AEs with an F1-score of 0.7459 and 0,7414, respectively. Comparing to the
iDISK, we could find both known and novel DS-AEs. Conclusion: We have
demonstrated the feasibility of detecting DS AE signals from Twitter with a
BioBERT-based deep learning pipeline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yefeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yunpeng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1"&gt;Jiang Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Query-Driven Topic Model. (arXiv:2106.07346v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07346</id>
        <link href="http://arxiv.org/abs/2106.07346"/>
        <updated>2021-06-23T01:48:37.521Z</updated>
        <summary type="html"><![CDATA[Topic modeling is an unsupervised method for revealing the hidden semantic
structure of a corpus. It has been increasingly widely adopted as a tool in the
social sciences, including political science, digital humanities and
sociological research in general. One desirable property of topic models is to
allow users to find topics describing a specific aspect of the corpus. A
possible solution is to incorporate domain-specific knowledge into topic
modeling, but this requires a specification from domain experts. We propose a
novel query-driven topic model that allows users to specify a simple query in
words or phrases and return query-related topics, thus avoiding tedious work
from domain experts. Our proposed approach is particularly attractive when the
user-specified query has a low occurrence in a text corpus, making it difficult
for traditional topic models built on word cooccurrence patterns to identify
relevant topics. Experimental results demonstrate the effectiveness of our
model in comparison with both classical topic models and neural topic models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1"&gt;Zheng Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yulan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Procter_R/0/1/0/all/0/1"&gt;Rob Procter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IITP@COLIEE 2019: Legal Information Retrieval using BM25 and BERT. (arXiv:2104.08653v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08653</id>
        <link href="http://arxiv.org/abs/2104.08653"/>
        <updated>2021-06-23T01:48:37.499Z</updated>
        <summary type="html"><![CDATA[Natural Language Processing (NLP) and Information Retrieval (IR) in the
judicial domain is an essential task. With the advent of availability
domain-specific data in electronic form and aid of different Artificial
intelligence (AI) technologies, automated language processing becomes more
comfortable, and hence it becomes feasible for researchers and developers to
provide various automated tools to the legal community to reduce human burden.
The Competition on Legal Information Extraction/Entailment (COLIEE-2019) run in
association with the International Conference on Artificial Intelligence and
Law (ICAIL)-2019 has come up with few challenging tasks. The shared defined
four sub-tasks (i.e. Task1, Task2, Task3 and Task4), which will be able to
provide few automated systems to the judicial system. The paper presents our
working note on the experiments carried out as a part of our participation in
all the sub-tasks defined in this shared task. We make use of different
Information Retrieval(IR) and deep learning based approaches to tackle these
problems. We obtain encouraging results in all these four sub-tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gain_B/0/1/0/all/0/1"&gt;Baban Gain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bandyopadhyay_D/0/1/0/all/0/1"&gt;Dibyanayan Bandyopadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saikh_T/0/1/0/all/0/1"&gt;Tanik Saikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1"&gt;Asif Ekbal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Who Blames or Endorses Whom? Entity-to-Entity Directed Sentiment Extraction in News Text. (arXiv:2106.01033v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01033</id>
        <link href="http://arxiv.org/abs/2106.01033"/>
        <updated>2021-06-23T01:48:37.480Z</updated>
        <summary type="html"><![CDATA[Understanding who blames or supports whom in news text is a critical research
question in computational social science. Traditional methods and datasets for
sentiment analysis are, however, not suitable for the domain of political text
as they do not consider the direction of sentiments expressed between entities.
In this paper, we propose a novel NLP task of identifying directed sentiment
relationship between political entities from a given news document, which we
call directed sentiment extraction. From a million-scale news corpus, we
construct a dataset of news sentences where sentiment relations of political
entities are manually annotated. We present a simple but effective approach for
utilizing a pretrained transformer, which infers the target class by predicting
multiple question-answering tasks and combining the outcomes. We demonstrate
the utility of our proposed method for social science research questions by
analyzing positive and negative opinions between political entities in two
major events: 2016 U.S. presidential election and COVID-19. The newly proposed
problem, data, and method will facilitate future studies on interdisciplinary
NLP methods and applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1"&gt;Kunwoo Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1"&gt;Zhufeng Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1"&gt;Jungseock Joo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying the Impact of Human Capital, Job History, and Language Factors on Job Seniority with a Large-scale Analysis of Resumes. (arXiv:2106.11846v1 [econ.GN])]]></title>
        <id>http://arxiv.org/abs/2106.11846</id>
        <link href="http://arxiv.org/abs/2106.11846"/>
        <updated>2021-06-23T01:48:37.435Z</updated>
        <summary type="html"><![CDATA[As job markets worldwide have become more competitive and applicant selection
criteria have become more opaque, and different (and sometimes contradictory)
information and advice is available for job seekers wishing to progress in
their careers, it has never been more difficult to determine which factors in a
r\'esum\'e most effectively help career progression. In this work we present a
novel, large scale dataset of over half a million r\'esum\'es with preliminary
analysis to begin to answer empirically which factors help or hurt people
wishing to transition to more senior roles as they progress in their career. We
find that previous experience forms the most important factor, outweighing
other aspects of human capital, and find which language factors in a r\'esum\'e
have significant effects. This lays the groundwork for future inquiry in career
trajectories using large scale data analysis and natural language processing
techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/econ/1/au:+Wright_A/0/1/0/all/0/1"&gt;Austin P Wright&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Ziems_C/0/1/0/all/0/1"&gt;Caleb Ziems&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Park_H/0/1/0/all/0/1"&gt;Haekyu Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Saad_Falcon_J/0/1/0/all/0/1"&gt;Jon Saad-Falcon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Chau_D/0/1/0/all/0/1"&gt;Duen Horng Chau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Yang_D/0/1/0/all/0/1"&gt;Diyi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Tomprou_M/0/1/0/all/0/1"&gt;Maria Tomprou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovering Mathematical Objects of Interest -- A Study of Mathematical Notations. (arXiv:2002.02712v3 [cs.DL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.02712</id>
        <link href="http://arxiv.org/abs/2002.02712"/>
        <updated>2021-06-23T01:48:37.249Z</updated>
        <summary type="html"><![CDATA[Mathematical notation, i.e., the writing system used to communicate concepts
in mathematics, encodes valuable information for a variety of information
search and retrieval systems. Yet, mathematical notations remain mostly
unutilized by today's systems. In this paper, we present the first in-depth
study on the distributions of mathematical notation in two large scientific
corpora: the open access arXiv (2.5B mathematical objects) and the mathematical
reviewing service for pure and applied mathematics zbMATH (61M mathematical
objects). Our study lays a foundation for future research projects on
mathematical information retrieval for large scientific corpora. Further, we
demonstrate the relevance of our results to a variety of use-cases. For
example, to assist semantic extraction systems, to improve scientific search
engines, and to facilitate specialized math recommendation systems. The
contributions of our presented research are as follows: (1) we present the
first distributional analysis of mathematical formulae on arXiv and zbMATH; (2)
we retrieve relevant mathematical objects for given textual search queries
(e.g., linking $P_{n}^{(\alpha, \beta)}\!\left(x\right)$ with `Jacobi
polynomial'); (3) we extend zbMATH's search engine by providing relevant
mathematical formulae; and (4) we exemplify the applicability of the results by
presenting auto-completion for math inputs as the first contribution to math
recommendation systems. To expedite future research projects, we have made
available our source code and data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Greiner_Petter_A/0/1/0/all/0/1"&gt;Andre Greiner-Petter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schubotz_M/0/1/0/all/0/1"&gt;Moritz Schubotz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mueller_F/0/1/0/all/0/1"&gt;Fabian Mueller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Breitinger_C/0/1/0/all/0/1"&gt;Corinna Breitinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohl_H/0/1/0/all/0/1"&gt;Howard S. Cohl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aizawa_A/0/1/0/all/0/1"&gt;Akiko Aizawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1"&gt;Bela Gipp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-tune the Entire RAG Architecture (including DPR retriever) for Question-Answering. (arXiv:2106.11517v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.11517</id>
        <link href="http://arxiv.org/abs/2106.11517"/>
        <updated>2021-06-23T01:48:37.203Z</updated>
        <summary type="html"><![CDATA[In this paper, we illustrate how to fine-tune the entire Retrieval Augment
Generation (RAG) architecture in an end-to-end manner. We highlighted the main
engineering challenges that needed to be addressed to achieve this objective.
We also compare how end-to-end RAG architecture outperforms the original RAG
architecture for the task of question answering. We have open-sourced our
implementation in the HuggingFace Transformers library.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Siriwardhana_S/0/1/0/all/0/1"&gt;Shamane Siriwardhana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weerasekera_R/0/1/0/all/0/1"&gt;Rivindu Weerasekera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_E/0/1/0/all/0/1"&gt;Elliott Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nanayakkara_S/0/1/0/all/0/1"&gt;Suranga Nanayakkara&lt;/a&gt;</name>
        </author>
    </entry>
</feed>