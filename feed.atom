<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://looperxx.github.io/ArxivDaily/index.html</id>
    <title>ArxivDaily</title>
    <updated>2021-08-03T02:06:35.353Z</updated>
    <generator>osmosfeed 1.11.0</generator>
    <link rel="alternate" href="https://looperxx.github.io/ArxivDaily/index.html"/>
    <link rel="self" href="https://looperxx.github.io/ArxivDaily/feed.atom"/>
    <entry>
        <title type="html"><![CDATA[Rethinking the Implementation Tricks and Monotonicity Constraint in Cooperative Multi-Agent Reinforcement Learning. (arXiv:2102.03479v13 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03479</id>
        <link href="http://arxiv.org/abs/2102.03479"/>
        <updated>2021-08-03T02:06:35.290Z</updated>
        <summary type="html"><![CDATA[Many complex multi-robot systems such as robot swarms control and autonomous
vehicle coordination can be modeled as Multi-Agent Reinforcement Learning
(MARL) tasks. QMIX, a widely popular MARL algorithm, has been used as a
baseline for the benchmark environments, e.g., Starcraft Multi-Agent Challenge
(SMAC), Difficulty-Enhanced Predator-Prey (DEPP). Recent variants of QMIX
target relaxing the monotonicity constraint of QMIX, allowing for performance
improvement in SMAC. In this paper, we investigate the code-level optimizations
of these variants and the monotonicity constraint. (1) We find that such
improvements of the variants are significantly affected by various code-level
optimizations. (2) The experiment results show that QMIX with normalized
optimizations outperforms other works in SMAC; (3) beyond the common wisdom
from these works, the monotonicity constraint can improve sample efficiency in
SMAC and DEPP. We also discuss why monotonicity constraints work well in purely
cooperative tasks with a theoretical analysis. We open-source the code at
\url{https://github.com/hijkzzz/pymarl2}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jian Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Siyang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harding_S/0/1/0/all/0/1"&gt;Seth Austin Harding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Haibin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1"&gt;Shih-wei Liao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Metro Origin-Destination Prediction via Heterogeneous Information Aggregation. (arXiv:2107.00946v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00946</id>
        <link href="http://arxiv.org/abs/2107.00946"/>
        <updated>2021-08-03T02:06:35.235Z</updated>
        <summary type="html"><![CDATA[Metro origin-destination prediction is a crucial yet challenging time-series
analysis task in intelligent transportation systems, which aims to accurately
forecast two specific types of cross-station ridership, i.e.,
Origin-Destination (OD) one and Destination-Origin (DO) one. However, complete
OD matrices of previous time intervals can not be obtained immediately in
online metro systems, and conventional methods only used limited information to
forecast the future OD and DO ridership separately. In this work, we proposed a
novel neural network module termed Heterogeneous Information Aggregation
Machine (HIAM), which fully exploits heterogeneous information of historical
data (e.g., incomplete OD matrices, unfinished order vectors, and DO matrices)
to jointly learn the evolutionary patterns of OD and DO ridership.
Specifically, an OD modeling branch estimates the potential destinations of
unfinished orders explicitly to complement the information of incomplete OD
matrices, while a DO modeling branch takes DO matrices as input to capture the
spatial-temporal distribution of DO ridership. Moreover, a Dual Information
Transformer is introduced to propagate the mutual information among OD features
and DO features for modeling the OD-DO causality and correlation. Based on the
proposed HIAM, we develop a unified Seq2Seq network to forecast the future OD
and DO ridership simultaneously. Extensive experiments conducted on two
large-scale benchmarks demonstrate the effectiveness of our method for online
metro origin-destination prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingbo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuying Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guanbin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Ziyi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1"&gt;Lei Bai Liang Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalization Properties of Stochastic Optimizers via Trajectory Analysis. (arXiv:2108.00781v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.00781</id>
        <link href="http://arxiv.org/abs/2108.00781"/>
        <updated>2021-08-03T02:06:35.185Z</updated>
        <summary type="html"><![CDATA[Despite the ubiquitous use of stochastic optimization algorithms in machine
learning, the precise impact of these algorithms on generalization performance
in realistic non-convex settings is still poorly understood. In this paper, we
provide an encompassing theoretical framework for investigating the
generalization properties of stochastic optimizers, which is based on their
dynamics. We first prove a generalization bound attributable to the optimizer
dynamics in terms of the celebrated Fernique-Talagrand functional applied to
the trajectory of the optimizer. This data- and algorithm-dependent bound is
shown to be the sharpest possible in the absence of further assumptions. We
then specialize this result by exploiting the Markovian structure of stochastic
optimizers, deriving generalization bounds in terms of the (data-dependent)
transition kernels associated with the optimization algorithms. In line with
recent work that has revealed connections between generalization and
heavy-tailed behavior in stochastic optimization, we link the generalization
error to the local tail behavior of the transition kernels. We illustrate that
the local power-law exponent of the kernel acts as an effective dimension,
which decreases as the transitions become "less Gaussian". We support our
theory with empirical results from a variety of neural networks, and we show
that both the Fernique-Talagrand functional and the local power-law exponent
are predictive of generalization performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hodgkinson_L/0/1/0/all/0/1"&gt;Liam Hodgkinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Simsekli_U/0/1/0/all/0/1"&gt;Umut &amp;#x15e;im&amp;#x15f;ekli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Khanna_R/0/1/0/all/0/1"&gt;Rajiv Khanna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mahoney_M/0/1/0/all/0/1"&gt;Michael W. Mahoney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topological Information Retrieval with Dilation-Invariant Bottleneck Comparative Measures. (arXiv:2104.01672v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01672</id>
        <link href="http://arxiv.org/abs/2104.01672"/>
        <updated>2021-08-03T02:06:35.179Z</updated>
        <summary type="html"><![CDATA[Appropriately representing elements in a database so that queries may be
accurately matched is a central task in information retrieval; recently, this
has been achieved by embedding the graphical structure of the database into a
manifold in a hierarchy-preserving manner using a variety of metrics.
Persistent homology is a tool commonly used in topological data analysis that
is able to rigorously characterize a database in terms of both its hierarchy
and connectivity structure. Computing persistent homology on a variety of
embedded datasets reveals that some commonly used embeddings fail to preserve
the connectivity. We show that those embeddings which successfully retain the
database topology coincide in persistent homology by introducing two
dilation-invariant comparative measures to capture this effect: in particular,
they address the issue of metric distortion on manifolds. We provide an
algorithm for their computation that exhibits greatly reduced time complexity
over existing methods. We use these measures to perform the first instance of
topology-based information retrieval and demonstrate its increased performance
over the standard bottleneck distance for persistent homology. We showcase our
approach on databases of different data varieties including text, videos, and
medical images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Vlontzos_A/0/1/0/all/0/1"&gt;Athanasios Vlontzos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yueqi Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schmidtke_L/0/1/0/all/0/1"&gt;Luca Schmidtke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kainz_B/0/1/0/all/0/1"&gt;Bernhard Kainz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Monod_A/0/1/0/all/0/1"&gt;Anthea Monod&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tensor Clustering with Planted Structures: Statistical Optimality and Computational Limits. (arXiv:2005.10743v3 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.10743</id>
        <link href="http://arxiv.org/abs/2005.10743"/>
        <updated>2021-08-03T02:06:35.173Z</updated>
        <summary type="html"><![CDATA[This paper studies the statistical and computational limits of high-order
clustering with planted structures. We focus on two clustering models, constant
high-order clustering (CHC) and rank-one higher-order clustering (ROHC), and
study the methods and theory for testing whether a cluster exists (detection)
and identifying the support of cluster (recovery).

Specifically, we identify the sharp boundaries of signal-to-noise ratio for
which CHC and ROHC detection/recovery are statistically possible. We also
develop the tight computational thresholds: when the signal-to-noise ratio is
below these thresholds, we prove that polynomial-time algorithms cannot solve
these problems under the computational hardness conjectures of hypergraphic
planted clique (HPC) detection and hypergraphic planted dense subgraph (HPDS)
recovery. We also propose polynomial-time tensor algorithms that achieve
reliable detection and recovery when the signal-to-noise ratio is above these
thresholds. Both sparsity and tensor structures yield the computational
barriers in high-order tensor clustering. The interplay between them results in
significant differences between high-order tensor clustering and matrix
clustering in literature in aspects of statistical and computational phase
transition diagrams, algorithmic approaches, hardness conjecture, and proof
techniques. To our best knowledge, we are the first to give a thorough
characterization of the statistical and computational trade-off for such a
double computational-barrier problem. Finally, we provide evidence for the
computational hardness conjectures of HPC detection (via low-degree polynomial
and Metropolis methods) and HPDS recovery (via low-degree polynomial method).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yuetian Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Anru R. Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Strategies for convex potential games and an application to decision-theoretic online learning. (arXiv:2106.10717v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10717</id>
        <link href="http://arxiv.org/abs/2106.10717"/>
        <updated>2021-08-03T02:06:35.154Z</updated>
        <summary type="html"><![CDATA[The backwards induction method due to Bellman~\cite{bellman1952theory} is a
popular approach to solving problems in optimiztion, optimal control, and many
other areas of applied math. In this paper we analyze the backwords induction
approach, under min/max conditions. We show that if the value function is has
strictly positive derivatives of order 1-4 then the optimal strategy for the
adversary is Brownian motion. Using that fact we analyze different potential
functions and show that the Normal-Hedge potential is optimal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Freund_Y/0/1/0/all/0/1"&gt;Yoav Freund&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Transformer for Efficient Machine Translation on Embedded Devices. (arXiv:2107.08199v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08199</id>
        <link href="http://arxiv.org/abs/2107.08199"/>
        <updated>2021-08-03T02:06:35.148Z</updated>
        <summary type="html"><![CDATA[The Transformer architecture is widely used for machine translation tasks.
However, its resource-intensive nature makes it challenging to implement on
constrained embedded devices, particularly where available hardware resources
can vary at run-time. We propose a dynamic machine translation model that
scales the Transformer architecture based on the available resources at any
particular time. The proposed approach, 'Dynamic-HAT', uses a HAT
SuperTransformer as the backbone to search for SubTransformers with different
accuracy-latency trade-offs at design time. The optimal SubTransformers are
sampled from the SuperTransformer at run-time, depending on latency
constraints. The Dynamic-HAT is tested on the Jetson Nano and the approach uses
inherited SubTransformers sampled directly from the SuperTransformer with a
switching time of <1s. Using inherited SubTransformers results in a BLEU score
loss of <1.5% because the SubTransformer configuration is not retrained from
scratch after sampling. However, to recover this loss in performance, the
dimensions of the design space can be reduced to tailor it to a family of
target hardware. The new reduced design space results in a BLEU score increase
of approximately 1% for sub-optimal models from the original design space, with
a wide range for performance scaling between 0.356s - 1.526s for the GPU and
2.9s - 7.31s for the CPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parry_H/0/1/0/all/0/1"&gt;Hishan Parry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xun_L/0/1/0/all/0/1"&gt;Lei Xun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabet_A/0/1/0/all/0/1"&gt;Amin Sabet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1"&gt;Jia Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1"&gt;Jonathon Hare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merrett_G/0/1/0/all/0/1"&gt;Geoff V. Merrett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Certified Defense via Latent Space Randomized Smoothing with Orthogonal Encoders. (arXiv:2108.00491v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00491</id>
        <link href="http://arxiv.org/abs/2108.00491"/>
        <updated>2021-08-03T02:06:35.142Z</updated>
        <summary type="html"><![CDATA[Randomized Smoothing (RS), being one of few provable defenses, has been
showing great effectiveness and scalability in terms of defending against
$\ell_2$-norm adversarial perturbations. However, the cost of MC sampling
needed in RS for evaluation is high and computationally expensive. To address
this issue, we investigate the possibility of performing randomized smoothing
and establishing the robust certification in the latent space of a network, so
that the overall dimensionality of tensors involved in computation could be
drastically reduced. To this end, we propose Latent Space Randomized Smoothing.
Another important aspect is that we use orthogonal modules, whose Lipschitz
property is known for free by design, to propagate the certified radius
estimated in the latent space back to the input space, providing valid
certifiable regions for the test samples in the input space. Experiments on
CIFAR10 and ImageNet show that our method achieves competitive certified
robustness but with a significant improvement of efficiency during the test
phase.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1"&gt;Huimin Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1"&gt;Jiahao Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Furong Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lattice Paths for Persistent Diagrams. (arXiv:2105.00351v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00351</id>
        <link href="http://arxiv.org/abs/2105.00351"/>
        <updated>2021-08-03T02:06:35.136Z</updated>
        <summary type="html"><![CDATA[Persistent homology has undergone significant development in recent years.
However, one outstanding challenge is to build a coherent statistical inference
procedure on persistent diagrams. In this paper, we first present a new lattice
path representation for persistent diagrams. We then develop a new exact
statistical inference procedure for lattice paths via combinatorial
enumerations. The lattice path method is applied to the topological
characterization of the protein structures of the COVID-19 virus. We
demonstrate that there are topological changes during the conformational change
of spike proteins.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chung_M/0/1/0/all/0/1"&gt;Moo K. Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ombao_H/0/1/0/all/0/1"&gt;Hernando Ombao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zeroth-Order Alternating Randomized Gradient Projection Algorithms for General Nonconvex-Concave Minimax Problems. (arXiv:2108.00473v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2108.00473</id>
        <link href="http://arxiv.org/abs/2108.00473"/>
        <updated>2021-08-03T02:06:35.119Z</updated>
        <summary type="html"><![CDATA[In this paper, we study zeroth-order algorithms for nonconvex-concave minimax
problems, which have attracted widely attention in machine learning, signal
processing and many other fields in recent years. We propose a zeroth-order
alternating randomized gradient projection (ZO-AGP) algorithm for smooth
nonconvex-concave minimax problems, and its iteration complexity to obtain an
$\varepsilon$-stationary point is bounded by $\mathcal{O}(\varepsilon^{-4})$,
and the number of function value estimation is bounded by
$\mathcal{O}(d_{x}\varepsilon^{-4}+d_{y}\varepsilon^{-6})$ per iteration.
Moreover, we propose a zeroth-order block alternating randomized proximal
gradient algorithm (ZO-BAPG) for solving block-wise nonsmooth nonconvex-concave
minimax optimization problems, and the iteration complexity to obtain an
$\varepsilon$-stationary point is bounded by $\mathcal{O}(\varepsilon^{-4})$
and the number of function value estimation per iteration is bounded by
$\mathcal{O}(K d_{x}\varepsilon^{-4}+d_{y}\varepsilon^{-6})$. To the best of
our knowledge, this is the first time that zeroth-order algorithms with
iteration complexity gurantee are developed for solving both general smooth and
block-wise nonsmooth nonconvex-concave minimax problems. Numerical results on
data poisoning attack problem validate the efficiency of the proposed
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jingjing Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Dai_Y/0/1/0/all/0/1"&gt;Yuhong Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to synthesise the ageing brain without longitudinal data. (arXiv:1912.02620v5 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.02620</id>
        <link href="http://arxiv.org/abs/1912.02620"/>
        <updated>2021-08-03T02:06:35.113Z</updated>
        <summary type="html"><![CDATA[How will my face look when I get older? Or, for a more challenging question:
How will my brain look when I get older? To answer this question one must
devise (and learn from data) a multivariate auto-regressive function which
given an image and a desired target age generates an output image. While
collecting data for faces may be easier, collecting longitudinal brain data is
not trivial. We propose a deep learning-based method that learns to simulate
subject-specific brain ageing trajectories without relying on longitudinal
data. Our method synthesises images conditioned on two factors: age (a
continuous variable), and status of Alzheimer's Disease (AD, an ordinal
variable). With an adversarial formulation we learn the joint distribution of
brain appearance, age and AD status, and define reconstruction losses to
address the challenging problem of preserving subject identity. We compare with
several benchmarks using two widely used datasets. We evaluate the quality and
realism of synthesised images using ground-truth longitudinal data and a
pre-trained age predictor. We show that, despite the use of cross-sectional
data, our model learns patterns of gray matter atrophy in the middle temporal
gyrus in patients with AD. To demonstrate generalisation ability, we train on
one dataset and evaluate predictions on the other. In conclusion, our model
shows an ability to separate age, disease influence and anatomy using only 2D
cross-sectional data that should should be useful in large studies into
neurodegenerative disease, that aim to combine several data sources. To
facilitate such future studies by the community at large our code is made
available at https://github.com/xiat0616/BrainAgeing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xia_T/0/1/0/all/0/1"&gt;Tian Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chartsias_A/0/1/0/all/0/1"&gt;Agisilaos Chartsias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengjia Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1"&gt;Sotirios A. Tsaftaris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decision Making in Monopoly using a Hybrid Deep Reinforcement Learning Approach. (arXiv:2103.00683v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00683</id>
        <link href="http://arxiv.org/abs/2103.00683"/>
        <updated>2021-08-03T02:06:35.107Z</updated>
        <summary type="html"><![CDATA[Learning to adapt and make real-time informed decisions in a dynamic and
complex environment is a challenging problem. Monopoly is a popular strategic
board game that requires players to make multiple decisions during the game.
Decision-making in Monopoly involves many real-world elements such as
strategizing, luck, and modeling of opponent's policies. In this paper, we
present novel representations for the state and action space for the full
version of Monopoly and define an improved reward function. Using these, we
show that our deep reinforcement learning agent can learn winning strategies
for Monopoly against different fixed-policy agents. In Monopoly, players can
take multiple actions even if it is not their turn to roll the dice. Some of
these actions occur more frequently than others, resulting in a skewed
distribution that adversely affects the performance of the learning agent. To
tackle the non-uniform distribution of actions, we propose a hybrid approach
that combines deep reinforcement learning (for frequent but complex decisions)
with a fixed policy approach (for infrequent but straightforward decisions).
Experimental results show that our hybrid agent outperforms a standard deep
reinforcement learning agent by 30% in the number of games won against
fixed-policy agents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haliem_M/0/1/0/all/0/1"&gt;Marina Haliem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bonjour_T/0/1/0/all/0/1"&gt;Trevor Bonjour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alsalem_A/0/1/0/all/0/1"&gt;Aala Alsalem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1"&gt;Shilpa Thomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1"&gt;Vaneet Aggarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhargava_B/0/1/0/all/0/1"&gt;Bharat Bhargava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kejriwal_M/0/1/0/all/0/1"&gt;Mayank Kejriwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Helmholtzian Eigenmap: Topological feature discovery & edge flow learning from point cloud data. (arXiv:2103.07626v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07626</id>
        <link href="http://arxiv.org/abs/2103.07626"/>
        <updated>2021-08-03T02:06:35.101Z</updated>
        <summary type="html"><![CDATA[The manifold Helmholtzian (1-Laplacian) operator $\Delta_1$ elegantly
generalizes the Laplace-Beltrami operator to vector fields on a manifold
$\mathcal M$. In this work, we propose the estimation of the manifold
Helmholtzian from point cloud data by a weighted 1-Laplacian $\mathbf{\mathcal
L}_1$. While higher order Laplacians ave been introduced and studied, this work
is the first to present a graph Helmholtzian constructed from a simplicial
complex as an estimator for the continuous operator in a non-parametric
setting. Equipped with the geometric and topological information about
$\mathcal M$, the Helmholtzian is a useful tool for the analysis of flows and
vector fields on $\mathcal M$ via the Helmholtz-Hodge theorem. In addition, the
$\mathbf{\mathcal L}_1$ allows the smoothing, prediction, and feature
extraction of the flows. We demonstrate these possibilities on substantial sets
of synthetic and real point cloud datasets with non-trivial topological
structures; and provide theoretical results on the limit of $\mathbf{\mathcal
L}_1$ to $\Delta_1$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yu-Chia Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Meila_M/0/1/0/all/0/1"&gt;Marina Meil&amp;#x103;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kevrekidis_I/0/1/0/all/0/1"&gt;Ioannis G. Kevrekidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Convergence of Multi-Agent Policy Gradient in Markov Potential Games. (arXiv:2106.01969v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01969</id>
        <link href="http://arxiv.org/abs/2106.01969"/>
        <updated>2021-08-03T02:06:35.094Z</updated>
        <summary type="html"><![CDATA[Potential games are arguably one of the most important and widely studied
classes of normal form games. They define the archetypal setting of multi-agent
coordination as all agent utilities are perfectly aligned with each other via a
common potential function. Can this intuitive framework be transplanted in the
setting of Markov Games? What are the similarities and differences between
multi-agent coordination with and without state dependence? We present a novel
definition of Markov Potential Games (MPG) that generalizes prior attempts at
capturing complex stateful multi-agent coordination. Counter-intuitively,
insights from normal-form potential games do not carry over as MPGs can consist
of settings where state-games can be zero-sum games. In the opposite direction,
Markov games where every state-game is a potential game are not necessarily
MPGs. Nevertheless, MPGs showcase standard desirable properties such as the
existence of deterministic Nash policies. In our main technical result, we
prove fast convergence of independent policy gradient to Nash policies by
adapting recent gradient dominance property arguments developed for single
agent MDPs to multi-agent learning settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leonardos_S/0/1/0/all/0/1"&gt;Stefanos Leonardos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Overman_W/0/1/0/all/0/1"&gt;Will Overman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panageas_I/0/1/0/all/0/1"&gt;Ioannis Panageas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piliouras_G/0/1/0/all/0/1"&gt;Georgios Piliouras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Probabilistic Circuits for Nonparametric Multi-Output Regression. (arXiv:2106.08687v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08687</id>
        <link href="http://arxiv.org/abs/2106.08687"/>
        <updated>2021-08-03T02:06:35.088Z</updated>
        <summary type="html"><![CDATA[Inspired by recent advances in the field of expert-based approximations of
Gaussian processes (GPs), we present an expert-based approach to large-scale
multi-output regression using single-output GP experts. Employing a deeply
structured mixture of single-output GPs encoded via a probabilistic circuit
allows us to capture correlations between multiple output dimensions
accurately. By recursively partitioning the covariate space and the output
space, posterior inference in our model reduces to inference on single-output
GP experts, which only need to be conditioned on a small subset of the
observations. We show that inference can be performed exactly and efficiently
in our model, that it can capture correlations between output dimensions and,
hence, often outperforms approaches that do not incorporate inter-output
correlations, as demonstrated on several data sets in terms of the negative log
predictive density.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhongjie Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Mingye Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trapp_M/0/1/0/all/0/1"&gt;Martin Trapp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Skryagin_A/0/1/0/all/0/1"&gt;Arseny Skryagin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1"&gt;Kristian Kersting&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COfEE: A Comprehensive Ontology for Event Extraction from text, with an online annotation tool. (arXiv:2107.10326v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.10326</id>
        <link href="http://arxiv.org/abs/2107.10326"/>
        <updated>2021-08-03T02:06:35.069Z</updated>
        <summary type="html"><![CDATA[Data is published on the web over time in great volumes, but majority of the
data is unstructured, making it hard to understand and difficult to interpret.
Information Extraction (IE) methods extract structured information from
unstructured data. One of the challenging IE tasks is Event Extraction (EE)
which seeks to derive information about specific incidents and their actors
from the text. EE is useful in many domains such as building a knowledge base,
information retrieval, summarization and online monitoring systems. In the past
decades, some event ontologies like ACE, CAMEO and ICEWS were developed to
define event forms, actors and dimensions of events observed in the text. These
event ontologies still have some shortcomings such as covering only a few
topics like political events, having inflexible structure in defining argument
roles, lack of analytical dimensions, and complexity in choosing event
sub-types. To address these concerns, we propose an event ontology, namely
COfEE, that incorporates both expert domain knowledge, previous ontologies and
a data-driven approach for identifying events from text. COfEE consists of two
hierarchy levels (event types and event sub-types) that include new categories
relating to environmental issues, cyberspace, criminal activity and natural
disasters which need to be monitored instantly. Also, dynamic roles according
to each event sub-type are defined to capture various dimensions of events. In
a follow-up experiment, the proposed ontology is evaluated on Wikipedia events,
and it is shown to be general and comprehensive. Moreover, in order to
facilitate the preparation of gold-standard data for event extraction, a
language-independent online tool is presented based on COfEE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balali_A/0/1/0/all/0/1"&gt;Ali Balali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asadpour_M/0/1/0/all/0/1"&gt;Masoud Asadpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jafari_S/0/1/0/all/0/1"&gt;Seyed Hossein Jafari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Millimeter Wave Communications with an Intelligent Reflector: Performance Optimization and Distributional Reinforcement Learning. (arXiv:2002.10572v3 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.10572</id>
        <link href="http://arxiv.org/abs/2002.10572"/>
        <updated>2021-08-03T02:06:35.062Z</updated>
        <summary type="html"><![CDATA[In this paper, a novel framework is proposed to optimize the downlink
multi-user communication of a millimeter wave base station, which is assisted
by a reconfigurable intelligent reflector (IR). In particular, a channel
estimation approach is developed to measure the channel state information (CSI)
in real-time. First, for a perfect CSI scenario, the precoding transmission of
the BS and the reflection coefficient of the IR are jointly optimized, via an
iterative approach, so as to maximize the sum of downlink rates towards
multiple users. Next, in the imperfect CSI scenario, a distributional
reinforcement learning (DRL) approach is proposed to learn the optimal IR
reflection and maximize the expectation of downlink capacity. In order to model
the transmission rate's probability distribution, a learning algorithm, based
on quantile regression (QR), is developed, and the proposed QR-DRL method is
proved to converge to a stable distribution of downlink transmission rate.
Simulation results show that, in the error-free CSI scenario, the proposed
approach yields over 30% and 2-fold increase in the downlink sum-rate, compared
with a fixed IR reflection scheme and direct transmission scheme, respectively.
Simulation results also show that by deploying more IR elements, the downlink
sum-rate can be significantly improved. However, as the number of IR components
increases, more time is required for channel estimation, and the slope of
increase in the IR-aided transmission rate will become smaller. Furthermore,
under limited knowledge of CSI, simulation results show that the proposed
QR-DRL method, which learns a full distribution of the downlink rate, yields a
better prediction accuracy and improves the downlink rate by 10% for online
deployments, compared with a Q-learning baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qianqian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saad_W/0/1/0/all/0/1"&gt;Walid Saad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1"&gt;Mehdi Bennis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Learning for Recurrent Neural Networks: an Empirical Evaluation. (arXiv:2103.07492v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07492</id>
        <link href="http://arxiv.org/abs/2103.07492"/>
        <updated>2021-08-03T02:06:35.055Z</updated>
        <summary type="html"><![CDATA[Learning continuously during all model lifetime is fundamental to deploy
machine learning solutions robust to drifts in the data distribution. Advances
in Continual Learning (CL) with recurrent neural networks could pave the way to
a large number of applications where incoming data is non stationary, like
natural language processing and robotics. However, the existing body of work on
the topic is still fragmented, with approaches which are application-specific
and whose assessment is based on heterogeneous learning protocols and datasets.
In this paper, we organize the literature on CL for sequential data processing
by providing a categorization of the contributions and a review of the
benchmarks. We propose two new benchmarks for CL with sequential data based on
existing datasets, whose characteristics resemble real-world applications. We
also provide a broad empirical evaluation of CL and Recurrent Neural Networks
in class-incremental scenario, by testing their ability to mitigate forgetting
with a number of different strategies which are not specific to sequential data
processing. Our results highlight the key role played by the sequence length
and the importance of a clear specification of the CL scenario.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cossu_A/0/1/0/all/0/1"&gt;Andrea Cossu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carta_A/0/1/0/all/0/1"&gt;Antonio Carta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1"&gt;Vincenzo Lomonaco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1"&gt;Davide Bacciu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MSMatch: Semi-Supervised Multispectral Scene Classification with Few Labels. (arXiv:2103.10368v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10368</id>
        <link href="http://arxiv.org/abs/2103.10368"/>
        <updated>2021-08-03T02:06:35.048Z</updated>
        <summary type="html"><![CDATA[Supervised learning techniques are at the center of many tasks in remote
sensing. Unfortunately, these methods, especially recent deep learning methods,
often require large amounts of labeled data for training. Even though
satellites acquire large amounts of data, labeling the data is often tedious,
expensive and requires expert knowledge. Hence, improved methods that require
fewer labeled samples are needed. We present MSMatch, the first semi-supervised
learning approach competitive with supervised methods on scene classification
on the EuroSAT and UC Merced Land Use benchmark datasets. We test both RGB and
multispectral images of EuroSAT and perform various ablation studies to
identify the critical parts of the model. The trained neural network achieves
state-of-the-art results on EuroSAT with an accuracy that is up to 19.76%
better than previous methods depending on the number of labeled training
examples. With just five labeled examples per class, we reach 94.53% and 95.86%
accuracy on the EuroSAT RGB and multispectral datasets, respectively. On the UC
Merced Land Use dataset, we outperform previous works by up to 5.59% and reach
90.71% with five labeled examples. Our results show that MSMatch is capable of
greatly reducing the requirements for labeled data. It translates well to
multispectral data and should enable various applications that are currently
infeasible due to a lack of labeled data. We provide the source code of MSMatch
online to enable easy reproduction and quick adoption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_P/0/1/0/all/0/1"&gt;Pablo G&amp;#xf3;mez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meoni_G/0/1/0/all/0/1"&gt;Gabriele Meoni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Momentum-based Gradient Methods in Multi-Objective Recommendation. (arXiv:2009.04695v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04695</id>
        <link href="http://arxiv.org/abs/2009.04695"/>
        <updated>2021-08-03T02:06:35.040Z</updated>
        <summary type="html"><![CDATA[Multi-objective gradient methods are becoming the standard for solving
multi-objective problems. Among others, they show promising results in
developing multi-objective recommender systems with both correlated and
conflicting objectives. Classic multi-gradient descent usually relies on the
combination of the gradients, not including the computation of first and second
moments of the gradients. This leads to a brittle behavior and misses important
areas in the solution space. In this work, we create a multi-objective
model-agnostic Adamize method that leverages the benefits of the Adam optimizer
in single-objective problems. This corrects and stabilizes the gradients of
every objective before calculating a common gradient descent vector that
optimizes all the objectives simultaneously. We evaluate the benefits of
multi-objective Adamize on two multi-objective recommender systems and for
three different objective combinations, both correlated or conflicting. We
report significant improvements, measured with three different Pareto front
metrics: hypervolume, coverage, and spacing. Finally, we show that the Adamized
Pareto front strictly dominates the previous one on multiple objective pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mitrevski_B/0/1/0/all/0/1"&gt;Blagoj Mitrevski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filipovic_M/0/1/0/all/0/1"&gt;Milena Filipovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1"&gt;Diego Antognini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glaude_E/0/1/0/all/0/1"&gt;Emma Lejal Glaude&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1"&gt;Boi Faltings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1"&gt;Claudiu Musat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis. (arXiv:2107.08264v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08264</id>
        <link href="http://arxiv.org/abs/2107.08264"/>
        <updated>2021-08-03T02:06:35.025Z</updated>
        <summary type="html"><![CDATA[Multimodal sentiment analysis aims to recognize people's attitudes from
multiple communication channels such as verbal content (i.e., text), voice, and
facial expressions. It has become a vibrant and important research topic in
natural language processing. Much research focuses on modeling the complex
intra- and inter-modal interactions between different communication channels.
However, current multimodal models with strong performance are often
deep-learning-based techniques and work like black boxes. It is not clear how
models utilize multimodal information for sentiment predictions. Despite recent
advances in techniques for enhancing the explainability of machine learning
models, they often target unimodal scenarios (e.g., images, sentences), and
little research has been done on explaining multimodal models. In this paper,
we present an interactive visual analytics system, M2Lens, to visualize and
explain multimodal models for sentiment analysis. M2Lens provides explanations
on intra- and inter-modal interactions at the global, subset, and local levels.
Specifically, it summarizes the influence of three typical interaction types
(i.e., dominance, complement, and conflict) on the model predictions. Moreover,
M2Lens identifies frequent and influential multimodal features and supports the
multi-faceted exploration of model behaviors from language, acoustic, and
visual modalities. Through two case studies and expert interviews, we
demonstrate our system can help users gain deep insights into the multimodal
models for sentiment analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xingbo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jianben He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1"&gt;Zhihua Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Muqiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1"&gt;Huamin Qu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Undecidability of Learnability. (arXiv:2106.01382v2 [cs.CC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01382</id>
        <link href="http://arxiv.org/abs/2106.01382"/>
        <updated>2021-08-03T02:06:35.018Z</updated>
        <summary type="html"><![CDATA[Machine learning researchers and practitioners steadily enlarge the multitude
of successful learning models. They achieve this through in-depth theoretical
analyses and experiential heuristics. However, there is no known
general-purpose procedure for rigorously evaluating whether newly proposed
models indeed successfully learn from data. We show that such a procedure
cannot exist. For PAC binary classification, uniform and universal online
learning, and exact learning through teacher-learner interactions, learnability
is in general undecidable, both in the sense of independence of the axioms in a
formal system and in the sense of uncomputability. Our proofs proceed via
computable constructions of function classes that encode the consistency
problem for formal systems and the halting problem for Turing machines into
complexity measures that characterize learnability. Our work shows that
undecidability appears in the theoretical foundations of machine learning:
There is no one-size-fits-all algorithm for deciding whether a machine learning
model can be successful. We cannot in general automatize the process of
assessing new learning models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Caro_M/0/1/0/all/0/1"&gt;Matthias C. Caro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Tunnel Gaussian Process Model for Learning Interpretable Flight's Landing Parameters. (arXiv:2011.09335v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.09335</id>
        <link href="http://arxiv.org/abs/2011.09335"/>
        <updated>2021-08-03T02:06:35.012Z</updated>
        <summary type="html"><![CDATA[Approach and landing accidents have resulted in a significant number of hull
losses worldwide. Technologies (e.g., instrument landing system) and procedures
(e.g., stabilized approach criteria) have been developed to reduce the risks.
In this paper, we propose a data-driven method to learn and interpret flight's
approach and landing parameters to facilitate comprehensible and actionable
insights into flight dynamics. Specifically, we develop two variants of tunnel
Gaussian process (TGP) models to elucidate aircraft's approach and landing
dynamics using advanced surface movement guidance and control system (A-SMGCS)
data, which then indicates the stability of flight. TGP hybridizes the
strengths of sparse variational Gaussian process and polar Gaussian process to
learn from a large amount of data in cylindrical coordinates. We examine TGP
qualitatively and quantitatively by synthesizing three complex trajectory
datasets and compared TGP against existing methods on trajectory learning.
Empirically, TGP demonstrates superior modeling performance. When applied to
operational A-SMGCS data, TGP provides the generative probabilistic description
of landing dynamics and interpretable tunnel views of approach and landing
parameters. These probabilistic tunnel models can facilitate the analysis of
procedure adherence and augment existing aircrew and air traffic controllers'
displays during the approach and landing procedures, enabling necessary
corrective actions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goh_S/0/1/0/all/0/1"&gt;Sim Kuan Goh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1"&gt;Narendra Pratap Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_Z/0/1/0/all/0/1"&gt;Zhi Jun Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1"&gt;Sameer Alam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tensor completion using geodesics on Segre manifolds. (arXiv:2108.00735v1 [math.DG])]]></title>
        <id>http://arxiv.org/abs/2108.00735</id>
        <link href="http://arxiv.org/abs/2108.00735"/>
        <updated>2021-08-03T02:06:35.005Z</updated>
        <summary type="html"><![CDATA[We propose a Riemannian conjugate gradient (CG) optimization method for
finding low rank approximations of incomplete tensors. Our main contribution
consists of an explicit expression of the geodesics on the Segre manifold.
These are exploited in our algorithm to perform the retractions. We apply our
method to movie rating predictions in a recommender system for the MovieLens
dataset, and identification of pure fluorophores via fluorescent spectroscopy
with missing data. In this last application, we recover the tensor
decomposition from less than $10\%$ of the data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Swijsen_L/0/1/0/all/0/1"&gt;Lars Swijsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Veken_J/0/1/0/all/0/1"&gt;Joeri Van der Veken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Vannieuwenhoven_N/0/1/0/all/0/1"&gt;Nick Vannieuwenhoven&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimising cost vs accuracy of decentralised analytics in fog computing environments. (arXiv:2012.05266v3 [cs.DC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05266</id>
        <link href="http://arxiv.org/abs/2012.05266"/>
        <updated>2021-08-03T02:06:34.987Z</updated>
        <summary type="html"><![CDATA[The exponential growth of devices and data at the edges of the Internet is
rising scalability and privacy concerns on approaches based exclusively on
remote cloud platforms. Data gravity, a fundamental concept in Fog Computing,
points towards decentralisation of computation for data analysis, as a viable
alternative to address those concerns. Decentralising AI tasks on several
cooperative devices means identifying the optimal set of locations or
Collection Points (CP for short) to use, in the continuum between full
centralisation (i.e., all data on a single device) and full decentralisation
(i.e., data on source locations). We propose an analytical framework able to
find the optimal operating point in this continuum, linking the accuracy of the
learning task with the corresponding network and computational cost for moving
data and running the distributed training at the CPs. We show through
simulations that the model accurately predicts the optimal trade-off, quite
often an intermediate point between full centralisation and full
decentralisation, showing also a significant cost saving w.r.t. both of them.
Finally, the analytical model admits closed-form or numeric solutions, making
it not only a performance evaluation instrument but also a design tool to
configure a given distributed learning task optimally before its deployment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Valerio_L/0/1/0/all/0/1"&gt;Lorenzo Valerio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Passarella_A/0/1/0/all/0/1"&gt;Andrea Passarella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1"&gt;Marco Conti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Control an Unstable System with One Minute of Data: Leveraging Gaussian Process Differentiation in Predictive Control. (arXiv:2103.04548v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04548</id>
        <link href="http://arxiv.org/abs/2103.04548"/>
        <updated>2021-08-03T02:06:34.981Z</updated>
        <summary type="html"><![CDATA[We present a straightforward and efficient way to control unstable robotic
systems using an estimated dynamics model. Specifically, we show how to exploit
the differentiability of Gaussian Processes to create a state-dependent
linearized approximation of the true continuous dynamics that can be integrated
with model predictive control. Our approach is compatible with most Gaussian
process approaches for system identification, and can learn an accurate model
using modest amounts of training data. We validate our approach by learning the
dynamics of an unstable system such as a segway with a 7-D state space and 2-D
input space (using only one minute of data), and we show that the resulting
controller is robust to unmodelled dynamics and disturbances, while
state-of-the-art control methods based on nominal models can fail under small
perturbations. Code is open sourced at
https://github.com/learning-and-control/core .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_I/0/1/0/all/0/1"&gt;Ivan D. Jimenez Rodriguez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosolia_U/0/1/0/all/0/1"&gt;Ugo Rosolia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ames_A/0/1/0/all/0/1"&gt;Aaron D. Ames&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1"&gt;Yisong Yue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed Conditional Generative Adversarial Networks (GANs) for Data-Driven Millimeter Wave Communications in UAV Networks. (arXiv:2102.01751v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01751</id>
        <link href="http://arxiv.org/abs/2102.01751"/>
        <updated>2021-08-03T02:06:34.976Z</updated>
        <summary type="html"><![CDATA[In this paper, a novel framework is proposed to perform data-driven
air-to-ground (A2G) channel estimation for millimeter wave (mmWave)
communications in an unmanned aerial vehicle (UAV) wireless network. First, an
effective channel estimation approach is developed to collect mmWave channel
information, allowing each UAV to train a stand-alone channel model via a
conditional generative adversarial network (CGAN) along each beamforming
direction. Next, in order to expand the application scenarios of the trained
channel model into a broader spatial-temporal domain, a cooperative framework,
based on a distributed CGAN architecture, is developed, allowing each UAV to
collaboratively learn the mmWave channel distribution in a fully-distributed
manner. To guarantee an efficient learning process, necessary and sufficient
conditions for the optimal UAV network topology that maximizes the learning
rate for cooperative channel modeling are derived, and the optimal CGAN
learning solution per UAV is subsequently characterized, based on the
distributed network structure. Simulation results show that the proposed
distributed CGAN approach is robust to the local training error at each UAV.
Meanwhile, a larger airborne network size requires more communication resources
per UAV to guarantee an efficient learning rate. The results also show that,
compared with a stand-alone CGAN without information sharing and two other
distributed schemes, namely: A multi-discriminator CGAN and a federated CGAN
method, the proposed distributed CGAN approach yields a higher modeling
accuracy while learning the environment, and it achieves a larger average data
rate in the online performance of UAV downlink mmWave communications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qianqian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferdowsi_A/0/1/0/all/0/1"&gt;Aidin Ferdowsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saad_W/0/1/0/all/0/1"&gt;Walid Saad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1"&gt;Mehdi Bennis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gates are not what you need in RNNs. (arXiv:2108.00527v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00527</id>
        <link href="http://arxiv.org/abs/2108.00527"/>
        <updated>2021-08-03T02:06:34.968Z</updated>
        <summary type="html"><![CDATA[Recurrent neural networks have flourished in many areas. Consequently, we can
see new RNN cells being developed continuously, usually by creating or using
gates in a new, original way. But what if we told you that gates in RNNs are
redundant? In this paper, we propose a new recurrent cell called Residual
Recurrent Unit (RRU) which beats traditional cells and does not employ a single
gate. It is based on the residual shortcut connection together with linear
transformations, ReLU, and normalization. To evaluate our cell's effectiveness,
we compare its performance against the widely-used GRU and LSTM cells and the
recently proposed Mogrifier LSTM on several tasks including, polyphonic music
modeling, language modeling, and sentiment analysis. Our experiments show that
RRU outperforms the traditional gated units on most of these tasks. Also, it
has better robustness to parameter selection, allowing immediate application in
new tasks without much tuning. We have implemented the RRU in TensorFlow, and
the code is made available at https://github.com/LUMII-Syslab/RRU .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zakovskis_R/0/1/0/all/0/1"&gt;Ronalds Zakovskis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Draguns_A/0/1/0/all/0/1"&gt;Andis Draguns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaile_E/0/1/0/all/0/1"&gt;Eliza Gaile&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozolins_E/0/1/0/all/0/1"&gt;Emils Ozolins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freivalds_K/0/1/0/all/0/1"&gt;Karlis Freivalds&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Uniformly Consistent Estimator of non-Gaussian Causal Effects Under the k-Triangle-Faithfulness Assumption. (arXiv:2107.01333v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01333</id>
        <link href="http://arxiv.org/abs/2107.01333"/>
        <updated>2021-08-03T02:06:34.962Z</updated>
        <summary type="html"><![CDATA[Kalisch and B\"{u}hlmann (2007) showed that for linear Gaussian models, under
the Causal Markov Assumption, the Strong Causal Faithfulness Assumption, and
the assumption of causal sufficiency, the PC algorithm is a uniformly
consistent estimator of the Markov Equivalence Class of the true causal DAG for
linear Gaussian models; it follows from this that for the identifiable causal
effects in the Markov Equivalence Class, there are uniformly consistent
estimators of causal effects as well. The $k$-Triangle-Faithfulness Assumption
is a strictly weaker assumption that avoids some implausible implications of
the Strong Causal Faithfulness Assumption and also allows for uniformly
consistent estimates of Markov Equivalence Classes (in a weakened sense), and
of identifiable causal effects. However, both of these assumptions are
restricted to linear Gaussian models. We propose the Generalized $k$-Triangle
Faithfulness, which can be applied to any smooth distribution. In addition,
under the Generalized $k$-Triangle Faithfulness Assumption, we describe the
Edge Estimation Algorithm that provides uniformly consistent estimates of
causal effects in some cases (and otherwise outputs "can't tell"), and the
\textit{Very Conservative }$SGS$ Algorithm that (in a slightly weaker sense) is
a uniformly consistent estimator of the Markov equivalence class of the true
DAG.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuyan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Spirtes_P/0/1/0/all/0/1"&gt;Peter Spirtes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pathwise Conditioning of Gaussian Processes. (arXiv:2011.04026v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04026</id>
        <link href="http://arxiv.org/abs/2011.04026"/>
        <updated>2021-08-03T02:06:34.956Z</updated>
        <summary type="html"><![CDATA[As Gaussian processes are used to answer increasingly complex questions,
analytic solutions become scarcer and scarcer. Monte Carlo methods act as a
convenient bridge for connecting intractable mathematical expressions with
actionable estimates via sampling. Conventional approaches for simulating
Gaussian process posteriors view samples as draws from marginal distributions
of process values at finite sets of input locations. This distribution-centric
characterization leads to generative strategies that scale cubically in the
size of the desired random vector. These methods are prohibitively expensive in
cases where we would, ideally, like to draw high-dimensional vectors or even
continuous sample paths. In this work, we investigate a different line of
reasoning: rather than focusing on distributions, we articulate Gaussian
conditionals at the level of random variables. We show how this pathwise
interpretation of conditioning gives rise to a general family of approximations
that lend themselves to efficiently sampling Gaussian process posteriors.
Starting from first principles, we derive these methods and analyze the
approximation errors they introduce. We, then, ground these results by
exploring the practical implications of pathwise conditioning in various
applied settings, such as global optimization and reinforcement learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wilson_J/0/1/0/all/0/1"&gt;James T. Wilson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Borovitskiy_V/0/1/0/all/0/1"&gt;Viacheslav Borovitskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Terenin_A/0/1/0/all/0/1"&gt;Alexander Terenin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mostowsky_P/0/1/0/all/0/1"&gt;Peter Mostowsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Deisenroth_M/0/1/0/all/0/1"&gt;Marc Peter Deisenroth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vision Xformers: Efficient Attention for Image Classification. (arXiv:2107.02239v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02239</id>
        <link href="http://arxiv.org/abs/2107.02239"/>
        <updated>2021-08-03T02:06:34.939Z</updated>
        <summary type="html"><![CDATA[We propose three improvements to vision transformers (ViT) to reduce the
number of trainable parameters without compromising classification accuracy. We
address two shortcomings of the early ViT architectures -- quadratic bottleneck
of the attention mechanism and the lack of an inductive bias in their
architectures that rely on unrolling the two-dimensional image structure.
Linear attention mechanisms overcome the bottleneck of quadratic complexity,
which restricts application of transformer models in vision tasks. We modify
the ViT architecture to work on longer sequence data by replacing the quadratic
attention with efficient transformers, such as Performer, Linformer and
Nystr\"omformer of linear complexity creating Vision X-formers (ViX). We show
that all three versions of ViX may be more accurate than ViT for image
classification while using far fewer parameters and computational resources. We
also compare their performance with FNet and multi-layer perceptron (MLP)
mixer. We further show that replacing the initial linear embedding layer by
convolutional layers in ViX further increases their performance. Furthermore,
our tests on recent vision transformer models, such as LeViT, Convolutional
vision Transformer (CvT), Compact Convolutional Transformer (CCT) and
Pooling-based Vision Transformer (PiT) show that replacing the attention with
Nystr\"omformer or Performer saves GPU usage and memory without deteriorating
the classification accuracy. We also show that replacing the standard learnable
1D position embeddings in ViT with Rotary Position Embedding (RoPE) give
further improvements in accuracy. Incorporating these changes can democratize
transformers by making them accessible to those with limited data and computing
resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jeevan_P/0/1/0/all/0/1"&gt;Pranav Jeevan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sethi_A/0/1/0/all/0/1"&gt;Amit Sethi&lt;/a&gt; (Indian Institute of Technology Bombay)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[QuantumNAS: Noise-Adaptive Search for Robust Quantum Circuits. (arXiv:2107.10845v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.10845</id>
        <link href="http://arxiv.org/abs/2107.10845"/>
        <updated>2021-08-03T02:06:34.932Z</updated>
        <summary type="html"><![CDATA[Quantum noise is the key challenge in Noisy Intermediate-Scale Quantum (NISQ)
computers. Previous work for mitigating noise has primarily focused on
gate-level or pulse-level noise-adaptive compilation. However, limited research
efforts have explored a higher level of optimization by making the quantum
circuits themselves resilient to noise.

We propose QuantumNAS, a comprehensive framework for noise-adaptive co-search
of the variational circuit and qubit mapping. Variational quantum circuits are
a promising approach for constructing QML and quantum simulation. However,
finding the best variational circuit and its optimal parameters is challenging
due to the large design space and parameter training cost. We propose to
decouple the circuit search and parameter training by introducing a novel
SuperCircuit. The SuperCircuit is constructed with multiple layers of
pre-defined parameterized gates and trained by iteratively sampling and
updating the parameter subsets (SubCircuits) of it. It provides an accurate
estimation of SubCircuits performance trained from scratch. Then we perform an
evolutionary co-search of SubCircuit and its qubit mapping. The SubCircuit
performance is estimated with parameters inherited from SuperCircuit and
simulated with real device noise models. Finally, we perform iterative gate
pruning and finetuning to remove redundant gates.

Extensively evaluated with 12 QML and VQE benchmarks on 10 quantum comput,
QuantumNAS significantly outperforms baselines. For QML, QuantumNAS is the
first to demonstrate over 95% 2-class, 85% 4-class, and 32% 10-class
classification accuracy on real QC. It also achieves the lowest eigenvalue for
VQE tasks on H2, H2O, LiH, CH4, BeH2 compared with UCCSD. We also open-source
QuantumEngine (https://github.com/mit-han-lab/pytorch-quantum) for fast
training of parameterized quantum circuits to facilitate future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hanrui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yongshan Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jiaqi Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yujun Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Pan_D/0/1/0/all/0/1"&gt;David Z. Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Chong_F/0/1/0/all/0/1"&gt;Frederic T. Chong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Han_S/0/1/0/all/0/1"&gt;Song Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Eager Splitting Strategy for Online Decision Trees. (arXiv:2010.10935v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10935</id>
        <link href="http://arxiv.org/abs/2010.10935"/>
        <updated>2021-08-03T02:06:34.925Z</updated>
        <summary type="html"><![CDATA[Decision tree ensembles are widely used in practice. In this work, we study
in ensemble settings the effectiveness of replacing the split strategy for the
state-of-the-art online tree learner, Hoeffding Tree, with a rigorous but more
eager splitting strategy that we had previously published as Hoeffding AnyTime
Tree. Hoeffding AnyTime Tree (HATT), uses the Hoeffding Test to determine
whether the current best candidate split is superior to the current split, with
the possibility of revision, while Hoeffding Tree aims to determine whether the
top candidate is better than the second best and if a test is selected, fixes
it for all posterity. HATT converges to the ideal batch tree while Hoeffding
Tree does not. We find that HATT is an efficacious base learner for online
bagging and online boosting ensembles. On UCI and synthetic streams, HATT as a
base learner outperforms HT within a 0.05 significance level for the majority
of tested ensembles on what we believe is the largest and most comprehensive
set of testbenches in the online learning literature. Our results indicate that
HATT is a superior alternative to Hoeffding Tree in a large number of ensemble
settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Manapragada_C/0/1/0/all/0/1"&gt;Chaitanya Manapragada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomes_H/0/1/0/all/0/1"&gt;Heitor M Gomes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1"&gt;Mahsa Salehi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bifet_A/0/1/0/all/0/1"&gt;Albert Bifet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Webb_G/0/1/0/all/0/1"&gt;Geoffrey I Webb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[i-Pulse: A NLP based novel approach for employee engagement in logistics organization. (arXiv:2106.07341v1 [cs.SI] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2106.07341</id>
        <link href="http://arxiv.org/abs/2106.07341"/>
        <updated>2021-08-03T02:06:34.897Z</updated>
        <summary type="html"><![CDATA[Although most logistics and freight forwarding organizations, in one way or
another, claim to have core values. The engagement of employees is a vast
structure that affects almost every part of the company's core environmental
values. There is little theoretical knowledge about the relationship between
firms and the engagement of employees. Based on research literature, this paper
aims to provide a novel approach for insight around employee engagement in a
logistics organization by implementing deep natural language processing
concepts. The artificial intelligence-enabled solution named Intelligent Pulse
(I-Pulse) can evaluate hundreds and thousands of pulse survey comments and
provides the actionable insights and gist of employee feedback. I-Pulse allows
the stakeholders to think in new ways in their organization, helping them to
have a powerful influence on employee engagement, retention, and efficiency.
This study is of corresponding interest to researchers and practitioners.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garg_R/0/1/0/all/0/1"&gt;Rachit Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiwelekar_A/0/1/0/all/0/1"&gt;Arvind W Kiwelekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Netak_L/0/1/0/all/0/1"&gt;Laxman D Netak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghodake_A/0/1/0/all/0/1"&gt;Akshay Ghodake&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rationally Inattentive Utility Maximization for Interpretable Deep Image Classification. (arXiv:2102.04594v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04594</id>
        <link href="http://arxiv.org/abs/2102.04594"/>
        <updated>2021-08-03T02:06:34.896Z</updated>
        <summary type="html"><![CDATA[Are deep convolutional neural networks (CNNs) for image classification
explainable by utility maximization with information acquisition costs? We
demonstrate that deep CNNs behave equivalently (in terms of necessary and
sufficient conditions) to rationally inattentive utility maximizers, a
generative model used extensively in economics for human decision making. Our
claim is based by extensive experiments on 200 deep CNNs from 5 popular
architectures. The parameters of our interpretable model are computed
efficiently via convex feasibility algorithms. As an application, we show that
our economics-based interpretable model can predict the classification
performance of deep CNNs trained with arbitrary parameters with accuracy
exceeding 94% . This eliminates the need to re-train the deep CNNs for image
classification. The theoretical foundation of our approach lies in Bayesian
revealed preference studied in micro-economics. All our results are on GitHub
and completely reproducible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pattanayak_K/0/1/0/all/0/1"&gt;Kunal Pattanayak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_V/0/1/0/all/0/1"&gt;Vikram Krishnamurthy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How COVID-19 Has Changed Crowdfunding: Evidence From GoFundMe. (arXiv:2106.09981v2 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09981</id>
        <link href="http://arxiv.org/abs/2106.09981"/>
        <updated>2021-08-03T02:06:34.896Z</updated>
        <summary type="html"><![CDATA[While the long-term effects of COVID-19 are yet to be determined, its
immediate impact on crowdfunding is nonetheless significant. This study takes a
computational approach to more deeply comprehend this change. Using a unique
data set of all the campaigns published over the past two years on GoFundMe, we
explore the factors that have led to the successful funding of a crowdfunding
project. In particular, we study a corpus of crowdfunded projects, analyzing
cover images and other variables commonly present on crowdfunding sites.
Furthermore, we construct a classifier and a regression model to assess the
significance of features based on XGBoost. In addition, we employ
counterfactual analysis to investigate the causality between features and the
success of crowdfunding. More importantly, sentiment analysis and the paired
sample t-test are performed to examine the differences in crowdfunding
campaigns before and after the COVID-19 outbreak that started in March 2020.
First, we note that there is significant racial disparity in crowdfunding
success. Second, we find that sad emotion expressed through the campaign's
description became significant after the COVID-19 outbreak. Considering all
these factors, our findings shed light on the impact of COVID-19 on
crowdfunding campaigns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Junda Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xupin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jiebo Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Algorithms for Learning Depth-2 Neural Networks with General ReLU Activations. (arXiv:2107.10209v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.10209</id>
        <link href="http://arxiv.org/abs/2107.10209"/>
        <updated>2021-08-03T02:06:34.896Z</updated>
        <summary type="html"><![CDATA[We present polynomial time and sample efficient algorithms for learning an
unknown depth-2 feedforward neural network with general ReLU activations, under
mild non-degeneracy assumptions. In particular, we consider learning an unknown
network of the form $f(x) = {a}^{\mathsf{T}}\sigma({W}^\mathsf{T}x+b)$, where
$x$ is drawn from the Gaussian distribution, and $\sigma(t) := \max(t,0)$ is
the ReLU activation. Prior works for learning networks with ReLU activations
assume that the bias $b$ is zero. In order to deal with the presence of the
bias terms, our proposed algorithm consists of robustly decomposing multiple
higher order tensors arising from the Hermite expansion of the function $f(x)$.
Using these ideas we also establish identifiability of the network parameters
under minimal assumptions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Awasthi_P/0/1/0/all/0/1"&gt;Pranjal Awasthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_A/0/1/0/all/0/1"&gt;Alex Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vijayaraghavan_A/0/1/0/all/0/1"&gt;Aravindan Vijayaraghavan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring the social influence of Kaggle virtual community on the M5 competition. (arXiv:2103.00501v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00501</id>
        <link href="http://arxiv.org/abs/2103.00501"/>
        <updated>2021-08-03T02:06:34.895Z</updated>
        <summary type="html"><![CDATA[One of the most significant differences of M5 over previous forecasting
competitions is that it was held on Kaggle, an online platform of data
scientists and machine learning practitioners. Kaggle provides a gathering
place, or virtual community, for web users who are interested in the M5
competition. Users can share code, models, features, loss functions, etc.
through online notebooks and discussion forums. This paper aims to study the
social influence of virtual community on user behaviors in the M5 competition.
We first research the content of the M5 virtual community by topic modeling and
trend analysis. Further, we perform social media analysis to identify the
potential relationship network of the virtual community. We study the roles and
characteristics of some key participants that promote the diffusion of
information within the M5 virtual community. Overall, this study provides
in-depth insights into the mechanism of the virtual community's influence on
the participants and has potential implications for future online competitions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xixi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yun Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1"&gt;Yanfei Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gait Characterization in Duchenne Muscular Dystrophy (DMD) Using a Single-Sensor Accelerometer: Classical Machine Learning and Deep Learning Approaches. (arXiv:2105.06295v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06295</id>
        <link href="http://arxiv.org/abs/2105.06295"/>
        <updated>2021-08-03T02:06:34.895Z</updated>
        <summary type="html"><![CDATA[Differences in gait patterns of children with Duchenne muscular dystrophy
(DMD) and typically developing (TD) peers are visible to the eye, but
quantification of those differences outside of the gait laboratory has been
elusive. We measured vertical, mediolateral, and anteroposterior acceleration
using a waist-worn iPhone accelerometer during ambulation across a typical
range of velocities. Six TD and six DMD children from 3-15 years of age
underwent seven walking/running tasks, including five 25m walk/run tests at a
slow walk to running speeds, a 6-minute walk test (6MWT), and a
100-meter-run/walk (100MRW). We extracted temporospatial clinical gait features
(CFs) and applied multiple Artificial Intelligence (AI) tools to differentiate
between DMD and TD control children using extracted features and raw data.
Extracted CFs showed reduced step length and a greater mediolateral component
of total power (TP) consistent with shorter strides and Trendelenberg-like gait
commonly observed in DMD. AI methods using CFs and raw data varied
ineffectiveness at differentiating between DMD and TD controls at different
speeds, with an accuracy of some methods exceeding 91%. We demonstrate that by
using AI tools with accelerometer data from a consumer-level smartphone, we can
identify DMD gait disturbance in toddlers to early teens.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ramli_A/0/1/0/all/0/1"&gt;Albara Ah Ramli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huanle Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hou_J/0/1/0/all/0/1"&gt;Jiahui Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_R/0/1/0/all/0/1"&gt;Rex Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nicorici_A/0/1/0/all/0/1"&gt;Alina Nicorici&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Aranki_D/0/1/0/all/0/1"&gt;Daniel Aranki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Owens_C/0/1/0/all/0/1"&gt;Corey Owens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prasad_P/0/1/0/all/0/1"&gt;Poonam Prasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+McDonald_C/0/1/0/all/0/1"&gt;Craig McDonald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Henricson_E/0/1/0/all/0/1"&gt;Erik Henricson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DTGAN: Differential Private Training for Tabular GANs. (arXiv:2107.02521v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02521</id>
        <link href="http://arxiv.org/abs/2107.02521"/>
        <updated>2021-08-03T02:06:34.895Z</updated>
        <summary type="html"><![CDATA[Tabular generative adversarial networks (TGAN) have recently emerged to cater
to the need of synthesizing tabular data -- the most widely used data format.
While synthetic tabular data offers the advantage of complying with privacy
regulations, there still exists a risk of privacy leakage via inference attacks
due to interpolating the properties of real data during training. Differential
private (DP) training algorithms provide theoretical guarantees for training
machine learning models by injecting statistical noise to prevent privacy
leaks. However, the challenges of applying DP on TGAN are to determine the most
optimal framework (i.e., PATE/DP-SGD) and neural network (i.e.,
Generator/Discriminator)to inject noise such that the data utility is well
maintained under a given privacy guarantee. In this paper, we propose DTGAN, a
novel conditional Wasserstein tabular GAN that comes in two variants DTGAN_G
and DTGAN_D, for providing a detailed comparison of tabular GANs trained using
DP-SGD for the generator vs discriminator, respectively. We elicit the privacy
analysis associated with training the generator with complex loss functions
(i.e., classification and information losses) needed for high quality tabular
data synthesis. Additionally, we rigorously evaluate the theoretical privacy
guarantees offered by DP empirically against membership and attribute inference
attacks. Our results on 3 datasets show that the DP-SGD framework is superior
to PATE and that a DP discriminator is more optimal for training convergence.
Thus, we find (i) DTGAN_D is capable of maintaining the highest data utility
across 4 ML models by up to 18% in terms of the average precision score for a
strict privacy budget, epsilon = 1, as compared to the prior studies and (ii)
DP effectively prevents privacy loss against inference attacks by restricting
the success probability of membership attacks to be close to 50%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kunar_A/0/1/0/all/0/1"&gt;Aditya Kunar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Birke_R/0/1/0/all/0/1"&gt;Robert Birke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zilong Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lydia Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MugRep: A Multi-Task Hierarchical Graph Representation Learning Framework for Real Estate Appraisal. (arXiv:2107.05180v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05180</id>
        <link href="http://arxiv.org/abs/2107.05180"/>
        <updated>2021-08-03T02:06:34.895Z</updated>
        <summary type="html"><![CDATA[Real estate appraisal refers to the process of developing an unbiased opinion
for real property's market value, which plays a vital role in decision-making
for various players in the marketplace (e.g., real estate agents, appraisers,
lenders, and buyers). However, it is a nontrivial task for accurate real estate
appraisal because of three major challenges: (1) The complicated influencing
factors for property value; (2) The asynchronously spatiotemporal dependencies
among real estate transactions; (3) The diversified correlations between
residential communities. To this end, we propose a Multi-Task Hierarchical
Graph Representation Learning (MugRep) framework for accurate real estate
appraisal. Specifically, by acquiring and integrating multi-source urban data,
we first construct a rich feature set to comprehensively profile the real
estate from multiple perspectives (e.g., geographical distribution, human
mobility distribution, and resident demographics distribution). Then, an
evolving real estate transaction graph and a corresponding event graph
convolution module are proposed to incorporate asynchronously spatiotemporal
dependencies among real estate transactions. Moreover, to further incorporate
valuable knowledge from the view of residential communities, we devise a
hierarchical heterogeneous community graph convolution module to capture
diversified correlations between residential communities. Finally, an urban
district partitioned multi-task learning module is introduced to generate
differently distributed value opinions for real estate. Extensive experiments
on two real-world datasets demonstrate the effectiveness of MugRep and its
components and features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weijia Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_L/0/1/0/all/0/1"&gt;Lijun Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hengshu Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Ji Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1"&gt;Dejing Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Hui Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing User' s Income Estimation with Super-App Alternative Data. (arXiv:2104.05831v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05831</id>
        <link href="http://arxiv.org/abs/2104.05831"/>
        <updated>2021-08-03T02:06:34.894Z</updated>
        <summary type="html"><![CDATA[This paper presents the advantages of alternative data from Super-Apps to
enhance user' s income estimation models. It compares the performance of these
alternative data sources with the performance of industry-accepted bureau
income estimators that takes into account only financial system information;
successfully showing that the alternative data manage to capture information
that bureau income estimators do not. By implementing the TreeSHAP method for
Stochastic Gradient Boosting Interpretation, this paper highlights which of the
customer' s behavioral and transactional patterns within a Super-App have a
stronger predictive power when estimating user' s income. Ultimately, this
paper shows the incentive for financial institutions to seek to incorporate
alternative data into constructing their risk profiles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suarez_G/0/1/0/all/0/1"&gt;Gabriel Suarez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raful_J/0/1/0/all/0/1"&gt;Juan Raful&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luque_M/0/1/0/all/0/1"&gt;Maria A. Luque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valencia_C/0/1/0/all/0/1"&gt;Carlos F. Valencia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Correa_Bahnsen_A/0/1/0/all/0/1"&gt;Alejandro Correa-Bahnsen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Learning of Visual Features by Non-Parametrically Predicting View Assignments with Support Samples. (arXiv:2104.13963v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13963</id>
        <link href="http://arxiv.org/abs/2104.13963"/>
        <updated>2021-08-03T02:06:34.894Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel method of learning by predicting view assignments
with support samples (PAWS). The method trains a model to minimize a
consistency loss, which ensures that different views of the same unlabeled
instance are assigned similar pseudo-labels. The pseudo-labels are generated
non-parametrically, by comparing the representations of the image views to
those of a set of randomly sampled labeled images. The distance between the
view representations and labeled representations is used to provide a weighting
over class labels, which we interpret as a soft pseudo-label. By
non-parametrically incorporating labeled samples in this way, PAWS extends the
distance-metric loss used in self-supervised methods such as BYOL and SwAV to
the semi-supervised setting. Despite the simplicity of the approach, PAWS
outperforms other semi-supervised methods across architectures, setting a new
state-of-the-art for a ResNet-50 on ImageNet trained with either 10% or 1% of
the labels, reaching 75.5% and 66.5% top-1 respectively. PAWS requires 4x to
12x less training than the previous best methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Assran_M/0/1/0/all/0/1"&gt;Mahmoud Assran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1"&gt;Mathilde Caron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1"&gt;Ishan Misra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bojanowski_P/0/1/0/all/0/1"&gt;Piotr Bojanowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1"&gt;Armand Joulin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ballas_N/0/1/0/all/0/1"&gt;Nicolas Ballas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1"&gt;Michael Rabbat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning with Unreliable Clients: Performance Analysis and Mechanism Design. (arXiv:2105.06256v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06256</id>
        <link href="http://arxiv.org/abs/2105.06256"/>
        <updated>2021-08-03T02:06:34.892Z</updated>
        <summary type="html"><![CDATA[Owing to the low communication costs and privacy-promoting capabilities,
Federated Learning (FL) has become a promising tool for training effective
machine learning models among distributed clients. However, with the
distributed architecture, low quality models could be uploaded to the
aggregator server by unreliable clients, leading to a degradation or even a
collapse of training. In this paper, we model these unreliable behaviors of
clients and propose a defensive mechanism to mitigate such a security risk.
Specifically, we first investigate the impact on the models caused by
unreliable clients by deriving a convergence upper bound on the loss function
based on the gradient descent updates. Our theoretical bounds reveal that with
a fixed amount of total computational resources, there exists an optimal number
of local training iterations in terms of convergence performance. We further
design a novel defensive mechanism, named deep neural network based secure
aggregation (DeepSA). Our experimental results validate our theoretical
analysis. In addition, the effectiveness of DeepSA is verified by comparing
with other state-of-the-art defensive mechanisms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1"&gt;Chuan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1"&gt;Ming Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1"&gt;Kang Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1"&gt;H. Vincent Poor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GPU Accelerated Exhaustive Search for Optimal Ensemble of Black-Box Optimization Algorithms. (arXiv:2012.04201v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04201</id>
        <link href="http://arxiv.org/abs/2012.04201"/>
        <updated>2021-08-03T02:06:34.876Z</updated>
        <summary type="html"><![CDATA[Black-box optimization is essential for tuning complex machine learning
algorithms which are easier to experiment with than to understand. In this
paper, we show that a simple ensemble of black-box optimization algorithms can
outperform any single one of them. However, searching for such an optimal
ensemble requires a large number of experiments. We propose a
Multi-GPU-optimized framework to accelerate a brute force search for the
optimal ensemble of black-box optimization algorithms by running many
experiments in parallel. The lightweight optimizations are performed by CPU
while expensive model training and evaluations are assigned to GPUs. We
evaluate 15 optimizers by training 2.7 million models and running 541,440
optimizations. On a DGX-1, the search time is reduced from more than 10 days on
two 20-core CPUs to less than 24 hours on 8-GPUs. With the optimal ensemble
found by GPU-accelerated exhaustive search, we won the 2nd place of NeurIPS
2020 black-box optimization challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tunguz_B/0/1/0/all/0/1"&gt;Bojan Tunguz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Titericz_G/0/1/0/all/0/1"&gt;Gilberto Titericz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge Graph-based Question Answering with Electronic Health Records. (arXiv:2010.09394v2 [cs.DB] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09394</id>
        <link href="http://arxiv.org/abs/2010.09394"/>
        <updated>2021-08-03T02:06:34.868Z</updated>
        <summary type="html"><![CDATA[Question Answering (QA) is a widely-used framework for developing and
evaluating an intelligent machine. In this light, QA on Electronic Health
Records (EHR), namely EHR QA, can work as a crucial milestone towards
developing an intelligent agent in healthcare. EHR data are typically stored in
a relational database, which can also be converted to a directed acyclic graph,
allowing two approaches for EHR QA: Table-based QA and Knowledge Graph-based
QA. We hypothesize that the graph-based approach is more suitable for EHR QA as
graphs can represent relations between entities and values more naturally
compared to tables, which essentially require JOIN operations. In this paper,
we propose a graph-based EHR QA where natural language queries are converted to
SPARQL instead of SQL. To validate our hypothesis, we create four EHR QA
datasets (graph-based VS table-based, and simplified database schema VS
original database schema), based on a table-based dataset MIMICSQL. We test
both a simple Seq2Seq model and a state-of-the-art EHR QA model on all datasets
where the graph-based datasets facilitated up to 34% higher accuracy than the
table-based dataset without any modification to the model architectures.
Finally, all datasets are open-sourced to encourage further EHR QA research in
both directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Junwoo Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1"&gt;Youngwoo Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Haneol Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1"&gt;Jaegul Choo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1"&gt;Edward Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting Video Captioning with Dynamic Loss Network. (arXiv:2107.11707v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.11707</id>
        <link href="http://arxiv.org/abs/2107.11707"/>
        <updated>2021-08-03T02:06:34.862Z</updated>
        <summary type="html"><![CDATA[Video captioning is one of the challenging problems at the intersection of
vision and language, having many real-life applications in video retrieval,
video surveillance, assisting visually challenged people, Human-machine
interface, and many more. Recent deep learning-based methods have shown
promising results but are still on the lower side than other vision tasks (such
as image classification, object detection). A significant drawback with
existing video captioning methods is that they are optimized over cross-entropy
loss function, which is uncorrelated to the de facto evaluation metrics (BLEU,
METEOR, CIDER, ROUGE).In other words, cross-entropy is not a proper surrogate
of the true loss function for video captioning. This paper addresses the
drawback by introducing a dynamic loss network (DLN), which provides an
additional feedback signal that directly reflects the evaluation metrics. Our
results on Microsoft Research Video Description Corpus (MSVD) and MSR-Video to
Text (MSRVTT) datasets outperform previous methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nasibullah/0/1/0/all/0/1"&gt;Nasibullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohanta_P/0/1/0/all/0/1"&gt;Partha Pratim Mohanta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Filtering in tractography using autoencoders (FINTA). (arXiv:2010.04007v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04007</id>
        <link href="http://arxiv.org/abs/2010.04007"/>
        <updated>2021-08-03T02:06:34.855Z</updated>
        <summary type="html"><![CDATA[Current brain white matter fiber tracking techniques show a number of
problems, including: generating large proportions of streamlines that do not
accurately describe the underlying anatomy; extracting streamlines that are not
supported by the underlying diffusion signal; and under-representing some fiber
populations, among others. In this paper, we describe a novel autoencoder-based
learning method to filter streamlines from diffusion MRI tractography, and
hence, to obtain more reliable tractograms. Our method, dubbed FINTA (Filtering
in Tractography using Autoencoders) uses raw, unlabeled tractograms to train
the autoencoder, and to learn a robust representation of brain streamlines.
Such an embedding is then used to filter undesired streamline samples using a
nearest neighbor algorithm. Our experiments on both synthetic and in vivo human
brain diffusion MRI tractography data obtain accuracy scores exceeding the 90\%
threshold on the test set. Results reveal that FINTA has a superior filtering
performance compared to conventional, anatomy-based methods, and the
RecoBundles state-of-the-art method. Additionally, we demonstrate that FINTA
can be applied to partial tractograms without requiring changes to the
framework. We also show that the proposed method generalizes well across
different tracking methods and datasets, and shortens significantly the
computation time for large (>1 M streamlines) tractograms. Together, this work
brings forward a new deep learning framework in tractography based on
autoencoders, which offers a flexible and powerful method for white matter
filtering and bundling that could enhance tractometry and connectivity
analyses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Legarreta_J/0/1/0/all/0/1"&gt;Jon Haitz Legarreta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Petit_L/0/1/0/all/0/1"&gt;Laurent Petit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rheault_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Rheault&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Theaud_G/0/1/0/all/0/1"&gt;Guillaume Theaud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lemaire_C/0/1/0/all/0/1"&gt;Carl Lemaire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Descoteaux_M/0/1/0/all/0/1"&gt;Maxime Descoteaux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jodoin_P/0/1/0/all/0/1"&gt;Pierre-Marc Jodoin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-Guided Deep Learning for Dynamical Systems: A survey. (arXiv:2107.01272v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01272</id>
        <link href="http://arxiv.org/abs/2107.01272"/>
        <updated>2021-08-03T02:06:34.848Z</updated>
        <summary type="html"><![CDATA[Modeling complex physical dynamics is a fundamental task in science and
engineering. Traditional physics-based models are interpretable but rely on
rigid assumptions. And the direct numerical approximation is usually
computationally intensive, requiring significant computational resources and
expertise. While deep learning (DL) provides novel alternatives for efficiently
recognizing complex patterns and emulating nonlinear dynamics, it does not
necessarily obey the governing laws of physical systems, nor do they generalize
well across different systems. Thus, the study of physics-guided DL emerged and
has gained great progress. It aims to take the best from both physics-based
modeling and state-of-the-art DL models to better solve scientific problems. In
this paper, we provide a structured overview of existing methodologies of
integrating prior physical knowledge or physics-based modeling into DL and
discuss the emerging opportunities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rui Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive KL-UCB based Bandit Algorithms for Markovian and i.i.d. Settings. (arXiv:2009.06606v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06606</id>
        <link href="http://arxiv.org/abs/2009.06606"/>
        <updated>2021-08-03T02:06:34.842Z</updated>
        <summary type="html"><![CDATA[In the regret-based formulation of multi-armed bandit (MAB) problems, except
in rare instances, much of the literature focuses on arms with i.i.d. rewards.
In this paper, we consider the problem of obtaining regret guarantees for MAB
problems in which the rewards of each arm form a Markov chain which may not
belong to a single parameter exponential family. To achieve logarithmic regret
in such problems is not difficult: a variation of standard KL-UCB does the job.
However, the constants obtained from such an analysis are poor for the
following reason: i.i.d. rewards are a special case of Markov rewards and it is
difficult to design an algorithm that works well independent of whether the
underlying model is truly Markovian or i.i.d. To overcome this issue, we
introduce a novel algorithm that identifies whether the rewards from each arm
are truly Markovian or i.i.d. using a Hellinger distance-based test. Our
algorithm then switches from using a standard KL-UCB to a specialized version
of KL-UCB when it determines that the arm reward is Markovian, thus resulting
in low regret for both i.i.d. and Markovian settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1"&gt;Arghyadip Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1"&gt;Sanjay Shakkottai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srikant_R/0/1/0/all/0/1"&gt;R. Srikant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributional Robust Batch Contextual Bandits. (arXiv:2006.05630v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05630</id>
        <link href="http://arxiv.org/abs/2006.05630"/>
        <updated>2021-08-03T02:06:34.822Z</updated>
        <summary type="html"><![CDATA[Policy learning using historical observational data is an important problem
that has found widespread applications. Examples include selecting offers,
prices, advertisements to send to customers, as well as selecting which
medication to prescribe to a patient. However, existing literature rests on the
crucial assumption that the future environment where the learned policy will be
deployed is the same as the past environment that has generated the data--an
assumption that is often false or too coarse an approximation. In this paper,
we lift this assumption and aim to learn a distributional robust policy with
incomplete (bandit) observational data. We propose a novel learning algorithm
that is able to learn a robust policy to adversarial perturbations and unknown
covariate shifts. We first present a policy evaluation procedure in the
ambiguous environment and then give a performance guarantee based on the theory
of uniform convergence. Additionally, we also give a heuristic algorithm to
solve the distributional robust policy learning problems efficiently. Finally,
we demonstrate the robustness of our methods in the synthetic and real-world
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Si_N/0/1/0/all/0/1"&gt;Nian Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhengyuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blanchet_J/0/1/0/all/0/1"&gt;Jose Blanchet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedLab: A Flexible Federated Learning Framework. (arXiv:2107.11621v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.11621</id>
        <link href="http://arxiv.org/abs/2107.11621"/>
        <updated>2021-08-03T02:06:34.816Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) is a machine learning field in which researchers try
to facilitate model learning process among multiparty without violating privacy
protection regulations. Considerable effort has been invested in FL
optimization and communication related researches. In this work, we introduce
FedLab, a lightweight open-source framework for FL simulation. The design of
FedLab focuses on FL algorithm effectiveness and communication efficiency.
Also, FedLab is scalable in different deployment scenario. We hope FedLab could
provide flexible API as well as reliable baseline implementations, and relieve
the burden of implementing novel approaches for researchers in FL community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1"&gt;Dun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1"&gt;Siqi Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xiangjing Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-end neural network approach to 3D reservoir simulation and adaptation. (arXiv:2102.10304v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10304</id>
        <link href="http://arxiv.org/abs/2102.10304"/>
        <updated>2021-08-03T02:06:34.809Z</updated>
        <summary type="html"><![CDATA[Reservoir simulation and adaptation (also known as history matching) are
typically considered as separate problems. While a set of models are aimed at
the solution of the forward simulation problem assuming all initial geological
parameters are known, the other set of models adjust geological parameters
under the fixed forward simulation model to fit production data. This results
in many difficulties for both reservoir engineers and developers of new
efficient computation schemes. We present a unified approach to reservoir
simulation and adaptation problems. A single neural network model allows a
forward pass from initial geological parameters of the 3D reservoir model
through dynamic state variables to well's production rates and backward
gradient propagation to any model inputs and variables. The model fitting and
geological parameters adaptation both become the optimization problem over
specific parts of the same neural network model. Standard gradient-based
optimization schemes can be used to find the optimal solution. Using real-world
oilfield model and historical production rates we demonstrate that the
suggested approach allows reservoir simulation and history matching with a
benefit of several orders of magnitude simulation speed-up. Finally, to
propagate this research we open-source a Python-based framework DeepField that
allows standard processing of reservoir models and reproducing the approach
presented in this paper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Illarionov_E/0/1/0/all/0/1"&gt;E. Illarionov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Temirchev_P/0/1/0/all/0/1"&gt;P. Temirchev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Voloskov_D/0/1/0/all/0/1"&gt;D. Voloskov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kostoev_R/0/1/0/all/0/1"&gt;R. Kostoev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simonov_M/0/1/0/all/0/1"&gt;M. Simonov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pissarenko_D/0/1/0/all/0/1"&gt;D. Pissarenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orlov_D/0/1/0/all/0/1"&gt;D. Orlov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koroteev_D/0/1/0/all/0/1"&gt;D. Koroteev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Generative Adversarial Networks in Cancer Imaging: New Applications, New Solutions. (arXiv:2107.09543v1 [eess.IV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2107.09543</id>
        <link href="http://arxiv.org/abs/2107.09543"/>
        <updated>2021-08-03T02:06:34.809Z</updated>
        <summary type="html"><![CDATA[Despite technological and medical advances, the detection, interpretation,
and treatment of cancer based on imaging data continue to pose significant
challenges. These include high inter-observer variability, difficulty of
small-sized lesion detection, nodule interpretation and malignancy
determination, inter- and intra-tumour heterogeneity, class imbalance,
segmentation inaccuracies, and treatment effect uncertainty. The recent
advancements in Generative Adversarial Networks (GANs) in computer vision as
well as in medical imaging may provide a basis for enhanced capabilities in
cancer detection and analysis. In this review, we assess the potential of GANs
to address a number of key challenges of cancer imaging, including data
scarcity and imbalance, domain and dataset shifts, data access and privacy,
data annotation and quantification, as well as cancer detection, tumour
profiling and treatment planning. We provide a critical appraisal of the
existing literature of GANs applied to cancer imagery, together with
suggestions on future research directions to address these challenges. We
analyse and discuss 163 papers that apply adversarial training techniques in
the context of cancer imaging and elaborate their methodologies, advantages and
limitations. With this work, we strive to bridge the gap between the needs of
the clinical cancer imaging community and the current and prospective research
on GANs in the artificial intelligence community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Osuala_R/0/1/0/all/0/1"&gt;Richard Osuala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kushibar_K/0/1/0/all/0/1"&gt;Kaisar Kushibar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Garrucho_L/0/1/0/all/0/1"&gt;Lidia Garrucho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Linardos_A/0/1/0/all/0/1"&gt;Akis Linardos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Szafranowska_Z/0/1/0/all/0/1"&gt;Zuzanna Szafranowska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Klein_S/0/1/0/all/0/1"&gt;Stefan Klein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Glocker_B/0/1/0/all/0/1"&gt;Ben Glocker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Diaz_O/0/1/0/all/0/1"&gt;Oliver Diaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1"&gt;Karim Lekadir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ATCN: Resource-Efficient Processing of Time Series on Edge. (arXiv:2011.05260v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.05260</id>
        <link href="http://arxiv.org/abs/2011.05260"/>
        <updated>2021-08-03T02:06:34.808Z</updated>
        <summary type="html"><![CDATA[This paper presents a scalable deep learning model called Agile Temporal
Convolutional Network (ATCN) for high-accurate fast classification and time
series prediction in resource-constrained embedded systems. ATCN is a family of
compact networks with formalized hyperparameters that enable
application-specific adjustments to be made to the model architecture. It is
primarily designed for embedded edge devices with very limited performance and
memory, such as wearable biomedical devices and real-time reliability
monitoring systems. ATCN makes fundamental improvements over the mainstream
temporal convolutional neural networks, including residual connections as time
attention machines to increase the network depth and accuracy and the
incorporation of separable depth-wise convolution to reduce the computational
complexity of the model. As part of the present work, three ATCN families,
namely T0, T1, and T2, are also presented and evaluated on different ranges of
embedded processors - Cortex-M7 and Cortex-A57 processor. An evaluation of the
ATCN models against the best-in-class InceptionTime shows that ATCN improves
both accuracy and execution time on a broad range of embedded and
cyber-physical applications with demand for real-time processing on the
embedded edge. At the same time, in contrast to existing solutions, ATCN is the
first deep learning-based approach that can be run on embedded microcontrollers
(Cortex-M7) with limited computational performance and memory capacity while
delivering state-of-the-art accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baharani_M/0/1/0/all/0/1"&gt;Mohammadreza Baharani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tabkhi_H/0/1/0/all/0/1"&gt;Hamed Tabkhi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tractable structured natural gradient descent using local parameterizations. (arXiv:2102.07405v7 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07405</id>
        <link href="http://arxiv.org/abs/2102.07405"/>
        <updated>2021-08-03T02:06:34.808Z</updated>
        <summary type="html"><![CDATA[Natural-gradient descent (NGD) on structured parameter spaces (e.g., low-rank
covariances) is computationally challenging due to difficult Fisher-matrix
computations. We address this issue by using \emph{local-parameter coordinates}
to obtain a flexible and efficient NGD method that works well for a
wide-variety of structured parameterizations. We show four applications where
our method (1) generalizes the exponential natural evolutionary strategy, (2)
recovers existing Newton-like algorithms, (3) yields new structured
second-order algorithms via matrix groups, and (4) gives new algorithms to
learn covariances of Gaussian and Wishart-based distributions. We show results
on a range of problems from deep learning, variational inference, and evolution
strategies. Our work opens a new direction for scalable structured geometric
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nielsen_F/0/1/0/all/0/1"&gt;Frank Nielsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mohammad Emtiyaz Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schmidt_M/0/1/0/all/0/1"&gt;Mark Schmidt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intelligent-Tire-Based Slip Ratio Estimation Using Machine Learning. (arXiv:2106.08961v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08961</id>
        <link href="http://arxiv.org/abs/2106.08961"/>
        <updated>2021-08-03T02:06:34.808Z</updated>
        <summary type="html"><![CDATA[Autonomous vehicles are most concerned about safety control issues, and the
slip ratio is critical to the safety of the vehicle control system. In this
paper, different machine learning algorithms (Neural Networks, Gradient
Boosting Machine, Random Forest, and Support Vector Machine) are used to train
the slip ratio estimation model based on the acceleration signals ($a_x$,
$a_y$, and $a_z$) from the tri-axial Micro-Electro Mechanical System (MEMS)
accelerometer utilized in the intelligent tire system, where the acceleration
signals are divided into four sets ($a_x/a_y/a_z$, $a_x/a_z$, $a_y/a_z$, and
$a_z$) as algorithm inputs. The experimental data used in this study are
collected through the MTS Flat-Trac tire test platform. Performance of
different slip ratio estimation models is compared using the NRMS errors in
10-fold cross-validation (CV). The results indicate that NN and GBM have more
promising accuracy, and the $a_z$ input type has a better performance compared
to other input types, with the best result being the estimation model of the NN
algorithm with $a_z$ as input, which results is 4.88\%. The present study with
the fusion of intelligent tire system and machine learning paves the way for
the accurate estimation of tire slip ratio under different driving conditions,
which will open up a new way of Autonomous vehicles, intelligent tires, and
tire slip ratio estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1"&gt;Nan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zepeng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jianfeng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Askari_H/0/1/0/all/0/1"&gt;Hassan Askari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explain and Improve: LRP-Inference Fine-Tuning for Image Captioning Models. (arXiv:2001.01037v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.01037</id>
        <link href="http://arxiv.org/abs/2001.01037"/>
        <updated>2021-08-03T02:06:34.807Z</updated>
        <summary type="html"><![CDATA[This paper analyzes the predictions of image captioning models with attention
mechanisms beyond visualizing the attention itself. We develop variants of
layer-wise relevance propagation (LRP) and gradient-based explanation methods,
tailored to image captioning models with attention mechanisms. We compare the
interpretability of attention heatmaps systematically against the explanations
provided by explanation methods such as LRP, Grad-CAM, and Guided Grad-CAM. We
show that explanation methods provide simultaneously pixel-wise image
explanations (supporting and opposing pixels of the input image) and linguistic
explanations (supporting and opposing words of the preceding sequence) for each
word in the predicted captions. We demonstrate with extensive experiments that
explanation methods 1) can reveal additional evidence used by the model to make
decisions compared to attention; 2) correlate to object locations with high
precision; 3) are helpful to "debug" the model, e.g. by analyzing the reasons
for hallucinated object words. With the observed properties of explanations, we
further design an LRP-inference fine-tuning strategy that reduces the issue of
object hallucination in image captioning models, and meanwhile, maintains the
sentence fluency. We conduct experiments with two widely used attention
mechanisms: the adaptive attention mechanism calculated with the additive
attention and the multi-head attention mechanism calculated with the scaled dot
product.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jiamei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1"&gt;Sebastian Lapuschkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1"&gt;Wojciech Samek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Binder_A/0/1/0/all/0/1"&gt;Alexander Binder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An End-to-End and Accurate PPG-based Respiratory Rate Estimation Approach Using Cycle Generative Adversarial Networks. (arXiv:2105.00594v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00594</id>
        <link href="http://arxiv.org/abs/2105.00594"/>
        <updated>2021-08-03T02:06:34.807Z</updated>
        <summary type="html"><![CDATA[Respiratory rate (RR) is a clinical sign representing ventilation. An
abnormal change in RR is often the first sign of health deterioration as the
body attempts to maintain oxygen delivery to its tissues. There has been a
growing interest in remotely monitoring of RR in everyday settings which has
made photoplethysmography (PPG) monitoring wearable devices an attractive
choice. PPG signals are useful sources for RR extraction due to the presence of
respiration-induced modulations in them. The existing PPG-based RR estimation
methods mainly rely on hand-crafted rules and manual parameters tuning. An
end-to-end deep learning approach was recently proposed, however, despite its
automatic nature, the performance of this method is not ideal using the real
world data. In this paper, we present an end-to-end and accurate pipeline for
RR estimation using Cycle Generative Adversarial Networks (CycleGAN) to
reconstruct respiratory signals from raw PPG signals. Our results demonstrate a
higher RR estimation accuracy of up to 2$\times$ (mean absolute error of
1.9$\pm$0.3 using five fold cross validation) compared to the state-of-th-art
using a identical publicly available dataset. Our results suggest that CycleGAN
can be a valuable method for RR estimation from raw PPG signals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aqajari_S/0/1/0/all/0/1"&gt;Seyed Amir Hossein Aqajari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1"&gt;Rui Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zargari_A/0/1/0/all/0/1"&gt;Amir Hosein Afandizadeh Zargari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahmani_A/0/1/0/all/0/1"&gt;Amir M. Rahmani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Modal Detection of Alzheimer's Disease from Speech and Text. (arXiv:2012.00096v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00096</id>
        <link href="http://arxiv.org/abs/2012.00096"/>
        <updated>2021-08-03T02:06:34.806Z</updated>
        <summary type="html"><![CDATA[Reliable detection of the prodromal stages of Alzheimer's disease (AD)
remains difficult even today because, unlike other neurocognitive impairments,
there is no definitive diagnosis of AD in vivo. In this context, existing
research has shown that patients often develop language impairment even in mild
AD conditions. We propose a multimodal deep learning method that utilizes
speech and the corresponding transcript simultaneously to detect AD. For audio
signals, the proposed audio-based network, a convolutional neural network (CNN)
based model, predicts the diagnosis for multiple speech segments, which are
combined for the final prediction. Similarly, we use contextual embedding
extracted from BERT concatenated with a CNN-generated embedding for classifying
the transcript. The individual predictions of the two models are then combined
to make the final classification. We also perform experiments to analyze the
model performance when Automated Speech Recognition (ASR) system generated
transcripts are used instead of manual transcription in the text-based model.
The proposed method achieves 85.3% 10-fold cross-validation accuracy when
trained and evaluated on the Dementiabank Pitt corpus.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1"&gt;Amish Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahoo_S/0/1/0/all/0/1"&gt;Sourav Sahoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Datar_A/0/1/0/all/0/1"&gt;Arnhav Datar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadiwala_J/0/1/0/all/0/1"&gt;Juned Kadiwala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shalu_H/0/1/0/all/0/1"&gt;Hrithwik Shalu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathew_J/0/1/0/all/0/1"&gt;Jimson Mathew&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Black-box Probe for Unsupervised Domain Adaptation without Model Transferring. (arXiv:2107.10174v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.10174</id>
        <link href="http://arxiv.org/abs/2107.10174"/>
        <updated>2021-08-03T02:06:34.806Z</updated>
        <summary type="html"><![CDATA[In recent years, researchers have been paying increasing attention to the
threats brought by deep learning models to data security and privacy,
especially in the field of domain adaptation. Existing unsupervised domain
adaptation (UDA) methods can achieve promising performance without transferring
data from source domain to target domain. However, UDA with representation
alignment or self-supervised pseudo-labeling relies on the transferred source
models. In many data-critical scenarios, methods based on model transferring
may suffer from membership inference attacks and expose private data. In this
paper, we aim to overcome a challenging new setting where the source models are
only queryable but cannot be transferred to the target domain. We propose
Black-box Probe Domain Adaptation (BPDA), which adopts query mechanism to probe
and refine information from source model using third-party dataset. In order to
gain more informative query results, we further propose Distributionally
Adversarial Training (DAT) to align the distribution of third-party data with
that of target data. BPDA uses public third-party dataset and adversarial
examples based on DAT as the information carrier between source and target
domains, dispensing with transferring source data or model. Experimental
results on benchmarks of Digit-Five, Office-Caltech, Office-31, Office-Home,
and DomainNet demonstrate the feasibility of BPDA without model transferring.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1"&gt;Kunhong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yucheng Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1"&gt;Yahong Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yunfeng Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bingshuai Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Take an Emotion Walk: Perceiving Emotions from Gaits Using Hierarchical Attention Pooling and Affective Mapping. (arXiv:1911.08708v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.08708</id>
        <link href="http://arxiv.org/abs/1911.08708"/>
        <updated>2021-08-03T02:06:34.805Z</updated>
        <summary type="html"><![CDATA[We present an autoencoder-based semi-supervised approach to classify
perceived human emotions from walking styles obtained from videos or
motion-captured data and represented as sequences of 3D poses. Given the motion
on each joint in the pose at each time step extracted from 3D pose sequences,
we hierarchically pool these joint motions in a bottom-up manner in the
encoder, following the kinematic chains in the human body. We also constrain
the latent embeddings of the encoder to contain the space of
psychologically-motivated affective features underlying the gaits. We train the
decoder to reconstruct the motions per joint per time step in a top-down manner
from the latent embeddings. For the annotated data, we also train a classifier
to map the latent embeddings to emotion labels. Our semi-supervised approach
achieves a mean average precision of 0.84 on the Emotion-Gait benchmark
dataset, which contains both labeled and unlabeled gaits collected from
multiple sources. We outperform current state-of-art algorithms for both
emotion recognition and action recognition from 3D gaits by 7%--23% on the
absolute. More importantly, we improve the average precision by 10%--50% on the
absolute on classes that each makes up less than 25% of the labeled part of the
Emotion-Gait benchmark dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1"&gt;Uttaran Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roncal_C/0/1/0/all/0/1"&gt;Christian Roncal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_T/0/1/0/all/0/1"&gt;Trisha Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1"&gt;Rohan Chandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kapsaskis_K/0/1/0/all/0/1"&gt;Kyra Kapsaskis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gray_K/0/1/0/all/0/1"&gt;Kurt Gray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1"&gt;Aniket Bera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1"&gt;Dinesh Manocha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Surgical Data Science -- from Concepts toward Clinical Translation. (arXiv:2011.02284v2 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.02284</id>
        <link href="http://arxiv.org/abs/2011.02284"/>
        <updated>2021-08-03T02:06:34.804Z</updated>
        <summary type="html"><![CDATA[Recent developments in data science in general and machine learning in
particular have transformed the way experts envision the future of surgery.
Surgical Data Science (SDS) is a new research field that aims to improve the
quality of interventional healthcare through the capture, organization,
analysis and modeling of data. While an increasing number of data-driven
approaches and clinical applications have been studied in the fields of
radiological and clinical data science, translational success stories are still
lacking in surgery. In this publication, we shed light on the underlying
reasons and provide a roadmap for future advances in the field. Based on an
international workshop involving leading researchers in the field of SDS, we
review current practice, key achievements and initiatives as well as available
standards and tools for a number of topics relevant to the field, namely (1)
infrastructure for data acquisition, storage and access in the presence of
regulatory constraints, (2) data annotation and sharing and (3) data analytics.
We further complement this technical perspective with (4) a review of currently
available SDS products and the translational progress from academia and (5) a
roadmap for faster clinical translation and exploitation of the full potential
of SDS, based on an international multi-round Delphi process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maier_Hein_L/0/1/0/all/0/1"&gt;Lena Maier-Hein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eisenmann_M/0/1/0/all/0/1"&gt;Matthias Eisenmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarikaya_D/0/1/0/all/0/1"&gt;Duygu Sarikaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marz_K/0/1/0/all/0/1"&gt;Keno M&amp;#xe4;rz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collins_T/0/1/0/all/0/1"&gt;Toby Collins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malpani_A/0/1/0/all/0/1"&gt;Anand Malpani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fallert_J/0/1/0/all/0/1"&gt;Johannes Fallert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feussner_H/0/1/0/all/0/1"&gt;Hubertus Feussner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giannarou_S/0/1/0/all/0/1"&gt;Stamatia Giannarou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mascagni_P/0/1/0/all/0/1"&gt;Pietro Mascagni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakawala_H/0/1/0/all/0/1"&gt;Hirenkumar Nakawala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_A/0/1/0/all/0/1"&gt;Adrian Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pugh_C/0/1/0/all/0/1"&gt;Carla Pugh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1"&gt;Danail Stoyanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vedula_S/0/1/0/all/0/1"&gt;Swaroop S. Vedula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cleary_K/0/1/0/all/0/1"&gt;Kevin Cleary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fichtinger_G/0/1/0/all/0/1"&gt;Gabor Fichtinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forestier_G/0/1/0/all/0/1"&gt;Germain Forestier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gibaud_B/0/1/0/all/0/1"&gt;Bernard Gibaud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grantcharov_T/0/1/0/all/0/1"&gt;Teodor Grantcharov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashizume_M/0/1/0/all/0/1"&gt;Makoto Hashizume&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heckmann_Notzel_D/0/1/0/all/0/1"&gt;Doreen Heckmann-N&amp;#xf6;tzel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kenngott_H/0/1/0/all/0/1"&gt;Hannes G. Kenngott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kikinis_R/0/1/0/all/0/1"&gt;Ron Kikinis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mundermann_L/0/1/0/all/0/1"&gt;Lars M&amp;#xfc;ndermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1"&gt;Nassir Navab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Onogur_S/0/1/0/all/0/1"&gt;Sinan Onogur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1"&gt;Raphael Sznitman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taylor_R/0/1/0/all/0/1"&gt;Russell H. Taylor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tizabi_M/0/1/0/all/0/1"&gt;Minu D. Tizabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wagner_M/0/1/0/all/0/1"&gt;Martin Wagner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hager_G/0/1/0/all/0/1"&gt;Gregory D. Hager&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neumuth_T/0/1/0/all/0/1"&gt;Thomas Neumuth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1"&gt;Nicolas Padoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collins_J/0/1/0/all/0/1"&gt;Justin Collins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gockel_I/0/1/0/all/0/1"&gt;Ines Gockel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goedeke_J/0/1/0/all/0/1"&gt;Jan Goedeke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashimoto_D/0/1/0/all/0/1"&gt;Daniel A. Hashimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joyeux_L/0/1/0/all/0/1"&gt;Luc Joyeux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1"&gt;Kyle Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leff_D/0/1/0/all/0/1"&gt;Daniel R. Leff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madani_A/0/1/0/all/0/1"&gt;Amin Madani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marcus_H/0/1/0/all/0/1"&gt;Hani J. Marcus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meireles_O/0/1/0/all/0/1"&gt;Ozanan Meireles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seitel_A/0/1/0/all/0/1"&gt;Alexander Seitel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teber_D/0/1/0/all/0/1"&gt;Dogu Teber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uckert_F/0/1/0/all/0/1"&gt;Frank &amp;#xdc;ckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_Stich_B/0/1/0/all/0/1"&gt;Beat P. M&amp;#xfc;ller-Stich&lt;/a&gt;, et al. (2 additional authors not shown)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Selecting the independent coordinates of manifolds with large aspect ratios. (arXiv:1907.01651v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1907.01651</id>
        <link href="http://arxiv.org/abs/1907.01651"/>
        <updated>2021-08-03T02:06:34.802Z</updated>
        <summary type="html"><![CDATA[Many manifold embedding algorithms fail apparently when the data manifold has
a large aspect ratio (such as a long, thin strip). Here, we formulate success
and failure in terms of finding a smooth embedding, showing also that the
problem is pervasive and more complex than previously recognized.
Mathematically, success is possible under very broad conditions, provided that
embedding is done by carefully selected eigenfunctions of the Laplace-Beltrami
operator $\Delta$. Hence, we propose a bicriterial Independent Eigencoordinate
Selection (IES) algorithm that selects smooth embeddings with few eigenvectors.
The algorithm is grounded in theory, has low computational overhead, and is
successful on synthetic and large real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yu-Chia Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Meila_M/0/1/0/all/0/1"&gt;Marina Meil&amp;#x103;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Efficiency of Sinkhorn and Greenkhorn and Their Acceleration for Optimal Transport. (arXiv:1906.01437v7 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.01437</id>
        <link href="http://arxiv.org/abs/1906.01437"/>
        <updated>2021-08-03T02:06:34.795Z</updated>
        <summary type="html"><![CDATA[We present several new complexity results for the algorithms that
approximately solve the optimal transport (OT) problem between two discrete
probability measures with at most $n$ atoms. First, we improve the complexity
bound of a greedy variant of the Sinkhorn algorithm, known as
\textit{Greenkhorn} algorithm, from $\widetilde{O}(n^2\varepsilon^{-3})$ to
$\widetilde{O}(n^2\varepsilon^{-2})$. Notably, this matches the best known
complexity bound of the Sinkhorn algorithm and sheds the light to superior
practical performance of the Greenkhorn algorithm. Second, we generalize an
adaptive primal-dual accelerated gradient descent (APDAGD)
algorithm~\citep{Dvurechensky-2018-Computational} with mirror mapping $\phi$
and prove that the resulting APDAMD algorithm achieves the complexity bound of
$\widetilde{O}(n^2\sqrt{\delta}\varepsilon^{-1})$ where $\delta>0$ refers to
the regularity of $\phi$. We demonstrate that the complexity bound of
$\widetilde{O}(\min\{n^{9/4}\varepsilon^{-1}, n^2\varepsilon^{-2}\})$ is
invalid for the APDAGD algorithm and establish a new complexity bound of
$\widetilde{O}(n^{5/2}\varepsilon^{-1})$. Moreover, we propose a
\textit{deterministic} accelerated Sinkhorn algorithm and prove that it
achieves the complexity bound of $\widetilde{O}(n^{7/3}\varepsilon^{-4/3})$ by
incorporating an estimate sequence. Therefore, the accelerated Sinkhorn
algorithm outperforms the Sinkhorn and Greenkhorn algorithms in terms of
$1/\varepsilon$ and the APDAGD and accelerated alternating
minimization~\citep{Guminov-2021-Combination} algorithms in terms of $n$.
Finally, we conduct experiments on synthetic data and real images with the
proposed algorithms in the paper and demonstrate their efficiency via numerical
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tianyi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1"&gt;Nhat Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deterministic error bounds for kernel-based learning techniques under bounded noise. (arXiv:2008.04005v3 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.04005</id>
        <link href="http://arxiv.org/abs/2008.04005"/>
        <updated>2021-08-03T02:06:34.780Z</updated>
        <summary type="html"><![CDATA[We consider the problem of reconstructing a function from a finite set of
noise-corrupted samples. Two kernel algorithms are analyzed, namely kernel
ridge regression and $\varepsilon$-support vector regression. By assuming the
ground-truth function belongs to the reproducing kernel Hilbert space of the
chosen kernel, and the measurement noise affecting the dataset is bounded, we
adopt an approximation theory viewpoint to establish \textit{deterministic},
finite-sample error bounds for the two models. Finally, we discuss their
connection with Gaussian processes and two numerical examples are provided. In
establishing our inequalities, we hope to help bring the fields of
non-parametric kernel learning and system identification for robust control
closer to each other.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Maddalena_E/0/1/0/all/0/1"&gt;Emilio T. Maddalena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Scharnhorst_P/0/1/0/all/0/1"&gt;Paul Scharnhorst&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jones_C/0/1/0/all/0/1"&gt;Colin N. Jones&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Height Estimation of Children under Five Years using Depth Images. (arXiv:2105.01688v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01688</id>
        <link href="http://arxiv.org/abs/2105.01688"/>
        <updated>2021-08-03T02:06:34.773Z</updated>
        <summary type="html"><![CDATA[Malnutrition is a global health crisis and is the leading cause of death
among children under five. Detecting malnutrition requires anthropometric
measurements of weight, height, and middle-upper arm circumference. However,
measuring them accurately is a challenge, especially in the global south, due
to limited resources. In this work, we propose a CNN-based approach to estimate
the height of standing children under five years from depth images collected
using a smart-phone. According to the SMART Methodology Manual [5], the
acceptable accuracy for height is less than 1.4 cm. On training our deep
learning model on 87131 depth images, our model achieved an average mean
absolute error of 1.64% on 57064 test images. For 70.3% test images, we
estimated height accurately within the acceptable 1.4 cm range. Thus, our
proposed solution can accurately detect stunting (low height-for-age) in
standing children below five years of age.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1"&gt;Anusua Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1"&gt;Mohit Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1"&gt;Nikhil Kumar Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hinsche_M/0/1/0/all/0/1"&gt;Markus Hinsche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1"&gt;Prashant Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matiaschek_M/0/1/0/all/0/1"&gt;Markus Matiaschek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Behrens_T/0/1/0/all/0/1"&gt;Tristan Behrens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Militeri_M/0/1/0/all/0/1"&gt;Mirco Militeri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Birge_C/0/1/0/all/0/1"&gt;Cameron Birge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaushik_S/0/1/0/all/0/1"&gt;Shivangi Kaushik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohapatra_A/0/1/0/all/0/1"&gt;Archisman Mohapatra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatterjee_R/0/1/0/all/0/1"&gt;Rita Chatterjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dodhia_R/0/1/0/all/0/1"&gt;Rahul Dodhia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferres_J/0/1/0/all/0/1"&gt;Juan Lavista Ferres&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Forecasting Thermoacoustic Instabilities in Liquid Propellant Rocket Engines Using Multimodal Bayesian Deep Learning. (arXiv:2107.06396v2 [physics.flu-dyn] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06396</id>
        <link href="http://arxiv.org/abs/2107.06396"/>
        <updated>2021-08-03T02:06:34.765Z</updated>
        <summary type="html"><![CDATA[The 100 MW cryogenic liquid oxygen/hydrogen multi-injector combustor BKD
operated by the DLR Institute of Space Propulsion is a research platform that
allows the study of thermoacoustic instabilities under realistic conditions,
representative of small upper stage rocket engines. We use data from BKD
experimental campaigns in which the static chamber pressure and fuel-oxidizer
ratio are varied such that the first tangential mode of the combustor is
excited under some conditions. We train an autoregressive Bayesian neural
network model to forecast the amplitude of the dynamic pressure time series,
inputting multiple sensor measurements (injector pressure/ temperature
measurements, static chamber pressure, high-frequency dynamic pressure
measurements, high-frequency OH* chemiluminescence measurements) and future
flow rate control signals. The Bayesian nature of our algorithms allows us to
work with a dataset whose size is restricted by the expense of each
experimental run, without making overconfident extrapolations. We find that the
networks are able to accurately forecast the evolution of the pressure
amplitude and anticipate instability events on unseen experimental runs 500
milliseconds in advance. We compare the predictive accuracy of multiple models
using different combinations of sensor inputs. We find that the high-frequency
dynamic pressure signal is particularly informative. We also use the technique
of integrated gradients to interpret the influence of different sensor inputs
on the model prediction. The negative log-likelihood of data points in the test
dataset indicates that predictive uncertainties are well-characterized by our
Bayesian model and simulating a sensor failure event results as expected in a
dramatic increase in the epistemic component of the uncertainty.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Sengupta_U/0/1/0/all/0/1"&gt;Ushnish Sengupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Waxenegger_Wilfing_G/0/1/0/all/0/1"&gt;G&amp;#xfc;nther Waxenegger-Wilfing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Martin_J/0/1/0/all/0/1"&gt;Jan Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Hardi_J/0/1/0/all/0/1"&gt;Justin Hardi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Juniper_M/0/1/0/all/0/1"&gt;Matthew P. Juniper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anonymizing Machine Learning Models. (arXiv:2007.13086v3 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.13086</id>
        <link href="http://arxiv.org/abs/2007.13086"/>
        <updated>2021-08-03T02:06:34.716Z</updated>
        <summary type="html"><![CDATA[There is a known tension between the need to analyze personal data to drive
business and privacy concerns. Many data protection regulations, including the
EU General Data Protection Regulation (GDPR) and the California Consumer
Protection Act (CCPA), set out strict restrictions and obligations on the
collection and processing of personal data. Moreover, machine learning models
themselves can be used to derive personal information, as demonstrated by
recent membership and attribute inference attacks. Anonymized data, however, is
exempt from the obligations set out in these regulations. It is therefore
desirable to be able to create models that are anonymized, thus also exempting
them from those obligations, in addition to providing better protection against
attacks.

Learning on anonymized data typically results in significant degradation in
accuracy. In this work, we propose a method that is able to achieve better
model accuracy by using the knowledge encoded within the trained model, and
guiding our anonymization process to minimize the impact on the model's
accuracy, a process we call accuracy-guided anonymization. We demonstrate that
by focusing on the model's accuracy rather than generic information loss
measures, our method outperforms state of the art k-anonymity methods in terms
of the achieved utility, in particular with high values of k and large numbers
of quasi-identifiers.

We also demonstrate that our approach has a similar, and sometimes even
better ability to prevent membership inference attacks as approaches based on
differential privacy, while averting some of their drawbacks such as
complexity, performance overhead and model-specific implementations. This makes
model-guided anonymization a legitimate substitute for such methods and a
practical approach to creating privacy-preserving models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goldsteen_A/0/1/0/all/0/1"&gt;Abigail Goldsteen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ezov_G/0/1/0/all/0/1"&gt;Gilad Ezov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shmelkin_R/0/1/0/all/0/1"&gt;Ron Shmelkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moffie_M/0/1/0/all/0/1"&gt;Micha Moffie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farkash_A/0/1/0/all/0/1"&gt;Ariel Farkash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Batch Selection Policy for Active Metric Learning. (arXiv:2102.07365v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07365</id>
        <link href="http://arxiv.org/abs/2102.07365"/>
        <updated>2021-08-03T02:06:34.708Z</updated>
        <summary type="html"><![CDATA[Active metric learning is the problem of incrementally selecting high-utility
batches of training data (typically, ordered triplets) to annotate, in order to
progressively improve a learned model of a metric over some input domain as
rapidly as possible. Standard approaches, which independently assess the
informativeness of each triplet in a batch, are susceptible to highly
correlated batches with many redundant triplets and hence low overall utility.
While a recent work \cite{kumari2020batch} proposes batch-decorrelation
strategies for metric learning, they rely on ad hoc heuristics to estimate the
correlation between two triplets at a time. We present a novel batch active
metric learning method that leverages the Maximum Entropy Principle to learn
the least biased estimate of triplet distribution for a given set of prior
constraints. To avoid redundancy between triplets, our method collectively
selects batches with maximum joint entropy, which simultaneously captures both
informativeness and diversity. We take advantage of the submodularity of the
joint entropy function to construct a tractable solution using an efficient
greedy algorithm based on Gram-Schmidt orthogonalization that is provably
$\left( 1 - \frac{1}{e} \right)$-optimal. Our approach is the first batch
active metric learning method to define a unified score that balances
informativeness and diversity for an entire batch of triplets. Experiments with
several real-world datasets demonstrate that our algorithm is robust,
generalizes well to different applications and input modalities, and
consistently outperforms the state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+K_P/0/1/0/all/0/1"&gt;Priyadarshini K&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1"&gt;Siddhartha Chaudhuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borkar_V/0/1/0/all/0/1"&gt;Vivek Borkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1"&gt;Subhasis Chaudhuri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Random Matrix Perspective on Random Tensors. (arXiv:2108.00774v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.00774</id>
        <link href="http://arxiv.org/abs/2108.00774"/>
        <updated>2021-08-03T02:06:34.679Z</updated>
        <summary type="html"><![CDATA[Tensor models play an increasingly prominent role in many fields, notably in
machine learning. In several applications of such models, such as community
detection, topic modeling and Gaussian mixture learning, one must estimate a
low-rank signal from a noisy tensor. Hence, understanding the fundamental
limits and the attainable performance of estimators of that signal inevitably
calls for the study of random tensors. Substantial progress has been achieved
on this subject thanks to recent efforts, under the assumption that the tensor
dimensions grow large. Yet, some of the most significant among these
results--in particular, a precise characterization of the abrupt phase
transition (in terms of signal-to-noise ratio) that governs the performance of
the maximum likelihood (ML) estimator of a symmetric rank-one model with
Gaussian noise--were derived on the basis of statistical physics ideas, which
are not easily accessible to non-experts.

In this work, we develop a sharply distinct approach, relying instead on
standard but powerful tools brought by years of advances in random matrix
theory. The key idea is to study the spectra of random matrices arising from
contractions of a given random tensor. We show how this gives access to
spectral properties of the random tensor itself. In the specific case of a
symmetric rank-one model with Gaussian noise, our technique yields a hitherto
unknown characterization of the local maximum of the ML problem that is global
above the phase transition threshold. This characterization is in terms of a
fixed-point equation satisfied by a formula that had only been previously
obtained via statistical physics methods. Moreover, our analysis sheds light on
certain properties of the landscape of the ML problem in the large-dimensional
setting. Our approach is versatile and can be extended to other models, such as
asymmetric, non-Gaussian and higher-order ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Goulart_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Henrique de Morais Goulart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Couillet_R/0/1/0/all/0/1"&gt;Romain Couillet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Comon_P/0/1/0/all/0/1"&gt;Pierre Comon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attend2Pack: Bin Packing through Deep Reinforcement Learning with Attention. (arXiv:2107.04333v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04333</id>
        <link href="http://arxiv.org/abs/2107.04333"/>
        <updated>2021-08-03T02:06:34.443Z</updated>
        <summary type="html"><![CDATA[This paper seeks to tackle the bin packing problem (BPP) through a learning
perspective. Building on self-attention-based encoding and deep reinforcement
learning algorithms, we propose a new end-to-end learning model for this task
of interest. By decomposing the combinatorial action space, as well as
utilizing a new training technique denoted as prioritized oversampling, which
is a general scheme to speed up on-policy learning, we achieve state-of-the-art
performance in a range of experimental settings. Moreover, although the
proposed approach attend2pack targets offline-BPP, we strip our method down to
the strict online-BPP setting where it is also able to achieve state-of-the-art
performance. With a set of ablation studies as well as comparisons against a
range of previous works, we hope to offer as a valid baseline approach to this
field of study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zi_B/0/1/0/all/0/1"&gt;Bin Zi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1"&gt;Xiaoyu Ge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Applications on Neuroimaging for Diagnosis and Prognosis of Epilepsy: A Review. (arXiv:2102.03336v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03336</id>
        <link href="http://arxiv.org/abs/2102.03336"/>
        <updated>2021-08-03T02:06:34.419Z</updated>
        <summary type="html"><![CDATA[Machine learning is playing an increasingly important role in medical image
analysis, spawning new advances in the clinical application of neuroimaging.
There have been some reviews of machine learning and epilepsy before, but they
mainly focused on electrophysiological signals such as
electroencephalography(EEG) or stereo electroencephalography(SEEG), while
ignoring the potential of neuroimaging in epilepsy research. Neuroimaging has
its important advantages in confirming the range of epileptic region, which
means a lot in presurgical evaluation and assessment after surgery. However,
EEG is difficult to locate the epilepsy lesion region in the brain. In this
review, we emphasize the interaction between neuroimaging and machine learning
in the context of the epilepsy diagnosis and prognosis. We start with an
overview of typical neuroimaging modalities used in epilepsy clinics, MRI, DTI,
fMRI, and PET. Then, we introduce three approaches for applying machine
learning methods to neuroimaging data: i) the two-step compositional approach
combining feature engineering and machine learning classifiers, ii) the
end-to-end approach, which is usually toward deep learning, and iii) the hybrid
approach using the advantages of the two methods. Subsequently, the application
of machine learning on epilepsy neuroimaging, such as segmentation,
localization and lateralization tasks, as well as tasks directly related to
diagnosis and prognosis are introduced in detail. Finally, we discuss the
current achievements, challenges, and potential future directions in this
field, hoping to pave the way for computer-aided diagnosis and prognosis of
epilepsy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Jie Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ran_X/0/1/0/all/0/1"&gt;Xuming Ran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Keyin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1"&gt;Chen Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yi Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Haiyan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Quanying Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis and Optimisation of Bellman Residual Errors with Neural Function Approximation. (arXiv:2106.08774v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08774</id>
        <link href="http://arxiv.org/abs/2106.08774"/>
        <updated>2021-08-03T02:06:34.410Z</updated>
        <summary type="html"><![CDATA[Recent development of Deep Reinforcement Learning has demonstrated superior
performance of neural networks in solving challenging problems with large or
even continuous state spaces. One specific approach is to deploy neural
networks to approximate value functions by minimising the Mean Squared Bellman
Error function. Despite great successes of Deep Reinforcement Learning,
development of reliable and efficient numerical algorithms to minimise the
Bellman Error is still of great scientific interest and practical demand. Such
a challenge is partially due to the underlying optimisation problem being
highly non-convex or using incorrect gradient information as done in
Semi-Gradient algorithms. In this work, we analyse the Mean Squared Bellman
Error from a smooth optimisation perspective combined with a Residual Gradient
formulation. Our contribution is two-fold.

First, we analyse critical points of the error function and provide technical
insights on the optimisation procure and design choices for neural networks.
When the existence of global minima is assumed and the objective fulfils
certain conditions we can eliminate suboptimal local minima when using
over-parametrised neural networks. We can construct an efficient Approximate
Newton's algorithm based on our analysis and confirm theoretical properties of
this algorithm such as being locally quadratically convergent to a global
minimum numerically.

Second, we demonstrate feasibility and generalisation capabilities of the
proposed algorithm empirically using continuous control problems and provide a
numerical verification of our critical point analysis. We outline the short
coming of Semi-Gradients. To benefit from an approximate Newton's algorithm
complete derivatives of the Mean Squared Bellman error must be considered
during training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gottwald_M/0/1/0/all/0/1"&gt;Martin Gottwald&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Gronauer_S/0/1/0/all/0/1"&gt;Sven Gronauer&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1"&gt;Hao Shen&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Diepold_K/0/1/0/all/0/1"&gt;Klaus Diepold&lt;/a&gt; (1) ((1) Technical University of Munich, (2) fortiss)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring the representativeness of the M5 competition data. (arXiv:2103.02941v2 [stat.AP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02941</id>
        <link href="http://arxiv.org/abs/2103.02941"/>
        <updated>2021-08-03T02:06:34.396Z</updated>
        <summary type="html"><![CDATA[The main objective of the M5 competition, which focused on forecasting the
hierarchical unit sales of Walmart, was to evaluate the accuracy and
uncertainty of forecasting methods in the field in order to identify best
practices and highlight their practical implications. However, whether the
findings of the M5 competition can be generalized and exploited by retail firms
to better support their decisions and operation depends on the extent to which
the M5 data is sufficiently similar to unit sales data of retailers that
operate in different regions, sell different types of products, and consider
different marketing strategies. To answer this question, we analyze the
characteristics of the M5 time series and compare them with those of two
grocery retailers, namely Corporaci\'on Favorita and a major Greek supermarket
chain, using feature spaces. Our results suggest that there are only small
discrepancies between the examined data sets, supporting the representativeness
of the M5 data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Theodorou_E/0/1/0/all/0/1"&gt;Evangelos Theodorou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shengjie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kang_Y/0/1/0/all/0/1"&gt;Yanfei Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Spiliotis_E/0/1/0/all/0/1"&gt;Evangelos Spiliotis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Makridakis_S/0/1/0/all/0/1"&gt;Spyros Makridakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Assimakopoulos_V/0/1/0/all/0/1"&gt;Vassilios Assimakopoulos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Weighted A*: Learning Graph Costs and Heuristics with Differentiable Anytime A*. (arXiv:2105.01480v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01480</id>
        <link href="http://arxiv.org/abs/2105.01480"/>
        <updated>2021-08-03T02:06:34.373Z</updated>
        <summary type="html"><![CDATA[Recently, the trend of incorporating differentiable algorithms into deep
learning architectures arose in machine learning research, as the fusion of
neural layers and algorithmic layers has been beneficial for handling
combinatorial data, such as shortest paths on graphs. Recent works related to
data-driven planning aim at learning either cost functions or heuristic
functions, but not both. We propose Neural Weighted A*, a differentiable
anytime planner able to produce improved representations of planar maps as
graph costs and heuristics. Training occurs end-to-end on raw images with
direct supervision on planning examples, thanks to a differentiable A* solver
integrated into the architecture. More importantly, the user can trade off
planning accuracy for efficiency at run-time, using a single, real-valued
parameter. The solution suboptimality is constrained within a linear bound
equal to the optimal path cost multiplied by the tradeoff parameter. We
experimentally show the validity of our claims by testing Neural Weighted A*
against several baselines, introducing a novel, tile-based navigation dataset.
We outperform similar architectures in planning accuracy and efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Archetti_A/0/1/0/all/0/1"&gt;Alberto Archetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cannici_M/0/1/0/all/0/1"&gt;Marco Cannici&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matteucci_M/0/1/0/all/0/1"&gt;Matteo Matteucci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpenCSI: An Open-Source Dataset for Indoor Localization Using CSI-Based Fingerprinting. (arXiv:2104.07963v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07963</id>
        <link href="http://arxiv.org/abs/2104.07963"/>
        <updated>2021-08-03T02:06:34.359Z</updated>
        <summary type="html"><![CDATA[Many applications require accurate indoor localization. Fingerprint-based
localization methods propose a solution to this problem, but rely on a radio
map that is effort-intensive to acquire. We automate the radio map acquisition
phase using a software-defined radio (SDR) and a wheeled robot. Furthermore, we
open-source a radio map acquired with our automated tool for a 3GPP Long-Term
Evolution (LTE) wireless link. To the best of our knowledge, this is the first
publicly available radio map containing channel state information (CSI).
Finally, we describe first localization experiments on this radio map using a
convolutional neural network to regress for location coordinates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gassner_A/0/1/0/all/0/1"&gt;Arthur Gassner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Musat_C/0/1/0/all/0/1"&gt;Claudiu Musat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rusu_A/0/1/0/all/0/1"&gt;Alexandru Rusu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Burg_A/0/1/0/all/0/1"&gt;Andreas Burg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Backdoor Scanning for Deep Neural Networks through K-Arm Optimization. (arXiv:2102.05123v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05123</id>
        <link href="http://arxiv.org/abs/2102.05123"/>
        <updated>2021-08-03T02:06:34.339Z</updated>
        <summary type="html"><![CDATA[Back-door attack poses a severe threat to deep learning systems. It injects
hidden malicious behaviors to a model such that any input stamped with a
special pattern can trigger such behaviors. Detecting back-door is hence of
pressing need. Many existing defense techniques use optimization to generate
the smallest input pattern that forces the model to misclassify a set of benign
inputs injected with the pattern to a target label. However, the complexity is
quadratic to the number of class labels such that they can hardly handle models
with many classes. Inspired by Multi-Arm Bandit in Reinforcement Learning, we
propose a K-Arm optimization method for backdoor detection. By iteratively and
stochastically selecting the most promising labels for optimization with the
guidance of an objective function, we substantially reduce the complexity,
allowing to handle models with many classes. Moreover, by iteratively refining
the selection of labels to optimize, it substantially mitigates the uncertainty
in choosing the right labels, improving detection accuracy. At the time of
submission, the evaluation of our method on over 4000 models in the IARPA
TrojAI competition from round 1 to the latest round 4 achieves top performance
on the leaderboard. Our technique also supersedes three state-of-the-art
techniques in terms of accuracy and the scanning time needed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_G/0/1/0/all/0/1"&gt;Guangyu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yingqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_G/0/1/0/all/0/1"&gt;Guanhong Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1"&gt;Shengwei An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1"&gt;Qiuling Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1"&gt;Siyuan Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shiqing Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiangyu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data. (arXiv:2106.08977v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08977</id>
        <link href="http://arxiv.org/abs/2106.08977"/>
        <updated>2021-08-03T02:06:34.336Z</updated>
        <summary type="html"><![CDATA[Weak supervision has shown promising results in many natural language
processing tasks, such as Named Entity Recognition (NER). Existing work mainly
focuses on learning deep NER models only with weak supervision, i.e., without
any human annotation, and shows that by merely using weakly labeled data, one
can achieve good performance, though still underperforms fully supervised NER
with manually/strongly labeled data. In this paper, we consider a more
practical scenario, where we have both a small amount of strongly labeled data
and a large amount of weakly labeled data. Unfortunately, we observe that
weakly labeled data does not necessarily improve, or even deteriorate the model
performance (due to the extensive noise in the weak labels) when we train deep
NER models over a simple or weighted combination of the strongly labeled and
weakly labeled data. To address this issue, we propose a new multi-stage
computational framework -- NEEDLE with three essential ingredients: (1) weak
label completion, (2) noise-aware loss function, and (3) final fine-tuning over
the strongly labeled data. Through experiments on E-commerce query NER and
Biomedical NER, we demonstrate that NEEDLE can effectively suppress the noise
of the weak labels and outperforms existing methods. In particular, we achieve
new SOTA F1-scores on 3 Biomedical NER datasets: BC5CDR-chem 93.74,
BC5CDR-disease 90.69, NCBI-disease 92.28.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haoming Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Danqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1"&gt;Tianyu Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1"&gt;Bing Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tuo Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emergent Quantumness in Neural Networks. (arXiv:2012.05082v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05082</id>
        <link href="http://arxiv.org/abs/2012.05082"/>
        <updated>2021-08-03T02:06:34.320Z</updated>
        <summary type="html"><![CDATA[It was recently shown that the Madelung equations, that is, a hydrodynamic
form of the Schr\"odinger equation, can be derived from a canonical ensemble of
neural networks where the quantum phase was identified with the free energy of
hidden variables. We consider instead a grand canonical ensemble of neural
networks, by allowing an exchange of neurons with an auxiliary subsystem, to
show that the free energy must also be multivalued. By imposing the
multivaluedness condition on the free energy we derive the Schr\"odinger
equation with "Planck's constant" determined by the chemical potential of
hidden variables. This shows that quantum mechanics provides a correct
statistical description of the dynamics of the grand canonical ensemble of
neural networks at the learning equilibrium. We also discuss implications of
the results for machine learning, fundamental physics and, in a more
speculative way, evolutionary biology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Katsnelson_M/0/1/0/all/0/1"&gt;Mikhail I. Katsnelson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Vanchurin_V/0/1/0/all/0/1"&gt;Vitaly Vanchurin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReCU: Reviving the Dead Weights in Binary Neural Networks. (arXiv:2103.12369v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12369</id>
        <link href="http://arxiv.org/abs/2103.12369"/>
        <updated>2021-08-03T02:06:34.310Z</updated>
        <summary type="html"><![CDATA[Binary neural networks (BNNs) have received increasing attention due to their
superior reductions of computation and memory. Most existing works focus on
either lessening the quantization error by minimizing the gap between the
full-precision weights and their binarization or designing a gradient
approximation to mitigate the gradient mismatch, while leaving the "dead
weights" untouched. This leads to slow convergence when training BNNs. In this
paper, for the first time, we explore the influence of "dead weights" which
refer to a group of weights that are barely updated during the training of
BNNs, and then introduce rectified clamp unit (ReCU) to revive the "dead
weights" for updating. We prove that reviving the "dead weights" by ReCU can
result in a smaller quantization error. Besides, we also take into account the
information entropy of the weights, and then mathematically analyze why the
weight standardization can benefit BNNs. We demonstrate the inherent
contradiction between minimizing the quantization error and maximizing the
information entropy, and then propose an adaptive exponential scheduler to
identify the range of the "dead weights". By considering the "dead weights",
our method offers not only faster BNN training, but also state-of-the-art
performance on CIFAR-10 and ImageNet, compared with recent methods. Code can be
available at https://github.com/z-hXu/ReCU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zihan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Mingbao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jianzhuang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yue Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Threat of Adversarial Attacks on Deep Learning in Computer Vision: Survey II. (arXiv:2108.00401v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00401</id>
        <link href="http://arxiv.org/abs/2108.00401"/>
        <updated>2021-08-03T02:06:34.290Z</updated>
        <summary type="html"><![CDATA[Deep Learning (DL) is the most widely used tool in the contemporary field of
computer vision. Its ability to accurately solve complex problems is employed
in vision research to learn deep neural models for a variety of tasks,
including security critical applications. However, it is now known that DL is
vulnerable to adversarial attacks that can manipulate its predictions by
introducing visually imperceptible perturbations in images and videos. Since
the discovery of this phenomenon in 2013~[1], it has attracted significant
attention of researchers from multiple sub-fields of machine intelligence. In
[2], we reviewed the contributions made by the computer vision community in
adversarial attacks on deep learning (and their defenses) until the advent of
year 2018. Many of those contributions have inspired new directions in this
area, which has matured significantly since witnessing the first generation
methods. Hence, as a legacy sequel of [2], this literature review focuses on
the advances in this area since 2018. To ensure authenticity, we mainly
consider peer-reviewed contributions published in the prestigious sources of
computer vision and machine learning research. Besides a comprehensive
literature review, the article also provides concise definitions of technical
terminologies for non-experts in this domain. Finally, this article discusses
challenges and future outlook of this direction based on the literature
reviewed herein and [2].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1"&gt;Naveed Akhtar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1"&gt;Ajmal Mian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kardan_N/0/1/0/all/0/1"&gt;Navid Kardan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Mubarak Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SFE-Net: EEG-based Emotion Recognition with Symmetrical Spatial Feature Extraction. (arXiv:2104.06308v4 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06308</id>
        <link href="http://arxiv.org/abs/2104.06308"/>
        <updated>2021-08-03T02:06:34.284Z</updated>
        <summary type="html"><![CDATA[Emotion recognition based on EEG (electroencephalography) has been widely
used in human-computer interaction, distance education and health care.
However, the conventional methods ignore the adjacent and symmetrical
characteristics of EEG signals, which also contain salient information related
to emotion. In this paper, a spatial folding ensemble network (SFE-Net) is
presented for EEG feature extraction and emotion recognition. Firstly, for the
undetected area between EEG electrodes, an improved Bicubic-EEG interpolation
algorithm is developed for EEG channels information completion, which allows us
to extract a wider range of adjacent space features. Then, motivated by the
spatial symmetric mechanism of human brain, we fold the input EEG channels data
with five different symmetrical strategies, which enable the proposed network
to extract the information of space features of EEG signals more effectively.
Finally, a 3DCNN-based spatial, temporal extraction, and a multi-voting
strategy of ensemble learning are integrated to model a new neural network.
With this network, the spatial features of different symmetric folding signals
can be extracted simultaneously, which greatly improves the robustness and
accuracy of emotion recognition. The experimental results on DEAP and SEED
datasets show that the proposed algorithm has comparable performance in terms
of recognition accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Deng_X/0/1/0/all/0/1"&gt;Xiangwen Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Junlin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shangming Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[InversionNet3D: Efficient and Scalable Learning for 3D Full Waveform Inversion. (arXiv:2103.14158v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14158</id>
        <link href="http://arxiv.org/abs/2103.14158"/>
        <updated>2021-08-03T02:06:34.278Z</updated>
        <summary type="html"><![CDATA[Seismic full-waveform inversion (FWI) techniques aim to find a
high-resolution subsurface geophysical model provided with waveform data. Some
recent effort in data-driven FWI has shown some encouraging results in
obtaining 2D velocity maps. However, due to high computational complexity and
large memory consumption, the reconstruction of 3D high-resolution velocity
maps via deep networks is still a great challenge. In this paper, we present
InversionNet3D, an efficient and scalable encoder-decoder network for 3D FWI.
The proposed method employs group convolution in the encoder to establish an
effective hierarchy for learning information from multiple sources while
cutting down unnecessary parameters and operations at the same time. The
introduction of invertible layers further reduces the memory consumption of
intermediate features during training and thus enables the development of
deeper networks with more layers and higher capacity as required by different
application scenarios. Experiments on the 3D Kimberlina dataset demonstrate
that InversionNet3D achieves state-of-the-art reconstruction performance with
lower computational cost and lower memory footprint compared to the baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1"&gt;Qili Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1"&gt;Shihang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wohlberg_B/0/1/0/all/0/1"&gt;Brendt Wohlberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Youzuo Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation. (arXiv:2103.03102v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03102</id>
        <link href="http://arxiv.org/abs/2103.03102"/>
        <updated>2021-08-03T02:06:34.272Z</updated>
        <summary type="html"><![CDATA[The accuracy of DL classifiers is unstable in that it often changes
significantly when retested on adversarial images, imperfect images, or
perturbed images. This paper adds to the small but fundamental body of work on
benchmarking the robustness of DL classifiers on defective images. Unlike
existed single-factor digital perturbation work, we provide state-of-the-art
two-factor perturbation that provides two natural perturbations on images
applied in different sequences. The two-factor perturbation includes (1) two
digital perturbations (Salt & pepper noise and Gaussian noise) applied in both
sequences. (2) one digital perturbation (salt & pepper noise) and a geometric
perturbation (rotation) applied in different sequences. To measure robust DL
classifiers, previous scientists provided 15 types of single-factor corruption.
We created 69 benchmarking image sets, including a clean set, sets with single
factor perturbations, and sets with two-factor perturbation conditions. To be
best of our knowledge, this is the first report that two-factor perturbed
images improves both robustness and accuracy of DL classifiers. Previous
research evaluating deep learning (DL) classifiers has often used top-1/top-5
accuracy, so researchers have usually offered tables, line diagrams, and bar
charts to display accuracy of DL classifiers. But these existed approaches
cannot quantitively evaluate robustness of DL classifiers. We innovate a new
two-dimensional, statistical visualization tool, including mean accuracy and
coefficient of variation (CV), to benchmark the robustness of DL classifiers.
All source codes and related image sets are shared on websites
(this http URL or
https://github.com/daiweiworking/RobustDeepLearningUsingPerturbations ) to
support future academic research and industry projects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1"&gt;Wei Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berleant_D/0/1/0/all/0/1"&gt;Daniel Berleant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast model-based clustering of partial records. (arXiv:2103.16336v4 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16336</id>
        <link href="http://arxiv.org/abs/2103.16336"/>
        <updated>2021-08-03T02:06:34.265Z</updated>
        <summary type="html"><![CDATA[Partially recorded data are frequently encountered in many applications and
usually clustered by first removing incomplete cases or features with missing
values, or by imputing missing values, followed by application of a clustering
algorithm to the resulting altered dataset. Here, we develop clustering
methodology through a model-based approach using the marginal density for the
observed values, assuming a finite mixture model of multivariate $t$
distributions. We compare our approximate algorithm to the corresponding full
expectation-maximization (EM) approach that considers the missing values in the
incomplete data set and makes a missing at random (MAR) assumption, as well as
case deletion and imputation methods. Since only the observed values are
utilized, our approach is computationally more efficient than imputation or
full EM. Simulation studies demonstrate that our approach has favorable
recovery of the true cluster partition compared to case deletion and imputation
under various missingness mechanisms, and is at least competitive with the full
EM approach, even when MAR assumptions are violated. Our methodology is
demonstrated on a problem of clustering gamma-ray bursts and is implemented at
https://github.com/emilygoren/MixtClust.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Goren_E/0/1/0/all/0/1"&gt;Emily M. Goren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Maitra_R/0/1/0/all/0/1"&gt;Ranjan Maitra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human Pose Regression with Residual Log-likelihood Estimation. (arXiv:2107.11291v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.11291</id>
        <link href="http://arxiv.org/abs/2107.11291"/>
        <updated>2021-08-03T02:06:34.247Z</updated>
        <summary type="html"><![CDATA[Heatmap-based methods dominate in the field of human pose estimation by
modelling the output distribution through likelihood heatmaps. In contrast,
regression-based methods are more efficient but suffer from inferior
performance. In this work, we explore maximum likelihood estimation (MLE) to
develop an efficient and effective regression-based methods. From the
perspective of MLE, adopting different regression losses is making different
assumptions about the output density function. A density function closer to the
true distribution leads to a better regression performance. In light of this,
we propose a novel regression paradigm with Residual Log-likelihood Estimation
(RLE) to capture the underlying output distribution. Concretely, RLE learns the
change of the distribution instead of the unreferenced underlying distribution
to facilitate the training process. With the proposed reparameterization
design, our method is compatible with off-the-shelf flow models. The proposed
method is effective, efficient and flexible. We show its potential in various
human pose estimation tasks with comprehensive experiments. Compared to the
conventional regression paradigm, regression with RLE bring 12.4 mAP
improvement on MSCOCO without any test-time overhead. Moreover, for the first
time, especially on multi-person pose estimation, our regression method is
superior to the heatmap-based methods. Our code is available at
https://github.com/Jeff-sjtu/res-loglikelihood-regression]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiefeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1"&gt;Siyuan Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1"&gt;Ailing Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Can Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1"&gt;Bo Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wentao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Cewu Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-shot Neural Architecture Search. (arXiv:2006.06863v9 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.06863</id>
        <link href="http://arxiv.org/abs/2006.06863"/>
        <updated>2021-08-03T02:06:34.241Z</updated>
        <summary type="html"><![CDATA[Efficient evaluation of a network architecture drawn from a large search
space remains a key challenge in Neural Architecture Search (NAS). Vanilla NAS
evaluates each architecture by training from scratch, which gives the true
performance but is extremely time-consuming. Recently, one-shot NAS
substantially reduces the computation cost by training only one supernetwork,
a.k.a. supernet, to approximate the performance of every architecture in the
search space via weight-sharing. However, the performance estimation can be
very inaccurate due to the co-adaption among operations. In this paper, we
propose few-shot NAS that uses multiple supernetworks, called sub-supernet,
each covering different regions of the search space to alleviate the undesired
co-adaption. Compared to one-shot NAS, few-shot NAS improves the accuracy of
architecture evaluation with a small increase of evaluation cost. With only up
to 7 sub-supernets, few-shot NAS establishes new SoTAs: on ImageNet, it finds
models that reach 80.5% top-1 accuracy at 600 MB FLOPS and 77.5% top-1 accuracy
at 238 MFLOPS; on CIFAR10, it reaches 98.72% top-1 accuracy without using extra
data or transfer learning. In Auto-GAN, few-shot NAS outperforms the previously
published results by up to 20%. Extensive experiments show that few-shot NAS
significantly improves various one-shot methods, including 4 gradient-based and
6 search-based methods on 3 different tasks in NasBench-201 and
NasBench1-shot-1.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yiyang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Linnan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuandong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fonseca_R/0/1/0/all/0/1"&gt;Rodrigo Fonseca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1"&gt;Tian Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IPOF: An Extremely and Excitingly Simple Outlier Detection Booster via Infinite Propagation. (arXiv:2108.00360v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00360</id>
        <link href="http://arxiv.org/abs/2108.00360"/>
        <updated>2021-08-03T02:06:34.234Z</updated>
        <summary type="html"><![CDATA[Outlier detection is one of the most popular and continuously rising topics
in the data mining field due to its crucial academic value and extensive
industrial applications. Among different settings, unsupervised outlier
detection is the most challenging and practical one, which attracts tremendous
efforts from diverse perspectives. In this paper, we consider the score-based
outlier detection category and point out that the performance of current
outlier detection algorithms might be further boosted by score propagation.
Specifically, we propose Infinite Propagation of Outlier Factor (iPOF)
algorithm, an extremely and excitingly simple outlier detection booster via
infinite propagation. By employing score-based outlier detectors for
initialization, iPOF updates each data point's outlier score by averaging the
outlier factors of its nearest common neighbors. Extensive experimental results
on numerous datasets in various domains demonstrate the effectiveness and
efficiency of iPOF significantly over several classical and recent
state-of-the-art methods. We also provide the parameter analysis on the number
of neighbors, the unique parameter in iPOF, and different initial outlier
detectors for general validation. It is worthy to note that iPOF brings in
positive improvements ranging from 2% to 46% on the average level, and in some
cases, iPOF boosts the performance over 3000% over the original outlier
detection algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Sibo Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Handong Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hongfu Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Human Haptic Gesture Interpretation for Robotic Systems. (arXiv:2012.01959v4 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01959</id>
        <link href="http://arxiv.org/abs/2012.01959"/>
        <updated>2021-08-03T02:06:34.206Z</updated>
        <summary type="html"><![CDATA[Physical human-robot interactions (pHRI) are less efficient and communicative
than human-human interactions, and a key reason is a lack of informative sense
of touch in robotic systems. Interpreting human touch gestures is a nuanced,
challenging task with extreme gaps between human and robot capability. Among
prior works that demonstrate human touch recognition capability, differences in
sensors, gesture classes, feature sets, and classification algorithms yield a
conglomerate of non-transferable results and a glaring lack of a standard. To
address this gap, this work presents 1) four proposed touch gesture classes
that cover an important subset of the gesture characteristics identified in the
literature, 2) the collection of an extensive force dataset on a common pHRI
robotic arm with only its internal wrist force-torque sensor, and 3) an
exhaustive performance comparison of combinations of feature sets and
classification algorithms on this dataset. We demonstrate high classification
accuracies among our proposed gesture definitions on a test set, emphasizing
that neural net-work classifiers on the raw data outperform other combinations
of feature sets and algorithms. The accompanying video is here:
https://youtu.be/gJPVImNKU68]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bianchini_E/0/1/0/all/0/1"&gt;Elizabeth Bibit Bianchini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1"&gt;Prateek Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salisbury_K/0/1/0/all/0/1"&gt;Kenneth Salisbury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Escaping from Zero Gradient: Revisiting Action-Constrained Reinforcement Learning via Frank-Wolfe Policy Optimization. (arXiv:2102.11055v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11055</id>
        <link href="http://arxiv.org/abs/2102.11055"/>
        <updated>2021-08-03T02:06:34.200Z</updated>
        <summary type="html"><![CDATA[Action-constrained reinforcement learning (RL) is a widely-used approach in
various real-world applications, such as scheduling in networked systems with
resource constraints and control of a robot with kinematic constraints. While
the existing projection-based approaches ensure zero constraint violation, they
could suffer from the zero-gradient problem due to the tight coupling of the
policy gradient and the projection, which results in sample-inefficient
training and slow convergence. To tackle this issue, we propose a learning
algorithm that decouples the action constraints from the policy parameter
update by leveraging state-wise Frank-Wolfe and a regression-based policy
update scheme. Moreover, we show that the proposed algorithm enjoys convergence
and policy improvement properties in the tabular case as well as generalizes
the popular DDPG algorithm for action-constrained RL in the general case.
Through experiments, we demonstrate that the proposed algorithm significantly
outperforms the benchmark methods on a variety of control tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jyun-Li Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hung_W/0/1/0/all/0/1"&gt;Wei Hung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shang-Hsuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_P/0/1/0/all/0/1"&gt;Ping-Chun Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xi Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Deep Learning for HAR with shallow LSTMs. (arXiv:2108.00702v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2108.00702</id>
        <link href="http://arxiv.org/abs/2108.00702"/>
        <updated>2021-08-03T02:06:34.169Z</updated>
        <summary type="html"><![CDATA[Recent studies in Human Activity Recognition (HAR) have shown that Deep
Learning methods are able to outperform classical Machine Learning algorithms.
One popular Deep Learning architecture in HAR is the DeepConvLSTM. In this
paper we propose to alter the DeepConvLSTM architecture to employ a 1-layered
instead of a 2-layered LSTM. We validate our architecture change on 5 publicly
available HAR datasets by comparing the predictive performance with and without
the change employing varying hidden units within the LSTM layer(s). Results
show that across all datasets, our architecture consistently improves on the
original one: Recognition performance increases up to 11.7% for the F1-score,
and our architecture significantly decreases the amount of learnable
parameters. This improvement over DeepConvLSTM decreases training time by as
much as 48%. Our results stand in contrast to the belief that one needs at
least a 2-layered LSTM when dealing with sequential data. Based on our results
we argue that said claim might not be applicable to sensor-based HAR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bock_M/0/1/0/all/0/1"&gt;Marius Bock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoelzemann_A/0/1/0/all/0/1"&gt;Alexander Hoelzemann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moeller_M/0/1/0/all/0/1"&gt;Michael Moeller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laerhoven_K/0/1/0/all/0/1"&gt;Kristof Van Laerhoven&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TDA-Net: Fusion of Persistent Homology and Deep Learning Features for COVID-19 Detection in Chest X-Ray Images. (arXiv:2101.08398v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08398</id>
        <link href="http://arxiv.org/abs/2101.08398"/>
        <updated>2021-08-03T02:06:34.163Z</updated>
        <summary type="html"><![CDATA[Topological Data Analysis (TDA) has emerged recently as a robust tool to
extract and compare the structure of datasets. TDA identifies features in data
such as connected components and holes and assigns a quantitative measure to
these features. Several studies reported that topological features extracted by
TDA tools provide unique information about the data, discover new insights, and
determine which feature is more related to the outcome. On the other hand, the
overwhelming success of deep neural networks in learning patterns and
relationships has been proven on a vast array of data applications, images in
particular. To capture the characteristics of both powerful tools, we propose
\textit{TDA-Net}, a novel ensemble network that fuses topological and deep
features for the purpose of enhancing model generalizability and accuracy. We
apply the proposed \textit{TDA-Net} to a critical application, which is the
automated detection of COVID-19 from CXR images. The experimental results
showed that the proposed network achieved excellent performance and suggests
the applicability of our method in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hajij_M/0/1/0/all/0/1"&gt;Mustafa Hajij&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamzmi_G/0/1/0/all/0/1"&gt;Ghada Zamzmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Batayneh_F/0/1/0/all/0/1"&gt;Fawwaz Batayneh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Multiple Timescales in Hierarchical Echo State Networks. (arXiv:2101.04223v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.04223</id>
        <link href="http://arxiv.org/abs/2101.04223"/>
        <updated>2021-08-03T02:06:34.152Z</updated>
        <summary type="html"><![CDATA[Echo state networks (ESNs) are a powerful form of reservoir computing that
only require training of linear output weights whilst the internal reservoir is
formed of fixed randomly connected neurons. With a correctly scaled
connectivity matrix, the neurons' activity exhibits the echo-state property and
responds to the input dynamics with certain timescales. Tuning the timescales
of the network can be necessary for treating certain tasks, and some
environments require multiple timescales for an efficient representation. Here
we explore the timescales in hierarchical ESNs, where the reservoir is
partitioned into two smaller linked reservoirs with distinct properties. Over
three different tasks (NARMA10, a reconstruction task in a volatile
environment, and psMNIST), we show that by selecting the hyper-parameters of
each partition such that they focus on different timescales, we achieve a
significant performance improvement over a single ESN. Through a linear
analysis, and under the assumption that the timescales of the first partition
are much shorter than the second's (typically corresponding to optimal
operating conditions), we interpret the feedforward coupling of the partitions
in terms of an effective representation of the input signal, provided by the
first partition to the second, whereby the instantaneous input signal is
expanded into a weighted combination of its time derivatives. Furthermore, we
propose a data-driven approach to optimise the hyper-parameters through a
gradient descent optimisation method that is an online approximation of
backpropagation through time. We demonstrate the application of the online
learning rule across all the tasks considered.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Manneschi_L/0/1/0/all/0/1"&gt;Luca Manneschi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ellis_M/0/1/0/all/0/1"&gt;Matthew O. A. Ellis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gigante_G/0/1/0/all/0/1"&gt;Guido Gigante&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_A/0/1/0/all/0/1"&gt;Andrew C. Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giudice_P/0/1/0/all/0/1"&gt;Paolo Del Giudice&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasilaki_E/0/1/0/all/0/1"&gt;Eleni Vasilaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-fidelity Prediction of Megapixel Longitudinal Phase-space Images of Electron Beams using Encoder-Decoder Neural Networks. (arXiv:2101.10437v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10437</id>
        <link href="http://arxiv.org/abs/2101.10437"/>
        <updated>2021-08-03T02:06:34.138Z</updated>
        <summary type="html"><![CDATA[Modeling of large-scale research facilities is extremely challenging due to
complex physical processes and engineering problems. Here, we adopt a
data-driven approach to model the longitudinal phase-space diagnostic beamline
at the photoinector of the European XFEL with an encoder-decoder neural network
model. A deep convolutional neural network (decoder) is used to build images
measured on the screen from a small feature map generated by another neural
network (encoder). We demonstrate that the model trained only with experimental
data can make high-fidelity predictions of megapixel images for the
longitudinal phase-space measurement without any prior knowledge of
photoinjectors and electron beams. The prediction significantly outperforms
existing methods. We also show the scalability and interpretability of the
model by sharing the same decoder with more than one encoder used for different
setups of the photoinjector, and propose a pragmatic way to model a facility
with various diagnostics and working points. This opens the door to a new way
of accurately modeling a photoinjector using neural networks and experimental
data. The approach can possibly be extended to the whole accelerator and even
other types of scientific facilities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Ye Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brinker_F/0/1/0/all/0/1"&gt;Frank Brinker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Decking_W/0/1/0/all/0/1"&gt;Winfried Decking&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomin_S/0/1/0/all/0/1"&gt;Sergey Tomin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schlarb_H/0/1/0/all/0/1"&gt;Holger Schlarb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MFES-HB: Efficient Hyperband with Multi-Fidelity Quality Measurements. (arXiv:2012.03011v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03011</id>
        <link href="http://arxiv.org/abs/2012.03011"/>
        <updated>2021-08-03T02:06:34.125Z</updated>
        <summary type="html"><![CDATA[Hyperparameter optimization (HPO) is a fundamental problem in automatic
machine learning (AutoML). However, due to the expensive evaluation cost of
models (e.g., training deep learning models or training models on large
datasets), vanilla Bayesian optimization (BO) is typically computationally
infeasible. To alleviate this issue, Hyperband (HB) utilizes the early stopping
mechanism to speed up configuration evaluations by terminating those
badly-performing configurations in advance. This leads to two kinds of quality
measurements: (1) many low-fidelity measurements for configurations that get
early-stopped, and (2) few high-fidelity measurements for configurations that
are evaluated without being early stopped. The state-of-the-art HB-style
method, BOHB, aims to combine the benefits of both BO and HB. Instead of
sampling configurations randomly in HB, BOHB samples configurations based on a
BO surrogate model, which is constructed with the high-fidelity measurements
only. However, the scarcity of high-fidelity measurements greatly hampers the
efficiency of BO to guide the configuration search. In this paper, we present
MFES-HB, an efficient Hyperband method that is capable of utilizing both the
high-fidelity and low-fidelity measurements to accelerate the convergence of
HPO tasks. Designing MFES-HB is not trivial as the low-fidelity measurements
can be biased yet informative to guide the configuration search. Thus we
propose to build a Multi- Fidelity Ensemble Surrogate (MFES) based on the
generalized Product of Experts framework, which can integrate useful
information from multi-fidelity measurements effectively. The empirical studies
on the real-world AutoML tasks demonstrate that MFES-HB can achieve 3.3-8.9x
speedups over the state-of-the-art approach - BOHB.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jiawei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jinyang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Ce Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1"&gt;Bin Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OPFython: A Python-Inspired Optimum-Path Forest Classifier. (arXiv:2001.10420v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.10420</id>
        <link href="http://arxiv.org/abs/2001.10420"/>
        <updated>2021-08-03T02:06:34.103Z</updated>
        <summary type="html"><![CDATA[Machine learning techniques have been paramount throughout the last years,
being applied in a wide range of tasks, such as classification, object
recognition, person identification, and image segmentation. Nevertheless,
conventional classification algorithms, e.g., Logistic Regression, Decision
Trees, and Bayesian classifiers, might lack complexity and diversity, not
suitable when dealing with real-world data. A recent graph-inspired classifier,
known as the Optimum-Path Forest, has proven to be a state-of-the-art
technique, comparable to Support Vector Machines and even surpassing it in some
tasks. This paper proposes a Python-based Optimum-Path Forest framework,
denoted as OPFython, where all of its functions and classes are based upon the
original C language implementation. Additionally, as OPFython is a Python-based
library, it provides a more friendly environment and a faster prototyping
workspace than the C language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosa_G/0/1/0/all/0/1"&gt;Gustavo Henrique de Rosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papa_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Paulo Papa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Falcao_A/0/1/0/all/0/1"&gt;Alexandre Xavier Falc&amp;#xe3;o&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The decomposition of the higher-order homology embedding constructed from the $k$-Laplacian. (arXiv:2107.10970v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.10970</id>
        <link href="http://arxiv.org/abs/2107.10970"/>
        <updated>2021-08-03T02:06:34.089Z</updated>
        <summary type="html"><![CDATA[The null space of the $k$-th order Laplacian $\mathbf{\mathcal L}_k$, known
as the {\em $k$-th homology vector space}, encodes the non-trivial topology of
a manifold or a network. Understanding the structure of the homology embedding
can thus disclose geometric or topological information from the data. The study
of the null space embedding of the graph Laplacian $\mathbf{\mathcal L}_0$ has
spurred new research and applications, such as spectral clustering algorithms
with theoretical guarantees and estimators of the Stochastic Block Model. In
this work, we investigate the geometry of the $k$-th homology embedding and
focus on cases reminiscent of spectral clustering. Namely, we analyze the {\em
connected sum} of manifolds as a perturbation to the direct sum of their
homology embeddings. We propose an algorithm to factorize the homology
embedding into subspaces corresponding to a manifold's simplest topological
components. The proposed framework is applied to the {\em shortest homologous
loop detection} problem, a problem known to be NP-hard in general. Our spectral
loop detection algorithm scales better than existing methods and is effective
on diverse data such as point clouds and images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yu-Chia Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Meila_M/0/1/0/all/0/1"&gt;Marina Meil&amp;#x103;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exact Pareto Optimal Search for Multi-Task Learning: Touring the Pareto Front. (arXiv:2108.00597v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00597</id>
        <link href="http://arxiv.org/abs/2108.00597"/>
        <updated>2021-08-03T02:06:34.081Z</updated>
        <summary type="html"><![CDATA[Multi-Task Learning (MTL) is a well-established paradigm for training deep
neural network models for multiple correlated tasks. Often the task objectives
conflict, requiring trade-offs between them during model building. In such
cases, MTL models can use gradient-based multi-objective optimization (MOO) to
find one or more Pareto optimal solutions. A common requirement in MTL
applications is to find an {\it Exact} Pareto optimal (EPO) solution, which
satisfies user preferences with respect to task-specific objective functions.
Further, to improve model generalization, various constraints on the weights
may need to be enforced during training. Addressing these requirements is
challenging because it requires a search direction that allows descent not only
towards the Pareto front but also towards the input preference, within the
constraints imposed and in a manner that scales to high-dimensional gradients.
We design and theoretically analyze such search directions and develop the
first scalable algorithm, with theoretical guarantees of convergence, to find
an EPO solution, including when box and equality constraints are imposed. Our
unique method combines multiple gradient descent with carefully controlled
ascent to traverse the Pareto front in a principled manner, making it robust to
initialization. This also facilitates systematic exploration of the Pareto
front, that we utilize to approximate the Pareto front for multi-criteria
decision-making. Empirical results show that our algorithm outperforms
competing methods on benchmark MTL datasets and MOO problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahapatra_D/0/1/0/all/0/1"&gt;Debabrata Mahapatra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajan_V/0/1/0/all/0/1"&gt;Vaibhav Rajan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthesis of Compositional Animations from Textual Descriptions. (arXiv:2103.14675v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14675</id>
        <link href="http://arxiv.org/abs/2103.14675"/>
        <updated>2021-08-03T02:06:34.075Z</updated>
        <summary type="html"><![CDATA["How can we animate 3D-characters from a movie script or move robots by
simply telling them what we would like them to do?" "How unstructured and
complex can we make a sentence and still generate plausible movements from it?"
These are questions that need to be answered in the long-run, as the field is
still in its infancy. Inspired by these problems, we present a new technique
for generating compositional actions, which handles complex input sentences.
Our output is a 3D pose sequence depicting the actions in the input sentence.
We propose a hierarchical two-stream sequential model to explore a finer
joint-level mapping between natural language sentences and 3D pose sequences
corresponding to the given motion. We learn two manifold representations of the
motion -- one each for the upper body and the lower body movements. Our model
can generate plausible pose sequences for short sentences describing single
actions as well as long compositional sentences describing multiple sequential
and superimposed actions. We evaluate our proposed model on the publicly
available KIT Motion-Language Dataset containing 3D pose data with
human-annotated sentences. Experimental results show that our model advances
the state-of-the-art on text-based motion synthesis in objective evaluations by
a margin of 50%. Qualitative evaluations based on a user study indicate that
our synthesized motions are perceived to be the closest to the ground-truth
motion captures for both short and compositional sentences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1"&gt;Anindita Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheema_N/0/1/0/all/0/1"&gt;Noshaba Cheema&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oguz_C/0/1/0/all/0/1"&gt;Cennet Oguz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slusallek_P/0/1/0/all/0/1"&gt;Philipp Slusallek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning transport cost from subset correspondence. (arXiv:1909.13203v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.13203</id>
        <link href="http://arxiv.org/abs/1909.13203"/>
        <updated>2021-08-03T02:06:34.064Z</updated>
        <summary type="html"><![CDATA[Learning to align multiple datasets is an important problem with many
applications, and it is especially useful when we need to integrate multiple
experiments or correct for confounding. Optimal transport (OT) is a principled
approach to align datasets, but a key challenge in applying OT is that we need
to specify a transport cost function that accurately captures how the two
datasets are related. Reliable cost functions are typically not available and
practitioners often resort to using hand-crafted or Euclidean cost even if it
may not be appropriate. In this work, we investigate how to learn the cost
function using a small amount of side information which is often available. The
side information we consider captures subset correspondence -- i.e. certain
subsets of points in the two data sets are known to be related. For example, we
may have some images labeled as cars in both datasets; or we may have a common
annotated cell type in single-cell data from two batches. We develop an
end-to-end optimizer (OT-SI) that differentiates through the Sinkhorn algorithm
and effectively learns the suitable cost function from side information. On
systematic experiments in images, marriage-matching and single-cell RNA-seq,
our method substantially outperform state-of-the-art benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Ruishan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balsubramani_A/0/1/0/all/0/1"&gt;Akshay Balsubramani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1"&gt;James Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The principle of weight divergence facilitation for unsupervised pattern recognition in spiking neural networks. (arXiv:2104.09943v2 [q-bio.NC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09943</id>
        <link href="http://arxiv.org/abs/2104.09943"/>
        <updated>2021-08-03T02:06:34.039Z</updated>
        <summary type="html"><![CDATA[Parallels between the signal processing tasks and biological neurons lead to
an understanding of the principles of self-organized optimization of input
signal recognition. In the present paper, we discuss such similarities among
biological and technical systems. We propose adding the well-known STDP
synaptic plasticity rule to direct the weight modification towards the state
associated with the maximal difference between background noise and correlated
signals. We use the principle of physically constrained weight growth as a
basis for such weights' modification control. It is proposed that the existence
and production of bio-chemical 'substances' needed for plasticity development
restrict a biological synaptic straight modification. In this paper, the
information about the noise-to-signal ratio controls such a substances'
production and storage and drives the neuron's synaptic pressures towards the
state with the best signal-to-noise ratio. We consider several experiments with
different input signal regimes to understand the functioning of the proposed
approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Nikitin_O/0/1/0/all/0/1"&gt;Oleg Nikitin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Lukyanova_O/0/1/0/all/0/1"&gt;Olga Lukyanova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kunin_A/0/1/0/all/0/1"&gt;Alex Kunin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel Thinning. (arXiv:2105.05842v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05842</id>
        <link href="http://arxiv.org/abs/2105.05842"/>
        <updated>2021-08-03T02:06:34.027Z</updated>
        <summary type="html"><![CDATA[We introduce kernel thinning, a new procedure for compressing a distribution
$\mathbb{P}$ more effectively than i.i.d. sampling or standard thinning. Given
a suitable reproducing kernel $\mathbf{k}$ and $\mathcal{O}(n^2)$ time, kernel
thinning compresses an $n$-point approximation to $\mathbb{P}$ into a
$\sqrt{n}$-point approximation with comparable worst-case integration error
across the associated reproducing kernel Hilbert space. With high probability,
the maximum discrepancy in integration error is
$\mathcal{O}_d(n^{-\frac{1}{2}}\sqrt{\log n})$ for compactly supported
$\mathbb{P}$ and $\mathcal{O}_d(n^{-\frac{1}{2}} \sqrt{(\log n)^{d+1}\log\log
n})$ for sub-exponential $\mathbb{P}$ on $\mathbb{R}^d$. In contrast, an
equal-sized i.i.d. sample from $\mathbb{P}$ suffers $\Omega(n^{-\frac14})$
integration error. Our sub-exponential guarantees resemble the classical
quasi-Monte Carlo error rates for uniform $\mathbb{P}$ on $[0,1]^d$ but apply
to general distributions on $\mathbb{R}^d$ and a wide range of common kernels.
We use our results to derive explicit non-asymptotic maximum mean discrepancy
bounds for Gaussian, Mat\'ern, and B-spline kernels and present two vignettes
illustrating the practical benefits of kernel thinning over i.i.d. sampling and
standard Markov chain Monte Carlo thinning, in dimensions $d=2$ through $100$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Dwivedi_R/0/1/0/all/0/1"&gt;Raaz Dwivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mackey_L/0/1/0/all/0/1"&gt;Lester Mackey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-shot calibration of low-cost air pollution (PM2.5) sensors using meta-learning. (arXiv:2108.00640v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00640</id>
        <link href="http://arxiv.org/abs/2108.00640"/>
        <updated>2021-08-03T02:06:34.021Z</updated>
        <summary type="html"><![CDATA[Low-cost particulate matter sensors are transforming air quality monitoring
because they have lower costs and greater mobility as compared to reference
monitors. Calibration of these low-cost sensors requires training data from
co-deployed reference monitors. Machine Learning based calibration gives better
performance than conventional techniques, but requires a large amount of
training data from the sensor, to be calibrated, co-deployed with a reference
monitor. In this work, we propose novel transfer learning methods for quick
calibration of sensors with minimal co-deployment with reference monitors.
Transfer learning utilizes a large amount of data from other sensors along with
a limited amount of data from the target sensor. Our extensive experimentation
finds the proposed Model-Agnostic- Meta-Learning (MAML) based transfer learning
method to be the most effective over other competitive baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_K/0/1/0/all/0/1"&gt;Kalpit Yadav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arora_V/0/1/0/all/0/1"&gt;Vipul Arora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1"&gt;Sonu Kumar Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1"&gt;Mohit Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1"&gt;Sachchida Nand Tripathi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Making Deep Learning-based Vulnerability Detectors Robust. (arXiv:2108.00669v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.00669</id>
        <link href="http://arxiv.org/abs/2108.00669"/>
        <updated>2021-08-03T02:06:33.989Z</updated>
        <summary type="html"><![CDATA[Automatically detecting software vulnerabilities in source code is an
important problem that has attracted much attention. In particular, deep
learning-based vulnerability detectors, or DL-based detectors, are attractive
because they do not need human experts to define features or patterns of
vulnerabilities. However, such detectors' robustness is unclear. In this paper,
we initiate the study in this aspect by demonstrating that DL-based detectors
are not robust against simple code transformations, dubbed attacks in this
paper, as these transformations may be leveraged for malicious purposes. As a
first step towards making DL-based detectors robust against such attacks, we
propose an innovative framework, dubbed ZigZag, which is centered at (i)
decoupling feature learning and classifier learning and (ii) using a
ZigZag-style strategy to iteratively refine them until they converge to robust
features and robust classifiers. Experimental results show that the ZigZag
framework can substantially improve the robustness of DL-based detectors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jing Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1"&gt;Deqing Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shouhuai Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yichen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1"&gt;Hai Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cohort Bias Adaptation in Aggregated Datasets for Lesion Segmentation. (arXiv:2108.00713v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.00713</id>
        <link href="http://arxiv.org/abs/2108.00713"/>
        <updated>2021-08-03T02:06:33.974Z</updated>
        <summary type="html"><![CDATA[Many automatic machine learning models developed for focal pathology (e.g.
lesions, tumours) detection and segmentation perform well, but do not
generalize as well to new patient cohorts, impeding their widespread adoption
into real clinical contexts. One strategy to create a more diverse,
generalizable training set is to naively pool datasets from different cohorts.
Surprisingly, training on this \it{big data} does not necessarily increase, and
may even reduce, overall performance and model generalizability, due to the
existence of cohort biases that affect label distributions. In this paper, we
propose a generalized affine conditioning framework to learn and account for
cohort biases across multi-source datasets, which we call Source-Conditioned
Instance Normalization (SCIN). Through extensive experimentation on three
different, large scale, multi-scanner, multi-centre Multiple Sclerosis (MS)
clinical trial MRI datasets, we show that our cohort bias adaptation method (1)
improves performance of the network on pooled datasets relative to naively
pooling datasets and (2) can quickly adapt to a new cohort by fine-tuning the
instance normalization parameters, thus learning the new cohort bias with only
10 labelled samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nichyporuk_B/0/1/0/all/0/1"&gt;Brennan Nichyporuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cardinell_J/0/1/0/all/0/1"&gt;Jillian Cardinell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Szeto_J/0/1/0/all/0/1"&gt;Justin Szeto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mehta_R/0/1/0/all/0/1"&gt;Raghav Mehta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1"&gt;Sotirios Tsaftaris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arnold_D/0/1/0/all/0/1"&gt;Douglas L. Arnold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arbel_T/0/1/0/all/0/1"&gt;Tal Arbel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervising Learning, Transfer Learning, and Knowledge Distillation with SimCLR. (arXiv:2108.00587v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00587</id>
        <link href="http://arxiv.org/abs/2108.00587"/>
        <updated>2021-08-03T02:06:33.969Z</updated>
        <summary type="html"><![CDATA[Recent breakthroughs in the field of semi-supervised learning have achieved
results that match state-of-the-art traditional supervised learning methods.
Most successful semi-supervised learning approaches in computer vision focus on
leveraging huge amount of unlabeled data, learning the general representation
via data augmentation and transformation, creating pseudo labels, implementing
different loss functions, and eventually transferring this knowledge to more
task-specific smaller models. In this paper, we aim to conduct our analyses on
three different aspects of SimCLR, the current state-of-the-art semi-supervised
learning framework for computer vision. First, we analyze properties of
contrast learning on fine-tuning, as we understand that contrast learning is
what makes this method so successful. Second, we research knowledge
distillation through teacher-forcing paradigm. We observe that when the teacher
and the student share the same base model, knowledge distillation will achieve
better result. Finally, we study how transfer learning works and its
relationship with the number of classes on different data sets. Our results
indicate that transfer learning performs better when number of classes are
smaller.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Khoi Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_Y/0/1/0/all/0/1"&gt;Yen Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_B/0/1/0/all/0/1"&gt;Bao Le&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-end Robustness for Sensing-Reasoning Machine Learning Pipelines. (arXiv:2003.00120v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.00120</id>
        <link href="http://arxiv.org/abs/2003.00120"/>
        <updated>2021-08-03T02:06:33.948Z</updated>
        <summary type="html"><![CDATA[As machine learning (ML) being applied to many mission-critical scenarios,
certifying ML model robustness becomes increasingly important. Many previous
works focuses on the robustness of independent ML and ensemble models, and can
only certify a very small magnitude of the adversarial perturbation. In this
paper, we take a different viewpoint and improve learning robustness by going
beyond independent ML and ensemble models. We aim at promoting the generic
Sensing-Reasoning machine learning pipeline which contains both the sensing
(e.g. deep neural networks) and reasoning (e.g. Markov logic networks (MLN))
components enriched with domain knowledge. Can domain knowledge help improve
learning robustness? Can we formally certify the end-to-end robustness of such
an ML pipeline? We first theoretically analyze the computational complexity of
checking the provable robustness in the reasoning component. We then derive the
provable robustness bound for several concrete reasoning components. We show
that for reasoning components such as MLN and a specific family of Bayesian
networks it is possible to certify the robustness of the whole pipeline even
with a large magnitude of perturbation which cannot be certified by existing
work. Finally, we conduct extensive real-world experiments on large scale
datasets to evaluate the certified robustness for Sensing-Reasoning ML
pipelines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhuolin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhikuan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pei_H/0/1/0/all/0/1"&gt;Hengzhi Pei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Boxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlas_B/0/1/0/all/0/1"&gt;Bojan Karlas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Ji Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Heng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Ce Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learn to Forget: Machine Unlearning via Neuron Masking. (arXiv:2003.10933v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.10933</id>
        <link href="http://arxiv.org/abs/2003.10933"/>
        <updated>2021-08-03T02:06:33.942Z</updated>
        <summary type="html"><![CDATA[Nowadays, machine learning models, especially neural networks, become
prevalent in many real-world applications.These models are trained based on a
one-way trip from user data: as long as users contribute their data, there is
no way to withdraw; and it is well-known that a neural network memorizes its
training data. This contradicts the "right to be forgotten" clause of GDPR,
potentially leading to law violations. To this end, machine unlearning becomes
a popular research topic, which allows users to eliminate memorization of their
private data from a trained machine learning model.In this paper, we propose
the first uniform metric called for-getting rate to measure the effectiveness
of a machine unlearning method. It is based on the concept of membership
inference and describes the transformation rate of the eliminated data from
"memorized" to "unknown" after conducting unlearning. We also propose a novel
unlearning method calledForsaken. It is superior to previous work in either
utility or efficiency (when achieving the same forgetting rate). We benchmark
Forsaken with eight standard datasets to evaluate its performance. The
experimental results show that it can achieve more than 90\% forgetting rate on
average and only causeless than 5\% accuracy loss.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhuo Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Ximeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jian Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhongyuan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jianfeng Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1"&gt;Kui Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Inference in Educational Systems: A Graphical Modeling Approach. (arXiv:2108.00654v1 [stat.AP])]]></title>
        <id>http://arxiv.org/abs/2108.00654</id>
        <link href="http://arxiv.org/abs/2108.00654"/>
        <updated>2021-08-03T02:06:33.934Z</updated>
        <summary type="html"><![CDATA[Educational systems have traditionally been evaluated using cross-sectional
studies, namely, examining a pretest, posttest, and single intervention.
Although this is a popular approach, it does not model valuable information
such as confounding variables, feedback to students, and other real-world
deviations of studies from ideal conditions. Moreover, learning inherently is a
sequential process and should involve a sequence of interventions. In this
paper, we propose various experimental and quasi-experimental designs for
educational systems and quantify them using the graphical model and directed
acyclic graph (DAG) language. We discuss the applications and limitations of
each method in education. Furthermore, we propose to model the education system
as time-varying treatments, confounders, and time-varying
treatments-confounders feedback. We show that if we control for a sufficient
set of confounders and use appropriate inference techniques such as the inverse
probability of treatment weighting (IPTW) or g-formula, we can close the
backdoor paths and derive the unbiased causal estimate of joint interventions
on the outcome. Finally, we compare the g-formula and IPTW performance and
discuss the pros and cons of using each method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tadayon_M/0/1/0/all/0/1"&gt;Manie Tadayon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pottie_G/0/1/0/all/0/1"&gt;Greg Pottie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Group Fisher Pruning for Practical Network Compression. (arXiv:2108.00708v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00708</id>
        <link href="http://arxiv.org/abs/2108.00708"/>
        <updated>2021-08-03T02:06:33.928Z</updated>
        <summary type="html"><![CDATA[Network compression has been widely studied since it is able to reduce the
memory and computation cost during inference. However, previous methods seldom
deal with complicated structures like residual connections, group/depth-wise
convolution and feature pyramid network, where channels of multiple layers are
coupled and need to be pruned simultaneously. In this paper, we present a
general channel pruning approach that can be applied to various complicated
structures. Particularly, we propose a layer grouping algorithm to find coupled
channels automatically. Then we derive a unified metric based on Fisher
information to evaluate the importance of a single channel and coupled
channels. Moreover, we find that inference speedup on GPUs is more correlated
with the reduction of memory rather than FLOPs, and thus we employ the memory
reduction of each channel to normalize the importance. Our method can be used
to prune any structures including those with coupled channels. We conduct
extensive experiments on various backbones, including the classic ResNet and
ResNeXt, mobile-friendly MobileNetV2, and the NAS-based RegNet, both on image
classification and object detection which is under-explored. Experimental
results validate that our method can effectively prune sophisticated networks,
boosting inference speed without sacrificing accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Liyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shilong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1"&gt;Zhanghui Kuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1"&gt;Aojun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1"&gt;Jing-Hao Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinjiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yimin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wenming Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1"&gt;Qingmin Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wayne Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample Efficient Policy Gradient Methods with Recursive Variance Reduction. (arXiv:1909.08610v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.08610</id>
        <link href="http://arxiv.org/abs/1909.08610"/>
        <updated>2021-08-03T02:06:33.907Z</updated>
        <summary type="html"><![CDATA[Improving the sample efficiency in reinforcement learning has been a
long-standing research problem. In this work, we aim to reduce the sample
complexity of existing policy gradient methods. We propose a novel policy
gradient algorithm called SRVR-PG, which only requires $O(1/\epsilon^{3/2})$
episodes to find an $\epsilon$-approximate stationary point of the nonconcave
performance function $J(\boldsymbol{\theta})$ (i.e., $\boldsymbol{\theta}$ such
that $\|\nabla J(\boldsymbol{\theta})\|_2^2\leq\epsilon$). This sample
complexity improves the existing result $O(1/\epsilon^{5/3})$ for stochastic
variance reduced policy gradient algorithms by a factor of
$O(1/\epsilon^{1/6})$. In addition, we also propose a variant of SRVR-PG with
parameter exploration, which explores the initial policy parameter from a prior
probability distribution. We conduct numerical experiments on classic control
problems in reinforcement learning to validate the performance of our proposed
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1"&gt;Pan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1"&gt;Felicia Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1"&gt;Quanquan Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information Stealing in Federated Learning Systems Based on Generative Adversarial Networks. (arXiv:2108.00701v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00701</id>
        <link href="http://arxiv.org/abs/2108.00701"/>
        <updated>2021-08-03T02:06:33.900Z</updated>
        <summary type="html"><![CDATA[An attack on deep learning systems where intelligent machines collaborate to
solve problems could cause a node in the network to make a mistake on a
critical judgment. At the same time, the security and privacy concerns of AI
have galvanized the attention of experts from multiple disciplines. In this
research, we successfully mounted adversarial attacks on a federated learning
(FL) environment using three different datasets. The attacks leveraged
generative adversarial networks (GANs) to affect the learning process and
strive to reconstruct the private data of users by learning hidden features
from shared local model parameters. The attack was target-oriented drawing data
with distinct class distribution from the CIFAR- 10, MNIST, and Fashion-MNIST
respectively. Moreover, by measuring the Euclidean distance between the real
data and the reconstructed adversarial samples, we evaluated the performance of
the adversary in the learning processes in various scenarios. At last, we
successfully reconstructed the real data of the victim from the shared global
model parameters with all the applied datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yuwei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chong_N/0/1/0/all/0/1"&gt;Ng Chong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ochiai_H/0/1/0/all/0/1"&gt;Hideya Ochiai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Learn to Demodulate with Uncertainty Quantification via Bayesian Meta-Learning. (arXiv:2108.00785v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00785</id>
        <link href="http://arxiv.org/abs/2108.00785"/>
        <updated>2021-08-03T02:06:33.888Z</updated>
        <summary type="html"><![CDATA[Meta-learning, or learning to learn, offers a principled framework for
few-shot learning. It leverages data from multiple related learning tasks to
infer an inductive bias that enables fast adaptation on a new task. The
application of meta-learning was recently proposed for learning how to
demodulate from few pilots. The idea is to use pilots received and stored for
offline use from multiple devices in order to meta-learn an adaptation
procedure with the aim of speeding up online training on new devices. Standard
frequentist learning, which can yield relatively accurate "hard" classification
decisions, is known to be poorly calibrated, particularly in the small-data
regime. Poor calibration implies that the soft scores output by the demodulator
are inaccurate estimates of the true probability of correct demodulation. In
this work, we introduce the use of Bayesian meta-learning via variational
inference for the purpose of obtaining well-calibrated few-pilot demodulators.
In a Bayesian framework, each neural network weight is represented by a
distribution, capturing epistemic uncertainty. Bayesian meta-learning optimizes
over the prior distribution of the weights. The resulting Bayesian ensembles
offer better calibrated soft decisions, at the computational cost of running
multiple instances of the neural network for demodulation. Numerical results
for single-input single-output Rayleigh fading channels with transmitter's
non-linearities are provided that compare symbol error rate and expected
calibration error for both frequentist and Bayesian meta-learning, illustrating
how the latter is both more accurate and better-calibrated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_K/0/1/0/all/0/1"&gt;Kfir M. Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Sangwoo Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simeone_O/0/1/0/all/0/1"&gt;Osvaldo Simeone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamai_S/0/1/0/all/0/1"&gt;Shlomo Shamai&lt;/a&gt; (Shitz)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ab-initio experimental violation of Bell inequalities. (arXiv:2108.00574v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2108.00574</id>
        <link href="http://arxiv.org/abs/2108.00574"/>
        <updated>2021-08-03T02:06:33.881Z</updated>
        <summary type="html"><![CDATA[The violation of a Bell inequality is the paradigmatic example of
device-independent quantum information: the nonclassicality of the data is
certified without the knowledge of the functioning of devices. In practice,
however, all Bell experiments rely on the precise understanding of the
underlying physical mechanisms. Given that, it is natural to ask: Can one
witness nonclassical behaviour in a truly black-box scenario? Here we propose
and implement, computationally and experimentally, a solution to this ab-initio
task. It exploits a robust automated optimization approach based on the
Stochastic Nelder-Mead algorithm. Treating preparation and measurement devices
as black-boxes, and relying on the observed statistics only, our adaptive
protocol approaches the optimal Bell inequality violation after a limited
number of iterations for a variety photonic states, measurement responses and
Bell scenarios. In particular, we exploit it for randomness certification from
unknown states and measurements. Our results demonstrate the power of automated
algorithms, opening a new venue for the experimental implementation of
device-independent quantum technologies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Poderini_D/0/1/0/all/0/1"&gt;Davide Poderini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Polino_E/0/1/0/all/0/1"&gt;Emanuele Polino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Rodari_G/0/1/0/all/0/1"&gt;Giovanni Rodari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Suprano_A/0/1/0/all/0/1"&gt;Alessia Suprano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Chaves_R/0/1/0/all/0/1"&gt;Rafael Chaves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Sciarrino_F/0/1/0/all/0/1"&gt;Fabio Sciarrino&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Piecewise Linear Units Improve Deep Neural Networks. (arXiv:2108.00700v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00700</id>
        <link href="http://arxiv.org/abs/2108.00700"/>
        <updated>2021-08-03T02:06:33.875Z</updated>
        <summary type="html"><![CDATA[The activation function is at the heart of a deep neural networks
nonlinearity; the choice of the function has great impact on the success of
training. Currently, many practitioners prefer the Rectified Linear Unit (ReLU)
due to its simplicity and reliability, despite its few drawbacks. While most
previous functions proposed to supplant ReLU have been hand-designed, recent
work on learning the function during training has shown promising results. In
this paper we propose an adaptive piecewise linear activation function, the
Piecewise Linear Unit (PiLU), which can be learned independently for each
dimension of the neural network. We demonstrate how PiLU is a generalised
rectifier unit and note its similarities with the Adaptive Piecewise Linear
Units, namely adaptive and piecewise linear. Across a distribution of 30
experiments, we show that for the same model architecture, hyperparameters, and
pre-processing, PiLU significantly outperforms ReLU: reducing classification
error by 18.53% on CIFAR-10 and 13.13% on CIFAR-100, for a minor increase in
the number of neurons. Further work should be dedicated to exploring
generalised piecewise linear units, as well as verifying these results across
other challenging domains and larger problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Inturrisi_J/0/1/0/all/0/1"&gt;Jordan Inturrisi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khoo_S/0/1/0/all/0/1"&gt;Sui Yang Khoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kouzani_A/0/1/0/all/0/1"&gt;Abbas Kouzani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pagliarella_R/0/1/0/all/0/1"&gt;Riccardo Pagliarella&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Indian Buffet Neural Networks for Bayesian Continual Learning. (arXiv:1912.02290v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.02290</id>
        <link href="http://arxiv.org/abs/1912.02290"/>
        <updated>2021-08-03T02:06:33.856Z</updated>
        <summary type="html"><![CDATA[We place an Indian Buffet process (IBP) prior over the structure of a
Bayesian Neural Network (BNN), thus allowing the complexity of the BNN to
increase and decrease automatically. We further extend this model such that the
prior on the structure of each hidden layer is shared globally across all
layers, using a Hierarchical-IBP (H-IBP). We apply this model to the problem of
resource allocation in Continual Learning (CL) where new tasks occur and the
network requires extra resources. Our model uses online variational inference
with reparameterisation of the Bernoulli and Beta distributions, which
constitute the IBP and H-IBP priors. As we automatically learn the number of
weights in each layer of the BNN, overfitting and underfitting problems are
largely overcome. We show empirically that our approach offers a competitive
edge over existing methods in CL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kessler_S/0/1/0/all/0/1"&gt;Samuel Kessler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nguyen_V/0/1/0/all/0/1"&gt;Vu Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zohren_S/0/1/0/all/0/1"&gt;Stefan Zohren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Roberts_S/0/1/0/all/0/1"&gt;Stephen Roberts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can you tell? SSNet -- a Sagittal Stratum-inspired Neural Network Framework for Sentiment Analysis. (arXiv:2006.12958v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.12958</id>
        <link href="http://arxiv.org/abs/2006.12958"/>
        <updated>2021-08-03T02:06:33.850Z</updated>
        <summary type="html"><![CDATA[When people try to understand nuanced language they typically process
multiple input sensor modalities to complete this cognitive task. It turns out
the human brain has even a specialized neuron formation, called sagittal
stratum, to help us understand sarcasm. We use this biological formation as the
inspiration for designing a neural network architecture that combines
predictions of different models on the same text to construct robust, accurate
and computationally efficient classifiers for sentiment analysis and study
several different realizations. Among them, we propose a systematic new
approach to combining multiple predictions based on a dedicated neural network
and develop mathematical analysis of it along with state-of-the-art
experimental results. We also propose a heuristic-hybrid technique for
combining models and back it up with experimental results on a representative
benchmark dataset and comparisons to other methods to show the advantages of
the new approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vassilev_A/0/1/0/all/0/1"&gt;Apostol Vassilev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1"&gt;Munawar Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1"&gt;Honglan Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiplicative updates for symmetric-cone factorizations. (arXiv:2108.00740v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2108.00740</id>
        <link href="http://arxiv.org/abs/2108.00740"/>
        <updated>2021-08-03T02:06:33.816Z</updated>
        <summary type="html"><![CDATA[Given a matrix $X\in \mathbb{R}^{m\times n}_+$ with non-negative entries, the
cone factorization problem over a cone $\mathcal{K}\subseteq \mathbb{R}^k$
concerns computing $\{ a_1,\ldots, a_{m} \} \subseteq \mathcal{K}$ and $\{
b_1,\ldots, b_{n} \} \subseteq~\mathcal{K}^*$ belonging to its dual so that
$X_{ij} = \langle a_i, b_j \rangle$ for all $i\in [m], j\in [n]$. Cone
factorizations are fundamental to mathematical optimization as they allow us to
express convex bodies as feasible regions of linear conic programs. In this
paper, we introduce and analyze the symmetric-cone multiplicative update (SCMU)
algorithm for computing cone factorizations when $\mathcal{K}$ is symmetric;
i.e., it is self-dual and homogeneous. Symmetric cones are of central interest
in mathematical optimization as they provide a common language for studying
linear optimization over the nonnegative orthant (linear programs), over the
second-order cone (second order cone programs), and over the cone of positive
semidefinite matrices (semidefinite programs). The SCMU algorithm is
multiplicative in the sense that the iterates are updated by applying a
meticulously chosen automorphism of the cone computed using a generalization of
the geometric mean to symmetric cones. Using an extension of Lieb's concavity
theorem and von Neumann's trace inequality to symmetric cones, we show that the
squared loss objective is non-decreasing along the trajectories of the SCMU
algorithm. Specialized to the nonnegative orthant, the SCMU algorithm
corresponds to the seminal algorithm by Lee and Seung for computing Nonnegative
Matrix Factorizations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Soh_Y/0/1/0/all/0/1"&gt;Yong Sheng Soh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Varvitsiotis_A/0/1/0/all/0/1"&gt;Antonios Varvitsiotis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistically Robust Neural Network Classification. (arXiv:1912.04884v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.04884</id>
        <link href="http://arxiv.org/abs/1912.04884"/>
        <updated>2021-08-03T02:06:33.811Z</updated>
        <summary type="html"><![CDATA[Despite their numerous successes, there are many scenarios where adversarial
risk metrics do not provide an appropriate measure of robustness. For example,
test-time perturbations may occur in a probabilistic manner rather than being
generated by an explicit adversary, while the poor train--test generalization
of adversarial metrics can limit their usage to simple problems. Motivated by
this, we develop a probabilistic robust risk framework, the statistically
robust risk (SRR), which considers pointwise corruption distributions, as
opposed to worst-case adversaries. The SRR provides a distinct and
complementary measure of robust performance, compared to natural and
adversarial risk. We show that the SRR admits estimation and training schemes
which are as simple and efficient as for the natural risk: these simply require
noising the inputs, but with a principled derivation for exactly how and why
this should be done. Furthermore, we demonstrate both theoretically and
experimentally that it can provide superior generalization performance compared
with adversarial risks, enabling application to high-dimensional datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_B/0/1/0/all/0/1"&gt;Benjie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Webb_S/0/1/0/all/0/1"&gt;Stefan Webb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rainforth_T/0/1/0/all/0/1"&gt;Tom Rainforth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FLASH: Fast Neural Architecture Search with Hardware Optimization. (arXiv:2108.00568v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00568</id>
        <link href="http://arxiv.org/abs/2108.00568"/>
        <updated>2021-08-03T02:06:33.799Z</updated>
        <summary type="html"><![CDATA[Neural architecture search (NAS) is a promising technique to design efficient
and high-performance deep neural networks (DNNs). As the performance
requirements of ML applications grow continuously, the hardware accelerators
start playing a central role in DNN design. This trend makes NAS even more
complicated and time-consuming for most real applications. This paper proposes
FLASH, a very fast NAS methodology that co-optimizes the DNN accuracy and
performance on a real hardware platform. As the main theoretical contribution,
we first propose the NN-Degree, an analytical metric to quantify the
topological characteristics of DNNs with skip connections (e.g., DenseNets,
ResNets, Wide-ResNets, and MobileNets). The newly proposed NN-Degree allows us
to do training-free NAS within one second and build an accuracy predictor by
training as few as 25 samples out of a vast search space with more than 63
billion configurations. Second, by performing inference on the target hardware,
we fine-tune and validate our analytical models to estimate the latency, area,
and energy consumption of various DNN architectures while executing standard ML
datasets. Third, we construct a hierarchical algorithm based on simplicial
homology global optimization (SHGO) to optimize the model-architecture
co-design process, while considering the area, latency, and energy consumption
of the target hardware. We demonstrate that, compared to the state-of-the-art
NAS approaches, our proposed hierarchical SHGO-based algorithm enables more
than four orders of magnitude speedup (specifically, the execution time of the
proposed algorithm is about 0.1 seconds). Finally, our experimental evaluations
show that FLASH is easily transferable to different hardware architectures,
thus enabling us to do NAS on a Raspberry Pi-3B processor in less than 3
seconds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guihong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mandal_S/0/1/0/all/0/1"&gt;Sumit K. Mandal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ogras_U/0/1/0/all/0/1"&gt;Umit Y. Ogras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marculescu_R/0/1/0/all/0/1"&gt;Radu Marculescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grain: Improving Data Efficiency of Graph Neural Networks via Diversified Influence Maximization. (arXiv:2108.00219v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00219</id>
        <link href="http://arxiv.org/abs/2108.00219"/>
        <updated>2021-08-03T02:06:33.771Z</updated>
        <summary type="html"><![CDATA[Data selection methods, such as active learning and core-set selection, are
useful tools for improving the data efficiency of deep learning models on
large-scale datasets. However, recent deep learning models have moved forward
from independent and identically distributed data to graph-structured data,
such as social networks, e-commerce user-item graphs, and knowledge graphs.
This evolution has led to the emergence of Graph Neural Networks (GNNs) that go
beyond the models existing data selection methods are designed for. Therefore,
we present Grain, an efficient framework that opens up a new perspective
through connecting data selection in GNNs with social influence maximization.
By exploiting the common patterns of GNNs, Grain introduces a novel feature
propagation concept, a diversified influence maximization objective with novel
influence and diversity functions, and a greedy algorithm with an approximation
guarantee into a unified framework. Empirical studies on public datasets
demonstrate that Grain significantly improves both the performance and
efficiency of data selection (including active learning and core-set selection)
for GNNs. To the best of our knowledge, this is the first attempt to bridge two
largely parallel threads of research, data selection, and social influence
maximization, in the setting of GNNs, paving new ways for improving data
efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wentao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yexin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1"&gt;Bin Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ECLARE: Extreme Classification with Label Graph Correlations. (arXiv:2108.00261v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00261</id>
        <link href="http://arxiv.org/abs/2108.00261"/>
        <updated>2021-08-03T02:06:33.753Z</updated>
        <summary type="html"><![CDATA[Deep extreme classification (XC) seeks to train deep architectures that can
tag a data point with its most relevant subset of labels from an extremely
large label set. The core utility of XC comes from predicting labels that are
rarely seen during training. Such rare labels hold the key to personalized
recommendations that can delight and surprise a user. However, the large number
of rare labels and small amount of training data per rare label offer
significant statistical and computational challenges. State-of-the-art deep XC
methods attempt to remedy this by incorporating textual descriptions of labels
but do not adequately address the problem. This paper presents ECLARE, a
scalable deep learning architecture that incorporates not only label text, but
also label correlations, to offer accurate real-time predictions within a few
milliseconds. Core contributions of ECLARE include a frugal architecture and
scalable techniques to train deep models along with label correlation graphs at
the scale of millions of labels. In particular, ECLARE offers predictions that
are 2 to 14% more accurate on both publicly available benchmark datasets as
well as proprietary datasets for a related products recommendation task sourced
from the Bing search engine. Code for ECLARE is available at
https://github.com/Extreme-classification/ECLARE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1"&gt;Anshul Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sachdeva_N/0/1/0/all/0/1"&gt;Noveen Sachdeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1"&gt;Sheshansh Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sumeet Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1"&gt;Purushottam Kar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1"&gt;Manik Varma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chest ImaGenome Dataset for Clinical Reasoning. (arXiv:2108.00316v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00316</id>
        <link href="http://arxiv.org/abs/2108.00316"/>
        <updated>2021-08-03T02:06:33.744Z</updated>
        <summary type="html"><![CDATA[Despite the progress in automatic detection of radiologic findings from chest
X-ray (CXR) images in recent years, a quantitative evaluation of the
explainability of these models is hampered by the lack of locally labeled
datasets for different findings. With the exception of a few expert-labeled
small-scale datasets for specific findings, such as pneumonia and pneumothorax,
most of the CXR deep learning models to date are trained on global "weak"
labels extracted from text reports, or trained via a joint image and
unstructured text learning strategy. Inspired by the Visual Genome effort in
the computer vision community, we constructed the first Chest ImaGenome dataset
with a scene graph data structure to describe $242,072$ images. Local
annotations are automatically produced using a joint rule-based natural
language processing (NLP) and atlas-based bounding box detection pipeline.
Through a radiologist constructed CXR ontology, the annotations for each CXR
are connected as an anatomy-centered scene graph, useful for image-level
reasoning and multimodal fusion applications. Overall, we provide: i) $1,256$
combinations of relation annotations between $29$ CXR anatomical locations
(objects with bounding box coordinates) and their attributes, structured as a
scene graph per image, ii) over $670,000$ localized comparison relations (for
improved, worsened, or no change) between the anatomical locations across
sequential exams, as well as ii) a manually annotated gold standard scene graph
dataset from $500$ unique patients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Joy T. Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agu_N/0/1/0/all/0/1"&gt;Nkechinyere N. Agu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lourentzou_I/0/1/0/all/0/1"&gt;Ismini Lourentzou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Arjun Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paguio_J/0/1/0/all/0/1"&gt;Joseph A. Paguio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1"&gt;Jasper S. Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dee_E/0/1/0/all/0/1"&gt;Edward C. Dee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitchell_W/0/1/0/all/0/1"&gt;William Mitchell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kashyap_S/0/1/0/all/0/1"&gt;Satyananda Kashyap&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giovannini_A/0/1/0/all/0/1"&gt;Andrea Giovannini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Celi_L/0/1/0/all/0/1"&gt;Leo A. Celi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1"&gt;Mehdi Moradi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You too Brutus! Trapping Hateful Users in Social Media: Challenges, Solutions & Insights. (arXiv:2108.00524v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2108.00524</id>
        <link href="http://arxiv.org/abs/2108.00524"/>
        <updated>2021-08-03T02:06:33.712Z</updated>
        <summary type="html"><![CDATA[Hate speech is regarded as one of the crucial issues plaguing the online
social media. The current literature on hate speech detection leverages
primarily the textual content to find hateful posts and subsequently identify
hateful users. However, this methodology disregards the social connections
between users. In this paper, we run a detailed exploration of the problem
space and investigate an array of models ranging from purely textual to graph
based to finally semi-supervised techniques using Graph Neural Networks (GNN)
that utilize both textual and graph-based features. We run exhaustive
experiments on two datasets -- Gab, which is loosely moderated and Twitter,
which is strictly moderated. Overall the AGNN model achieves 0.791 macro
F1-score on the Gab dataset and 0.780 macro F1-score on the Twitter dataset
using only 5% of the labeled instances, considerably outperforming all the
other models including the fully supervised ones. We perform detailed error
analysis on the best performing text and graph based models and observe that
hateful users have unique network neighborhood signatures and the AGNN model
benefits by paying attention to these signatures. This property, as we observe,
also allows the model to generalize well across platforms in a zero-shot
setting. Lastly, we utilize the best performing GNN model to analyze the
evolution of hateful users and their targets over time in Gab.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Das_M/0/1/0/all/0/1"&gt;Mithun Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saha_P/0/1/0/all/0/1"&gt;Punyajoy Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dutt_R/0/1/0/all/0/1"&gt;Ritam Dutt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1"&gt;Pawan Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1"&gt;Animesh Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathew_B/0/1/0/all/0/1"&gt;Binny Mathew&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pruning Neural Networks with Interpolative Decompositions. (arXiv:2108.00065v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00065</id>
        <link href="http://arxiv.org/abs/2108.00065"/>
        <updated>2021-08-03T02:06:33.692Z</updated>
        <summary type="html"><![CDATA[We introduce a principled approach to neural network pruning that casts the
problem as a structured low-rank matrix approximation. Our method uses a novel
application of a matrix factorization technique called the interpolative
decomposition to approximate the activation output of a network layer. This
technique selects neurons or channels in the layer and propagates a corrective
interpolation matrix to the next layer, resulting in a dense, pruned network
with minimal degradation before fine tuning. We demonstrate how to prune a
neural network by first building a set of primitives to prune a single fully
connected or convolution layer and then composing these primitives to prune
deep multi-layer networks. Theoretical guarantees are provided for pruning a
single hidden layer fully connected network. Pruning with interpolative
decompositions achieves strong empirical results compared to the
state-of-the-art on multiple applications from one and two hidden layer
networks on Fashion MNIST to VGG and ResNets on CIFAR-10. Notably, we achieve
an accuracy of 93.62 $\pm$ 0.36% using VGG-16 on CIFAR-10, with a 51% FLOPS
reduction. This gains 0.02% from the full-sized model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chee_J/0/1/0/all/0/1"&gt;Jerry Chee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Renz_M/0/1/0/all/0/1"&gt;Megan Renz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Damle_A/0/1/0/all/0/1"&gt;Anil Damle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1"&gt;Chris De Sa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Foundations of data imbalance and solutions for a data democracy. (arXiv:2108.00071v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00071</id>
        <link href="http://arxiv.org/abs/2108.00071"/>
        <updated>2021-08-03T02:06:33.685Z</updated>
        <summary type="html"><![CDATA[Dealing with imbalanced data is a prevalent problem while performing
classification on the datasets. Many times, this problem contributes to bias
while making decisions or implementing policies. Thus, it is vital to
understand the factors which cause imbalance in the data (or class imbalance).
Such hidden biases and imbalances can lead to data tyranny and a major
challenge to a data democracy. In this chapter, two essential statistical
elements are resolved: the degree of class imbalance and the complexity of the
concept; solving such issues helps in building the foundations of a data
democracy. Furthermore, statistical measures which are appropriate in these
scenarios are discussed and implemented on a real-life dataset (car insurance
claims). In the end, popular data-level methods such as random oversampling,
random undersampling, synthetic minority oversampling technique, Tomek link,
and others are implemented in Python, and their performance is compared.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1"&gt;Ajay Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chong_D/0/1/0/all/0/1"&gt;Deri Chong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Batarseh_F/0/1/0/all/0/1"&gt;Feras A. Batarseh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simple, Fast, and Flexible Framework for Matrix Completion with Infinite Width Neural Networks. (arXiv:2108.00131v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00131</id>
        <link href="http://arxiv.org/abs/2108.00131"/>
        <updated>2021-08-03T02:06:33.672Z</updated>
        <summary type="html"><![CDATA[Matrix completion problems arise in many applications including
recommendation systems, computer vision, and genomics. Increasingly larger
neural networks have been successful in many of these applications, but at
considerable computational costs. Remarkably, taking the width of a neural
network to infinity allows for improved computational performance. In this
work, we develop an infinite width neural network framework for matrix
completion that is simple, fast, and flexible. Simplicity and speed come from
the connection between the infinite width limit of neural networks and kernels
known as neural tangent kernels (NTK). In particular, we derive the NTK for
fully connected and convolutional neural networks for matrix completion. The
flexibility stems from a feature prior, which allows encoding relationships
between coordinates of the target matrix, akin to semi-supervised learning. The
effectiveness of our framework is demonstrated through competitive results for
virtual drug screening and image inpainting/reconstruction. We also provide an
implementation in Python to make our framework accessible on standard hardware
to a broad audience.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Radhakrishnan_A/0/1/0/all/0/1"&gt;Adityanarayanan Radhakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stefanakis_G/0/1/0/all/0/1"&gt;George Stefanakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belkin_M/0/1/0/all/0/1"&gt;Mikhail Belkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uhler_C/0/1/0/all/0/1"&gt;Caroline Uhler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diverse Linguistic Features for Assessing Reading Difficulty of Educational Filipino Texts. (arXiv:2108.00241v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00241</id>
        <link href="http://arxiv.org/abs/2108.00241"/>
        <updated>2021-08-03T02:06:33.666Z</updated>
        <summary type="html"><![CDATA[In order to ensure quality and effective learning, fluency, and
comprehension, the proper identification of the difficulty levels of reading
materials should be observed. In this paper, we describe the development of
automatic machine learning-based readability assessment models for educational
Filipino texts using the most diverse set of linguistic features for the
language. Results show that using a Random Forest model obtained a high
performance of 62.7% in terms of accuracy, and 66.1% when using the optimal
combination of feature sets consisting of traditional and syllable
pattern-based predictors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Imperial_J/0/1/0/all/0/1"&gt;Joseph Marvin Imperial&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ong_E/0/1/0/all/0/1"&gt;Ethel Ong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning of Context-Aware Pitch Prosody Representations. (arXiv:2007.09060v4 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.09060</id>
        <link href="http://arxiv.org/abs/2007.09060"/>
        <updated>2021-08-03T02:06:33.659Z</updated>
        <summary type="html"><![CDATA[In music and speech, meaning is derived at multiple levels of context.
Affect, for example, can be inferred both by a short sound token and by sonic
patterns over a longer temporal window such as an entire recording. In this
letter, we focus on inferring meaning from this dichotomy of contexts. We show
how contextual representations of short sung vocal lines can be implicitly
learned from fundamental frequency ($F_0$) and thus be used as a meaningful
feature space for downstream Music Information Retrieval (MIR) tasks. We
propose three self-supervised deep learning paradigms which leverage pseudotask
learning of these two levels of context to produce latent representation
spaces. We evaluate the usefulness of these representations by embedding unseen
pitch contours into each space and conducting downstream classification tasks.
Our results show that contextual representation can enhance downstream
classification by as much as 15\% as compared to using traditional statistical
contour features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Noufi_C/0/1/0/all/0/1"&gt;Camille Noufi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1"&gt;Prateek Verma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bilevel Optimization for Machine Learning: Algorithm Design and Convergence Analysis. (arXiv:2108.00330v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00330</id>
        <link href="http://arxiv.org/abs/2108.00330"/>
        <updated>2021-08-03T02:06:33.630Z</updated>
        <summary type="html"><![CDATA[Bilevel optimization has become a powerful framework in various machine
learning applications including meta-learning, hyperparameter optimization, and
network architecture search. There are generally two classes of bilevel
optimization formulations for machine learning: 1) problem-based bilevel
optimization, whose inner-level problem is formulated as finding a minimizer of
a given loss function; and 2) algorithm-based bilevel optimization, whose
inner-level solution is an output of a fixed algorithm. For the first class,
two popular types of gradient-based algorithms have been proposed for
hypergradient estimation via approximate implicit differentiation (AID) and
iterative differentiation (ITD). Algorithms for the second class include the
popular model-agnostic meta-learning (MAML) and almost no inner loop (ANIL).
However, the convergence rate and fundamental limitations of bilevel
optimization algorithms have not been well explored.

This thesis provides a comprehensive convergence rate analysis for bilevel
algorithms in the aforementioned two classes. We further propose principled
algorithm designs for bilevel optimization with higher efficiency and
scalability. For the problem-based formulation, we provide a convergence rate
analysis for AID- and ITD-based bilevel algorithms. We then develop
acceleration bilevel algorithms, for which we provide shaper convergence
analysis with relaxed assumptions. We also provide the first lower bounds for
bilevel optimization, and establish the optimality by providing matching upper
bounds under certain conditions. We finally propose new stochastic bilevel
optimization algorithms with lower complexity and higher efficiency in
practice. For the algorithm-based formulation, we develop a theoretical
convergence for general multi-step MAML and ANIL, and characterize the impact
of parameter selections and loss geometries on the their complexities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_K/0/1/0/all/0/1"&gt;Kaiyi Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Greedy Network Enlarging. (arXiv:2108.00177v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00177</id>
        <link href="http://arxiv.org/abs/2108.00177"/>
        <updated>2021-08-03T02:06:33.600Z</updated>
        <summary type="html"><![CDATA[Recent studies on deep convolutional neural networks present a simple
paradigm of architecture design, i.e., models with more MACs typically achieve
better accuracy, such as EfficientNet and RegNet. These works try to enlarge
all the stages in the model with one unified rule by sampling and statistical
methods. However, we observe that some network architectures have similar MACs
and accuracies, but their allocations on computations for different stages are
quite different. In this paper, we propose to enlarge the capacity of CNN
models by improving their width, depth and resolution on stage level. Under the
assumption that the top-performing smaller CNNs are a proper subcomponent of
the top-performing larger CNNs, we propose an greedy network enlarging method
based on the reallocation of computations. With step-by-step modifying the
computations on different stages, the enlarged network will be equipped with
optimal allocation and utilization of MACs. On EfficientNet, our method
consistently outperforms the performance of the original scaling method. In
particular, with application of our method on GhostNet, we achieve
state-of-the-art 80.9% and 84.3% ImageNet top-1 accuracies under the setting of
600M and 4.4B MACs, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chuanjian Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1"&gt;Kai Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1"&gt;An Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yiping Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chunjing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunhe Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Level Graph Matching Networks for Deep Graph Similarity Learning. (arXiv:2007.04395v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.04395</id>
        <link href="http://arxiv.org/abs/2007.04395"/>
        <updated>2021-08-03T02:06:33.589Z</updated>
        <summary type="html"><![CDATA[While the celebrated graph neural networks yield effective representations
for individual nodes of a graph, there has been relatively less success in
extending to the task of graph similarity learning. Recent work on graph
similarity learning has considered either global-level graph-graph interactions
or low-level node-node interactions, however ignoring the rich cross-level
interactions (e.g., between each node of one graph and the other whole graph).
In this paper, we propose a multi-level graph matching network (MGMN) framework
for computing the graph similarity between any pair of graph-structured objects
in an end-to-end fashion. In particular, the proposed MGMN consists of a
node-graph matching network for effectively learning cross-level interactions
between each node of one graph and the other whole graph, and a siamese graph
neural network to learn global-level interactions between two input graphs.
Furthermore, to compensate for the lack of standard benchmark datasets, we have
created and collected a set of datasets for both the graph-graph classification
and graph-graph regression tasks with different sizes in order to evaluate the
effectiveness and robustness of our models. Comprehensive experiments
demonstrate that MGMN consistently outperforms state-of-the-art baseline models
on both the graph-graph classification and graph-graph regression tasks.
Compared with previous work, MGMN also exhibits stronger robustness as the
sizes of the two input graphs increase.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1"&gt;Xiang Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lingfei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Saizhuo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengfei Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1"&gt;Fangli Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Alex X. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chunming Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1"&gt;Shouling Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BoA-PTA, A Bayesian Optimization Accelerated Error-Free SPICE Solver. (arXiv:2108.00257v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00257</id>
        <link href="http://arxiv.org/abs/2108.00257"/>
        <updated>2021-08-03T02:06:33.583Z</updated>
        <summary type="html"><![CDATA[One of the greatest challenges in IC design is the repeated executions of
computationally expensive SPICE simulations, particularly when highly complex
chip testing/verification is involved. Recently, pseudo transient analysis
(PTA) has shown to be one of the most promising continuation SPICE solver.
However, the PTA efficiency is highly influenced by the inserted
pseudo-parameters. In this work, we proposed BoA-PTA, a Bayesian optimization
accelerated PTA that can substantially accelerate simulations and improve
convergence performance without introducing extra errors. Furthermore, our
method does not require any pre-computation data or offline training. The
acceleration framework can either be implemented to speed up ongoing repeated
simulations immediately or to improve new simulations of completely different
circuits. BoA-PTA is equipped with cutting-edge machine learning techniques,
e.g., deep learning, Gaussian process, Bayesian optimization, non-stationary
monotonic transformation, and variational inference via parameterization. We
assess BoA-PTA in 43 benchmark circuits against other SOTA SPICE solvers and
demonstrate an average 2.3x (maximum 3.5x) speed-up over the original CEPTA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xing_W/0/1/0/all/0/1"&gt;Wei W. Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1"&gt;Xiang Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1"&gt;Dan Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Weishen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1"&gt;Zhou Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RLTutor: Reinforcement Learning Based Adaptive Tutoring System by Modeling Virtual Student with Fewer Interactions. (arXiv:2108.00268v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.00268</id>
        <link href="http://arxiv.org/abs/2108.00268"/>
        <updated>2021-08-03T02:06:33.576Z</updated>
        <summary type="html"><![CDATA[A major challenge in the field of education is providing review schedules
that present learned items at appropriate intervals to each student so that
memory is retained over time. In recent years, attempts have been made to
formulate item reviews as sequential decision-making problems to realize
adaptive instruction based on the knowledge state of students. It has been
reported previously that reinforcement learning can help realize mathematical
models of students learning strategies to maintain a high memory rate. However,
optimization using reinforcement learning requires a large number of
interactions, and thus it cannot be applied directly to actual students. In
this study, we propose a framework for optimizing teaching strategies by
constructing a virtual model of the student while minimizing the interaction
with the actual teaching target. In addition, we conducted an experiment
considering actual instructions using the mathematical model and confirmed that
the model performance is comparable to that of conventional teaching methods.
Our framework can directly substitute mathematical models used in experiments
with human students, and our results can serve as a buffer between theoretical
instructional optimization and practical applications in e-learning systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kubotani_Y/0/1/0/all/0/1"&gt;Yoshiki Kubotani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fukuhara_Y/0/1/0/all/0/1"&gt;Yoshihiro Fukuhara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morishima_S/0/1/0/all/0/1"&gt;Shigeo Morishima&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Debiasing Samples from Online Learning Using Bootstrap. (arXiv:2108.00236v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00236</id>
        <link href="http://arxiv.org/abs/2108.00236"/>
        <updated>2021-08-03T02:06:33.560Z</updated>
        <summary type="html"><![CDATA[It has been recently shown in the literature that the sample averages from
online learning experiments are biased when used to estimate the mean reward.
To correct the bias, off-policy evaluation methods, including importance
sampling and doubly robust estimators, typically calculate the propensity
score, which is unavailable in this setting due to unknown reward distribution
and the adaptive policy. This paper provides a procedure to debias the samples
using bootstrap, which doesn't require the knowledge of the reward distribution
at all. Numerical experiments demonstrate the effective bias reduction for
samples generated by popular multi-armed bandit algorithms such as
Explore-Then-Commit (ETC), UCB, Thompson sampling and $\epsilon$-greedy. We
also analyze and provide theoretical justifications for the procedure under the
ETC algorithm, including the asymptotic convergence of the bias decay rate in
the real and bootstrap worlds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Ningyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xuefeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1"&gt;Yi Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speech2AffectiveGestures: Synthesizing Co-Speech Gestures with Generative Adversarial Affective Expression Learning. (arXiv:2108.00262v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2108.00262</id>
        <link href="http://arxiv.org/abs/2108.00262"/>
        <updated>2021-08-03T02:06:33.553Z</updated>
        <summary type="html"><![CDATA[We present a generative adversarial network to synthesize 3D pose sequences
of co-speech upper-body gestures with appropriate affective expressions. Our
network consists of two components: a generator to synthesize gestures from a
joint embedding space of features encoded from the input speech and the seed
poses, and a discriminator to distinguish between the synthesized pose
sequences and real 3D pose sequences. We leverage the Mel-frequency cepstral
coefficients and the text transcript computed from the input speech in separate
encoders in our generator to learn the desired sentiments and the associated
affective cues. We design an affective encoder using multi-scale
spatial-temporal graph convolutions to transform 3D pose sequences into latent,
pose-based affective features. We use our affective encoder in both our
generator, where it learns affective features from the seed poses to guide the
gesture synthesis, and our discriminator, where it enforces the synthesized
gestures to contain the appropriate affective expressions. We perform extensive
evaluations on two benchmark datasets for gesture synthesis from the speech,
the TED Gesture Dataset and the GENEA Challenge 2020 Dataset. Compared to the
best baselines, we improve the mean absolute joint error by 10--33%, the mean
acceleration difference by 8--58%, and the Fr\'echet Gesture Distance by
21--34%. We also conduct a user study and observe that compared to the best
current baselines, around 15.28% of participants indicated our synthesized
gestures appear more plausible, and around 16.32% of participants felt the
gestures had more appropriate affective expressions aligned with the speech.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1"&gt;Uttaran Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Childs_E/0/1/0/all/0/1"&gt;Elizabeth Childs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rewkowski_N/0/1/0/all/0/1"&gt;Nicholas Rewkowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1"&gt;Dinesh Manocha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flip Learning: Erase to Segment. (arXiv:2108.00752v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00752</id>
        <link href="http://arxiv.org/abs/2108.00752"/>
        <updated>2021-08-03T02:06:33.541Z</updated>
        <summary type="html"><![CDATA[Nodule segmentation from breast ultrasound images is challenging yet
essential for the diagnosis. Weakly-supervised segmentation (WSS) can help
reduce time-consuming and cumbersome manual annotation. Unlike existing
weakly-supervised approaches, in this study, we propose a novel and general WSS
framework called Flip Learning, which only needs the box annotation.
Specifically, the target in the label box will be erased gradually to flip the
classification tag, and the erased region will be considered as the
segmentation result finally. Our contribution is three-fold. First, our
proposed approach erases on superpixel level using a Multi-agent Reinforcement
Learning framework to exploit the prior boundary knowledge and accelerate the
learning process. Second, we design two rewards: classification score and
intensity distribution reward, to avoid under- and over-segmentation,
respectively. Third, we adopt a coarse-to-fine learning strategy to reduce the
residual errors and improve the segmentation performance. Extensively validated
on a large dataset, our proposed approach achieves competitive performance and
shows great potential to narrow the gap between fully-supervised and
weakly-supervised learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yuhao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuxin Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chaoyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_H/0/1/0/all/0/1"&gt;Haoran Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravikumar_N/0/1/0/all/0/1"&gt;Nishant Ravikumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1"&gt;Alejandro F Frangi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jianqiao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1"&gt;Dong Ni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Plant Root System Algorithm Based on Swarm Intelligence for One-dimensional Biomedical Signal Feature Engineering. (arXiv:2108.00214v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00214</id>
        <link href="http://arxiv.org/abs/2108.00214"/>
        <updated>2021-08-03T02:06:33.533Z</updated>
        <summary type="html"><![CDATA[To date, very few biomedical signals have transitioned from research
applications to clinical applications. This is largely due to the lack of trust
in the diagnostic ability of non-stationary signals. To reach the level of
clinical diagnostic application, classification using high-quality signal
features is necessary. While there has been considerable progress in machine
learning in recent years, especially deep learning, progress has been quite
limited in the field of feature engineering. This study proposes a feature
extraction algorithm based on group intelligence which we call a Plant Root
System (PRS) algorithm. Importantly, the correlation between features produced
by this PRS algorithm and traditional features is low, and the accuracy of
several widely-used classifiers was found to be substantially improved with the
addition of PRS features. It is expected that more biomedical signals can be
applied to clinical diagnosis using the proposed algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1"&gt;Rui Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hase_K/0/1/0/all/0/1"&gt;Kazunori Hase&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical analysis on Transparent Algorithmic Exploration in Recommender Systems. (arXiv:2108.00151v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.00151</id>
        <link href="http://arxiv.org/abs/2108.00151"/>
        <updated>2021-08-03T02:06:33.527Z</updated>
        <summary type="html"><![CDATA[All learning algorithms for recommendations face inevitable and critical
trade-off between exploiting partial knowledge of a user's preferences for
short-term satisfaction and exploring additional user preferences for long-term
coverage. Although exploration is indispensable for long success of a
recommender system, the exploration has been considered as the risk to decrease
user satisfaction. The reason for the risk is that items chosen for exploration
frequently mismatch with the user's interests. To mitigate this risk,
recommender systems have mixed items chosen for exploration into a
recommendation list, disguising the items as recommendations to elicit feedback
on the items to discover the user's additional tastes. This mix-in approach has
been widely used in many recommenders, but there is rare research, evaluating
the effectiveness of the mix-in approach or proposing a new approach for
eliciting user feedback without deceiving users. In this work, we aim to
propose a new approach for feedback elicitation without any deception and
compare our approach to the conventional mix-in approach for evaluation. To
this end, we designed a recommender interface that reveals which items are for
exploration and conducted a within-subject study with 94 MTurk workers. Our
results indicated that users left significantly more feedback on items chosen
for exploration with our interface. Besides, users evaluated that our new
interface is better than the conventional mix-in interface in terms of novelty,
diversity, transparency, trust, and satisfaction. Finally, path analysis show
that, in only our new interface, exploration caused to increase user-centric
evaluation metrics. Our work paves the way for how to design an interface,
which utilizes learning algorithm based on users' feedback signals, giving
better user experience and gathering more feedback data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kihwan Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian analysis of the prevalence bias: learning and predicting from imbalanced data. (arXiv:2108.00250v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00250</id>
        <link href="http://arxiv.org/abs/2108.00250"/>
        <updated>2021-08-03T02:06:33.520Z</updated>
        <summary type="html"><![CDATA[Datasets are rarely a realistic approximation of the target population. Say,
prevalence is misrepresented, image quality is above clinical standards, etc.
This mismatch is known as sampling bias. Sampling biases are a major hindrance
for machine learning models. They cause significant gaps between model
performance in the lab and in the real world. Our work is a solution to
prevalence bias. Prevalence bias is the discrepancy between the prevalence of a
pathology and its sampling rate in the training dataset, introduced upon
collecting data or due to the practioner rebalancing the training batches. This
paper lays the theoretical and computational framework for training models, and
for prediction, in the presence of prevalence bias. Concretely a bias-corrected
loss function, as well as bias-corrected predictive rules, are derived under
the principles of Bayesian risk minimization. The loss exhibits a direct
connection to the information gain. It offers a principled alternative to
heuristic training losses and complements test-time procedures based on
selecting an operating point from summary curves. It integrates seamlessly in
the current paradigm of (deep) learning using stochastic backpropagation and
naturally with Bayesian models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Folgoc_L/0/1/0/all/0/1"&gt;Loic Le Folgoc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baltatzis_V/0/1/0/all/0/1"&gt;Vasileios Baltatzis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alansary_A/0/1/0/all/0/1"&gt;Amir Alansary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1"&gt;Sujal Desai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Devaraj_A/0/1/0/all/0/1"&gt;Anand Devaraj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ellis_S/0/1/0/all/0/1"&gt;Sam Ellis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manzanera_O/0/1/0/all/0/1"&gt;Octavio E. Martinez Manzanera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanavati_F/0/1/0/all/0/1"&gt;Fahdi Kanavati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1"&gt;Arjun Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schnabel_J/0/1/0/all/0/1"&gt;Julia Schnabel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1"&gt;Ben Glocker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical learning method for predicting density-matrix based electron dynamics. (arXiv:2108.00318v1 [physics.chem-ph])]]></title>
        <id>http://arxiv.org/abs/2108.00318</id>
        <link href="http://arxiv.org/abs/2108.00318"/>
        <updated>2021-08-03T02:06:33.514Z</updated>
        <summary type="html"><![CDATA[We develop a statistical method to learn a molecular Hamiltonian matrix from
a time-series of electron density matrices. We extend our previous method to
larger molecular systems by incorporating physical properties to reduce
dimensionality, while also exploiting regularization techniques like ridge
regression for addressing multicollinearity. With the learned Hamiltonian we
can solve the Time-Dependent Hartree-Fock (TDHF) equation to propagate the
electron density in time, and predict its dynamics for field-free and field-on
scenarios. We observe close quantitative agreement between the predicted
dynamics and ground truth for both field-off trajectories similar to the
training data, and field-on trajectories outside of the training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Gupta_P/0/1/0/all/0/1"&gt;Prachi Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Bhat_H/0/1/0/all/0/1"&gt;Harish S. Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ranka_K/0/1/0/all/0/1"&gt;Karnamohit Ranka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Isborn_C/0/1/0/all/0/1"&gt;Christine M. Isborn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional Bures Metric for Domain Adaptation. (arXiv:2108.00302v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00302</id>
        <link href="http://arxiv.org/abs/2108.00302"/>
        <updated>2021-08-03T02:06:33.503Z</updated>
        <summary type="html"><![CDATA[As a vital problem in classification-oriented transfer, unsupervised domain
adaptation (UDA) has attracted widespread attention in recent years. Previous
UDA methods assume the marginal distributions of different domains are shifted
while ignoring the discriminant information in the label distributions. This
leads to classification performance degeneration in real applications. In this
work, we focus on the conditional distribution shift problem which is of great
concern to current conditional invariant models. We aim to seek a kernel
covariance embedding for conditional distribution which remains yet unexplored.
Theoretically, we propose the Conditional Kernel Bures (CKB) metric for
characterizing conditional distribution discrepancy, and derive an empirical
estimation for the CKB metric without introducing the implicit kernel feature
map. It provides an interpretable approach to understand the knowledge transfer
mechanism. The established consistency theory of the empirical estimation
provides a theoretical guarantee for convergence. A conditional distribution
matching network is proposed to learn the conditional invariant and
discriminative features for UDA. Extensive experiments and analysis show the
superiority of our proposed model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;You-Wei Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_C/0/1/0/all/0/1"&gt;Chuan-Xian Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inverse Reinforcement Learning for Strategy Identification. (arXiv:2108.00293v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00293</id>
        <link href="http://arxiv.org/abs/2108.00293"/>
        <updated>2021-08-03T02:06:33.471Z</updated>
        <summary type="html"><![CDATA[In adversarial environments, one side could gain an advantage by identifying
the opponent's strategy. For example, in combat games, if an opponents strategy
is identified as overly aggressive, one could lay a trap that exploits the
opponent's aggressive nature. However, an opponent's strategy is not always
apparent and may need to be estimated from observations of their actions. This
paper proposes to use inverse reinforcement learning (IRL) to identify
strategies in adversarial environments. Specifically, the contributions of this
work are 1) the demonstration of this concept on gaming combat data generated
from three pre-defined strategies and 2) the framework for using IRL to achieve
strategy identification. The numerical experiments demonstrate that the
recovered rewards can be identified using a variety of techniques. In this
paper, the recovered reward are visually displayed, clustered using
unsupervised learning, and classified using a supervised learner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rucker_M/0/1/0/all/0/1"&gt;Mark Rucker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adams_S/0/1/0/all/0/1"&gt;Stephen Adams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayes_R/0/1/0/all/0/1"&gt;Roy Hayes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beling_P/0/1/0/all/0/1"&gt;Peter A. Beling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured DropConnect for Uncertainty Inference in Image Classification. (arXiv:2106.08624v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08624</id>
        <link href="http://arxiv.org/abs/2106.08624"/>
        <updated>2021-08-03T02:06:33.463Z</updated>
        <summary type="html"><![CDATA[With the complexity of the network structure, uncertainty inference has
become an important task to improve the classification accuracy for artificial
intelligence systems. For image classification tasks, we propose a structured
DropConnect (SDC) framework to model the output of a deep neural network by a
Dirichlet distribution. We introduce a DropConnect strategy on weights in the
fully connected layers during training. In test, we split the network into
several sub-networks, and then model the Dirichlet distribution by match its
moments with the mean and variance of the outputs of these sub-networks. The
entropy of the estimated Dirichlet distribution is finally utilized for
uncertainty inference. In this paper, this framework is implemented on LeNet$5$
and VGG$16$ models for misclassification detection and out-of-distribution
detection on MNIST and CIFAR-$10$ datasets. Experimental results show that the
performance of the proposed SDC can be comparable to other uncertainty
inference methods. Furthermore, the SDC is adapted well to different network
structures with certain generalization capabilities and research prospects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1"&gt;Wenqing Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiyang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weidong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhanyu Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Hybrid Ensemble Feature Selection Design for Candidate Biomarkers Discovery from Transcriptome Profiles. (arXiv:2108.00290v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00290</id>
        <link href="http://arxiv.org/abs/2108.00290"/>
        <updated>2021-08-03T02:06:33.440Z</updated>
        <summary type="html"><![CDATA[The discovery of disease biomarkers from gene expression data has been
greatly advanced by feature selection (FS) methods, especially using ensemble
FS (EFS) strategies with perturbation at the data level (i.e., homogeneous,
Hom-EFS) or method level (i.e., heterogeneous, Het-EFS). Here we proposed a
Hybrid EFS (Hyb-EFS) design that explores both types of perturbation to improve
the stability and the predictive power of candidate biomarkers. With this,
Hyb-EFS aims to disrupt associations of good performance with a single dataset,
single algorithm, or a specific combination of both, which is particularly
interesting for better reproducibility of genomic biomarkers. We investigated
the adequacy of our approach for microarray data related to four types of
cancer, carrying out an extensive comparison with other ensemble and single FS
approaches. Five FS methods were used in our experiments: Wx, Symmetrical
Uncertainty (SU), Gain Ratio (GR), Characteristic Direction (GeoDE), and
ReliefF. We observed that the Hyb-EFS and Het-EFS approaches attenuated the
large performance variation observed for most single FS and Hom-EFS across
distinct datasets. Also, the Hyb-EFS improved upon the stability of the Het-EFS
within our domain. Comparing the Hyb-EFS and Het-EFS composed of the
top-performing selectors (Wx, GR, and SU), our hybrid approach surpassed the
equivalent heterogeneous design and the best Hom-EFS (Hom-Wx). Interestingly,
the rankings produced by our Hyb-EFS reached greater biological plausibility,
with a notably high enrichment for cancer-related genes and pathways. Thus, our
experiments suggest the potential of the proposed Hybrid EFS design in
discovering candidate biomarkers from microarray data. Finally, we provide an
open-source framework to support similar analyses in other domains, both as a
user-friendly application and a plain Python package.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Colombelli_F/0/1/0/all/0/1"&gt;Felipe Colombelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kowalski_T/0/1/0/all/0/1"&gt;Thayne Woycinck Kowalski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Recamonde_Mendoza_M/0/1/0/all/0/1"&gt;Mariana Recamonde-Mendoza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PAN++: Towards Efficient and Accurate End-to-End Spotting of Arbitrarily-Shaped Text. (arXiv:2105.00405v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00405</id>
        <link href="http://arxiv.org/abs/2105.00405"/>
        <updated>2021-08-03T02:06:33.422Z</updated>
        <summary type="html"><![CDATA[Scene text detection and recognition have been well explored in the past few
years. Despite the progress, efficient and accurate end-to-end spotting of
arbitrarily-shaped text remains challenging. In this work, we propose an
end-to-end text spotting framework, termed PAN++, which can efficiently detect
and recognize text of arbitrary shapes in natural scenes. PAN++ is based on the
kernel representation that reformulates a text line as a text kernel (central
region) surrounded by peripheral pixels. By systematically comparing with
existing scene text representations, we show that our kernel representation can
not only describe arbitrarily-shaped text but also well distinguish adjacent
text. Moreover, as a pixel-based representation, the kernel representation can
be predicted by a single fully convolutional network, which is very friendly to
real-time applications. Taking the advantages of the kernel representation, we
design a series of components as follows: 1) a computationally efficient
feature enhancement network composed of stacked Feature Pyramid Enhancement
Modules (FPEMs); 2) a lightweight detection head cooperating with Pixel
Aggregation (PA); and 3) an efficient attention-based recognition head with
Masked RoI. Benefiting from the kernel representation and the tailored
components, our method achieves high inference speed while maintaining
competitive accuracy. Extensive experiments show the superiority of our method.
For example, the proposed PAN++ achieves an end-to-end text spotting F-measure
of 64.9 at 29.2 FPS on the Total-Text dataset, which significantly outperforms
the previous best method. Code will be available at: https://git.io/PAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenhai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1"&gt;Enze Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xuebo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1"&gt;Ding Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhibo Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1"&gt;Tong Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chunhua Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fair Representation Learning using Interpolation Enabled Disentanglement. (arXiv:2108.00295v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00295</id>
        <link href="http://arxiv.org/abs/2108.00295"/>
        <updated>2021-08-03T02:06:33.416Z</updated>
        <summary type="html"><![CDATA[With the growing interest in the machine learning community to solve
real-world problems, it has become crucial to uncover the hidden reasoning
behind their decisions by focusing on the fairness and auditing the predictions
made by these black-box models. In this paper, we propose a novel method to
address two key issues: (a) Can we simultaneously learn fair disentangled
representations while ensuring the utility of the learned representation for
downstream tasks, and (b)Can we provide theoretical insights into when the
proposed approach will be both fair and accurate. To address the former, we
propose the method FRIED, Fair Representation learning using Interpolation
Enabled Disentanglement. In our architecture, by imposing a critic-based
adversarial framework, we enforce the interpolated points in the latent space
to be more realistic. This helps in capturing the data manifold effectively and
enhances the utility of the learned representation for downstream prediction
tasks. We address the latter question by developing a theory on
fairness-accuracy trade-offs using classifier-based conditional mutual
information estimation. We demonstrate the effectiveness of FRIED on datasets
of different modalities - tabular, text, and image datasets. We observe that
the representations learned by FRIED are overall fairer in comparison to
existing baselines and also accurate for downstream prediction tasks.
Additionally, we evaluate FRIED on a real-world healthcare claims dataset where
we conduct an expert aided model auditing study providing useful insights into
opioid ad-diction patterns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1"&gt;Akshita Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinzamuri_B/0/1/0/all/0/1"&gt;Bhanukiran Vinzamuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1"&gt;Chandan K. Reddy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Learning Approach to Predict Blood Pressure from PPG Signals. (arXiv:2108.00099v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00099</id>
        <link href="http://arxiv.org/abs/2108.00099"/>
        <updated>2021-08-03T02:06:33.398Z</updated>
        <summary type="html"><![CDATA[Blood Pressure (BP) is one of the four primary vital signs indicating the
status of the body's vital (life-sustaining) functions. BP is difficult to
continuously monitor using a sphygmomanometer (i.e. a blood pressure cuff),
especially in everyday-setting. However, other health signals which can be
easily and continuously acquired, such as photoplethysmography (PPG), show some
similarities with the Aortic Pressure waveform. Based on these similarities, in
recent years several methods were proposed to predict BP from the PPG signal.
Building on these results, we propose an advanced personalized data-driven
approach that uses a three-layer deep neural network to estimate BP based on
PPG signals. Different from previous work, the proposed model analyzes the PPG
signal in time-domain and automatically extracts the most critical features for
this specific application, then uses a variation of recurrent neural networks
called Long-Short-Term-Memory (LSTM) to map the extracted features to the BP
value associated with that time window. Experimental results on two separate
standard hospital datasets, yielded absolute errors mean and absolute error
standard deviation for systolic and diastolic BP values outperforming prior
works.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tazarv_A/0/1/0/all/0/1"&gt;Ali Tazarv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levorato_M/0/1/0/all/0/1"&gt;Marco Levorato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-informed Dyna-Style Model-Based Deep Reinforcement Learning for Dynamic Control. (arXiv:2108.00128v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00128</id>
        <link href="http://arxiv.org/abs/2108.00128"/>
        <updated>2021-08-03T02:06:33.392Z</updated>
        <summary type="html"><![CDATA[Model-based reinforcement learning (MBRL) is believed to have much higher
sample efficiency compared to model-free algorithms by learning a predictive
model of the environment. However, the performance of MBRL highly relies on the
quality of the learned model, which is usually built in a black-box manner and
may have poor predictive accuracy outside of the data distribution. The
deficiencies of the learned model may prevent the policy from being fully
optimized. Although some uncertainty analysis-based remedies have been proposed
to alleviate this issue, model bias still poses a great challenge for MBRL. In
this work, we propose to leverage the prior knowledge of underlying physics of
the environment, where the governing laws are (partially) known. In particular,
we developed a physics-informed MBRL framework, where governing equations and
physical constraints are utilized to inform the model learning and policy
search. By incorporating the prior information of the environment, the quality
of the learned model can be notably improved, while the required interactions
with the environment are significantly reduced, leading to better sample
efficiency and learning performance. The effectiveness and merit have been
demonstrated over a handful of classic control problems, where the environments
are governed by canonical ordinary/partial differential equations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xin-Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jian-Xun Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Separation Capacity of Random Neural Networks. (arXiv:2108.00207v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00207</id>
        <link href="http://arxiv.org/abs/2108.00207"/>
        <updated>2021-08-03T02:06:33.385Z</updated>
        <summary type="html"><![CDATA[Neural networks with random weights appear in a variety of machine learning
applications, most prominently as the initialization of many deep learning
algorithms and as a computationally cheap alternative to fully learned neural
networks. In the present article we enhance the theoretical understanding of
random neural nets by addressing the following data separation problem: under
what conditions can a random neural network make two classes $\mathcal{X}^-,
\mathcal{X}^+ \subset \mathbb{R}^d$ (with positive distance) linearly
separable? We show that a sufficiently large two-layer ReLU-network with
standard Gaussian weights and uniformly distributed biases can solve this
problem with high probability. Crucially, the number of required neurons is
explicitly linked to geometric properties of the underlying sets
$\mathcal{X}^-, \mathcal{X}^+$ and their mutual arrangement. This
instance-specific viewpoint allows us to overcome the usual curse of
dimensionality (exponential width of the layers) in non-pathological situations
where the data carries low-complexity structure. We quantify the relevant
structure of the data in terms of a novel notion of mutual complexity (based on
a localized version of Gaussian mean width), which leads to sound and
informative separation guarantees. We connect our result with related lines of
work on approximation, memorization, and generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dirksen_S/0/1/0/all/0/1"&gt;Sjoerd Dirksen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Genzel_M/0/1/0/all/0/1"&gt;Martin Genzel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacques_L/0/1/0/all/0/1"&gt;Laurent Jacques&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stollenwerk_A/0/1/0/all/0/1"&gt;Alexander Stollenwerk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Freezing Sub-Models During Incremental Process Discovery: Extended Version. (arXiv:2108.00215v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00215</id>
        <link href="http://arxiv.org/abs/2108.00215"/>
        <updated>2021-08-03T02:06:33.379Z</updated>
        <summary type="html"><![CDATA[Process discovery aims to learn a process model from observed process
behavior. From a user's perspective, most discovery algorithms work like a
black box. Besides parameter tuning, there is no interaction between the user
and the algorithm. Interactive process discovery allows the user to exploit
domain knowledge and to guide the discovery process. Previously, an incremental
discovery approach has been introduced where a model, considered to be under
construction, gets incrementally extended by user-selected process behavior.
This paper introduces a novel approach that additionally allows the user to
freeze model parts within the model under construction. Frozen sub-models are
not altered by the incremental approach when new behavior is added to the
model. The user can thus steer the discovery algorithm. Our experiments show
that freezing sub-models can lead to higher quality models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schuster_D/0/1/0/all/0/1"&gt;Daniel Schuster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zelst_S/0/1/0/all/0/1"&gt;Sebastiaan J. van Zelst&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aalst_W/0/1/0/all/0/1"&gt;Wil M. P. van der Aalst&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CrossFormer: A Versatile Vision Transformer Based on Cross-scale Attention. (arXiv:2108.00154v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00154</id>
        <link href="http://arxiv.org/abs/2108.00154"/>
        <updated>2021-08-03T02:06:33.358Z</updated>
        <summary type="html"><![CDATA[Transformers have made much progress in dealing with visual tasks. However,
existing vision transformers still do not possess an ability that is important
to visual input: building the attention among features of different scales. The
reasons for this problem are two-fold: (1) Input embeddings of each layer are
equal-scale without cross-scale features; (2) Some vision transformers
sacrifice the small-scale features of embeddings to lower the cost of the
self-attention module. To make up this defect, we propose Cross-scale Embedding
Layer (CEL) and Long Short Distance Attention (LSDA). In particular, CEL blends
each embedding with multiple patches of different scales, providing the model
with cross-scale embeddings. LSDA splits the self-attention module into a
short-distance and long-distance one, also lowering the cost but keeping both
small-scale and large-scale features in embeddings. Through these two designs,
we achieve cross-scale attention. Besides, we propose dynamic position bias for
vision transformers to make the popular relative position bias apply to
variable-sized images. Based on these proposed modules, we construct our vision
architecture called CrossFormer. Experiments show that CrossFormer outperforms
other transformers on several representative visual tasks, especially object
detection and segmentation. The code has been released:
https://github.com/cheerss/CrossFormer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenxiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1"&gt;Lu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Long Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1"&gt;Deng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiaofei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zooming Into the Darknet: Characterizing Internet Background Radiation and its Structural Changes. (arXiv:2108.00079v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.00079</id>
        <link href="http://arxiv.org/abs/2108.00079"/>
        <updated>2021-08-03T02:06:33.351Z</updated>
        <summary type="html"><![CDATA[Network telescopes or "Darknets" provide a unique window into Internet-wide
malicious activities associated with malware propagation, denial of service
attacks, scanning performed for network reconnaissance, and others. Analyses of
the resulting data can provide actionable insights to security analysts that
can be used to prevent or mitigate cyber-threats. Large Darknets, however,
observe millions of nefarious events on a daily basis which makes the
transformation of the captured information into meaningful insights
challenging. We present a novel framework for characterizing Darknet behavior
and its temporal evolution aiming to address this challenge. The proposed
framework: (i) Extracts a high dimensional representation of Darknet events
composed of features distilled from Darknet data and other external sources;
(ii) Learns, in an unsupervised fashion, an information-preserving
low-dimensional representation of these events (using deep representation
learning) that is amenable to clustering; (iv) Performs clustering of the
scanner data in the resulting representation space and provides interpretable
insights using optimal decision trees; and (v) Utilizes the clustering outcomes
as "signatures" that can be used to detect structural changes in the Darknet
activities. We evaluate the proposed system on a large operational Network
Telescope and demonstrate its ability to detect real-world, high-impact
cybersecurity incidents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kallitsis_M/0/1/0/all/0/1"&gt;Michalis Kallitsis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Honavar_V/0/1/0/all/0/1"&gt;Vasant Honavar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prajapati_R/0/1/0/all/0/1"&gt;Rupesh Prajapati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dinghao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yen_J/0/1/0/all/0/1"&gt;John Yen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pure Exploration and Regret Minimization in Matching Bandits. (arXiv:2108.00230v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.00230</id>
        <link href="http://arxiv.org/abs/2108.00230"/>
        <updated>2021-08-03T02:06:33.344Z</updated>
        <summary type="html"><![CDATA[Finding an optimal matching in a weighted graph is a standard combinatorial
problem. We consider its semi-bandit version where either a pair or a full
matching is sampled sequentially. We prove that it is possible to leverage a
rank-1 assumption on the adjacency matrix to reduce the sample complexity and
the regret of off-the-shelf algorithms up to reaching a linear dependency in
the number of vertices (up to poly log terms).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sentenac_F/0/1/0/all/0/1"&gt;Flore Sentenac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yi_J/0/1/0/all/0/1"&gt;Jialin Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Calauzenes_C/0/1/0/all/0/1"&gt;Cl&amp;#xe9;ment Calauz&amp;#xe8;nes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Perchet_V/0/1/0/all/0/1"&gt;Vianney Perchet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vojnovic_M/0/1/0/all/0/1"&gt;Milan Vojnovic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Learning of Visual Features by Non-Parametrically Predicting View Assignments with Support Samples. (arXiv:2104.13963v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13963</id>
        <link href="http://arxiv.org/abs/2104.13963"/>
        <updated>2021-08-03T02:06:33.337Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel method of learning by predicting view assignments
with support samples (PAWS). The method trains a model to minimize a
consistency loss, which ensures that different views of the same unlabeled
instance are assigned similar pseudo-labels. The pseudo-labels are generated
non-parametrically, by comparing the representations of the image views to
those of a set of randomly sampled labeled images. The distance between the
view representations and labeled representations is used to provide a weighting
over class labels, which we interpret as a soft pseudo-label. By
non-parametrically incorporating labeled samples in this way, PAWS extends the
distance-metric loss used in self-supervised methods such as BYOL and SwAV to
the semi-supervised setting. Despite the simplicity of the approach, PAWS
outperforms other semi-supervised methods across architectures, setting a new
state-of-the-art for a ResNet-50 on ImageNet trained with either 10% or 1% of
the labels, reaching 75.5% and 66.5% top-1 respectively. PAWS requires 4x to
12x less training than the previous best methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Assran_M/0/1/0/all/0/1"&gt;Mahmoud Assran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1"&gt;Mathilde Caron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1"&gt;Ishan Misra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bojanowski_P/0/1/0/all/0/1"&gt;Piotr Bojanowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1"&gt;Armand Joulin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ballas_N/0/1/0/all/0/1"&gt;Nicolas Ballas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1"&gt;Michael Rabbat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human Pose Regression with Residual Log-likelihood Estimation. (arXiv:2107.11291v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.11291</id>
        <link href="http://arxiv.org/abs/2107.11291"/>
        <updated>2021-08-03T02:06:33.329Z</updated>
        <summary type="html"><![CDATA[Heatmap-based methods dominate in the field of human pose estimation by
modelling the output distribution through likelihood heatmaps. In contrast,
regression-based methods are more efficient but suffer from inferior
performance. In this work, we explore maximum likelihood estimation (MLE) to
develop an efficient and effective regression-based methods. From the
perspective of MLE, adopting different regression losses is making different
assumptions about the output density function. A density function closer to the
true distribution leads to a better regression performance. In light of this,
we propose a novel regression paradigm with Residual Log-likelihood Estimation
(RLE) to capture the underlying output distribution. Concretely, RLE learns the
change of the distribution instead of the unreferenced underlying distribution
to facilitate the training process. With the proposed reparameterization
design, our method is compatible with off-the-shelf flow models. The proposed
method is effective, efficient and flexible. We show its potential in various
human pose estimation tasks with comprehensive experiments. Compared to the
conventional regression paradigm, regression with RLE bring 12.4 mAP
improvement on MSCOCO without any test-time overhead. Moreover, for the first
time, especially on multi-person pose estimation, our regression method is
superior to the heatmap-based methods. Our code is available at
https://github.com/Jeff-sjtu/res-loglikelihood-regression]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiefeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1"&gt;Siyuan Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1"&gt;Ailing Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Can Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1"&gt;Bo Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wentao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Cewu Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning with Noisy Labels via Sparse Regularization. (arXiv:2108.00192v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00192</id>
        <link href="http://arxiv.org/abs/2108.00192"/>
        <updated>2021-08-03T02:06:33.321Z</updated>
        <summary type="html"><![CDATA[Learning with noisy labels is an important and challenging task for training
accurate deep neural networks. Some commonly-used loss functions, such as Cross
Entropy (CE), suffer from severe overfitting to noisy labels. Robust loss
functions that satisfy the symmetric condition were tailored to remedy this
problem, which however encounter the underfitting effect. In this paper, we
theoretically prove that \textbf{any loss can be made robust to noisy labels}
by restricting the network output to the set of permutations over a fixed
vector. When the fixed vector is one-hot, we only need to constrain the output
to be one-hot, which however produces zero gradients almost everywhere and thus
makes gradient-based optimization difficult. In this work, we introduce the
sparse regularization strategy to approximate the one-hot constraint, which is
composed of network output sharpening operation that enforces the output
distribution of a network to be sharp and the $\ell_p$-norm ($p\le 1$)
regularization that promotes the network output to be sparse. This simple
approach guarantees the robustness of arbitrary loss functions while not
hindering the fitting ability. Experimental results demonstrate that our method
can significantly improve the performance of commonly-used loss functions in
the presence of noisy labels and class imbalance, and outperform the
state-of-the-art methods. The code is available at
https://github.com/hitcszx/lnl_sr.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xiong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xianming Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chenyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_D/0/1/0/all/0/1"&gt;Deming Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Junjun Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1"&gt;Xiangyang Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Soft Calibration Objectives for Neural Networks. (arXiv:2108.00106v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00106</id>
        <link href="http://arxiv.org/abs/2108.00106"/>
        <updated>2021-08-03T02:06:33.296Z</updated>
        <summary type="html"><![CDATA[Optimal decision making requires that classifiers produce uncertainty
estimates consistent with their empirical accuracy. However, deep neural
networks are often under- or over-confident in their predictions. Consequently,
methods have been developed to improve the calibration of their predictive
uncertainty both during training and post-hoc. In this work, we propose
differentiable losses to improve calibration based on a soft (continuous)
version of the binning operation underlying popular calibration-error
estimators. When incorporated into training, these soft calibration losses
achieve state-of-the-art single-model ECE across multiple datasets with less
than 1% decrease in accuracy. For instance, we observe an 82% reduction in ECE
(70% relative to the post-hoc rescaled ECE) in exchange for a 0.7% relative
decrease in accuracy relative to the cross entropy baseline on CIFAR-100. When
incorporated post-training, the soft-binning-based calibration error objective
improves upon temperature scaling, a popular recalibration method. Overall,
experiments across losses and datasets demonstrate that using
calibration-sensitive procedures yield better uncertainty estimates under
dataset shift than the standard practice of using a cross entropy loss and
post-hoc recalibration methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karandikar_A/0/1/0/all/0/1"&gt;Archit Karandikar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cain_N/0/1/0/all/0/1"&gt;Nicholas Cain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1"&gt;Dustin Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1"&gt;Balaji Lakshminarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shlens_J/0/1/0/all/0/1"&gt;Jonathon Shlens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1"&gt;Michael C. Mozer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roelofs_B/0/1/0/all/0/1"&gt;Becca Roelofs&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Feature Tracker: A Novel Application for Deep Convolutional Neural Networks. (arXiv:2108.00105v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00105</id>
        <link href="http://arxiv.org/abs/2108.00105"/>
        <updated>2021-08-03T02:06:33.289Z</updated>
        <summary type="html"><![CDATA[Feature tracking is the building block of many applications such as visual
odometry, augmented reality, and target tracking. Unfortunately, the
state-of-the-art vision-based tracking algorithms fail in surgical images due
to the challenges imposed by the nature of such environments. In this paper, we
proposed a novel and unified deep learning-based approach that can learn how to
track features reliably as well as learn how to detect such reliable features
for tracking purposes. The proposed network dubbed as Deep-PT, consists of a
tracker network which is a convolutional neural network simulating
cross-correlation in terms of deep learning and two fully connected networks
that operate on the output of intermediate layers of the tracker to detect
features and predict trackability of the detected points. The ability to detect
features based on the capabilities of the tracker distinguishes the proposed
method from previous algorithms used in this area and improves the robustness
of the algorithms against dynamics of the scene. The network is trained using
multiple datasets due to the lack of specialized dataset for feature tracking
datasets and extensive comparisons are conducted to compare the accuracy of
Deep-PT against recent pixel tracking algorithms. As the experiments suggest,
the proposed deep architecture deliberately learns what to track and how to
track and outperforms the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parchami_M/0/1/0/all/0/1"&gt;Mostafa Parchami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sayed_S/0/1/0/all/0/1"&gt;Saif Iftekar Sayed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A survey of Monte Carlo methods for noisy and costly densities with application to reinforcement learning. (arXiv:2108.00490v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00490</id>
        <link href="http://arxiv.org/abs/2108.00490"/>
        <updated>2021-08-03T02:06:32.927Z</updated>
        <summary type="html"><![CDATA[This survey gives an overview of Monte Carlo methodologies using surrogate
models, for dealing with densities which are intractable, costly, and/or noisy.
This type of problem can be found in numerous real-world scenarios, including
stochastic optimization and reinforcement learning, where each evaluation of a
density function may incur some computationally-expensive or even physical
(real-world activity) cost, likely to give different results each time. The
surrogate model does not incur this cost, but there are important trade-offs
and considerations involved in the choice and design of such methodologies. We
classify the different methodologies into three main classes and describe
specific instances of algorithms under a unified notation. A modular scheme
which encompasses the considered methods is also presented. A range of
application scenarios is discussed, with special attention to the
likelihood-free setting and reinforcement learning. Several numerical
comparisons are also provided.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Llorente_F/0/1/0/all/0/1"&gt;F. Llorente&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martino_L/0/1/0/all/0/1"&gt;L. Martino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Read_J/0/1/0/all/0/1"&gt;J. Read&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Delgado_D/0/1/0/all/0/1"&gt;D. Delgado&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning who is in the market from time series: market participant discovery through adversarial calibration of multi-agent simulators. (arXiv:2108.00664v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00664</id>
        <link href="http://arxiv.org/abs/2108.00664"/>
        <updated>2021-08-03T02:06:32.921Z</updated>
        <summary type="html"><![CDATA[In electronic trading markets often only the price or volume time series,
that result from interaction of multiple market participants, are directly
observable. In order to test trading strategies before deploying them to
real-time trading, multi-agent market environments calibrated so that the time
series that result from interaction of simulated agents resemble historical are
often used. To ensure adequate testing, one must test trading strategies in a
variety of market scenarios -- which includes both scenarios that represent
ordinary market days as well as stressed markets (most recently observed due to
the beginning of COVID pandemic). In this paper, we address the problem of
multi-agent simulator parameter calibration to allow simulator capture
characteristics of different market regimes. We propose a novel two-step method
to train a discriminator that is able to distinguish between "real" and "fake"
price and volume time series as a part of GAN with self-attention, and then
utilize it within an optimization framework to tune parameters of a simulator
model with known agent archetypes to represent a market scenario. We conclude
with experimental results that demonstrate effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Storchan_V/0/1/0/all/0/1"&gt;Victor Storchan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vyetrenko_S/0/1/0/all/0/1"&gt;Svitlana Vyetrenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balch_T/0/1/0/all/0/1"&gt;Tucker Balch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Driven Macroscopic Modeling across Knudsen Numbers for Rarefied Gas Dynamics and Application to Rayleigh Scattering. (arXiv:2108.00413v1 [physics.flu-dyn])]]></title>
        <id>http://arxiv.org/abs/2108.00413</id>
        <link href="http://arxiv.org/abs/2108.00413"/>
        <updated>2021-08-03T02:06:32.916Z</updated>
        <summary type="html"><![CDATA[Macroscopic modeling of the gas dynamics across Knudsen numbers from dense
gas region to rarefied gas region remains a great challenge. The reason is
macroscopic models lack accurate constitutive relations valid across different
Knudsen numbers. To address this problem, we proposed a Data-driven, KnUdsen
number Adaptive Linear constitutive relation model named DUAL. The DUAL model
is accurate across a range of Knudsen numbers, from dense to rarefied, through
learning to adapt Knudsen number change from observed data. It is consistent
with the Navier-Stokes equation under the hydrodynamic limit, by utilizing a
constrained neural network. In addition, it naturally satisfies the second law
of thermodynamics and is robust to noisy data. We test the DUAL model on the
calculation of Rayleigh scattering spectra. The DUAL model gives accurate
spectra for various Knudsen numbers and is superior to traditional perturbation
and moment expansion methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Candi Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shiyi Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Masking Neural Networks Using Reachability Graphs to Predict Process Events. (arXiv:2108.00404v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00404</id>
        <link href="http://arxiv.org/abs/2108.00404"/>
        <updated>2021-08-03T02:06:32.910Z</updated>
        <summary type="html"><![CDATA[Decay Replay Mining is a deep learning method that utilizes process model
notations to predict the next event. However, this method does not intertwine
the neural network with the structure of the process model to its full extent.
This paper proposes an approach to further interlock the process model of Decay
Replay Mining with its neural network for next event prediction. The approach
uses a masking layer which is initialized based on the reachability graph of
the process model. Additionally, modifications to the neural network
architecture are proposed to increase the predictive performance. Experimental
results demonstrate the value of the approach and underscore the importance of
discovering precise and generalized process models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Theis_J/0/1/0/all/0/1"&gt;Julian Theis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darabi_H/0/1/0/all/0/1"&gt;Houshang Darabi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Environmental Enforcement with Near Real-Time Monitoring: Likelihood-Based Detection of Structural Expansion of Intensive Livestock Farms. (arXiv:2105.14159v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14159</id>
        <link href="http://arxiv.org/abs/2105.14159"/>
        <updated>2021-08-03T02:06:32.905Z</updated>
        <summary type="html"><![CDATA[Much environmental enforcement in the United States has historically relied
on either self-reported data or physical, resource-intensive, infrequent
inspections. Advances in remote sensing and computer vision, however, have the
potential to augment compliance monitoring by detecting early warning signs of
noncompliance. We demonstrate a process for rapid identification of significant
structural expansion using Planet's 3m/pixel satellite imagery products and
focusing on Concentrated Animal Feeding Operations (CAFOs) in the US as a test
case. Unpermitted building expansion has been a particular challenge with
CAFOs, which pose significant health and environmental risks. Using new
hand-labeled dataset of 145,053 images of 1,513 CAFOs, we combine
state-of-the-art building segmentation with a likelihood-based change-point
detection model to provide a robust signal of building expansion (AUC = 0.86).
A major advantage of this approach is that it can work with higher cadence
(daily to weekly), but lower resolution (3m/pixel), satellite imagery than
previously used in similar environmental settings. It is also highly
generalizable and thus provides a near real-time monitoring tool to prioritize
enforcement resources in other settings where unpermitted construction poses
environmental risk, e.g. zoning, habitat modification, or wetland protection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chugg_B/0/1/0/all/0/1"&gt;Ben Chugg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_B/0/1/0/all/0/1"&gt;Brandon Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eicher_S/0/1/0/all/0/1"&gt;Seiji Eicher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sandy Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_D/0/1/0/all/0/1"&gt;Daniel E. Ho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CNN based Channel Estimation using NOMA for mmWave Massive MIMO System. (arXiv:2108.00367v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.00367</id>
        <link href="http://arxiv.org/abs/2108.00367"/>
        <updated>2021-08-03T02:06:32.896Z</updated>
        <summary type="html"><![CDATA[Non-Orthogonal Multiple Access (NOMA) schemes are being actively explored to
address some of the major challenges in 5th Generation (5G) Wireless
communications. Channel estimation is exceptionally challenging in scenarios
where NOMA schemes are integrated with millimeter wave (mmWave) massive
multiple-input multiple-output (MIMO) systems. An accurate estimation of the
channel is essential in exploiting the benefits of the pairing of the duo-NOMA
and mmWave. This paper proposes a convolutional neural network (CNN) based
approach to estimate the channel for NOMA based millimeter wave (mmWave)
massive multiple-input multiple-output (MIMO) systems built on a hybrid
architecture. Initially, users are grouped into different clusters based on
their channel gains and beamforming technique is performed to maximize the
signal in the direction of desired cluster. A coarse estimation of the channel
is first made from the received signal and this estimate is given as the input
to CNN to fine estimate the channel coefficients. Numerical illustrations show
that the proposed method outperforms least square (LS) estimate, minimum mean
square error (MMSE) estimate and are close to the Cramer-Rao Bound (CRB).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+S_A/0/1/0/all/0/1"&gt;Anu T S&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Raveendran_T/0/1/0/all/0/1"&gt;Tara Raveendran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Realised Volatility Forecasting: Machine Learning via Financial Word Embedding. (arXiv:2108.00480v1 [q-fin.CP])]]></title>
        <id>http://arxiv.org/abs/2108.00480</id>
        <link href="http://arxiv.org/abs/2108.00480"/>
        <updated>2021-08-03T02:06:32.875Z</updated>
        <summary type="html"><![CDATA[We develop FinText, a novel, state-of-the-art, financial word embedding from
Dow Jones Newswires Text News Feed Database. Incorporating this word embedding
in a machine learning model produces a substantial increase in volatility
forecasting performance on days with volatility jumps for 23 NASDAQ stocks from
27 July 2007 to 18 November 2016. A simple ensemble model, combining our word
embedding and another machine learning model that uses limit order book data,
provides the best forecasting performance for both normal and jump volatility
days. Finally, we use Integrated Gradients and SHAP (SHapley Additive
exPlanations) to make the results more 'explainable' and the model comparisons
more transparent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Rahimikia_E/0/1/0/all/0/1"&gt;Eghbal Rahimikia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Zohren_S/0/1/0/all/0/1"&gt;Stefan Zohren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Poon_S/0/1/0/all/0/1"&gt;Ser-Huang Poon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Underwater Image via Adaptive Color and Contrast Enhancement, and Denoising. (arXiv:2104.01073v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01073</id>
        <link href="http://arxiv.org/abs/2104.01073"/>
        <updated>2021-08-03T02:06:32.870Z</updated>
        <summary type="html"><![CDATA[Images captured underwater are often characterized by low contrast, color
distortion, and noise. To address these visual degradations, we propose a novel
scheme by constructing an adaptive color and contrast enhancement, and
denoising (ACCE-D) framework for underwater image enhancement. In the proposed
framework, Difference of Gaussian (DoG) filter and bilateral filter are
respectively employed to decompose the high-frequency and low-frequency
components. Benefited from this separation, we utilize soft-thresholding
operation to suppress the noise in the high-frequency component. Specially, the
low-frequency component is enhanced by using an adaptive color and contrast
enhancement (ACCE) strategy. The proposed ACCE is an adaptive variational
framework implemented in the HSI color space, which integrates data term and
regularized term, as well as introduces Gaussian weight and Heaviside function
to avoid over-enhancement and oversaturation. Moreover, we derive a numerical
solution for ACCE, and adopt a pyramid-based strategy to accelerate the solving
procedure. Experimental results demonstrate that our strategy is effective in
color correction, visibility improvement, and detail revealing. Comparison with
state-of-the-art techniques also validate the superiority of proposed method.
Furthermore, we have verified the utility of our proposed ACCE-D for enhancing
other types of degraded scenes, including foggy scene, sandstorm scene and
low-light scene.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xinjie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_G/0/1/0/all/0/1"&gt;Guojia Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Kunqian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1"&gt;Zhenkuan Pan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthetic Active Distribution System Generation via Unbalanced Graph Generative Adversarial Network. (arXiv:2108.00599v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2108.00599</id>
        <link href="http://arxiv.org/abs/2108.00599"/>
        <updated>2021-08-03T02:06:32.865Z</updated>
        <summary type="html"><![CDATA[Real active distribution networks with associated smart meter (SM) data are
critical for power researchers. However, it is practically difficult for
researchers to obtain such comprehensive datasets from utilities due to privacy
concerns. To bridge this gap, an implicit generative model with Wasserstein GAN
objectives, namely unbalanced graph generative adversarial network (UG-GAN), is
designed to generate synthetic three-phase unbalanced active distribution
system connectivity. The basic idea is to learn the distribution of random
walks both over a real-world system and across each phase of line segments,
capturing the underlying local properties of an individual real-world
distribution network and generating specific synthetic networks accordingly.
Then, to create a comprehensive synthetic test case, a network correction and
extension process is proposed to obtain time-series nodal demands and standard
distribution grid components with realistic parameters, including distributed
energy resources (DERs) and capacity banks. A Midwest distribution system with
1-year SM data has been utilized to validate the performance of our method.
Case studies with several power applications demonstrate that synthetic active
networks generated by the proposed framework can mimic almost all features of
real-world networks while avoiding the disclosure of confidential information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yan_R/0/1/0/all/0/1"&gt;Rong Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yuan_Y/0/1/0/all/0/1"&gt;Yuxuan Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhaoyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Geng_G/0/1/0/all/0/1"&gt;Guangchao Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jiang_Q/0/1/0/all/0/1"&gt;Quanyuan Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DECAF: Deep Extreme Classification with Label Features. (arXiv:2108.00368v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00368</id>
        <link href="http://arxiv.org/abs/2108.00368"/>
        <updated>2021-08-03T02:06:32.859Z</updated>
        <summary type="html"><![CDATA[Extreme multi-label classification (XML) involves tagging a data point with
its most relevant subset of labels from an extremely large label set, with
several applications such as product-to-product recommendation with millions of
products. Although leading XML algorithms scale to millions of labels, they
largely ignore label meta-data such as textual descriptions of the labels. On
the other hand, classical techniques that can utilize label metadata via
representation learning using deep networks struggle in extreme settings. This
paper develops the DECAF algorithm that addresses these challenges by learning
models enriched by label metadata that jointly learn model parameters and
feature representations using deep networks and offer accurate classification
at the scale of millions of labels. DECAF makes specific contributions to model
architecture design, initialization, and training, enabling it to offer up to
2-6% more accurate prediction than leading extreme classifiers on publicly
available benchmark product-to-product recommendation datasets, such as
LF-AmazonTitles-1.3M. At the same time, DECAF was found to be up to 22x faster
at inference than leading deep extreme classifiers, which makes it suitable for
real-time applications that require predictions within a few milliseconds. The
code for DECAF is available at the following URL
https://github.com/Extreme-classification/DECAF.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1"&gt;Anshul Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dahiya_K/0/1/0/all/0/1"&gt;Kunal Dahiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1"&gt;Sheshansh Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saini_D/0/1/0/all/0/1"&gt;Deepak Saini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sumeet Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1"&gt;Purushottam Kar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1"&gt;Manik Varma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-driven model for hydraulic fracturing design optimization. Part II: Inverse problem. (arXiv:2108.00751v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00751</id>
        <link href="http://arxiv.org/abs/2108.00751"/>
        <updated>2021-08-03T02:06:32.852Z</updated>
        <summary type="html"><![CDATA[We describe a stacked model for predicting the cumulative fluid production
for an oil well with a multistage-fracture completion based on a combination of
Ridge Regression and CatBoost algorithms. The model is developed based on an
extended digital field data base of reservoir, well and fracturing design
parameters. The database now includes more than 5000 wells from 23 oilfields of
Western Siberia (Russia), with 6687 fracturing operations in total. Starting
with 387 parameters characterizing each well, including construction, reservoir
properties, fracturing design features and production, we end up with 38 key
parameters used as input features for each well in the model training process.
The model demonstrates physically explainable dependencies plots of the target
on the design parameters (number of stages, proppant mass, average and final
proppant concentrations and fluid rate). We developed a set of methods
including those based on the use of Euclidean distance and clustering
techniques to perform similar (offset) wells search, which is useful for a
field engineer to analyze earlier fracturing treatments on similar wells. These
approaches are also adapted for obtaining the optimization parameters
boundaries for the particular pilot well, as part of the field testing campaign
of the methodology. An inverse problem (selecting an optimum set of fracturing
design parameters to maximize production) is formulated as optimizing a high
dimensional black box approximation function constrained by boundaries and
solved with four different optimization methods: surrogate-based optimization,
sequential least squares programming, particle swarm optimization and
differential evolution. A recommendation system containing all the above
methods is designed to advise a production stimulation engineer on an optimized
fracturing design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duplyakov_V/0/1/0/all/0/1"&gt;Viktor Duplyakov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morozov_A/0/1/0/all/0/1"&gt;Anton Morozov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Popkov_D/0/1/0/all/0/1"&gt;Dmitriy Popkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shel_E/0/1/0/all/0/1"&gt;Egor Shel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vainshtein_A/0/1/0/all/0/1"&gt;Albert Vainshtein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1"&gt;Evgeny Burnaev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osiptsov_A/0/1/0/all/0/1"&gt;Andrei Osiptsov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paderin_G/0/1/0/all/0/1"&gt;Grigory Paderin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep graph matching meets mixed-integer linear programming: Relax at your own risk ?. (arXiv:2108.00394v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00394</id>
        <link href="http://arxiv.org/abs/2108.00394"/>
        <updated>2021-08-03T02:06:32.836Z</updated>
        <summary type="html"><![CDATA[Graph matching is an important problem that has received widespread
attention, especially in the field of computer vision. Recently,
state-of-the-art methods seek to incorporate graph matching with deep learning.
However, there is no research to explain what role the graph matching algorithm
plays in the model. Therefore, we propose an approach integrating a MILP
formulation of the graph matching problem. This formulation is solved to
optimal and it provides inherent baseline. Meanwhile, similar approaches are
derived by releasing the optimal guarantee of the graph matching solver and by
introducing a quality level. This quality level controls the quality of the
solutions provided by the graph matching solver. In addition, several
relaxations of the graph matching problem are put to the test. Our experimental
evaluation gives several theoretical insights and guides the direction of deep
graph matching methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhoubo Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Puqing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raveaux_R/0/1/0/all/0/1"&gt;Romain Raveaux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Huadong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Markov Random Field Inference with Uncertainty Quantification. (arXiv:2108.00570v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2108.00570</id>
        <link href="http://arxiv.org/abs/2108.00570"/>
        <updated>2021-08-03T02:06:32.830Z</updated>
        <summary type="html"><![CDATA[Statistical machine learning has widespread application in various domains.
These methods include probabilistic algorithms, such as Markov Chain
Monte-Carlo (MCMC), which rely on generating random numbers from probability
distributions. These algorithms are computationally expensive on conventional
processors, yet their statistical properties, namely interpretability and
uncertainty quantification (UQ) compared to deep learning, make them an
attractive alternative approach. Therefore, hardware specialization can be
adopted to address the shortcomings of conventional processors in running these
applications.

In this paper, we propose a high-throughput accelerator for Markov Random
Field (MRF) inference, a powerful model for representing a wide range of
applications, using MCMC with Gibbs sampling. We propose a tiled architecture
which takes advantage of near-memory computing, and memory optimizations
tailored to the semantics of MRF. Additionally, we propose a novel hybrid
on-chip/off-chip memory system and logging scheme to efficiently support UQ.
This memory system design is not specific to MRF models and is applicable to
applications using probabilistic algorithms. In addition, it dramatically
reduces off-chip memory bandwidth requirements.

We implemented an FPGA prototype of our proposed architecture using
high-level synthesis tools and achieved 146MHz frequency for an accelerator
with 32 function units on an Intel Arria 10 FPGA. Compared to prior work on
FPGA, our accelerator achieves 26X speedup. Furthermore, our proposed memory
system and logging scheme to support UQ reduces off-chip bandwidth by 71% for
two applications. ASIC analysis in 15nm shows our design with 2048 function
units running at 3GHz outperforms GPU implementations of motion estimation and
stereo vision on Nvidia RTX2080Ti by 120X-210X, occupying only 7.7% of the
area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bashizade_R/0/1/0/all/0/1"&gt;Ramin Bashizade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiangyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1"&gt;Sayan Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lebeck_A/0/1/0/all/0/1"&gt;Alvin R. Lebeck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable Deep Few-shot Anomaly Detection with Deviation Networks. (arXiv:2108.00462v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00462</id>
        <link href="http://arxiv.org/abs/2108.00462"/>
        <updated>2021-08-03T02:06:32.820Z</updated>
        <summary type="html"><![CDATA[Existing anomaly detection paradigms overwhelmingly focus on training
detection models using exclusively normal data or unlabeled data (mostly normal
samples). One notorious issue with these approaches is that they are weak in
discriminating anomalies from normal samples due to the lack of the knowledge
about the anomalies. Here, we study the problem of few-shot anomaly detection,
in which we aim at using a few labeled anomaly examples to train
sample-efficient discriminative detection models. To address this problem, we
introduce a novel weakly-supervised anomaly detection framework to train
detection models without assuming the examples illustrating all possible
classes of anomaly.

Specifically, the proposed approach learns discriminative normality
(regularity) by leveraging the labeled anomalies and a prior probability to
enforce expressive representations of normality and unbounded deviated
representations of abnormality. This is achieved by an end-to-end optimization
of anomaly scores with a neural deviation learning, in which the anomaly scores
of normal samples are imposed to approximate scalar scores drawn from the prior
while that of anomaly examples is enforced to have statistically significant
deviations from these sampled scores in the upper tail. Furthermore, our model
is optimized to learn fine-grained normality and abnormality by top-K
multiple-instance-learning-based feature subspace deviation learning, allowing
more generalized representations. Comprehensive experiments on nine real-world
image anomaly detection benchmarks show that our model is substantially more
sample-efficient and robust, and performs significantly better than
state-of-the-art competing methods in both closed-set and open-set settings.
Our model can also offer explanation capability as a result of its prior-driven
anomaly score learning. Code and datasets are available at:
https://git.io/DevNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1"&gt;Guansong Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1"&gt;Choubo Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chunhua Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1"&gt;Anton van den Hengel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bucketed PCA Neural Networks with Neurons Mirroring Signals. (arXiv:2108.00605v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00605</id>
        <link href="http://arxiv.org/abs/2108.00605"/>
        <updated>2021-08-03T02:06:32.814Z</updated>
        <summary type="html"><![CDATA[The bucketed PCA neural network (PCA-NN) with transforms is developed here in
an effort to benchmark deep neural networks (DNN's), for problems on supervised
classification. Most classical PCA models apply PCA to the entire training data
set to establish a reductive representation and then employ non-network tools
such as high-order polynomial classifiers. In contrast, the bucketed PCA-NN
applies PCA to individual buckets which are constructed in two consecutive
phases, as well as retains a genuine architecture of a neural network. This
facilitates a fair apple-to-apple comparison to DNN's, esp. to reveal that a
major chunk of accuracy achieved by many impressive DNN's could possibly be
explained by the bucketed PCA-NN (e.g., 96% out of 98% for the MNIST data set
as an example). Compared with most DNN's, the three building blocks of the
bucketed PCA-NN are easier to comprehend conceptually - PCA, transforms, and
bucketing for error correction. Furthermore, unlike the somewhat quasi-random
neurons ubiquitously observed in DNN's, the PCA neurons resemble or mirror the
input signals and are more straightforward to decipher as a result.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jackie Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning. (arXiv:2108.00352v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.00352</id>
        <link href="http://arxiv.org/abs/2108.00352"/>
        <updated>2021-08-03T02:06:32.809Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning in computer vision aims to pre-train an image
encoder using a large amount of unlabeled images or (image, text) pairs. The
pre-trained image encoder can then be used as a feature extractor to build
downstream classifiers for many downstream tasks with a small amount of or no
labeled training data. In this work, we propose BadEncoder, the first backdoor
attack to self-supervised learning. In particular, our BadEncoder injects
backdoors into a pre-trained image encoder such that the downstream classifiers
built based on the backdoored image encoder for different downstream tasks
simultaneously inherit the backdoor behavior. We formulate our BadEncoder as an
optimization problem and we propose a gradient descent based method to solve
it, which produces a backdoored image encoder from a clean one. Our extensive
empirical evaluation results on multiple datasets show that our BadEncoder
achieves high attack success rates while preserving the accuracy of the
downstream classifiers. We also show the effectiveness of BadEncoder using two
publicly available, real-world image encoders, i.e., Google's image encoder
pre-trained on ImageNet and OpenAI's Contrastive Language-Image Pre-training
(CLIP) image encoder pre-trained on 400 million (image, text) pairs collected
from the Internet. Moreover, we consider defenses including Neural Cleanse and
MNTD (empirical defenses) as well as PatchGuard (a provable defense). Our
results show that these defenses are insufficient to defend against BadEncoder,
highlighting the needs for new defenses against our BadEncoder. Our code is
publicly available at: https://github.com/jjy1994/BadEncoder.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1"&gt;Jinyuan Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yupei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1"&gt;Neil Zhenqiang Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STEP: Spatial Temporal Graph Convolutional Networks for Emotion Perception from Gaits. (arXiv:1910.12906v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.12906</id>
        <link href="http://arxiv.org/abs/1910.12906"/>
        <updated>2021-08-03T02:06:32.791Z</updated>
        <summary type="html"><![CDATA[We present a novel classifier network called STEP, to classify perceived
human emotion from gaits, based on a Spatial Temporal Graph Convolutional
Network (ST-GCN) architecture. Given an RGB video of an individual walking, our
formulation implicitly exploits the gait features to classify the emotional
state of the human into one of four emotions: happy, sad, angry, or neutral. We
use hundreds of annotated real-world gait videos and augment them with
thousands of annotated synthetic gaits generated using a novel generative
network called STEP-Gen, built on an ST-GCN based Conditional Variational
Autoencoder (CVAE). We incorporate a novel push-pull regularization loss in the
CVAE formulation of STEP-Gen to generate realistic gaits and improve the
classification accuracy of STEP. We also release a novel dataset (E-Gait),
which consists of $2,177$ human gaits annotated with perceived emotions along
with thousands of synthetic gaits. In practice, STEP can learn the affective
features and exhibits classification accuracy of 89% on E-Gait, which is 14 -
30% more accurate over prior methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1"&gt;Uttaran Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_T/0/1/0/all/0/1"&gt;Trisha Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1"&gt;Rohan Chandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Randhavane_T/0/1/0/all/0/1"&gt;Tanmay Randhavane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1"&gt;Aniket Bera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1"&gt;Dinesh Manocha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Robust Object Detection: Bayesian RetinaNet for Homoscedastic Aleatoric Uncertainty Modeling. (arXiv:2108.00784v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00784</id>
        <link href="http://arxiv.org/abs/2108.00784"/>
        <updated>2021-08-03T02:06:32.784Z</updated>
        <summary type="html"><![CDATA[According to recent studies, commonly used computer vision datasets contain
about 4% of label errors. For example, the COCO dataset is known for its high
level of noise in data labels, which limits its use for training robust neural
deep architectures in a real-world scenario. To model such a noise, in this
paper we have proposed the homoscedastic aleatoric uncertainty estimation, and
present a series of novel loss functions to address the problem of image object
detection at scale. Specifically, the proposed functions are based on Bayesian
inference and we have incorporated them into the common community-adopted
object detection deep learning architecture RetinaNet. We have also shown that
modeling of homoscedastic aleatoric uncertainty using our novel functions
allows to increase the model interpretability and to improve the object
detection performance being evaluated on the COCO dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khanzhina_N/0/1/0/all/0/1"&gt;Natalia Khanzhina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapenok_A/0/1/0/all/0/1"&gt;Alexey Lapenok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filchenkov_A/0/1/0/all/0/1"&gt;Andrey Filchenkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Machine-Learning-Based Direction-of-Origin Filter for the Identification of Radio Frequency Interference in the Search for Technosignatures. (arXiv:2108.00559v1 [astro-ph.IM])]]></title>
        <id>http://arxiv.org/abs/2108.00559</id>
        <link href="http://arxiv.org/abs/2108.00559"/>
        <updated>2021-08-03T02:06:32.778Z</updated>
        <summary type="html"><![CDATA[Radio frequency interference (RFI) mitigation remains a major challenge in
the search for radio technosignatures. Typical mitigation strategies include a
direction-of-origin (DoO) filter, where a signal is classified as RFI if it is
detected in multiple directions on the sky. These classifications generally
rely on estimates of signal properties, such as frequency and frequency drift
rate. Convolutional neural networks (CNNs) offer a promising complement to
existing filters because they can be trained to analyze dynamic spectra
directly, instead of relying on inferred signal properties. In this work, we
compiled several data sets consisting of labeled pairs of images of dynamic
spectra, and we designed and trained a CNN that can determine whether or not a
signal detected in one scan is also present in another scan. This CNN-based DoO
filter outperforms both a baseline 2D correlation model as well as existing DoO
filters over a range of metrics, with precision and recall values of 99.15% and
97.81%, respectively. We found that the CNN reduces the number of signals
requiring visual inspection after the application of traditional DoO filters by
a factor of 6-16 in nominal situations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Pinchuk_P/0/1/0/all/0/1"&gt;Pavlo Pinchuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Margot_J/0/1/0/all/0/1"&gt;Jean-Luc Margot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer-based Map Matching with Model Limited Ground-Truth Data using Transfer-Learning Approach. (arXiv:2108.00439v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00439</id>
        <link href="http://arxiv.org/abs/2108.00439"/>
        <updated>2021-08-03T02:06:32.771Z</updated>
        <summary type="html"><![CDATA[In many trajectory-based applications, it is necessary to map raw GPS
trajectories onto road networks in digital maps, which is commonly referred to
as a map-matching process. While most previous map-matching methods have
focused on using rule-based algorithms to deal with the map-matching problems,
in this paper, we consider the map-matching task from the data perspective,
proposing a deep learning-based map-matching model. We build a
Transformer-based map-matching model with a transfer learning approach. We
generate synthetic trajectory data to pre-train the Transformer model and then
fine-tune the model with a limited number of ground-truth data to minimize the
model development cost and reduce the real-to-virtual gap. Three metrics
(Average Hamming Distance, F-score, and BLEU) at two levels (point and segment
level) are used to evaluate the model performance. The results indicate that
the proposed model outperforms existing models. Furthermore, we use the
attention weights of the Transformer to plot the map-matching process and find
how the model matches the road segments correctly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1"&gt;Zhixiong Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1"&gt;Seongjin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeo_H/0/1/0/all/0/1"&gt;Hwasoo Yeo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chest ImaGenome Dataset for Clinical Reasoning. (arXiv:2108.00316v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00316</id>
        <link href="http://arxiv.org/abs/2108.00316"/>
        <updated>2021-08-03T02:06:32.765Z</updated>
        <summary type="html"><![CDATA[Despite the progress in automatic detection of radiologic findings from chest
X-ray (CXR) images in recent years, a quantitative evaluation of the
explainability of these models is hampered by the lack of locally labeled
datasets for different findings. With the exception of a few expert-labeled
small-scale datasets for specific findings, such as pneumonia and pneumothorax,
most of the CXR deep learning models to date are trained on global "weak"
labels extracted from text reports, or trained via a joint image and
unstructured text learning strategy. Inspired by the Visual Genome effort in
the computer vision community, we constructed the first Chest ImaGenome dataset
with a scene graph data structure to describe $242,072$ images. Local
annotations are automatically produced using a joint rule-based natural
language processing (NLP) and atlas-based bounding box detection pipeline.
Through a radiologist constructed CXR ontology, the annotations for each CXR
are connected as an anatomy-centered scene graph, useful for image-level
reasoning and multimodal fusion applications. Overall, we provide: i) $1,256$
combinations of relation annotations between $29$ CXR anatomical locations
(objects with bounding box coordinates) and their attributes, structured as a
scene graph per image, ii) over $670,000$ localized comparison relations (for
improved, worsened, or no change) between the anatomical locations across
sequential exams, as well as ii) a manually annotated gold standard scene graph
dataset from $500$ unique patients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Joy T. Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agu_N/0/1/0/all/0/1"&gt;Nkechinyere N. Agu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lourentzou_I/0/1/0/all/0/1"&gt;Ismini Lourentzou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Arjun Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paguio_J/0/1/0/all/0/1"&gt;Joseph A. Paguio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1"&gt;Jasper S. Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dee_E/0/1/0/all/0/1"&gt;Edward C. Dee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitchell_W/0/1/0/all/0/1"&gt;William Mitchell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kashyap_S/0/1/0/all/0/1"&gt;Satyananda Kashyap&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giovannini_A/0/1/0/all/0/1"&gt;Andrea Giovannini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Celi_L/0/1/0/all/0/1"&gt;Leo A. Celi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1"&gt;Mehdi Moradi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepTrack: Lightweight Deep Learning for Vehicle Path Prediction in Highways. (arXiv:2108.00505v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00505</id>
        <link href="http://arxiv.org/abs/2108.00505"/>
        <updated>2021-08-03T02:06:32.757Z</updated>
        <summary type="html"><![CDATA[Vehicle trajectory prediction is an essential task for enabling many
intelligent transportation systems. While there have been some promising
advances in the field, there is a need for new agile algorithms with smaller
model sizes and lower computational requirements. This article presents
DeepTrack, a novel deep learning algorithm customized for real-time vehicle
trajectory prediction in highways. In contrast to previous methods, the vehicle
dynamics are encoded using Agile Temporal Convolutional Networks (ATCNs) to
provide more robust time prediction with less computation. ATCN also uses
depthwise convolution, which reduces the complexity of models compared to
existing approaches in terms of model size and operations. Overall, our
experimental results demonstrate that DeepTrack achieves comparable accuracy to
state-of-the-art trajectory prediction models but with smaller model sizes and
lower computational complexity, making it more suitable for real-world
deployment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baharani_M/0/1/0/all/0/1"&gt;Mohammadreza Baharani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katariya_V/0/1/0/all/0/1"&gt;Vinit Katariya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morris_N/0/1/0/all/0/1"&gt;Nichole Morris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shoghli_O/0/1/0/all/0/1"&gt;Omidreza Shoghli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tabkhi_H/0/1/0/all/0/1"&gt;Hamed Tabkhi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Angle Based Feature Learning in GNN for 3D Object Detection using Point Cloud. (arXiv:2108.00780v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00780</id>
        <link href="http://arxiv.org/abs/2108.00780"/>
        <updated>2021-08-03T02:06:32.734Z</updated>
        <summary type="html"><![CDATA[In this paper, we present new feature encoding methods for Detection of 3D
objects in point clouds. We used a graph neural network (GNN) for Detection of
3D objects namely cars, pedestrians, and cyclists. Feature encoding is one of
the important steps in Detection of 3D objects. The dataset used is point cloud
data which is irregular and unstructured and it needs to be encoded in such a
way that ensures better feature encapsulation. Earlier works have used relative
distance as one of the methods to encode the features. These methods are not
resistant to rotation variance problems in Graph Neural Networks. We have
included angular-based measures while performing feature encoding in graph
neural networks. Along with that, we have performed a comparison between other
methods like Absolute, Relative, Euclidean distances, and a combination of the
Angle and Relative methods. The model is trained and evaluated on the subset of
the KITTI object detection benchmark dataset under resource constraints. Our
results demonstrate that a combination of angle measures and relative distance
has performed better than other methods. In comparison to the baseline
method(relative), it achieved better performance. We also performed time
analysis of various feature encoding methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ansari_M/0/1/0/all/0/1"&gt;Md Afzal Ansari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meraz_M/0/1/0/all/0/1"&gt;Md Meraz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1"&gt;Pavan Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javed_M/0/1/0/all/0/1"&gt;Mohammed Javed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CARLA: A Python Library to Benchmark Algorithmic Recourse and Counterfactual Explanation Algorithms. (arXiv:2108.00783v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00783</id>
        <link href="http://arxiv.org/abs/2108.00783"/>
        <updated>2021-08-03T02:06:32.727Z</updated>
        <summary type="html"><![CDATA[Counterfactual explanations provide means for prescriptive model explanations
by suggesting actionable feature changes (e.g., increase income) that allow
individuals to achieve favorable outcomes in the future (e.g., insurance
approval). Choosing an appropriate method is a crucial aspect for meaningful
counterfactual explanations. As documented in recent reviews, there exists a
quickly growing literature with available methods. Yet, in the absence of
widely available opensource implementations, the decision in favor of certain
models is primarily based on what is readily available. Going forward - to
guarantee meaningful comparisons across explanation methods - we present CARLA
(Counterfactual And Recourse LibrAry), a python library for benchmarking
counterfactual explanation methods across both different data sets and
different machine learning models. In summary, our work provides the following
contributions: (i) an extensive benchmark of 11 popular counterfactual
explanation methods, (ii) a benchmarking framework for research on future
counterfactual explanation methods, and (iii) a standardized set of integrated
evaluation measures and data sets for transparent and extensive comparisons of
these methods. We have open-sourced CARLA and our experimental results on
Github, making them available as competitive baselines. We welcome
contributions from other research groups and practitioners.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pawelczyk_M/0/1/0/all/0/1"&gt;Martin Pawelczyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bielawski_S/0/1/0/all/0/1"&gt;Sascha Bielawski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heuvel_J/0/1/0/all/0/1"&gt;Johannes van den Heuvel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richter_T/0/1/0/all/0/1"&gt;Tobias Richter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasneci_G/0/1/0/all/0/1"&gt;Gjergji Kasneci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Control Direct Current Motor for Steering in Real Time via Reinforcement Learning. (arXiv:2108.00138v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00138</id>
        <link href="http://arxiv.org/abs/2108.00138"/>
        <updated>2021-08-03T02:06:32.721Z</updated>
        <summary type="html"><![CDATA[Model free techniques have been successful at optimal control of complex
systems at an expense of copious amounts of data and computation. However, it
is often desired to obtain a control policy in a short period of time with
minimal data use and computational burden. To this end, we make use of the NFQ
algorithm for steering position control of a golf cart in both a real hardware
and a simulated environment that was built from real-world interaction. The
controller learns to apply a sequence of voltage signals in the presence of
environmental uncertainties and inherent non-linearities that challenge the the
control task. We were able to increase the rate of successful control under
four minutes in simulation and under 11 minutes in real hardware.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Watson_T/0/1/0/all/0/1"&gt;Thomas Watson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poudel_B/0/1/0/all/0/1"&gt;Bibek Poudel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed Learning for Time-varying Networks: A Scalable Design. (arXiv:2108.00231v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2108.00231</id>
        <link href="http://arxiv.org/abs/2108.00231"/>
        <updated>2021-08-03T02:06:32.711Z</updated>
        <summary type="html"><![CDATA[The wireless network is undergoing a trend from "onnection of things" to
"connection of intelligence". With data spread over the communication networks
and computing capability enhanced on the devices, distributed learning becomes
a hot topic in both industrial and academic communities. Many frameworks, such
as federated learning and federated distillation, have been proposed. However,
few of them takes good care of obstacles such as the time-varying topology
resulted by the characteristics of wireless networks. In this paper, we propose
a distributed learning framework based on a scalable deep neural network (DNN)
design. By exploiting the permutation equivalence and invariance properties of
the learning tasks, the DNNs with different scales for different clients can be
built up based on two basic parameter sub-matrices. Further, model aggregation
can also be conducted based on these two sub-matrices to improve the learning
convergence and performance. Finally, simulation results verify the benefits of
the proposed framework by compared with some baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huangfu_Y/0/1/0/all/0/1"&gt;Yourui Huangfu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Rong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1"&gt;Yiqun Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably Efficient Lottery Ticket Discovery. (arXiv:2108.00259v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.00259</id>
        <link href="http://arxiv.org/abs/2108.00259"/>
        <updated>2021-08-03T02:06:32.704Z</updated>
        <summary type="html"><![CDATA[The lottery ticket hypothesis (LTH) claims that randomly-initialized, dense
neural networks contain (sparse) subnetworks that, when trained an equal amount
in isolation, can match the dense network's performance. Although LTH is useful
for discovering efficient network architectures, its three-step process --
pre-training, pruning, and re-training -- is computationally expensive, as the
dense model must be fully pre-trained. Luckily, "early-bird" tickets can be
discovered within neural networks that are minimally pre-trained, allowing for
the creation of efficient, LTH-inspired training procedures. Yet, no
theoretical foundation of this phenomenon exists. We derive an analytical bound
for the number of pre-training iterations that must be performed for a winning
ticket to be discovered, thus providing a theoretical understanding of when and
why such early-bird tickets exist. By adopting a greedy forward selection
pruning strategy, we directly connect the pruned network's performance to the
loss of the dense network from which it was derived, revealing a threshold in
the number of pre-training iterations beyond which high-performing subnetworks
are guaranteed to exist. We demonstrate the validity of our theoretical results
across a variety of architectures and datasets, including multi-layer
perceptrons (MLPs) trained on MNIST and several deep convolutional neural
network (CNN) architectures trained on CIFAR10 and ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wolfe_C/0/1/0/all/0/1"&gt;Cameron R. Wolfe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qihan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kim_J/0/1/0/all/0/1"&gt;Junhyung Lyle Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kyrillidis_A/0/1/0/all/0/1"&gt;Anastasios Kyrillidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive t-Momentum-based Optimization for Unknown Ratio of Outliers in Amateur Data in Imitation Learning. (arXiv:2108.00625v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00625</id>
        <link href="http://arxiv.org/abs/2108.00625"/>
        <updated>2021-08-03T02:06:32.682Z</updated>
        <summary type="html"><![CDATA[Behavioral cloning (BC) bears a high potential for safe and direct transfer
of human skills to robots. However, demonstrations performed by human operators
often contain noise or imperfect behaviors that can affect the efficiency of
the imitator if left unchecked. In order to allow the imitators to effectively
learn from imperfect demonstrations, we propose to employ the robust t-momentum
optimization algorithm. This algorithm builds on the Student's t-distribution
in order to deal with heavy-tailed data and reduce the effect of outlying
observations. We extend the t-momentum algorithm to allow for an adaptive and
automatic robustness and show empirically how the algorithm can be used to
produce robust BC imitators against datasets with unknown heaviness. Indeed,
the imitators trained with the t-momentum-based Adam optimizers displayed
robustness to imperfect demonstrations on two different manipulation tasks with
different robots and revealed the capability to take advantage of the
additional data while reducing the adverse effect of non-optimal behaviors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ilboudo_W/0/1/0/all/0/1"&gt;Wendyam Eric Lionel Ilboudo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kobayashi_T/0/1/0/all/0/1"&gt;Taisuke Kobayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugimoto_K/0/1/0/all/0/1"&gt;Kenji Sugimoto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Faster Rates of Differentially Private Stochastic Convex Optimization. (arXiv:2108.00331v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00331</id>
        <link href="http://arxiv.org/abs/2108.00331"/>
        <updated>2021-08-03T02:06:32.675Z</updated>
        <summary type="html"><![CDATA[In this paper, we revisit the problem of Differentially Private Stochastic
Convex Optimization (DP-SCO) and provide excess population risks for some
special classes of functions that are faster than the previous results of
general convex and strongly convex functions. In the first part of the paper,
we study the case where the population risk function satisfies the Tysbakov
Noise Condition (TNC) with some parameter $\theta>1$. Specifically, we first
show that under some mild assumptions on the loss functions, there is an
algorithm whose output could achieve an upper bound of
$\tilde{O}((\frac{1}{\sqrt{n}}+\frac{\sqrt{d\log
\frac{1}{\delta}}}{n\epsilon})^\frac{\theta}{\theta-1})$ for $(\epsilon,
\delta)$-DP when $\theta\geq 2$, here $n$ is the sample size and $d$ is the
dimension of the space. Then we address the inefficiency issue, improve the
upper bounds by $\text{Poly}(\log n)$ factors and extend to the case where
$\theta\geq \bar{\theta}>1$ for some known $\bar{\theta}$. Next we show that
the excess population risk of population functions satisfying TNC with
parameter $\theta>1$ is always lower bounded by
$\Omega((\frac{d}{n\epsilon})^\frac{\theta}{\theta-1}) $ and
$\Omega((\frac{\sqrt{d\log
\frac{1}{\delta}}}{n\epsilon})^\frac{\theta}{\theta-1})$ for $\epsilon$-DP and
$(\epsilon, \delta)$-DP, respectively. In the second part, we focus on a
special case where the population risk function is strongly convex. Unlike the
previous studies, here we assume the loss function is {\em non-negative} and
{\em the optimal value of population risk is sufficiently small}. With these
additional assumptions, we propose a new method whose output could achieve an
upper bound of
$O(\frac{d\log\frac{1}{\delta}}{n^2\epsilon^2}+\frac{1}{n^{\tau}})$ for any
$\tau\geq 1$ in $(\epsilon,\delta)$-DP model if the sample size $n$ is
sufficiently large.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1"&gt;Jinyan Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Di Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One Million Scenes for Autonomous Driving: ONCE Dataset. (arXiv:2106.11037v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11037</id>
        <link href="http://arxiv.org/abs/2106.11037"/>
        <updated>2021-08-03T02:06:32.661Z</updated>
        <summary type="html"><![CDATA[Current perception models in autonomous driving have become notorious for
greatly relying on a mass of annotated data to cover unseen cases and address
the long-tail problem. On the other hand, learning from unlabeled large-scale
collected data and incrementally self-training powerful recognition models have
received increasing attention and may become the solutions of next-generation
industry-level powerful and robust perception models in autonomous driving.
However, the research community generally suffered from data inadequacy of
those essential real-world scene data, which hampers the future exploration of
fully/semi/self-supervised methods for 3D perception. In this paper, we
introduce the ONCE (One millioN sCenEs) dataset for 3D object detection in the
autonomous driving scenario. The ONCE dataset consists of 1 million LiDAR
scenes and 7 million corresponding camera images. The data is selected from 144
driving hours, which is 20x longer than the largest 3D autonomous driving
dataset available (e.g. nuScenes and Waymo), and it is collected across a range
of different areas, periods and weather conditions. To facilitate future
research on exploiting unlabeled data for 3D detection, we additionally provide
a benchmark in which we reproduce and evaluate a variety of self-supervised and
semi-supervised methods on the ONCE dataset. We conduct extensive analyses on
those methods and provide valuable observations on their performance related to
the scale of used data. Data, code, and more information are available at
https://once-for-auto-driving.github.io/index.html.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1"&gt;Jiageng Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_M/0/1/0/all/0/1"&gt;Minzhe Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1"&gt;Chenhan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1"&gt;Hanxue Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jingheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yamin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1"&gt;Chaoqiang Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jie Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chunjing Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object-aware Contrastive Learning for Debiased Scene Representation. (arXiv:2108.00049v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00049</id>
        <link href="http://arxiv.org/abs/2108.00049"/>
        <updated>2021-08-03T02:06:32.654Z</updated>
        <summary type="html"><![CDATA[Contrastive self-supervised learning has shown impressive results in learning
visual representations from unlabeled images by enforcing invariance against
different data augmentations. However, the learned representations are often
contextually biased to the spurious scene correlations of different objects or
object and background, which may harm their generalization on the downstream
tasks. To tackle the issue, we develop a novel object-aware contrastive
learning framework that first (a) localizes objects in a self-supervised manner
and then (b) debias scene correlations via appropriate data augmentations
considering the inferred object locations. For (a), we propose the contrastive
class activation map (ContraCAM), which finds the most discriminative regions
(e.g., objects) in the image compared to the other images using the
contrastively trained models. We further improve the ContraCAM to detect
multiple objects and entire shapes via an iterative refinement procedure. For
(b), we introduce two data augmentations based on ContraCAM, object-aware
random crop and background mixup, which reduce contextual and background biases
during contrastive self-supervised learning, respectively. Our experiments
demonstrate the effectiveness of our representation learning framework,
particularly when trained under multi-object images or evaluated under the
background (and distribution) shifted images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1"&gt;Sangwoo Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1"&gt;Hyunwoo Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1"&gt;Kihyuk Sohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chun-Liang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1"&gt;Jinwoo Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UAV Trajectory Planning in Wireless Sensor Networks for Energy Consumption Minimization by Deep Reinforcement Learning. (arXiv:2108.00354v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2108.00354</id>
        <link href="http://arxiv.org/abs/2108.00354"/>
        <updated>2021-08-03T02:06:32.646Z</updated>
        <summary type="html"><![CDATA[Unmanned aerial vehicles (UAVs) have emerged as a promising candidate
solution for data collection of large-scale wireless sensor networks (WSNs). In
this paper, we investigate a UAV-aided WSN, where cluster heads (CHs) receive
data from their member nodes, and a UAV is dispatched to collect data from CHs
along the planned trajectory. We aim to minimize the total energy consumption
of the UAV-WSN system in a complete round of data collection. Toward this end,
we formulate the energy consumption minimization problem as a constrained
combinatorial optimization problem by jointly selecting CHs from nodes within
clusters and planning the UAV's visiting order to the selected CHs. The
formulated energy consumption minimization problem is NP-hard, and hence, hard
to solve optimally. In order to tackle this challenge, we propose a novel deep
reinforcement learning (DRL) technique, pointer network-A* (Ptr-A*), which can
efficiently learn from experiences the UAV trajectory policy for minimizing the
energy consumption. The UAV's start point and the WSN with a set of
pre-determined clusters are fed into the Ptr-A*, and the Ptr-A* outputs a group
of CHs and the visiting order to these CHs, i.e., the UAV's trajectory. The
parameters of the Ptr-A* are trained on small-scale clusters problem instances
for faster training by using the actor-critic algorithm in an unsupervised
manner. At inference, three search strategies are also proposed to improve the
quality of solutions. Simulation results show that the trained models based on
20-clusters and 40-clusters have a good generalization ability to solve the
UAV's trajectory planning problem in WSNs with different numbers of clusters,
without the need to retrain the models. Furthermore, the results show that our
proposed DRL algorithm outperforms two baseline techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_B/0/1/0/all/0/1"&gt;Botao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bedeer_E/0/1/0/all/0/1"&gt;Ebrahim Bedeer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Ha H. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Barton_R/0/1/0/all/0/1"&gt;Robert Barton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Henry_J/0/1/0/all/0/1"&gt;Jerome Henry&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Take an Emotion Walk: Perceiving Emotions from Gaits Using Hierarchical Attention Pooling and Affective Mapping. (arXiv:1911.08708v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.08708</id>
        <link href="http://arxiv.org/abs/1911.08708"/>
        <updated>2021-08-03T02:06:32.638Z</updated>
        <summary type="html"><![CDATA[We present an autoencoder-based semi-supervised approach to classify
perceived human emotions from walking styles obtained from videos or
motion-captured data and represented as sequences of 3D poses. Given the motion
on each joint in the pose at each time step extracted from 3D pose sequences,
we hierarchically pool these joint motions in a bottom-up manner in the
encoder, following the kinematic chains in the human body. We also constrain
the latent embeddings of the encoder to contain the space of
psychologically-motivated affective features underlying the gaits. We train the
decoder to reconstruct the motions per joint per time step in a top-down manner
from the latent embeddings. For the annotated data, we also train a classifier
to map the latent embeddings to emotion labels. Our semi-supervised approach
achieves a mean average precision of 0.84 on the Emotion-Gait benchmark
dataset, which contains both labeled and unlabeled gaits collected from
multiple sources. We outperform current state-of-art algorithms for both
emotion recognition and action recognition from 3D gaits by 7%--23% on the
absolute. More importantly, we improve the average precision by 10%--50% on the
absolute on classes that each makes up less than 25% of the labeled part of the
Emotion-Gait benchmark dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1"&gt;Uttaran Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roncal_C/0/1/0/all/0/1"&gt;Christian Roncal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_T/0/1/0/all/0/1"&gt;Trisha Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1"&gt;Rohan Chandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kapsaskis_K/0/1/0/all/0/1"&gt;Kyra Kapsaskis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gray_K/0/1/0/all/0/1"&gt;Kurt Gray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1"&gt;Aniket Bera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1"&gt;Dinesh Manocha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Pest Detection with DNN on the Edge for Precision Agriculture. (arXiv:2108.00421v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00421</id>
        <link href="http://arxiv.org/abs/2108.00421"/>
        <updated>2021-08-03T02:06:32.612Z</updated>
        <summary type="html"><![CDATA[Artificial intelligence has smoothly penetrated several economic activities,
especially monitoring and control applications, including the agriculture
sector. However, research efforts toward low-power sensing devices with fully
functional machine learning (ML) on-board are still fragmented and limited in
smart farming. Biotic stress is one of the primary causes of crop yield
reduction. With the development of deep learning in computer vision technology,
autonomous detection of pest infestation through images has become an important
research direction for timely crop disease diagnosis. This paper presents an
embedded system enhanced with ML functionalities, ensuring continuous detection
of pest infestation inside fruit orchards. The embedded solution is based on a
low-power embedded sensing system along with a Neural Accelerator able to
capture and process images inside common pheromone-based traps. Three different
ML algorithms have been trained and deployed, highlighting the capabilities of
the platform. Moreover, the proposed approach guarantees an extended battery
life thanks to the integration of energy harvesting functionalities. Results
show how it is possible to automate the task of pest infestation for unlimited
time without the farmer's intervention.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Albanese_A/0/1/0/all/0/1"&gt;Andrea Albanese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nardello_M/0/1/0/all/0/1"&gt;Matteo Nardello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brunelli_D/0/1/0/all/0/1"&gt;Davide Brunelli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReCU: Reviving the Dead Weights in Binary Neural Networks. (arXiv:2103.12369v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12369</id>
        <link href="http://arxiv.org/abs/2103.12369"/>
        <updated>2021-08-03T02:06:32.606Z</updated>
        <summary type="html"><![CDATA[Binary neural networks (BNNs) have received increasing attention due to their
superior reductions of computation and memory. Most existing works focus on
either lessening the quantization error by minimizing the gap between the
full-precision weights and their binarization or designing a gradient
approximation to mitigate the gradient mismatch, while leaving the "dead
weights" untouched. This leads to slow convergence when training BNNs. In this
paper, for the first time, we explore the influence of "dead weights" which
refer to a group of weights that are barely updated during the training of
BNNs, and then introduce rectified clamp unit (ReCU) to revive the "dead
weights" for updating. We prove that reviving the "dead weights" by ReCU can
result in a smaller quantization error. Besides, we also take into account the
information entropy of the weights, and then mathematically analyze why the
weight standardization can benefit BNNs. We demonstrate the inherent
contradiction between minimizing the quantization error and maximizing the
information entropy, and then propose an adaptive exponential scheduler to
identify the range of the "dead weights". By considering the "dead weights",
our method offers not only faster BNN training, but also state-of-the-art
performance on CIFAR-10 and ImageNet, compared with recent methods. Code can be
available at https://github.com/z-hXu/ReCU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zihan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Mingbao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jianzhuang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yue Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Reinforcement Learning Approach for Scheduling in mmWave Networks. (arXiv:2108.00548v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2108.00548</id>
        <link href="http://arxiv.org/abs/2108.00548"/>
        <updated>2021-08-03T02:06:32.596Z</updated>
        <summary type="html"><![CDATA[We consider a source that wishes to communicate with a destination at a
desired rate, over a mmWave network where links are subject to blockage and
nodes to failure (e.g., in a hostile military environment). To achieve
resilience to link and node failures, we here explore a state-of-the-art Soft
Actor-Critic (SAC) deep reinforcement learning algorithm, that adapts the
information flow through the network, without using knowledge of the link
capacities or network topology. Numerical evaluations show that our algorithm
can achieve the desired rate even in dynamic environments and it is robust
against blockage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dogan_M/0/1/0/all/0/1"&gt;Mine Gokce Dogan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ezzeldin_Y/0/1/0/all/0/1"&gt;Yahya H. Ezzeldin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fragouli_C/0/1/0/all/0/1"&gt;Christina Fragouli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bohannon_A/0/1/0/all/0/1"&gt;Addison W. Bohannon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using deep learning to detect patients at risk for prostate cancer despite benign biopsies. (arXiv:2106.14256v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14256</id>
        <link href="http://arxiv.org/abs/2106.14256"/>
        <updated>2021-08-03T02:06:32.587Z</updated>
        <summary type="html"><![CDATA[Background: Transrectal ultrasound guided systematic biopsies of the prostate
is a routine procedure to establish a prostate cancer diagnosis. However, the
10-12 prostate core biopsies only sample a relatively small volume of the
prostate, and tumour lesions in regions between biopsy cores can be missed,
leading to a well-known low sensitivity to detect clinically relevant cancer.
As a proof-of-principle, we developed and validated a deep convolutional neural
network model to distinguish between morphological patterns in benign prostate
biopsy whole slide images from men with and without established cancer.
Methods: This study included 14,354 hematoxylin and eosin stained whole slide
images from benign prostate biopsies from 1,508 men in two groups: men without
an established prostate cancer (PCa) diagnosis and men with at least one core
biopsy diagnosed with PCa. 80% of the participants were assigned as training
data and used for model optimization (1,211 men), and the remaining 20% (297
men) as a held-out test set used to evaluate model performance. An ensemble of
10 deep convolutional neural network models was optimized for classification of
biopsies from men with and without established cancer. Hyperparameter
optimization and model selection was performed by cross-validation in the
training data . Results: Area under the receiver operating characteristic curve
(ROC-AUC) was estimated as 0.727 (bootstrap 95% CI: 0.708-0.745) on biopsy
level and 0.738 (bootstrap 95% CI: 0.682 - 0.796) on man level. At a
specificity of 0.9 the model had an estimated sensitivity of 0.348. Conclusion:
The developed model has the ability to detect men with risk of missed PCa due
to under-sampling of the prostate. The proposed model has the potential to
reduce the number of false negative cases in routine systematic prostate
biopsies and to indicate men who could benefit from MRI-guided re-biopsy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_B/0/1/0/all/0/1"&gt;Boing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yinxi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Weitz_P/0/1/0/all/0/1"&gt;Philippe Weitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lindberg_J/0/1/0/all/0/1"&gt;Johan Lindberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hartman_J/0/1/0/all/0/1"&gt;Johan Hartman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Egevad_L/0/1/0/all/0/1"&gt;Lars Egevad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gronberg_H/0/1/0/all/0/1"&gt;Henrik Gr&amp;#xf6;nberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Eklund_M/0/1/0/all/0/1"&gt;Martin Eklund&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rantalainen_M/0/1/0/all/0/1"&gt;Mattias Rantalainen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transparent Object Tracking Benchmark. (arXiv:2011.10875v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10875</id>
        <link href="http://arxiv.org/abs/2011.10875"/>
        <updated>2021-08-03T02:06:32.568Z</updated>
        <summary type="html"><![CDATA[Visual tracking has achieved considerable progress in recent years. However,
current research in the field mainly focuses on tracking of opaque objects,
while little attention is paid to transparent object tracking. In this paper,
we make the first attempt in exploring this problem by proposing a Transparent
Object Tracking Benchmark (TOTB). Specifically, TOTB consists of 225 videos
(86K frames) from 15 diverse transparent object categories. Each sequence is
manually labeled with axis-aligned bounding boxes. To the best of our
knowledge, TOTB is the first benchmark dedicated to transparent object
tracking. In order to understand how existing trackers perform and to provide
comparison for future research on TOTB, we extensively evaluate 25
state-of-the-art tracking algorithms. The evaluation results exhibit that more
efforts are needed to improve transparent object tracking. Besides, we observe
some nontrivial findings from the evaluation that are discrepant with some
common beliefs in opaque object tracking. For example, we find that deeper
features are not always good for improvements. Moreover, to encourage future
research, we introduce a novel tracker, named TransATOM, which leverages
transparency features for tracking and surpasses all 25 evaluated approaches by
a large margin. By releasing TOTB, we expect to facilitate future research and
application of transparent object tracking in both the academia and industry.
The TOTB and evaluation results as well as TransATOM are available at
https://hengfan2010.github.io/projects/TOTB.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1"&gt;Heng Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miththanthaya_H/0/1/0/all/0/1"&gt;Halady Akhilesha Miththanthaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harshit/0/1/0/all/0/1"&gt;Harshit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajan_S/0/1/0/all/0/1"&gt;Siranjiv Ramana Rajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaoqiong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1"&gt;Zhilin Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yuewei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1"&gt;Haibin Ling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CSC-Unet: A Novel Convolutional Sparse Coding Strategy based Neural Network for Semantic Segmentation. (arXiv:2108.00408v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00408</id>
        <link href="http://arxiv.org/abs/2108.00408"/>
        <updated>2021-08-03T02:06:32.562Z</updated>
        <summary type="html"><![CDATA[It is a challenging task to accurately perform semantic segmentation due to
the complexity of real picture scenes. Many semantic segmentation methods based
on traditional deep learning insufficiently captured the semantic and
appearance information of images, which put limit on their generality and
robustness for various application scenes. In this paper, we proposed a novel
strategy that reformulated the popularly-used convolution operation to
multi-layer convolutional sparse coding block to ease the aforementioned
deficiency. This strategy can be possibly used to significantly improve the
segmentation performance of any semantic segmentation model that involves
convolutional operations. To prove the effectiveness of our idea, we chose the
widely-used U-Net model for the demonstration purpose, and we designed CSC-Unet
model series based on U-Net. Through extensive analysis and experiments, we
provided credible evidence showing that the multi-layer convolutional sparse
coding block enables semantic segmentation model to converge faster, can
extract finer semantic and appearance information of images, and improve the
ability to recover spatial detail information. The best CSC-Unet model
significantly outperforms the results of the original U-Net on three public
datasets with different scenarios, i.e., 87.14% vs. 84.71% on DeepCrack
dataset, 68.91% vs. 67.09% on Nuclei dataset, and 53.68% vs. 48.82% on CamVid
dataset, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Haitong Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shuang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xia Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1"&gt;Qin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kaiyue Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1"&gt;Hongjie Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1"&gt;Nizhuan Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectroscopic Approach to Correction and Visualisation of Bright-Field Light Transmission Microscopy Biological Data. (arXiv:1903.06519v4 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1903.06519</id>
        <link href="http://arxiv.org/abs/1903.06519"/>
        <updated>2021-08-03T02:06:32.554Z</updated>
        <summary type="html"><![CDATA[The most realistic information about the transparent sample such as a live
cell can be obtained only using bright-field light microscopy. At
high-intensity pulsing LED illumination, we captured a primary
12-bit-per-channel (bpc) response from an observed sample using a bright-field
microscope equipped with a high-resolution (4872x3248) image sensor. In order
to suppress data distortions originating from the light interactions with
elements in the optical path, poor sensor reproduction (geometrical defects of
the camera sensor and some peculiarities of sensor sensitivity), we propose a
spectroscopic approach for the correction of this uncompressed 12-bpc data by
simultaneous calibration of all parts of the experimental arrangement.
Moreover, the final intensities of the corrected images are proportional to the
photon fluxes detected by a camera sensor. It can be visualized in 8-bpc
intensity depth after the Least Information Loss compression [Lect. Notes
Bioinform. 9656, 527 (2016)].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Platonova_G/0/1/0/all/0/1"&gt;Ganna Platonova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Stys_D/0/1/0/all/0/1"&gt;Dalibor Stys&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Soucek_P/0/1/0/all/0/1"&gt;Pavel Soucek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lonhus_K/0/1/0/all/0/1"&gt;Kirill Lonhus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Valenta_J/0/1/0/all/0/1"&gt;Jan Valenta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rychtarikova_R/0/1/0/all/0/1"&gt;Renata Rychtarikova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Group Fisher Pruning for Practical Network Compression. (arXiv:2108.00708v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00708</id>
        <link href="http://arxiv.org/abs/2108.00708"/>
        <updated>2021-08-03T02:06:32.548Z</updated>
        <summary type="html"><![CDATA[Network compression has been widely studied since it is able to reduce the
memory and computation cost during inference. However, previous methods seldom
deal with complicated structures like residual connections, group/depth-wise
convolution and feature pyramid network, where channels of multiple layers are
coupled and need to be pruned simultaneously. In this paper, we present a
general channel pruning approach that can be applied to various complicated
structures. Particularly, we propose a layer grouping algorithm to find coupled
channels automatically. Then we derive a unified metric based on Fisher
information to evaluate the importance of a single channel and coupled
channels. Moreover, we find that inference speedup on GPUs is more correlated
with the reduction of memory rather than FLOPs, and thus we employ the memory
reduction of each channel to normalize the importance. Our method can be used
to prune any structures including those with coupled channels. We conduct
extensive experiments on various backbones, including the classic ResNet and
ResNeXt, mobile-friendly MobileNetV2, and the NAS-based RegNet, both on image
classification and object detection which is under-explored. Experimental
results validate that our method can effectively prune sophisticated networks,
boosting inference speed without sacrificing accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Liyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shilong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1"&gt;Zhanghui Kuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1"&gt;Aojun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1"&gt;Jing-Hao Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinjiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yimin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wenming Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1"&gt;Qingmin Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wayne Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation. (arXiv:2103.03102v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03102</id>
        <link href="http://arxiv.org/abs/2103.03102"/>
        <updated>2021-08-03T02:06:32.535Z</updated>
        <summary type="html"><![CDATA[The accuracy of DL classifiers is unstable in that it often changes
significantly when retested on adversarial images, imperfect images, or
perturbed images. This paper adds to the small but fundamental body of work on
benchmarking the robustness of DL classifiers on defective images. Unlike
existed single-factor digital perturbation work, we provide state-of-the-art
two-factor perturbation that provides two natural perturbations on images
applied in different sequences. The two-factor perturbation includes (1) two
digital perturbations (Salt & pepper noise and Gaussian noise) applied in both
sequences. (2) one digital perturbation (salt & pepper noise) and a geometric
perturbation (rotation) applied in different sequences. To measure robust DL
classifiers, previous scientists provided 15 types of single-factor corruption.
We created 69 benchmarking image sets, including a clean set, sets with single
factor perturbations, and sets with two-factor perturbation conditions. To be
best of our knowledge, this is the first report that two-factor perturbed
images improves both robustness and accuracy of DL classifiers. Previous
research evaluating deep learning (DL) classifiers has often used top-1/top-5
accuracy, so researchers have usually offered tables, line diagrams, and bar
charts to display accuracy of DL classifiers. But these existed approaches
cannot quantitively evaluate robustness of DL classifiers. We innovate a new
two-dimensional, statistical visualization tool, including mean accuracy and
coefficient of variation (CV), to benchmark the robustness of DL classifiers.
All source codes and related image sets are shared on websites
(this http URL or
https://github.com/daiweiworking/RobustDeepLearningUsingPerturbations ) to
support future academic research and industry projects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1"&gt;Wei Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berleant_D/0/1/0/all/0/1"&gt;Daniel Berleant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bespoke Fractal Sampling Patterns for Discrete Fourier Space via the Kaleidoscope Transform. (arXiv:2108.00639v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.00639</id>
        <link href="http://arxiv.org/abs/2108.00639"/>
        <updated>2021-08-03T02:06:32.529Z</updated>
        <summary type="html"><![CDATA[Sampling strategies are important for sparse imaging methodologies,
especially those employing the discrete Fourier transform (DFT). Chaotic
sensing is one such methodology that employs deterministic, fractal sampling in
conjunction with finite, iterative reconstruction schemes to form an image from
limited samples. Using a sampling pattern constructed entirely from periodic
lines in DFT space, chaotic sensing was found to outperform traditional
compressed sensing for magnetic resonance imaging; however, only one such
sampling pattern was presented and the reason for its fractal nature was not
proven. Through the introduction of a novel image transform known as the
kaleidoscope transform, which formalises and extends upon the concept of
downsampling and concatenating an image with itself, this paper: (1)
demonstrates a fundamental relationship between multiplication in modular
arithmetic and downsampling; (2) provides a rigorous mathematical explanation
for the fractal nature of the sampling pattern in the DFT; and (3) leverages
this understanding to develop a collection of novel fractal sampling patterns
for the 2D DFT with customisable properties. The ability to design tailor-made
fractal sampling patterns expands the utility of the DFT in chaotic imaging and
may form the basis for a bespoke chaotic sensing methodology, in which the
fractal sampling matches the imaging task for improved reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+White_J/0/1/0/all/0/1"&gt;Jacob M. White&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Crozier_S/0/1/0/all/0/1"&gt;Stuart Crozier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chandra_S/0/1/0/all/0/1"&gt;Shekhar S. Chandra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SPEAR : Semi-supervised Data Programming in Python. (arXiv:2108.00373v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00373</id>
        <link href="http://arxiv.org/abs/2108.00373"/>
        <updated>2021-08-03T02:06:32.510Z</updated>
        <summary type="html"><![CDATA[We present SPEAR, an open-source python library for data programming with
semi supervision. The package implements several recent data programming
approaches including facility to programmatically label and build training
data. SPEAR facilitates weak supervision in the form of heuristics (or rules)
and association of noisy labels to the training dataset. These noisy labels are
aggregated to assign labels to the unlabeled data for downstream tasks. We have
implemented several label aggregation approaches that aggregate the noisy
labels and then train using the noisily labeled set in a cascaded manner. Our
implementation also includes other approaches that jointly aggregate and train
the model. Thus, in our python package, we integrate several cascade and joint
data-programming approaches while also providing the facility of data
programming by letting the user define labeling functions or rules. The code
and tutorial notebooks are available at
\url{https://github.com/decile-team/spear}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abhishek_G/0/1/0/all/0/1"&gt;Guttu Sai Abhishek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ingole_H/0/1/0/all/0/1"&gt;Harshad Ingole&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laturia_P/0/1/0/all/0/1"&gt;Parth Laturia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dorna_V/0/1/0/all/0/1"&gt;Vineeth Dorna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maheshwari_A/0/1/0/all/0/1"&gt;Ayush Maheshwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1"&gt;Ganesh Ramakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1"&gt;Rishabh Iyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Machine-learning Based Initialization for Joint Statistical Iterative Dual-energy CT with Application to Proton Therapy. (arXiv:2108.00109v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.00109</id>
        <link href="http://arxiv.org/abs/2108.00109"/>
        <updated>2021-08-03T02:06:32.504Z</updated>
        <summary type="html"><![CDATA[Dual-energy CT (DECT) has been widely investigated to generate more
informative and more accurate images in the past decades. For example,
Dual-Energy Alternating Minimization (DEAM) algorithm achieves sub-percentage
uncertainty in estimating proton stopping-power mappings from experimental 3-mm
collimated phantom data. However, elapsed time of iterative DECT algorithms is
not clinically acceptable, due to their low convergence rate and the tremendous
geometry of modern helical CT scanners. A CNN-based initialization method is
introduced to reduce the computational time of iterative DECT algorithms. DEAM
is used as an example of iterative DECT algorithms in this work. The simulation
results show that our method generates denoised images with greatly improved
estimation accuracy for adipose, tonsils, and muscle tissue. Also, it reduces
elapsed time by approximately 5-fold for DEAM to reach the same objective
function value for both simulated and real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ge_T/0/1/0/all/0/1"&gt;Tao Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Medrano_M/0/1/0/all/0/1"&gt;Maria Medrano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liao_R/0/1/0/all/0/1"&gt;Rui Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Politte_D/0/1/0/all/0/1"&gt;David G. Politte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Williamson_J/0/1/0/all/0/1"&gt;Jeffrey F. Williamson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+OSullivan_J/0/1/0/all/0/1"&gt;Joseph A. O&amp;#x27;Sullivan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extracting Grammars from a Neural Network Parser for Anomaly Detection in Unknown Formats. (arXiv:2108.00103v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00103</id>
        <link href="http://arxiv.org/abs/2108.00103"/>
        <updated>2021-08-03T02:06:32.463Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning has recently shown promise as a technique for training
an artificial neural network to parse sentences in some unknown format. A key
aspect of this approach is that rather than explicitly inferring a grammar that
describes the format, the neural network learns to perform various parsing
actions (such as merging two tokens) over a corpus of sentences, with the goal
of maximizing the total reward, which is roughly based on the estimated
frequency of the resulting parse structures. This can allow the learning
process to more easily explore different action choices, since a given choice
may change the optimality of the parse (as expressed by the total reward), but
will not result in the failure to parse a sentence. However, the approach also
exhibits limitations: first, the neural network does not provide production
rules for the grammar that it uses during parsing; second, because this neural
network can successfully parse any sentence, it cannot be directly used to
identify sentences that deviate from the format of the training sentences,
i.e., that are anomalous. In this paper, we address these limitations by
presenting procedures for extracting production rules from the neural network,
and for using these rules to determine whether a given sentence is nominal or
anomalous, when compared to structures observed within training data. In the
latter case, an attempt is made to identify the location of the anomaly.
Additionally, a two pass mechanism is presented for dealing with formats
containing high-entropy information. We empirically evaluate the approach on
artificial formats, demonstrating effectiveness, but also identifying
limitations. By further improving parser learning, and leveraging rule
extraction and anomaly detection, one might begin to understand common errors,
either benign or malicious, in practical formats.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grushin_A/0/1/0/all/0/1"&gt;Alexander Grushin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woods_W/0/1/0/all/0/1"&gt;Walt Woods&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervising Learning, Transfer Learning, and Knowledge Distillation with SimCLR. (arXiv:2108.00587v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00587</id>
        <link href="http://arxiv.org/abs/2108.00587"/>
        <updated>2021-08-03T02:06:32.456Z</updated>
        <summary type="html"><![CDATA[Recent breakthroughs in the field of semi-supervised learning have achieved
results that match state-of-the-art traditional supervised learning methods.
Most successful semi-supervised learning approaches in computer vision focus on
leveraging huge amount of unlabeled data, learning the general representation
via data augmentation and transformation, creating pseudo labels, implementing
different loss functions, and eventually transferring this knowledge to more
task-specific smaller models. In this paper, we aim to conduct our analyses on
three different aspects of SimCLR, the current state-of-the-art semi-supervised
learning framework for computer vision. First, we analyze properties of
contrast learning on fine-tuning, as we understand that contrast learning is
what makes this method so successful. Second, we research knowledge
distillation through teacher-forcing paradigm. We observe that when the teacher
and the student share the same base model, knowledge distillation will achieve
better result. Finally, we study how transfer learning works and its
relationship with the number of classes on different data sets. Our results
indicate that transfer learning performs better when number of classes are
smaller.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Khoi Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_Y/0/1/0/all/0/1"&gt;Yen Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_B/0/1/0/all/0/1"&gt;Bao Le&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Cross-Modal Distillation for Thermal Infrared Tracking. (arXiv:2108.00187v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00187</id>
        <link href="http://arxiv.org/abs/2108.00187"/>
        <updated>2021-08-03T02:06:32.441Z</updated>
        <summary type="html"><![CDATA[The target representation learned by convolutional neural networks plays an
important role in Thermal Infrared (TIR) tracking. Currently, most of the
top-performing TIR trackers are still employing representations learned by the
model trained on the RGB data. However, this representation does not take into
account the information in the TIR modality itself, limiting the performance of
TIR tracking. To solve this problem, we propose to distill representations of
the TIR modality from the RGB modality with Cross-Modal Distillation (CMD) on a
large amount of unlabeled paired RGB-TIR data. We take advantage of the
two-branch architecture of the baseline tracker, i.e. DiMP, for cross-modal
distillation working on two components of the tracker. Specifically, we use one
branch as a teacher module to distill the representation learned by the model
into the other branch. Benefiting from the powerful model in the RGB modality,
the cross-modal distillation can learn the TIR-specific representation for
promoting TIR tracking. The proposed approach can be incorporated into
different baseline trackers conveniently as a generic and independent
component. Furthermore, the semantic coherence of paired RGB and TIR images is
utilized as a supervised signal in the distillation loss for cross-modal
knowledge transfer. In practice, three different approaches are explored to
generate paired RGB-TIR patches with the same semantics for training in an
unsupervised way. It is easy to extend to an even larger scale of unlabeled
training data. Extensive experiments on the LSOTB-TIR dataset and PTB-TIR
dataset demonstrate that our proposed cross-modal distillation method
effectively learns TIR-specific target representations transferred from the RGB
modality. Our tracker outperforms the baseline tracker by achieving absolute
gains of 2.3% Success, 2.7% Precision, and 2.5% Normalized Precision
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jingxian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lichao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Y/0/1/0/all/0/1"&gt;Yufei Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Garcia_A/0/1/0/all/0/1"&gt;Abel Gonzalez-Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Peng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yanning Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pro-UIGAN: Progressive Face Hallucination from Occluded Thumbnails. (arXiv:2108.00602v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00602</id>
        <link href="http://arxiv.org/abs/2108.00602"/>
        <updated>2021-08-03T02:06:32.419Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the task of hallucinating an authentic
high-resolution (HR) face from an occluded thumbnail. We propose a multi-stage
Progressive Upsampling and Inpainting Generative Adversarial Network, dubbed
Pro-UIGAN, which exploits facial geometry priors to replenish and upsample (8*)
the occluded and tiny faces (16*16 pixels). Pro-UIGAN iteratively (1) estimates
facial geometry priors for low-resolution (LR) faces and (2) acquires
non-occluded HR face images under the guidance of the estimated priors. Our
multi-stage hallucination network super-resolves and inpaints occluded LR faces
in a coarse-to-fine manner, thus reducing unwanted blurriness and artifacts
significantly. Specifically, we design a novel cross-modal transformer module
for facial priors estimation, in which an input face and its landmark features
are formulated as queries and keys, respectively. Such a design encourages
joint feature learning across the input facial and landmark features, and deep
feature correspondences will be discovered by attention. Thus, facial
appearance features and facial geometry priors are learned in a mutual
promotion manner. Extensive experiments demonstrate that our Pro-UIGAN achieves
visually pleasing HR faces, reaching superior performance in downstream tasks,
i.e., face alignment, face parsing, face recognition and expression
classification, compared with other state-of-the-art (SotA) methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaobo Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Ping Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scale-aware Neural Network for Semantic Segmentation of Multi-resolution Remotely Sensed Images. (arXiv:2103.07935v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07935</id>
        <link href="http://arxiv.org/abs/2103.07935"/>
        <updated>2021-08-03T02:06:32.393Z</updated>
        <summary type="html"><![CDATA[Assigning geospatial objects with specific categories at the pixel level is a
fundamental task in remote sensing image analysis. Along with rapid development
in sensor technologies, remotely sensed images can be captured at multiple
spatial resolutions (MSR) with information content manifested at different
scales. Extracting information from these MSR images represents huge
opportunities for enhanced feature representation and characterisation.
However, MSR images suffer from two critical issues: 1) increased scale
variation of geo-objects and 2) loss of detailed information at coarse spatial
resolutions. To bridge these gaps, in this paper, we propose a novel
scale-aware neural network (SaNet) for semantic segmentation of MSR remotely
sensed imagery. SaNet deploys a densely connected feature network (DCFPN)
module to capture high-quality multi-scale context, such that the scale
variation is handled properly and the quality of segmentation is increased for
both large and small objects. A spatial feature recalibration (SFR) module is
further incorporated into the network to learn intact semantic content with
enhanced spatial relationships, where the negative effects of information loss
are removed. The combination of DCFPN and SFR allows SaNet to learn scale-aware
feature representation, which outperforms the existing multi-scale feature
representation. Extensive experiments on three semantic segmentation datasets
demonstrated the effectiveness of the proposed SaNet in cross-resolution
segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Libo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1"&gt;Shenghui Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Ce Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Rui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1"&gt;Chenxi Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1"&gt;Xiaoliang Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atkinson_P/0/1/0/all/0/1"&gt;Peter M. Atkinson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Developing a Compressed Object Detection Model based on YOLOv4 for Deployment on Embedded GPU Platform of Autonomous System. (arXiv:2108.00392v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00392</id>
        <link href="http://arxiv.org/abs/2108.00392"/>
        <updated>2021-08-03T02:06:32.352Z</updated>
        <summary type="html"><![CDATA[Latest CNN-based object detection models are quite accurate but require a
high-performance GPU to run in real-time. They still are heavy in terms of
memory size and speed for an embedded system with limited memory space. Since
the object detection for autonomous system is run on an embedded processor, it
is preferable to compress the detection network as light as possible while
preserving the detection accuracy. There are several popular lightweight
detection models but their accuracy is too low for safe driving applications.
Therefore, this paper proposes a new object detection model, referred as
YOffleNet, which is compressed at a high ratio while minimizing the accuracy
loss for real-time and safe driving application on an autonomous system. The
backbone network architecture is based on YOLOv4, but we could compress the
network greatly by replacing the high-calculation-load CSP DenseNet with the
lighter modules of ShuffleNet. Experiments with KITTI dataset showed that the
proposed YOffleNet is compressed by 4.7 times than the YOLOv4-s that could
achieve as fast as 46 FPS on an embedded GPU system(NVIDIA Jetson AGX Xavier).
Compared to the high compression ratio, the accuracy is reduced slightly to
85.8% mAP, that is only 2.6% lower than YOLOv4-s. Thus, the proposed network
showed a high potential to be deployed on the embedded system of the autonomous
system for the real-time and accurate object detection applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sim_I/0/1/0/all/0/1"&gt;Issac Sim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1"&gt;Ju-Hyung Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1"&gt;Young-Wan Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1"&gt;JiHwan You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1"&gt;SeonTaek Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Young-Keun Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Decentralized Federated Learning Framework via Committee Mechanism with Convergence Guarantee. (arXiv:2108.00365v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00365</id>
        <link href="http://arxiv.org/abs/2108.00365"/>
        <updated>2021-08-03T02:06:32.228Z</updated>
        <summary type="html"><![CDATA[Federated learning allows multiple participants to collaboratively train an
efficient model without exposing data privacy. However, this distributed
machine learning training method is prone to attacks from Byzantine clients,
which interfere with the training of the global model by modifying the model or
uploading the false gradient. In this paper, we propose a novel serverless
federated learning framework Committee Mechanism based Federated Learning
(CMFL), which can ensure the robustness of the algorithm with convergence
guarantee. In CMFL, a committee system is set up to screen the uploaded local
gradients. The committee system selects the local gradients rated by the
elected members for the aggregation procedure through the selection strategy,
and replaces the committee member through the election strategy. Based on the
different considerations of model performance and defense, two opposite
selection strategies are designed for the sake of both accuracy and robustness.
Extensive experiments illustrate that CMFL achieves faster convergence and
better accuracy than the typical Federated Learning, in the meanwhile obtaining
better robustness than the traditional Byzantine-tolerant algorithms, in the
manner of a decentralized approach. In addition, we theoretically analyze and
prove the convergence of CMFL under different election and selection
strategies, which coincides with the experimental results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Che_C/0/1/0/all/0/1"&gt;Chunjiang Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoli Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiaoyu He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zibin Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SSPU-Net: Self-Supervised Point Cloud Upsampling via Differentiable Rendering. (arXiv:2108.00454v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00454</id>
        <link href="http://arxiv.org/abs/2108.00454"/>
        <updated>2021-08-03T02:06:32.193Z</updated>
        <summary type="html"><![CDATA[Point clouds obtained from 3D sensors are usually sparse. Existing methods
mainly focus on upsampling sparse point clouds in a supervised manner by using
dense ground truth point clouds. In this paper, we propose a self-supervised
point cloud upsampling network (SSPU-Net) to generate dense point clouds
without using ground truth. To achieve this, we exploit the consistency between
the input sparse point cloud and generated dense point cloud for the shapes and
rendered images. Specifically, we first propose a neighbor expansion unit (NEU)
to upsample the sparse point clouds, where the local geometric structures of
the sparse point clouds are exploited to learn weights for point interpolation.
Then, we develop a differentiable point cloud rendering unit (DRU) as an
end-to-end module in our network to render the point cloud into multi-view
images. Finally, we formulate a shape-consistent loss and an image-consistent
loss to train the network so that the shapes of the sparse and dense point
clouds are as consistent as possible. Extensive results on the CAD and scanned
datasets demonstrate that our method can achieve impressive results in a
self-supervised manner. Code is available at https://github.com/Avlon/SSPU-Net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yifan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hui_L/0/1/0/all/0/1"&gt;Le Hui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jin Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multivariate Time Series Imputation by Graph Neural Networks. (arXiv:2108.00298v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00298</id>
        <link href="http://arxiv.org/abs/2108.00298"/>
        <updated>2021-08-03T02:06:32.175Z</updated>
        <summary type="html"><![CDATA[Dealing with missing values and incomplete time series is a labor-intensive
and time-consuming inevitable task when handling data coming from real-world
applications. Effective spatio-temporal representations would allow imputation
methods to reconstruct missing temporal data by exploiting information coming
from sensors at different locations. However, standard methods fall short in
capturing the nonlinear time and space dependencies existing within networks of
interconnected sensors and do not take full advantage of the available - and
often strong - relational information. Notably, most of state-of-the-art
imputation methods based on deep learning do not explicitly model relational
aspects and, in any case, do not exploit processing frameworks able to
adequately represent structured spatio-temporal data. Conversely, graph neural
networks have recently surged in popularity as both expressive and scalable
tools for processing sequential data with relational inductive biases. In this
work, we present the first assessment of graph neural networks in the context
of multivariate time series imputation. In particular, we introduce a novel
graph neural network architecture, named GRIL, which aims at reconstructing
missing data in the different channels of a multivariate time series by
learning spatial-temporal representations through message passing. Preliminary
empirical results show that our model outperforms state-of-the-art methods in
the imputation task on relevant benchmarks with mean absolute error
improvements often higher than 20%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cini_A/0/1/0/all/0/1"&gt;Andrea Cini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marisca_I/0/1/0/all/0/1"&gt;Ivan Marisca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alippi_C/0/1/0/all/0/1"&gt;Cesare Alippi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovering "Semantics" in Super-Resolution Networks. (arXiv:2108.00406v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00406</id>
        <link href="http://arxiv.org/abs/2108.00406"/>
        <updated>2021-08-03T02:06:32.154Z</updated>
        <summary type="html"><![CDATA[Super-resolution (SR) is a fundamental and representative task of low-level
vision area. It is generally thought that the features extracted from the SR
network have no specific semantic information, and the network simply learns
complex non-linear mappings from input to output. Can we find any "semantics"
in SR networks? In this paper, we give affirmative answers to this question. By
analyzing the feature representations with dimensionality reduction and
visualization, we successfully discover the deep semantic representations in SR
networks, \textit{i.e.}, deep degradation representations (DDR), which relate
to the image degradation types and degrees. We also reveal the differences in
representation semantics between classification and SR networks. Through
extensive experiments and analysis, we draw a series of observations and
conclusions, which are of great significance for future work, such as
interpreting the intrinsic mechanisms of low-level CNN networks and developing
new evaluation approaches for blind SR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yihao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Anran Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jinjin Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhipeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wenhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1"&gt;Chao Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subjective Image Quality Assessment with Boosted Triplet Comparisons. (arXiv:2108.00201v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00201</id>
        <link href="http://arxiv.org/abs/2108.00201"/>
        <updated>2021-08-03T02:06:32.132Z</updated>
        <summary type="html"><![CDATA[In subjective full-reference image quality assessment, differences between
perceptual image qualities of the reference image and its distorted versions
are evaluated, often using degradation category ratings (DCR). However, the DCR
has been criticized since differences between rating categories on this ordinal
scale might not be perceptually equidistant, and observers may have different
understandings of the categories. Pair comparisons (PC) of distorted images,
followed by Thurstonian reconstruction of scale values, overcome these
problems. In addition, PC is more sensitive than DCR, and it can provide scale
values in fractional, just noticeable difference (JND) units that express a
precise perceptional interpretation. Still, the comparison of images of nearly
the same quality can be difficult. We introduce boosting techniques embedded in
more general triplet comparisons (TC) that increase the sensitivity even more.
Boosting amplifies the artefacts of distorted images, enlarges their visual
representation by zooming, increases the visibility of the distortions by a
flickering effect, or combines some of the above. Experimental results show the
effectiveness of boosted TC for seven types of distortion. We crowdsourced over
1.7 million responses to triplet questions. A detailed analysis shows that
boosting increases the discriminatory power and allows to reduce the number of
subjective ratings without sacrificing the accuracy of the resulting relative
image quality values. Our technique paves the way to fine-grained image quality
datasets, allowing for more distortion levels, yet with high-quality subjective
annotations. We also provide the details for Thurstonian scale reconstruction
from TC and our annotated dataset, KonFiG-IQA, containing 10 source images,
processed using 7 distortion types at 12 or even 30 levels, uniformly spaced
over a span of 3 JND units.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Men_H/0/1/0/all/0/1"&gt;Hui Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Hanhe Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jenadeleh_M/0/1/0/all/0/1"&gt;Mohsen Jenadeleh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saupe_D/0/1/0/all/0/1"&gt;Dietmar Saupe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PyMAF: 3D Human Pose and Shape Regression with Pyramidal Mesh Alignment Feedback Loop. (arXiv:2103.16507v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16507</id>
        <link href="http://arxiv.org/abs/2103.16507"/>
        <updated>2021-08-03T02:06:32.085Z</updated>
        <summary type="html"><![CDATA[Regression-based methods have recently shown promising results in
reconstructing human meshes from monocular images. By directly mapping raw
pixels to model parameters, these methods can produce parametric models in a
feed-forward manner via neural networks. However, minor deviation in parameters
may lead to noticeable misalignment between the estimated meshes and image
evidences. To address this issue, we propose a Pyramidal Mesh Alignment
Feedback (PyMAF) loop to leverage a feature pyramid and rectify the predicted
parameters explicitly based on the mesh-image alignment status in our deep
regressor. In PyMAF, given the currently predicted parameters, mesh-aligned
evidences will be extracted from finer-resolution features accordingly and fed
back for parameter rectification. To reduce noise and enhance the reliability
of these evidences, an auxiliary pixel-wise supervision is imposed on the
feature encoder, which provides mesh-image correspondence guidance for our
network to preserve the most related information in spatial features. The
efficacy of our approach is validated on several benchmarks, including
Human3.6M, 3DPW, LSP, and COCO, where experimental results show that our
approach consistently improves the mesh-image alignment of the reconstruction.
The project page with code and video results can be found at
https://hongwenzhang.github.io/pymaf.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongwen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yating Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xinchi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1"&gt;Wanli Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yebin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Limin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zhenan Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting Video Captioning with Dynamic Loss Network. (arXiv:2107.11707v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.11707</id>
        <link href="http://arxiv.org/abs/2107.11707"/>
        <updated>2021-08-03T02:06:32.079Z</updated>
        <summary type="html"><![CDATA[Video captioning is one of the challenging problems at the intersection of
vision and language, having many real-life applications in video retrieval,
video surveillance, assisting visually challenged people, Human-machine
interface, and many more. Recent deep learning-based methods have shown
promising results but are still on the lower side than other vision tasks (such
as image classification, object detection). A significant drawback with
existing video captioning methods is that they are optimized over cross-entropy
loss function, which is uncorrelated to the de facto evaluation metrics (BLEU,
METEOR, CIDER, ROUGE).In other words, cross-entropy is not a proper surrogate
of the true loss function for video captioning. This paper addresses the
drawback by introducing a dynamic loss network (DLN), which provides an
additional feedback signal that directly reflects the evaluation metrics. Our
results on Microsoft Research Video Description Corpus (MSVD) and MSR-Video to
Text (MSRVTT) datasets outperform previous methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nasibullah/0/1/0/all/0/1"&gt;Nasibullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohanta_P/0/1/0/all/0/1"&gt;Partha Pratim Mohanta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Head Self-Attention via Vision Transformer for Zero-Shot Learning. (arXiv:2108.00045v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00045</id>
        <link href="http://arxiv.org/abs/2108.00045"/>
        <updated>2021-08-03T02:06:32.070Z</updated>
        <summary type="html"><![CDATA[Zero-Shot Learning (ZSL) aims to recognise unseen object classes, which are
not observed during the training phase. The existing body of works on ZSL
mostly relies on pretrained visual features and lacks the explicit attribute
localisation mechanism on images. In this work, we propose an attention-based
model in the problem settings of ZSL to learn attributes useful for unseen
class recognition. Our method uses an attention mechanism adapted from Vision
Transformer to capture and learn discriminative attributes by splitting images
into small patches. We conduct experiments on three popular ZSL benchmarks
(i.e., AWA2, CUB and SUN) and set new state-of-the-art harmonic mean results
{on all the three datasets}, which illustrate the effectiveness of our proposed
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alamri_F/0/1/0/all/0/1"&gt;Faisal Alamri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1"&gt;Anjan Dutta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Stress Monitoring using Wearable Sensors in Everyday Settings. (arXiv:2108.00144v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00144</id>
        <link href="http://arxiv.org/abs/2108.00144"/>
        <updated>2021-08-03T02:06:32.061Z</updated>
        <summary type="html"><![CDATA[Since stress contributes to a broad range of mental and physical health
problems, the objective assessment of stress is essential for behavioral and
physiological studies. Although several studies have evaluated stress levels in
controlled settings, objective stress assessment in everyday settings is still
largely under-explored due to challenges arising from confounding contextual
factors and limited adherence for self-reports. In this paper, we explore the
objective prediction of stress levels in everyday settings based on heart rate
(HR) and heart rate variability (HRV) captured via low-cost and easy-to-wear
photoplethysmography (PPG) sensors that are widely available on newer smart
wearable devices. We present a layered system architecture for personalized
stress monitoring that supports a tunable collection of data samples for
labeling, and present a method for selecting informative samples from the
stream of real-time data for labeling. We captured the stress levels of
fourteen volunteers through self-reported questionnaires over periods of
between 1-3 months, and explored binary stress detection based on HR and HRV
using Machine Learning Methods. We observe promising preliminary results given
that the dataset is collected in the challenging environments of everyday
settings. The binary stress detector is fairly accurate and can detect
stressful vs non-stressful samples with a macro-F1 score of up to \%76. Our
study lays the groundwork for more sophisticated labeling strategies that
generate context-aware, personalized models that will empower health
professionals to provide personalized interventions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tazarv_A/0/1/0/all/0/1"&gt;Ali Tazarv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labbaf_S/0/1/0/all/0/1"&gt;Sina Labbaf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reich_S/0/1/0/all/0/1"&gt;Stephanie M. Reich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dutt_N/0/1/0/all/0/1"&gt;Nikil Dutt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahmani_A/0/1/0/all/0/1"&gt;Amir M. Rahmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levorato_M/0/1/0/all/0/1"&gt;Marco Levorato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Surgical Data Science -- from Concepts toward Clinical Translation. (arXiv:2011.02284v2 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.02284</id>
        <link href="http://arxiv.org/abs/2011.02284"/>
        <updated>2021-08-03T02:06:32.050Z</updated>
        <summary type="html"><![CDATA[Recent developments in data science in general and machine learning in
particular have transformed the way experts envision the future of surgery.
Surgical Data Science (SDS) is a new research field that aims to improve the
quality of interventional healthcare through the capture, organization,
analysis and modeling of data. While an increasing number of data-driven
approaches and clinical applications have been studied in the fields of
radiological and clinical data science, translational success stories are still
lacking in surgery. In this publication, we shed light on the underlying
reasons and provide a roadmap for future advances in the field. Based on an
international workshop involving leading researchers in the field of SDS, we
review current practice, key achievements and initiatives as well as available
standards and tools for a number of topics relevant to the field, namely (1)
infrastructure for data acquisition, storage and access in the presence of
regulatory constraints, (2) data annotation and sharing and (3) data analytics.
We further complement this technical perspective with (4) a review of currently
available SDS products and the translational progress from academia and (5) a
roadmap for faster clinical translation and exploitation of the full potential
of SDS, based on an international multi-round Delphi process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maier_Hein_L/0/1/0/all/0/1"&gt;Lena Maier-Hein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eisenmann_M/0/1/0/all/0/1"&gt;Matthias Eisenmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarikaya_D/0/1/0/all/0/1"&gt;Duygu Sarikaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marz_K/0/1/0/all/0/1"&gt;Keno M&amp;#xe4;rz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collins_T/0/1/0/all/0/1"&gt;Toby Collins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malpani_A/0/1/0/all/0/1"&gt;Anand Malpani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fallert_J/0/1/0/all/0/1"&gt;Johannes Fallert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feussner_H/0/1/0/all/0/1"&gt;Hubertus Feussner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giannarou_S/0/1/0/all/0/1"&gt;Stamatia Giannarou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mascagni_P/0/1/0/all/0/1"&gt;Pietro Mascagni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakawala_H/0/1/0/all/0/1"&gt;Hirenkumar Nakawala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_A/0/1/0/all/0/1"&gt;Adrian Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pugh_C/0/1/0/all/0/1"&gt;Carla Pugh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1"&gt;Danail Stoyanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vedula_S/0/1/0/all/0/1"&gt;Swaroop S. Vedula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cleary_K/0/1/0/all/0/1"&gt;Kevin Cleary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fichtinger_G/0/1/0/all/0/1"&gt;Gabor Fichtinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forestier_G/0/1/0/all/0/1"&gt;Germain Forestier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gibaud_B/0/1/0/all/0/1"&gt;Bernard Gibaud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grantcharov_T/0/1/0/all/0/1"&gt;Teodor Grantcharov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashizume_M/0/1/0/all/0/1"&gt;Makoto Hashizume&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heckmann_Notzel_D/0/1/0/all/0/1"&gt;Doreen Heckmann-N&amp;#xf6;tzel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kenngott_H/0/1/0/all/0/1"&gt;Hannes G. Kenngott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kikinis_R/0/1/0/all/0/1"&gt;Ron Kikinis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mundermann_L/0/1/0/all/0/1"&gt;Lars M&amp;#xfc;ndermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1"&gt;Nassir Navab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Onogur_S/0/1/0/all/0/1"&gt;Sinan Onogur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1"&gt;Raphael Sznitman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taylor_R/0/1/0/all/0/1"&gt;Russell H. Taylor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tizabi_M/0/1/0/all/0/1"&gt;Minu D. Tizabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wagner_M/0/1/0/all/0/1"&gt;Martin Wagner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hager_G/0/1/0/all/0/1"&gt;Gregory D. Hager&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neumuth_T/0/1/0/all/0/1"&gt;Thomas Neumuth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1"&gt;Nicolas Padoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collins_J/0/1/0/all/0/1"&gt;Justin Collins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gockel_I/0/1/0/all/0/1"&gt;Ines Gockel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goedeke_J/0/1/0/all/0/1"&gt;Jan Goedeke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashimoto_D/0/1/0/all/0/1"&gt;Daniel A. Hashimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joyeux_L/0/1/0/all/0/1"&gt;Luc Joyeux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1"&gt;Kyle Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leff_D/0/1/0/all/0/1"&gt;Daniel R. Leff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madani_A/0/1/0/all/0/1"&gt;Amin Madani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marcus_H/0/1/0/all/0/1"&gt;Hani J. Marcus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meireles_O/0/1/0/all/0/1"&gt;Ozanan Meireles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seitel_A/0/1/0/all/0/1"&gt;Alexander Seitel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teber_D/0/1/0/all/0/1"&gt;Dogu Teber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uckert_F/0/1/0/all/0/1"&gt;Frank &amp;#xdc;ckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_Stich_B/0/1/0/all/0/1"&gt;Beat P. M&amp;#xfc;ller-Stich&lt;/a&gt;, et al. (2 additional authors not shown)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-Informed Machine Learning Method for Large-Scale Data Assimilation Problems. (arXiv:2108.00037v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00037</id>
        <link href="http://arxiv.org/abs/2108.00037"/>
        <updated>2021-08-03T02:06:32.041Z</updated>
        <summary type="html"><![CDATA[We develop a physics-informed machine learning approach for large-scale data
assimilation and parameter estimation and apply it for estimating
transmissivity and hydraulic head in the two-dimensional steady-state
subsurface flow model of the Hanford Site given synthetic measurements of said
variables. In our approach, we extend the physics-informed conditional
Karhunen-Lo\'{e}ve expansion (PICKLE) method for modeling subsurface flow with
unknown flux (Neumann) and varying head (Dirichlet) boundary conditions. We
demonstrate that the PICKLE method is comparable in accuracy with the standard
maximum a posteriori (MAP) method, but is significantly faster than MAP for
large-scale problems. Both methods use a mesh to discretize the computational
domain. In MAP, the parameters and states are discretized on the mesh;
therefore, the size of the MAP parameter estimation problem directly depends on
the mesh size. In PICKLE, the mesh is used to evaluate the residuals of the
governing equation, while the parameters and states are approximated by the
truncated conditional Karhunen-Lo\'{e}ve expansions with the number of
parameters controlled by the smoothness of the parameter and state fields, and
not by the mesh size. For a considered example, we demonstrate that the
computational cost of PICKLE increases near linearly (as $N_{FV}^{1.15}$) with
the number of grid points $N_{FV}$, while that of MAP increases much faster as
$N_{FV}^{3.28}$. We demonstrated that once trained for one set of Dirichlet
boundary conditions (i.e., one river stage), the PICKLE method provides
accurate estimates of the hydraulic head for any value of the Dirichlet
boundary conditions (i.e., for any river stage).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yeung_Y/0/1/0/all/0/1"&gt;Yu-Hong Yeung&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Barajas_Solano_D/0/1/0/all/0/1"&gt;David A. Barajas-Solano&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Tartakovsky_A/0/1/0/all/0/1"&gt;Alexandre M. Tartakovsky&lt;/a&gt; (1 and 2) ((1) Physical and Computational Sciences Directorate, Pacific Northwest National Laboratory, (2) Department of Civil and Environmental Engineering, University of Illinois Urbana-Champaign)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Generative Adversarial Networks in Cancer Imaging: New Applications, New Solutions. (arXiv:2107.09543v1 [eess.IV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2107.09543</id>
        <link href="http://arxiv.org/abs/2107.09543"/>
        <updated>2021-08-03T02:06:32.034Z</updated>
        <summary type="html"><![CDATA[Despite technological and medical advances, the detection, interpretation,
and treatment of cancer based on imaging data continue to pose significant
challenges. These include high inter-observer variability, difficulty of
small-sized lesion detection, nodule interpretation and malignancy
determination, inter- and intra-tumour heterogeneity, class imbalance,
segmentation inaccuracies, and treatment effect uncertainty. The recent
advancements in Generative Adversarial Networks (GANs) in computer vision as
well as in medical imaging may provide a basis for enhanced capabilities in
cancer detection and analysis. In this review, we assess the potential of GANs
to address a number of key challenges of cancer imaging, including data
scarcity and imbalance, domain and dataset shifts, data access and privacy,
data annotation and quantification, as well as cancer detection, tumour
profiling and treatment planning. We provide a critical appraisal of the
existing literature of GANs applied to cancer imagery, together with
suggestions on future research directions to address these challenges. We
analyse and discuss 163 papers that apply adversarial training techniques in
the context of cancer imaging and elaborate their methodologies, advantages and
limitations. With this work, we strive to bridge the gap between the needs of
the clinical cancer imaging community and the current and prospective research
on GANs in the artificial intelligence community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Osuala_R/0/1/0/all/0/1"&gt;Richard Osuala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kushibar_K/0/1/0/all/0/1"&gt;Kaisar Kushibar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Garrucho_L/0/1/0/all/0/1"&gt;Lidia Garrucho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Linardos_A/0/1/0/all/0/1"&gt;Akis Linardos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Szafranowska_Z/0/1/0/all/0/1"&gt;Zuzanna Szafranowska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Klein_S/0/1/0/all/0/1"&gt;Stefan Klein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Glocker_B/0/1/0/all/0/1"&gt;Ben Glocker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Diaz_O/0/1/0/all/0/1"&gt;Oliver Diaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1"&gt;Karim Lekadir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning based Virtual Point Tracking for Real-Time Target-less Dynamic Displacement Measurement in Railway Applications. (arXiv:2101.06702v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06702</id>
        <link href="http://arxiv.org/abs/2101.06702"/>
        <updated>2021-08-03T02:06:32.028Z</updated>
        <summary type="html"><![CDATA[In the application of computer-vision based displacement measurement, an
optical target is usually required to prove the reference. In the case that the
optical target cannot be attached to the measuring objective, edge detection,
feature matching and template matching are the most common approaches in
target-less photogrammetry. However, their performance significantly relies on
parameter settings. This becomes problematic in dynamic scenes where
complicated background texture exists and varies over time. To tackle this
issue, we propose virtual point tracking for real-time target-less dynamic
displacement measurement, incorporating deep learning techniques and domain
knowledge. Our approach consists of three steps: 1) automatic calibration for
detection of region of interest; 2) virtual point detection for each video
frame using deep convolutional neural network; 3) domain-knowledge based rule
engine for point tracking in adjacent frames. The proposed approach can be
executed on an edge computer in a real-time manner (i.e. over 30 frames per
second). We demonstrate our approach for a railway application, where the
lateral displacement of the wheel on the rail is measured during operation. We
also implement an algorithm using template matching and line detection as the
baseline for comparison. The numerical experiments have been performed to
evaluate the performance and the latency of our approach in the harsh railway
environment with noisy and varying backgrounds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1"&gt;Dachuan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabanovic_E/0/1/0/all/0/1"&gt;Eldar Sabanovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rizzetto_L/0/1/0/all/0/1"&gt;Luca Rizzetto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Skrickij_V/0/1/0/all/0/1"&gt;Viktor Skrickij&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliverio_R/0/1/0/all/0/1"&gt;Roberto Oliverio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaviani_N/0/1/0/all/0/1"&gt;Nadia Kaviani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yunguang Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bureika_G/0/1/0/all/0/1"&gt;Gintautas Bureika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ricci_S/0/1/0/all/0/1"&gt;Stefano Ricci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hecht_M/0/1/0/all/0/1"&gt;Markus Hecht&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AINet: Association Implantation for Superpixel Segmentation. (arXiv:2101.10696v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10696</id>
        <link href="http://arxiv.org/abs/2101.10696"/>
        <updated>2021-08-03T02:06:31.986Z</updated>
        <summary type="html"><![CDATA[Recently, some approaches are proposed to harness deep convolutional networks
to facilitate superpixel segmentation. The common practice is to first evenly
divide the image into a pre-defined number of grids and then learn to associate
each pixel with its surrounding grids. However, simply applying a series of
convolution operations with limited receptive fields can only implicitly
perceive the relations between the pixel and its surrounding grids.
Consequently, existing methods often fail to provide an effective context when
inferring the association map. To remedy this issue, we propose a novel
\textbf{A}ssociation \textbf{I}mplantation (AI) module to enable the network to
explicitly capture the relations between the pixel and its surrounding grids.
The proposed AI module directly implants the features of grid cells to the
surrounding of its corresponding central pixel, and conducts convolution on the
padded window to adaptively transfer knowledge between them. With such an
implantation operation, the network could explicitly harvest the pixel-grid
level context, which is more in line with the target of superpixel segmentation
comparing to the pixel-wise relation. Furthermore, to pursue better boundary
precision, we design a boundary-perceiving loss to help the network
discriminate the pixels around boundaries in hidden feature level, which could
benefit the subsequent inferring modules to accurately identify more boundary
pixels. Extensive experiments on BSDS500 and NYUv2 datasets show that our
method could not only achieve state-of-the-art performance but maintain
satisfactory inference efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yaxiong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yunchao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1"&gt;Xueming Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Li Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Goal-constrained Sparse Reinforcement Learning for End-to-End Driving. (arXiv:2103.09189v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09189</id>
        <link href="http://arxiv.org/abs/2103.09189"/>
        <updated>2021-08-03T02:06:31.976Z</updated>
        <summary type="html"><![CDATA[Deep reinforcement Learning for end-to-end driving is limited by the need of
complex reward engineering. Sparse rewards can circumvent this challenge but
suffers from long training time and leads to sub-optimal policy. In this work,
we explore full-control driving with only goal-constrained sparse reward and
propose a curriculum learning approach for end-to-end driving using only
navigation view maps that benefit from small virtual-to-real domain gap. To
address the complexity of multiple driving policies, we learn concurrent
individual policies selected at inference by a navigation system. We
demonstrate the ability of our proposal to generalize on unseen road layout,
and to drive significantly longer than in the training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_P/0/1/0/all/0/1"&gt;Pranav Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beaucorps_P/0/1/0/all/0/1"&gt;Pierre de Beaucorps&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1"&gt;Raoul de Charette&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PLUMENet: Efficient 3D Object Detection from Stereo Images. (arXiv:2101.06594v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06594</id>
        <link href="http://arxiv.org/abs/2101.06594"/>
        <updated>2021-08-03T02:06:31.966Z</updated>
        <summary type="html"><![CDATA[3D object detection is a key component of many robotic applications such as
self-driving vehicles. While many approaches rely on expensive 3D sensors such
as LiDAR to produce accurate 3D estimates, methods that exploit stereo cameras
have recently shown promising results at a lower cost. Existing approaches
tackle this problem in two steps: first depth estimation from stereo images is
performed to produce a pseudo LiDAR point cloud, which is then used as input to
a 3D object detector. However, this approach is suboptimal due to the
representation mismatch, as the two tasks are optimized in two different metric
spaces. In this paper we propose a model that unifies these two tasks and
performs them in the same metric space. Specifically, we directly construct a
pseudo LiDAR feature volume (PLUME) in 3D space, which is then used to solve
both depth estimation and object detection tasks. Our approach achieves
state-of-the-art performance with much faster inference times when compared to
existing methods on the challenging KITTI benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Bin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1"&gt;Rui Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1"&gt;Ming Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1"&gt;Raquel Urtasun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parameter-Efficient Person Re-identification in the 3D Space. (arXiv:2006.04569v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04569</id>
        <link href="http://arxiv.org/abs/2006.04569"/>
        <updated>2021-08-03T02:06:31.958Z</updated>
        <summary type="html"><![CDATA[People live in a 3D world. However, existing works on person
re-identification (re-id) mostly consider the semantic representation learning
in a 2D space, intrinsically limiting the understanding of people. In this
work, we address this limitation by exploring the prior knowledge of the 3D
body structure. Specifically, we project 2D images to a 3D space and introduce
a novel parameter-efficient Omni-scale Graph Network (OG-Net) to learn the
pedestrian representation directly from 3D point clouds. OG-Net effectively
exploits the local information provided by sparse 3D points and takes advantage
of the structure and appearance information in a coherent manner. With the help
of 3D geometry information, we can learn a new type of deep re-id feature free
from noisy variants, such as scale and viewpoint. To our knowledge, we are
among the first attempts to conduct person re-identification in the 3D space.
We demonstrate through extensive experiments that the proposed method (1) eases
the matching difficulty in the traditional 2D space, (2) exploits the
complementary information of 2D appearance and 3D structure, (3) achieves
competitive results with limited parameters on four large-scale person re-id
datasets, and (4) has good scalability to unseen datasets. Our code, models and
generated 3D human data are publicly available at
https://github.com/layumi/person-reid-3d .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhedong Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1"&gt;Nenggan Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Spatial-spectral Network Learning for Hyperspectral Compressive Snapshot Reconstruction. (arXiv:2012.12086v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12086</id>
        <link href="http://arxiv.org/abs/2012.12086"/>
        <updated>2021-08-03T02:06:31.942Z</updated>
        <summary type="html"><![CDATA[Hyperspectral compressive imaging takes advantage of compressive sensing
theory to achieve coded aperture snapshot measurement without temporal
scanning, and the entire three-dimensional spatial-spectral data is captured by
a two-dimensional projection during a single integration period. Its core issue
is how to reconstruct the underlying hyperspectral image using compressive
sensing reconstruction algorithms. Due to the diversity in the spectral
response characteristics and wavelength range of different spectral imaging
devices, previous works are often inadequate to capture complex spectral
variations or lack the adaptive capacity to new hyperspectral imagers. In order
to address these issues, we propose an unsupervised spatial-spectral network to
reconstruct hyperspectral images only from the compressive snapshot
measurement. The proposed network acts as a conditional generative model
conditioned on the snapshot measurement, and it exploits the spatial-spectral
attention module to capture the joint spatial-spectral correlation of
hyperspectral images. The network parameters are optimized to make sure that
the network output can closely match the given snapshot measurement according
to the imaging model, thus the proposed network can adapt to different imaging
settings, which can inherently enhance the applicability of the network.
Extensive experiments upon multiple datasets demonstrate that our network can
achieve better reconstruction results than the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yubao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Ying Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qingshan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kankanhalli_M/0/1/0/all/0/1"&gt;Mohan Kankanhalli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Edge-competing Pathological Liver Vessel Segmentation with Limited Labels. (arXiv:2108.00384v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00384</id>
        <link href="http://arxiv.org/abs/2108.00384"/>
        <updated>2021-08-03T02:06:31.925Z</updated>
        <summary type="html"><![CDATA[The microvascular invasion (MVI) is a major prognostic factor in
hepatocellular carcinoma, which is one of the malignant tumors with the highest
mortality rate. The diagnosis of MVI needs discovering the vessels that contain
hepatocellular carcinoma cells and counting their number in each vessel, which
depends heavily on experiences of the doctor, is largely subjective and
time-consuming. However, there is no algorithm as yet tailored for the MVI
detection from pathological images. This paper collects the first pathological
liver image dataset containing 522 whole slide images with labels of vessels,
MVI, and hepatocellular carcinoma grades. The first and essential step for the
automatic diagnosis of MVI is the accurate segmentation of vessels. The unique
characteristics of pathological liver images, such as super-large size,
multi-scale vessel, and blurred vessel edges, make the accurate vessel
segmentation challenging. Based on the collected dataset, we propose an
Edge-competing Vessel Segmentation Network (EVS-Net), which contains a
segmentation network and two edge segmentation discriminators. The segmentation
network, combined with an edge-aware self-supervision mechanism, is devised to
conduct vessel segmentation with limited labeled patches. Meanwhile, two
discriminators are introduced to distinguish whether the segmented vessel and
background contain residual features in an adversarial manner. In the training
stage, two discriminators are devised tocompete for the predicted position of
edges. Exhaustive experiments demonstrate that, with only limited labeled
patches, EVS-Net achieves a close performance of fully supervised methods,
which provides a convenient tool for the pathological liver vessel
segmentation. Code is publicly available at
https://github.com/zju-vipa/EVS-Net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zunlei Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhonghua Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinchao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiuming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1"&gt;Lechao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuexuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1"&gt;Mingli Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DySMHO: Data-Driven Discovery of Governing Equations for Dynamical Systems via Moving Horizon Optimization. (arXiv:2108.00069v1 [math.DS])]]></title>
        <id>http://arxiv.org/abs/2108.00069</id>
        <link href="http://arxiv.org/abs/2108.00069"/>
        <updated>2021-08-03T02:06:31.892Z</updated>
        <summary type="html"><![CDATA[Discovering the governing laws underpinning physical and chemical phenomena
is a key step towards understanding and ultimately controlling systems in
science and engineering. We introduce Discovery of Dynamical Systems via Moving
Horizon Optimization (DySMHO), a scalable machine learning framework for
identifying governing laws in the form of differential equations from
large-scale noisy experimental data sets. DySMHO consists of a novel moving
horizon dynamic optimization strategy that sequentially learns the underlying
governing equations from a large dictionary of basis functions. The sequential
nature of DySMHO allows leveraging statistical arguments for eliminating
irrelevant basis functions, avoiding overfitting to recover accurate and
parsimonious forms of the governing equations. Canonical nonlinear dynamical
system examples are used to demonstrate that DySMHO can accurately recover the
governing laws, is robust to high levels of measurement noise and that it can
handle challenges such as multiple time scale dynamics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Lejarza_F/0/1/0/all/0/1"&gt;Fernando Lejarza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Baldea_M/0/1/0/all/0/1"&gt;Michael Baldea&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coordinate descent on the orthogonal group for recurrent neural network training. (arXiv:2108.00051v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00051</id>
        <link href="http://arxiv.org/abs/2108.00051"/>
        <updated>2021-08-03T02:06:31.886Z</updated>
        <summary type="html"><![CDATA[We propose to use stochastic Riemannian coordinate descent on the orthogonal
group for recurrent neural network training. The algorithm rotates successively
two columns of the recurrent matrix, an operation that can be efficiently
implemented as a multiplication by a Givens matrix. In the case when the
coordinate is selected uniformly at random at each iteration, we prove the
convergence of the proposed algorithm under standard assumptions on the loss
function, stepsize and minibatch noise. In addition, we numerically demonstrate
that the Riemannian gradient in recurrent neural network training has an
approximately sparse structure. Leveraging this observation, we propose a
faster variant of the proposed algorithm that relies on the Gauss-Southwell
rule. Experiments on a benchmark recurrent neural network training problem are
presented to demonstrate the effectiveness of the proposed algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Massart_E/0/1/0/all/0/1"&gt;Estelle Massart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abrol_V/0/1/0/all/0/1"&gt;Vinayak Abrol&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time Instance Segmentation with Discriminative Orientation Maps. (arXiv:2106.12204v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12204</id>
        <link href="http://arxiv.org/abs/2106.12204"/>
        <updated>2021-08-03T02:06:31.879Z</updated>
        <summary type="html"><![CDATA[Although instance segmentation has made considerable advancement over recent
years, it's still a challenge to design high accuracy algorithms with real-time
performance. In this paper, we propose a real-time instance segmentation
framework termed OrienMask. Upon the one-stage object detector YOLOv3, a mask
head is added to predict some discriminative orientation maps, which are
explicitly defined as spatial offset vectors for both foreground and background
pixels. Thanks to the discrimination ability of orientation maps, masks can be
recovered without the need for extra foreground segmentation. All instances
that match with the same anchor size share a common orientation map. This
special sharing strategy reduces the amortized memory utilization for mask
predictions but without loss of mask granularity. Given the surviving box
predictions after NMS, instance masks can be concurrently constructed from the
corresponding orientation maps with low complexity. Owing to the concise design
for mask representation and its effective integration with the anchor-based
object detector, our method is qualified under real-time conditions while
maintaining competitive accuracy. Experiments on COCO benchmark show that
OrienMask achieves 34.8 mask AP at the speed of 42.7 fps evaluated with a
single RTX 2080 Ti. The code is available at https://github.com/duwt/OrienMask.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1"&gt;Wentao Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1"&gt;Zhiyu Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shuya Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_C/0/1/0/all/0/1"&gt;Chengyu Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiman Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1"&gt;Tingming Bai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Batch Selection Policy for Active Metric Learning. (arXiv:2102.07365v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07365</id>
        <link href="http://arxiv.org/abs/2102.07365"/>
        <updated>2021-08-03T02:06:31.872Z</updated>
        <summary type="html"><![CDATA[Active metric learning is the problem of incrementally selecting high-utility
batches of training data (typically, ordered triplets) to annotate, in order to
progressively improve a learned model of a metric over some input domain as
rapidly as possible. Standard approaches, which independently assess the
informativeness of each triplet in a batch, are susceptible to highly
correlated batches with many redundant triplets and hence low overall utility.
While a recent work \cite{kumari2020batch} proposes batch-decorrelation
strategies for metric learning, they rely on ad hoc heuristics to estimate the
correlation between two triplets at a time. We present a novel batch active
metric learning method that leverages the Maximum Entropy Principle to learn
the least biased estimate of triplet distribution for a given set of prior
constraints. To avoid redundancy between triplets, our method collectively
selects batches with maximum joint entropy, which simultaneously captures both
informativeness and diversity. We take advantage of the submodularity of the
joint entropy function to construct a tractable solution using an efficient
greedy algorithm based on Gram-Schmidt orthogonalization that is provably
$\left( 1 - \frac{1}{e} \right)$-optimal. Our approach is the first batch
active metric learning method to define a unified score that balances
informativeness and diversity for an entire batch of triplets. Experiments with
several real-world datasets demonstrate that our algorithm is robust,
generalizes well to different applications and input modalities, and
consistently outperforms the state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+K_P/0/1/0/all/0/1"&gt;Priyadarshini K&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1"&gt;Siddhartha Chaudhuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borkar_V/0/1/0/all/0/1"&gt;Vivek Borkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1"&gt;Subhasis Chaudhuri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured Context Enhancement Network for Mouse Pose Estimation. (arXiv:2012.00630v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00630</id>
        <link href="http://arxiv.org/abs/2012.00630"/>
        <updated>2021-08-03T02:06:31.865Z</updated>
        <summary type="html"><![CDATA[Automated analysis of mouse behaviours is crucial for many applications in
neuroscience. However, quantifying mouse behaviours from videos or images
remains a challenging problem, where pose estimation plays an important role in
describing mouse behaviours. Although deep learning based methods have made
promising advances in human pose estimation, they cannot be directly applied to
pose estimation of mice due to different physiological natures. Particularly,
since mouse body is highly deformable, it is a challenge to accurately locate
different keypoints on the mouse body. In this paper, we propose a novel
Hourglass network based model, namely Graphical Model based Structured Context
Enhancement Network (GM-SCENet) where two effective modules, i.e., Structured
Context Mixer (SCM) and Cascaded Multi-Level Supervision (CMLS) are
subsequently implemented. SCM can adaptively learn and enhance the proposed
structured context information of each mouse part by a novel graphical model
that takes into account the motion difference between body parts. Then, the
CMLS module is designed to jointly train the proposed SCM and the Hourglass
network by generating multi-level information, increasing the robustness of the
whole network.Using the multi-level prediction information from SCM and CMLS,
we develop an inference method to ensure the accuracy of the localisation
results. Finally, we evaluate our proposed approach against several
baselines...]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1"&gt;Feixiang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zheheng Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhihua Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Fang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Long Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_L/0/1/0/all/0/1"&gt;Lei Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhile Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haikuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_M/0/1/0/all/0/1"&gt;Minrui Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Ling Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Huiyu Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New Semi-supervised Learning Benchmark for Classifying View and Diagnosing Aortic Stenosis from Echocardiograms. (arXiv:2108.00080v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00080</id>
        <link href="http://arxiv.org/abs/2108.00080"/>
        <updated>2021-08-03T02:06:31.848Z</updated>
        <summary type="html"><![CDATA[Semi-supervised image classification has shown substantial progress in
learning from limited labeled data, but recent advances remain largely untested
for clinical applications. Motivated by the urgent need to improve timely
diagnosis of life-threatening heart conditions, especially aortic stenosis, we
develop a benchmark dataset to assess semi-supervised approaches to two tasks
relevant to cardiac ultrasound (echocardiogram) interpretation: view
classification and disease severity classification. We find that a
state-of-the-art method called MixMatch achieves promising gains in heldout
accuracy on both tasks, learning from a large volume of truly unlabeled images
as well as a labeled set collected at great expense to achieve better
performance than is possible with the labeled set alone. We further pursue
patient-level diagnosis prediction, which requires aggregating across hundreds
of images of diverse view types, most of which are irrelevant, to make a
coherent prediction. The best patient-level performance is achieved by new
methods that prioritize diagnosis predictions from images that are predicted to
be clinically-relevant views and transfer knowledge from the view task to the
diagnosis task. We hope our released Tufts Medical Echocardiogram Dataset and
evaluation framework inspire further improvements in multi-task semi-supervised
learning for clinical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhe Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1"&gt;Gary Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wessler_B/0/1/0/all/0/1"&gt;Benjamin Wessler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1"&gt;Michael C. Hughes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Optimization in Materials Science: A Survey. (arXiv:2108.00002v1 [cond-mat.mtrl-sci])]]></title>
        <id>http://arxiv.org/abs/2108.00002</id>
        <link href="http://arxiv.org/abs/2108.00002"/>
        <updated>2021-08-03T02:06:31.842Z</updated>
        <summary type="html"><![CDATA[Bayesian optimization is used in many areas of AI for the optimization of
black-box processes and has achieved impressive improvements of the state of
the art for a lot of applications. It intelligently explores large and complex
design spaces while minimizing the number of evaluations of the expensive
underlying process to be optimized. Materials science considers the problem of
optimizing materials' properties given a large design space that defines how to
synthesize or process them, with evaluations requiring expensive experiments or
simulations -- a very similar setting. While Bayesian optimization is also a
popular approach to tackle such problems, there is almost no overlap between
the two communities that are investigating the same concepts. We present a
survey of Bayesian optimization approaches in materials science to increase
cross-fertilization and avoid duplication of work. We highlight common
challenges and opportunities for joint research efforts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Kotthoff_L/0/1/0/all/0/1"&gt;Lars Kotthoff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Wahab_H/0/1/0/all/0/1"&gt;Hud Wahab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Johnson_P/0/1/0/all/0/1"&gt;Patrick Johnson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tensor-Train Density Estimation. (arXiv:2108.00089v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00089</id>
        <link href="http://arxiv.org/abs/2108.00089"/>
        <updated>2021-08-03T02:06:31.835Z</updated>
        <summary type="html"><![CDATA[Estimation of probability density function from samples is one of the central
problems in statistics and machine learning. Modern neural network-based models
can learn high dimensional distributions but have problems with hyperparameter
selection and are often prone to instabilities during training and inference.
We propose a new efficient tensor train-based model for density estimation
(TTDE). Such density parametrization allows exact sampling, calculation of
cumulative and marginal density functions, and partition function. It also has
very intuitive hyperparameters. We develop an efficient non-adversarial
training procedure for TTDE based on the Riemannian optimization. Experimental
results demonstrate the competitive performance of the proposed method in
density estimation and sampling tasks, while TTDE significantly outperforms
competitors in training speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Novikov_G/0/1/0/all/0/1"&gt;Georgii S. Novikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panov_M/0/1/0/all/0/1"&gt;Maxim E. Panov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oseledets_I/0/1/0/all/0/1"&gt;Ivan V. Oseledets&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Drop Points for LiDAR Scan Synthesis. (arXiv:2102.11952v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11952</id>
        <link href="http://arxiv.org/abs/2102.11952"/>
        <updated>2021-08-03T02:06:31.828Z</updated>
        <summary type="html"><![CDATA[3D laser scanning by LiDAR sensors plays an important role for mobile robots
to understand their surroundings. Nevertheless, not all systems have high
resolution and accuracy due to hardware limitations, weather conditions, and so
on. Generative modeling of LiDAR data as scene priors is one of the promising
solutions to compensate for unreliable or incomplete observations. In this
paper, we propose a novel generative model for learning LiDAR data based on
generative adversarial networks. As in the related studies, we process LiDAR
data as a compact yet lossless representation, a cylindrical depth map.
However, despite the smoothness of real-world objects, many points on the depth
map are dropped out through the laser measurement, which causes learning
difficulty on generative models. To circumvent this issue, we introduce
measurement uncertainty into the generation process, which allows the model to
learn a disentangled representation of the underlying shape and the dropout
noises from a collection of real LiDAR data. To simulate the lossy measurement,
we adopt a differentiable sampling framework to drop points based on the
learned uncertainty. We demonstrate the effectiveness of our method on
synthesis and reconstruction tasks using two datasets. We further showcase
potential applications by restoring LiDAR data with various types of
corruption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nakashima_K/0/1/0/all/0/1"&gt;Kazuto Nakashima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurazume_R/0/1/0/all/0/1"&gt;Ryo Kurazume&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TDA-Net: Fusion of Persistent Homology and Deep Learning Features for COVID-19 Detection in Chest X-Ray Images. (arXiv:2101.08398v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08398</id>
        <link href="http://arxiv.org/abs/2101.08398"/>
        <updated>2021-08-03T02:06:31.821Z</updated>
        <summary type="html"><![CDATA[Topological Data Analysis (TDA) has emerged recently as a robust tool to
extract and compare the structure of datasets. TDA identifies features in data
such as connected components and holes and assigns a quantitative measure to
these features. Several studies reported that topological features extracted by
TDA tools provide unique information about the data, discover new insights, and
determine which feature is more related to the outcome. On the other hand, the
overwhelming success of deep neural networks in learning patterns and
relationships has been proven on a vast array of data applications, images in
particular. To capture the characteristics of both powerful tools, we propose
\textit{TDA-Net}, a novel ensemble network that fuses topological and deep
features for the purpose of enhancing model generalizability and accuracy. We
apply the proposed \textit{TDA-Net} to a critical application, which is the
automated detection of COVID-19 from CXR images. The experimental results
showed that the proposed network achieved excellent performance and suggests
the applicability of our method in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hajij_M/0/1/0/all/0/1"&gt;Mustafa Hajij&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamzmi_G/0/1/0/all/0/1"&gt;Ghada Zamzmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Batayneh_F/0/1/0/all/0/1"&gt;Fawwaz Batayneh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward Robust Autotuning of Noisy Quantum Dot Devices. (arXiv:2108.00043v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2108.00043</id>
        <link href="http://arxiv.org/abs/2108.00043"/>
        <updated>2021-08-03T02:06:31.802Z</updated>
        <summary type="html"><![CDATA[The current autotuning approaches for quantum dot (QD) devices, while showing
some success, lack an assessment of data reliability. This leads to unexpected
failures when noisy data is processed by an autonomous system. In this work, we
propose a framework for robust autotuning of QD devices that combines a machine
learning (ML) state classifier with a data quality control module. The data
quality control module acts as a ``gatekeeper'' system, ensuring that only
reliable data is processed by the state classifier. Lower data quality results
in either device recalibration or termination. To train both ML systems, we
enhance the QD simulation by incorporating synthetic noise typical of QD
experiments. We confirm that the inclusion of synthetic noise in the training
of the state classifier significantly improves the performance, resulting in an
accuracy of 95.1(7) % when tested on experimental data. We then validate the
functionality of the data quality control module by showing the state
classifier performance deteriorates with decreasing data quality, as expected.
Our results establish a robust and flexible ML framework for autonomous tuning
of noisy QD devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Ziegler_J/0/1/0/all/0/1"&gt;Joshua Ziegler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+McJunkin_T/0/1/0/all/0/1"&gt;Thomas McJunkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Joseph_E/0/1/0/all/0/1"&gt;E. S. Joseph&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kalantre_S/0/1/0/all/0/1"&gt;Sandesh S. Kalantre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Harpt_B/0/1/0/all/0/1"&gt;Benjamin Harpt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Savage_D/0/1/0/all/0/1"&gt;D. E. Savage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Lagally_M/0/1/0/all/0/1"&gt;M. G. Lagally&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Eriksson_M/0/1/0/all/0/1"&gt;M. A. Eriksson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Taylor_J/0/1/0/all/0/1"&gt;Jacob M. Taylor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Zwolak_J/0/1/0/all/0/1"&gt;Justyna P. Zwolak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A 3D model-based approach for fitting masks to faces in the wild. (arXiv:2103.00803v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00803</id>
        <link href="http://arxiv.org/abs/2103.00803"/>
        <updated>2021-08-03T02:06:31.794Z</updated>
        <summary type="html"><![CDATA[Face recognition now requires a large number of labelled masked face images
in the era of this unprecedented COVID-19 pandemic. Unfortunately, the rapid
spread of the virus has left us little time to prepare for such dataset in the
wild. To circumvent this issue, we present a 3D model-based approach called
WearMask3D for augmenting face images of various poses to the masked face
counterparts. Our method proceeds by first fitting a 3D morphable model on the
input image, second overlaying the mask surface onto the face model and warping
the respective mask texture, and last projecting the 3D mask back to 2D. The
mask texture is adapted based on the brightness and resolution of the input
image. By working in 3D, our method can produce more natural masked faces of
diverse poses from a single mask texture. To compare precisely between
different augmentation approaches, we have constructed a dataset comprising
masked and unmasked faces with labels called MFW-mini. Experimental results
demonstrate WearMask3D produces more realistic masked faces, and utilizing
these images for training leads to state-of-the-art recognition accuracy for
masked faces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1"&gt;Je Hyeong Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hanjo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minsoo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nam_G/0/1/0/all/0/1"&gt;Gi Pyo Nam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1"&gt;Junghyun Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1"&gt;Hyeong-Seok Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_I/0/1/0/all/0/1"&gt;Ig-Jae Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OPFython: A Python-Inspired Optimum-Path Forest Classifier. (arXiv:2001.10420v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.10420</id>
        <link href="http://arxiv.org/abs/2001.10420"/>
        <updated>2021-08-03T02:06:31.787Z</updated>
        <summary type="html"><![CDATA[Machine learning techniques have been paramount throughout the last years,
being applied in a wide range of tasks, such as classification, object
recognition, person identification, and image segmentation. Nevertheless,
conventional classification algorithms, e.g., Logistic Regression, Decision
Trees, and Bayesian classifiers, might lack complexity and diversity, not
suitable when dealing with real-world data. A recent graph-inspired classifier,
known as the Optimum-Path Forest, has proven to be a state-of-the-art
technique, comparable to Support Vector Machines and even surpassing it in some
tasks. This paper proposes a Python-based Optimum-Path Forest framework,
denoted as OPFython, where all of its functions and classes are based upon the
original C language implementation. Additionally, as OPFython is a Python-based
library, it provides a more friendly environment and a faster prototyping
workspace than the C language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosa_G/0/1/0/all/0/1"&gt;Gustavo Henrique de Rosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papa_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Paulo Papa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Falcao_A/0/1/0/all/0/1"&gt;Alexandre Xavier Falc&amp;#xe3;o&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Choreographer: Music Conditioned 3D Dance Generation with AIST++. (arXiv:2101.08779v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08779</id>
        <link href="http://arxiv.org/abs/2101.08779"/>
        <updated>2021-08-03T02:06:31.779Z</updated>
        <summary type="html"><![CDATA[We present AIST++, a new multi-modal dataset of 3D dance motion and music,
along with FACT, a Full-Attention Cross-modal Transformer network for
generating 3D dance motion conditioned on music. The proposed AIST++ dataset
contains 5.2 hours of 3D dance motion in 1408 sequences, covering 10 dance
genres with multi-view videos with known camera poses -- the largest dataset of
this kind to our knowledge. We show that naively applying sequence models such
as transformers to this dataset for the task of music conditioned 3D motion
generation does not produce satisfactory 3D motion that is well correlated with
the input music. We overcome these shortcomings by introducing key changes in
its architecture design and supervision: FACT model involves a deep cross-modal
transformer block with full-attention that is trained to predict $N$ future
motions. We empirically show that these changes are key factors in generating
long sequences of realistic dance motion that are well-attuned to the input
music. We conduct extensive experiments on AIST++ with user studies, where our
method outperforms recent state-of-the-art methods both qualitatively and
quantitatively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruilong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ross_D/0/1/0/all/0/1"&gt;David A. Ross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanazawa_A/0/1/0/all/0/1"&gt;Angjoo Kanazawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to synthesise the ageing brain without longitudinal data. (arXiv:1912.02620v5 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.02620</id>
        <link href="http://arxiv.org/abs/1912.02620"/>
        <updated>2021-08-03T02:06:31.769Z</updated>
        <summary type="html"><![CDATA[How will my face look when I get older? Or, for a more challenging question:
How will my brain look when I get older? To answer this question one must
devise (and learn from data) a multivariate auto-regressive function which
given an image and a desired target age generates an output image. While
collecting data for faces may be easier, collecting longitudinal brain data is
not trivial. We propose a deep learning-based method that learns to simulate
subject-specific brain ageing trajectories without relying on longitudinal
data. Our method synthesises images conditioned on two factors: age (a
continuous variable), and status of Alzheimer's Disease (AD, an ordinal
variable). With an adversarial formulation we learn the joint distribution of
brain appearance, age and AD status, and define reconstruction losses to
address the challenging problem of preserving subject identity. We compare with
several benchmarks using two widely used datasets. We evaluate the quality and
realism of synthesised images using ground-truth longitudinal data and a
pre-trained age predictor. We show that, despite the use of cross-sectional
data, our model learns patterns of gray matter atrophy in the middle temporal
gyrus in patients with AD. To demonstrate generalisation ability, we train on
one dataset and evaluate predictions on the other. In conclusion, our model
shows an ability to separate age, disease influence and anatomy using only 2D
cross-sectional data that should should be useful in large studies into
neurodegenerative disease, that aim to combine several data sources. To
facilitate such future studies by the community at large our code is made
available at https://github.com/xiat0616/BrainAgeing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xia_T/0/1/0/all/0/1"&gt;Tian Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chartsias_A/0/1/0/all/0/1"&gt;Agisilaos Chartsias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengjia Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1"&gt;Sotirios A. Tsaftaris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vision Xformers: Efficient Attention for Image Classification. (arXiv:2107.02239v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02239</id>
        <link href="http://arxiv.org/abs/2107.02239"/>
        <updated>2021-08-03T02:06:31.748Z</updated>
        <summary type="html"><![CDATA[We propose three improvements to vision transformers (ViT) to reduce the
number of trainable parameters without compromising classification accuracy. We
address two shortcomings of the early ViT architectures -- quadratic bottleneck
of the attention mechanism and the lack of an inductive bias in their
architectures that rely on unrolling the two-dimensional image structure.
Linear attention mechanisms overcome the bottleneck of quadratic complexity,
which restricts application of transformer models in vision tasks. We modify
the ViT architecture to work on longer sequence data by replacing the quadratic
attention with efficient transformers, such as Performer, Linformer and
Nystr\"omformer of linear complexity creating Vision X-formers (ViX). We show
that all three versions of ViX may be more accurate than ViT for image
classification while using far fewer parameters and computational resources. We
also compare their performance with FNet and multi-layer perceptron (MLP)
mixer. We further show that replacing the initial linear embedding layer by
convolutional layers in ViX further increases their performance. Furthermore,
our tests on recent vision transformer models, such as LeViT, Convolutional
vision Transformer (CvT), Compact Convolutional Transformer (CCT) and
Pooling-based Vision Transformer (PiT) show that replacing the attention with
Nystr\"omformer or Performer saves GPU usage and memory without deteriorating
the classification accuracy. We also show that replacing the standard learnable
1D position embeddings in ViT with Rotary Position Embedding (RoPE) give
further improvements in accuracy. Incorporating these changes can democratize
transformers by making them accessible to those with limited data and computing
resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jeevan_P/0/1/0/all/0/1"&gt;Pranav Jeevan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sethi_A/0/1/0/all/0/1"&gt;Amit Sethi&lt;/a&gt; (Indian Institute of Technology Bombay)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learn to Match: Automatic Matching Network Design for Visual Tracking. (arXiv:2108.00803v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00803</id>
        <link href="http://arxiv.org/abs/2108.00803"/>
        <updated>2021-08-03T02:06:31.725Z</updated>
        <summary type="html"><![CDATA[Siamese tracking has achieved groundbreaking performance in recent years,
where the essence is the efficient matching operator cross-correlation and its
variants. Besides the remarkable success, it is important to note that the
heuristic matching network design relies heavily on expert experience.
Moreover, we experimentally find that one sole matching operator is difficult
to guarantee stable tracking in all challenging environments. Thus, in this
work, we introduce six novel matching operators from the perspective of feature
fusion instead of explicit similarity learning, namely Concatenation,
Pointwise-Addition, Pairwise-Relation, FiLM, Simple-Transformer and
Transductive-Guidance, to explore more feasibility on matching operator
selection. The analyses reveal these operators' selective adaptability on
different environment degradation types, which inspires us to combine them to
explore complementary features. To this end, we propose binary channel
manipulation (BCM) to search for the optimal combination of these operators.
BCM determines to retrain or discard one operator by learning its contribution
to other tracking steps. By inserting the learned matching networks to a strong
baseline tracker Ocean, our model achieves favorable gains by $67.2 \rightarrow
71.4$, $52.6 \rightarrow 58.3$, $70.3 \rightarrow 76.0$ success on OTB100,
LaSOT, and TrackingNet, respectively. Notably, Our tracker, dubbed AutoMatch,
uses less than half of training data/time than the baseline tracker, and runs
at 50 FPS using PyTorch. Code and model will be released at
https://github.com/JudasDie/SOTS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhipeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yihao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Weiming Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SHD360: A Benchmark Dataset for Salient Human Detection in 360{\deg} Videos. (arXiv:2105.11578v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11578</id>
        <link href="http://arxiv.org/abs/2105.11578"/>
        <updated>2021-08-03T02:06:31.712Z</updated>
        <summary type="html"><![CDATA[Salient human detection (SHD) in dynamic 360{\deg} immersive videos is of
great importance for various applications such as robotics, inter-human and
human-object interaction in augmented reality. However, 360{\deg} video SHD has
been seldom discussed in the computer vision community due to a lack of
datasets with large-scale omnidirectional videos and rich annotations. To this
end, we propose SHD360, the first 360{\deg} video SHD dataset which contains
various real-life daily scenes. Our SHD360 provides six-level hierarchical
annotations for 6,268 key frames uniformly sampled from 37,403 omnidirectional
video frames at 4K resolution. Specifically, each collected frame is labeled
with a super-class, a sub-class, associated attributes (e.g., geometrical
distortion), bounding boxes and per-pixel object-/instance-level masks. As a
result, our SHD360 contains totally 16,238 salient human instances with
manually annotated pixel-wise ground truth. Since so far there is no method
proposed for 360{\deg} image/video SHD, we systematically benchmark 11
representative state-of-the-art salient object detection (SOD) approaches on
our SHD360, and explore key issues derived from extensive experimenting
results. We hope our proposed dataset and benchmark could serve as a good
starting point for advancing human-centric researches towards 360{\deg}
panoramic data. Our dataset and benchmark will be publicly available at
https://github.com/PanoAsh/SHD360.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1"&gt;Wassim Hamidouche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deforges_O/0/1/0/all/0/1"&gt;Olivier Deforges&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reducing Effects of Swath Gaps on Unsupervised Machine Learning Models for NASA MODIS Instruments. (arXiv:2106.07113v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07113</id>
        <link href="http://arxiv.org/abs/2106.07113"/>
        <updated>2021-08-03T02:06:31.702Z</updated>
        <summary type="html"><![CDATA[Due to the nature of their pathways, NASA Terra and NASA Aqua satellites
capture imagery containing swath gaps, which are areas of no data. Swath gaps
can overlap the region of interest (ROI) completely, often rendering the entire
imagery unusable by Machine Learning (ML) models. This problem is further
exacerbated when the ROI rarely occurs (e.g. a hurricane) and, on occurrence,
is partially overlapped with a swath gap. With annotated data as supervision, a
model can learn to differentiate between the area of focus and the swath gap.
However, annotation is expensive and currently the vast majority of existing
data is unannotated. Hence, we propose an augmentation technique that
considerably removes the existence of swath gaps in order to allow CNNs to
focus on the ROI, and thus successfully use data with swath gaps for training.
We experiment on the UC Merced Land Use Dataset, where we add swath gaps
through empty polygons (up to 20 percent areas) and then apply augmentation
techniques to fill the swath gaps. We compare the model trained with our
augmentation techniques on the swath gap-filled data with the model trained on
the original swath gap-less data and note highly augmented performance.
Additionally, we perform a qualitative analysis using activation maps that
visualizes the effectiveness of our trained network in not paying attention to
the swath gaps. We also evaluate our results with a human baseline and show
that, in certain cases, the filled swath gaps look so realistic that even a
human evaluator did not distinguish between original satellite images and swath
gap-filled images. Since this method is aimed at unlabeled data, it is widely
generalizable and impactful for large scale unannotated datasets from various
space data domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Sarah Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_E/0/1/0/all/0/1"&gt;Esther Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1"&gt;Anirudh Koul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1"&gt;Siddha Ganju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Praveen_S/0/1/0/all/0/1"&gt;Satyarth Praveen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasam_M/0/1/0/all/0/1"&gt;Meher Anand Kasam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explain and Improve: LRP-Inference Fine-Tuning for Image Captioning Models. (arXiv:2001.01037v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.01037</id>
        <link href="http://arxiv.org/abs/2001.01037"/>
        <updated>2021-08-03T02:06:31.685Z</updated>
        <summary type="html"><![CDATA[This paper analyzes the predictions of image captioning models with attention
mechanisms beyond visualizing the attention itself. We develop variants of
layer-wise relevance propagation (LRP) and gradient-based explanation methods,
tailored to image captioning models with attention mechanisms. We compare the
interpretability of attention heatmaps systematically against the explanations
provided by explanation methods such as LRP, Grad-CAM, and Guided Grad-CAM. We
show that explanation methods provide simultaneously pixel-wise image
explanations (supporting and opposing pixels of the input image) and linguistic
explanations (supporting and opposing words of the preceding sequence) for each
word in the predicted captions. We demonstrate with extensive experiments that
explanation methods 1) can reveal additional evidence used by the model to make
decisions compared to attention; 2) correlate to object locations with high
precision; 3) are helpful to "debug" the model, e.g. by analyzing the reasons
for hallucinated object words. With the observed properties of explanations, we
further design an LRP-inference fine-tuning strategy that reduces the issue of
object hallucination in image captioning models, and meanwhile, maintains the
sentence fluency. We conduct experiments with two widely used attention
mechanisms: the adaptive attention mechanism calculated with the additive
attention and the multi-head attention mechanism calculated with the scaled dot
product.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jiamei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1"&gt;Sebastian Lapuschkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1"&gt;Wojciech Samek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Binder_A/0/1/0/all/0/1"&gt;Alexander Binder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Channel-wise Knowledge Distillation for Dense Prediction. (arXiv:2011.13256v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.13256</id>
        <link href="http://arxiv.org/abs/2011.13256"/>
        <updated>2021-08-03T02:06:31.670Z</updated>
        <summary type="html"><![CDATA[Knowledge distillation (KD) has been proven to be a simple and effective tool
for training compact models. Almost all KD variants for dense prediction tasks
align the student and teacher networks' feature maps in the spatial domain,
typically by minimizing point-wise and/or pair-wise discrepancy. Observing that
in semantic segmentation, some layers' feature activations of each channel tend
to encode saliency of scene categories (analogue to class activation mapping),
we propose to align features channel-wise between the student and teacher
networks. To this end, we first transform the feature map of each channel into
a probabilty map using softmax normalization, and then minimize the
Kullback-Leibler (KL) divergence of the corresponding channels of the two
networks. By doing so, our method focuses on mimicking the soft distributions
of channels between networks. In particular, the KL divergence enables learning
to pay more attention to the most salient regions of the channel-wise maps,
presumably corresponding to the most useful signals for semantic segmentation.
Experiments demonstrate that our channel-wise distillation outperforms almost
all existing spatial distillation methods for semantic segmentation
considerably, and requires less computational cost during training. We
consistently achieve superior performance on three benchmarks with various
network structures. Code is available at: https://git.io/ChannelDis]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shu_C/0/1/0/all/0/1"&gt;Changyong Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yifan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfei Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chunhua Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Filtering in tractography using autoencoders (FINTA). (arXiv:2010.04007v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04007</id>
        <link href="http://arxiv.org/abs/2010.04007"/>
        <updated>2021-08-03T02:06:31.662Z</updated>
        <summary type="html"><![CDATA[Current brain white matter fiber tracking techniques show a number of
problems, including: generating large proportions of streamlines that do not
accurately describe the underlying anatomy; extracting streamlines that are not
supported by the underlying diffusion signal; and under-representing some fiber
populations, among others. In this paper, we describe a novel autoencoder-based
learning method to filter streamlines from diffusion MRI tractography, and
hence, to obtain more reliable tractograms. Our method, dubbed FINTA (Filtering
in Tractography using Autoencoders) uses raw, unlabeled tractograms to train
the autoencoder, and to learn a robust representation of brain streamlines.
Such an embedding is then used to filter undesired streamline samples using a
nearest neighbor algorithm. Our experiments on both synthetic and in vivo human
brain diffusion MRI tractography data obtain accuracy scores exceeding the 90\%
threshold on the test set. Results reveal that FINTA has a superior filtering
performance compared to conventional, anatomy-based methods, and the
RecoBundles state-of-the-art method. Additionally, we demonstrate that FINTA
can be applied to partial tractograms without requiring changes to the
framework. We also show that the proposed method generalizes well across
different tracking methods and datasets, and shortens significantly the
computation time for large (>1 M streamlines) tractograms. Together, this work
brings forward a new deep learning framework in tractography based on
autoencoders, which offers a flexible and powerful method for white matter
filtering and bundling that could enhance tractometry and connectivity
analyses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Legarreta_J/0/1/0/all/0/1"&gt;Jon Haitz Legarreta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Petit_L/0/1/0/all/0/1"&gt;Laurent Petit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rheault_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Rheault&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Theaud_G/0/1/0/all/0/1"&gt;Guillaume Theaud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lemaire_C/0/1/0/all/0/1"&gt;Carl Lemaire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Descoteaux_M/0/1/0/all/0/1"&gt;Maxime Descoteaux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jodoin_P/0/1/0/all/0/1"&gt;Pierre-Marc Jodoin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual approach for object tracking based on optical flow and swarm intelligence. (arXiv:1808.08186v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1808.08186</id>
        <link href="http://arxiv.org/abs/1808.08186"/>
        <updated>2021-08-03T02:06:31.636Z</updated>
        <summary type="html"><![CDATA[In Computer Vision,object tracking is a very old and complex problem.Though
there are several existing algorithms for object tracking, still there are
several challenges remain to be solved. For instance, variation of illumination
of light, noise, occlusion, sudden start and stop of moving object, shading
etc,make the object tracking a complex problem not only for dynamic background
but also for static background. In this paper we propose a dual approach for
object tracking based on optical flow and swarm Intelligence.The optical flow
based KLT(Kanade-Lucas-Tomasi) tracker, tracks the dominant points of the
target object from first frame to last frame of a video sequence;whereas swarm
Intelligence based PSO (Particle Swarm Optimization) tracker simultaneously
tracks the boundary information of the target object from second frame to last
frame of the same video sequence.This dual function of tracking makes the
trackers very much robust with respect to the above stated problems. The
flexibility of our approach is that it can be successfully applicable in
variable background as well as static background.We compare the performance of
the proposed dual tracking algorithm with several benchmark datasets and obtain
very competitive results in general and in most of the cases we obtained
superior results using dual tracking algorithm. We also compare the performance
of the proposed dual tracker with some existing PSO based algorithms for
tracking and achieved better results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Misra_R/0/1/0/all/0/1"&gt;Rajesh Misra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ray_K/0/1/0/all/0/1"&gt;Kumar S. Ray&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training face verification models from generated face identity data. (arXiv:2108.00800v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00800</id>
        <link href="http://arxiv.org/abs/2108.00800"/>
        <updated>2021-08-03T02:06:31.629Z</updated>
        <summary type="html"><![CDATA[Machine learning tools are becoming increasingly powerful and widely used.
Unfortunately membership attacks, which seek to uncover information from data
sets used in machine learning, have the potential to limit data sharing. In
this paper we consider an approach to increase the privacy protection of data
sets, as applied to face recognition. Using an auxiliary face recognition
model, we build on the StyleGAN generative adversarial network and feed it with
latent codes combining two distinct sub-codes, one encoding visual identity
factors, and, the other, non-identity factors. By independently varying these
vectors during image generation, we create a synthetic data set of fictitious
face identities. We use this data set to train a face recognition model. The
model performance degrades in comparison to the state-of-the-art of face
verification. When tested with a simple membership attack our model provides
good privacy protection, however the model performance degrades in comparison
to the state-of-the-art of face verification. We find that the addition of a
small amount of private data greatly improves the performance of our model,
which highlights the limitations of using synthetic data to train machine
learning models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Conway_D/0/1/0/all/0/1"&gt;Dennis Conway&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simon_L/0/1/0/all/0/1"&gt;Loic Simon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lechervy_A/0/1/0/all/0/1"&gt;Alexis Lechervy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jurie_F/0/1/0/all/0/1"&gt;Frederic Jurie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient and Robust Registration on the 3D Special Euclidean Group. (arXiv:1904.05519v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.05519</id>
        <link href="http://arxiv.org/abs/1904.05519"/>
        <updated>2021-08-03T02:06:31.613Z</updated>
        <summary type="html"><![CDATA[We present an accurate, robust and fast method for registration of 3D scans.
Our motion estimation optimizes a robust cost function on the intrinsic
representation of rigid motions, i.e., the Special Euclidean group
$\mathbb{SE}(3)$. We exploit the geometric properties of Lie groups as well as
the robustness afforded by an iteratively reweighted least squares
optimization. We also generalize our approach to a joint multiview method that
simultaneously solves for the registration of a set of scans. We demonstrate
the efficacy of our approach by thorough experimental validation. Our approach
significantly outperforms the state-of-the-art robust 3D registration method
based on a line process in terms of both speed and accuracy. We also show that
this line process method is a special case of our principled geometric
solution. Finally, we also present scenarios where global registration based on
feature correspondences fails but multiview ICP based on our robust motion
estimation is successful.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1"&gt;Uttaran Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Govindu_V/0/1/0/all/0/1"&gt;Venu Madhav Govindu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Representative elementary volume via averaged scalar Minkowski functionals. (arXiv:2008.03727v2 [physics.comp-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.03727</id>
        <link href="http://arxiv.org/abs/2008.03727"/>
        <updated>2021-08-03T02:06:31.602Z</updated>
        <summary type="html"><![CDATA[Representative Elementary Volume (REV) at which the material properties do
not vary with change in volume is an important quantity for making measurements
or simulations which represent the whole. We discuss the geometrical method to
evaluation of REV based on the quantities coming in the Steiner formula from
convex geometry. For bodies in the three-space this formula gives us four
scalar functionals known as scalar Minkowski functionals. We demonstrate on
certain samples that the values of such averaged functionals almost stabilize
for cells for which the length of edges are greater than certain threshold
value R. Therefore, from this point of view, it is reasonable to consider cubes
of volume R^3 as representative elementary volumes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Andreeva_M/0/1/0/all/0/1"&gt;M.V. Andreeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kalyuzhnyuk_A/0/1/0/all/0/1"&gt;A.V. Kalyuzhnyuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Krutko_V/0/1/0/all/0/1"&gt;V.V. Krutko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Russkikh_N/0/1/0/all/0/1"&gt;N.E. Russkikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Taimanov_I/0/1/0/all/0/1"&gt;I.A. Taimanov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BezierSeg: Parametric Shape Representation for Fast Object Segmentation in Medical Images. (arXiv:2108.00760v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00760</id>
        <link href="http://arxiv.org/abs/2108.00760"/>
        <updated>2021-08-03T02:06:31.596Z</updated>
        <summary type="html"><![CDATA[Delineating the lesion area is an important task in image-based diagnosis.
Pixel-wise classification is a popular approach to segmenting the region of
interest. However, at fuzzy boundaries such methods usually result in glitches,
discontinuity, or disconnection, inconsistent with the fact that lesions are
solid and smooth. To overcome these undesirable artifacts, we propose the
BezierSeg model which outputs bezier curves encompassing the region of
interest. Directly modelling the contour with analytic equations ensures that
the segmentation is connected, continuous, and the boundary is smooth. In
addition, it offers sub-pixel accuracy. Without loss of accuracy, the bezier
contour can be resampled and overlaid with images of any resolution. Moreover,
a doctor can conveniently adjust the curve's control points to refine the
result. Our experiments show that the proposed method runs in real time and
achieves accuracy competitive with pixel-wise segmentation models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haichou Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yishu Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zeqin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haohua Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jing_B/0/1/0/all/0/1"&gt;Bingzhong Jing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chaofeng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-scale super-resolution generation of low-resolution scanned pathological images. (arXiv:2105.07200v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07200</id>
        <link href="http://arxiv.org/abs/2105.07200"/>
        <updated>2021-08-03T02:06:31.477Z</updated>
        <summary type="html"><![CDATA[Background. Digital pathology has aroused widespread interest in modern
pathology. The key of digitalization is to scan the whole slide image (WSI) at
high magnification. The lager the magnification is, the richer details WSI will
provide, but the scanning time is longer and the file size of obtained is
larger. Methods. We design a strategy to scan slides with low resolution (5X)
and a super-resolution method is proposed to restore the image details when in
diagnosis. The method is based on a multi-scale generative adversarial network,
which sequentially generates three high-resolution images such as 10X, 20X and
40X. Results. The peak-signal-to-noise-ratio of 10X to 40X generated images are
24.16, 22.27 and 20.44, and the structural-similarity-index are 0.845, 0.680
and 0.512, which are better than other super-resolution networks. Visual
scoring average and standard deviation from three pathologists is 3.63
plus-minus 0.52, 3.70 plus-minus 0.57 and 3.74 plus-minus 0.56 and the p value
of analysis of variance is 0.37, indicating that generated images include
sufficient information for diagnosis. The average value of Kappa test is 0.99,
meaning the diagnosis of generated images is highly consistent with that of the
real images. Conclusion. This proposed method can generate high-quality 10X,
20X, 40X images from 5X images at the same time, in which the time and storage
costs of digitalization can be effectively reduced up to 1/64 of the previous
costs. The proposed method provides a better alternative for low-cost storage,
faster image share of digital pathology. Keywords. Digital pathology;
Super-resolution; Low resolution scanning; Low cost]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sun_K/0/1/0/all/0/1"&gt;Kai Sun&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yanhua Gao&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/eess/1/au:+Xie_T/0/1/0/all/0/1"&gt;Ting Xie&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xun Wang&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qingqing Yang&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1"&gt;Le Chen&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kuansong Wang&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/eess/1/au:+Yu_G/0/1/0/all/0/1"&gt;Gang Yu&lt;/a&gt; (1) ((1) Department of Biomedical Engineering, School of Basic Medical Sciences, Central South University, 172 Tongzipo Road, Changsha, 410013, China. (2) Department of Ultrasound, Shaanxi Provincial People&amp;#x27;s Hospital,256 Youyixi Road, Xi&amp;#x27;an, 710068, China. (3) Department of Pathology, School of Basic Medical Sciences, Central South University, 172 Tongzipo Road, Changsha, 410013, China.)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Height Estimation of Children under Five Years using Depth Images. (arXiv:2105.01688v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01688</id>
        <link href="http://arxiv.org/abs/2105.01688"/>
        <updated>2021-08-03T02:06:31.441Z</updated>
        <summary type="html"><![CDATA[Malnutrition is a global health crisis and is the leading cause of death
among children under five. Detecting malnutrition requires anthropometric
measurements of weight, height, and middle-upper arm circumference. However,
measuring them accurately is a challenge, especially in the global south, due
to limited resources. In this work, we propose a CNN-based approach to estimate
the height of standing children under five years from depth images collected
using a smart-phone. According to the SMART Methodology Manual [5], the
acceptable accuracy for height is less than 1.4 cm. On training our deep
learning model on 87131 depth images, our model achieved an average mean
absolute error of 1.64% on 57064 test images. For 70.3% test images, we
estimated height accurately within the acceptable 1.4 cm range. Thus, our
proposed solution can accurately detect stunting (low height-for-age) in
standing children below five years of age.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1"&gt;Anusua Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1"&gt;Mohit Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1"&gt;Nikhil Kumar Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hinsche_M/0/1/0/all/0/1"&gt;Markus Hinsche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1"&gt;Prashant Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matiaschek_M/0/1/0/all/0/1"&gt;Markus Matiaschek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Behrens_T/0/1/0/all/0/1"&gt;Tristan Behrens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Militeri_M/0/1/0/all/0/1"&gt;Mirco Militeri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Birge_C/0/1/0/all/0/1"&gt;Cameron Birge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaushik_S/0/1/0/all/0/1"&gt;Shivangi Kaushik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohapatra_A/0/1/0/all/0/1"&gt;Archisman Mohapatra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatterjee_R/0/1/0/all/0/1"&gt;Rita Chatterjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dodhia_R/0/1/0/all/0/1"&gt;Rahul Dodhia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferres_J/0/1/0/all/0/1"&gt;Juan Lavista Ferres&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixing-AdaSIN: Constructing a De-biased Dataset using Adaptive Structural Instance Normalization and Texture Mixing. (arXiv:2103.14255v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14255</id>
        <link href="http://arxiv.org/abs/2103.14255"/>
        <updated>2021-08-03T02:06:31.434Z</updated>
        <summary type="html"><![CDATA[Following the pandemic outbreak, several works have proposed to diagnose
COVID-19 with deep learning in computed tomography (CT); reporting performance
on-par with experts. However, models trained/tested on the same in-distribution
data may rely on the inherent data biases for successful prediction, failing to
generalize on out-of-distribution samples or CT with different scanning
protocols. Early attempts have partly addressed bias-mitigation and
generalization through augmentation or re-sampling, but are still limited by
collection costs and the difficulty of quantifying bias in medical images. In
this work, we propose Mixing-AdaSIN; a bias mitigation method that uses a
generative model to generate de-biased images by mixing texture information
between different labeled CT scans with semantically similar features. Here, we
use Adaptive Structural Instance Normalization (AdaSIN) to enhance de-biasing
generation quality and guarantee structural consistency. Following, a
classifier trained with the generated images learns to correctly predict the
label without bias and generalizes better. To demonstrate the efficacy of our
method, we construct a biased COVID-19 vs. bacterial pneumonia dataset based on
CT protocols and compare with existing state-of-the-art de-biasing methods. Our
experiments show that classifiers trained with de-biased generated images
report improved in-distribution performance and generalization on an external
COVID-19 dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kang_M/0/1/0/all/0/1"&gt;Myeongkyun Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chikontwe_P/0/1/0/all/0/1"&gt;Philip Chikontwe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Luna_M/0/1/0/all/0/1"&gt;Miguel Luna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hong_K/0/1/0/all/0/1"&gt;Kyung Soo Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ahn_J/0/1/0/all/0/1"&gt;June Hong Ahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1"&gt;Sang Hyun Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MSMatch: Semi-Supervised Multispectral Scene Classification with Few Labels. (arXiv:2103.10368v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10368</id>
        <link href="http://arxiv.org/abs/2103.10368"/>
        <updated>2021-08-03T02:06:31.416Z</updated>
        <summary type="html"><![CDATA[Supervised learning techniques are at the center of many tasks in remote
sensing. Unfortunately, these methods, especially recent deep learning methods,
often require large amounts of labeled data for training. Even though
satellites acquire large amounts of data, labeling the data is often tedious,
expensive and requires expert knowledge. Hence, improved methods that require
fewer labeled samples are needed. We present MSMatch, the first semi-supervised
learning approach competitive with supervised methods on scene classification
on the EuroSAT and UC Merced Land Use benchmark datasets. We test both RGB and
multispectral images of EuroSAT and perform various ablation studies to
identify the critical parts of the model. The trained neural network achieves
state-of-the-art results on EuroSAT with an accuracy that is up to 19.76%
better than previous methods depending on the number of labeled training
examples. With just five labeled examples per class, we reach 94.53% and 95.86%
accuracy on the EuroSAT RGB and multispectral datasets, respectively. On the UC
Merced Land Use dataset, we outperform previous works by up to 5.59% and reach
90.71% with five labeled examples. Our results show that MSMatch is capable of
greatly reducing the requirements for labeled data. It translates well to
multispectral data and should enable various applications that are currently
infeasible due to a lack of labeled data. We provide the source code of MSMatch
online to enable easy reproduction and quick adoption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_P/0/1/0/all/0/1"&gt;Pablo G&amp;#xf3;mez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meoni_G/0/1/0/all/0/1"&gt;Gabriele Meoni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bootstrapped Self-Supervised Training with Monocular Video for Semantic Segmentation and Depth Estimation. (arXiv:2103.11031v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11031</id>
        <link href="http://arxiv.org/abs/2103.11031"/>
        <updated>2021-08-03T02:06:31.403Z</updated>
        <summary type="html"><![CDATA[For a robot deployed in the world, it is desirable to have the ability of
autonomous learning to improve its initial pre-set knowledge. We formalize this
as a bootstrapped self-supervised learning problem where a system is initially
bootstrapped with supervised training on a labeled dataset and we look for a
self-supervised training method that can subsequently improve the system over
the supervised training baseline using only unlabeled data. In this work, we
leverage temporal consistency between frames in monocular video to perform this
bootstrapped self-supervised training. We show that a well-trained
state-of-the-art semantic segmentation network can be further improved through
our method. In addition, we show that the bootstrapped self-supervised training
framework can help a network learn depth estimation better than pure supervised
training or self-supervised training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yihao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leonard_J/0/1/0/all/0/1"&gt;John J. Leonard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Continual Learning Via Pseudo Labels. (arXiv:2104.07164v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07164</id>
        <link href="http://arxiv.org/abs/2104.07164"/>
        <updated>2021-08-03T02:06:31.397Z</updated>
        <summary type="html"><![CDATA[Continual learning aims to learn new tasks incrementally using less
computation and memory resources instead of retraining the model from scratch
whenever new task arrives. However, existing approaches are designed in
supervised fashion assuming all data from new tasks have been manually
annotated, which are not practical for many real-life applications. In this
work, we propose to use pseudo label instead of the ground truth to make
continual learning feasible in unsupervised mode. The pseudo labels of new data
are obtained by applying global clustering algorithm and we propose to use the
model updated from last incremental step as the feature extractor. Due to the
scarcity of existing work, we introduce a new benchmark experimental protocol
for unsupervised continual learning of image classification task under
class-incremental setting where no class label is provided for each incremental
learning step. Our method is evaluated on the CIFAR-100 and ImageNet (ILSVRC)
datasets by incorporating the pseudo label with various existing supervised
approaches and show promising results in unsupervised scenario.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jiangpeng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1"&gt;Fengqing Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Weakly-supervised Object Localization via Causal Intervention. (arXiv:2104.10351v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10351</id>
        <link href="http://arxiv.org/abs/2104.10351"/>
        <updated>2021-08-03T02:06:31.375Z</updated>
        <summary type="html"><![CDATA[The recent emerged weakly supervised object localization (WSOL) methods can
learn to localize an object in the image only using image-level labels.
Previous works endeavor to perceive the interval objects from the small and
sparse discriminative attention map, yet ignoring the co-occurrence confounder
(e.g., bird and sky), which makes the model inspection (e.g., CAM) hard to
distinguish between the object and context. In this paper, we make an early
attempt to tackle this challenge via causal intervention (CI). Our proposed
method, dubbed CI-CAM, explores the causalities among images, contexts, and
categories to eliminate the biased co-occurrence in the class activation maps
thus improving the accuracy of object localization. Extensive experiments on
several benchmarks demonstrate the effectiveness of CI-CAM in learning the
clear object boundaries from confounding contexts. Particularly, in
CUB-200-2011 which severely suffers from the co-occurrence confounder, CI-CAM
significantly outperforms the traditional CAM-based baseline (58.39% vs 52.4%
in top-1 localization accuracy). While in more general scenarios such as
ImageNet, CI-CAM can also perform on par with the state of the arts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shao_F/0/1/0/all/0/1"&gt;Feifei Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yawei Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Li Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_L/0/1/0/all/0/1"&gt;Lu Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1"&gt;Siliang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1"&gt;Jun Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generic Event Boundary Detection: A Benchmark for Event Segmentation. (arXiv:2101.10511v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10511</id>
        <link href="http://arxiv.org/abs/2101.10511"/>
        <updated>2021-08-03T02:06:31.367Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel task together with a new benchmark for detecting
generic, taxonomy-free event boundaries that segment a whole video into chunks.
Conventional work in temporal video segmentation and action detection focuses
on localizing pre-defined action categories and thus does not scale to generic
videos. Cognitive Science has known since last century that humans consistently
segment videos into meaningful temporal chunks. This segmentation happens
naturally, without pre-defined event categories and without being explicitly
asked to do so. Here, we repeat these cognitive experiments on mainstream CV
datasets; with our novel annotation guideline which addresses the complexities
of taxonomy-free event boundary annotation, we introduce the task of Generic
Event Boundary Detection (GEBD) and the new benchmark Kinetics-GEBD. Our
Kinetics-GEBD has the largest number of boundaries (e.g. 32 of ActivityNet, 8
of EPIC-Kitchens-100) which are in-the-wild, taxonomy-free, cover generic event
change, and respect human perception diversity. We view GEBD as an important
stepping stone towards understanding the video as a whole, and believe it has
been previously neglected due to a lack of proper task definition and
annotations. Through experiment and human study we demonstrate the value of the
annotations. Further, we benchmark supervised and un-supervised GEBD approaches
on the TAPOS dataset and our Kinetics-GEBD, together with method design
explorations that suggest future directions. We release our annotations and
baseline codes at CVPR'21 LOVEU Challenge:
https://sites.google.com/view/loveucvpr21.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1"&gt;Mike Zheng Shou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1"&gt;Stan Weixian Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weiyao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghadiyaram_D/0/1/0/all/0/1"&gt;Deepti Ghadiyaram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feiszli_M/0/1/0/all/0/1"&gt;Matt Feiszli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast and High-Quality Blind Multi-Spectral Image Pansharpening. (arXiv:2103.09943v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09943</id>
        <link href="http://arxiv.org/abs/2103.09943"/>
        <updated>2021-08-03T02:06:31.361Z</updated>
        <summary type="html"><![CDATA[Blind pansharpening addresses the problem of generating a high
spatial-resolution multi-spectral (HRMS) image given a low spatial-resolution
multi-spectral (LRMS) image with the guidance of its associated spatially
misaligned high spatial-resolution panchromatic (PAN) image without parametric
side information. In this paper, we propose a fast approach to blind
pansharpening and achieve state-of-the-art image reconstruction quality.
Typical blind pansharpening algorithms are often computationally intensive
since the blur kernel and the target HRMS image are often computed using
iterative solvers and in an alternating fashion. To achieve fast blind
pansharpening, we decouple the solution of the blur kernel and of the HRMS
image. First, we estimate the blur kernel by computing the kernel coefficients
with minimum total generalized variation that blur a downsampled version of the
PAN image to approximate a linear combination of the LRMS image channels. Then,
we estimate each channel of the HRMS image using local Laplacian prior to
regularize the relationship between each HRMS channel and the PAN image.
Solving the HRMS image is accelerated by both parallelizing across the channels
and by fast numerical algorithms for each channel. Due to the fast scheme and
the powerful priors we used on the blur kernel coefficients (total generalized
variation) and on the cross-channel relationship (local Laplacian prior),
numerical experiments demonstrate that our algorithm outperforms
state-of-the-art model-based counterparts in terms of both computational time
and reconstruction quality of the HRMS images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1"&gt;Lantao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Dehong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansour_H/0/1/0/all/0/1"&gt;Hassan Mansour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boufounos_P/0/1/0/all/0/1"&gt;Petros T. Boufounos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthesis of Compositional Animations from Textual Descriptions. (arXiv:2103.14675v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14675</id>
        <link href="http://arxiv.org/abs/2103.14675"/>
        <updated>2021-08-03T02:06:31.350Z</updated>
        <summary type="html"><![CDATA["How can we animate 3D-characters from a movie script or move robots by
simply telling them what we would like them to do?" "How unstructured and
complex can we make a sentence and still generate plausible movements from it?"
These are questions that need to be answered in the long-run, as the field is
still in its infancy. Inspired by these problems, we present a new technique
for generating compositional actions, which handles complex input sentences.
Our output is a 3D pose sequence depicting the actions in the input sentence.
We propose a hierarchical two-stream sequential model to explore a finer
joint-level mapping between natural language sentences and 3D pose sequences
corresponding to the given motion. We learn two manifold representations of the
motion -- one each for the upper body and the lower body movements. Our model
can generate plausible pose sequences for short sentences describing single
actions as well as long compositional sentences describing multiple sequential
and superimposed actions. We evaluate our proposed model on the publicly
available KIT Motion-Language Dataset containing 3D pose data with
human-annotated sentences. Experimental results show that our model advances
the state-of-the-art on text-based motion synthesis in objective evaluations by
a margin of 50%. Qualitative evaluations based on a user study indicate that
our synthesized motions are perceived to be the closest to the ground-truth
motion captures for both short and compositional sentences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1"&gt;Anindita Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheema_N/0/1/0/all/0/1"&gt;Noshaba Cheema&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oguz_C/0/1/0/all/0/1"&gt;Cennet Oguz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slusallek_P/0/1/0/all/0/1"&gt;Philipp Slusallek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning with robustness to missing data: A novel approach to the detection of COVID-19. (arXiv:2103.13833v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13833</id>
        <link href="http://arxiv.org/abs/2103.13833"/>
        <updated>2021-08-03T02:06:31.341Z</updated>
        <summary type="html"><![CDATA[In the context of the current global pandemic and the limitations of the
RT-PCR test, we propose a novel deep learning architecture, DFCN (Denoising
Fully Connected Network). Since medical facilities around the world differ
enormously in what laboratory tests or chest imaging may be available, DFCN is
designed to be robust to missing input data. An ablation study extensively
evaluates the performance benefits of the DFCN as well as its robustness to
missing inputs. Data from 1088 patients with confirmed RT-PCR results are
obtained from two independent medical facilities. The data includes results
from 27 laboratory tests and a chest x-ray scored by a deep learning model.
Training and test datasets are taken from different medical facilities. Data is
made publicly available. The performance of DFCN in predicting the RT-PCR
result is compared with 3 related architectures as well as a Random Forest
baseline. All models are trained with varying levels of masked input data to
encourage robustness to missing inputs. Missing data is simulated at test time
by masking inputs randomly. DFCN outperforms all other models with statistical
significance using random subsets of input data with 2-27 available inputs.
When all 28 inputs are available DFCN obtains an AUC of 0.924, higher than any
other model. Furthermore, with clinically meaningful subsets of parameters
consisting of just 6 and 7 inputs respectively, DFCN achieves higher AUCs than
any other model, with values of 0.909 and 0.919.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Calli_E/0/1/0/all/0/1"&gt;Erdi &amp;#xc7;all&amp;#x131;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Murphy_K/0/1/0/all/0/1"&gt;Keelin Murphy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kurstjens_S/0/1/0/all/0/1"&gt;Steef Kurstjens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Samson_T/0/1/0/all/0/1"&gt;Tijs Samson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Herpers_R/0/1/0/all/0/1"&gt;Robert Herpers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Smits_H/0/1/0/all/0/1"&gt;Henk Smits&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rutten_M/0/1/0/all/0/1"&gt;Matthieu Rutten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ginneken_B/0/1/0/all/0/1"&gt;Bram van Ginneken&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Simple Baseline for Semi-supervised Semantic Segmentation with Strong Data Augmentation. (arXiv:2104.07256v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07256</id>
        <link href="http://arxiv.org/abs/2104.07256"/>
        <updated>2021-08-03T02:06:31.305Z</updated>
        <summary type="html"><![CDATA[Recently, significant progress has been made on semantic segmentation.
However, the success of supervised semantic segmentation typically relies on a
large amount of labelled data, which is time-consuming and costly to obtain.
Inspired by the success of semi-supervised learning methods in image
classification, here we propose a simple yet effective semi-supervised learning
framework for semantic segmentation. We demonstrate that the devil is in the
details: a set of simple design and training techniques can collectively
improve the performance of semi-supervised semantic segmentation significantly.
Previous works [3, 27] fail to employ strong augmentation in pseudo label
learning efficiently, as the large distribution change caused by strong
augmentation harms the batch normalisation statistics. We design a new batch
normalisation, namely distribution-specific batch normalisation (DSBN) to
address this problem and demonstrate the importance of strong augmentation for
semantic segmentation. Moreover, we design a self correction loss which is
effective in noise resistance. We conduct a series of ablation studies to show
the effectiveness of each component. Our method achieves state-of-the-art
results in the semi-supervised settings on the Cityscapes and Pascal VOC
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Jianlong Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yifan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chunhua Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhibin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised and Unregistered Hyperspectral Image Super-Resolution with Mutual Dirichlet-Net. (arXiv:1904.12175v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.12175</id>
        <link href="http://arxiv.org/abs/1904.12175"/>
        <updated>2021-08-03T02:06:31.292Z</updated>
        <summary type="html"><![CDATA[Hyperspectral images (HSI) provide rich spectral information that contributed
to the successful performance improvement of numerous computer vision tasks.
However, it can only be achieved at the expense of images' spatial resolution.
Hyperspectral image super-resolution (HSI-SR) addresses this problem by fusing
low resolution (LR) HSI with multispectral image (MSI) carrying much higher
spatial resolution (HR). All existing HSI-SR approaches require the LR HSI and
HR MSI to be well registered and the reconstruction accuracy of the HR HSI
relies heavily on the registration accuracy of different modalities. This paper
exploits the uncharted problem domain of HSI-SR without the requirement of
multi-modality registration. Given the unregistered LR HSI and HR MSI with
overlapped regions, we design a unique unsupervised learning structure linking
the two unregistered modalities by projecting them into the same statistical
space through the same encoder. The mutual information (MI) is further adopted
to capture the non-linear statistical dependencies between the representations
from two modalities (carrying spatial information) and their raw inputs. By
maximizing the MI, spatial correlations between different modalities can be
well characterized to further reduce the spectral distortion. A collaborative
$l_{2,1}$ norm is employed as the reconstruction error instead of the more
common $l_2$ norm, so that individual pixels can be recovered as accurately as
possible. With this design, the network allows to extract correlated spectral
and spatial information from unregistered images that better preserves the
spectral information. The proposed method is referred to as unregistered and
unsupervised mutual Dirichlet Net ($u^2$-MDN). Extensive experimental results
using benchmark HSI datasets demonstrate the superior performance of $u^2$-MDN
as compared to the state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1"&gt;Ying Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1"&gt;Hairong Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwan_C/0/1/0/all/0/1"&gt;Chiman Kwan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yokoya_N/0/1/0/all/0/1"&gt;Naoto Yokoya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1"&gt;Jocelyn Chanussot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Angle Based Feature Learning in GNN for 3D Object Detection using Point Cloud. (arXiv:2108.00780v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00780</id>
        <link href="http://arxiv.org/abs/2108.00780"/>
        <updated>2021-08-03T02:06:31.285Z</updated>
        <summary type="html"><![CDATA[In this paper, we present new feature encoding methods for Detection of 3D
objects in point clouds. We used a graph neural network (GNN) for Detection of
3D objects namely cars, pedestrians, and cyclists. Feature encoding is one of
the important steps in Detection of 3D objects. The dataset used is point cloud
data which is irregular and unstructured and it needs to be encoded in such a
way that ensures better feature encapsulation. Earlier works have used relative
distance as one of the methods to encode the features. These methods are not
resistant to rotation variance problems in Graph Neural Networks. We have
included angular-based measures while performing feature encoding in graph
neural networks. Along with that, we have performed a comparison between other
methods like Absolute, Relative, Euclidean distances, and a combination of the
Angle and Relative methods. The model is trained and evaluated on the subset of
the KITTI object detection benchmark dataset under resource constraints. Our
results demonstrate that a combination of angle measures and relative distance
has performed better than other methods. In comparison to the baseline
method(relative), it achieved better performance. We also performed time
analysis of various feature encoding methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ansari_M/0/1/0/all/0/1"&gt;Md Afzal Ansari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meraz_M/0/1/0/all/0/1"&gt;Md Meraz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1"&gt;Pavan Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javed_M/0/1/0/all/0/1"&gt;Mohammed Javed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flip Learning: Erase to Segment. (arXiv:2108.00752v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00752</id>
        <link href="http://arxiv.org/abs/2108.00752"/>
        <updated>2021-08-03T02:06:31.274Z</updated>
        <summary type="html"><![CDATA[Nodule segmentation from breast ultrasound images is challenging yet
essential for the diagnosis. Weakly-supervised segmentation (WSS) can help
reduce time-consuming and cumbersome manual annotation. Unlike existing
weakly-supervised approaches, in this study, we propose a novel and general WSS
framework called Flip Learning, which only needs the box annotation.
Specifically, the target in the label box will be erased gradually to flip the
classification tag, and the erased region will be considered as the
segmentation result finally. Our contribution is three-fold. First, our
proposed approach erases on superpixel level using a Multi-agent Reinforcement
Learning framework to exploit the prior boundary knowledge and accelerate the
learning process. Second, we design two rewards: classification score and
intensity distribution reward, to avoid under- and over-segmentation,
respectively. Third, we adopt a coarse-to-fine learning strategy to reduce the
residual errors and improve the segmentation performance. Extensively validated
on a large dataset, our proposed approach achieves competitive performance and
shows great potential to narrow the gap between fully-supervised and
weakly-supervised learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yuhao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuxin Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chaoyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_H/0/1/0/all/0/1"&gt;Haoran Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravikumar_N/0/1/0/all/0/1"&gt;Nishant Ravikumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1"&gt;Alejandro F Frangi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jianqiao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1"&gt;Dong Ni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs. (arXiv:2103.13744v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13744</id>
        <link href="http://arxiv.org/abs/2103.13744"/>
        <updated>2021-08-03T02:06:31.265Z</updated>
        <summary type="html"><![CDATA[NeRF synthesizes novel views of a scene with unprecedented quality by fitting
a neural radiance field to RGB images. However, NeRF requires querying a deep
Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering
times, even on modern GPUs. In this paper, we demonstrate that real-time
rendering is possible by utilizing thousands of tiny MLPs instead of one single
large MLP. In our setting, each individual MLP only needs to represent parts of
the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining
this divide-and-conquer strategy with further optimizations, rendering is
accelerated by three orders of magnitude compared to the original NeRF model
without incurring high storage costs. Further, using teacher-student
distillation for training, we show that this speed-up can be achieved without
sacrificing visual quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reiser_C/0/1/0/all/0/1"&gt;Christian Reiser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1"&gt;Songyou Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1"&gt;Yiyi Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1"&gt;Andreas Geiger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Robust Object Detection: Bayesian RetinaNet for Homoscedastic Aleatoric Uncertainty Modeling. (arXiv:2108.00784v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00784</id>
        <link href="http://arxiv.org/abs/2108.00784"/>
        <updated>2021-08-03T02:06:31.244Z</updated>
        <summary type="html"><![CDATA[According to recent studies, commonly used computer vision datasets contain
about 4% of label errors. For example, the COCO dataset is known for its high
level of noise in data labels, which limits its use for training robust neural
deep architectures in a real-world scenario. To model such a noise, in this
paper we have proposed the homoscedastic aleatoric uncertainty estimation, and
present a series of novel loss functions to address the problem of image object
detection at scale. Specifically, the proposed functions are based on Bayesian
inference and we have incorporated them into the common community-adopted
object detection deep learning architecture RetinaNet. We have also shown that
modeling of homoscedastic aleatoric uncertainty using our novel functions
allows to increase the model interpretability and to improve the object
detection performance being evaluated on the COCO dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khanzhina_N/0/1/0/all/0/1"&gt;Natalia Khanzhina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapenok_A/0/1/0/all/0/1"&gt;Alexey Lapenok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filchenkov_A/0/1/0/all/0/1"&gt;Andrey Filchenkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Feature Tracker: A Novel Application for Deep Convolutional Neural Networks. (arXiv:2108.00105v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00105</id>
        <link href="http://arxiv.org/abs/2108.00105"/>
        <updated>2021-08-03T02:06:31.238Z</updated>
        <summary type="html"><![CDATA[Feature tracking is the building block of many applications such as visual
odometry, augmented reality, and target tracking. Unfortunately, the
state-of-the-art vision-based tracking algorithms fail in surgical images due
to the challenges imposed by the nature of such environments. In this paper, we
proposed a novel and unified deep learning-based approach that can learn how to
track features reliably as well as learn how to detect such reliable features
for tracking purposes. The proposed network dubbed as Deep-PT, consists of a
tracker network which is a convolutional neural network simulating
cross-correlation in terms of deep learning and two fully connected networks
that operate on the output of intermediate layers of the tracker to detect
features and predict trackability of the detected points. The ability to detect
features based on the capabilities of the tracker distinguishes the proposed
method from previous algorithms used in this area and improves the robustness
of the algorithms against dynamics of the scene. The network is trained using
multiple datasets due to the lack of specialized dataset for feature tracking
datasets and extensive comparisons are conducted to compare the accuracy of
Deep-PT against recent pixel tracking algorithms. As the experiments suggest,
the proposed deep architecture deliberately learns what to track and how to
track and outperforms the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parchami_M/0/1/0/all/0/1"&gt;Mostafa Parchami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sayed_S/0/1/0/all/0/1"&gt;Saif Iftekar Sayed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LDDMM-Face: Large Deformation Diffeomorphic Metric Learning for Flexible and Consistent Face Alignment. (arXiv:2108.00690v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00690</id>
        <link href="http://arxiv.org/abs/2108.00690"/>
        <updated>2021-08-03T02:06:31.233Z</updated>
        <summary type="html"><![CDATA[We innovatively propose a flexible and consistent face alignment framework,
LDDMM-Face, the key contribution of which is a deformation layer that naturally
embeds facial geometry in a diffeomorphic way. Instead of predicting facial
landmarks via heatmap or coordinate regression, we formulate this task in a
diffeomorphic registration manner and predict momenta that uniquely
parameterize the deformation between initial boundary and true boundary, and
then perform large deformation diffeomorphic metric mapping (LDDMM)
simultaneously for curve and landmark to localize the facial landmarks. Due to
the embedding of LDDMM into a deep network, LDDMM-Face can consistently
annotate facial landmarks without ambiguity and flexibly handle various
annotation schemes, and can even predict dense annotations from sparse ones.
Our method can be easily integrated into various face alignment networks. We
extensively evaluate LDDMM-Face on four benchmark datasets: 300W, WFLW, HELEN
and COFW-68. LDDMM-Face is comparable or superior to state-of-the-art methods
for traditional within-dataset and same-annotation settings, but truly
distinguishes itself with outstanding performance when dealing with
weakly-supervised learning (partial-to-full), challenging cases (e.g., occluded
faces), and different training and prediction datasets. In addition, LDDMM-Face
shows promising results on the most challenging task of predicting across
datasets with different annotation schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Huilin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1"&gt;Junyan Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1"&gt;Pujin Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xiaoying Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PoseFusion2: Simultaneous Background Reconstruction and Human Shape Recovery in Real-time. (arXiv:2108.00695v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00695</id>
        <link href="http://arxiv.org/abs/2108.00695"/>
        <updated>2021-08-03T02:06:31.221Z</updated>
        <summary type="html"><![CDATA[Dynamic environments that include unstructured moving objects pose a hard
problem for Simultaneous Localization and Mapping (SLAM) performance. The
motion of rigid objects can be typically tracked by exploiting their texture
and geometric features. However, humans moving in the scene are often one of
the most important, interactive targets - they are very hard to track and
reconstruct robustly due to non-rigid shapes. In this work, we present a fast,
learning-based human object detector to isolate the dynamic human objects and
realise a real-time dense background reconstruction framework. We go further by
estimating and reconstructing the human pose and shape. The final output
environment maps not only provide the dense static backgrounds but also contain
the dynamic human meshes and their trajectories. Our Dynamic SLAM system runs
at around 26 frames per second (fps) on GPUs, while additionally turning on
accurate human pose estimation can be executed at up to 10 fps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huayan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1"&gt;Tin Lun Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vijayakumar_S/0/1/0/all/0/1"&gt;Sethu Vijayakumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Feature Fusion for Video Advertisements Tagging Via Stacking Ensemble. (arXiv:2108.00679v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00679</id>
        <link href="http://arxiv.org/abs/2108.00679"/>
        <updated>2021-08-03T02:06:31.213Z</updated>
        <summary type="html"><![CDATA[Automated tagging of video advertisements has been a critical yet challenging
problem, and it has drawn increasing interests in last years as its
applications seem to be evident in many fields. Despite sustainable efforts
have been made, the tagging task is still suffered from several challenges,
such as, efficiently feature fusion approach is desirable, but under-explored
in previous studies. In this paper, we present our approach for Multimodal
Video Ads Tagging in the 2021 Tencent Advertising Algorithm Competition.
Specifically, we propose a novel multi-modal feature fusion framework, with the
goal to combine complementary information from multiple modalities. This
framework introduces stacking-based ensembling approach to reduce the influence
of varying levels of noise and conflicts between different modalities. Thus,
our framework can boost the performance of the tagging task, compared to
previous methods. To empirically investigate the effectiveness and robustness
of the proposed framework, we conduct extensive experiments on the challenge
datasets. The obtained results suggest that our framework can significantly
outperform related approaches and our method ranks as the 1st place on the
final leaderboard, with a Global Average Precision (GAP) of 82.63%. To better
promote the research in this field, we will release our code in the final
version.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1"&gt;Qingsong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1"&gt;Hai Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhimin Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kele Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning TFIDF Enhanced Joint Embedding for Recipe-Image Cross-Modal Retrieval Service. (arXiv:2108.00724v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00724</id>
        <link href="http://arxiv.org/abs/2108.00724"/>
        <updated>2021-08-03T02:06:31.207Z</updated>
        <summary type="html"><![CDATA[It is widely acknowledged that learning joint embeddings of recipes with
images is challenging due to the diverse composition and deformation of
ingredients in cooking procedures. We present a Multi-modal Semantics enhanced
Joint Embedding approach (MSJE) for learning a common feature space between the
two modalities (text and image), with the ultimate goal of providing
high-performance cross-modal retrieval services. Our MSJE approach has three
unique features. First, we extract the TFIDF feature from the title,
ingredients and cooking instructions of recipes. By determining the
significance of word sequences through combining LSTM learned features with
their TFIDF features, we encode a recipe into a TFIDF weighted vector for
capturing significant key terms and how such key terms are used in the
corresponding cooking instructions. Second, we combine the recipe TFIDF feature
with the recipe sequence feature extracted through two-stage LSTM networks,
which is effective in capturing the unique relationship between a recipe and
its associated image(s). Third, we further incorporate TFIDF enhanced category
semantics to improve the mapping of image modality and to regulate the
similarity loss function during the iterative learning of cross-modal joint
embedding. Experiments on the benchmark dataset Recipe1M show the proposed
approach outperforms the state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zhongwei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Ling Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yanzhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1"&gt;Luo Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recursively Refined R-CNN: Instance Segmentation with Self-RoI Rebalancing. (arXiv:2104.01329v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01329</id>
        <link href="http://arxiv.org/abs/2104.01329"/>
        <updated>2021-08-03T02:06:31.190Z</updated>
        <summary type="html"><![CDATA[Within the field of instance segmentation, most of the state-of-the-art deep
learning networks rely nowadays on cascade architectures, where multiple object
detectors are trained sequentially, re-sampling the ground truth at each step.
This offers a solution to the problem of exponentially vanishing positive
samples. However, it also translates into an increase in network complexity in
terms of the number of parameters. To address this issue, we propose
Recursively Refined R-CNN (R^3-CNN) which avoids duplicates by introducing a
loop mechanism instead. At the same time, it achieves a quality boost using a
recursive re-sampling technique, where a specific IoU quality is utilized in
each recursion to eventually equally cover the positive spectrum. Our
experiments highlight the specific encoding of the loop mechanism in the
weights, requiring its usage at inference time. The R^3-CNN architecture is
able to surpass the recently proposed HTC model, while reducing the number of
parameters significantly. Experiments on COCO minival 2017 dataset show
performance boost independently from the utilized baseline model. The code is
available online at https://github.com/IMPLabUniPr/mmdetection/tree/r3_cnn.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rossi_L/0/1/0/all/0/1"&gt;Leonardo Rossi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karimi_A/0/1/0/all/0/1"&gt;Akbar Karimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prati_A/0/1/0/all/0/1"&gt;Andrea Prati&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Deep Feature Calibration for Cross-Modal Joint Embedding Learning. (arXiv:2108.00705v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00705</id>
        <link href="http://arxiv.org/abs/2108.00705"/>
        <updated>2021-08-03T02:06:31.184Z</updated>
        <summary type="html"><![CDATA[This paper introduces a two-phase deep feature calibration framework for
efficient learning of semantics enhanced text-image cross-modal joint
embedding, which clearly separates the deep feature calibration in data
preprocessing from training the joint embedding model. We use the Recipe1M
dataset for the technical description and empirical validation. In
preprocessing, we perform deep feature calibration by combining deep feature
engineering with semantic context features derived from raw text-image input
data. We leverage LSTM to identify key terms, NLP methods to produce ranking
scores for key terms before generating the key term feature. We leverage
wideResNet50 to extract and encode the image category semantics to help
semantic alignment of the learned recipe and image embeddings in the joint
latent space. In joint embedding learning, we perform deep feature calibration
by optimizing the batch-hard triplet loss function with soft-margin and double
negative sampling, also utilizing the category-based alignment loss and
discriminator-based alignment loss. Extensive experiments demonstrate that our
SEJE approach with the deep feature calibration significantly outperforms the
state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zhongwei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Ling Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1"&gt;Luo Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrent Mask Refinement for Few-Shot Medical Image Segmentation. (arXiv:2108.00622v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00622</id>
        <link href="http://arxiv.org/abs/2108.00622"/>
        <updated>2021-08-03T02:06:31.177Z</updated>
        <summary type="html"><![CDATA[Although having achieved great success in medical image segmentation, deep
convolutional neural networks usually require a large dataset with manual
annotations for training and are difficult to generalize to unseen classes.
Few-shot learning has the potential to address these challenges by learning new
classes from only a few labeled examples. In this work, we propose a new
framework for few-shot medical image segmentation based on prototypical
networks. Our innovation lies in the design of two key modules: 1) a context
relation encoder (CRE) that uses correlation to capture local relation features
between foreground and background regions; and 2) a recurrent mask refinement
module that repeatedly uses the CRE and a prototypical network to recapture the
change of context relationship and refine the segmentation mask iteratively.
Experiments on two abdomen CT datasets and an abdomen MRI dataset show the
proposed method obtains substantial improvement over the state-of-the-art
methods by an average of 16.32%, 8.45% and 6.24% in terms of DSC, respectively.
Code is publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Hao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xingwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1"&gt;Shanlin Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1"&gt;Xiangyi Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xiaohui Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RINDNet: Edge Detection for Discontinuity in Reflectance, Illumination, Normal and Depth. (arXiv:2108.00616v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00616</id>
        <link href="http://arxiv.org/abs/2108.00616"/>
        <updated>2021-08-03T02:06:31.171Z</updated>
        <summary type="html"><![CDATA[As a fundamental building block in computer vision, edges can be categorised
into four types according to the discontinuity in surface-Reflectance,
Illumination, surface-Normal or Depth. While great progress has been made in
detecting generic or individual types of edges, it remains under-explored to
comprehensively study all four edge types together. In this paper, we propose a
novel neural network solution, RINDNet, to jointly detect all four types of
edges. Taking into consideration the distinct attributes of each type of edges
and the relationship between them, RINDNet learns effective representations for
each of them and works in three stages. In stage I, RINDNet uses a common
backbone to extract features shared by all edges. Then in stage II it branches
to prepare discriminative features for each edge type by the corresponding
decoder. In stage III, an independent decision head for each type aggregates
the features from previous stages to predict the initial results. Additionally,
an attention module learns attention maps for all types to capture the
underlying relations between them, and these maps are combined with initial
results to generate the final edge detection results. For training and
evaluation, we construct the first public benchmark, BSDS-RIND, with all four
types of edges carefully annotated. In our experiments, RINDNet yields
promising results in comparison with state-of-the-art methods. Additional
analysis is presented in supplementary material.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pu_M/0/1/0/all/0/1"&gt;Mengyang Pu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yaping Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_Q/0/1/0/all/0/1"&gt;Qingji Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1"&gt;Haibin Ling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Investigating Attention Mechanism in 3D Point Cloud Object Detection. (arXiv:2108.00620v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00620</id>
        <link href="http://arxiv.org/abs/2108.00620"/>
        <updated>2021-08-03T02:06:31.164Z</updated>
        <summary type="html"><![CDATA[Object detection in three-dimensional (3D) space attracts much interest from
academia and industry since it is an essential task in AI-driven applications
such as robotics, autonomous driving, and augmented reality. As the basic
format of 3D data, the point cloud can provide detailed geometric information
about the objects in the original 3D space. However, due to 3D data's sparsity
and unorderedness, specially designed networks and modules are needed to
process this type of data. Attention mechanism has achieved impressive
performance in diverse computer vision tasks; however, it is unclear how
attention modules would affect the performance of 3D point cloud object
detection and what sort of attention modules could fit with the inherent
properties of 3D data. This work investigates the role of the attention
mechanism in 3D point cloud object detection and provides insights into the
potential of different attention modules. To achieve that, we comprehensively
investigate classical 2D attentions, novel 3D attentions, including the latest
point cloud transformers on SUN RGB-D and ScanNetV2 datasets. Based on the
detailed experiments and analysis, we conclude the effects of different
attention modules. This paper is expected to serve as a reference source for
benefiting attention-embedded 3D point cloud object detection. The code and
trained models are available at:
https://github.com/ShiQiu0419/attentions_in_3D_detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1"&gt;Shi Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yunfan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1"&gt;Saeed Anwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chongyi Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Perception for Ambiguous Objects Classification. (arXiv:2108.00737v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00737</id>
        <link href="http://arxiv.org/abs/2108.00737"/>
        <updated>2021-08-03T02:06:31.148Z</updated>
        <summary type="html"><![CDATA[Recent visual pose estimation and tracking solutions provide notable results
on popular datasets such as T-LESS and YCB. However, in the real world, we can
find ambiguous objects that do not allow exact classification and detection
from a single view. In this work, we propose a framework that, given a single
view of an object, provides the coordinates of a next viewpoint to discriminate
the object against similar ones, if any, and eliminates ambiguities. We also
describe a complete pipeline from a real object's scans to the viewpoint
selection and classification. We validate our approach with a Franka Emika
Panda robot and common household objects featured with ambiguities. We released
the source code to reproduce our experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Safronov_E/0/1/0/all/0/1"&gt;Evgenii Safronov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piga_N/0/1/0/all/0/1"&gt;Nicola Piga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colledanchise_M/0/1/0/all/0/1"&gt;Michele Colledanchise&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Natale_L/0/1/0/all/0/1"&gt;Lorenzo Natale&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Learning with Local Attention-Aware Feature. (arXiv:2108.00475v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00475</id>
        <link href="http://arxiv.org/abs/2108.00475"/>
        <updated>2021-08-03T02:06:31.143Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a novel methodology for self-supervised learning for
generating global and local attention-aware visual features. Our approach is
based on training a model to differentiate between specific image
transformations of an input sample and the patched images. Utilizing this
approach, the proposed method is able to outperform the previous best
competitor by 1.03% on the Tiny-ImageNet dataset and by 2.32% on the STL-10
dataset. Furthermore, our approach outperforms the fully-supervised learning
method on the STL-10 dataset. Experimental results and visualizations show the
capability of successfully learning global and local attention-aware visual
representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1"&gt;Trung X. Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mina_R/0/1/0/all/0/1"&gt;Rusty John Lloyd Mina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Issa_D/0/1/0/all/0/1"&gt;Dias Issa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1"&gt;Chang D. Yoo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Audiovisual Representation Learning for Remote Sensing Data. (arXiv:2108.00688v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00688</id>
        <link href="http://arxiv.org/abs/2108.00688"/>
        <updated>2021-08-03T02:06:31.137Z</updated>
        <summary type="html"><![CDATA[Many current deep learning approaches make extensive use of backbone networks
pre-trained on large datasets like ImageNet, which are then fine-tuned to
perform a certain task. In remote sensing, the lack of comparable large
annotated datasets and the wide diversity of sensing platforms impedes similar
developments. In order to contribute towards the availability of pre-trained
backbone networks in remote sensing, we devise a self-supervised approach for
pre-training deep neural networks. By exploiting the correspondence between
geo-tagged audio recordings and remote sensing imagery, this is done in a
completely label-free manner, eliminating the need for laborious manual
annotation. For this purpose, we introduce the SoundingEarth dataset, which
consists of co-located aerial imagery and audio samples all around the world.
Using this dataset, we then pre-train ResNet models to map samples from both
modalities into a common embedding space, which encourages the models to
understand key properties of a scene that influence both visual and auditory
appearance. To validate the usefulness of the proposed approach, we evaluate
the transfer learning performance of pre-trained weights obtained against
weights obtained through other means. By fine-tuning the models on a number of
commonly used remote sensing datasets, we show that our approach outperforms
existing pre-training strategies for remote sensing imagery. The dataset, code
and pre-trained model weights will be available at
https://github.com/khdlr/SoundingEarth.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heidler_K/0/1/0/all/0/1"&gt;Konrad Heidler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1"&gt;Lichao Mou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1"&gt;Di Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1"&gt;Pu Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guangyao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1"&gt;Chuang Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Ji-Rong Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiao Xiang Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cohort Bias Adaptation in Aggregated Datasets for Lesion Segmentation. (arXiv:2108.00713v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.00713</id>
        <link href="http://arxiv.org/abs/2108.00713"/>
        <updated>2021-08-03T02:06:31.120Z</updated>
        <summary type="html"><![CDATA[Many automatic machine learning models developed for focal pathology (e.g.
lesions, tumours) detection and segmentation perform well, but do not
generalize as well to new patient cohorts, impeding their widespread adoption
into real clinical contexts. One strategy to create a more diverse,
generalizable training set is to naively pool datasets from different cohorts.
Surprisingly, training on this \it{big data} does not necessarily increase, and
may even reduce, overall performance and model generalizability, due to the
existence of cohort biases that affect label distributions. In this paper, we
propose a generalized affine conditioning framework to learn and account for
cohort biases across multi-source datasets, which we call Source-Conditioned
Instance Normalization (SCIN). Through extensive experimentation on three
different, large scale, multi-scanner, multi-centre Multiple Sclerosis (MS)
clinical trial MRI datasets, we show that our cohort bias adaptation method (1)
improves performance of the network on pooled datasets relative to naively
pooling datasets and (2) can quickly adapt to a new cohort by fine-tuning the
instance normalization parameters, thus learning the new cohort bias with only
10 labelled samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nichyporuk_B/0/1/0/all/0/1"&gt;Brennan Nichyporuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cardinell_J/0/1/0/all/0/1"&gt;Jillian Cardinell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Szeto_J/0/1/0/all/0/1"&gt;Justin Szeto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mehta_R/0/1/0/all/0/1"&gt;Raghav Mehta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1"&gt;Sotirios Tsaftaris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arnold_D/0/1/0/all/0/1"&gt;Douglas L. Arnold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arbel_T/0/1/0/all/0/1"&gt;Tal Arbel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Congested Crowd Instance Localization with Dilated Convolutional Swin Transformer. (arXiv:2108.00584v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00584</id>
        <link href="http://arxiv.org/abs/2108.00584"/>
        <updated>2021-08-03T02:06:31.112Z</updated>
        <summary type="html"><![CDATA[Crowd localization is a new computer vision task, evolved from crowd
counting. Different from the latter, it provides more precise location
information for each instance, not just counting numbers for the whole crowd
scene, which brings greater challenges, especially in extremely congested crowd
scenes. In this paper, we focus on how to achieve precise instance localization
in high-density crowd scenes, and to alleviate the problem that the feature
extraction ability of the traditional model is reduced due to the target
occlusion, the image blur, etc. To this end, we propose a Dilated Convolutional
Swin Transformer (DCST) for congested crowd scenes. Specifically, a
window-based vision transformer is introduced into the crowd localization task,
which effectively improves the capacity of representation learning. Then, the
well-designed dilated convolutional module is inserted into some different
stages of the transformer to enhance the large-range contextual information.
Extensive experiments evidence the effectiveness of the proposed methods and
achieve state-of-the-art performance on five popular datasets. Especially, the
proposed model achieves F1-measure of 77.5\% and MAE of 84.2 in terms of
localization and counting performance, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Junyu Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1"&gt;Maoguo Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuelong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PSE-Match: A Viewpoint-free Place Recognition Method with Parallel Semantic Embedding. (arXiv:2108.00552v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00552</id>
        <link href="http://arxiv.org/abs/2108.00552"/>
        <updated>2021-08-03T02:06:31.078Z</updated>
        <summary type="html"><![CDATA[Accurate localization on autonomous driving cars is essential for autonomy
and driving safety, especially for complex urban streets and search-and-rescue
subterranean environments where high-accurate GPS is not available. However
current odometry estimation may introduce the drifting problems in long-term
navigation without robust global localization. The main challenges involve
scene divergence under the interference of dynamic environments and effective
perception of observation and object layout variance from different viewpoints.
To tackle these challenges, we present PSE-Match, a viewpoint-free place
recognition method based on parallel semantic analysis of isolated semantic
attributes from 3D point-cloud models. Compared with the original point cloud,
the observed variance of semantic attributes is smaller. PSE-Match incorporates
a divergence place learning network to capture different semantic attributes
parallelly through the spherical harmonics domain. Using both existing
benchmark datasets and two in-field collected datasets, our experiments show
that the proposed method achieves above 70% average recall with top one
retrieval and above 95% average recall with top ten retrieval cases. And
PSE-Match has also demonstrated an obvious generalization ability with a
limited training dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1"&gt;Peng Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lingyun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Egorov_A/0/1/0/all/0/1"&gt;Anton Egorov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bing Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BundleTrack: 6D Pose Tracking for Novel Objects without Instance or Category-Level 3D Models. (arXiv:2108.00516v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00516</id>
        <link href="http://arxiv.org/abs/2108.00516"/>
        <updated>2021-08-03T02:06:31.072Z</updated>
        <summary type="html"><![CDATA[Tracking the 6D pose of objects in video sequences is important for robot
manipulation. Most prior efforts, however, often assume that the target
object's CAD model, at least at a category-level, is available for offline
training or during online template matching. This work proposes BundleTrack, a
general framework for 6D pose tracking of novel objects, which does not depend
upon 3D models, either at the instance or category-level. It leverages the
complementary attributes of recent advances in deep learning for segmentation
and robust feature extraction, as well as memory-augmented pose graph
optimization for spatiotemporal consistency. This enables long-term, low-drift
tracking under various challenging scenarios, including significant occlusions
and object motions. Comprehensive experiments given two public benchmarks
demonstrate that the proposed approach significantly outperforms state-of-art,
category-level 6D tracking or dynamic SLAM methods. When compared against
state-of-art methods that rely on an object instance CAD model, comparable
performance is achieved, despite the proposed method's reduced information
requirements. An efficient implementation in CUDA provides a real-time
performance of 10Hz for the entire framework. Code is available at:
https://github.com/wenbowen123/BundleTrack]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1"&gt;Bowen Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bekris_K/0/1/0/all/0/1"&gt;Kostas Bekris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object-to-Scene: Learning to Transfer Object Knowledge to Indoor Scene Recognition. (arXiv:2108.00399v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00399</id>
        <link href="http://arxiv.org/abs/2108.00399"/>
        <updated>2021-08-03T02:06:31.053Z</updated>
        <summary type="html"><![CDATA[Accurate perception of the surrounding scene is helpful for robots to make
reasonable judgments and behaviours. Therefore, developing effective scene
representation and recognition methods are of significant importance in
robotics. Currently, a large body of research focuses on developing novel
auxiliary features and networks to improve indoor scene recognition ability.
However, few of them focus on directly constructing object features and
relations for indoor scene recognition. In this paper, we analyze the
weaknesses of current methods and propose an Object-to-Scene (OTS) method,
which extracts object features and learns object relations to recognize indoor
scenes. The proposed OTS first extracts object features based on the
segmentation network and the proposed object feature aggregation module (OFAM).
Afterwards, the object relations are calculated and the scene representation is
constructed based on the proposed object attention module (OAM) and global
relation aggregation module (GRAM). The final results in this work show that
OTS successfully extracts object features and learns object relations from the
segmentation network. Moreover, OTS outperforms the state-of-the-art methods by
more than 2\% on indoor scene recognition without using any additional streams.
Code is publicly available at: https://github.com/FreeformRobotics/OTS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miao_B/0/1/0/all/0/1"&gt;Bo Miao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Liguang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1"&gt;Ajmal Mian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1"&gt;Tin Lun Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yangsheng Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Pest Detection with DNN on the Edge for Precision Agriculture. (arXiv:2108.00421v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00421</id>
        <link href="http://arxiv.org/abs/2108.00421"/>
        <updated>2021-08-03T02:06:31.047Z</updated>
        <summary type="html"><![CDATA[Artificial intelligence has smoothly penetrated several economic activities,
especially monitoring and control applications, including the agriculture
sector. However, research efforts toward low-power sensing devices with fully
functional machine learning (ML) on-board are still fragmented and limited in
smart farming. Biotic stress is one of the primary causes of crop yield
reduction. With the development of deep learning in computer vision technology,
autonomous detection of pest infestation through images has become an important
research direction for timely crop disease diagnosis. This paper presents an
embedded system enhanced with ML functionalities, ensuring continuous detection
of pest infestation inside fruit orchards. The embedded solution is based on a
low-power embedded sensing system along with a Neural Accelerator able to
capture and process images inside common pheromone-based traps. Three different
ML algorithms have been trained and deployed, highlighting the capabilities of
the platform. Moreover, the proposed approach guarantees an extended battery
life thanks to the integration of energy harvesting functionalities. Results
show how it is possible to automate the task of pest infestation for unlimited
time without the farmer's intervention.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Albanese_A/0/1/0/all/0/1"&gt;Andrea Albanese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nardello_M/0/1/0/all/0/1"&gt;Matteo Nardello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brunelli_D/0/1/0/all/0/1"&gt;Davide Brunelli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FLASH: Fast Neural Architecture Search with Hardware Optimization. (arXiv:2108.00568v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00568</id>
        <link href="http://arxiv.org/abs/2108.00568"/>
        <updated>2021-08-03T02:06:31.033Z</updated>
        <summary type="html"><![CDATA[Neural architecture search (NAS) is a promising technique to design efficient
and high-performance deep neural networks (DNNs). As the performance
requirements of ML applications grow continuously, the hardware accelerators
start playing a central role in DNN design. This trend makes NAS even more
complicated and time-consuming for most real applications. This paper proposes
FLASH, a very fast NAS methodology that co-optimizes the DNN accuracy and
performance on a real hardware platform. As the main theoretical contribution,
we first propose the NN-Degree, an analytical metric to quantify the
topological characteristics of DNNs with skip connections (e.g., DenseNets,
ResNets, Wide-ResNets, and MobileNets). The newly proposed NN-Degree allows us
to do training-free NAS within one second and build an accuracy predictor by
training as few as 25 samples out of a vast search space with more than 63
billion configurations. Second, by performing inference on the target hardware,
we fine-tune and validate our analytical models to estimate the latency, area,
and energy consumption of various DNN architectures while executing standard ML
datasets. Third, we construct a hierarchical algorithm based on simplicial
homology global optimization (SHGO) to optimize the model-architecture
co-design process, while considering the area, latency, and energy consumption
of the target hardware. We demonstrate that, compared to the state-of-the-art
NAS approaches, our proposed hierarchical SHGO-based algorithm enables more
than four orders of magnitude speedup (specifically, the execution time of the
proposed algorithm is about 0.1 seconds). Finally, our experimental evaluations
show that FLASH is easily transferable to different hardware architectures,
thus enabling us to do NAS on a Raspberry Pi-3B processor in less than 3
seconds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guihong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mandal_S/0/1/0/all/0/1"&gt;Sumit K. Mandal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ogras_U/0/1/0/all/0/1"&gt;Umit Y. Ogras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marculescu_R/0/1/0/all/0/1"&gt;Radu Marculescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Effective and Robust Detector for Logo Detection. (arXiv:2108.00422v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00422</id>
        <link href="http://arxiv.org/abs/2108.00422"/>
        <updated>2021-08-03T02:06:31.026Z</updated>
        <summary type="html"><![CDATA[In recent years, intellectual property (IP), which represents literary,
inventions, artistic works, etc, gradually attract more and more people's
attention. Particularly, with the rise of e-commerce, the IP not only
represents the product design and brands, but also represents the images/videos
displayed on e-commerce platforms. Unfortunately, some attackers adopt some
adversarial methods to fool the well-trained logo detection model for
infringement. To overcome this problem, a novel logo detector based on the
mechanism of looking and thinking twice is proposed in this paper for robust
logo detection. The proposed detector is different from other mainstream
detectors, which can effectively detect small objects, long-tail objects, and
is robust to adversarial images. In detail, we extend detectoRS algorithm to a
cascade schema with an equalization loss function, multi-scale transformations,
and adversarial data augmentation. A series of experimental results have shown
that the proposed method can effectively improve the robustness of the
detection model. Moreover, we have applied the proposed methods to competition
ACM MM2021 Robust Logo Detection that is organized by Alibaba on the Tianchi
platform and won top 2 in 36489 teams. Code is available at
https://github.com/jiaxiaojunQAQ/Robust-Logo-Detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xiaojun Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1"&gt;Huanqian Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yonglin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xingxing Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1"&gt;Xiaochun Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GTNet:Guided Transformer Network for Detecting Human-Object Interactions. (arXiv:2108.00596v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00596</id>
        <link href="http://arxiv.org/abs/2108.00596"/>
        <updated>2021-08-03T02:06:31.008Z</updated>
        <summary type="html"><![CDATA[The human-object interaction (HOI) detection task refers to localizing
humans, localizing objects, and predicting the interactions between each
human-object pair. HOI is considered one of the fundamental steps in truly
understanding complex visual scenes. For detecting HOI, it is important to
utilize relative spatial configurations and object semantics to find salient
spatial regions of images that highlight the interactions between human object
pairs. This issue is addressed by the proposed self-attention based guided
transformer network, GTNet. GTNet encodes this spatial contextual information
in human and object visual features via self-attention while achieving a 4%-6%
improvement over previous state of the art results on both the V-COCO and
HICO-DET datasets. Code will be made available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Iftekhar_A/0/1/0/all/0/1"&gt;A S M Iftekhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Satish Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McEver_R/0/1/0/all/0/1"&gt;R. Austin McEver&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1"&gt;Suya You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manjunath_B/0/1/0/all/0/1"&gt;B.S. Manjunath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple Classifiers Based Maximum Classifier Discrepancy for Unsupervised Domain Adaptation. (arXiv:2108.00610v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00610</id>
        <link href="http://arxiv.org/abs/2108.00610"/>
        <updated>2021-08-03T02:06:30.996Z</updated>
        <summary type="html"><![CDATA[Adversarial training based on the maximum classifier discrepancy between the
two classifier structures has achieved great success in unsupervised domain
adaptation tasks for image classification. The approach adopts the structure of
two classifiers, though simple and intuitive, the learned classification
boundary may not well represent the data property in the new domain. In this
paper, we propose to extend the structure to multiple classifiers to further
boost its performance. To this end, we propose a very straightforward approach
to adding more classifiers. We employ the principle that the classifiers are
different from each other to construct a discrepancy loss function for multiple
classifiers. Through the loss function construction method, we make it possible
to add any number of classifiers to the original framework. The proposed
approach is validated through extensive experimental evaluations. We
demonstrate that, on average, adopting the structure of three classifiers
normally yields the best performance as a trade-off between the accuracy and
efficiency. With minimum extra computational costs, the proposed approach can
significantly improve the original algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yiju Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1"&gt;Taejoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guanghui Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraphFPN: Graph Feature Pyramid Network for Object Detection. (arXiv:2108.00580v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00580</id>
        <link href="http://arxiv.org/abs/2108.00580"/>
        <updated>2021-08-03T02:06:30.988Z</updated>
        <summary type="html"><![CDATA[Feature pyramids have been proven powerful in image understanding tasks that
require multi-scale features. State-of-the-art methods for multi-scale feature
learning focus on performing feature interactions across space and scales using
neural networks with a fixed topology. In this paper, we propose graph feature
pyramid networks that are capable of adapting their topological structures to
varying intrinsic image structures and supporting simultaneous feature
interactions across all scales. We first define an image-specific superpixel
hierarchy for each input image to represent its intrinsic image structures. The
graph feature pyramid network inherits its structure from this superpixel
hierarchy. Contextual and hierarchical layers are designed to achieve feature
interactions within the same scale and across different scales. To make these
layers more powerful, we introduce two types of local channel attention for
graph neural networks by generalizing global channel attention for
convolutional neural networks. The proposed graph feature pyramid network can
enhance the multiscale features from a convolutional feature pyramid network.
We evaluate our graph feature pyramid network in the object detection task by
integrating it into the Faster R-CNN algorithm. The modified algorithm
outperforms not only previous state-of-the-art feature pyramid-based methods
with a clear margin but also other popular detection methods on both MS-COCO
2017 validation and test datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1"&gt;Gangming Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1"&gt;Weifeng Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yizhou Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CSC-Unet: A Novel Convolutional Sparse Coding Strategy based Neural Network for Semantic Segmentation. (arXiv:2108.00408v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00408</id>
        <link href="http://arxiv.org/abs/2108.00408"/>
        <updated>2021-08-03T02:06:30.938Z</updated>
        <summary type="html"><![CDATA[It is a challenging task to accurately perform semantic segmentation due to
the complexity of real picture scenes. Many semantic segmentation methods based
on traditional deep learning insufficiently captured the semantic and
appearance information of images, which put limit on their generality and
robustness for various application scenes. In this paper, we proposed a novel
strategy that reformulated the popularly-used convolution operation to
multi-layer convolutional sparse coding block to ease the aforementioned
deficiency. This strategy can be possibly used to significantly improve the
segmentation performance of any semantic segmentation model that involves
convolutional operations. To prove the effectiveness of our idea, we chose the
widely-used U-Net model for the demonstration purpose, and we designed CSC-Unet
model series based on U-Net. Through extensive analysis and experiments, we
provided credible evidence showing that the multi-layer convolutional sparse
coding block enables semantic segmentation model to converge faster, can
extract finer semantic and appearance information of images, and improve the
ability to recover spatial detail information. The best CSC-Unet model
significantly outperforms the results of the original U-Net on three public
datasets with different scenarios, i.e., 87.14% vs. 84.71% on DeepCrack
dataset, 68.91% vs. 67.09% on Nuclei dataset, and 53.68% vs. 48.82% on CamVid
dataset, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Haitong Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shuang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xia Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1"&gt;Qin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kaiyue Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1"&gt;Hongjie Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1"&gt;Nizhuan Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Threat of Adversarial Attacks on Deep Learning in Computer Vision: Survey II. (arXiv:2108.00401v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00401</id>
        <link href="http://arxiv.org/abs/2108.00401"/>
        <updated>2021-08-03T02:06:30.932Z</updated>
        <summary type="html"><![CDATA[Deep Learning (DL) is the most widely used tool in the contemporary field of
computer vision. Its ability to accurately solve complex problems is employed
in vision research to learn deep neural models for a variety of tasks,
including security critical applications. However, it is now known that DL is
vulnerable to adversarial attacks that can manipulate its predictions by
introducing visually imperceptible perturbations in images and videos. Since
the discovery of this phenomenon in 2013~[1], it has attracted significant
attention of researchers from multiple sub-fields of machine intelligence. In
[2], we reviewed the contributions made by the computer vision community in
adversarial attacks on deep learning (and their defenses) until the advent of
year 2018. Many of those contributions have inspired new directions in this
area, which has matured significantly since witnessing the first generation
methods. Hence, as a legacy sequel of [2], this literature review focuses on
the advances in this area since 2018. To ensure authenticity, we mainly
consider peer-reviewed contributions published in the prestigious sources of
computer vision and machine learning research. Besides a comprehensive
literature review, the article also provides concise definitions of technical
terminologies for non-experts in this domain. Finally, this article discusses
challenges and future outlook of this direction based on the literature
reviewed herein and [2].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1"&gt;Naveed Akhtar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1"&gt;Ajmal Mian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kardan_N/0/1/0/all/0/1"&gt;Navid Kardan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Mubarak Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable Deep Few-shot Anomaly Detection with Deviation Networks. (arXiv:2108.00462v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00462</id>
        <link href="http://arxiv.org/abs/2108.00462"/>
        <updated>2021-08-03T02:06:30.926Z</updated>
        <summary type="html"><![CDATA[Existing anomaly detection paradigms overwhelmingly focus on training
detection models using exclusively normal data or unlabeled data (mostly normal
samples). One notorious issue with these approaches is that they are weak in
discriminating anomalies from normal samples due to the lack of the knowledge
about the anomalies. Here, we study the problem of few-shot anomaly detection,
in which we aim at using a few labeled anomaly examples to train
sample-efficient discriminative detection models. To address this problem, we
introduce a novel weakly-supervised anomaly detection framework to train
detection models without assuming the examples illustrating all possible
classes of anomaly.

Specifically, the proposed approach learns discriminative normality
(regularity) by leveraging the labeled anomalies and a prior probability to
enforce expressive representations of normality and unbounded deviated
representations of abnormality. This is achieved by an end-to-end optimization
of anomaly scores with a neural deviation learning, in which the anomaly scores
of normal samples are imposed to approximate scalar scores drawn from the prior
while that of anomaly examples is enforced to have statistically significant
deviations from these sampled scores in the upper tail. Furthermore, our model
is optimized to learn fine-grained normality and abnormality by top-K
multiple-instance-learning-based feature subspace deviation learning, allowing
more generalized representations. Comprehensive experiments on nine real-world
image anomaly detection benchmarks show that our model is substantially more
sample-efficient and robust, and performs significantly better than
state-of-the-art competing methods in both closed-set and open-set settings.
Our model can also offer explanation capability as a result of its prior-driven
anomaly score learning. Code and datasets are available at:
https://git.io/DevNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1"&gt;Guansong Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1"&gt;Choubo Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chunhua Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1"&gt;Anton van den Hengel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CERL: A Unified Optimization Framework for Light Enhancement with Realistic Noise. (arXiv:2108.00478v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00478</id>
        <link href="http://arxiv.org/abs/2108.00478"/>
        <updated>2021-08-03T02:06:30.919Z</updated>
        <summary type="html"><![CDATA[Low-light images captured in the real world are inevitably corrupted by
sensor noise. Such noise is spatially variant and highly dependent on the
underlying pixel intensity, deviating from the oversimplified assumptions in
conventional denoising. Existing light enhancement methods either overlook the
important impact of real-world noise during enhancement, or treat noise removal
as a separate pre- or post-processing step. We present Coordinated Enhancement
for Real-world Low-light Noisy Images (CERL), that seamlessly integrates light
enhancement and noise suppression parts into a unified and physics-grounded
optimization framework. For the real low-light noise removal part, we customize
a self-supervised denoising model that can easily be adapted without referring
to clean ground-truth images. For the light enhancement part, we also improve
the design of a state-of-the-art backbone. The two parts are then joint
formulated into one principled plug-and-play optimization. Our approach is
compared against state-of-the-art low-light enhancement methods both
qualitatively and quantitatively. Besides standard benchmarks, we further
collect and test on a new realistic low-light mobile photography dataset
(RLMP), whose mobile-captured photos display heavier realistic noise than those
taken by high-quality cameras. CERL consistently produces the most visually
pleasing and artifact-free results across all experiments. Our RLMP dataset and
codes are available at: https://github.com/VITA-Group/CERL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zeyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yifan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Dong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BORM: Bayesian Object Relation Model for Indoor Scene Recognition. (arXiv:2108.00397v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00397</id>
        <link href="http://arxiv.org/abs/2108.00397"/>
        <updated>2021-08-03T02:06:30.913Z</updated>
        <summary type="html"><![CDATA[Scene recognition is a fundamental task in robotic perception. For human
beings, scene recognition is reasonable because they have abundant object
knowledge of the real world. The idea of transferring prior object knowledge
from humans to scene recognition is significant but still less exploited. In
this paper, we propose to utilize meaningful object representations for indoor
scene representation. First, we utilize an improved object model (IOM) as a
baseline that enriches the object knowledge by introducing a scene parsing
algorithm pretrained on the ADE20K dataset with rich object categories related
to the indoor scene. To analyze the object co-occurrences and pairwise object
relations, we formulate the IOM from a Bayesian perspective as the Bayesian
object relation model (BORM). Meanwhile, we incorporate the proposed BORM with
the PlacesCNN model as the combined Bayesian object relation model (CBORM) for
scene recognition and significantly outperforms the state-of-the-art methods on
the reduced Places365 dataset, and SUN RGB-D dataset without retraining,
showing the excellent generalization ability of the proposed method. Code can
be found at https://github.com/hszhoushen/borm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Liguang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cen_J/0/1/0/all/0/1"&gt;Jun Cen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xingchao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zhenglong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1"&gt;Tin Lun Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yangsheng Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Maritime Obstacle Detection from Weak Annotations by Scaffolding. (arXiv:2108.00564v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00564</id>
        <link href="http://arxiv.org/abs/2108.00564"/>
        <updated>2021-08-03T02:06:30.906Z</updated>
        <summary type="html"><![CDATA[Coastal water autonomous boats rely on robust perception methods for obstacle
detection and timely collision avoidance. The current state-of-the-art is based
on deep segmentation networks trained on large datasets. Per-pixel ground truth
labeling of such datasets, however, is labor-intensive and expensive. We
observe that far less information is required for practical obstacle avoidance
- the location of water edge on static obstacles like shore and approximate
location and bounds of dynamic obstacles in the water is sufficient to plan a
reaction. We propose a new scaffolding learning regime (SLR) that allows
training obstacle detection segmentation networks only from such weak
annotations, thus significantly reducing the cost of ground-truth labeling.
Experiments show that maritime obstacle segmentation networks trained using SLR
substantially outperform the same networks trained with dense ground truth
labels. Thus accuracy is not sacrificed for labelling simplicity but is in fact
improved, which is a remarkable result.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zust_L/0/1/0/all/0/1"&gt;Lojze &amp;#x17d;ust&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kristan_M/0/1/0/all/0/1"&gt;Matej Kristan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyper360 -- a Next Generation Toolset for Immersive Media. (arXiv:2108.00430v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00430</id>
        <link href="http://arxiv.org/abs/2108.00430"/>
        <updated>2021-08-03T02:06:30.845Z</updated>
        <summary type="html"><![CDATA[Spherical 360{\deg} video is a novel media format, rapidly becoming adopted
in media production and consumption of immersive media. Due to its novelty,
there is a lack of tools for producing highly engaging interactive 360{\deg}
video for consumption on a multitude of platforms. In this work, we describe
the work done so far in the Hyper360 project on tools for mixed 360{\deg} video
and 3D content. Furthermore, the first pilots which have been produced with the
Hyper360 tools and results of the audience assessment of the produced pilots
are presented.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fassold_H/0/1/0/all/0/1"&gt;Hannes Fassold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karakottas_A/0/1/0/all/0/1"&gt;Antonis Karakottas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsatsou_D/0/1/0/all/0/1"&gt;Dorothea Tsatsou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zarpalas_D/0/1/0/all/0/1"&gt;Dimitrios Zarpalas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takacs_B/0/1/0/all/0/1"&gt;Barnabas Takacs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fuhrhop_C/0/1/0/all/0/1"&gt;Christian Fuhrhop&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manfredi_A/0/1/0/all/0/1"&gt;Angelo Manfredi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patz_N/0/1/0/all/0/1"&gt;Nicolas Patz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tonoli_S/0/1/0/all/0/1"&gt;Simona Tonoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dulskaia_I/0/1/0/all/0/1"&gt;Iana Dulskaia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep graph matching meets mixed-integer linear programming: Relax at your own risk ?. (arXiv:2108.00394v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00394</id>
        <link href="http://arxiv.org/abs/2108.00394"/>
        <updated>2021-08-03T02:06:30.834Z</updated>
        <summary type="html"><![CDATA[Graph matching is an important problem that has received widespread
attention, especially in the field of computer vision. Recently,
state-of-the-art methods seek to incorporate graph matching with deep learning.
However, there is no research to explain what role the graph matching algorithm
plays in the model. Therefore, we propose an approach integrating a MILP
formulation of the graph matching problem. This formulation is solved to
optimal and it provides inherent baseline. Meanwhile, similar approaches are
derived by releasing the optimal guarantee of the graph matching solver and by
introducing a quality level. This quality level controls the quality of the
solutions provided by the graph matching solver. In addition, several
relaxations of the graph matching problem are put to the test. Our experimental
evaluation gives several theoretical insights and guides the direction of deep
graph matching methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhoubo Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Puqing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raveaux_R/0/1/0/all/0/1"&gt;Romain Raveaux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Huadong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WAS-VTON: Warping Architecture Search for Virtual Try-on Network. (arXiv:2108.00386v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00386</id>
        <link href="http://arxiv.org/abs/2108.00386"/>
        <updated>2021-08-03T02:06:30.817Z</updated>
        <summary type="html"><![CDATA[Despite recent progress on image-based virtual try-on, current methods are
constraint by shared warping networks and thus fail to synthesize natural
try-on results when faced with clothing categories that require different
warping operations. In this paper, we address this problem by finding clothing
category-specific warping networks for the virtual try-on task via Neural
Architecture Search (NAS). We introduce a NAS-Warping Module and elaborately
design a bilevel hierarchical search space to identify the optimal
network-level and operation-level flow estimation architecture. Given the
network-level search space, containing different numbers of warping blocks, and
the operation-level search space with different convolution operations, we
jointly learn a combination of repeatable warping cells and convolution
operations specifically for the clothing-person alignment. Moreover, a
NAS-Fusion Module is proposed to synthesize more natural final try-on results,
which is realized by leveraging particular skip connections to produce
better-fused features that are required for seamlessly fusing the warped
clothing and the unchanged person part. We adopt an efficient and stable
one-shot searching strategy to search the above two modules. Extensive
experiments demonstrate that our WAS-VTON significantly outperforms the
previous fixed-architecture try-on methods with more natural warping results
and virtual try-on results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zhenyu Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xujie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1"&gt;Fuwei Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Haoye Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kampffmeyer_M/0/1/0/all/0/1"&gt;Michael C. Kampffmeyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1"&gt;Haonan Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Free-Viewpoint Performance Rendering under ComplexHuman-object Interactions. (arXiv:2108.00362v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00362</id>
        <link href="http://arxiv.org/abs/2108.00362"/>
        <updated>2021-08-03T02:06:30.800Z</updated>
        <summary type="html"><![CDATA[4D reconstruction of human-object interaction is critical for immersive VR/AR
experience and human activity understanding. Recent advances still fail to
recover fine geometry and texture results from sparse RGB inputs, especially
under challenging human-object interactions scenarios. In this paper, we
propose a neural human performance capture and rendering system to generate
both high-quality geometry and photo-realistic texture of both human and
objects under challenging interaction scenarios in arbitrary novel views, from
only sparse RGB streams. To deal with complex occlusions raised by human-object
interactions, we adopt a layer-wise scene decoupling strategy and perform
volumetric reconstruction and neural rendering of the human and object.
Specifically, for geometry reconstruction, we propose an interaction-aware
human-object capture scheme that jointly considers the human reconstruction and
object reconstruction with their correlations. Occlusion-aware human
reconstruction and robust human-aware object tracking are proposed for
consistent 4D human-object dynamic reconstruction. For neural texture
rendering, we propose a layer-wise human-object rendering scheme, which
combines direction-aware neural blending weight learning and spatial-temporal
texture completion to provide high-resolution and photo-realistic texture
results in the occluded scenarios. Extensive experiments demonstrate the
effectiveness of our approach to achieve high-quality geometry and texture
reconstruction in free viewpoints for challenging human-object interactions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1"&gt;Guoxing Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yizhang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_A/0/1/0/all/0/1"&gt;Anqi Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1"&gt;Pei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yuheng Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jingya Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jingyi Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Boundary Knowledge Translation for Foreground Segmentation. (arXiv:2108.00379v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00379</id>
        <link href="http://arxiv.org/abs/2108.00379"/>
        <updated>2021-08-03T02:06:30.733Z</updated>
        <summary type="html"><![CDATA[When confronted with objects of unknown types in an image, humans can
effortlessly and precisely tell their visual boundaries. This recognition
mechanism and underlying generalization capability seem to contrast to
state-of-the-art image segmentation networks that rely on large-scale
category-aware annotated training samples. In this paper, we make an attempt
towards building models that explicitly account for visual boundary knowledge,
in hope to reduce the training effort on segmenting unseen categories.
Specifically, we investigate a new task termed as Boundary Knowledge
Translation (BKT). Given a set of fully labeled categories, BKT aims to
translate the visual boundary knowledge learned from the labeled categories, to
a set of novel categories, each of which is provided only a few labeled
samples. To this end, we propose a Translation Segmentation Network
(Trans-Net), which comprises a segmentation network and two boundary
discriminators. The segmentation network, combined with a boundary-aware
self-supervised mechanism, is devised to conduct foreground segmentation, while
the two discriminators work together in an adversarial manner to ensure an
accurate segmentation of the novel categories under light supervision.
Exhaustive experiments demonstrate that, with only tens of labeled samples as
guidance, Trans-Net achieves close results on par with fully supervised
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zunlei Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1"&gt;Lechao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinchao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yajie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1"&gt;Xiangtong Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1"&gt;Mingli Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Style Curriculum Learning for Robust Medical Image Segmentation. (arXiv:2108.00402v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.00402</id>
        <link href="http://arxiv.org/abs/2108.00402"/>
        <updated>2021-08-03T02:06:30.727Z</updated>
        <summary type="html"><![CDATA[The performance of deep segmentation models often degrades due to
distribution shifts in image intensities between the training and test data
sets. This is particularly pronounced in multi-centre studies involving data
acquired using multi-vendor scanners, with variations in acquisition protocols.
It is challenging to address this degradation because the shift is often not
known \textit{a priori} and hence difficult to model. We propose a novel
framework to ensure robust segmentation in the presence of such distribution
shifts. Our contribution is three-fold. First, inspired by the spirit of
curriculum learning, we design a novel style curriculum to train the
segmentation models using an easy-to-hard mode. A style transfer model with
style fusion is employed to generate the curriculum samples. Gradually focusing
on complex and adversarial style samples can significantly boost the robustness
of the models. Second, instead of subjectively defining the curriculum
complexity, we adopt an automated gradient manipulation method to control the
hard and adversarial sample generation process. Third, we propose the Local
Gradient Sign strategy to aggregate the gradient locally and stabilise training
during gradient manipulation. The proposed framework can generalise to unknown
distribution without using any target data. Extensive experiments on the public
M\&Ms Challenge dataset demonstrate that our proposed framework can generalise
deep models well to unknown distributions and achieve significant improvements
in segmentation accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhendong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Manh_V/0/1/0/all/0/1"&gt;Van Manh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaoqiong Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1"&gt;Karim Lekadir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Campello_V/0/1/0/all/0/1"&gt;V&amp;#xed;ctor Campello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ravikumar_N/0/1/0/all/0/1"&gt;Nishant Ravikumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Frangi_A/0/1/0/all/0/1"&gt;Alejandro F Frangi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ni_D/0/1/0/all/0/1"&gt;Dong Ni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Instance-level Spatial-Temporal Patterns for Person Re-identification. (arXiv:2108.00171v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00171</id>
        <link href="http://arxiv.org/abs/2108.00171"/>
        <updated>2021-08-03T02:06:30.720Z</updated>
        <summary type="html"><![CDATA[Person re-identification (Re-ID) aims to match pedestrians under dis-joint
cameras. Most Re-ID methods formulate it as visual representation learning and
image search, and its accuracy is consequently affected greatly by the search
space. Spatial-temporal information has been proven to be efficient to filter
irrelevant negative samples and significantly improve Re-ID accuracy. However,
existing spatial-temporal person Re-ID methods are still rough and do not
exploit spatial-temporal information sufficiently. In this paper, we propose a
novel Instance-level and Spatial-Temporal Disentangled Re-ID method (InSTD), to
improve Re-ID accuracy. In our proposed framework, personalized information
such as moving direction is explicitly considered to further narrow down the
search space. Besides, the spatial-temporal transferring probability is
disentangled from joint distribution to marginal distribution, so that outliers
can also be well modeled. Abundant experimental analyses are presented, which
demonstrates the superiority and provides more insights into our method. The
proposed method achieves mAP of 90.8% on Market-1501 and 89.1% on
DukeMTMC-reID, improving from the baseline 82.2% and 72.7%, respectively.
Besides, in order to provide a better benchmark for person re-identification,
we release a cleaned data list of DukeMTMC-reID with this paper:
https://github.com/RenMin1991/cleaned-DukeMTMC-reID/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1"&gt;Min Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lingxiao He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1"&gt;Xingyu Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunlong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1"&gt;Tieniu Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Applications of Artificial Neural Networks in Microorganism Image Analysis: A Comprehensive Review from Conventional Multilayer Perceptron to Popular Convolutional Neural Network and Potential Visual Transformer. (arXiv:2108.00358v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00358</id>
        <link href="http://arxiv.org/abs/2108.00358"/>
        <updated>2021-08-03T02:06:30.707Z</updated>
        <summary type="html"><![CDATA[Microorganisms are widely distributed in the human daily living environment.
They play an essential role in environmental pollution control, disease
prevention and treatment, and food and drug production. The identification,
counting, and detection are the basic steps for making full use of different
microorganisms. However, the conventional analysis methods are expensive,
laborious, and time-consuming. To overcome these limitations, artificial neural
networks are applied for microorganism image analysis. We conduct this review
to understand the development process of microorganism image analysis based on
artificial neural networks. In this review, the background and motivation are
introduced first. Then, the development of artificial neural networks and
representative networks are introduced. After that, the papers related to
microorganism image analysis based on classical and deep neural networks are
reviewed from the perspectives of different tasks. In the end, the methodology
analysis and potential direction are discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinghua Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1"&gt;Marcin Grzegorzek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Few-shot Open-set Classifiers using Exemplar Reconstruction. (arXiv:2108.00340v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00340</id>
        <link href="http://arxiv.org/abs/2108.00340"/>
        <updated>2021-08-03T02:06:30.700Z</updated>
        <summary type="html"><![CDATA[We study the problem of how to identify samples from unseen categories
(open-set classification) when there are only a few samples given from the seen
categories (few-shot setting). The challenge of learning a good abstraction for
a class with very few samples makes it extremely difficult to detect samples
from the unseen categories; consequently, open-set recognition has received
minimal attention in the few-shot setting. Most open-set few-shot
classification methods regularize the softmax score to indicate uniform
probability for open class samples but we argue that this approach is often
inaccurate, especially at a fine-grained level. Instead, we propose a novel
exemplar reconstruction-based meta-learning strategy for jointly detecting open
class samples, as well as, categorizing samples from seen classes via
metric-based classification. The exemplars, which act as representatives of a
class, can either be provided in the training dataset or estimated in the
feature domain. Our framework, named Reconstructing Exemplar based Few-shot
Open-set ClaSsifier (ReFOCS), is tested on a wide variety of datasets and the
experimental results clearly highlight our method as the new state of the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1"&gt;Sayak Nag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raychaudhuri_D/0/1/0/all/0/1"&gt;Dripta S. Raychaudhuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1"&gt;Sujoy Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1"&gt;Amit K. Roy-Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SyDog: A Synthetic Dog Dataset for Improved 2D Pose Estimation. (arXiv:2108.00249v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00249</id>
        <link href="http://arxiv.org/abs/2108.00249"/>
        <updated>2021-08-03T02:06:30.683Z</updated>
        <summary type="html"><![CDATA[Estimating the pose of animals can facilitate the understanding of animal
motion which is fundamental in disciplines such as biomechanics, neuroscience,
ethology, robotics and the entertainment industry. Human pose estimation models
have achieved high performance due to the huge amount of training data
available. Achieving the same results for animal pose estimation is challenging
due to the lack of animal pose datasets. To address this problem we introduce
SyDog: a synthetic dataset of dogs containing ground truth pose and bounding
box coordinates which was generated using the game engine, Unity. We
demonstrate that pose estimation models trained on SyDog achieve better
performance than models trained purely on real data and significantly reduce
the need for the labour intensive labelling of images. We release the SyDog
dataset as a training and evaluation benchmark for research in animal motion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shooter_M/0/1/0/all/0/1"&gt;Moira Shooter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malleson_C/0/1/0/all/0/1"&gt;Charles Malleson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hilton_A/0/1/0/all/0/1"&gt;Adrian Hilton&lt;/a&gt; (University of Surrey)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Word2Pix: Word to Pixel Cross Attention Transformer in Visual Grounding. (arXiv:2108.00205v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00205</id>
        <link href="http://arxiv.org/abs/2108.00205"/>
        <updated>2021-08-03T02:06:30.675Z</updated>
        <summary type="html"><![CDATA[Current one-stage methods for visual grounding encode the language query as
one holistic sentence embedding before fusion with visual feature. Such a
formulation does not treat each word of a query sentence on par when modeling
language to visual attention, therefore prone to neglect words which are less
important for sentence embedding but critical for visual grounding. In this
paper we propose Word2Pix: a one-stage visual grounding network based on
encoder-decoder transformer architecture that enables learning for textual to
visual feature correspondence via word to pixel attention. The embedding of
each word from the query sentence is treated alike by attending to visual
pixels individually instead of single holistic sentence embedding. In this way,
each word is given equivalent opportunity to adjust the language to vision
attention towards the referent target through multiple stacks of transformer
decoder layers. We conduct the experiments on RefCOCO, RefCOCO+ and RefCOCOg
datasets and the proposed Word2Pix outperforms existing one-stage methods by a
notable margin. The results obtained also show that Word2Pix surpasses
two-stage visual grounding models, while at the same time keeping the merits of
one-stage paradigm namely end-to-end training and real-time inference speed
intact.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Heng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Joey Tianyi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ong_Y/0/1/0/all/0/1"&gt;Yew-Soon Ong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowing When to Quit: Selective Cascaded Regression with Patch Attention for Real-Time Face Alignment. (arXiv:2108.00377v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00377</id>
        <link href="http://arxiv.org/abs/2108.00377"/>
        <updated>2021-08-03T02:06:30.642Z</updated>
        <summary type="html"><![CDATA[Facial landmarks (FLM) estimation is a critical component in many
face-related applications. In this work, we aim to optimize for both accuracy
and speed and explore the trade-off between them. Our key observation is that
not all faces are created equal. Frontal faces with neutral expressions
converge faster than faces with extreme poses or expressions. To differentiate
among samples, we train our model to predict the regression error after each
iteration. If the current iteration is accurate enough, we stop iterating,
saving redundant iterations while keeping the accuracy in check. We also
observe that as neighboring patches overlap, we can infer all facial landmarks
(FLMs) with only a small number of patches without a major accuracy sacrifice.
Architecturally, we offer a multi-scale, patch-based, lightweight feature
extractor with a fine-grained local patch attention module, which computes a
patch weighting according to the information in the patch itself and enhances
the expressive power of the patch features. We analyze the patch attention data
to infer where the model is attending when regressing facial landmarks and
compare it to face attention in humans. Our model runs in real-time on a mobile
device GPU, with 95 Mega Multiply-Add (MMA) operations, outperforming all
state-of-the-art methods under 1000 MMA, with a normalized mean error of 8.16
on the 300W challenging dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shapira_G/0/1/0/all/0/1"&gt;Gil Shapira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levy_N/0/1/0/all/0/1"&gt;Noga Levy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldin_I/0/1/0/all/0/1"&gt;Ishay Goldin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jevnisek_R/0/1/0/all/0/1"&gt;Roy J. Jevnisek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning. (arXiv:2108.00352v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.00352</id>
        <link href="http://arxiv.org/abs/2108.00352"/>
        <updated>2021-08-03T02:06:30.633Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning in computer vision aims to pre-train an image
encoder using a large amount of unlabeled images or (image, text) pairs. The
pre-trained image encoder can then be used as a feature extractor to build
downstream classifiers for many downstream tasks with a small amount of or no
labeled training data. In this work, we propose BadEncoder, the first backdoor
attack to self-supervised learning. In particular, our BadEncoder injects
backdoors into a pre-trained image encoder such that the downstream classifiers
built based on the backdoored image encoder for different downstream tasks
simultaneously inherit the backdoor behavior. We formulate our BadEncoder as an
optimization problem and we propose a gradient descent based method to solve
it, which produces a backdoored image encoder from a clean one. Our extensive
empirical evaluation results on multiple datasets show that our BadEncoder
achieves high attack success rates while preserving the accuracy of the
downstream classifiers. We also show the effectiveness of BadEncoder using two
publicly available, real-world image encoders, i.e., Google's image encoder
pre-trained on ImageNet and OpenAI's Contrastive Language-Image Pre-training
(CLIP) image encoder pre-trained on 400 million (image, text) pairs collected
from the Internet. Moreover, we consider defenses including Neural Cleanse and
MNTD (empirical defenses) as well as PatchGuard (a provable defense). Our
results show that these defenses are insufficient to defend against BadEncoder,
highlighting the needs for new defenses against our BadEncoder. Our code is
publicly available at: https://github.com/jjy1994/BadEncoder.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1"&gt;Jinyuan Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yupei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1"&gt;Neil Zhenqiang Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HiFT: Hierarchical Feature Transformer for Aerial Tracking. (arXiv:2108.00202v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00202</id>
        <link href="http://arxiv.org/abs/2108.00202"/>
        <updated>2021-08-03T02:06:30.623Z</updated>
        <summary type="html"><![CDATA[Most existing Siamese-based tracking methods execute the classification and
regression of the target object based on the similarity maps. However, they
either employ a single map from the last convolutional layer which degrades the
localization accuracy in complex scenarios or separately use multiple maps for
decision making, introducing intractable computations for aerial mobile
platforms. Thus, in this work, we propose an efficient and effective
hierarchical feature transformer (HiFT) for aerial tracking. Hierarchical
similarity maps generated by multi-level convolutional layers are fed into the
feature transformer to achieve the interactive fusion of spatial (shallow
layers) and semantics cues (deep layers). Consequently, not only the global
contextual information can be raised, facilitating the target search, but also
our end-to-end architecture with the transformer can efficiently learn the
interdependencies among multi-level features, thereby discovering a
tracking-tailored feature space with strong discriminability. Comprehensive
evaluations on four aerial benchmarks have proven the effectiveness of HiFT.
Real-world tests on the aerial platform have strongly validated its
practicability with a real-time speed. Our code is available at
https://github.com/vision4robotics/HiFT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1"&gt;Ziang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Changhong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Junjie Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bowen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yiming Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unlimited Neighborhood Interaction for Heterogeneous Trajectory Prediction. (arXiv:2108.00238v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.00238</id>
        <link href="http://arxiv.org/abs/2108.00238"/>
        <updated>2021-08-03T02:06:30.605Z</updated>
        <summary type="html"><![CDATA[Understanding complex social interactions among agents is a key challenge for
trajectory prediction. Most existing methods consider the interactions between
pairwise traffic agents or in a local area, while the nature of interactions is
unlimited, involving an uncertain number of agents and non-local areas
simultaneously. Besides, they only focus on homogeneous trajectory prediction,
namely those among agents of the same category, while neglecting people's
diverse reaction patterns toward traffic agents in different categories. To
address these problems, we propose a simple yet effective Unlimited
Neighborhood Interaction Network (UNIN), which predicts trajectories of
heterogeneous agents in multiply categories. Specifically, the proposed
unlimited neighborhood interaction module generates the fused-features of all
agents involved in an interaction simultaneously, which is adaptive to any
number of agents and any range of interaction area. Meanwhile, a hierarchical
graph attention module is proposed to obtain category-tocategory interaction
and agent-to-agent interaction. Finally, parameters of a Gaussian Mixture Model
are estimated for generating the future trajectories. Extensive experimental
results on benchmark datasets demonstrate a significant performance improvement
of our method over the state-ofthe-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1"&gt;Fang Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Le Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Sanping Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1"&gt;Wei Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1"&gt;Zhenxing Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1"&gt;Nanning Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1"&gt;Gang Hua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ELLIPSDF: Joint Object Pose and Shape Optimization with a Bi-level Ellipsoid and Signed Distance Function Description. (arXiv:2108.00355v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00355</id>
        <link href="http://arxiv.org/abs/2108.00355"/>
        <updated>2021-08-03T02:06:30.599Z</updated>
        <summary type="html"><![CDATA[Autonomous systems need to understand the semantics and geometry of their
surroundings in order to comprehend and safely execute object-level task
specifications. This paper proposes an expressive yet compact model for joint
object pose and shape optimization, and an associated optimization algorithm to
infer an object-level map from multi-view RGB-D camera observations. The model
is expressive because it captures the identities, positions, orientations, and
shapes of objects in the environment. It is compact because it relies on a
low-dimensional latent representation of implicit object shape, allowing
onboard storage of large multi-category object maps. Different from other works
that rely on a single object representation format, our approach has a bi-level
object model that captures both the coarse level scale as well as the fine
level shape details. Our approach is evaluated on the large-scale real-world
ScanNet dataset and compared against state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shan_M/0/1/0/all/0/1"&gt;Mo Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1"&gt;Qiaojun Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jau_Y/0/1/0/all/0/1"&gt;You-Yi Jau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atanasov_N/0/1/0/all/0/1"&gt;Nikolay Atanasov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self Context and Shape Prior for Sensorless Freehand 3D Ultrasound Reconstruction. (arXiv:2108.00274v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00274</id>
        <link href="http://arxiv.org/abs/2108.00274"/>
        <updated>2021-08-03T02:06:30.590Z</updated>
        <summary type="html"><![CDATA[3D ultrasound (US) is widely used for its rich diagnostic information.
However, it is criticized for its limited field of view. 3D freehand US
reconstruction is promising in addressing the problem by providing broad range
and freeform scan. The existing deep learning based methods only focus on the
basic cases of skill sequences, and the model relies on the training data
heavily. The sequences in real clinical practice are a mix of diverse skills
and have complex scanning paths. Besides, deep models should adapt themselves
to the testing cases with prior knowledge for better robustness, rather than
only fit to the training cases. In this paper, we propose a novel approach to
sensorless freehand 3D US reconstruction considering the complex skill
sequences. Our contribution is three-fold. First, we advance a novel online
learning framework by designing a differentiable reconstruction algorithm. It
realizes an end-to-end optimization from section sequences to the reconstructed
volume. Second, a self-supervised learning method is developed to explore the
context information that reconstructed by the testing data itself, promoting
the perception of the model. Third, inspired by the effectiveness of shape
prior, we also introduce adversarial training to strengthen the learning of
anatomical shape prior in the reconstructed volume. By mining the context and
structural cues of the testing data, our online learning methods can drive the
model to handle complex skill sequences. Experimental results on developmental
dysplasia of the hip US and fetal US datasets show that, our proposed method
can outperform the start-of-the-art methods regarding the shift errors and path
similarities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1"&gt;Mingyuan Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaoqiong Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yuhao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuxin Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xindi Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravikumar_N/0/1/0/all/0/1"&gt;Nishant Ravikumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1"&gt;Alejandro F Frangi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1"&gt;Dong Ni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Delving into Deep Image Prior for Adversarial Defense: A Novel Reconstruction-based Defense Framework. (arXiv:2108.00180v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00180</id>
        <link href="http://arxiv.org/abs/2108.00180"/>
        <updated>2021-08-03T02:06:30.581Z</updated>
        <summary type="html"><![CDATA[Deep learning based image classification models are shown vulnerable to
adversarial attacks by injecting deliberately crafted noises to clean images.
To defend against adversarial attacks in a training-free and attack-agnostic
manner, this work proposes a novel and effective reconstruction-based defense
framework by delving into deep image prior (DIP). Fundamentally different from
existing reconstruction-based defenses, the proposed method analyzes and
explicitly incorporates the model decision process into our defense. Given an
adversarial image, firstly we map its reconstructed images during DIP
optimization to the model decision space, where cross-boundary images can be
detected and on-boundary images can be further localized. Then, adversarial
noise is purified by perturbing on-boundary images along the reverse direction
to the adversarial image. Finally, on-manifold images are stitched to construct
an image that can be correctly predicted by the victim classifier. Extensive
experiments demonstrate that the proposed method outperforms existing
state-of-the-art reconstruction-based methods both in defending white-box
attacks and defense-aware attacks. Moreover, the proposed method can maintain a
high visual quality during adversarial image reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1"&gt;Li Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yongwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1"&gt;Xin Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1"&gt;Kaiwen Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Ping Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hua Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Z. Jane Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LASOR: Learning Accurate 3D Human Pose and Shape Via Synthetic Occlusion-Aware Data and Neural Mesh Rendering. (arXiv:2108.00351v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00351</id>
        <link href="http://arxiv.org/abs/2108.00351"/>
        <updated>2021-08-03T02:06:30.573Z</updated>
        <summary type="html"><![CDATA[A key challenge in the task of human pose and shape estimation is occlusion,
including self-occlusions, object-human occlusions, and inter-person
occlusions. The lack of diverse and accurate pose and shape training data
becomes a major bottleneck, especially for scenes with occlusions in the wild.
In this paper, we focus on the estimation of human pose and shape in the case
of inter-person occlusions, while also handling object-human occlusions and
self-occlusion. We propose a framework that synthesizes occlusion-aware
silhouette and 2D keypoints data and directly regress to the SMPL pose and
shape parameters. A neural 3D mesh renderer is exploited to enable silhouette
supervision on the fly, which contributes to great improvements in shape
estimation. In addition, keypoints-and-silhouette-driven training data in
panoramic viewpoints are synthesized to compensate for the lack of viewpoint
diversity in any existing dataset. Experimental results show that we are among
state-of-the-art on the 3DPW dataset in terms of pose accuracy and evidently
outperform the rank-1 method in terms of shape accuracy. Top performance is
also achieved on SSP-3D in terms of shape prediction accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kaibing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_R/0/1/0/all/0/1"&gt;Renshu Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toyoura_M/0/1/0/all/0/1"&gt;Masahiro Toyoura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Gang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning with Noisy Labels via Sparse Regularization. (arXiv:2108.00192v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00192</id>
        <link href="http://arxiv.org/abs/2108.00192"/>
        <updated>2021-08-03T02:06:30.550Z</updated>
        <summary type="html"><![CDATA[Learning with noisy labels is an important and challenging task for training
accurate deep neural networks. Some commonly-used loss functions, such as Cross
Entropy (CE), suffer from severe overfitting to noisy labels. Robust loss
functions that satisfy the symmetric condition were tailored to remedy this
problem, which however encounter the underfitting effect. In this paper, we
theoretically prove that \textbf{any loss can be made robust to noisy labels}
by restricting the network output to the set of permutations over a fixed
vector. When the fixed vector is one-hot, we only need to constrain the output
to be one-hot, which however produces zero gradients almost everywhere and thus
makes gradient-based optimization difficult. In this work, we introduce the
sparse regularization strategy to approximate the one-hot constraint, which is
composed of network output sharpening operation that enforces the output
distribution of a network to be sharp and the $\ell_p$-norm ($p\le 1$)
regularization that promotes the network output to be sparse. This simple
approach guarantees the robustness of arbitrary loss functions while not
hindering the fitting ability. Experimental results demonstrate that our method
can significantly improve the performance of commonly-used loss functions in
the presence of noisy labels and class imbalance, and outperform the
state-of-the-art methods. The code is available at
https://github.com/hitcszx/lnl_sr.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xiong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xianming Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chenyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_D/0/1/0/all/0/1"&gt;Deming Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Junjun Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1"&gt;Xiangyang Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Adversarially Robust and Domain Generalizable Stereo Matching by Rethinking DNN Feature Backbones. (arXiv:2108.00335v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00335</id>
        <link href="http://arxiv.org/abs/2108.00335"/>
        <updated>2021-08-03T02:06:30.542Z</updated>
        <summary type="html"><![CDATA[Stereo matching has recently witnessed remarkable progress using Deep Neural
Networks (DNNs). But, how robust are they? Although it has been well-known that
DNNs often suffer from adversarial vulnerability with a catastrophic drop in
performance, the situation is even worse in stereo matching. This paper first
shows that a type of weak white-box attacks can fail state-of-the-art methods.
The attack is learned by a proposed stereo-constrained projected gradient
descent (PGD) method in stereo matching. This observation raises serious
concerns for the deployment of DNN-based stereo matching. Parallel to the
adversarial vulnerability, DNN-based stereo matching is typically trained under
the so-called simulation to reality pipeline, and thus domain generalizability
is an important problem. This paper proposes to rethink the learnable DNN-based
feature backbone towards adversarially-robust and domain generalizable stereo
matching, either by completely removing it or by applying it only to the left
reference image. It computes the matching cost volume using the classic
multi-scale census transform (i.e., local binary pattern) of the raw input
stereo images, followed by a stacked Hourglass head sub-network solving the
matching problem. In experiments, the proposed method is tested in the
SceneFlow dataset and the KITTI2015 benchmark. It significantly improves the
adversarial robustness, while retaining accuracy performance comparable to
state-of-the-art methods. It also shows better generalizability from simulation
(SceneFlow) to real (KITTI) datasets when no fine-tuning is used.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1"&gt;Kelvin Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Healey_C/0/1/0/all/0/1"&gt;Christopher Healey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tianfu Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HR-Crime: Human-Related Anomaly Detection in Surveillance Videos. (arXiv:2108.00246v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00246</id>
        <link href="http://arxiv.org/abs/2108.00246"/>
        <updated>2021-08-03T02:06:30.533Z</updated>
        <summary type="html"><![CDATA[The automatic detection of anomalies captured by surveillance settings is
essential for speeding the otherwise laborious approach. To date, UCF-Crime is
the largest available dataset for automatic visual analysis of anomalies and
consists of real-world crime scenes of various categories. In this paper, we
introduce HR-Crime, a subset of the UCF-Crime dataset suitable for
human-related anomaly detection tasks. We rely on state-of-the-art techniques
to build the feature extraction pipeline for human-related anomaly detection.
Furthermore, we present the baseline anomaly detection analysis on the
HR-Crime. HR-Crime as well as the developed feature extraction pipeline and the
extracted features will be publicly available for further research in the
field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boekhoudt_K/0/1/0/all/0/1"&gt;Kayleigh Boekhoudt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matei_A/0/1/0/all/0/1"&gt;Alina Matei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aghaei_M/0/1/0/all/0/1"&gt;Maya Aghaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talavera_E/0/1/0/all/0/1"&gt;Estefan&amp;#xed;a Talavera&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards explainable artificial intelligence (XAI) for early anticipation of traffic accidents. (arXiv:2108.00273v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00273</id>
        <link href="http://arxiv.org/abs/2108.00273"/>
        <updated>2021-08-03T02:06:30.522Z</updated>
        <summary type="html"><![CDATA[Traffic accident anticipation is a vital function of Automated Driving
Systems (ADSs) for providing a safety-guaranteed driving experience. An
accident anticipation model aims to predict accidents promptly and accurately
before they occur. Existing Artificial Intelligence (AI) models of accident
anticipation lack a human-interpretable explanation of their decision-making.
Although these models perform well, they remain a black-box to the ADS users,
thus difficult to get their trust. To this end, this paper presents a Gated
Recurrent Unit (GRU) network that learns spatio-temporal relational features
for the early anticipation of traffic accidents from dashcam video data. A
post-hoc attention mechanism named Grad-CAM is integrated into the network to
generate saliency maps as the visual explanation of the accident anticipation
decision. An eye tracker captures human eye fixation points for generating
human attention maps. The explainability of network-generated saliency maps is
evaluated in comparison to human attention maps. Qualitative and quantitative
results on a public crash dataset confirm that the proposed explainable network
can anticipate an accident on average 4.57 seconds before it occurs, with
94.02% average precision. In further, various post-hoc attention-based XAI
methods are evaluated and compared. It confirms that the Grad-CAM chosen by
this study can generate high-quality, human-interpretable saliency maps (with
1.42 Normalized Scanpath Saliency) for explaining the crash anticipation
decision. Importantly, results confirm that the proposed AI model, with a
human-inspired design, can outperform humans in the accident anticipation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1"&gt;Muhammad Monjurul Karim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1"&gt;Ruwen Qin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-scale Matching Networks for Semantic Correspondence. (arXiv:2108.00211v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00211</id>
        <link href="http://arxiv.org/abs/2108.00211"/>
        <updated>2021-08-03T02:06:30.514Z</updated>
        <summary type="html"><![CDATA[Deep features have been proven powerful in building accurate dense semantic
correspondences in various previous works. However, the multi-scale and
pyramidal hierarchy of convolutional neural networks has not been well studied
to learn discriminative pixel-level features for semantic correspondence. In
this paper, we propose a multi-scale matching network that is sensitive to tiny
semantic differences between neighboring pixels. We follow the coarse-to-fine
matching strategy and build a top-down feature and matching enhancement scheme
that is coupled with the multi-scale hierarchy of deep convolutional neural
networks. During feature enhancement, intra-scale enhancement fuses
same-resolution feature maps from multiple layers together via local
self-attention and cross-scale enhancement hallucinates higher-resolution
feature maps along the top-down hierarchy. Besides, we learn complementary
matching details at different scales thus the overall matching score is refined
by features of different semantic levels gradually. Our multi-scale matching
network can be trained end-to-end easily with few additional learnable
parameters. Experimental results demonstrate that the proposed method achieves
state-of-the-art performance on three popular benchmarks with high
computational efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Dongyang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1"&gt;Ziyang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1"&gt;Zhenghao Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1"&gt;Gangming Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1"&gt;Weifeng Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yizhou Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CrossFormer: A Versatile Vision Transformer Based on Cross-scale Attention. (arXiv:2108.00154v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00154</id>
        <link href="http://arxiv.org/abs/2108.00154"/>
        <updated>2021-08-03T02:06:30.495Z</updated>
        <summary type="html"><![CDATA[Transformers have made much progress in dealing with visual tasks. However,
existing vision transformers still do not possess an ability that is important
to visual input: building the attention among features of different scales. The
reasons for this problem are two-fold: (1) Input embeddings of each layer are
equal-scale without cross-scale features; (2) Some vision transformers
sacrifice the small-scale features of embeddings to lower the cost of the
self-attention module. To make up this defect, we propose Cross-scale Embedding
Layer (CEL) and Long Short Distance Attention (LSDA). In particular, CEL blends
each embedding with multiple patches of different scales, providing the model
with cross-scale embeddings. LSDA splits the self-attention module into a
short-distance and long-distance one, also lowering the cost but keeping both
small-scale and large-scale features in embeddings. Through these two designs,
we achieve cross-scale attention. Besides, we propose dynamic position bias for
vision transformers to make the popular relative position bias apply to
variable-sized images. Based on these proposed modules, we construct our vision
architecture called CrossFormer. Experiments show that CrossFormer outperforms
other transformers on several representative visual tasks, especially object
detection and segmentation. The code has been released:
https://github.com/cheerss/CrossFormer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenxiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1"&gt;Lu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Long Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1"&gt;Deng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiaofei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On The State of Data In Computer Vision: Human Annotations Remain Indispensable for Developing Deep Learning Models. (arXiv:2108.00114v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00114</id>
        <link href="http://arxiv.org/abs/2108.00114"/>
        <updated>2021-08-03T02:06:30.486Z</updated>
        <summary type="html"><![CDATA[High-quality labeled datasets play a crucial role in fueling the development
of machine learning (ML), and in particular the development of deep learning
(DL). However, since the emergence of the ImageNet dataset and the AlexNet
model in 2012, the size of new open-source labeled vision datasets has remained
roughly constant. Consequently, only a minority of publications in the computer
vision community tackle supervised learning on datasets that are orders of
magnitude larger than Imagenet. In this paper, we survey computer vision
research domains that study the effects of such large datasets on model
performance across different vision tasks. We summarize the community's current
understanding of those effects, and highlight some open questions related to
training with massive datasets. In particular, we tackle: (a) The largest
datasets currently used in computer vision research and the interesting
takeaways from training on such datasets; (b) The effectiveness of pre-training
on large datasets; (c) Recent advancements and hurdles facing synthetic
datasets; (d) An overview of double descent and sample non-monotonicity
phenomena; and finally, (e) A brief discussion of lifelong/continual learning
and how it fares compared to learning from huge labeled datasets in an offline
setting. Overall, our findings are that research on optimization for deep
learning focuses on perfecting the training routine and thus making DL models
less data hungry, while research on synthetic datasets aims to offset the cost
of data labeling. However, for the time being, acquiring non-synthetic labeled
data remains indispensable to boost performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Emam_Z/0/1/0/all/0/1"&gt;Zeyad Emam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kondrich_A/0/1/0/all/0/1"&gt;Andrew Kondrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harrison_S/0/1/0/all/0/1"&gt;Sasha Harrison&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lau_F/0/1/0/all/0/1"&gt;Felix Lau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yushi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1"&gt;Aerin Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Branson_E/0/1/0/all/0/1"&gt;Elliot Branson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Margin-Aware Intra-Class Novelty Identification for Medical Images. (arXiv:2108.00117v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00117</id>
        <link href="http://arxiv.org/abs/2108.00117"/>
        <updated>2021-08-03T02:06:30.477Z</updated>
        <summary type="html"><![CDATA[Traditional anomaly detection methods focus on detecting inter-class
variations while medical image novelty identification is inherently an
intra-class detection problem. For example, a machine learning model trained
with normal chest X-ray and common lung abnormalities, is expected to discover
and flag idiopathic pulmonary fibrosis which a rare lung disease and unseen by
the model during training. The nuances from intra-class variations and lack of
relevant training data in medical image analysis pose great challenges for
existing anomaly detection methods. To tackle the challenges, we propose a
hybrid model - Transformation-based Embedding learning for Novelty Detection
(TEND) which without any out-of-distribution training data, performs novelty
identification by combining both autoencoder-based and classifier-based method.
With a pre-trained autoencoder as image feature extractor, TEND learns to
discriminate the feature embeddings of in-distribution data from the
transformed counterparts as fake out-of-distribution inputs. To enhance the
separation, a distance objective is optimized to enforce a margin between the
two classes. Extensive experimental results on both natural image datasets and
medical image datasets are presented and our method out-performs
state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xiaoyuan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gichoya_J/0/1/0/all/0/1"&gt;Judy Wawira Gichoya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Purkayastha_S/0/1/0/all/0/1"&gt;Saptarshi Purkayastha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_I/0/1/0/all/0/1"&gt;Imon Banerjee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pose-Guided Feature Learning with Knowledge Distillation for Occluded Person Re-Identification. (arXiv:2108.00139v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00139</id>
        <link href="http://arxiv.org/abs/2108.00139"/>
        <updated>2021-08-03T02:06:30.442Z</updated>
        <summary type="html"><![CDATA[Occluded person re-identification (ReID) aims to match person images with
occlusion. It is fundamentally challenging because of the serious occlusion
which aggravates the misalignment problem between images. At the cost of
incorporating a pose estimator, many works introduce pose information to
alleviate the misalignment in both training and testing. To achieve high
accuracy while preserving low inference complexity, we propose a network named
Pose-Guided Feature Learning with Knowledge Distillation (PGFL-KD), where the
pose information is exploited to regularize the learning of semantics aligned
features but is discarded in testing. PGFL-KD consists of a main branch (MB),
and two pose-guided branches, \ieno, a foreground-enhanced branch (FEB), and a
body part semantics aligned branch (SAB). The FEB intends to emphasise the
features of visible body parts while excluding the interference of obstructions
and background (\ieno, foreground feature alignment). The SAB encourages
different channel groups to focus on different body parts to have body part
semantics aligned representation. To get rid of the dependency on pose
information when testing, we regularize the MB to learn the merits of the FEB
and SAB through knowledge distillation and interaction-based training.
Extensive experiments on occluded, partial, and holistic ReID tasks show the
effectiveness of our proposed network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1"&gt;Kecheng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1"&gt;Cuiling Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenjun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiawei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhizheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zheng-Jun Zha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional Bures Metric for Domain Adaptation. (arXiv:2108.00302v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00302</id>
        <link href="http://arxiv.org/abs/2108.00302"/>
        <updated>2021-08-03T02:06:30.434Z</updated>
        <summary type="html"><![CDATA[As a vital problem in classification-oriented transfer, unsupervised domain
adaptation (UDA) has attracted widespread attention in recent years. Previous
UDA methods assume the marginal distributions of different domains are shifted
while ignoring the discriminant information in the label distributions. This
leads to classification performance degeneration in real applications. In this
work, we focus on the conditional distribution shift problem which is of great
concern to current conditional invariant models. We aim to seek a kernel
covariance embedding for conditional distribution which remains yet unexplored.
Theoretically, we propose the Conditional Kernel Bures (CKB) metric for
characterizing conditional distribution discrepancy, and derive an empirical
estimation for the CKB metric without introducing the implicit kernel feature
map. It provides an interpretable approach to understand the knowledge transfer
mechanism. The established consistency theory of the empirical estimation
provides a theoretical guarantee for convergence. A conditional distribution
matching network is proposed to learn the conditional invariant and
discriminative features for UDA. Extensive experiments and analysis show the
superiority of our proposed model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;You-Wei Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_C/0/1/0/all/0/1"&gt;Chuan-Xian Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Dynamic 3D Spontaneous Micro-expression Database: Establishment and Evaluation. (arXiv:2108.00166v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00166</id>
        <link href="http://arxiv.org/abs/2108.00166"/>
        <updated>2021-08-03T02:06:30.427Z</updated>
        <summary type="html"><![CDATA[Micro-expressions are spontaneous, unconscious facial movements that show
people's true inner emotions and have great potential in related fields of
psychological testing. Since the face is a 3D deformation object, the
occurrence of an expression can arouse spatial deformation of the face, but
limited by the available databases are 2D videos, which lack the description of
3D spatial information of micro-expressions. Therefore, we proposed a new
micro-expression database containing 2D video sequences and 3D point clouds
sequences. The database includes 259 micro-expressions sequences, and these
samples were classified using the objective method based on facial action
coding system, as well as the non-objective method that combines video contents
and participants' self-reports. We extracted facial 2D and 3D features using
local binary patterns on three orthogonal planes and curvature descriptors,
respectively, and performed baseline evaluations of the two features and their
fusion results with leave-one-subject-out(LOSO) and 10-fold cross-validation
methods. The best fusion performances were 58.84% and 73.03% for non-objective
classification and 66.36% and 77.42% for objective classification, both of
which have improved performance compared to using LBP-TOP features only.The
database offers original and cropped micro-expression samples, which will
facilitate the exploration and research on 3D Spatio-temporal features of
micro-expressions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fengping Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1"&gt;Chun Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_D/0/1/0/all/0/1"&gt;Danmin Miao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis. (arXiv:2107.08264v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08264</id>
        <link href="http://arxiv.org/abs/2107.08264"/>
        <updated>2021-08-03T02:06:30.405Z</updated>
        <summary type="html"><![CDATA[Multimodal sentiment analysis aims to recognize people's attitudes from
multiple communication channels such as verbal content (i.e., text), voice, and
facial expressions. It has become a vibrant and important research topic in
natural language processing. Much research focuses on modeling the complex
intra- and inter-modal interactions between different communication channels.
However, current multimodal models with strong performance are often
deep-learning-based techniques and work like black boxes. It is not clear how
models utilize multimodal information for sentiment predictions. Despite recent
advances in techniques for enhancing the explainability of machine learning
models, they often target unimodal scenarios (e.g., images, sentences), and
little research has been done on explaining multimodal models. In this paper,
we present an interactive visual analytics system, M2Lens, to visualize and
explain multimodal models for sentiment analysis. M2Lens provides explanations
on intra- and inter-modal interactions at the global, subset, and local levels.
Specifically, it summarizes the influence of three typical interaction types
(i.e., dominance, complement, and conflict) on the model predictions. Moreover,
M2Lens identifies frequent and influential multimodal features and supports the
multi-faceted exploration of model behaviors from language, acoustic, and
visual modalities. Through two case studies and expert interviews, we
demonstrate our system can help users gain deep insights into the multimodal
models for sentiment analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xingbo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jianben He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1"&gt;Zhihua Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Muqiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1"&gt;Huamin Qu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thermal Image Super-Resolution Using Second-Order Channel Attention with Varying Receptive Fields. (arXiv:2108.00094v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.00094</id>
        <link href="http://arxiv.org/abs/2108.00094"/>
        <updated>2021-08-03T02:06:30.395Z</updated>
        <summary type="html"><![CDATA[Thermal images model the long-infrared range of the electromagnetic spectrum
and provide meaningful information even when there is no visible illumination.
Yet, unlike imagery that represents radiation from the visible continuum,
infrared images are inherently low-resolution due to hardware constraints. The
restoration of thermal images is critical for applications that involve safety,
search and rescue, and military operations. In this paper, we introduce a
system to efficiently reconstruct thermal images. Specifically, we explore how
to effectively attend to contrasting receptive fields (RFs) where increasing
the RFs of a network can be computationally expensive. For this purpose, we
introduce a deep attention to varying receptive fields network (AVRFN). We
supply a gated convolutional layer with higher-order information extracted from
disparate RFs, whereby an RF is parameterized by a dilation rate. In this way,
the dilation rate can be tuned to use fewer parameters thus increasing the
efficacy of AVRFN. Our experimental results show an improvement over the state
of the art when compared against competing thermal image super-resolution
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gutierrez_N/0/1/0/all/0/1"&gt;Nolan B. Gutierrez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Beksi_W/0/1/0/all/0/1"&gt;William J. Beksi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[T$_k$ML-AP: Adversarial Attacks to Top-$k$ Multi-Label Learning. (arXiv:2108.00146v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00146</id>
        <link href="http://arxiv.org/abs/2108.00146"/>
        <updated>2021-08-03T02:06:30.388Z</updated>
        <summary type="html"><![CDATA[Top-$k$ multi-label learning, which returns the top-$k$ predicted labels from
an input, has many practical applications such as image annotation, document
analysis, and web search engine. However, the vulnerabilities of such
algorithms with regards to dedicated adversarial perturbation attacks have not
been extensively studied previously. In this work, we develop methods to create
adversarial perturbations that can be used to attack top-$k$ multi-label
learning-based image annotation systems (TkML-AP). Our methods explicitly
consider the top-$k$ ranking relation and are based on novel loss functions.
Experimental evaluations on large-scale benchmark datasets including PASCAL VOC
and MS COCO demonstrate the effectiveness of our methods in reducing the
performance of state-of-the-art top-$k$ multi-label learning methods, under
both untargeted and targeted attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shu Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1"&gt;Lipeng Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1"&gt;Siwei Lyu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiplex Graph Networks for Multimodal Brain Network Analysis. (arXiv:2108.00158v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00158</id>
        <link href="http://arxiv.org/abs/2108.00158"/>
        <updated>2021-08-03T02:06:30.381Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose MGNet, a simple and effective multiplex graph
convolutional network (GCN) model for multimodal brain network analysis. The
proposed method integrates tensor representation into the multiplex GCN model
to extract the latent structures of a set of multimodal brain networks, which
allows an intuitive 'grasping' of the common space for multimodal data.
Multimodal representations are then generated with multiplex GCNs to capture
specific graph structures. We conduct classification task on two challenging
real-world datasets (HIV and Bipolar disorder), and the proposed MGNet
demonstrates state-of-the-art performance compared to competitive benchmark
methods. Apart from objective evaluations, this study may bear special
significance upon network theory to the understanding of human connectome in
different modalities. The code is available at
https://github.com/ZhaomingKong/MGNets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1"&gt;Zhaoming Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lichao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Hao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_L/0/1/0/all/0/1"&gt;Liang Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lifang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scene Inference for Object Illumination Editing. (arXiv:2108.00150v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00150</id>
        <link href="http://arxiv.org/abs/2108.00150"/>
        <updated>2021-08-03T02:06:30.360Z</updated>
        <summary type="html"><![CDATA[The seamless illumination integration between a foreground object and a
background scene is an important but challenging task in computer vision and
augmented reality community. However, to our knowledge, there is no publicly
available high-quality dataset that meets the illumination seamless integration
task, which greatly hinders the development of this research direction. To this
end, we apply a physically-based rendering method to create a large-scale,
high-quality dataset, named IH dataset, which provides rich illumination
information for seamless illumination integration task. In addition, we propose
a deep learning-based SI-GAN method, a multi-task collaborative network, which
makes full use of the multi-scale attention mechanism and adversarial learning
strategy to directly infer mapping relationship between the inserted foreground
object and corresponding background environment, and edit object illumination
according to the proposed illumination exchange mechanism in parallel network.
By this means, we can achieve the seamless illumination integration without
explicit estimation of 3D geometric information. Comprehensive experiments on
both our dataset and real-world images collected from the Internet show that
our proposed SI-GAN provides a practical and effective solution for image-based
object illumination editing, and validate the superiority of our method against
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1"&gt;Zhongyun Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1"&gt;Chengjiang Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1"&gt;Gang Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Daquan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuanzhen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiaming Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Chunxia Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MTVR: Multilingual Moment Retrieval in Videos. (arXiv:2108.00061v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00061</id>
        <link href="http://arxiv.org/abs/2108.00061"/>
        <updated>2021-08-03T02:06:30.354Z</updated>
        <summary type="html"><![CDATA[We introduce mTVR, a large-scale multilingual video moment retrieval dataset,
containing 218K English and Chinese queries from 21.8K TV show video clips. The
dataset is collected by extending the popular TVR dataset (in English) with
paired Chinese queries and subtitles. Compared to existing moment retrieval
datasets, mTVR is multilingual, larger, and comes with diverse annotations. We
further propose mXML, a multilingual moment retrieval model that learns and
operates on data from both languages, via encoder parameter sharing and
language neighborhood constraints. We demonstrate the effectiveness of mXML on
the newly collected MTVR dataset, where mXML outperforms strong monolingual
baselines while using fewer parameters. In addition, we also provide detailed
dataset analyses and model ablations. Data and code are publicly available at
https://github.com/jayleicn/mTVRetrieval]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1"&gt;Tamara L. Berg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object-aware Contrastive Learning for Debiased Scene Representation. (arXiv:2108.00049v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00049</id>
        <link href="http://arxiv.org/abs/2108.00049"/>
        <updated>2021-08-03T02:06:30.347Z</updated>
        <summary type="html"><![CDATA[Contrastive self-supervised learning has shown impressive results in learning
visual representations from unlabeled images by enforcing invariance against
different data augmentations. However, the learned representations are often
contextually biased to the spurious scene correlations of different objects or
object and background, which may harm their generalization on the downstream
tasks. To tackle the issue, we develop a novel object-aware contrastive
learning framework that first (a) localizes objects in a self-supervised manner
and then (b) debias scene correlations via appropriate data augmentations
considering the inferred object locations. For (a), we propose the contrastive
class activation map (ContraCAM), which finds the most discriminative regions
(e.g., objects) in the image compared to the other images using the
contrastively trained models. We further improve the ContraCAM to detect
multiple objects and entire shapes via an iterative refinement procedure. For
(b), we introduce two data augmentations based on ContraCAM, object-aware
random crop and background mixup, which reduce contextual and background biases
during contrastive self-supervised learning, respectively. Our experiments
demonstrate the effectiveness of our representation learning framework,
particularly when trained under multi-object images or evaluated under the
background (and distribution) shifted images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1"&gt;Sangwoo Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1"&gt;Hyunwoo Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1"&gt;Kihyuk Sohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chun-Liang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1"&gt;Jinwoo Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MultiCite: Modeling realistic citations requires moving beyond the single-sentence single-label setting. (arXiv:2107.00414v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00414</id>
        <link href="http://arxiv.org/abs/2107.00414"/>
        <updated>2021-08-03T02:06:30.299Z</updated>
        <summary type="html"><![CDATA[Citation context analysis (CCA) is an important task in natural language
processing that studies how and why scholars discuss each others' work. Despite
decades of study, traditional frameworks for CCA have largely relied on
overly-simplistic assumptions of how authors cite, which ignore several
important phenomena. For instance, scholarly papers often contain rich
discussions of cited work that span multiple sentences and express multiple
intents concurrently. Yet, CCA is typically approached as a single-sentence,
single-label classification task, and thus existing datasets fail to capture
this interesting discourse. In our work, we address this research gap by
proposing a novel framework for CCA as a document-level context extraction and
labeling task. We release MultiCite, a new dataset of 12,653 citation contexts
from over 1,200 computational linguistics papers. Not only is it the largest
collection of expert-annotated citation contexts to-date, MultiCite contains
multi-sentence, multi-label citation contexts within full paper texts. Finally,
we demonstrate how our dataset, while still usable for training classic CCA
models, also supports the development of new types of models for CCA beyond
fixed-width text classification. We release our code and dataset at
https://github.com/allenai/multicite.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1"&gt;Anne Lauscher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_B/0/1/0/all/0/1"&gt;Brandon Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuehl_B/0/1/0/all/0/1"&gt;Bailey Kuehl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johnson_S/0/1/0/all/0/1"&gt;Sophie Johnson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1"&gt;David Jurgens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1"&gt;Arman Cohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1"&gt;Kyle Lo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exemplars-guided Empathetic Response Generation Controlled by the Elements of Human Communication. (arXiv:2106.11791v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11791</id>
        <link href="http://arxiv.org/abs/2106.11791"/>
        <updated>2021-08-03T02:06:30.292Z</updated>
        <summary type="html"><![CDATA[The majority of existing methods for empathetic response generation rely on
the emotion of the context to generate empathetic responses. However, empathy
is much more than generating responses with an appropriate emotion. It also
often entails subtle expressions of understanding and personal resonance with
the situation of the other interlocutor. Unfortunately, such qualities are
difficult to quantify and the datasets lack the relevant annotations. To
address this issue, in this paper we propose an approach that relies on
exemplars to cue the generative model on fine stylistic properties that signal
empathy to the interlocutor. To this end, we employ dense passage retrieval to
extract relevant exemplary responses from the training set. Three elements of
human communication -- emotional presence, interpretation, and exploration, and
sentiment are additionally introduced using synthetic labels to guide the
generation towards empathy. The human evaluation is also extended by these
elements of human communication. We empirically show that these approaches
yield significant improvements in empathetic response quality in terms of both
automated and human-evaluated metrics. The implementation is available at
https://github.com/declare-lab/exemplary-empathy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1"&gt;Navonil Majumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1"&gt;Deepanway Ghosal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hazarika_D/0/1/0/all/0/1"&gt;Devamanyu Hazarika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1"&gt;Alexander Gelbukh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1"&gt;Rada Mihalcea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1"&gt;Soujanya Poria&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New Semi-supervised Learning Benchmark for Classifying View and Diagnosing Aortic Stenosis from Echocardiograms. (arXiv:2108.00080v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00080</id>
        <link href="http://arxiv.org/abs/2108.00080"/>
        <updated>2021-08-03T02:06:30.267Z</updated>
        <summary type="html"><![CDATA[Semi-supervised image classification has shown substantial progress in
learning from limited labeled data, but recent advances remain largely untested
for clinical applications. Motivated by the urgent need to improve timely
diagnosis of life-threatening heart conditions, especially aortic stenosis, we
develop a benchmark dataset to assess semi-supervised approaches to two tasks
relevant to cardiac ultrasound (echocardiogram) interpretation: view
classification and disease severity classification. We find that a
state-of-the-art method called MixMatch achieves promising gains in heldout
accuracy on both tasks, learning from a large volume of truly unlabeled images
as well as a labeled set collected at great expense to achieve better
performance than is possible with the labeled set alone. We further pursue
patient-level diagnosis prediction, which requires aggregating across hundreds
of images of diverse view types, most of which are irrelevant, to make a
coherent prediction. The best patient-level performance is achieved by new
methods that prioritize diagnosis predictions from images that are predicted to
be clinically-relevant views and transfer knowledge from the view task to the
diagnosis task. We hope our released Tufts Medical Echocardiogram Dataset and
evaluation framework inspire further improvements in multi-task semi-supervised
learning for clinical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhe Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1"&gt;Gary Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wessler_B/0/1/0/all/0/1"&gt;Benjamin Wessler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1"&gt;Michael C. Hughes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manifold-Inspired Single Image Interpolation. (arXiv:2108.00145v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00145</id>
        <link href="http://arxiv.org/abs/2108.00145"/>
        <updated>2021-08-03T02:06:30.254Z</updated>
        <summary type="html"><![CDATA[Manifold models consider natural-image patches to be on a low-dimensional
manifold embedded in a high dimensional state space and each patch and its
similar patches to approximately lie on a linear affine subspace. Manifold
models are closely related to semi-local similarity, a well-known property of
natural images, referring to that for most natural-image patches, several
similar patches can be found in its spatial neighborhood. Many approaches to
single image interpolation use manifold models to exploit semi-local similarity
by two mutually exclusive parts: i) searching each target patch's similar
patches and ii) operating on the searched similar patches, the target patch and
the measured input pixels to estimate the target patch. Unfortunately, aliasing
in the input image makes it challenging for both parts. A very few works
explicitly deal with those challenges and only ad-hoc solutions are proposed.

To overcome the challenge in the first part, we propose a carefully-designed
adaptive technique to remove aliasing in severely aliased regions, which cannot
be removed from traditional techniques. This technique enables reliable
identification of similar patches even in the presence of strong aliasing. To
overcome the challenge in the second part, we propose to use the
aliasing-removed image to guide the initialization of the interpolated image
and develop a progressive scheme to refine the interpolated image based on
manifold models. Experimental results demonstrate that our approach
reconstructs edges with both smoothness along contours and sharpness across
profiles, and achieves an average Peak Signal-to-Noise Ratio (PSNR)
significantly higher than existing model-based approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1"&gt;Lantao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kuida Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orchard_M/0/1/0/all/0/1"&gt;Michael T. Orchard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparing object recognition in humans and deep convolutional neural networks -- An eye tracking study. (arXiv:2108.00107v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00107</id>
        <link href="http://arxiv.org/abs/2108.00107"/>
        <updated>2021-08-03T02:06:30.233Z</updated>
        <summary type="html"><![CDATA[Deep convolutional neural networks (DCNNs) and the ventral visual pathway
share vast architectural and functional similarities in visual challenges such
as object recognition. Recent insights have demonstrated that both hierarchical
cascades can be compared in terms of both exerted behavior and underlying
activation. However, these approaches ignore key differences in spatial
priorities of information processing. In this proof-of-concept study, we
demonstrate a comparison of human observers (N = 45) and three feedforward
DCNNs through eye tracking and saliency maps. The results reveal fundamentally
different resolutions in both visualization methods that need to be considered
for an insightful comparison. Moreover, we provide evidence that a DCNN with
biologically plausible receptive field sizes called vNet reveals higher
agreement with human viewing behavior as contrasted with a standard ResNet
architecture. We find that image-specific factors such as category, animacy,
arousal, and valence have a direct link to the agreement of spatial object
recognition priorities in humans and DCNNs, while other measures such as
difficulty and general image properties do not. With this approach, we try to
open up new perspectives at the intersection of biological and computer vision
research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dyck_L/0/1/0/all/0/1"&gt;Leonard E. van Dyck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwitt_R/0/1/0/all/0/1"&gt;Roland Kwitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Denzler_S/0/1/0/all/0/1"&gt;Sebastian J. Denzler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gruber_W/0/1/0/all/0/1"&gt;Walter R. Gruber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Head Self-Attention via Vision Transformer for Zero-Shot Learning. (arXiv:2108.00045v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00045</id>
        <link href="http://arxiv.org/abs/2108.00045"/>
        <updated>2021-08-03T02:06:30.226Z</updated>
        <summary type="html"><![CDATA[Zero-Shot Learning (ZSL) aims to recognise unseen object classes, which are
not observed during the training phase. The existing body of works on ZSL
mostly relies on pretrained visual features and lacks the explicit attribute
localisation mechanism on images. In this work, we propose an attention-based
model in the problem settings of ZSL to learn attributes useful for unseen
class recognition. Our method uses an attention mechanism adapted from Vision
Transformer to capture and learn discriminative attributes by splitting images
into small patches. We conduct experiments on three popular ZSL benchmarks
(i.e., AWA2, CUB and SUN) and set new state-of-the-art harmonic mean results
{on all the three datasets}, which illustrate the effectiveness of our proposed
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alamri_F/0/1/0/all/0/1"&gt;Faisal Alamri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1"&gt;Anjan Dutta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[i-Pulse: A NLP based novel approach for employee engagement in logistics organization. (arXiv:2106.07341v1 [cs.SI] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2106.07341</id>
        <link href="http://arxiv.org/abs/2106.07341"/>
        <updated>2021-08-03T02:06:30.172Z</updated>
        <summary type="html"><![CDATA[Although most logistics and freight forwarding organizations, in one way or
another, claim to have core values. The engagement of employees is a vast
structure that affects almost every part of the company's core environmental
values. There is little theoretical knowledge about the relationship between
firms and the engagement of employees. Based on research literature, this paper
aims to provide a novel approach for insight around employee engagement in a
logistics organization by implementing deep natural language processing
concepts. The artificial intelligence-enabled solution named Intelligent Pulse
(I-Pulse) can evaluate hundreds and thousands of pulse survey comments and
provides the actionable insights and gist of employee feedback. I-Pulse allows
the stakeholders to think in new ways in their organization, helping them to
have a powerful influence on employee engagement, retention, and efficiency.
This study is of corresponding interest to researchers and practitioners.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garg_R/0/1/0/all/0/1"&gt;Rachit Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiwelekar_A/0/1/0/all/0/1"&gt;Arvind W Kiwelekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Netak_L/0/1/0/all/0/1"&gt;Laxman D Netak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghodake_A/0/1/0/all/0/1"&gt;Akshay Ghodake&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HLE-UPC at SemEval-2021 Task 5: Multi-Depth DistilBERT for Toxic Spans Detection. (arXiv:2104.00639v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00639</id>
        <link href="http://arxiv.org/abs/2104.00639"/>
        <updated>2021-08-03T02:06:30.151Z</updated>
        <summary type="html"><![CDATA[This paper presents our submission to SemEval-2021 Task 5: Toxic Spans
Detection. The purpose of this task is to detect the spans that make a text
toxic, which is a complex labour for several reasons. Firstly, because of the
intrinsic subjectivity of toxicity, and secondly, due to toxicity not always
coming from single words like insults or offends, but sometimes from whole
expressions formed by words that may not be toxic individually. Following this
idea of focusing on both single words and multi-word expressions, we study the
impact of using a multi-depth DistilBERT model, which uses embeddings from
different layers to estimate the final per-token toxicity. Our quantitative
results show that using information from multiple depths boosts the performance
of the model. Finally, we also analyze our best model qualitatively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Palliser_Sans_R/0/1/0/all/0/1"&gt;Rafel Palliser-Sans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rial_Farras_A/0/1/0/all/0/1"&gt;Albert Rial-Farr&amp;#xe0;s&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controlling Weather Field Synthesis Using Variational Autoencoders. (arXiv:2108.00048v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00048</id>
        <link href="http://arxiv.org/abs/2108.00048"/>
        <updated>2021-08-03T02:06:30.145Z</updated>
        <summary type="html"><![CDATA[One of the consequences of climate change is anobserved increase in the
frequency of extreme cli-mate events. That poses a challenge for
weatherforecast and generation algorithms, which learnfrom historical data but
should embed an often un-certain bias to create correct scenarios. This
paperinvestigates how mapping climate data to a knowndistribution using
variational autoencoders mighthelp explore such biases and control the
synthesisof weather fields towards more extreme climatescenarios. We
experimented using a monsoon-affected precipitation dataset from southwest
In-dia, which should give a roughly stable pattern ofrainy days and ease our
investigation. We reportcompelling results showing that mapping complexweather
data to a known distribution implementsan efficient control for weather field
synthesis to-wards more (or less) extreme scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1"&gt;Dario Augusto Borges Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_J/0/1/0/all/0/1"&gt;Jorge Guevara Diaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zadrozny_B/0/1/0/all/0/1"&gt;Bianca Zadrozny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watson_C/0/1/0/all/0/1"&gt;Campbell Watson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Transformer for Efficient Machine Translation on Embedded Devices. (arXiv:2107.08199v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08199</id>
        <link href="http://arxiv.org/abs/2107.08199"/>
        <updated>2021-08-03T02:06:30.134Z</updated>
        <summary type="html"><![CDATA[The Transformer architecture is widely used for machine translation tasks.
However, its resource-intensive nature makes it challenging to implement on
constrained embedded devices, particularly where available hardware resources
can vary at run-time. We propose a dynamic machine translation model that
scales the Transformer architecture based on the available resources at any
particular time. The proposed approach, 'Dynamic-HAT', uses a HAT
SuperTransformer as the backbone to search for SubTransformers with different
accuracy-latency trade-offs at design time. The optimal SubTransformers are
sampled from the SuperTransformer at run-time, depending on latency
constraints. The Dynamic-HAT is tested on the Jetson Nano and the approach uses
inherited SubTransformers sampled directly from the SuperTransformer with a
switching time of <1s. Using inherited SubTransformers results in a BLEU score
loss of <1.5% because the SubTransformer configuration is not retrained from
scratch after sampling. However, to recover this loss in performance, the
dimensions of the design space can be reduced to tailor it to a family of
target hardware. The new reduced design space results in a BLEU score increase
of approximately 1% for sub-optimal models from the original design space, with
a wide range for performance scaling between 0.356s - 1.526s for the GPU and
2.9s - 7.31s for the CPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parry_H/0/1/0/all/0/1"&gt;Hishan Parry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xun_L/0/1/0/all/0/1"&gt;Lei Xun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabet_A/0/1/0/all/0/1"&gt;Amin Sabet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1"&gt;Jia Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1"&gt;Jonathon Hare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merrett_G/0/1/0/all/0/1"&gt;Geoff V. Merrett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatial Language Understanding for Object Search in Partially Observed City-scale Environments. (arXiv:2012.02705v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02705</id>
        <link href="http://arxiv.org/abs/2012.02705"/>
        <updated>2021-08-03T02:06:30.126Z</updated>
        <summary type="html"><![CDATA[Humans use spatial language to naturally describe object locations and their
relations. Interpreting spatial language not only adds a perceptual modality
for robots, but also reduces the barrier of interfacing with humans. Previous
work primarily considers spatial language as goal specification for instruction
following tasks in fully observable domains, often paired with reference paths
for reward-based learning. However, spatial language is inherently subjective
and potentially ambiguous or misleading. Hence, in this paper, we consider
spatial language as a form of stochastic observation. We propose SLOOP (Spatial
Language Object-Oriented POMDP), a new framework for partially observable
decision making with a probabilistic observation model for spatial language. We
apply SLOOP to object search in city-scale environments. To interpret
ambiguous, context-dependent prepositions (e.g. front), we design a simple
convolutional neural network that predicts the language provider's latent frame
of reference (FoR) given the environment context. Search strategies are
computed via an online POMDP planner based on Monte Carlo Tree Search.
Evaluation based on crowdsourced language data, collected over areas of five
cities in OpenStreetMap, shows that our approach achieves faster search and
higher success rate compared to baselines, with a wider margin as the spatial
language becomes more complex. Finally, we demonstrate the proposed method in
AirSim, a realistic simulator where a drone is tasked to find cars in a
neighborhood environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1"&gt;Kaiyu Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bayazit_D/0/1/0/all/0/1"&gt;Deniz Bayazit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathew_R/0/1/0/all/0/1"&gt;Rebecca Mathew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1"&gt;Ellie Pavlick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tellex_S/0/1/0/all/0/1"&gt;Stefanie Tellex&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COfEE: A Comprehensive Ontology for Event Extraction from text, with an online annotation tool. (arXiv:2107.10326v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.10326</id>
        <link href="http://arxiv.org/abs/2107.10326"/>
        <updated>2021-08-03T02:06:30.107Z</updated>
        <summary type="html"><![CDATA[Data is published on the web over time in great volumes, but majority of the
data is unstructured, making it hard to understand and difficult to interpret.
Information Extraction (IE) methods extract structured information from
unstructured data. One of the challenging IE tasks is Event Extraction (EE)
which seeks to derive information about specific incidents and their actors
from the text. EE is useful in many domains such as building a knowledge base,
information retrieval, summarization and online monitoring systems. In the past
decades, some event ontologies like ACE, CAMEO and ICEWS were developed to
define event forms, actors and dimensions of events observed in the text. These
event ontologies still have some shortcomings such as covering only a few
topics like political events, having inflexible structure in defining argument
roles, lack of analytical dimensions, and complexity in choosing event
sub-types. To address these concerns, we propose an event ontology, namely
COfEE, that incorporates both expert domain knowledge, previous ontologies and
a data-driven approach for identifying events from text. COfEE consists of two
hierarchy levels (event types and event sub-types) that include new categories
relating to environmental issues, cyberspace, criminal activity and natural
disasters which need to be monitored instantly. Also, dynamic roles according
to each event sub-type are defined to capture various dimensions of events. In
a follow-up experiment, the proposed ontology is evaluated on Wikipedia events,
and it is shown to be general and comprehensive. Moreover, in order to
facilitate the preparation of gold-standard data for event extraction, a
language-independent online tool is presented based on COfEE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balali_A/0/1/0/all/0/1"&gt;Ali Balali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asadpour_M/0/1/0/all/0/1"&gt;Masoud Asadpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jafari_S/0/1/0/all/0/1"&gt;Seyed Hossein Jafari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data. (arXiv:2106.08977v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08977</id>
        <link href="http://arxiv.org/abs/2106.08977"/>
        <updated>2021-08-03T02:06:30.097Z</updated>
        <summary type="html"><![CDATA[Weak supervision has shown promising results in many natural language
processing tasks, such as Named Entity Recognition (NER). Existing work mainly
focuses on learning deep NER models only with weak supervision, i.e., without
any human annotation, and shows that by merely using weakly labeled data, one
can achieve good performance, though still underperforms fully supervised NER
with manually/strongly labeled data. In this paper, we consider a more
practical scenario, where we have both a small amount of strongly labeled data
and a large amount of weakly labeled data. Unfortunately, we observe that
weakly labeled data does not necessarily improve, or even deteriorate the model
performance (due to the extensive noise in the weak labels) when we train deep
NER models over a simple or weighted combination of the strongly labeled and
weakly labeled data. To address this issue, we propose a new multi-stage
computational framework -- NEEDLE with three essential ingredients: (1) weak
label completion, (2) noise-aware loss function, and (3) final fine-tuning over
the strongly labeled data. Through experiments on E-commerce query NER and
Biomedical NER, we demonstrate that NEEDLE can effectively suppress the noise
of the weak labels and outperforms existing methods. In particular, we achieve
new SOTA F1-scores on 3 Biomedical NER datasets: BC5CDR-chem 93.74,
BC5CDR-disease 90.69, NCBI-disease 92.28.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haoming Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Danqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1"&gt;Tianyu Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1"&gt;Bing Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tuo Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scientia Potentia Est -- On the Role of Knowledge in Computational Argumentation. (arXiv:2107.00281v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00281</id>
        <link href="http://arxiv.org/abs/2107.00281"/>
        <updated>2021-08-03T02:06:30.076Z</updated>
        <summary type="html"><![CDATA[Despite extensive research efforts in the recent years, computational
modeling of argumentation remains one of the most challenging areas of natural
language processing (NLP). This is primarily due to inherent complexity of the
cognitive processes behind human argumentation, which commonly combine and
integrate plethora of different types of knowledge, requiring from
computational models capabilities that are far beyond what is needed for most
other (i.e., simpler) natural language understanding tasks. The existing large
body of work on mining, assessing, generating, and reasoning over arguments
largely acknowledges that much more common sense and world knowledge needs to
be integrated into computational models that would accurately model
argumentation. A systematic overview and organization of the types of knowledge
introduced in existing models of computational argumentation (CA) is, however,
missing and this hinders targeted progress in the field. In this survey paper,
we fill this gap by (1) proposing a pyramid of types of knowledge required in
CA tasks, (2) analysing the state of the art with respect to the reliance and
exploitation of these types of knowledge, for each of the for main research
areas in CA, and (3) outlining and discussing directions for future research
efforts in CA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1"&gt;Anne Lauscher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wachsmuth_H/0/1/0/all/0/1"&gt;Henning Wachsmuth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1"&gt;Iryna Gurevych&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1"&gt;Goran Glava&amp;#x161;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LICHEE: Improving Language Model Pre-training with Multi-grained Tokenization. (arXiv:2108.00801v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00801</id>
        <link href="http://arxiv.org/abs/2108.00801"/>
        <updated>2021-08-03T02:06:30.069Z</updated>
        <summary type="html"><![CDATA[Language model pre-training based on large corpora has achieved tremendous
success in terms of constructing enriched contextual representations and has
led to significant performance gains on a diverse range of Natural Language
Understanding (NLU) tasks. Despite the success, most current pre-trained
language models, such as BERT, are trained based on single-grained
tokenization, usually with fine-grained characters or sub-words, making it hard
for them to learn the precise meaning of coarse-grained words and phrases. In
this paper, we propose a simple yet effective pre-training method named LICHEE
to efficiently incorporate multi-grained information of input text. Our method
can be applied to various pre-trained language models and improve their
representation capability. Extensive experiments conducted on CLUE and
SuperGLUE demonstrate that our method achieves comprehensive improvements on a
wide variety of NLU tasks in both Chinese and English with little extra
inference cost incurred, and that our best ensemble model achieves the
state-of-the-art performance on CLUE benchmark competition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1"&gt;Weidong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1"&gt;Mingjun Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lusheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1"&gt;Di Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jinwen Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhenhua Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jianbo Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dialogue-oriented Pre-training. (arXiv:2106.00420v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00420</id>
        <link href="http://arxiv.org/abs/2106.00420"/>
        <updated>2021-08-03T02:06:30.051Z</updated>
        <summary type="html"><![CDATA[Pre-trained language models (PrLM) has been shown powerful in enhancing a
broad range of downstream tasks including various dialogue related ones.
However, PrLMs are usually trained on general plain text with common language
model (LM) training objectives, which cannot sufficiently capture dialogue
exclusive features due to the limitation of such training setting, so that
there is an immediate need to fill the gap between a specific dialogue task and
the LM task. As it is unlikely to collect huge dialogue data for
dialogue-oriented pre-training, in this paper, we propose three strategies to
simulate the conversation features on general plain text. Our proposed method
differs from existing post-training methods that it may yield a general-purpose
PrLM and does not individualize to any detailed task while keeping the
capability of learning dialogue related features including speaker awareness,
continuity and consistency. The resulted Dialog-PrLM is fine-tuned on three
public multi-turn dialogue datasets and helps achieve significant and
consistent improvement over the plain PrLMs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hai Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distantly-Supervised Long-Tailed Relation Extraction Using Constraint Graphs. (arXiv:2105.11225v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11225</id>
        <link href="http://arxiv.org/abs/2105.11225"/>
        <updated>2021-08-03T02:06:30.021Z</updated>
        <summary type="html"><![CDATA[Label noise and long-tailed distributions are two major challenges in
distantly supervised relation extraction. Recent studies have shown great
progress on denoising, but pay little attention to the problem of long-tailed
relations. In this paper, we introduce constraint graphs to model the
dependencies between relation labels. On top of that, we further propose a
novel constraint graph-based relation extraction framework(CGRE) to handle the
two challenges simultaneously. CGRE employs graph convolution networks (GCNs)
to propagate information from data-rich relation nodes to data-poor relation
nodes, and thus boosts the representation learning of long-tailed relations. To
further improve the noise immunity, a constraint-aware attention module is
designed in CGRE to integrate the constraint information. Experimental results
on a widely-used benchmark dataset indicate that our approach achieves
significant improvements over the previous methods for both denoising and
long-tailed relation extraction. Our dataset and codes are available at
https://github.com/tmliang/CGRE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1"&gt;Tianming Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaoyan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1"&gt;Gaurav Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Maozu Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Modal Detection of Alzheimer's Disease from Speech and Text. (arXiv:2012.00096v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00096</id>
        <link href="http://arxiv.org/abs/2012.00096"/>
        <updated>2021-08-03T02:06:30.014Z</updated>
        <summary type="html"><![CDATA[Reliable detection of the prodromal stages of Alzheimer's disease (AD)
remains difficult even today because, unlike other neurocognitive impairments,
there is no definitive diagnosis of AD in vivo. In this context, existing
research has shown that patients often develop language impairment even in mild
AD conditions. We propose a multimodal deep learning method that utilizes
speech and the corresponding transcript simultaneously to detect AD. For audio
signals, the proposed audio-based network, a convolutional neural network (CNN)
based model, predicts the diagnosis for multiple speech segments, which are
combined for the final prediction. Similarly, we use contextual embedding
extracted from BERT concatenated with a CNN-generated embedding for classifying
the transcript. The individual predictions of the two models are then combined
to make the final classification. We also perform experiments to analyze the
model performance when Automated Speech Recognition (ASR) system generated
transcripts are used instead of manual transcription in the text-based model.
The proposed method achieves 85.3% 10-fold cross-validation accuracy when
trained and evaluated on the Dementiabank Pitt corpus.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1"&gt;Amish Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahoo_S/0/1/0/all/0/1"&gt;Sourav Sahoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Datar_A/0/1/0/all/0/1"&gt;Arnhav Datar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadiwala_J/0/1/0/all/0/1"&gt;Juned Kadiwala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shalu_H/0/1/0/all/0/1"&gt;Hrithwik Shalu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathew_J/0/1/0/all/0/1"&gt;Jimson Mathew&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can you tell? SSNet -- a Sagittal Stratum-inspired Neural Network Framework for Sentiment Analysis. (arXiv:2006.12958v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.12958</id>
        <link href="http://arxiv.org/abs/2006.12958"/>
        <updated>2021-08-03T02:06:30.007Z</updated>
        <summary type="html"><![CDATA[When people try to understand nuanced language they typically process
multiple input sensor modalities to complete this cognitive task. It turns out
the human brain has even a specialized neuron formation, called sagittal
stratum, to help us understand sarcasm. We use this biological formation as the
inspiration for designing a neural network architecture that combines
predictions of different models on the same text to construct robust, accurate
and computationally efficient classifiers for sentiment analysis and study
several different realizations. Among them, we propose a systematic new
approach to combining multiple predictions based on a dedicated neural network
and develop mathematical analysis of it along with state-of-the-art
experimental results. We also propose a heuristic-hybrid technique for
combining models and back it up with experimental results on a representative
benchmark dataset and comparisons to other methods to show the advantages of
the new approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vassilev_A/0/1/0/all/0/1"&gt;Apostol Vassilev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1"&gt;Munawar Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1"&gt;Honglan Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relation Aware Semi-autoregressive Semantic Parsing for NL2SQL. (arXiv:2108.00804v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00804</id>
        <link href="http://arxiv.org/abs/2108.00804"/>
        <updated>2021-08-03T02:06:29.997Z</updated>
        <summary type="html"><![CDATA[Natural language to SQL (NL2SQL) aims to parse a natural language with a
given database into a SQL query, which widely appears in practical Internet
applications. Jointly encode database schema and question utterance is a
difficult but important task in NL2SQL. One solution is to treat the input as a
heterogeneous graph. However, it failed to learn good word representation in
question utterance. Learning better word representation is important for
constructing a well-designed NL2SQL system. To solve the challenging task, we
present a Relation aware Semi-autogressive Semantic Parsing (\MODN) ~framework,
which is more adaptable for NL2SQL. It first learns relation embedding over the
schema entities and question words with predefined schema relations with
ELECTRA and relation aware transformer layer as backbone. Then we decode the
query SQL with a semi-autoregressive parser and predefined SQL syntax. From
empirical results and case study, our model shows its effectiveness in learning
better word representation in NL2SQL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Junyang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yongbo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yongliang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yang Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1"&gt;Yanghua Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Domain Adaptation in Neural Machine Translation Through Multidimensional Tagging. (arXiv:2102.10160v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10160</id>
        <link href="http://arxiv.org/abs/2102.10160"/>
        <updated>2021-08-03T02:06:29.968Z</updated>
        <summary type="html"><![CDATA[While NMT has achieved remarkable results in the last 5 years, production
systems come with strict quality requirements in arbitrarily niche domains that
are not always adequately covered by readily available parallel corpora. This
is typically addressed by training domain specific models, using fine-tuning
methods and some variation of back-translation on top of in-domain monolingual
corpora. However, industrial practitioners can rarely afford to focus on a
single domain. A far more typical scenario includes a set of closely related,
yet succinctly different sub-domains. At Booking.com, we need to translate
property descriptions, user reviews, as well as messages, (for example those
sent between a customer and an agent or property manager). An editor might need
to translate articles across a set of different topics. An e-commerce platform
would typically need to translate both the description of each item and the
user generated content related to them. To this end, we propose MDT: a novel
method to simultaneously fine-tune on several sub-domains by passing
multidimensional sentence-level information to the model during training and
inference. We show that MDT achieves results competitive to N specialist models
each fine-tuned on a single constituent domain, while effectively serving all N
sub-domains, therefore cutting development and maintenance costs by the same
factor. Besides BLEU (industry standard automatic evaluation metric known to
only weakly correlate with human judgement) we also report rigorous human
evaluation results for all models and sub-domains as well as specific examples
that better contextualise the performance of each model in terms of adequacy
and fluency. To facilitate further research, we plan to make the code available
upon acceptance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stergiadis_E/0/1/0/all/0/1"&gt;Emmanouil Stergiadis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Satendra Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kovalev_F/0/1/0/all/0/1"&gt;Fedor Kovalev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levin_P/0/1/0/all/0/1"&gt;Pavel Levin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relevance-guided Supervision for OpenQA with ColBERT. (arXiv:2007.00814v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.00814</id>
        <link href="http://arxiv.org/abs/2007.00814"/>
        <updated>2021-08-03T02:06:29.947Z</updated>
        <summary type="html"><![CDATA[Systems for Open-Domain Question Answering (OpenQA) generally depend on a
retriever for finding candidate passages in a large corpus and a reader for
extracting answers from those passages. In much recent work, the retriever is a
learned component that uses coarse-grained vector representations of questions
and passages. We argue that this modeling choice is insufficiently expressive
for dealing with the complexity of natural language questions. To address this,
we define ColBERT-QA, which adapts the scalable neural retrieval model ColBERT
to OpenQA. ColBERT creates fine-grained interactions between questions and
passages. We propose an efficient weak supervision strategy that iteratively
uses ColBERT to create its own training data. This greatly improves OpenQA
retrieval on Natural Questions, SQuAD, and TriviaQA, and the resulting system
attains state-of-the-art extractive OpenQA performance on all three datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1"&gt;Omar Khattab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1"&gt;Christopher Potts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1"&gt;Matei Zaharia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Answer Retrieval on Clinical Notes. (arXiv:2108.00775v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.00775</id>
        <link href="http://arxiv.org/abs/2108.00775"/>
        <updated>2021-08-03T02:06:29.941Z</updated>
        <summary type="html"><![CDATA[Retrieving answer passages from long documents is a complex task requiring
semantic understanding of both discourse and document context. We approach this
challenge specifically in a clinical scenario, where doctors retrieve cohorts
of patients based on diagnoses and other latent medical aspects. We introduce
CAPR, a rule-based self-supervision objective for training Transformer language
models for domain-specific passage matching. In addition, we contribute a novel
retrieval dataset based on clinical notes to simulate this scenario on a large
corpus of clinical notes. We apply our objective in four Transformer-based
architectures: Contextual Document Vectors, Bi-, Poly- and Cross-encoders. From
our extensive evaluation on MIMIC-III and three other healthcare datasets, we
report that CAPR outperforms strong baselines in the retrieval of
domain-specific passages and effectively generalizes across rule-based and
human-labeled passages. This makes the model powerful especially in zero-shot
scenarios where only limited training data is available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grundmann_P/0/1/0/all/0/1"&gt;Paul Grundmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arnold_S/0/1/0/all/0/1"&gt;Sebastian Arnold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loser_A/0/1/0/all/0/1"&gt;Alexander L&amp;#xf6;ser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explain and Improve: LRP-Inference Fine-Tuning for Image Captioning Models. (arXiv:2001.01037v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.01037</id>
        <link href="http://arxiv.org/abs/2001.01037"/>
        <updated>2021-08-03T02:06:29.934Z</updated>
        <summary type="html"><![CDATA[This paper analyzes the predictions of image captioning models with attention
mechanisms beyond visualizing the attention itself. We develop variants of
layer-wise relevance propagation (LRP) and gradient-based explanation methods,
tailored to image captioning models with attention mechanisms. We compare the
interpretability of attention heatmaps systematically against the explanations
provided by explanation methods such as LRP, Grad-CAM, and Guided Grad-CAM. We
show that explanation methods provide simultaneously pixel-wise image
explanations (supporting and opposing pixels of the input image) and linguistic
explanations (supporting and opposing words of the preceding sequence) for each
word in the predicted captions. We demonstrate with extensive experiments that
explanation methods 1) can reveal additional evidence used by the model to make
decisions compared to attention; 2) correlate to object locations with high
precision; 3) are helpful to "debug" the model, e.g. by analyzing the reasons
for hallucinated object words. With the observed properties of explanations, we
further design an LRP-inference fine-tuning strategy that reduces the issue of
object hallucination in image captioning models, and meanwhile, maintains the
sentence fluency. We conduct experiments with two widely used attention
mechanisms: the adaptive attention mechanism calculated with the additive
attention and the multi-head attention mechanism calculated with the scaled dot
product.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jiamei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1"&gt;Sebastian Lapuschkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1"&gt;Wojciech Samek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Binder_A/0/1/0/all/0/1"&gt;Alexander Binder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TabPert: An Effective Platform for Tabular Perturbation. (arXiv:2108.00603v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00603</id>
        <link href="http://arxiv.org/abs/2108.00603"/>
        <updated>2021-08-03T02:06:29.926Z</updated>
        <summary type="html"><![CDATA[To truly grasp reasoning ability, a Natural Language Inference model should
be evaluated on counterfactual data. TabPert facilitates this by assisting in
the generation of such counterfactual data for assessing model tabular
reasoning issues. TabPert allows a user to update a table, change its
associated hypotheses, change their labels, and highlight rows that are
important for hypothesis classification. TabPert also captures information
about the techniques used to automatically produce the table, as well as the
strategies employed to generate the challenging hypotheses. These
counterfactual tables and hypotheses, as well as the metadata, can then be used
to explore an existing model's shortcomings methodically and quantitatively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1"&gt;Nupur Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1"&gt;Vivek Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rai_A/0/1/0/all/0/1"&gt;Anshul Rai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1"&gt;Gaurav Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From LSAT: The Progress and Challenges of Complex Reasoning. (arXiv:2108.00648v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00648</id>
        <link href="http://arxiv.org/abs/2108.00648"/>
        <updated>2021-08-03T02:06:29.896Z</updated>
        <summary type="html"><![CDATA[Complex reasoning aims to draw a correct inference based on complex rules. As
a hallmark of human intelligence, it involves a degree of explicit reading
comprehension, interpretation of logical knowledge and complex rule
application. In this paper, we take a step forward in complex reasoning by
systematically studying the three challenging and domain-general tasks of the
Law School Admission Test (LSAT), including analytical reasoning, logical
reasoning and reading comprehension. We propose a hybrid reasoning system to
integrate these three tasks and achieve impressive overall performance on the
LSAT tests. The experimental results demonstrate that our system endows itself
a certain complex reasoning ability, especially the fundamental reading
comprehension and challenging logical reasoning capacities. Further analysis
also shows the effectiveness of combining the pre-trained models with the
task-specific reasoning module, and integrating symbolic knowledge into
discrete interpretable reasoning steps in complex reasoning. We further shed a
light on the potential future directions, like unsupervised symbolic knowledge
extraction, model interpretability, few-shot learning and comprehensive
benchmark for complex reasoning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Siyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhongkun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1"&gt;Wanjun Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Ming Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1"&gt;Zhongyu Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhumin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1"&gt;Nan Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geolocation differences of language use in urban areas. (arXiv:2108.00533v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00533</id>
        <link href="http://arxiv.org/abs/2108.00533"/>
        <updated>2021-08-03T02:06:29.874Z</updated>
        <summary type="html"><![CDATA[The explosion in the availability of natural language data in the era of
social media has given rise to a host of applications such as sentiment
analysis and opinion mining. Simultaneously, the growing availability of
precise geolocation information is enabling visualization of global phenomena
such as environmental changes and disease propagation. Opportunities for
tracking spatial variations in language use, however, have largely been
overlooked, especially on small spatial scales. Here we explore the use of
Twitter data with precise geolocation information to resolve spatial variations
in language use on an urban scale down to single city blocks. We identify
several categories of language tokens likely to show distinctive patterns of
use and develop quantitative methods to visualize the spatial distributions
associated with these patterns. Our analysis concentrates on comparison of
contrasting pairs of Tweet distributions from the same category, each defined
by a set of tokens. Our work shows that analysis of small-scale variations can
provide unique information on correlations between language use and social
context which are highly valuable to a wide range of fields from linguistic
science and commercial advertising to social services.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kellert_O/0/1/0/all/0/1"&gt;Olga Kellert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matlis_N/0/1/0/all/0/1"&gt;Nicholas H. Matlis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Content Preservation in Text Style Transfer Using Reverse Attention and Conditional Layer Normalization. (arXiv:2108.00449v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00449</id>
        <link href="http://arxiv.org/abs/2108.00449"/>
        <updated>2021-08-03T02:06:29.866Z</updated>
        <summary type="html"><![CDATA[Text style transfer aims to alter the style (e.g., sentiment) of a sentence
while preserving its content. A common approach is to map a given sentence to
content representation that is free of style, and the content representation is
fed to a decoder with a target style. Previous methods in filtering style
completely remove tokens with style at the token level, which incurs the loss
of content information. In this paper, we propose to enhance content
preservation by implicitly removing the style information of each token with
reverse attention, and thereby retain the content. Furthermore, we fuse content
information when building the target style representation, making it dynamic
with respect to the content. Our method creates not only style-independent
content representation, but also content-dependent style representation in
transferring style. Empirical results show that our method outperforms the
state-of-the-art baselines by a large margin in terms of content preservation.
In addition, it is also competitive in terms of style transfer accuracy and
fluency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dongkyu Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1"&gt;Zhiliang Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1"&gt;Lanqing Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Nevin L. Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You too Brutus! Trapping Hateful Users in Social Media: Challenges, Solutions & Insights. (arXiv:2108.00524v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2108.00524</id>
        <link href="http://arxiv.org/abs/2108.00524"/>
        <updated>2021-08-03T02:06:29.855Z</updated>
        <summary type="html"><![CDATA[Hate speech is regarded as one of the crucial issues plaguing the online
social media. The current literature on hate speech detection leverages
primarily the textual content to find hateful posts and subsequently identify
hateful users. However, this methodology disregards the social connections
between users. In this paper, we run a detailed exploration of the problem
space and investigate an array of models ranging from purely textual to graph
based to finally semi-supervised techniques using Graph Neural Networks (GNN)
that utilize both textual and graph-based features. We run exhaustive
experiments on two datasets -- Gab, which is loosely moderated and Twitter,
which is strictly moderated. Overall the AGNN model achieves 0.791 macro
F1-score on the Gab dataset and 0.780 macro F1-score on the Twitter dataset
using only 5% of the labeled instances, considerably outperforming all the
other models including the fully supervised ones. We perform detailed error
analysis on the best performing text and graph based models and observe that
hateful users have unique network neighborhood signatures and the AGNN model
benefits by paying attention to these signatures. This property, as we observe,
also allows the model to generalize well across platforms in a zero-shot
setting. Lastly, we utilize the best performing GNN model to analyze the
evolution of hateful users and their targets over time in Gab.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Das_M/0/1/0/all/0/1"&gt;Mithun Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saha_P/0/1/0/all/0/1"&gt;Punyajoy Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dutt_R/0/1/0/all/0/1"&gt;Ritam Dutt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1"&gt;Pawan Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1"&gt;Animesh Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathew_B/0/1/0/all/0/1"&gt;Binny Mathew&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transfer Learning for Mining Feature Requests and Bug Reports from Tweets and App Store Reviews. (arXiv:2108.00663v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00663</id>
        <link href="http://arxiv.org/abs/2108.00663"/>
        <updated>2021-08-03T02:06:29.849Z</updated>
        <summary type="html"><![CDATA[Identifying feature requests and bug reports in user comments holds great
potential for development teams. However, automated mining of RE-related
information from social media and app stores is challenging since (1) about 70%
of user comments contain noisy, irrelevant information, (2) the amount of user
comments grows daily making manual analysis unfeasible, and (3) user comments
are written in different languages. Existing approaches build on traditional
machine learning (ML) and deep learning (DL), but fail to detect feature
requests and bug reports with high Recall and acceptable Precision which is
necessary for this task. In this paper, we investigate the potential of
transfer learning (TL) for the classification of user comments. Specifically,
we train both monolingual and multilingual BERT models and compare the
performance with state-of-the-art methods. We found that monolingual BERT
models outperform existing baseline methods in the classification of English
App Reviews as well as English and Italian Tweets. However, we also observed
that the application of heavyweight TL models does not necessarily lead to
better performance. In fact, our multilingual BERT models perform worse than
traditional ML methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henao_P/0/1/0/all/0/1"&gt;Pablo Restrepo Henao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fischbach_J/0/1/0/all/0/1"&gt;Jannik Fischbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spies_D/0/1/0/all/0/1"&gt;Dominik Spies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frattini_J/0/1/0/all/0/1"&gt;Julian Frattini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogelsang_A/0/1/0/all/0/1"&gt;Andreas Vogelsang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information. (arXiv:2108.00391v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00391</id>
        <link href="http://arxiv.org/abs/2108.00391"/>
        <updated>2021-08-03T02:06:29.842Z</updated>
        <summary type="html"><![CDATA[Commonly-used transformer language models depend on a tokenization schema
which sets an unchangeable subword vocabulary prior to pre-training, destined
to be applied to all downstream tasks regardless of domain shift, novel word
formations, or other sources of vocabulary mismatch. Recent work has shown that
"token-free" models can be trained directly on characters or bytes, but
training these models from scratch requires substantial computational
resources, and this implies discarding the many domain-specific models that
were trained on tokens. In this paper, we present XRayEmb, a method for
retrofitting existing token-based models with character-level information.
XRayEmb is composed of a character-level "encoder" that computes vector
representations of character sequences, and a generative component that decodes
from the internal representation to a character sequence. We show that
incorporating XRayEmb's learned vectors into sequences of pre-trained token
embeddings helps performance on both autoregressive and masked pre-trained
transformer architectures and on both sequence-level and sequence tagging
tasks, particularly on non-standard English text.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pinter_Y/0/1/0/all/0/1"&gt;Yuval Pinter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stent_A/0/1/0/all/0/1"&gt;Amanda Stent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dredze_M/0/1/0/all/0/1"&gt;Mark Dredze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1"&gt;Jacob Eisenstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MuSiQue: Multi-hop Questions via Single-hop Question Composition. (arXiv:2108.00573v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00573</id>
        <link href="http://arxiv.org/abs/2108.00573"/>
        <updated>2021-08-03T02:06:29.835Z</updated>
        <summary type="html"><![CDATA[To build challenging multi-hop question answering datasets, we propose a
bottom-up semi-automatic process of constructing multi-hop question via
composition of single-hop questions. Constructing multi-hop questions as
composition of single-hop questions allows us to exercise greater control over
the quality of the resulting multi-hop questions. This process allows building
a dataset with (i) connected reasoning where each step needs the answer from a
previous step; (ii) minimal train-test leakage by eliminating even partial
overlap of reasoning steps; (iii) variable number of hops and composition
structures; and (iv) contrasting unanswerable questions by modifying the
context. We use this process to construct a new multihop QA dataset:
MuSiQue-Ans with ~25K 2-4 hop questions using seed questions from 5 existing
single-hop datasets. Our experiments demonstrate that MuSique is challenging
for state-of-the-art QA models (e.g., human-machine gap of $~$30 F1 pts),
significantly harder than existing datasets (2x human-machine gap), and
substantially less cheatable (e.g., a single-hop model is worse by 30 F1 pts).
We also build an even more challenging dataset, MuSiQue-Full, consisting of
answerable and unanswerable contrast question pairs, where model performance
drops further by 13+ F1 pts. For data and code, see
\url{https://github.com/stonybrooknlp/musique}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_H/0/1/0/all/0/1"&gt;Harsh Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1"&gt;Niranjan Balasubramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1"&gt;Tushar Khot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1"&gt;Ashish Sabharwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Logic-Consistency Text Generation from Semantic Parses. (arXiv:2108.00577v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00577</id>
        <link href="http://arxiv.org/abs/2108.00577"/>
        <updated>2021-08-03T02:06:29.733Z</updated>
        <summary type="html"><![CDATA[Text generation from semantic parses is to generate textual descriptions for
formal representation inputs such as logic forms and SQL queries. This is
challenging due to two reasons: (1) the complex and intensive inner logic with
the data scarcity constraint, (2) the lack of automatic evaluation metrics for
logic consistency. To address these two challenges, this paper first proposes
SNOWBALL, a framework for logic consistent text generation from semantic parses
that employs an iterative training procedure by recursively augmenting the
training set with quality control. Second, we propose a novel automatic metric,
BLEC, for evaluating the logical consistency between the semantic parses and
generated texts. The experimental results on two benchmark datasets, Logic2Text
and Spider, demonstrate the SNOWBALL framework enhances the logic consistency
on both BLEC and human evaluation. Furthermore, our statistical analysis
reveals that BLEC is more logically consistent with human evaluation than
general-purpose automatic metrics including BLEU, ROUGE and, BLEURT. Our data
and code are available at https://github.com/Ciaranshu/relogic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shu_C/0/1/0/all/0/1"&gt;Chang Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yusen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xiangyu Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1"&gt;Peng Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1"&gt;Tao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is My Model Using The Right Evidence? Systematic Probes for Examining Evidence-Based Tabular Reasoning. (arXiv:2108.00578v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00578</id>
        <link href="http://arxiv.org/abs/2108.00578"/>
        <updated>2021-08-03T02:06:29.726Z</updated>
        <summary type="html"><![CDATA[While neural models routinely report state-of-the-art performance across NLP
tasks involving reasoning, their outputs are often observed to not properly use
and reason on the evidence presented to them in the inputs. A model that
reasons properly is expected to attend to the right parts of the input, be
self-consistent in its predictions across examples, avoid spurious patterns in
inputs, and to ignore biasing from its underlying pre-trained language model in
a nuanced, context-sensitive fashion (e.g. handling counterfactuals). Do
today's models do so? In this paper, we study this question using the problem
of reasoning on tabular data. The tabular nature of the input is particularly
suited for the study as it admits systematic probes targeting the properties
listed above. Our experiments demonstrate that a BERT-based model
representative of today's state-of-the-art fails to properly reason on the
following counts: it often (a) misses the relevant evidence, (b) suffers from
hypothesis and knowledge biases, and, (c) relies on annotation artifacts and
knowledge from pre-trained language models as primary evidence rather than
relying on reasoning on the premises in the tabular input.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1"&gt;Vivek Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhat_R/0/1/0/all/0/1"&gt;Riyaz A. Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosal_A/0/1/0/all/0/1"&gt;Atreya Ghosal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_M/0/1/0/all/0/1"&gt;Manish Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1"&gt;Maneesh Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1"&gt;Vivek Srikumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DECAF: Deep Extreme Classification with Label Features. (arXiv:2108.00368v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00368</id>
        <link href="http://arxiv.org/abs/2108.00368"/>
        <updated>2021-08-03T02:06:29.720Z</updated>
        <summary type="html"><![CDATA[Extreme multi-label classification (XML) involves tagging a data point with
its most relevant subset of labels from an extremely large label set, with
several applications such as product-to-product recommendation with millions of
products. Although leading XML algorithms scale to millions of labels, they
largely ignore label meta-data such as textual descriptions of the labels. On
the other hand, classical techniques that can utilize label metadata via
representation learning using deep networks struggle in extreme settings. This
paper develops the DECAF algorithm that addresses these challenges by learning
models enriched by label metadata that jointly learn model parameters and
feature representations using deep networks and offer accurate classification
at the scale of millions of labels. DECAF makes specific contributions to model
architecture design, initialization, and training, enabling it to offer up to
2-6% more accurate prediction than leading extreme classifiers on publicly
available benchmark product-to-product recommendation datasets, such as
LF-AmazonTitles-1.3M. At the same time, DECAF was found to be up to 22x faster
at inference than leading deep extreme classifiers, which makes it suitable for
real-time applications that require predictions within a few milliseconds. The
code for DECAF is available at the following URL
https://github.com/Extreme-classification/DECAF.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1"&gt;Anshul Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dahiya_K/0/1/0/all/0/1"&gt;Kunal Dahiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1"&gt;Sheshansh Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saini_D/0/1/0/all/0/1"&gt;Deepak Saini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sumeet Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1"&gt;Purushottam Kar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1"&gt;Manik Varma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Realised Volatility Forecasting: Machine Learning via Financial Word Embedding. (arXiv:2108.00480v1 [q-fin.CP])]]></title>
        <id>http://arxiv.org/abs/2108.00480</id>
        <link href="http://arxiv.org/abs/2108.00480"/>
        <updated>2021-08-03T02:06:29.702Z</updated>
        <summary type="html"><![CDATA[We develop FinText, a novel, state-of-the-art, financial word embedding from
Dow Jones Newswires Text News Feed Database. Incorporating this word embedding
in a machine learning model produces a substantial increase in volatility
forecasting performance on days with volatility jumps for 23 NASDAQ stocks from
27 July 2007 to 18 November 2016. A simple ensemble model, combining our word
embedding and another machine learning model that uses limit order book data,
provides the best forecasting performance for both normal and jump volatility
days. Finally, we use Integrated Gradients and SHAP (SHapley Additive
exPlanations) to make the results more 'explainable' and the model comparisons
more transparent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Rahimikia_E/0/1/0/all/0/1"&gt;Eghbal Rahimikia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Zohren_S/0/1/0/all/0/1"&gt;Stefan Zohren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Poon_S/0/1/0/all/0/1"&gt;Ser-Huang Poon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ConveRT, an Application to FAQ Answering. (arXiv:2108.00719v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00719</id>
        <link href="http://arxiv.org/abs/2108.00719"/>
        <updated>2021-08-03T02:06:29.688Z</updated>
        <summary type="html"><![CDATA[Knowledgeable FAQ chatbots are a valuable resource to any organization.
Unlike traditional call centers or FAQ web pages, they provide instant
responses and are always available. Our experience running a COVID19 chatbot
revealed the lack of resources available for FAQ answering in non-English
languages. While powerful and efficient retrieval-based models exist for
English, it is rarely the case for other languages which do not have the same
amount of training data available. In this work, we propose a novel pretaining
procedure to adapt ConveRT, an English SOTA conversational agent, to other
languages with less training data available. We apply it for the first time to
the task of Dutch FAQ answering related to the COVID19 vaccine. We show it
performs better than an open-source alternative in a low-data regime and
high-data regime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bruyn_M/0/1/0/all/0/1"&gt;Maxime De Bruyn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lotfi_E/0/1/0/all/0/1"&gt;Ehsan Lotfi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buhmann_J/0/1/0/all/0/1"&gt;Jeska Buhmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daelemans_W/0/1/0/all/0/1"&gt;Walter Daelemans&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chest ImaGenome Dataset for Clinical Reasoning. (arXiv:2108.00316v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00316</id>
        <link href="http://arxiv.org/abs/2108.00316"/>
        <updated>2021-08-03T02:06:29.681Z</updated>
        <summary type="html"><![CDATA[Despite the progress in automatic detection of radiologic findings from chest
X-ray (CXR) images in recent years, a quantitative evaluation of the
explainability of these models is hampered by the lack of locally labeled
datasets for different findings. With the exception of a few expert-labeled
small-scale datasets for specific findings, such as pneumonia and pneumothorax,
most of the CXR deep learning models to date are trained on global "weak"
labels extracted from text reports, or trained via a joint image and
unstructured text learning strategy. Inspired by the Visual Genome effort in
the computer vision community, we constructed the first Chest ImaGenome dataset
with a scene graph data structure to describe $242,072$ images. Local
annotations are automatically produced using a joint rule-based natural
language processing (NLP) and atlas-based bounding box detection pipeline.
Through a radiologist constructed CXR ontology, the annotations for each CXR
are connected as an anatomy-centered scene graph, useful for image-level
reasoning and multimodal fusion applications. Overall, we provide: i) $1,256$
combinations of relation annotations between $29$ CXR anatomical locations
(objects with bounding box coordinates) and their attributes, structured as a
scene graph per image, ii) over $670,000$ localized comparison relations (for
improved, worsened, or no change) between the anatomical locations across
sequential exams, as well as ii) a manually annotated gold standard scene graph
dataset from $500$ unique patients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Joy T. Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agu_N/0/1/0/all/0/1"&gt;Nkechinyere N. Agu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lourentzou_I/0/1/0/all/0/1"&gt;Ismini Lourentzou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Arjun Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paguio_J/0/1/0/all/0/1"&gt;Joseph A. Paguio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1"&gt;Jasper S. Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dee_E/0/1/0/all/0/1"&gt;Edward C. Dee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitchell_W/0/1/0/all/0/1"&gt;William Mitchell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kashyap_S/0/1/0/all/0/1"&gt;Satyananda Kashyap&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giovannini_A/0/1/0/all/0/1"&gt;Andrea Giovannini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Celi_L/0/1/0/all/0/1"&gt;Leo A. Celi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1"&gt;Mehdi Moradi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-based Aspect Reasoning for Knowledge Base Question Answering on Clinical Notes. (arXiv:2108.00513v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00513</id>
        <link href="http://arxiv.org/abs/2108.00513"/>
        <updated>2021-08-03T02:06:29.652Z</updated>
        <summary type="html"><![CDATA[Question Answering (QA) in clinical notes has gained a lot of attention in
the past few years. Existing machine reading comprehension approaches in
clinical domain can only handle questions about a single block of clinical
texts and fail to retrieve information about different patients and clinical
notes. To handle more complex questions, we aim at creating knowledge base from
clinical notes to link different patients and clinical notes, and performing
knowledge base question answering (KBQA). Based on the expert annotations in
n2c2, we first created the ClinicalKBQA dataset that includes 8,952 QA pairs
and covers questions about seven medical topics through 322 question templates.
Then, we proposed an attention-based aspect reasoning (AAR) method for KBQA and
investigated the impact of different aspects of answers (e.g., entity, type,
path, and context) for prediction. The AAR method achieves better performance
due to the well-designed encoder and attention mechanism. In the experiments,
we find that both aspects, type and path, enable the model to identify answers
satisfying the general conditions and produce lower precision and higher
recall. On the other hand, the aspects, entity and context, limit the answers
by node-specific information and lead to higher precision and lower recall.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Ping Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1"&gt;Tian Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_K/0/1/0/all/0/1"&gt;Khushbu Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhury_S/0/1/0/all/0/1"&gt;Sutanay Choudhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1"&gt;Chandan K. Reddy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer-Encoder-GRU (T-E-GRU) for Chinese Sentiment Analysis on Chinese Comment Text. (arXiv:2108.00400v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00400</id>
        <link href="http://arxiv.org/abs/2108.00400"/>
        <updated>2021-08-03T02:06:29.607Z</updated>
        <summary type="html"><![CDATA[Chinese sentiment analysis (CSA) has always been one of the challenges in
natural language processing due to its complexity and uncertainty. Transformer
has succeeded in capturing semantic features, but it uses position encoding to
capture sequence features, which has great shortcomings compared with the
recurrent model. In this paper, we propose T-E-GRU for Chinese sentiment
analysis, which combine transformer encoder and GRU. We conducted experiments
on three Chinese comment datasets. In view of the confusion of punctuation
marks in Chinese comment texts, we selectively retain some punctuation marks
with sentence segmentation ability. The experimental results show that T-E-GRU
outperforms classic recurrent model and recurrent model with attention.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Binlong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wei Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Continual Entity Learning in Language Models for Conversational Agents. (arXiv:2108.00082v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00082</id>
        <link href="http://arxiv.org/abs/2108.00082"/>
        <updated>2021-08-03T02:06:29.557Z</updated>
        <summary type="html"><![CDATA[Neural language models (LM) trained on diverse corpora are known to work well
on previously seen entities, however, updating these models with dynamically
changing entities such as place names, song titles and shopping items requires
re-training from scratch and collecting full sentences containing these
entities. We aim to address this issue, by introducing entity-aware language
models (EALM), where we integrate entity models trained on catalogues of
entities into the pre-trained LMs. Our combined language model adaptively adds
information from the entity models into the pre-trained LM depending on the
sentence context. Our entity models can be updated independently of the
pre-trained LM, enabling us to influence the distribution of entities output by
the final LM, without any further training of the pre-trained LM. We show
significant perplexity improvements on task-oriented dialogue datasets,
especially on long-tailed utterances, with an ability to continually adapt to
new entities (to an extent).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gadde_R/0/1/0/all/0/1"&gt;Ravi Teja Gadde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bulyko_I/0/1/0/all/0/1"&gt;Ivan Bulyko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Psychologically Informed Part-of-Speech Analysis of Depression in Social Media. (arXiv:2108.00279v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00279</id>
        <link href="http://arxiv.org/abs/2108.00279"/>
        <updated>2021-08-03T02:06:29.550Z</updated>
        <summary type="html"><![CDATA[In this work, we provide an extensive part-of-speech analysis of the
discourse of social media users with depression. Research in psychology
revealed that depressed users tend to be self-focused, more preoccupied with
themselves and ruminate more about their lives and emotions. Our work aims to
make use of large-scale datasets and computational methods for a quantitative
exploration of discourse. We use the publicly available depression dataset from
the Early Risk Prediction on the Internet Workshop (eRisk) 2018 and extract
part-of-speech features and several indices based on them. Our results reveal
statistically significant differences between the depressed and non-depressed
individuals confirming findings from the existing psychology literature. Our
work provides insights regarding the way in which depressed individuals are
expressing themselves on social media platforms, allowing for better-informed
computational models to help monitor and prevent mental illnesses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bucur_A/0/1/0/all/0/1"&gt;Ana-Maria Bucur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Podina_I/0/1/0/all/0/1"&gt;Ioana R. Podin&amp;#x103;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dinu_L/0/1/0/all/0/1"&gt;Liviu P. Dinu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ECLARE: Extreme Classification with Label Graph Correlations. (arXiv:2108.00261v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00261</id>
        <link href="http://arxiv.org/abs/2108.00261"/>
        <updated>2021-08-03T02:06:29.542Z</updated>
        <summary type="html"><![CDATA[Deep extreme classification (XC) seeks to train deep architectures that can
tag a data point with its most relevant subset of labels from an extremely
large label set. The core utility of XC comes from predicting labels that are
rarely seen during training. Such rare labels hold the key to personalized
recommendations that can delight and surprise a user. However, the large number
of rare labels and small amount of training data per rare label offer
significant statistical and computational challenges. State-of-the-art deep XC
methods attempt to remedy this by incorporating textual descriptions of labels
but do not adequately address the problem. This paper presents ECLARE, a
scalable deep learning architecture that incorporates not only label text, but
also label correlations, to offer accurate real-time predictions within a few
milliseconds. Core contributions of ECLARE include a frugal architecture and
scalable techniques to train deep models along with label correlation graphs at
the scale of millions of labels. In particular, ECLARE offers predictions that
are 2 to 14% more accurate on both publicly available benchmark datasets as
well as proprietary datasets for a related products recommendation task sourced
from the Bing search engine. Code for ECLARE is available at
https://github.com/Extreme-classification/ECLARE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1"&gt;Anshul Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sachdeva_N/0/1/0/all/0/1"&gt;Noveen Sachdeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1"&gt;Sheshansh Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sumeet Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1"&gt;Purushottam Kar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1"&gt;Manik Varma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Knowledge-Embedded Attention to Augment Pre-trained Language Models for Fine-Grained Emotion Recognition. (arXiv:2108.00194v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00194</id>
        <link href="http://arxiv.org/abs/2108.00194"/>
        <updated>2021-08-03T02:06:29.510Z</updated>
        <summary type="html"><![CDATA[Modern emotion recognition systems are trained to recognize only a small set
of emotions, and hence fail to capture the broad spectrum of emotions people
experience and express in daily life. In order to engage in more empathetic
interactions, future AI has to perform \textit{fine-grained} emotion
recognition, distinguishing between many more varied emotions. Here, we focus
on improving fine-grained emotion recognition by introducing external knowledge
into a pre-trained self-attention model. We propose Knowledge-Embedded
Attention (KEA) to use knowledge from emotion lexicons to augment the
contextual representations from pre-trained ELECTRA and BERT models. Our
results and error analyses outperform previous models on several datasets, and
is better able to differentiate closely-confusable emotions, such as afraid and
terrified.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suresh_V/0/1/0/all/0/1"&gt;Varsha Suresh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ong_D/0/1/0/all/0/1"&gt;Desmond C. Ong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tensor completion using geodesics on Segre manifolds. (arXiv:2108.00735v1 [math.DG])]]></title>
        <id>http://arxiv.org/abs/2108.00735</id>
        <link href="http://arxiv.org/abs/2108.00735"/>
        <updated>2021-08-03T02:06:29.491Z</updated>
        <summary type="html"><![CDATA[We propose a Riemannian conjugate gradient (CG) optimization method for
finding low rank approximations of incomplete tensors. Our main contribution
consists of an explicit expression of the geodesics on the Segre manifold.
These are exploited in our algorithm to perform the retractions. We apply our
method to movie rating predictions in a recommender system for the MovieLens
dataset, and identification of pure fluorophores via fluorescent spectroscopy
with missing data. In this last application, we recover the tensor
decomposition from less than $10\%$ of the data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Swijsen_L/0/1/0/all/0/1"&gt;Lars Swijsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Veken_J/0/1/0/all/0/1"&gt;Joeri Van der Veken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Vannieuwenhoven_N/0/1/0/all/0/1"&gt;Nick Vannieuwenhoven&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning of Context-Aware Pitch Prosody Representations. (arXiv:2007.09060v4 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.09060</id>
        <link href="http://arxiv.org/abs/2007.09060"/>
        <updated>2021-08-03T02:06:29.478Z</updated>
        <summary type="html"><![CDATA[In music and speech, meaning is derived at multiple levels of context.
Affect, for example, can be inferred both by a short sound token and by sonic
patterns over a longer temporal window such as an entire recording. In this
letter, we focus on inferring meaning from this dichotomy of contexts. We show
how contextual representations of short sung vocal lines can be implicitly
learned from fundamental frequency ($F_0$) and thus be used as a meaningful
feature space for downstream Music Information Retrieval (MIR) tasks. We
propose three self-supervised deep learning paradigms which leverage pseudotask
learning of these two levels of context to produce latent representation
spaces. We evaluate the usefulness of these representations by embedding unseen
pitch contours into each space and conducting downstream classification tasks.
Our results show that contextual representation can enhance downstream
classification by as much as 15\% as compared to using traditional statistical
contour features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Noufi_C/0/1/0/all/0/1"&gt;Camille Noufi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1"&gt;Prateek Verma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Social Meaning Detection with Pragmatic Masking and Surrogate Fine-Tuning. (arXiv:2108.00356v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00356</id>
        <link href="http://arxiv.org/abs/2108.00356"/>
        <updated>2021-08-03T02:06:29.447Z</updated>
        <summary type="html"><![CDATA[Masked language models (MLMs) are pretrained with a denoising objective that,
while useful, is in a mismatch with the objective of downstream fine-tuning. We
propose pragmatic masking and surrogate fine-tuning as two strategies that
exploit social cues to drive pre-trained representations toward a broad set of
concepts useful for a wide class of social meaning tasks. To test our methods,
we introduce a new benchmark of 15 different Twitter datasets for social
meaning detection. Our methods achieve 2.34% F1 over a competitive baseline,
while outperforming other transfer learning methods such as multi-task learning
and domain-specific language models pretrained on large datasets. With only 5%
of training data (severely few-shot), our methods enable an impressive 68.74%
average F1, and we observe promising results in a zero-shot setting involving
six datasets from three different languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chiyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1"&gt;Muhammad Abdul-Mageed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elmadany_A/0/1/0/all/0/1"&gt;AbdelRahim Elmadany&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagoudi_E/0/1/0/all/0/1"&gt;El Moatez Billah Nagoudi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structural Guidance for Transformer Language Models. (arXiv:2108.00104v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00104</id>
        <link href="http://arxiv.org/abs/2108.00104"/>
        <updated>2021-08-03T02:06:29.439Z</updated>
        <summary type="html"><![CDATA[Transformer-based language models pre-trained on large amounts of text data
have proven remarkably successful in learning generic transferable linguistic
representations. Here we study whether structural guidance leads to more
human-like systematic linguistic generalization in Transformer language models
without resorting to pre-training on very large amounts of data. We explore two
general ideas. The "Generative Parsing" idea jointly models the incremental
parse and word sequence as part of the same sequence modeling task. The
"Structural Scaffold" idea guides the language model's representation via
additional structure loss that separately predicts the incremental constituency
parse. We train the proposed models along with a vanilla Transformer language
model baseline on a 14 million-token and a 46 million-token subset of the BLLIP
dataset, and evaluate models' syntactic generalization performances on SG Test
Suites and sized BLiMP. Experiment results across two benchmarks suggest
converging evidence that generative structural supervisions can induce more
robust and humanlike linguistic generalization in Transformer language models
without the need for data intensive pre-training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_P/0/1/0/all/0/1"&gt;Peng Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naseem_T/0/1/0/all/0/1"&gt;Tahira Naseem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1"&gt;Roger Levy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Astudillo_R/0/1/0/all/0/1"&gt;Ram&amp;#xf3;n Fernandez Astudillo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-cultural Mood Perception in Pop Songs and its Alignment with Mood Detection Algorithms. (arXiv:2108.00768v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.00768</id>
        <link href="http://arxiv.org/abs/2108.00768"/>
        <updated>2021-08-03T02:06:29.432Z</updated>
        <summary type="html"><![CDATA[Do people from different cultural backgrounds perceive the mood in music the
same way? How closely do human ratings across different cultures approximate
automatic mood detection algorithms that are often trained on corpora of
predominantly Western popular music? Analyzing 166 participants responses from
Brazil, South Korea, and the US, we examined the similarity between the ratings
of nine categories of perceived moods in music and estimated their alignment
with four popular mood detection algorithms. We created a dataset of 360 recent
pop songs drawn from major music charts of the countries and constructed
semantically identical mood descriptors across English, Korean, and Portuguese
languages. Multiple participants from the three countries rated their
familiarity, preference, and perceived moods for a given song. Ratings were
highly similar within and across cultures for basic mood attributes such as
sad, cheerful, and energetic. However, we found significant cross-cultural
differences for more complex characteristics such as dreamy and love. To our
surprise, the results of mood detection algorithms were uniformly correlated
across human ratings from all three countries and did not show a detectable
bias towards any particular culture. Our study thus suggests that the mood
detection algorithms can be considered as an objective measure at least within
the popular music context.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Harin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoeger_F/0/1/0/all/0/1"&gt;Frank Hoeger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schoenwiesner_M/0/1/0/all/0/1"&gt;Marc Schoenwiesner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_M/0/1/0/all/0/1"&gt;Minsu Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacoby_N/0/1/0/all/0/1"&gt;Nori Jacoby&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MTVR: Multilingual Moment Retrieval in Videos. (arXiv:2108.00061v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00061</id>
        <link href="http://arxiv.org/abs/2108.00061"/>
        <updated>2021-08-03T02:06:29.423Z</updated>
        <summary type="html"><![CDATA[We introduce mTVR, a large-scale multilingual video moment retrieval dataset,
containing 218K English and Chinese queries from 21.8K TV show video clips. The
dataset is collected by extending the popular TVR dataset (in English) with
paired Chinese queries and subtitles. Compared to existing moment retrieval
datasets, mTVR is multilingual, larger, and comes with diverse annotations. We
further propose mXML, a multilingual moment retrieval model that learns and
operates on data from both languages, via encoder parameter sharing and
language neighborhood constraints. We demonstrate the effectiveness of mXML on
the newly collected MTVR dataset, where mXML outperforms strong monolingual
baselines while using fewer parameters. In addition, we also provide detailed
dataset analyses and model ablations. Data and code are publicly available at
https://github.com/jayleicn/mTVRetrieval]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1"&gt;Tamara L. Berg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Opinion Prediction with User Fingerprinting. (arXiv:2108.00270v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00270</id>
        <link href="http://arxiv.org/abs/2108.00270"/>
        <updated>2021-08-03T02:06:29.400Z</updated>
        <summary type="html"><![CDATA[Opinion prediction is an emerging research area with diverse real-world
applications, such as market research and situational awareness. We identify
two lines of approaches to the problem of opinion prediction. One uses
topic-based sentiment analysis with time-series modeling, while the other uses
static embedding of text. The latter approaches seek user-specific solutions by
generating user fingerprints. Such approaches are useful in predicting user's
reactions to unseen content. In this work, we propose a novel dynamic
fingerprinting method that leverages contextual embedding of user's comments
conditioned on relevant user's reading history. We integrate BERT variants with
a recurrent neural network to generate predictions. The results show up to 13\%
improvement in micro F1-score compared to previous approaches. Experimental
results show novel insights that were previously unknown such as better
predictions for an increase in dynamic history length, the impact of the nature
of the article on performance, thereby laying the foundation for further
research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tumarada_K/0/1/0/all/0/1"&gt;Kishore Tumarada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yifan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1"&gt;Dr. Fan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dragut_D/0/1/0/all/0/1"&gt;Dr. Eduard Dragut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gnawali_D/0/1/0/all/0/1"&gt;Dr. Omprakash Gnawali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_D/0/1/0/all/0/1"&gt;Dr. Arjun Mukherjee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relevance-guided Supervision for OpenQA with ColBERT. (arXiv:2007.00814v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.00814</id>
        <link href="http://arxiv.org/abs/2007.00814"/>
        <updated>2021-08-03T02:06:29.392Z</updated>
        <summary type="html"><![CDATA[Systems for Open-Domain Question Answering (OpenQA) generally depend on a
retriever for finding candidate passages in a large corpus and a reader for
extracting answers from those passages. In much recent work, the retriever is a
learned component that uses coarse-grained vector representations of questions
and passages. We argue that this modeling choice is insufficiently expressive
for dealing with the complexity of natural language questions. To address this,
we define ColBERT-QA, which adapts the scalable neural retrieval model ColBERT
to OpenQA. ColBERT creates fine-grained interactions between questions and
passages. We propose an efficient weak supervision strategy that iteratively
uses ColBERT to create its own training data. This greatly improves OpenQA
retrieval on Natural Questions, SQuAD, and TriviaQA, and the resulting system
attains state-of-the-art extractive OpenQA performance on all three datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1"&gt;Omar Khattab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1"&gt;Christopher Potts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1"&gt;Matei Zaharia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human Evaluation of Creative NLG Systems: An Interdisciplinary Survey on Recent Papers. (arXiv:2108.00308v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00308</id>
        <link href="http://arxiv.org/abs/2108.00308"/>
        <updated>2021-08-03T02:06:29.385Z</updated>
        <summary type="html"><![CDATA[We survey human evaluation in papers presenting work on creative natural
language generation that have been published in INLG 2020 and ICCC 2020. The
most typical human evaluation method is a scaled survey, typically on a 5 point
scale, while many other less common methods exist. The most commonly evaluated
parameters are meaning, syntactic correctness, novelty, relevance and emotional
value, among many others. Our guidelines for future evaluation include clearly
defining the goal of the generative system, asking questions as concrete as
possible, testing the evaluation setup, using multiple different evaluation
setups, reporting the entire evaluation process and potential biases clearly,
and finally analyzing the evaluation results in a more profound way than merely
reporting the most typical statistics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hamalainen_M/0/1/0/all/0/1"&gt;Mika H&amp;#xe4;m&amp;#xe4;l&amp;#xe4;inen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alnajjar_K/0/1/0/all/0/1"&gt;Khalid Alnajjar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning TFIDF Enhanced Joint Embedding for Recipe-Image Cross-Modal Retrieval Service. (arXiv:2108.00724v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00724</id>
        <link href="http://arxiv.org/abs/2108.00724"/>
        <updated>2021-08-03T02:06:29.377Z</updated>
        <summary type="html"><![CDATA[It is widely acknowledged that learning joint embeddings of recipes with
images is challenging due to the diverse composition and deformation of
ingredients in cooking procedures. We present a Multi-modal Semantics enhanced
Joint Embedding approach (MSJE) for learning a common feature space between the
two modalities (text and image), with the ultimate goal of providing
high-performance cross-modal retrieval services. Our MSJE approach has three
unique features. First, we extract the TFIDF feature from the title,
ingredients and cooking instructions of recipes. By determining the
significance of word sequences through combining LSTM learned features with
their TFIDF features, we encode a recipe into a TFIDF weighted vector for
capturing significant key terms and how such key terms are used in the
corresponding cooking instructions. Second, we combine the recipe TFIDF feature
with the recipe sequence feature extracted through two-stage LSTM networks,
which is effective in capturing the unique relationship between a recipe and
its associated image(s). Third, we further incorporate TFIDF enhanced category
semantics to improve the mapping of image modality and to regulate the
similarity loss function during the iterative learning of cross-modal joint
embedding. Experiments on the benchmark dataset Recipe1M show the proposed
approach outperforms the state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zhongwei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Ling Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yanzhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1"&gt;Luo Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diverse Linguistic Features for Assessing Reading Difficulty of Educational Filipino Texts. (arXiv:2108.00241v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00241</id>
        <link href="http://arxiv.org/abs/2108.00241"/>
        <updated>2021-08-03T02:06:29.368Z</updated>
        <summary type="html"><![CDATA[In order to ensure quality and effective learning, fluency, and
comprehension, the proper identification of the difficulty levels of reading
materials should be observed. In this paper, we describe the development of
automatic machine learning-based readability assessment models for educational
Filipino texts using the most diverse set of linguistic features for the
language. Results show that using a Random Forest model obtained a high
performance of 62.7% in terms of accuracy, and 66.1% when using the optimal
combination of feature sets consisting of traditional and syllable
pattern-based predictors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Imperial_J/0/1/0/all/0/1"&gt;Joseph Marvin Imperial&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ong_E/0/1/0/all/0/1"&gt;Ethel Ong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Word2Pix: Word to Pixel Cross Attention Transformer in Visual Grounding. (arXiv:2108.00205v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00205</id>
        <link href="http://arxiv.org/abs/2108.00205"/>
        <updated>2021-08-03T02:06:29.360Z</updated>
        <summary type="html"><![CDATA[Current one-stage methods for visual grounding encode the language query as
one holistic sentence embedding before fusion with visual feature. Such a
formulation does not treat each word of a query sentence on par when modeling
language to visual attention, therefore prone to neglect words which are less
important for sentence embedding but critical for visual grounding. In this
paper we propose Word2Pix: a one-stage visual grounding network based on
encoder-decoder transformer architecture that enables learning for textual to
visual feature correspondence via word to pixel attention. The embedding of
each word from the query sentence is treated alike by attending to visual
pixels individually instead of single holistic sentence embedding. In this way,
each word is given equivalent opportunity to adjust the language to vision
attention towards the referent target through multiple stacks of transformer
decoder layers. We conduct the experiments on RefCOCO, RefCOCO+ and RefCOCOg
datasets and the proposed Word2Pix outperforms existing one-stage methods by a
notable margin. The results obtained also show that Word2Pix surpasses
two-stage visual grounding models, while at the same time keeping the merits of
one-stage paradigm namely end-to-end training and real-time inference speed
intact.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Heng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Joey Tianyi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ong_Y/0/1/0/all/0/1"&gt;Yew-Soon Ong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Zero Attentive Relevance Matching Networkfor Review Modeling in Recommendation System. (arXiv:2101.06387v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06387</id>
        <link href="http://arxiv.org/abs/2101.06387"/>
        <updated>2021-08-03T02:06:29.266Z</updated>
        <summary type="html"><![CDATA[User and item reviews are valuable for the construction of recommender
systems. In general, existing review-based methods for recommendation can be
broadly categorized into two groups: the siamese models that build static user
and item representations from their reviews respectively, and the
interaction-based models that encode user and item dynamically according to the
similarity or relationships of their reviews. Although the interaction-based
models have more model capacity and fit human purchasing behavior better,
several problematic model designs and assumptions of the existing
interaction-based models lead to its suboptimal performance compared to
existing siamese models. In this paper, we identify three problems of the
existing interaction-based recommendation models and propose a couple of
solutions as well as a new interaction-based model to incorporate review data
for rating prediction. Our model implements a relevance matching model with
regularized training losses to discover user relevant information from long
item reviews, and it also adapts a zero attention strategy to dynamically
balance the item-dependent and item-independent information extracted from user
reviews. Empirical experiments and case studies on Amazon Product Benchmark
datasets show that our model can extract effective and interpretable user/item
representations from their reviews and outperforms multiple types of
state-of-the-art review-based recommendation models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1"&gt;Hansi Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhichao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1"&gt;Qingyao Ai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Momentum-based Gradient Methods in Multi-Objective Recommendation. (arXiv:2009.04695v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04695</id>
        <link href="http://arxiv.org/abs/2009.04695"/>
        <updated>2021-08-03T02:06:29.253Z</updated>
        <summary type="html"><![CDATA[Multi-objective gradient methods are becoming the standard for solving
multi-objective problems. Among others, they show promising results in
developing multi-objective recommender systems with both correlated and
conflicting objectives. Classic multi-gradient descent usually relies on the
combination of the gradients, not including the computation of first and second
moments of the gradients. This leads to a brittle behavior and misses important
areas in the solution space. In this work, we create a multi-objective
model-agnostic Adamize method that leverages the benefits of the Adam optimizer
in single-objective problems. This corrects and stabilizes the gradients of
every objective before calculating a common gradient descent vector that
optimizes all the objectives simultaneously. We evaluate the benefits of
multi-objective Adamize on two multi-objective recommender systems and for
three different objective combinations, both correlated or conflicting. We
report significant improvements, measured with three different Pareto front
metrics: hypervolume, coverage, and spacing. Finally, we show that the Adamized
Pareto front strictly dominates the previous one on multiple objective pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mitrevski_B/0/1/0/all/0/1"&gt;Blagoj Mitrevski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filipovic_M/0/1/0/all/0/1"&gt;Milena Filipovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1"&gt;Diego Antognini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glaude_E/0/1/0/all/0/1"&gt;Emma Lejal Glaude&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1"&gt;Boi Faltings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1"&gt;Claudiu Musat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The History of Speech Recognition to the Year 2030. (arXiv:2108.00084v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00084</id>
        <link href="http://arxiv.org/abs/2108.00084"/>
        <updated>2021-08-03T02:06:29.238Z</updated>
        <summary type="html"><![CDATA[The decade from 2010 to 2020 saw remarkable improvements in automatic speech
recognition. Many people now use speech recognition on a daily basis, for
example to perform voice search queries, send text messages, and interact with
voice assistants like Amazon Alexa and Siri by Apple. Before 2010 most people
rarely used speech recognition. Given the remarkable changes in the state of
speech recognition over the previous decade, what can we expect over the coming
decade? I attempt to forecast the state of speech recognition research and
applications by the year 2030. While the changes to general speech recognition
accuracy will not be as dramatic as in the previous decade, I suggest we have
an exciting decade of progress in speech technology ahead of us.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hannun_A/0/1/0/all/0/1"&gt;Awni Hannun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Answer Retrieval on Clinical Notes. (arXiv:2108.00775v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.00775</id>
        <link href="http://arxiv.org/abs/2108.00775"/>
        <updated>2021-08-03T02:06:29.231Z</updated>
        <summary type="html"><![CDATA[Retrieving answer passages from long documents is a complex task requiring
semantic understanding of both discourse and document context. We approach this
challenge specifically in a clinical scenario, where doctors retrieve cohorts
of patients based on diagnoses and other latent medical aspects. We introduce
CAPR, a rule-based self-supervision objective for training Transformer language
models for domain-specific passage matching. In addition, we contribute a novel
retrieval dataset based on clinical notes to simulate this scenario on a large
corpus of clinical notes. We apply our objective in four Transformer-based
architectures: Contextual Document Vectors, Bi-, Poly- and Cross-encoders. From
our extensive evaluation on MIMIC-III and three other healthcare datasets, we
report that CAPR outperforms strong baselines in the retrieval of
domain-specific passages and effectively generalizes across rule-based and
human-labeled passages. This makes the model powerful especially in zero-shot
scenarios where only limited training data is available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grundmann_P/0/1/0/all/0/1"&gt;Paul Grundmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arnold_S/0/1/0/all/0/1"&gt;Sebastian Arnold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loser_A/0/1/0/all/0/1"&gt;Alexander L&amp;#xf6;ser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Jointly Optimizing Query Encoder and Product Quantization to Improve Retrieval Performance. (arXiv:2108.00644v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.00644</id>
        <link href="http://arxiv.org/abs/2108.00644"/>
        <updated>2021-08-03T02:06:29.167Z</updated>
        <summary type="html"><![CDATA[Recently, Information Retrieval community has witnessed fast-paced advances
in Dense Retrieval (DR), which performs first-stage retrieval by encoding
documents in a low-dimensional embedding space and querying them with
embedding-based search. Despite the impressive ranking performance, previous
studies usually adopt brute-force search to acquire candidates, which is
prohibitive in practical Web search scenarios due to its tremendous memory
usage and time cost. To overcome these problems, vector compression methods, a
branch of Approximate Nearest Neighbor Search (ANNS), have been adopted in many
practical embedding-based retrieval applications. One of the most popular
methods is Product Quantization (PQ). However, although existing vector
compression methods including PQ can help improve the efficiency of DR, they
incur severely decayed retrieval performance due to the separation between
encoding and compression. To tackle this problem, we present JPQ, which stands
for Joint optimization of query encoding and Product Quantization. It trains
the query encoder and PQ index jointly in an end-to-end manner based on three
optimization strategies, namely ranking-oriented loss, PQ centroid
optimization, and end-to-end negative sampling. We evaluate JPQ on two publicly
available retrieval benchmarks. Experimental results show that JPQ
significantly outperforms existing popular vector compression methods in terms
of different trade-off settings. Compared with previous DR models that use
brute-force search, JPQ almost matches the best retrieval performance with 30x
compression on index size. The compressed index further brings 10x speedup on
CPU and 2x speedup on GPU in query latency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_J/0/1/0/all/0/1"&gt;Jingtao Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1"&gt;Jiaxin Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yiqun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jiafeng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Min Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shaoping Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WLV-RIT at GermEval 2021: Multitask Learning with Transformers to Detect Toxic, Engaging, and Fact-Claiming Comments. (arXiv:2108.00057v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00057</id>
        <link href="http://arxiv.org/abs/2108.00057"/>
        <updated>2021-08-03T02:06:29.145Z</updated>
        <summary type="html"><![CDATA[This paper addresses the identification of toxic, engaging, and fact-claiming
comments on social media. We used the dataset made available by the organizers
of the GermEval-2021 shared task containing over 3,000 manually annotated
Facebook comments in German. Considering the relatedness of the three tasks, we
approached the problem using large pre-trained transformer models and multitask
learning. Our results indicate that multitask learning achieves performance
superior to the more common single task learning approach in all three tasks.
We submit our best systems to GermEval-2021 under the team name WLV-RIT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Morgan_S/0/1/0/all/0/1"&gt;Skye Morgan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1"&gt;Tharindu Ranasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1"&gt;Marcos Zampieri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SamWalker++: recommendation with informative sampling strategy. (arXiv:2011.07734v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.07734</id>
        <link href="http://arxiv.org/abs/2011.07734"/>
        <updated>2021-08-03T02:06:29.135Z</updated>
        <summary type="html"><![CDATA[Recommendation from implicit feedback is a highly challenging task due to the
lack of reliable negative feedback data. Existing methods address this
challenge by treating all the un-observed data as negative (dislike) but
downweight the confidence of these data. However, this treatment causes two
problems: (1) Confidence weights of the unobserved data are usually assigned
manually, which lack flexibility and may create empirical bias on evaluating
user's preference. (2) To handle massive volume of the unobserved feedback
data, most of the existing methods rely on stochastic inference and data
sampling strategies. However, since a user is only aware of a very small
fraction of items in a large dataset, it is difficult for existing samplers to
select informative training instances in which the user really dislikes the
item rather than does not know it.

To address the above two problems, we propose two novel recommendation
methods SamWalker and SamWalker++ that support both adaptive confidence
assignment and efficient model learning. SamWalker models data confidence with
a social network-aware function, which can adaptively specify different weights
to different data according to users' social contexts. However, the social
network information may not be available in many recommender systems, which
hinders application of SamWalker. Thus, we further propose SamWalker++, which
does not require any side information and models data confidence with a
constructed pseudo-social network. We also develop fast random-walk-based
sampling strategies for our SamWalker and SamWalker++ to adaptively draw
informative training instances, which can speed up gradient estimation and
reduce sampling variance. Extensive experiments on five real-world datasets
demonstrate the superiority of the proposed SamWalker and SamWalker++.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Can Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiawei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Sheng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1"&gt;Qihao Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chun Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis. (arXiv:2107.08264v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08264</id>
        <link href="http://arxiv.org/abs/2107.08264"/>
        <updated>2021-08-03T02:06:28.916Z</updated>
        <summary type="html"><![CDATA[Multimodal sentiment analysis aims to recognize people's attitudes from
multiple communication channels such as verbal content (i.e., text), voice, and
facial expressions. It has become a vibrant and important research topic in
natural language processing. Much research focuses on modeling the complex
intra- and inter-modal interactions between different communication channels.
However, current multimodal models with strong performance are often
deep-learning-based techniques and work like black boxes. It is not clear how
models utilize multimodal information for sentiment predictions. Despite recent
advances in techniques for enhancing the explainability of machine learning
models, they often target unimodal scenarios (e.g., images, sentences), and
little research has been done on explaining multimodal models. In this paper,
we present an interactive visual analytics system, M2Lens, to visualize and
explain multimodal models for sentiment analysis. M2Lens provides explanations
on intra- and inter-modal interactions at the global, subset, and local levels.
Specifically, it summarizes the influence of three typical interaction types
(i.e., dominance, complement, and conflict) on the model predictions. Moreover,
M2Lens identifies frequent and influential multimodal features and supports the
multi-faceted exploration of model behaviors from language, acoustic, and
visual modalities. Through two case studies and expert interviews, we
demonstrate our system can help users gain deep insights into the multimodal
models for sentiment analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xingbo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jianben He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1"&gt;Zhihua Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Muqiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1"&gt;Huamin Qu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical analysis on Transparent Algorithmic Exploration in Recommender Systems. (arXiv:2108.00151v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.00151</id>
        <link href="http://arxiv.org/abs/2108.00151"/>
        <updated>2021-08-03T02:06:28.832Z</updated>
        <summary type="html"><![CDATA[All learning algorithms for recommendations face inevitable and critical
trade-off between exploiting partial knowledge of a user's preferences for
short-term satisfaction and exploring additional user preferences for long-term
coverage. Although exploration is indispensable for long success of a
recommender system, the exploration has been considered as the risk to decrease
user satisfaction. The reason for the risk is that items chosen for exploration
frequently mismatch with the user's interests. To mitigate this risk,
recommender systems have mixed items chosen for exploration into a
recommendation list, disguising the items as recommendations to elicit feedback
on the items to discover the user's additional tastes. This mix-in approach has
been widely used in many recommenders, but there is rare research, evaluating
the effectiveness of the mix-in approach or proposing a new approach for
eliciting user feedback without deceiving users. In this work, we aim to
propose a new approach for feedback elicitation without any deception and
compare our approach to the conventional mix-in approach for evaluation. To
this end, we designed a recommender interface that reveals which items are for
exploration and conducted a within-subject study with 94 MTurk workers. Our
results indicated that users left significantly more feedback on items chosen
for exploration with our interface. Besides, users evaluated that our new
interface is better than the conventional mix-in interface in terms of novelty,
diversity, transparency, trust, and satisfaction. Finally, path analysis show
that, in only our new interface, exploration caused to increase user-centric
evaluation metrics. Our work paves the way for how to design an interface,
which utilizes learning algorithm based on users' feedback signals, giving
better user experience and gathering more feedback data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kihwan Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SurpriseNet: Melody Harmonization Conditioning on User-controlled Surprise Contours. (arXiv:2108.00378v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.00378</id>
        <link href="http://arxiv.org/abs/2108.00378"/>
        <updated>2021-08-03T02:06:28.809Z</updated>
        <summary type="html"><![CDATA[The surprisingness of a song is an essential and seemingly subjective factor
in determining whether the listener likes it. With the help of information
theory, it can be described as the transition probability of a music sequence
modeled as a Markov chain. In this study, we introduce the concept of deriving
entropy variations over time, so that the surprise contour of each chord
sequence can be extracted. Based on this, we propose a user-controllable
framework that uses a conditional variational autoencoder (CVAE) to harmonize
the melody based on the given chord surprise indication. Through explicit
conditions, the model can randomly generate various and harmonic chord
progressions for a melody, and the Spearman's correlation and p-value
significance show that the resulting chord progressions match the given
surprise contour quite well. The vanilla CVAE model was evaluated in a basic
melody harmonization task (no surprise control) in terms of six objective
metrics. The results of experiments on the Hooktheory Lead Sheet Dataset show
that our model achieves performance comparable to the state-of-the-art melody
harmonization model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yi-Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hung-Shin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yen-Hsing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hsin-Min Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BigGraphVis: Leveraging Streaming Algorithms and GPU Acceleration for Visualizing Big Graphs. (arXiv:2108.00529v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2108.00529</id>
        <link href="http://arxiv.org/abs/2108.00529"/>
        <updated>2021-08-03T02:06:28.800Z</updated>
        <summary type="html"><![CDATA[Graph layouts are key to exploring massive graphs. An enormous number of
nodes and edges do not allow network analysis software to produce meaningful
visualization of the pervasive networks. Long computation time, memory and
display limitations encircle the software's ability to explore massive graphs.
This paper introduces BigGraphVis, a new parallel graph visualization method
that uses GPU parallel processing and community detection algorithm to
visualize graph communities. We combine parallelized streaming community
detection algorithm and probabilistic data structure to leverage parallel
processing of Graphics Processing Unit (GPU). To the best of our knowledge,
this is the first attempt to combine the power of streaming algorithms coupled
with GPU computing to tackle big graph visualization challenges. Our method
extracts community information in a few passes on the edge list, and renders
the community structures using the ForceAtlas2 algorithm. Our experiment with
massive real-life graphs indicates that about 70 to 95 percent speedup can be
achieved by visualizing graph communities, and the visualization appears to be
meaningful and reliable. The biggest graph that we examined contains above 3
million nodes and 34 million edges, and the layout computation took about five
minutes. We also observed that the BigGraphVis coloring strategy can be
successfully applied to produce a more informative ForceAtlas2 layout.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moradi_E/0/1/0/all/0/1"&gt;Ehsan Moradi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mondal_D/0/1/0/all/0/1"&gt;Debajyoti Mondal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speech2AffectiveGestures: Synthesizing Co-Speech Gestures with Generative Adversarial Affective Expression Learning. (arXiv:2108.00262v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2108.00262</id>
        <link href="http://arxiv.org/abs/2108.00262"/>
        <updated>2021-08-03T02:06:28.790Z</updated>
        <summary type="html"><![CDATA[We present a generative adversarial network to synthesize 3D pose sequences
of co-speech upper-body gestures with appropriate affective expressions. Our
network consists of two components: a generator to synthesize gestures from a
joint embedding space of features encoded from the input speech and the seed
poses, and a discriminator to distinguish between the synthesized pose
sequences and real 3D pose sequences. We leverage the Mel-frequency cepstral
coefficients and the text transcript computed from the input speech in separate
encoders in our generator to learn the desired sentiments and the associated
affective cues. We design an affective encoder using multi-scale
spatial-temporal graph convolutions to transform 3D pose sequences into latent,
pose-based affective features. We use our affective encoder in both our
generator, where it learns affective features from the seed poses to guide the
gesture synthesis, and our discriminator, where it enforces the synthesized
gestures to contain the appropriate affective expressions. We perform extensive
evaluations on two benchmark datasets for gesture synthesis from the speech,
the TED Gesture Dataset and the GENEA Challenge 2020 Dataset. Compared to the
best baselines, we improve the mean absolute joint error by 10--33%, the mean
acceleration difference by 8--58%, and the Fr\'echet Gesture Distance by
21--34%. We also conduct a user study and observe that compared to the best
current baselines, around 15.28% of participants indicated our synthesized
gestures appear more plausible, and around 16.32% of participants felt the
gestures had more appropriate affective expressions aligned with the speech.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1"&gt;Uttaran Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Childs_E/0/1/0/all/0/1"&gt;Elizabeth Childs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rewkowski_N/0/1/0/all/0/1"&gt;Nicholas Rewkowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1"&gt;Dinesh Manocha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ECLARE: Extreme Classification with Label Graph Correlations. (arXiv:2108.00261v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00261</id>
        <link href="http://arxiv.org/abs/2108.00261"/>
        <updated>2021-08-03T02:06:28.775Z</updated>
        <summary type="html"><![CDATA[Deep extreme classification (XC) seeks to train deep architectures that can
tag a data point with its most relevant subset of labels from an extremely
large label set. The core utility of XC comes from predicting labels that are
rarely seen during training. Such rare labels hold the key to personalized
recommendations that can delight and surprise a user. However, the large number
of rare labels and small amount of training data per rare label offer
significant statistical and computational challenges. State-of-the-art deep XC
methods attempt to remedy this by incorporating textual descriptions of labels
but do not adequately address the problem. This paper presents ECLARE, a
scalable deep learning architecture that incorporates not only label text, but
also label correlations, to offer accurate real-time predictions within a few
milliseconds. Core contributions of ECLARE include a frugal architecture and
scalable techniques to train deep models along with label correlation graphs at
the scale of millions of labels. In particular, ECLARE offers predictions that
are 2 to 14% more accurate on both publicly available benchmark datasets as
well as proprietary datasets for a related products recommendation task sourced
from the Bing search engine. Code for ECLARE is available at
https://github.com/Extreme-classification/ECLARE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1"&gt;Anshul Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sachdeva_N/0/1/0/all/0/1"&gt;Noveen Sachdeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1"&gt;Sheshansh Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sumeet Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1"&gt;Purushottam Kar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1"&gt;Manik Varma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relevance ranking for proximity full-text search based on additional indexes with multi-component keys. (arXiv:2108.00410v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.00410</id>
        <link href="http://arxiv.org/abs/2108.00410"/>
        <updated>2021-08-03T02:06:28.765Z</updated>
        <summary type="html"><![CDATA[The problem of proximity full-text search is considered. If a search query
contains high-frequently occurring words, then multi-component key indexes
deliver an improvement in the search speed compared with ordinary inverted
indexes. It was shown that we can increase the search speed by up to 130 times
in cases when queries consist of high-frequently occurring words. In this
paper, we investigate how the multi-component key index architecture affects
the quality of the search. We consider several well-known methods of relevance
ranking, where these methods are of different authors. Using these methods, we
perform the search in the ordinary inverted index and then in an index enhanced
with multi-component key indexes. The results show that with multi-component
key indexes we obtain search results that are very close, in terms of relevance
ranking, to the search results that are obtained by means of ordinary inverted
indexes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Veretennikov_A/0/1/0/all/0/1"&gt;Alexander B. Veretennikov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DECAF: Deep Extreme Classification with Label Features. (arXiv:2108.00368v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00368</id>
        <link href="http://arxiv.org/abs/2108.00368"/>
        <updated>2021-08-03T02:06:28.738Z</updated>
        <summary type="html"><![CDATA[Extreme multi-label classification (XML) involves tagging a data point with
its most relevant subset of labels from an extremely large label set, with
several applications such as product-to-product recommendation with millions of
products. Although leading XML algorithms scale to millions of labels, they
largely ignore label meta-data such as textual descriptions of the labels. On
the other hand, classical techniques that can utilize label metadata via
representation learning using deep networks struggle in extreme settings. This
paper develops the DECAF algorithm that addresses these challenges by learning
models enriched by label metadata that jointly learn model parameters and
feature representations using deep networks and offer accurate classification
at the scale of millions of labels. DECAF makes specific contributions to model
architecture design, initialization, and training, enabling it to offer up to
2-6% more accurate prediction than leading extreme classifiers on publicly
available benchmark product-to-product recommendation datasets, such as
LF-AmazonTitles-1.3M. At the same time, DECAF was found to be up to 22x faster
at inference than leading deep extreme classifiers, which makes it suitable for
real-time applications that require predictions within a few milliseconds. The
code for DECAF is available at the following URL
https://github.com/Extreme-classification/DECAF.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1"&gt;Anshul Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dahiya_K/0/1/0/all/0/1"&gt;Kunal Dahiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1"&gt;Sheshansh Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saini_D/0/1/0/all/0/1"&gt;Deepak Saini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sumeet Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1"&gt;Purushottam Kar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1"&gt;Manik Varma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pose-Guided Feature Learning with Knowledge Distillation for Occluded Person Re-Identification. (arXiv:2108.00139v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00139</id>
        <link href="http://arxiv.org/abs/2108.00139"/>
        <updated>2021-08-03T02:06:28.717Z</updated>
        <summary type="html"><![CDATA[Occluded person re-identification (ReID) aims to match person images with
occlusion. It is fundamentally challenging because of the serious occlusion
which aggravates the misalignment problem between images. At the cost of
incorporating a pose estimator, many works introduce pose information to
alleviate the misalignment in both training and testing. To achieve high
accuracy while preserving low inference complexity, we propose a network named
Pose-Guided Feature Learning with Knowledge Distillation (PGFL-KD), where the
pose information is exploited to regularize the learning of semantics aligned
features but is discarded in testing. PGFL-KD consists of a main branch (MB),
and two pose-guided branches, \ieno, a foreground-enhanced branch (FEB), and a
body part semantics aligned branch (SAB). The FEB intends to emphasise the
features of visible body parts while excluding the interference of obstructions
and background (\ieno, foreground feature alignment). The SAB encourages
different channel groups to focus on different body parts to have body part
semantics aligned representation. To get rid of the dependency on pose
information when testing, we regularize the MB to learn the merits of the FEB
and SAB through knowledge distillation and interaction-based training.
Extensive experiments on occluded, partial, and holistic ReID tasks show the
effectiveness of our proposed network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1"&gt;Kecheng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1"&gt;Cuiling Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenjun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiawei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhizheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zheng-Jun Zha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Deep Feature Calibration for Cross-Modal Joint Embedding Learning. (arXiv:2108.00705v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00705</id>
        <link href="http://arxiv.org/abs/2108.00705"/>
        <updated>2021-08-03T02:06:28.695Z</updated>
        <summary type="html"><![CDATA[This paper introduces a two-phase deep feature calibration framework for
efficient learning of semantics enhanced text-image cross-modal joint
embedding, which clearly separates the deep feature calibration in data
preprocessing from training the joint embedding model. We use the Recipe1M
dataset for the technical description and empirical validation. In
preprocessing, we perform deep feature calibration by combining deep feature
engineering with semantic context features derived from raw text-image input
data. We leverage LSTM to identify key terms, NLP methods to produce ranking
scores for key terms before generating the key term feature. We leverage
wideResNet50 to extract and encode the image category semantics to help
semantic alignment of the learned recipe and image embeddings in the joint
latent space. In joint embedding learning, we perform deep feature calibration
by optimizing the batch-hard triplet loss function with soft-margin and double
negative sampling, also utilizing the category-based alignment loss and
discriminator-based alignment loss. Extensive experiments demonstrate that our
SEJE approach with the deep feature calibration significantly outperforms the
state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zhongwei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Ling Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1"&gt;Luo Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Point-to-Distribution Joint Geometry and Color Metric for Point Cloud Quality Assessment. (arXiv:2108.00054v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2108.00054</id>
        <link href="http://arxiv.org/abs/2108.00054"/>
        <updated>2021-08-03T02:06:28.684Z</updated>
        <summary type="html"><![CDATA[Point clouds (PCs) are a powerful 3D visual representation paradigm for many
emerging application domains, especially virtual and augmented reality, and
autonomous vehicles. However, the large amount of PC data required for highly
immersive and realistic experiences requires the availability of efficient,
lossy PC coding solutions are critical. Recently, two MPEG PC coding standards
have been developed to address the relevant application requirements and
further developments are expected in the future. In this context, the
assessment of PC quality, notably for decoded PCs, is critical and asks for the
design of efficient objective PC quality metrics. In this paper, a novel
point-to-distribution metric is proposed for PC quality assessment considering
both the geometry and texture. This new quality metric exploits the
scale-invariance property of the Mahalanobis distance to assess first the
geometry and color point-to-distribution distortions, which are after fused to
obtain a joint geometry and color quality metric. The proposed quality metric
significantly outperforms the best PC quality assessment metrics in the
literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Javaheri_A/0/1/0/all/0/1"&gt;Alireza Javaheri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brites_C/0/1/0/all/0/1"&gt;Catarina Brites&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pereira_F/0/1/0/all/0/1"&gt;Fernando Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ascenso_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Ascenso&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End to End Bangla Speech Synthesis. (arXiv:2108.00500v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.00500</id>
        <link href="http://arxiv.org/abs/2108.00500"/>
        <updated>2021-08-03T02:06:28.669Z</updated>
        <summary type="html"><![CDATA[Text-to-Speech (TTS) system is a system where speech is synthesized from a
given text following any particular approach. Concatenative synthesis, Hidden
Markov Model (HMM) based synthesis, Deep Learning (DL) based synthesis with
multiple building blocks, etc. are the main approaches for implementing a TTS
system. Here, we are presenting our deep learning-based end-to-end Bangla
speech synthesis system. It has been implemented with minimal human annotation
using only 3 major components (Encoder, Decoder, Post-processing net including
waveform synthesis). It does not require any frontend preprocessor and
Grapheme-to-Phoneme (G2P) converter. Our model has been trained with
phonetically balanced 20 hours of single speaker speech data. It has obtained a
3.79 Mean Opinion Score (MOS) on a scale of 5.0 as subjective evaluation and a
0.77 Perceptual Evaluation of Speech Quality(PESQ) score on a scale of [-0.5,
4.5] as objective evaluation. It is outperforming all existing non-commercial
state-of-the-art Bangla TTS systems based on naturalness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharjee_P/0/1/0/all/0/1"&gt;Prithwiraj Bhattacharjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raju_R/0/1/0/all/0/1"&gt;Rajan Saha Raju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmad_A/0/1/0/all/0/1"&gt;Arif Ahmad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1"&gt;M. Shahidur Rahman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Batch Selection Policy for Active Metric Learning. (arXiv:2102.07365v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07365</id>
        <link href="http://arxiv.org/abs/2102.07365"/>
        <updated>2021-08-03T02:06:28.648Z</updated>
        <summary type="html"><![CDATA[Active metric learning is the problem of incrementally selecting high-utility
batches of training data (typically, ordered triplets) to annotate, in order to
progressively improve a learned model of a metric over some input domain as
rapidly as possible. Standard approaches, which independently assess the
informativeness of each triplet in a batch, are susceptible to highly
correlated batches with many redundant triplets and hence low overall utility.
While a recent work \cite{kumari2020batch} proposes batch-decorrelation
strategies for metric learning, they rely on ad hoc heuristics to estimate the
correlation between two triplets at a time. We present a novel batch active
metric learning method that leverages the Maximum Entropy Principle to learn
the least biased estimate of triplet distribution for a given set of prior
constraints. To avoid redundancy between triplets, our method collectively
selects batches with maximum joint entropy, which simultaneously captures both
informativeness and diversity. We take advantage of the submodularity of the
joint entropy function to construct a tractable solution using an efficient
greedy algorithm based on Gram-Schmidt orthogonalization that is provably
$\left( 1 - \frac{1}{e} \right)$-optimal. Our approach is the first batch
active metric learning method to define a unified score that balances
informativeness and diversity for an entire batch of triplets. Experiments with
several real-world datasets demonstrate that our algorithm is robust,
generalizes well to different applications and input modalities, and
consistently outperforms the state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+K_P/0/1/0/all/0/1"&gt;Priyadarshini K&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1"&gt;Siddhartha Chaudhuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borkar_V/0/1/0/all/0/1"&gt;Vivek Borkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1"&gt;Subhasis Chaudhuri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Choreographer: Music Conditioned 3D Dance Generation with AIST++. (arXiv:2101.08779v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08779</id>
        <link href="http://arxiv.org/abs/2101.08779"/>
        <updated>2021-08-03T02:06:28.610Z</updated>
        <summary type="html"><![CDATA[We present AIST++, a new multi-modal dataset of 3D dance motion and music,
along with FACT, a Full-Attention Cross-modal Transformer network for
generating 3D dance motion conditioned on music. The proposed AIST++ dataset
contains 5.2 hours of 3D dance motion in 1408 sequences, covering 10 dance
genres with multi-view videos with known camera poses -- the largest dataset of
this kind to our knowledge. We show that naively applying sequence models such
as transformers to this dataset for the task of music conditioned 3D motion
generation does not produce satisfactory 3D motion that is well correlated with
the input music. We overcome these shortcomings by introducing key changes in
its architecture design and supervision: FACT model involves a deep cross-modal
transformer block with full-attention that is trained to predict $N$ future
motions. We empirically show that these changes are key factors in generating
long sequences of realistic dance motion that are well-attuned to the input
music. We conduct extensive experiments on AIST++ with user studies, where our
method outperforms recent state-of-the-art methods both qualitatively and
quantitatively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruilong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ross_D/0/1/0/all/0/1"&gt;David A. Ross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanazawa_A/0/1/0/all/0/1"&gt;Angjoo Kanazawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Feature Fusion for Video Advertisements Tagging Via Stacking Ensemble. (arXiv:2108.00679v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00679</id>
        <link href="http://arxiv.org/abs/2108.00679"/>
        <updated>2021-08-03T02:06:28.590Z</updated>
        <summary type="html"><![CDATA[Automated tagging of video advertisements has been a critical yet challenging
problem, and it has drawn increasing interests in last years as its
applications seem to be evident in many fields. Despite sustainable efforts
have been made, the tagging task is still suffered from several challenges,
such as, efficiently feature fusion approach is desirable, but under-explored
in previous studies. In this paper, we present our approach for Multimodal
Video Ads Tagging in the 2021 Tencent Advertising Algorithm Competition.
Specifically, we propose a novel multi-modal feature fusion framework, with the
goal to combine complementary information from multiple modalities. This
framework introduces stacking-based ensembling approach to reduce the influence
of varying levels of noise and conflicts between different modalities. Thus,
our framework can boost the performance of the tagging task, compared to
previous methods. To empirically investigate the effectiveness and robustness
of the proposed framework, we conduct extensive experiments on the challenge
datasets. The obtained results suggest that our framework can significantly
outperform related approaches and our method ranks as the 1st place on the
final leaderboard, with a Global Average Precision (GAP) of 82.63%. To better
promote the research in this field, we will release our code in the final
version.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1"&gt;Qingsong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1"&gt;Hai Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhimin Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kele Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Private Retrieval, Computing and Learning: Recent Progress and Future Challenges. (arXiv:2108.00026v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.00026</id>
        <link href="http://arxiv.org/abs/2108.00026"/>
        <updated>2021-08-03T02:06:28.542Z</updated>
        <summary type="html"><![CDATA[Most of our lives are conducted in the cyberspace. The human notion of
privacy translates into a cyber notion of privacy on many functions that take
place in the cyberspace. This article focuses on three such functions: how to
privately retrieve information from cyberspace (privacy in information
retrieval), how to privately leverage large-scale distributed/parallel
processing (privacy in distributed computing), and how to learn/train machine
learning models from private data spread across multiple users (privacy in
distributed (federated) learning). The article motivates each privacy setting,
describes the problem formulation, summarizes breakthrough results in the
history of each problem, and gives recent results and discusses some of the
major ideas that emerged in each field. In addition, the cross-cutting
techniques and interconnections between the three topics are discussed along
with a set of open problems and challenges.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ulukus_S/0/1/0/all/0/1"&gt;Sennur Ulukus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1"&gt;Salman Avestimehr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gastpar_M/0/1/0/all/0/1"&gt;Michael Gastpar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jafar_S/0/1/0/all/0/1"&gt;Syed Jafar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tandon_R/0/1/0/all/0/1"&gt;Ravi Tandon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1"&gt;Chao Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[E-GraphSAGE: A Graph Neural Network based Intrusion Detection System. (arXiv:2103.16329v5 [cs.NI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16329</id>
        <link href="http://arxiv.org/abs/2103.16329"/>
        <updated>2021-08-02T01:58:25.434Z</updated>
        <summary type="html"><![CDATA[This paper presents a new Network Intrusion Detection System (NIDS) based on
Graph Neural Networks (GNNs). GNNs are a relatively new sub-field of deep
neural networks, which can leverage the inherent structure of graph-based data.
Training and evaluation data for NIDSs are typically represented as flow
records, which can naturally be represented in a graph format. This establishes
the potential and motivation for exploring GNNs for network intrusion
detection, which is the focus of this paper. Current approaches to graph
representation learning can only consider topological information and/or node
features, but not edge features. This is a key limitation for the use of
current GNN models for network intrusion detection, since critical flow
information for the detection of anomalous or malicious traffic, e.g. flow
size, flow duration, etc., is represented as edge features in a graph
representation. In this paper, we propose E-GraphSAGE, a first GNN approach
which overcomes this limitation and which allows capturing the edge features of
a graph, in addition to node features and topological information. We present a
novel NIDS based on E-GraphSAGE, and our extensive experimental evaluation on
six recent NIDS benchmark datasets shows that it outperforms the
state-of-the-art in regards to key classification metrics in four out of six
cases, and closely matches it in the other two cases. Our research and initial
basic system demonstrates the potential of GNNs for network intrusion
detection, and provides motivation for further research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lo_W/0/1/0/all/0/1"&gt;Wai Weng Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Layeghy_S/0/1/0/all/0/1"&gt;Siamak Layeghy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarhan_M/0/1/0/all/0/1"&gt;Mohanad Sarhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gallagher_M/0/1/0/all/0/1"&gt;Marcus Gallagher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Portmann_M/0/1/0/all/0/1"&gt;Marius Portmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PILOT: Efficient Planning by Imitation Learning and Optimisation for Safe Autonomous Driving. (arXiv:2011.00509v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.00509</id>
        <link href="http://arxiv.org/abs/2011.00509"/>
        <updated>2021-08-02T01:58:25.414Z</updated>
        <summary type="html"><![CDATA[Achieving a proper balance between planning quality, safety and efficiency is
a major challenge for autonomous driving. Optimisation-based motion planners
are capable of producing safe, smooth and comfortable plans, but often at the
cost of runtime efficiency. On the other hand, naively deploying trajectories
produced by efficient-to-run deep imitation learning approaches might risk
compromising safety. In this paper, we present PILOT -- a planning framework
that comprises an imitation neural network followed by an efficient optimiser
that actively rectifies the network's plan, guaranteeing fulfilment of safety
and comfort requirements. The objective of the efficient optimiser is the same
as the objective of an expensive-to-run optimisation-based planning system that
the neural network is trained offline to imitate. This efficient optimiser
provides a key layer of online protection from learning failures or deficiency
in out-of-distribution situations that might compromise safety or comfort.
Using a state-of-the-art, runtime-intensive optimisation-based method as the
expert, we demonstrate in simulated autonomous driving experiments in CARLA
that PILOT achieves a seven-fold reduction in runtime when compared to the
expert it imitates without sacrificing planning quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pulver_H/0/1/0/all/0/1"&gt;Henry Pulver&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eiras_F/0/1/0/all/0/1"&gt;Francisco Eiras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carozza_L/0/1/0/all/0/1"&gt;Ludovico Carozza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hawasly_M/0/1/0/all/0/1"&gt;Majd Hawasly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1"&gt;Stefano V. Albrecht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramamoorthy_S/0/1/0/all/0/1"&gt;Subramanian Ramamoorthy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Autoencoder-based Error-bounded Compression for Scientific Data. (arXiv:2105.11730v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11730</id>
        <link href="http://arxiv.org/abs/2105.11730"/>
        <updated>2021-08-02T01:58:25.387Z</updated>
        <summary type="html"><![CDATA[Error-bounded lossy compression is becoming an indispensable technique for
the success of today's scientific projects with vast volumes of data produced
during the simulations or instrument data acquisitions. Not only can it
significantly reduce data size, but it also can control the compression errors
based on user-specified error bounds. Autoencoder (AE) models have been widely
used in image compression, but few AE-based compression approaches support
error-bounding features, which are highly required by scientific applications.
To address this issue, we explore using convolutional autoencoders to improve
error-bounded lossy compression for scientific data, with the following three
key contributions. (1) We provide an in-depth investigation of the
characteristics of various autoencoder models and develop an error-bounded
autoencoder-based framework in terms of the SZ model. (2) We optimize the
compression quality for main stages in our designed AE-based error-bounded
compression framework, fine-tuning the block sizes and latent sizes and also
optimizing the compression efficiency of latent vectors. (3) We evaluate our
proposed solution using five real-world scientific datasets and comparing them
with six other related works. Experiments show that our solution exhibits a
very competitive compression quality from among all the compressors in our
tests. In absolute terms, it can obtain a much better compression quality (100%
~ 800% improvement in compression ratio with the same data distortion) compared
with SZ2.1 and ZFP in cases with a high compression ratio.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jinyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Di_S/0/1/0/all/0/1"&gt;Sheng Di&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1"&gt;Kai Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1"&gt;Sian Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dingwen Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xin Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zizhong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cappello_F/0/1/0/all/0/1"&gt;Franck Cappello&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Intrusion Prevention Policies through Optimal Stopping. (arXiv:2106.07160v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07160</id>
        <link href="http://arxiv.org/abs/2106.07160"/>
        <updated>2021-08-02T01:58:25.365Z</updated>
        <summary type="html"><![CDATA[We study automated intrusion prevention using reinforcement learning. In a
novel approach, we formulate the problem of intrusion prevention as an optimal
stopping problem. This formulation allows us insight into the structure of the
optimal policies, which turn out to be threshold based. Since the computation
of the optimal defender policy using dynamic programming is not feasible for
practical cases, we approximate the optimal policy through reinforcement
learning in a simulation environment. To define the dynamics of the simulation,
we emulate the target infrastructure and collect measurements. Our evaluations
show that the learned policies are close to optimal and that they indeed can be
expressed using thresholds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hammar_K/0/1/0/all/0/1"&gt;Kim Hammar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stadler_R/0/1/0/all/0/1"&gt;Rolf Stadler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VQCPC-GAN: Variable-Length Adversarial Audio Synthesis Using Vector-Quantized Contrastive Predictive Coding. (arXiv:2105.01531v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01531</id>
        <link href="http://arxiv.org/abs/2105.01531"/>
        <updated>2021-08-02T01:58:25.319Z</updated>
        <summary type="html"><![CDATA[Influenced by the field of Computer Vision, Generative Adversarial Networks
(GANs) are often adopted for the audio domain using fixed-size two-dimensional
spectrogram representations as the "image data". However, in the (musical)
audio domain, it is often desired to generate output of variable duration. This
paper presents VQCPC-GAN, an adversarial framework for synthesizing
variable-length audio by exploiting Vector-Quantized Contrastive Predictive
Coding (VQCPC). A sequence of VQCPC tokens extracted from real audio data
serves as conditional input to a GAN architecture, providing step-wise
time-dependent features of the generated content. The input noise z
(characteristic in adversarial architectures) remains fixed over time, ensuring
temporal consistency of global features. We evaluate the proposed model by
comparing a diverse set of metrics against various strong baselines. Results
show that, even though the baselines score best, VQCPC-GAN achieves comparable
performance even when generating variable-length audio. Numerous sound examples
are provided in the accompanying website, and we release the code for
reproducibility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nistal_J/0/1/0/all/0/1"&gt;Javier Nistal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aouameur_C/0/1/0/all/0/1"&gt;Cyran Aouameur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lattner_S/0/1/0/all/0/1"&gt;Stefan Lattner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richard_G/0/1/0/all/0/1"&gt;Ga&amp;#xeb;l Richard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Time-Aware Evidence Ranking for Fact-Checking. (arXiv:2009.06402v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06402</id>
        <link href="http://arxiv.org/abs/2009.06402"/>
        <updated>2021-08-02T01:58:25.301Z</updated>
        <summary type="html"><![CDATA[Truth can vary over time. Fact-checking decisions on claim veracity should
therefore take into account temporal information of both the claim and
supporting or refuting evidence. In this work, we investigate the hypothesis
that the timestamp of a Web page is crucial to how it should be ranked for a
given claim. We delineate four temporal ranking methods that constrain evidence
ranking differently and simulate hypothesis-specific evidence rankings given
the evidence timestamps as gold standard. Evidence ranking in three
fact-checking models is ultimately optimized using a learning-to-rank loss
function. Our study reveals that time-aware evidence ranking not only surpasses
relevance assumptions based purely on semantic similarity or position in a
search results list, but also improves veracity predictions of time-sensitive
claims in particular.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Allein_L/0/1/0/all/0/1"&gt;Liesbeth Allein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1"&gt;Isabelle Augenstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1"&gt;Marie-Francine Moens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse generative modeling via parameter-reduction of Boltzmann machines: application to protein-sequence families. (arXiv:2011.11259v3 [q-bio.BM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11259</id>
        <link href="http://arxiv.org/abs/2011.11259"/>
        <updated>2021-08-02T01:58:25.285Z</updated>
        <summary type="html"><![CDATA[Boltzmann machines (BM) are widely used as generative models. For example,
pairwise Potts models (PM), which are instances of the BM class, provide
accurate statistical models of families of evolutionarily related protein
sequences. Their parameters are the local fields, which describe site-specific
patterns of amino-acid conservation, and the two-site couplings, which mirror
the coevolution between pairs of sites. This coevolution reflects structural
and functional constraints acting on protein sequences during evolution. The
most conservative choice to describe the coevolution signal is to include all
possible two-site couplings into the PM. This choice, typical of what is known
as Direct Coupling Analysis, has been successful for predicting residue
contacts in the three-dimensional structure, mutational effects, and in
generating new functional sequences. However, the resulting PM suffers from
important over-fitting effects: many couplings are small, noisy and hardly
interpretable; the PM is close to a critical point, meaning that it is highly
sensitive to small parameter perturbations. In this work, we introduce a
general parameter-reduction procedure for BMs, via a controlled iterative
decimation of the less statistically significant couplings, identified by an
information-based criterion that selects either weak or statistically
unsupported couplings. For several protein families, our procedure allows one
to remove more than $90\%$ of the PM couplings, while preserving the predictive
and generative properties of the original dense PM, and the resulting model is
far away from criticality, hence more robust to noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Barrat_Charlaix_P/0/1/0/all/0/1"&gt;Pierre Barrat-Charlaix&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Muntoni_A/0/1/0/all/0/1"&gt;Anna Paola Muntoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Shimagaki_K/0/1/0/all/0/1"&gt;Kai Shimagaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Weigt_M/0/1/0/all/0/1"&gt;Martin Weigt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zamponi_F/0/1/0/all/0/1"&gt;Francesco Zamponi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SoK: A Modularized Approach to Study the Security of Automatic Speech Recognition Systems. (arXiv:2103.10651v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10651</id>
        <link href="http://arxiv.org/abs/2103.10651"/>
        <updated>2021-08-02T01:58:25.279Z</updated>
        <summary type="html"><![CDATA[With the wide use of Automatic Speech Recognition (ASR) in applications such
as human machine interaction, simultaneous interpretation, audio transcription,
etc., its security protection becomes increasingly important. Although recent
studies have brought to light the weaknesses of popular ASR systems that enable
out-of-band signal attack, adversarial attack, etc., and further proposed
various remedies (signal smoothing, adversarial training, etc.), a systematic
understanding of ASR security (both attacks and defenses) is still missing,
especially on how realistic such threats are and how general existing
protection could be. In this paper, we present our systematization of knowledge
for ASR security and provide a comprehensive taxonomy for existing work based
on a modularized workflow. More importantly, we align the research in this
domain with that on security in Image Recognition System (IRS), which has been
extensively studied, using the domain knowledge in the latter to help
understand where we stand in the former. Generally, both IRS and ASR are
perceptual systems. Their similarities allow us to systematically study
existing literature in ASR security based on the spectrum of attacks and
defense solutions proposed for IRS, and pinpoint the directions of more
advanced attacks and the directions potentially leading to more effective
protection in ASR. In contrast, their differences, especially the complexity of
ASR compared with IRS, help us learn unique challenges and opportunities in ASR
security. Particularly, our experimental study shows that transfer learning
across ASR models is feasible, even in the absence of knowledge about models
(even their types) and training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuxuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiangshan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1"&gt;Xuejing Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shengzhi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaofeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1"&gt;Shanqing Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DiGNet: Learning Scalable Self-Driving Policies for Generic Traffic Scenarios with Graph Neural Networks. (arXiv:2011.06775v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.06775</id>
        <link href="http://arxiv.org/abs/2011.06775"/>
        <updated>2021-08-02T01:58:25.261Z</updated>
        <summary type="html"><![CDATA[Traditional decision and planning frameworks for self-driving vehicles (SDVs)
scale poorly in new scenarios, thus they require tedious hand-tuning of rules
and parameters to maintain acceptable performance in all foreseeable cases.
Recently, self-driving methods based on deep learning have shown promising
results with better generalization capability but less hand engineering effort.
However, most of the previous learning-based methods are trained and evaluated
in limited driving scenarios with scattered tasks, such as lane-following,
autonomous braking, and conditional driving. In this paper, we propose a
graph-based deep network to achieve scalable self-driving that can handle
massive traffic scenarios. Specifically, more than 7,000 km of evaluation is
conducted in a high-fidelity driving simulator, in which our method can obey
the traffic rules and safely navigate the vehicle in a large variety of urban,
rural, and highway environments, including unprotected left turns, narrow
roads, roundabouts, and pedestrian-rich intersections. Demonstration videos are
available at https://caipeide.github.io/dignet/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1"&gt;Peide Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hengli Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yuxiang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Ming Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Verifiable Fingerprinting Scheme for Generative Adversarial Networks. (arXiv:2106.11760v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11760</id>
        <link href="http://arxiv.org/abs/2106.11760"/>
        <updated>2021-08-02T01:58:25.255Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel fingerprinting scheme for the Intellectual
Property (IP) protection of Generative Adversarial Networks (GANs). Prior
solutions for classification models adopt adversarial examples as the
fingerprints, which can raise stealthiness and robustness problems when they
are applied to the GAN models. Our scheme constructs a composite deep learning
model from the target GAN and a classifier. Then we generate stealthy
fingerprint samples from this composite model, and register them to the
classifier for effective ownership verification. This scheme inspires three
concrete methodologies to practically protect the modern GAN models.
Theoretical analysis proves that these methods can satisfy different security
requirements necessary for IP protection. We also conduct extensive experiments
to show that our solutions outperform existing strategies in terms of
stealthiness, functionality-preserving and unremovability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guanlin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Guowen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1"&gt;Han Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1"&gt;Shangwei Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Run Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiwei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianwei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seq2Tens: An Efficient Representation of Sequences by Low-Rank Tensor Projections. (arXiv:2006.07027v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07027</id>
        <link href="http://arxiv.org/abs/2006.07027"/>
        <updated>2021-08-02T01:58:25.248Z</updated>
        <summary type="html"><![CDATA[Sequential data such as time series, video, or text can be challenging to
analyse as the ordered structure gives rise to complex dependencies. At the
heart of this is non-commutativity, in the sense that reordering the elements
of a sequence can completely change its meaning. We use a classical
mathematical object -- the tensor algebra -- to capture such dependencies. To
address the innate computational complexity of high degree tensors, we use
compositions of low-rank tensor projections. This yields modular and scalable
building blocks for neural networks that give state-of-the-art performance on
standard benchmarks such as multivariate time series classification and
generative models for video.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Toth_C/0/1/0/all/0/1"&gt;Csaba Toth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bonnier_P/0/1/0/all/0/1"&gt;Patric Bonnier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oberhauser_H/0/1/0/all/0/1"&gt;Harald Oberhauser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Task 1A DCASE 2021: Acoustic Scene Classification with mismatch-devices using squeeze-excitation technique and low-complexity constraint. (arXiv:2107.14658v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.14658</id>
        <link href="http://arxiv.org/abs/2107.14658"/>
        <updated>2021-08-02T01:58:25.239Z</updated>
        <summary type="html"><![CDATA[Acoustic scene classification (ASC) is one of the most popular problems in
the field of machine listening. The objective of this problem is to classify an
audio clip into one of the predefined scenes using only the audio data. This
problem has considerably progressed over the years in the different editions of
DCASE. It usually has several subtasks that allow to tackle this problem with
different approaches. The subtask presented in this report corresponds to a ASC
problem that is constrained by the complexity of the model as well as having
audio recorded from different devices, known as mismatch devices (real and
simulated). The work presented in this report follows the research line carried
out by the team in previous years. Specifically, a system based on two steps is
proposed: a two-dimensional representation of the audio using the Gamamtone
filter bank and a convolutional neural network using squeeze-excitation
techniques. The presented system outperforms the baseline by about 17
percentage points.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naranjo_Alcazar_J/0/1/0/all/0/1"&gt;Javier Naranjo-Alcazar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Castanos_S/0/1/0/all/0/1"&gt;Sergi Perez-Castanos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cobos_M/0/1/0/all/0/1"&gt;Maximo Cobos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferri_F/0/1/0/all/0/1"&gt;Francesc J. Ferri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuccarello_P/0/1/0/all/0/1"&gt;Pedro Zuccarello&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ByPE-VAE: Bayesian Pseudocoresets Exemplar VAE. (arXiv:2107.09286v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.09286</id>
        <link href="http://arxiv.org/abs/2107.09286"/>
        <updated>2021-08-02T01:58:25.232Z</updated>
        <summary type="html"><![CDATA[Recent studies show that advanced priors play a major role in deep generative
models. Exemplar VAE, as a variant of VAE with an exemplar-based prior, has
achieved impressive results. However, due to the nature of model design, an
exemplar-based model usually requires vast amounts of data to participate in
training, which leads to huge computational complexity. To address this issue,
we propose Bayesian Pseudocoresets Exemplar VAE (ByPE-VAE), a new variant of
VAE with a prior based on Bayesian pseudocoreset. The proposed prior is
conditioned on a small-scale pseudocoreset rather than the whole dataset for
reducing the computational cost and avoiding overfitting. Simultaneously, we
obtain the optimal pseudocoreset via a stochastic optimization algorithm during
VAE training aiming to minimize the Kullback-Leibler divergence between the
prior based on the pseudocoreset and that based on the whole dataset.
Experimental results show that ByPE-VAE can achieve competitive improvements
over the state-of-the-art VAEs in the tasks of density estimation,
representation learning, and generative data augmentation. Particularly, on a
basic VAE architecture, ByPE-VAE is up to 3 times faster than Exemplar VAE
while almost holding the performance. Code is available at our supplementary
materials.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1"&gt;Qingzhong Ai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lirong He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1"&gt;Liangjian Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shiyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Piecewise-linear modelling with feature selection for Li-ion battery end of life prognosis. (arXiv:2104.07576v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07576</id>
        <link href="http://arxiv.org/abs/2104.07576"/>
        <updated>2021-08-02T01:58:25.216Z</updated>
        <summary type="html"><![CDATA[The complex nature of lithium-ion battery degradation has led to many machine
learning based approaches to health forecasting being proposed in literature.
However, machine learning can be computationally intensive. Linear approaches
are faster but have previously been too inflexible for successful prognosis.
For both techniques, the choice and quality of the inputs is a limiting factor
of performance. Piecewise-linear models, combined with automated feature
selection, offer a fast and flexible alternative without being as
computationally intensive as machine learning. Here, a piecewise-linear
approach to battery health forecasting was compared to a Gaussian process
regression tool and found to perform equally well. The input feature selection
process demonstrated the benefit of limiting the correlation between inputs.
Further trials found that the piecewise-linear approach was robust to changing
input size and availability of training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Greenbank_S/0/1/0/all/0/1"&gt;Samuel Greenbank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Howey_D/0/1/0/all/0/1"&gt;David A. Howey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fedlearn-Algo: A flexible open-source privacy-preserving machine learning platform. (arXiv:2107.04129v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04129</id>
        <link href="http://arxiv.org/abs/2107.04129"/>
        <updated>2021-08-02T01:58:25.204Z</updated>
        <summary type="html"><![CDATA[In this paper, we present Fedlearn-Algo, an open-source privacy preserving
machine learning platform. We use this platform to demonstrate our research and
development results on privacy preserving machine learning algorithms. As the
first batch of novel FL algorithm examples, we release vertical federated
kernel binary classification model and vertical federated random forest model.
They have been tested to be more efficient than existing vertical federated
learning models in our practice. Besides the novel FL algorithm examples, we
also release a machine communication module. The uniform data transfer
interface supports transferring widely used data formats between machines. We
will maintain this platform by adding more functional modules and algorithm
examples. The code is available at https://github.com/fedlearnAI/fedlearn-algo.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1"&gt;Chaowei Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiazhou Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_T/0/1/0/all/0/1"&gt;Tao Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1"&gt;Huasong Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1"&gt;Houpu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1"&gt;Peng Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1"&gt;Liefeng Bo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yanqing Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Adversarial Networks for Spatio-temporal Data: A Survey. (arXiv:2008.08903v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.08903</id>
        <link href="http://arxiv.org/abs/2008.08903"/>
        <updated>2021-08-02T01:58:25.198Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) have shown remarkable success in
producing realistic-looking images in the computer vision area. Recently,
GAN-based techniques are shown to be promising for spatio-temporal-based
applications such as trajectory prediction, events generation and time-series
data imputation. While several reviews for GANs in computer vision have been
presented, no one has considered addressing the practical applications and
challenges relevant to spatio-temporal data. In this paper, we have conducted a
comprehensive review of the recent developments of GANs for spatio-temporal
data. We summarise the application of popular GAN architectures for
spatio-temporal data and the common practices for evaluating the performance of
spatio-temporal applications with GANs. Finally, we point out future research
directions to benefit researchers in this area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1"&gt;Nan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1"&gt;Hao Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1"&gt;Wei Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Sichen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_K/0/1/0/all/0/1"&gt;Kyle Kai Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prabowo_A/0/1/0/all/0/1"&gt;Arian Prabowo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1"&gt;Mohammad Saiedur Rahaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1"&gt;Flora D. Salim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Connectivity for Data Distribution in Robot Teams. (arXiv:2103.05091v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05091</id>
        <link href="http://arxiv.org/abs/2103.05091"/>
        <updated>2021-08-02T01:58:25.192Z</updated>
        <summary type="html"><![CDATA[Many algorithms for control of multi-robot teams operate under the assumption
that low-latency, global state information necessary to coordinate agent
actions can readily be disseminated among the team. However, in harsh
environments with no existing communication infrastructure, robots must form
ad-hoc networks, forcing the team to operate in a distributed fashion. To
overcome this challenge, we propose a task-agnostic, decentralized, low-latency
method for data distribution in ad-hoc networks using Graph Neural Networks
(GNN). Our approach enables multi-agent algorithms based on global state
information to function by ensuring it is available at each robot. To do this,
agents glean information about the topology of the network from packet
transmissions and feed it to a GNN running locally which instructs the agent
when and where to transmit the latest state information. We train the
distributed GNN communication policies via reinforcement learning using the
average Age of Information as the reward function and show that it improves
training stability compared to task-specific reward functions. Our approach
performs favorably compared to industry-standard methods for data distribution
such as random flooding and round robin. We also show that the trained policies
generalize to larger teams of both static and mobile agents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tolstaya_E/0/1/0/all/0/1"&gt;Ekaterina Tolstaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Butler_L/0/1/0/all/0/1"&gt;Landon Butler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mox_D/0/1/0/all/0/1"&gt;Daniel Mox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paulos_J/0/1/0/all/0/1"&gt;James Paulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vijay Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1"&gt;Alejandro Ribeiro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Sample Complexity of Best-$k$ Items Selection from Pairwise Comparisons. (arXiv:2007.03133v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.03133</id>
        <link href="http://arxiv.org/abs/2007.03133"/>
        <updated>2021-08-02T01:58:25.185Z</updated>
        <summary type="html"><![CDATA[This paper studies the sample complexity (aka number of comparisons) bounds
for the active best-$k$ items selection from pairwise comparisons. From a given
set of items, the learner can make pairwise comparisons on every pair of items,
and each comparison returns an independent noisy result about the preferred
item. At any time, the learner can adaptively choose a pair of items to compare
according to past observations (i.e., active learning). The learner's goal is
to find the (approximately) best-$k$ items with a given confidence, while
trying to use as few comparisons as possible. In this paper, we study two
problems: (i) finding the probably approximately correct (PAC) best-$k$ items
and (ii) finding the exact best-$k$ items, both under strong stochastic
transitivity and stochastic triangle inequality. For PAC best-$k$ items
selection, we first show a lower bound and then propose an algorithm whose
sample complexity upper bound matches the lower bound up to a constant factor.
For the exact best-$k$ items selection, we first prove a worst-instance lower
bound. We then propose two algorithms based on our PAC best items selection
algorithms: one works for $k=1$ and is sample complexity optimal up to a loglog
factor, and the other works for all values of $k$ and is sample complexity
optimal up to a log factor.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1"&gt;Wenbo Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jia Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shroff_N/0/1/0/all/0/1"&gt;Ness B. Shroff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepKriging: Spatially Dependent Deep Neural Networks for Spatial Prediction. (arXiv:2007.11972v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.11972</id>
        <link href="http://arxiv.org/abs/2007.11972"/>
        <updated>2021-08-02T01:58:25.166Z</updated>
        <summary type="html"><![CDATA[In spatial statistics, a common objective is to predict the values of a
spatial process at unobserved locations by exploiting spatial dependence. In
geostatistics, Kriging provides the best linear unbiased predictor using
covariance functions and is often associated with Gaussian processes. However,
when considering non-linear prediction for non-Gaussian and categorical data,
the Kriging prediction is not necessarily optimal, and the associated variance
is often overly optimistic. We propose to use deep neural networks (DNNs) for
spatial prediction. Although DNNs are widely used for general classification
and prediction, they have not been studied thoroughly for data with spatial
dependence. In this work, we propose a novel neural network structure for
spatial prediction by adding an embedding layer of spatial coordinates with
basis functions. We show in theory that the proposed DeepKriging method has
multiple advantages over Kriging and classical DNNs only with spatial
coordinates as features. We also provide density prediction for uncertainty
quantification without any distributional assumption and apply the method to
PM$_{2.5}$ concentrations across the continental United States.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wanfang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuxiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Reich_B/0/1/0/all/0/1"&gt;Brian J Reich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Ying Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Impact of Data on the Stability of Learning-Based Control- Extended Version. (arXiv:2011.10596v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10596</id>
        <link href="http://arxiv.org/abs/2011.10596"/>
        <updated>2021-08-02T01:58:25.148Z</updated>
        <summary type="html"><![CDATA[Despite the existence of formal guarantees for learning-based control
approaches, the relationship between data and control performance is still
poorly understood. In this paper, we propose a Lyapunov-based measure for
quantifying the impact of data on the certifiable control performance. By
modeling unknown system dynamics through Gaussian processes, we can determine
the interrelation between model uncertainty and satisfaction of stability
conditions. This allows us to directly asses the impact of data on the provable
stationary control performance, and thereby the value of the data for the
closed-loop system performance. Our approach is applicable to a wide variety of
unknown nonlinear systems that are to be controlled by a generic learning-based
control law, and the results obtained in numerical simulations indicate the
efficacy of the proposed measure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lederer_A/0/1/0/all/0/1"&gt;Armin Lederer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Capone_A/0/1/0/all/0/1"&gt;Alexandre Capone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Beckers_T/0/1/0/all/0/1"&gt;Thomas Beckers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Umlauft_J/0/1/0/all/0/1"&gt;Jonas Umlauft&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hirche_S/0/1/0/all/0/1"&gt;Sandra Hirche&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis. (arXiv:2103.14910v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14910</id>
        <link href="http://arxiv.org/abs/2103.14910"/>
        <updated>2021-08-02T01:58:25.136Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose MINE to perform novel view synthesis and depth
estimation via dense 3D reconstruction from a single image. Our approach is a
continuous depth generalization of the Multiplane Images (MPI) by introducing
the NEural radiance fields (NeRF). Given a single image as input, MINE predicts
a 4-channel image (RGB and volume density) at arbitrary depth values to jointly
reconstruct the camera frustum and fill in occluded contents. The reconstructed
and inpainted frustum can then be easily rendered into novel RGB or depth views
using differentiable rendering. Extensive experiments on RealEstate10K, KITTI
and Flowers Light Fields show that our MINE outperforms state-of-the-art by a
large margin in novel view synthesis. We also achieve competitive results in
depth estimation on iBims-1 and NYU-v2 without annotated depth supervision. Our
source code is available at https://github.com/vincentfung13/MINE]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiaxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zijian Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1"&gt;Qi She&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1"&gt;Henghui Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Changhu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1"&gt;Gim Hee Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Methods for Pruning Deep Neural Networks. (arXiv:2011.00241v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.00241</id>
        <link href="http://arxiv.org/abs/2011.00241"/>
        <updated>2021-08-02T01:58:25.131Z</updated>
        <summary type="html"><![CDATA[This paper presents a survey of methods for pruning deep neural networks. It
begins by categorising over 150 studies based on the underlying approach used
and then focuses on three categories: methods that use magnitude based pruning,
methods that utilise clustering to identify redundancy, and methods that use
sensitivity analysis to assess the effect of pruning. Some of the key
influencing studies within these categories are presented to highlight the
underlying approaches and results achieved. Most studies present results which
are distributed in the literature as new architectures, algorithms and data
sets have developed with time, making comparison across different studied
difficult. The paper therefore provides a resource for the community that can
be used to quickly compare the results from many different methods on a variety
of data sets, and a range of architectures, including AlexNet, ResNet, DenseNet
and VGG. The resource is illustrated by comparing the results published for
pruning AlexNet and ResNet50 on ImageNet and ResNet56 and VGG16 on the CIFAR10
data to reveal which pruning methods work well in terms of retaining accuracy
whilst achieving good compression rates. The paper concludes by identifying
some promising directions for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vadera_S/0/1/0/all/0/1"&gt;Sunil Vadera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ameen_S/0/1/0/all/0/1"&gt;Salem Ameen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Effects of Mild Over-parameterization on the Optimization Landscape of Shallow ReLU Neural Networks. (arXiv:2006.01005v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.01005</id>
        <link href="http://arxiv.org/abs/2006.01005"/>
        <updated>2021-08-02T01:58:25.115Z</updated>
        <summary type="html"><![CDATA[We study the effects of mild over-parameterization on the optimization
landscape of a simple ReLU neural network of the form
$\mathbf{x}\mapsto\sum_{i=1}^k\max\{0,\mathbf{w}_i^{\top}\mathbf{x}\}$, in a
well-studied teacher-student setting where the target values are generated by
the same architecture, and when directly optimizing over the population squared
loss with respect to Gaussian inputs. We prove that while the objective is
strongly convex around the global minima when the teacher and student networks
possess the same number of neurons, it is not even \emph{locally convex} after
any amount of over-parameterization. Moreover, related desirable properties
(e.g., one-point strong convexity and the Polyak-{\L}ojasiewicz condition) also
do not hold even locally. On the other hand, we establish that the objective
remains one-point strongly convex in \emph{most} directions (suitably defined),
and show an optimization guarantee under this property. For the non-global
minima, we prove that adding even just a single neuron will turn a non-global
minimum into a saddle point. This holds under some technical conditions which
we validate empirically. These results provide a possible explanation for why
recovering a global minimum becomes significantly easier when we
over-parameterize, even if the amount of over-parameterization is very
moderate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Safran_I/0/1/0/all/0/1"&gt;Itay Safran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yehudai_G/0/1/0/all/0/1"&gt;Gilad Yehudai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1"&gt;Ohad Shamir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-Time Regression with Dividing Local Gaussian Processes. (arXiv:2006.09446v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09446</id>
        <link href="http://arxiv.org/abs/2006.09446"/>
        <updated>2021-08-02T01:58:25.097Z</updated>
        <summary type="html"><![CDATA[The increased demand for online prediction and the growing availability of
large data sets drives the need for computationally efficient models. While
exact Gaussian process regression shows various favorable theoretical
properties (uncertainty estimate, unlimited expressive power), the poor scaling
with respect to the training set size prohibits its application in big data
regimes in real-time. Therefore, this paper proposes dividing local Gaussian
processes, which are a novel, computationally efficient modeling approach based
on Gaussian process regression. Due to an iterative, data-driven division of
the input space, they achieve a sublinear computational complexity in the total
number of training points in practice, while providing excellent predictive
distributions. A numerical evaluation on real-world data sets shows their
advantages over other state-of-the-art methods in terms of accuracy as well as
prediction and update speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lederer_A/0/1/0/all/0/1"&gt;Armin Lederer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Conejo_A/0/1/0/all/0/1"&gt;Alejandro Jose Ordonez Conejo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maier_K/0/1/0/all/0/1"&gt;Korbinian Maier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1"&gt;Wenxin Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Umlauft_J/0/1/0/all/0/1"&gt;Jonas Umlauft&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hirche_S/0/1/0/all/0/1"&gt;Sandra Hirche&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-driven modeling of time-domain induced polarization. (arXiv:2107.14796v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14796</id>
        <link href="http://arxiv.org/abs/2107.14796"/>
        <updated>2021-08-02T01:58:25.066Z</updated>
        <summary type="html"><![CDATA[We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Berube_C/0/1/0/all/0/1"&gt;Charles L. B&amp;#xe9;rub&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berube_P/0/1/0/all/0/1"&gt;Pierre B&amp;#xe9;rub&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Debiased Explainable Pairwise Ranking from Implicit Feedback. (arXiv:2107.14768v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.14768</id>
        <link href="http://arxiv.org/abs/2107.14768"/>
        <updated>2021-08-02T01:58:25.056Z</updated>
        <summary type="html"><![CDATA[Recent work in recommender systems has emphasized the importance of fairness,
with a particular interest in bias and transparency, in addition to predictive
accuracy. In this paper, we focus on the state of the art pairwise ranking
model, Bayesian Personalized Ranking (BPR), which has previously been found to
outperform pointwise models in predictive accuracy, while also being able to
handle implicit feedback. Specifically, we address two limitations of BPR: (1)
BPR is a black box model that does not explain its outputs, thus limiting the
user's trust in the recommendations, and the analyst's ability to scrutinize a
model's outputs; and (2) BPR is vulnerable to exposure bias due to the data
being Missing Not At Random (MNAR). This exposure bias usually translates into
an unfairness against the least popular items because they risk being
under-exposed by the recommender system. In this work, we first propose a novel
explainable loss function and a corresponding Matrix Factorization-based model
called Explainable Bayesian Personalized Ranking (EBPR) that generates
recommendations along with item-based explanations. Then, we theoretically
quantify additional exposure bias resulting from the explainability, and use it
as a basis to propose an unbiased estimator for the ideal EBPR loss. The result
is a ranking model that aptly captures both debiased and explainable user
preferences. Finally, we perform an empirical study on three real-world
datasets that demonstrate the advantages of our proposed models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Damak_K/0/1/0/all/0/1"&gt;Khalil Damak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khenissi_S/0/1/0/all/0/1"&gt;Sami Khenissi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasraoui_O/0/1/0/all/0/1"&gt;Olfa Nasraoui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the importance of cross-task features for class-incremental learning. (arXiv:2106.11930v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11930</id>
        <link href="http://arxiv.org/abs/2106.11930"/>
        <updated>2021-08-02T01:58:25.046Z</updated>
        <summary type="html"><![CDATA[In class-incremental learning, an agent with limited resources needs to learn
a sequence of classification tasks, forming an ever growing classification
problem, with the constraint of not being able to access data from previous
tasks. The main difference with task-incremental learning, where a task-ID is
available at inference time, is that the learner also needs to perform
cross-task discrimination, i.e. distinguish between classes that have not been
seen together. Approaches to tackle this problem are numerous and mostly make
use of an external memory (buffer) of non-negligible size. In this paper, we
ablate the learning of cross-task features and study its influence on the
performance of basic replay strategies used for class-IL. We also define a new
forgetting measure for class-incremental learning, and see that forgetting is
not the principal cause of low performance. Our experimental results show that
future algorithms for class-incremental learning should not only prevent
forgetting, but also aim to improve the quality of the cross-task features, and
the knowledge transfer between tasks. This is especially important when tasks
contain limited amount of data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soutif__Cormerais_A/0/1/0/all/0/1"&gt;Albin Soutif--Cormerais&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masana_M/0/1/0/all/0/1"&gt;Marc Masana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1"&gt;Joost Van de Weijer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Twardowski_B/0/1/0/all/0/1"&gt;Bart&amp;#x142;omiej Twardowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lithography Hotspot Detection via Heterogeneous Federated Learning with Local Adaptation. (arXiv:2107.04367v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04367</id>
        <link href="http://arxiv.org/abs/2107.04367"/>
        <updated>2021-08-02T01:58:25.022Z</updated>
        <summary type="html"><![CDATA[As technology scaling is approaching the physical limit, lithography hotspot
detection has become an essential task in design for manufacturability. While
the deployment of pattern matching or machine learning in hotspot detection can
help save significant simulation time, such methods typically demand for
non-trivial quality data to build the model, which most design houses are short
of. Moreover, the design houses are also unwilling to directly share such data
with the other houses to build a unified model, which can be ineffective for
the design house with unique design patterns due to data insufficiency. On the
other hand, with data homogeneity in each design house, the locally trained
models can be easily over-fitted, losing generalization ability and robustness.
In this paper, we propose a heterogeneous federated learning framework for
lithography hotspot detection that can address the aforementioned issues. On
one hand, the framework can build a more robust centralized global sub-model
through heterogeneous knowledge sharing while keeping local data private. On
the other hand, the global sub-model can be combined with a local sub-model to
better adapt to local data heterogeneity. The experimental results show that
the proposed framework can overcome the challenge of non-independent and
identically distributed (non-IID) data and heterogeneous communication to
achieve very high performance in comparison to other state-of-the-art methods
while guaranteeing a good convergence rate in various scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xuezhong Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1"&gt;Jingyu Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jinming Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiran Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuo_C/0/1/0/all/0/1"&gt;Cheng Zhuo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthetic flow-based cryptomining attack generation through Generative Adversarial Networks. (arXiv:2107.14776v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.14776</id>
        <link href="http://arxiv.org/abs/2107.14776"/>
        <updated>2021-08-02T01:58:25.005Z</updated>
        <summary type="html"><![CDATA[Due to the growing rise of cyber attacks in the Internet, flow-based data
sets are crucial to increase the performance of the Machine Learning (ML)
components that run in network-based intrusion detection systems (IDS). To
overcome the existing network traffic data shortage in attack analysis, recent
works propose Generative Adversarial Networks (GANs) for synthetic flow-based
network traffic generation. Data privacy is appearing more and more as a strong
requirement when processing such network data, which suggests to find solutions
where synthetic data can fully replace real data. Because of the
ill-convergence of the GAN training, none of the existing solutions can
generate high-quality fully synthetic data that can totally substitute real
data in the training of IDS ML components. Therefore, they mix real with
synthetic data, which acts only as data augmentation components, leading to
privacy breaches as real data is used. In sharp contrast, in this work we
propose a novel deterministic way to measure the quality of the synthetic data
produced by a GAN both with respect to the real data and to its performance
when used for ML tasks. As a byproduct, we present a heuristic that uses these
metrics for selecting the best performing generator during GAN training,
leading to a stopping criterion. An additional heuristic is proposed to select
the best performing GANs when different types of synthetic data are to be used
in the same ML task. We demonstrate the adequacy of our proposal by generating
synthetic cryptomining attack traffic and normal traffic flow-based data using
an enhanced version of a Wasserstein GAN. We show that the generated synthetic
network traffic can completely replace real data when training a ML-based
cryptomining detector, obtaining similar performance and avoiding privacy
violations, since real data is not used in the training of the ML-based
detector.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mozo_A/0/1/0/all/0/1"&gt;Alberto Mozo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Prieto_A/0/1/0/all/0/1"&gt;&amp;#xc1;ngel Gonz&amp;#xe1;lez-Prieto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pastor_A/0/1/0/all/0/1"&gt;Antonio Pastor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_Canaval_S/0/1/0/all/0/1"&gt;Sandra G&amp;#xf3;mez-Canaval&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talavera_E/0/1/0/all/0/1"&gt;Edgar Talavera&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards General Function Approximation in Zero-Sum Markov Games. (arXiv:2107.14702v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2107.14702</id>
        <link href="http://arxiv.org/abs/2107.14702"/>
        <updated>2021-08-02T01:58:24.998Z</updated>
        <summary type="html"><![CDATA[This paper considers two-player zero-sum finite-horizon Markov games with
simultaneous moves. The study focuses on the challenging settings where the
value function or the model is parameterized by general function classes.
Provably efficient algorithms for both decoupled and {coordinated} settings are
developed. In the {decoupled} setting where the agent controls a single player
and plays against an arbitrary opponent, we propose a new model-free algorithm.
The sample complexity is governed by the Minimax Eluder dimension -- a new
dimension of the function class in Markov games. As a special case, this method
improves the state-of-the-art algorithm by a $\sqrt{d}$ factor in the regret
when the reward function and transition kernel are parameterized with
$d$-dimensional linear features. In the {coordinated} setting where both
players are controlled by the agent, we propose a model-based algorithm and a
model-free algorithm. In the model-based algorithm, we prove that sample
complexity can be bounded by a generalization of Witness rank to Markov games.
The model-free algorithm enjoys a $\sqrt{K}$-regret upper bound where $K$ is
the number of episodes. Our algorithms are based on new techniques of alternate
optimism.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1"&gt;Baihe Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jason D. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhaoran Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhuoran Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ChemRL-GEM: Geometry Enhanced Molecular Representation Learning for Property Prediction. (arXiv:2106.06130v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06130</id>
        <link href="http://arxiv.org/abs/2106.06130"/>
        <updated>2021-08-02T01:58:24.993Z</updated>
        <summary type="html"><![CDATA[Effective molecular representation learning is of great importance to
facilitate molecular property prediction, which is a fundamental task for the
drug and material industry. Recent advances in graph neural networks (GNNs)
have shown great promise in applying GNNs for molecular representation
learning. Moreover, a few recent studies have also demonstrated successful
applications of self-supervised learning methods to pre-train the GNNs to
overcome the problem of insufficient labeled molecules. However, existing GNNs
and pre-training strategies usually treat molecules as topological graph data
without fully utilizing the molecular geometry information. Whereas, the
three-dimensional (3D) spatial structure of a molecule, a.k.a molecular
geometry, is one of the most critical factors for determining molecular
physical, chemical, and biological properties. To this end, we propose a novel
Geometry Enhanced Molecular representation learning method (GEM) for Chemical
Representation Learning (ChemRL). At first, we design a geometry-based GNN
architecture that simultaneously models atoms, bonds, and bond angles in a
molecule. To be specific, we devised double graphs for a molecule: The first
one encodes the atom-bond relations; The second one encodes bond-angle
relations. Moreover, on top of the devised GNN architecture, we propose several
novel geometry-level self-supervised learning strategies to learn spatial
knowledge by utilizing the local and global molecular 3D structures. We compare
ChemRL-GEM with various state-of-the-art (SOTA) baselines on different
molecular benchmarks and exhibit that ChemRL-GEM can significantly outperform
all baselines in both regression and classification tasks. For example, the
experimental results show an overall improvement of 8.8% on average compared to
SOTA baselines on the regression tasks, demonstrating the superiority of the
proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1"&gt;Xiaomin Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lihang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jieqiong Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Donglong He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shanzhuo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingbo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haifeng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An iterative coordinate descent algorithm to compute sparse low-rank approximations. (arXiv:2107.14608v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14608</id>
        <link href="http://arxiv.org/abs/2107.14608"/>
        <updated>2021-08-02T01:58:24.987Z</updated>
        <summary type="html"><![CDATA[In this paper, we describe a new algorithm to build a few sparse principal
components from a given data matrix. Our approach does not explicitly create
the covariance matrix of the data and can be viewed as an extension of the
Kogbetliantz algorithm to build an approximate singular value decomposition for
a few principal components. We show the performance of the proposed algorithm
to recover sparse principal components on various datasets from the literature
and perform dimensionality reduction for classification applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rusu_C/0/1/0/all/0/1"&gt;Cristian Rusu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practical Attacks on Voice Spoofing Countermeasures. (arXiv:2107.14642v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.14642</id>
        <link href="http://arxiv.org/abs/2107.14642"/>
        <updated>2021-08-02T01:58:24.982Z</updated>
        <summary type="html"><![CDATA[Voice authentication has become an integral part in security-critical
operations, such as bank transactions and call center conversations. The
vulnerability of automatic speaker verification systems (ASVs) to spoofing
attacks instigated the development of countermeasures (CMs), whose task is to
tell apart bonafide and spoofed speech. Together, ASVs and CMs form today's
voice authentication platforms, advertised as an impregnable access control
mechanism. We develop the first practical attack on CMs, and show how a
malicious actor may efficiently craft audio samples to bypass voice
authentication in its strictest form. Previous works have primarily focused on
non-proactive attacks or adversarial strategies against ASVs that do not
produce speech in the victim's voice. The repercussions of our attacks are far
more severe, as the samples we generate sound like the victim, eliminating any
chance of plausible deniability. Moreover, the few existing adversarial attacks
against CMs mistakenly optimize spoofed speech in the feature space and do not
take into account the existence of ASVs, resulting in inferior synthetic audio
that fails in realistic settings. We eliminate these obstacles through our key
technical contribution: a novel joint loss function that enables mounting
advanced adversarial attacks against combined ASV/CM deployments directly in
the time domain. Our adversarials achieve concerning black-box success rates
against state-of-the-art authentication platforms (up to 93.57\%). Finally, we
perform the first targeted, over-telephony-network attack on CMs, bypassing
several challenges and enabling various potential threats, given the increased
use of voice biometrics in call centers. Our results call into question the
security of modern voice authentication systems in light of the real threat of
attackers bypassing these measures to gain access to users' most valuable
resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kassis_A/0/1/0/all/0/1"&gt;Andre Kassis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hengartner_U/0/1/0/all/0/1"&gt;Urs Hengartner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A common variable minimax theorem for graphs. (arXiv:2107.14747v1 [math.SP])]]></title>
        <id>http://arxiv.org/abs/2107.14747</id>
        <link href="http://arxiv.org/abs/2107.14747"/>
        <updated>2021-08-02T01:58:24.976Z</updated>
        <summary type="html"><![CDATA[Let $\mathcal{G} = \{G_1 = (V, E_1), \dots, G_m = (V, E_m)\}$ be a collection
of $m$ graphs defined on a common set of vertices $V$ but with different edge
sets $E_1, \dots, E_m$. Informally, a function $f :V \rightarrow \mathbb{R}$ is
smooth with respect to $G_k = (V,E_k)$ if $f(u) \sim f(v)$ whenever $(u, v) \in
E_k$. We study the problem of understanding whether there exists a nonconstant
function that is smooth with respect to all graphs in $\mathcal{G}$,
simultaneously, and how to find it if it exists.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Coifman_R/0/1/0/all/0/1"&gt;Ronald R. Coifman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Marshall_N/0/1/0/all/0/1"&gt;Nicholas F. Marshall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Steinerberger_S/0/1/0/all/0/1"&gt;Stefan Steinerberger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Propagation Graph Estimation from Individual's Time Series of Observed States. (arXiv:2005.04954v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.04954</id>
        <link href="http://arxiv.org/abs/2005.04954"/>
        <updated>2021-08-02T01:58:24.958Z</updated>
        <summary type="html"><![CDATA[Various things propagate through the medium of individuals. Some individuals
follow the others and take the states similar to their states a small number of
time steps later. In this paper, we study the problem of estimating the state
propagation order of individuals from the real-valued state sequences of all
the individuals. We propose a method to estimate the propagation direction
between individuals by the sum of the time delay of one individual's state
positions from the other individual's matched state position averaged over the
minimum cost alignments and show how to calculate it efficiently. The
propagation order estimated by our proposed method is demonstrated to be
significantly more accurate than that by a baseline method for our synthetic
datasets, and also to be consistent with visually recognizable propagation
orders for the dataset of Japanese stock price time series and biological cell
firing state sequences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hayashi_T/0/1/0/all/0/1"&gt;Tatsuya Hayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakamura_A/0/1/0/all/0/1"&gt;Atsuyoshi Nakamura&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Attention Networks for Low-Level Vision-and-Language Navigation. (arXiv:1911.12377v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.12377</id>
        <link href="http://arxiv.org/abs/1911.12377"/>
        <updated>2021-08-02T01:58:24.951Z</updated>
        <summary type="html"><![CDATA[Vision-and-Language Navigation (VLN) is a challenging task in which an agent
needs to follow a language-specified path to reach a target destination. The
goal gets even harder as the actions available to the agent get simpler and
move towards low-level, atomic interactions with the environment. This setting
takes the name of low-level VLN. In this paper, we strive for the creation of
an agent able to tackle three key issues: multi-modality, long-term
dependencies, and adaptability towards different locomotive settings. To that
end, we devise "Perceive, Transform, and Act" (PTA): a fully-attentive VLN
architecture that leaves the recurrent approach behind and the first
Transformer-like architecture incorporating three different modalities -
natural language, images, and low-level actions for the agent control. In
particular, we adopt an early fusion strategy to merge lingual and visual
information efficiently in our encoder. We then propose to refine the decoding
phase with a late fusion extension between the agent's history of actions and
the perceptual modalities. We experimentally validate our model on two
datasets: PTA achieves promising results in low-level VLN on R2R and achieves
good performance in the recently proposed R4R benchmark. Our code is publicly
available at https://github.com/aimagelab/perceive-transform-and-act.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Landi_F/0/1/0/all/0/1"&gt;Federico Landi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1"&gt;Lorenzo Baraldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1"&gt;Marcella Cornia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Corsini_M/0/1/0/all/0/1"&gt;Massimiliano Corsini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1"&gt;Rita Cucchiara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latent Factor Decomposition Model: Applications for Questionnaire Data. (arXiv:2104.15106v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.15106</id>
        <link href="http://arxiv.org/abs/2104.15106"/>
        <updated>2021-08-02T01:58:24.944Z</updated>
        <summary type="html"><![CDATA[The analysis of clinical questionnaire data comes with many inherent
challenges. These challenges include the handling of data with missing fields,
as well as the overall interpretation of a dataset with many fields of
different scales and forms. While numerous methods have been developed to
address these challenges, they are often not robust, statistically sound, or
easily interpretable. Here, we propose a latent factor modeling framework that
extends the principal component analysis for both categorical and quantitative
data with missing elements. The model simultaneously provides the principal
components (basis) and each patients' projections on these bases in a latent
space. We show an application of our modeling framework through Irritable Bowel
Syndrome (IBS) symptoms, where we find correlations between these projections
and other standardized patient symptom scales. This latent factor model can be
easily applied to different clinical questionnaire datasets for clustering
analysis and interpretable inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McLaughlin_C/0/1/0/all/0/1"&gt;Connor J. McLaughlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kokkotou_E/0/1/0/all/0/1"&gt;Efi G. Kokkotou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_J/0/1/0/all/0/1"&gt;Jean A. King&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Conboy_L/0/1/0/all/0/1"&gt;Lisa A. Conboy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yousefi_A/0/1/0/all/0/1"&gt;Ali Yousefi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A data-science-driven short-term analysis of Amazon, Apple, Google, and Microsoft stocks. (arXiv:2107.14695v1 [q-fin.ST])]]></title>
        <id>http://arxiv.org/abs/2107.14695</id>
        <link href="http://arxiv.org/abs/2107.14695"/>
        <updated>2021-08-02T01:58:24.936Z</updated>
        <summary type="html"><![CDATA[In this paper, we implement a combination of technical analysis and
machine/deep learning-based analysis to build a trend classification model. The
goal of the paper is to apprehend short-term market movement, and incorporate
it to improve the underlying stochastic model. Also, the analysis presented in
this paper can be implemented in a \emph{model-independent} fashion. We execute
a data-science-driven technique that makes short-term forecasts dependent on
the price trends of current stock market data. Based on the analysis, three
different labels are generated for a data set: $+1$ (buy signal), $0$ (hold
signal), or $-1$ (sell signal). We propose a detailed analysis of four major
stocks- Amazon, Apple, Google, and Microsoft. We implement various technical
indicators to label the data set according to the trend and train various
models for trend estimation. Statistical analysis of the outputs and
classification results are obtained.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Ekapure_S/0/1/0/all/0/1"&gt;Shubham Ekapure&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Jiruwala_N/0/1/0/all/0/1"&gt;Nuruddin Jiruwala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Patnaik_S/0/1/0/all/0/1"&gt;Sohan Patnaik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+SenGupta_I/0/1/0/all/0/1"&gt;Indranil SenGupta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Variational Autoencoders for Semi-Supervised Learning: In Defense of Product-of-Experts. (arXiv:2101.07240v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.07240</id>
        <link href="http://arxiv.org/abs/2101.07240"/>
        <updated>2021-08-02T01:58:24.930Z</updated>
        <summary type="html"><![CDATA[Multimodal generative models should be able to learn a meaningful latent
representation that enables a coherent joint generation of all modalities
(e.g., images and text). Many applications also require the ability to
accurately sample modalities conditioned on observations of a subset of the
modalities. Often not all modalities may be observed for all training data
points, so semi-supervised learning should be possible. In this study, we
propose a novel product-of-experts (PoE) based variational autoencoder that
have these desired properties. We benchmark it against a mixture-of-experts
(MoE) approach and an approach of combining the modalities with an additional
encoder network. An empirical evaluation shows that the PoE based models can
outperform the contrasted models. Our experiments support the intuition that
PoE models are more suited for a conjunctive combination of modalities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kutuzova_S/0/1/0/all/0/1"&gt;Svetlana Kutuzova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_O/0/1/0/all/0/1"&gt;Oswin Krause&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McCloskey_D/0/1/0/all/0/1"&gt;Douglas McCloskey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nielsen_M/0/1/0/all/0/1"&gt;Mads Nielsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Igel_C/0/1/0/all/0/1"&gt;Christian Igel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Voice2Series: Reprogramming Acoustic Models for Time Series Classification. (arXiv:2106.09296v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09296</id>
        <link href="http://arxiv.org/abs/2106.09296"/>
        <updated>2021-08-02T01:58:24.913Z</updated>
        <summary type="html"><![CDATA[Learning to classify time series with limited data is a practical yet
challenging problem. Current methods are primarily based on hand-designed
feature extraction rules or domain-specific data augmentation. Motivated by the
advances in deep speech processing models and the fact that voice data are
univariate temporal signals, in this paper, we propose Voice2Series (V2S), a
novel end-to-end approach that reprograms acoustic models for time series
classification, through input transformation learning and output label mapping.
Leveraging the representation learning power of a large-scale pre-trained
speech processing model, on 30 different time series tasks we show that V2S
either outperforms or is tied with state-of-the-art methods on 20 tasks, and
improves their average accuracy by 1.84%. We further provide a theoretical
justification of V2S by proving its population risk is upper bounded by the
source risk and a Wasserstein distance accounting for feature alignment via
reprogramming. Our results offer new and effective means to time series
classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chao-Han Huck Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1"&gt;Yun-Yun Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Pin-Yu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FloMo: Tractable Motion Prediction with Normalizing Flows. (arXiv:2103.03614v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03614</id>
        <link href="http://arxiv.org/abs/2103.03614"/>
        <updated>2021-08-02T01:58:24.907Z</updated>
        <summary type="html"><![CDATA[The future motion of traffic participants is inherently uncertain. To plan
safely, therefore, an autonomous agent must take into account multiple possible
trajectory outcomes and prioritize them. Recently, this problem has been
addressed with generative neural networks. However, most generative models
either do not learn the true underlying trajectory distribution reliably, or do
not allow predictions to be associated with likelihoods. In our work, we model
motion prediction directly as a density estimation problem with a normalizing
flow between a noise distribution and the future motion distribution. Our
model, named FloMo, allows likelihoods to be computed in a single network pass
and can be trained directly with maximum likelihood estimation. Furthermore, we
propose a method to stabilize training flows on trajectory datasets and a new
data augmentation transformation that improves the performance and
generalization of our model. Our method achieves state-of-the-art performance
on three popular prediction datasets, with a significant gap to most competing
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Scholler_C/0/1/0/all/0/1"&gt;Christoph Sch&amp;#xf6;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1"&gt;Alois Knoll&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed Representations of Atoms and Materials for Machine Learning. (arXiv:2107.14664v1 [cond-mat.mtrl-sci])]]></title>
        <id>http://arxiv.org/abs/2107.14664</id>
        <link href="http://arxiv.org/abs/2107.14664"/>
        <updated>2021-08-02T01:58:24.902Z</updated>
        <summary type="html"><![CDATA[The use of machine learning is becoming increasingly common in computational
materials science. To build effective models of the chemistry of materials,
useful machine-based representations of atoms and their compounds are required.
We derive distributed representations of compounds from their chemical formulas
only, via pooling operations of distributed representations of atoms. These
compound representations are evaluated on ten different tasks, such as the
prediction of formation energy and band gap, and are found to be competitive
with existing benchmarks that make use of structure, and even superior in cases
where only composition is available. Finally, we introduce a new approach for
learning distributed representations of atoms, named SkipAtom, which makes use
of the growing information in materials structure databases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Antunes_L/0/1/0/all/0/1"&gt;Luis M. Antunes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Grau_Crespo_R/0/1/0/all/0/1"&gt;Ricardo Grau-Crespo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Butler_K/0/1/0/all/0/1"&gt;Keith T. Butler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Defending against Backdoors in Federated Learning with Robust Learning Rate. (arXiv:2007.03767v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.03767</id>
        <link href="http://arxiv.org/abs/2007.03767"/>
        <updated>2021-08-02T01:58:24.896Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) allows a set of agents to collaboratively train a
model without sharing their potentially sensitive data. This makes FL suitable
for privacy-preserving applications. At the same time, FL is susceptible to
adversarial attacks due to decentralized and unvetted data. One important line
of attacks against FL is the backdoor attacks. In a backdoor attack, an
adversary tries to embed a backdoor functionality to the model during training
that can later be activated to cause a desired misclassification. To prevent
backdoor attacks, we propose a lightweight defense that requires minimal change
to the FL protocol. At a high level, our defense is based on carefully
adjusting the aggregation server's learning rate, per dimension and per round,
based on the sign information of agents' updates. We first conjecture the
necessary steps to carry a successful backdoor attack in FL setting, and then,
explicitly formulate the defense based on our conjecture. Through experiments,
we provide empirical evidence that supports our conjecture, and we test our
defense against backdoor attacks under different settings. We observe that
either backdoor is completely eliminated, or its accuracy is significantly
reduced. Overall, our experiments suggest that our defense significantly
outperforms some of the recently proposed defenses in the literature. We
achieve this by having minimal influence over the accuracy of the trained
models. In addition, we also provide convergence rate analysis for our proposed
scheme.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozdayi_M/0/1/0/all/0/1"&gt;Mustafa Safa Ozdayi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kantarcioglu_M/0/1/0/all/0/1"&gt;Murat Kantarcioglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gel_Y/0/1/0/all/0/1"&gt;Yulia R. Gel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CNN depth analysis with different channel inputs for Acoustic Scene Classification. (arXiv:1906.04591v3 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.04591</id>
        <link href="http://arxiv.org/abs/1906.04591"/>
        <updated>2021-08-02T01:58:24.890Z</updated>
        <summary type="html"><![CDATA[Acoustic scene classification (ASC) has been approached in the last years
using deep learning techniques such as convolutional neural networks or
recurrent neural networks. Many state-of-the-art solutions are based on image
classification frameworks and, as such, a 2D representation of the audio signal
is considered for training these networks. Finding the most suitable audio
representation is still a research area of interest. In this paper, different
log-Mel representations and combinations are analyzed. Experiments show that
the best results are obtained using the harmonic and percussive components plus
the difference between left and right stereo channels, (L-R). On the other
hand, it is a common strategy to ensemble different models in order to increase
the final accuracy. Even though averaging different model predictions is a
common choice, an exhaustive analysis of different ensemble techniques has not
been presented in ASC problems. In this paper, geometric and arithmetic mean
plus the Ordered Weighted Averaging (OWA) operator are studied as aggregation
operators for the output of the different models of the ensemble. Finally, the
work carried out in this paper is highly oriented towards real-time
implementations. In this context, as the number of applications for audio
classification on edge devices is increasing exponentially, we also analyze
different network depths and efficient solutions for aggregating ensemble
predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Castanos_S/0/1/0/all/0/1"&gt;Sergi Perez-Castanos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naranjo_Alcazar_J/0/1/0/all/0/1"&gt;Javier Naranjo-Alcazar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuccarello_P/0/1/0/all/0/1"&gt;Pedro Zuccarello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cobos_M/0/1/0/all/0/1"&gt;Maximo Cobos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferri_F/0/1/0/all/0/1"&gt;Frances J. Ferri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Variational Learning for Grounded Language Acquisition. (arXiv:2107.14593v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.14593</id>
        <link href="http://arxiv.org/abs/2107.14593"/>
        <updated>2021-08-02T01:58:24.883Z</updated>
        <summary type="html"><![CDATA[We propose a learning system in which language is grounded in visual percepts
without specific pre-defined categories of terms. We present a unified
generative method to acquire a shared semantic/visual embedding that enables
the learning of language about a wide range of real-world objects. We evaluate
the efficacy of this learning by predicting the semantics of objects and
comparing the performance with neural and non-neural inputs. We show that this
generative approach exhibits promising results in language grounding without
pre-specifying visual categories under low resource settings. Our experiments
demonstrate that this approach is generalizable to multilingual, highly varied
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pillai_N/0/1/0/all/0/1"&gt;Nisha Pillai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matuszek_C/0/1/0/all/0/1"&gt;Cynthia Matuszek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferraro_F/0/1/0/all/0/1"&gt;Francis Ferraro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Who's Afraid of Thomas Bayes?. (arXiv:2107.14601v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14601</id>
        <link href="http://arxiv.org/abs/2107.14601"/>
        <updated>2021-08-02T01:58:24.865Z</updated>
        <summary type="html"><![CDATA[In many cases, neural networks perform well on test data, but tend to
overestimate their confidence on out-of-distribution data. This has led to
adoption of Bayesian neural networks, which better capture uncertainty and
therefore more accurately reflect the model's confidence. For machine learning
security researchers, this raises the natural question of how making a model
Bayesian affects the security of the model. In this work, we explore the
interplay between Bayesianism and two measures of security: model privacy and
adversarial robustness. We demonstrate that Bayesian neural networks are more
vulnerable to membership inference attacks in general, but are at least as
robust as their non-Bayesian counterparts to adversarial examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Galinkin_E/0/1/0/all/0/1"&gt;Erick Galinkin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finding Stable Groups of Cross-Correlated Features in Two Data Sets With Common Samples. (arXiv:2009.05079v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.05079</id>
        <link href="http://arxiv.org/abs/2009.05079"/>
        <updated>2021-08-02T01:58:24.860Z</updated>
        <summary type="html"><![CDATA[Data sets in which measurements of different types are obtained from a common
set of samples appear in many scientific applications. In the analysis of such
data, an important problem is to identify groups of features from different
data types that are strongly associated. Given two data types, a bimodule is a
pair $(A,B)$ of feature sets from the two types such that the aggregate
cross-correlation between the features in $A$ and those in $B$ is large. A
bimodule $(A,B)$ is stable if $A$ coincides with the set of features that have
significant aggregate correlation with the features in $B$, and vice-versa. We
develop an, iterative, testing-based procedure called BSP to identify stable
bimodules. BSP relies on approximate p-values derived from the permutation
moments of sums of squared sample correlations between a single feature of one
type and a group of features of the second type. We carry out a thorough
simulation study to assess the performance of BSP, and present an extended
application to the problem of expression quantitative trait loci (eQTL)
analysis using recent data from the GTEx project. In addition, we apply BSP to
climatology data to identify regions in North America where annual temperature
variation affects precipitation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Dewaskar_M/0/1/0/all/0/1"&gt;Miheer Dewaskar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Palowitch_J/0/1/0/all/0/1"&gt;John Palowitch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+He_M/0/1/0/all/0/1"&gt;Mark He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Love_M/0/1/0/all/0/1"&gt;Michael I. Love&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nobel_A/0/1/0/all/0/1"&gt;Andrew B. Nobel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Perceptually Guided End-to-End Text-to-Speech With MOS Prediction. (arXiv:2011.01174v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.01174</id>
        <link href="http://arxiv.org/abs/2011.01174"/>
        <updated>2021-08-02T01:58:24.854Z</updated>
        <summary type="html"><![CDATA[Although recent end-to-end text-to-speech (TTS) systems have achieved
high-quality synthesized speech, there are still several factors that degrade
the quality of synthesized speech, including lack of training data or
information loss during knowledge distillation. To address the problem, we
propose a novel way to train a TTS model under the supervision of perceptual
loss, which measures the distance between the maximum speech quality score and
the predicted one. We first pre-train a mean opinion score (MOS) prediction
model and then train a TTS model in the direction of maximizing the MOS of
synthesized speech predicted by the pre-trained MOS prediction model. Through
this method, we can improve the quality of synthesized speech universally
(i.e., regardless of the network architecture or the cause of the speech
quality degradation) and efficiently (i.e., without increasing the inference
time or the model complexity). The evaluation results for MOS and phoneme error
rate demonstrate that our proposed approach improves previous models in terms
of both naturalness and intelligibility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yeunju Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jung_Y/0/1/0/all/0/1"&gt;Youngmoon Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Suh_Y/0/1/0/all/0/1"&gt;Youngjoo Suh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hoirin Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tiny Machine Learning for Concept Drift. (arXiv:2107.14759v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14759</id>
        <link href="http://arxiv.org/abs/2107.14759"/>
        <updated>2021-08-02T01:58:24.848Z</updated>
        <summary type="html"><![CDATA[Tiny Machine Learning (TML) is a new research area whose goal is to design
machine and deep learning techniques able to operate in Embedded Systems and
IoT units, hence satisfying the severe technological constraints on memory,
computation, and energy characterizing these pervasive devices. Interestingly,
the related literature mainly focused on reducing the computational and memory
demand of the inference phase of machine and deep learning models. At the same
time, the training is typically assumed to be carried out in Cloud or edge
computing systems (due to the larger memory and computational requirements).
This assumption results in TML solutions that might become obsolete when the
process generating the data is affected by concept drift (e.g., due to
periodicity or seasonality effect, faults or malfunctioning affecting sensors
or actuators, or changes in the users' behavior), a common situation in
real-world application scenarios. For the first time in the literature, this
paper introduces a Tiny Machine Learning for Concept Drift (TML-CD) solution
based on deep learning feature extractors and a k-nearest neighbors classifier
integrating a hybrid adaptation module able to deal with concept drift
affecting the data-generating process. This adaptation module continuously
updates (in a passive way) the knowledge base of TML-CD and, at the same time,
employs a Change Detection Test to inspect for changes (in an active way) to
quickly adapt to concept drift by removing the obsolete knowledge. Experimental
results on both image and audio benchmarks show the effectiveness of the
proposed solution, whilst the porting of TML-CD on three off-the-shelf
micro-controller units shows the feasibility of what is proposed in real-world
pervasive systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Disabato_S/0/1/0/all/0/1"&gt;Simone Disabato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roveri_M/0/1/0/all/0/1"&gt;Manuel Roveri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Hierarchical Structures for Few-Shot Musical Instrument Recognition. (arXiv:2107.07029v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.07029</id>
        <link href="http://arxiv.org/abs/2107.07029"/>
        <updated>2021-08-02T01:58:24.836Z</updated>
        <summary type="html"><![CDATA[Deep learning work on musical instrument recognition has generally focused on
instrument classes for which we have abundant data. In this work, we exploit
hierarchical relationships between instruments in a few-shot learning setup to
enable classification of a wider set of musical instruments, given a few
examples at inference. We apply a hierarchical loss function to the training of
prototypical networks, combined with a method to aggregate prototypes
hierarchically, mirroring the structure of a predefined musical instrument
hierarchy. These extensions require no changes to the network architecture and
new levels can be easily added or removed. Compared to a non-hierarchical
few-shot baseline, our method leads to a significant increase in classification
accuracy and significant decrease mistake severity on instrument classes unseen
in training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_H/0/1/0/all/0/1"&gt;Hugo Flores Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aguilar_A/0/1/0/all/0/1"&gt;Aldo Aguilar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manilow_E/0/1/0/all/0/1"&gt;Ethan Manilow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pardo_B/0/1/0/all/0/1"&gt;Bryan Pardo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Perceiver IO: A General Architecture for Structured Inputs & Outputs. (arXiv:2107.14795v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14795</id>
        <link href="http://arxiv.org/abs/2107.14795"/>
        <updated>2021-08-02T01:58:24.821Z</updated>
        <summary type="html"><![CDATA[The recently-proposed Perceiver model obtains good results on several domains
(images, audio, multimodal, point clouds) while scaling linearly in compute and
memory with the input size. While the Perceiver supports many kinds of inputs,
it can only produce very simple outputs such as class scores. Perceiver IO
overcomes this limitation without sacrificing the original's appealing
properties by learning to flexibly query the model's latent space to produce
outputs of arbitrary size and semantics. Perceiver IO still decouples model
depth from data size and still scales linearly with data size, but now with
respect to both input and output sizes. The full Perceiver IO model achieves
strong results on tasks with highly structured output spaces, such as natural
language and visual understanding, StarCraft II, and multi-task and multi-modal
domains. As highlights, Perceiver IO matches a Transformer-based BERT baseline
on the GLUE language benchmark without the need for input tokenization and
achieves state-of-the-art performance on Sintel optical flow estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1"&gt;Andrew Jaegle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borgeaud_S/0/1/0/all/0/1"&gt;Sebastian Borgeaud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alayrac_J/0/1/0/all/0/1"&gt;Jean-Baptiste Alayrac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doersch_C/0/1/0/all/0/1"&gt;Carl Doersch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ionescu_C/0/1/0/all/0/1"&gt;Catalin Ionescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1"&gt;David Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koppula_S/0/1/0/all/0/1"&gt;Skanda Koppula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brock_A/0/1/0/all/0/1"&gt;Andrew Brock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shelhamer_E/0/1/0/all/0/1"&gt;Evan Shelhamer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henaff_O/0/1/0/all/0/1"&gt;Olivier H&amp;#xe9;naff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Botvinick_M/0/1/0/all/0/1"&gt;Matthew M. Botvinick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1"&gt;Andrew Zisserman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1"&gt;Oriol Vinyals&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Carreira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Efficacy of Small Self-Supervised Contrastive Models without Distillation Signals. (arXiv:2107.14762v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14762</id>
        <link href="http://arxiv.org/abs/2107.14762"/>
        <updated>2021-08-02T01:58:24.805Z</updated>
        <summary type="html"><![CDATA[It is a consensus that small models perform quite poorly under the paradigm
of self-supervised contrastive learning. Existing methods usually adopt a large
off-the-shelf model to transfer knowledge to the small one via knowledge
distillation. Despite their effectiveness, distillation-based methods may not
be suitable for some resource-restricted scenarios due to the huge
computational expenses of deploying a large model. In this paper, we study the
issue of training self-supervised small models without distillation signals. We
first evaluate the representation spaces of the small models and make two
non-negligible observations: (i) small models can complete the pretext task
without overfitting despite its limited capacity; (ii) small models universally
suffer the problem of over-clustering. Then we verify multiple assumptions that
are considered to alleviate the over-clustering phenomenon. Finally, we combine
the validated techniques and improve the baseline of five small architectures
with considerable margins, which indicates that training small self-supervised
contrastive models is feasible even without distillation signals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1"&gt;Haizhou Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Youcai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1"&gt;Siliang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1"&gt;Wenjie Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yaqian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yandong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1"&gt;Yueting Zhuang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Generative Learning via Schr\"{o}dinger Bridge. (arXiv:2106.10410v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10410</id>
        <link href="http://arxiv.org/abs/2106.10410"/>
        <updated>2021-08-02T01:58:24.799Z</updated>
        <summary type="html"><![CDATA[We propose to learn a generative model via entropy interpolation with a
Schr\"{o}dinger Bridge. The generative learning task can be formulated as
interpolating between a reference distribution and a target distribution based
on the Kullback-Leibler divergence. At the population level, this entropy
interpolation is characterized via an SDE on $[0,1]$ with a time-varying drift
term. At the sample level, we derive our Schr\"{o}dinger Bridge algorithm by
plugging the drift term estimated by a deep score estimator and a deep density
ratio estimator into the Euler-Maruyama method. Under some mild smoothness
assumptions of the target distribution, we prove the consistency of both the
score estimator and the density ratio estimator, and then establish the
consistency of the proposed Schr\"{o}dinger Bridge approach. Our theoretical
results guarantee that the distribution learned by our approach converges to
the target distribution. Experimental results on multimodal synthetic data and
benchmark data support our theoretical findings and indicate that the
generative model via Schr\"{o}dinger Bridge is comparable with state-of-the-art
GANs, suggesting a new formulation of generative learning. We demonstrate its
usefulness in image interpolation and image inpainting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Gefei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1"&gt;Yuling Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1"&gt;Qian Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Can Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Connections between Numerical Algorithms for PDEs and Neural Networks. (arXiv:2107.14742v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2107.14742</id>
        <link href="http://arxiv.org/abs/2107.14742"/>
        <updated>2021-08-02T01:58:24.793Z</updated>
        <summary type="html"><![CDATA[We investigate numerous structural connections between numerical algorithms
for partial differential equations (PDEs) and neural architectures. Our goal is
to transfer the rich set of mathematical foundations from the world of PDEs to
neural networks. Besides structural insights we provide concrete examples and
experimental evaluations of the resulting architectures. Using the example of
generalised nonlinear diffusion in 1D, we consider explicit schemes,
acceleration strategies thereof, implicit schemes, and multigrid approaches. We
connect these concepts to residual networks, recurrent neural networks, and
U-net architectures. Our findings inspire a symmetric residual network design
with provable stability guarantees and justify the effectiveness of skip
connections in neural networks from a numerical perspective. Moreover, we
present U-net architectures that implement multigrid techniques for learning
efficient solutions of partial differential equation models, and motivate
uncommon design choices such as trainable nonmonotone activation functions.
Experimental evaluations show that the proposed architectures save half of the
trainable parameters and can thus outperform standard ones with the same model
complexity. Our considerations serve as a basis for explaining the success of
popular neural architectures and provide a blueprint for developing new
mathematically well-founded neural building blocks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Alt_T/0/1/0/all/0/1"&gt;Tobias Alt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Schrader_K/0/1/0/all/0/1"&gt;Karl Schrader&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Augustin_M/0/1/0/all/0/1"&gt;Matthias Augustin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Peter_P/0/1/0/all/0/1"&gt;Pascal Peter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Weickert_J/0/1/0/all/0/1"&gt;Joachim Weickert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DCT2net: an interpretable shallow CNN for image denoising. (arXiv:2107.14803v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.14803</id>
        <link href="http://arxiv.org/abs/2107.14803"/>
        <updated>2021-08-02T01:58:24.786Z</updated>
        <summary type="html"><![CDATA[This work tackles the issue of noise removal from images, focusing on the
well-known DCT image denoising algorithm. The latter, stemming from signal
processing, has been well studied over the years. Though very simple, it is
still used in crucial parts of state-of-the-art "traditional" denoising
algorithms such as BM3D. Since a few years however, deep convolutional neural
networks (CNN) have outperformed their traditional counterparts, making signal
processing methods less attractive. In this paper, we demonstrate that a DCT
denoiser can be seen as a shallow CNN and thereby its original linear transform
can be tuned through gradient descent in a supervised manner, improving
considerably its performance. This gives birth to a fully interpretable CNN
called DCT2net. To deal with remaining artifacts induced by DCT2net, an
original hybrid solution between DCT and DCT2net is proposed combining the best
that these two methods can offer; DCT2net is selected to process non-stationary
image patches while DCT is optimal for piecewise smooth patches. Experiments on
artificially noisy images demonstrate that two-layer DCT2net provides
comparable results to BM3D and is as fast as DnCNN algorithm composed of more
than a dozen of layers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Herbreteau_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Herbreteau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kervrann_C/0/1/0/all/0/1"&gt;Charles Kervrann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text Classification and Clustering with Annealing Soft Nearest Neighbor Loss. (arXiv:2107.14597v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14597</id>
        <link href="http://arxiv.org/abs/2107.14597"/>
        <updated>2021-08-02T01:58:24.769Z</updated>
        <summary type="html"><![CDATA[We define disentanglement as how far class-different data points from each
other are, relative to the distances among class-similar data points. When
maximizing disentanglement during representation learning, we obtain a
transformed feature representation where the class memberships of the data
points are preserved. If the class memberships of the data points are
preserved, we would have a feature representation space in which a nearest
neighbour classifier or a clustering algorithm would perform well. We take
advantage of this method to learn better natural language representation, and
employ it on text classification and text clustering tasks. Through
disentanglement, we obtain text representations with better-defined clusters
and improve text classification performance. Our approach had a test
classification accuracy of as high as 90.11% and test clustering accuracy of
88% on the AG News dataset, outperforming our baseline models -- without any
other training tricks or regularization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agarap_A/0/1/0/all/0/1"&gt;Abien Fred Agarap&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CAM-GAN: Continual Adaptation Modules for Generative Adversarial Networks. (arXiv:2103.04032v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04032</id>
        <link href="http://arxiv.org/abs/2103.04032"/>
        <updated>2021-08-02T01:58:24.710Z</updated>
        <summary type="html"><![CDATA[We present a continual learning approach for generative adversarial networks
(GANs), by designing and leveraging parameter-efficient feature map
transformations. Our approach is based on learning a set of global and
task-specific parameters. The global parameters are fixed across tasks whereas
the task-specific parameters act as local adapters for each task, and help in
efficiently obtaining task-specific feature maps. Moreover, we propose an
element-wise addition of residual bias in the transformed feature space, which
further helps stabilize GAN training in such settings. Our approach also
leverages task similarity information based on the Fisher information matrix.
Leveraging this knowledge from previous tasks significantly improves the model
performance. In addition, the similarity measure also helps reduce the
parameter growth in continual adaptation and helps to learn a compact model. In
contrast to the recent approaches for continually-learned GANs, the proposed
approach provides a memory-efficient way to perform effective continual data
generation. Through extensive experiments on challenging and diverse datasets,
we show that the feature-map-transformation approach outperforms
state-of-the-art methods for continually-learned GANs, with substantially fewer
parameters. The proposed method generates high-quality samples that can also
improve the generative-replay-based continual learning for discriminative
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Varshney_S/0/1/0/all/0/1"&gt;Sakshi Varshney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_V/0/1/0/all/0/1"&gt;Vinay Kumar Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+K_S/0/1/0/all/0/1"&gt;Srijith P K&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1"&gt;Lawrence Carin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rai_P/0/1/0/all/0/1"&gt;Piyush Rai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unveiling the potential of Graph Neural Networks for robust Intrusion Detection. (arXiv:2107.14756v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.14756</id>
        <link href="http://arxiv.org/abs/2107.14756"/>
        <updated>2021-08-02T01:58:24.703Z</updated>
        <summary type="html"><![CDATA[The last few years have seen an increasing wave of attacks with serious
economic and privacy damages, which evinces the need for accurate Network
Intrusion Detection Systems (NIDS). Recent works propose the use of Machine
Learning (ML) techniques for building such systems (e.g., decision trees,
neural networks). However, existing ML-based NIDS are barely robust to common
adversarial attacks, which limits their applicability to real networks. A
fundamental problem of these solutions is that they treat and classify flows
independently. In contrast, in this paper we argue the importance of focusing
on the structural patterns of attacks, by capturing not only the individual
flow features, but also the relations between different flows (e.g., the
source/destination hosts they share). To this end, we use a graph
representation that keeps flow records and their relationships, and propose a
novel Graph Neural Network (GNN) model tailored to process and learn from such
graph-structured information. In our evaluation, we first show that the
proposed GNN model achieves state-of-the-art results in the well-known
CIC-IDS2017 dataset. Moreover, we assess the robustness of our solution under
two common adversarial attacks, that intentionally modify the packet size and
inter-arrival times to avoid detection. The results show that our model is able
to maintain the same level of accuracy as in previous experiments, while
state-of-the-art ML techniques degrade up to 50% their accuracy (F1-score)
under these attacks. This unprecedented level of robustness is mainly induced
by the capability of our GNN model to learn flow patterns of attacks structured
as graphs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pujol_Perich_D/0/1/0/all/0/1"&gt;David Pujol-Perich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suarez_Varela_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Su&amp;#xe1;rez-Varela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cabellos_Aparicio_A/0/1/0/all/0/1"&gt;Albert Cabellos-Aparicio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barlet_Ros_P/0/1/0/all/0/1"&gt;Pere Barlet-Ros&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Strategically Efficient Exploration in Competitive Multi-agent Reinforcement Learning. (arXiv:2107.14698v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14698</id>
        <link href="http://arxiv.org/abs/2107.14698"/>
        <updated>2021-08-02T01:58:24.697Z</updated>
        <summary type="html"><![CDATA[High sample complexity remains a barrier to the application of reinforcement
learning (RL), particularly in multi-agent systems. A large body of work has
demonstrated that exploration mechanisms based on the principle of optimism
under uncertainty can significantly improve the sample efficiency of RL in
single agent tasks. This work seeks to understand the role of optimistic
exploration in non-cooperative multi-agent settings. We will show that, in
zero-sum games, optimistic exploration can cause the learner to waste time
sampling parts of the state space that are irrelevant to strategic play, as
they can only be reached through cooperation between both players. To address
this issue, we introduce a formal notion of strategically efficient exploration
in Markov games, and use this to develop two strategically efficient learning
algorithms for finite Markov games. We demonstrate that these methods can be
significantly more sample efficient than their optimistic counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Loftin_R/0/1/0/all/0/1"&gt;Robert Loftin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1"&gt;Aadirupa Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Devlin_S/0/1/0/all/0/1"&gt;Sam Devlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1"&gt;Katja Hofmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can non-specialists provide high quality gold standard labels in challenging modalities?. (arXiv:2107.14682v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14682</id>
        <link href="http://arxiv.org/abs/2107.14682"/>
        <updated>2021-08-02T01:58:24.691Z</updated>
        <summary type="html"><![CDATA[Probably yes. -- Supervised Deep Learning dominates performance scores for
many computer vision tasks and defines the state-of-the-art. However, medical
image analysis lags behind natural image applications. One of the many reasons
is the lack of well annotated medical image data available to researchers. One
of the first things researchers are told is that we require significant
expertise to reliably and accurately interpret and label such data. We see
significant inter- and intra-observer variability between expert annotations of
medical images. Still, it is a widely held assumption that novice annotators
are unable to provide useful annotations for use by clinical Deep Learning
models. In this work we challenge this assumption and examine the implications
of using a minimally trained novice labelling workforce to acquire annotations
for a complex medical image dataset. We study the time and cost implications of
using novice annotators, the raw performance of novice annotators compared to
gold-standard expert annotators, and the downstream effects on a trained Deep
Learning segmentation model's performance for detecting a specific congenital
heart disease (hypoplastic left heart syndrome) in fetal ultrasound imaging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Budd_S/0/1/0/all/0/1"&gt;Samuel Budd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Day_T/0/1/0/all/0/1"&gt;Thomas Day&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simpson_J/0/1/0/all/0/1"&gt;John Simpson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lloyd_K/0/1/0/all/0/1"&gt;Karen Lloyd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matthew_J/0/1/0/all/0/1"&gt;Jacqueline Matthew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Skelton_E/0/1/0/all/0/1"&gt;Emily Skelton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Razavi_R/0/1/0/all/0/1"&gt;Reza Razavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1"&gt;Bernhard Kainz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When Deep Learners Change Their Mind: Learning Dynamics for Active Learning. (arXiv:2107.14707v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14707</id>
        <link href="http://arxiv.org/abs/2107.14707"/>
        <updated>2021-08-02T01:58:24.676Z</updated>
        <summary type="html"><![CDATA[Active learning aims to select samples to be annotated that yield the largest
performance improvement for the learning algorithm. Many methods approach this
problem by measuring the informativeness of samples and do this based on the
certainty of the network predictions for samples. However, it is well-known
that neural networks are overly confident about their prediction and are
therefore an untrustworthy source to assess sample informativeness. In this
paper, we propose a new informativeness-based active learning method. Our
measure is derived from the learning dynamics of a neural network. More
precisely we track the label assignment of the unlabeled data pool during the
training of the algorithm. We capture the learning dynamics with a metric
called label-dispersion, which is low when the network consistently assigns the
same label to the sample during the training of the network and high when the
assigned label changes frequently. We show that label-dispersion is a promising
predictor of the uncertainty of the network, and show on two benchmark datasets
that an active learning algorithm based on label-dispersion obtains excellent
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bengar_J/0/1/0/all/0/1"&gt;Javad Zolfaghari Bengar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raducanu_B/0/1/0/all/0/1"&gt;Bogdan Raducanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1"&gt;Joost van de Weijer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DQ-SGD: Dynamic Quantization in SGD for Communication-Efficient Distributed Learning. (arXiv:2107.14575v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14575</id>
        <link href="http://arxiv.org/abs/2107.14575"/>
        <updated>2021-08-02T01:58:24.670Z</updated>
        <summary type="html"><![CDATA[Gradient quantization is an emerging technique in reducing communication
costs in distributed learning. Existing gradient quantization algorithms often
rely on engineering heuristics or empirical observations, lacking a systematic
approach to dynamically quantize gradients. This paper addresses this issue by
proposing a novel dynamically quantized SGD (DQ-SGD) framework, enabling us to
dynamically adjust the quantization scheme for each gradient descent step by
exploring the trade-off between communication cost and convergence error. We
derive an upper bound, tight in some cases, of the convergence error for a
restricted family of quantization schemes and loss functions. We design our
DQ-SGD algorithm via minimizing the communication cost under the convergence
error constraints. Finally, through extensive experiments on large-scale
natural language processing and computer vision tasks on AG-News, CIFAR-10, and
CIFAR-100 datasets, we demonstrate that our quantization scheme achieves better
tradeoffs between the communication cost and learning performance than other
state-of-the-art gradient quantization methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_G/0/1/0/all/0/1"&gt;Guangfeng Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shao-Lun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1"&gt;Tian Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"&gt;Linqi Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DadaGP: A Dataset of Tokenized GuitarPro Songs for Sequence Models. (arXiv:2107.14653v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.14653</id>
        <link href="http://arxiv.org/abs/2107.14653"/>
        <updated>2021-08-02T01:58:24.663Z</updated>
        <summary type="html"><![CDATA[Originating in the Renaissance and burgeoning in the digital era, tablatures
are a commonly used music notation system which provides explicit
representations of instrument fingerings rather than pitches. GuitarPro has
established itself as a widely used tablature format and software enabling
musicians to edit and share songs for musical practice, learning, and
composition. In this work, we present DadaGP, a new symbolic music dataset
comprising 26,181 song scores in the GuitarPro format covering 739 musical
genres, along with an accompanying tokenized format well-suited for generative
sequence models such as the Transformer. The tokenized format is inspired by
event-based MIDI encodings, often used in symbolic music generation models. The
dataset is released with an encoder/decoder which converts GuitarPro files to
tokens and back. We present results of a use case in which DadaGP is used to
train a Transformer-based model to generate new songs in GuitarPro format. We
discuss other relevant use cases for the dataset (guitar-bass transcription,
music style transfer and artist/genre classification) as well as ethical
implications. DadaGP opens up the possibility to train GuitarPro score
generators, fine-tune models on custom data, create new styles of music,
AI-powered songwriting apps, and human-AI improvisation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sarmento_P/0/1/0/all/0/1"&gt;Pedro Sarmento&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Adarsh Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carr_C/0/1/0/all/0/1"&gt;CJ Carr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zukowski_Z/0/1/0/all/0/1"&gt;Zack Zukowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barthet_M/0/1/0/all/0/1"&gt;Mathieu Barthet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi-Hsuan Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sensing and Mapping for Better Roads: Initial Plan for Using Federated Learning and Implementing a Digital Twin to Identify the Road Conditions in a Developing Country -- Sri Lanka. (arXiv:2107.14551v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2107.14551</id>
        <link href="http://arxiv.org/abs/2107.14551"/>
        <updated>2021-08-02T01:58:24.654Z</updated>
        <summary type="html"><![CDATA[We propose how a developing country like Sri Lanka can benefit from
privacy-enabled machine learning techniques such as Federated Learning to
detect road conditions using crowd-sourced data collection and proposed the
idea of implementing a Digital Twin for the national road system in Sri Lanka.
Developing countries such as Sri Lanka are far behind in implementing smart
road systems and smart cities compared to the developed countries. The proposed
work discussed in this paper matches the UN Sustainable Development Goal (SDG)
9: "Build Resilient Infrastructure, Promote Inclusive and Sustainable
Industrialization and Foster Innovation". Our proposed work discusses how the
government and private sector vehicles that conduct routine trips to collect
crowd-sourced data using smartphone devices to identify the road conditions and
detect where the potholes, surface unevenness (roughness), and other major
distresses are located on the roads. We explore Mobile Edge Computing (MEC)
techniques that can bring machine learning intelligence closer to the edge
devices where produced data is stored and show how the applications of
Federated Learning can be made to detect and improve road conditions. During
the second phase of this study, we plan to implement a Digital Twin for the
road system in Sri Lanka. We intend to use data provided by both Dedicated and
Non-Dedicated systems in the proposed Digital Twin for the road system. As of
writing this paper, and best to our knowledge, there is no Digital Twin system
implemented for roads and other infrastructure systems in Sri Lanka. The
proposed Digital Twin will be one of the first implementations of such systems
in Sri Lanka. Lessons learned from this pilot project will benefit other
developing countries who wish to follow the same path and make data-driven
decisions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Munasinghe_T/0/1/0/all/0/1"&gt;Thilanka Munasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pasindu_H/0/1/0/all/0/1"&gt;HR Pasindu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Surrogate Modelling for Injection Molding Processes using Machine Learning. (arXiv:2107.14574v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14574</id>
        <link href="http://arxiv.org/abs/2107.14574"/>
        <updated>2021-08-02T01:58:24.647Z</updated>
        <summary type="html"><![CDATA[Injection molding is one of the most popular manufacturing methods for the
modeling of complex plastic objects. Faster numerical simulation of the
technological process would allow for faster and cheaper design cycles of new
products. In this work, we propose a baseline for a data processing pipeline
that includes the extraction of data from Moldflow simulation projects and the
prediction of the fill time and deflection distributions over 3-dimensional
surfaces using machine learning models. We propose algorithms for engineering
of features, including information of injector gates parameters that will
mostly affect the time for plastic to reach the particular point of the form
for fill time prediction, and geometrical features for deflection prediction.
We propose and evaluate baseline machine learning models for fill time and
deflection distribution prediction and provide baseline values of MSE and RMSE
metrics. Finally, we measure the execution time of our solution and show that
it significantly exceeds the time of simulation with Moldflow software:
approximately 17 times and 14 times faster for mean and median total times
respectively, comparing the times of all analysis stages for deflection
prediction. Our solution has been implemented in a prototype web application
that was approved by the management board of Fiat Chrysler Automobiles and
Illogic SRL. As one of the promising applications of this surrogate modelling
approach, we envision the use of trained models as a fast objective function in
the task of optimization of technological parameters of the injection molding
process (meaning optimal placement of gates), which could significantly aid
engineers in this task, or even automate it.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Uglov_A/0/1/0/all/0/1"&gt;Arsenii Uglov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikolaev_S/0/1/0/all/0/1"&gt;Sergei Nikolaev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belov_S/0/1/0/all/0/1"&gt;Sergei Belov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Padalitsa_D/0/1/0/all/0/1"&gt;Daniil Padalitsa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greenkina_T/0/1/0/all/0/1"&gt;Tatiana Greenkina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biagio_M/0/1/0/all/0/1"&gt;Marco San Biagio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cacciatori_F/0/1/0/all/0/1"&gt;Fabio Cacciatori&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can You Hear It? Backdoor Attacks via Ultrasonic Triggers. (arXiv:2107.14569v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.14569</id>
        <link href="http://arxiv.org/abs/2107.14569"/>
        <updated>2021-08-02T01:58:24.640Z</updated>
        <summary type="html"><![CDATA[Deep neural networks represent a powerful option for many real-world
applications due to their ability to model even complex data relations.
However, such neural networks can also be prohibitively expensive to train,
making it common to either outsource the training process to third parties or
use pretrained neural networks. Unfortunately, such practices make neural
networks vulnerable to various attacks, where one attack is the backdoor
attack. In such an attack, the third party training the model may maliciously
inject hidden behaviors into the model. Still, if a particular input (called
trigger) is fed into a neural network, the network will respond with a wrong
result.

In this work, we explore the option of backdoor attacks to automatic speech
recognition systems where we inject inaudible triggers. By doing so, we make
the backdoor attack challenging to detect for legitimate users, and thus,
potentially more dangerous. We conduct experiments on two versions of datasets
and three neural networks and explore the performance of our attack concerning
the duration, position, and type of the trigger. Our results indicate that less
than 1% of poisoned data is sufficient to deploy a backdoor attack and reach a
100% attack success rate. What is more, while the trigger is inaudible, making
it without limitations with respect to the duration of the signal, we observed
that even short, non-continuous triggers result in highly successful attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koffas_S/0/1/0/all/0/1"&gt;Stefanos Koffas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1"&gt;Mauro Conti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Picek_S/0/1/0/all/0/1"&gt;Stjepan Picek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient DP-SGD Mechanism for Large Scale NLP Models. (arXiv:2107.14586v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.14586</id>
        <link href="http://arxiv.org/abs/2107.14586"/>
        <updated>2021-08-02T01:58:24.623Z</updated>
        <summary type="html"><![CDATA[Recent advances in deep learning have drastically improved performance on
many Natural Language Understanding (NLU) tasks. However, the data used to
train NLU models may contain private information such as addresses or phone
numbers, particularly when drawn from human subjects. It is desirable that
underlying models do not expose private information contained in the training
data. Differentially Private Stochastic Gradient Descent (DP-SGD) has been
proposed as a mechanism to build privacy-preserving models. However, DP-SGD can
be prohibitively slow to train. In this work, we propose a more efficient
DP-SGD for training using a GPU infrastructure and apply it to fine-tuning
models based on LSTM and transformer architectures. We report faster training
times, alongside accuracy, theoretical privacy guarantees and success of
Membership inference attacks for our models and observe that fine-tuning with
proposed variant of DP-SGD can yield competitive models without significant
degradation in training time and improvement in privacy protection. We also
make observations such as looser theoretical $\epsilon, \delta$ can translate
into significant practical privacy gains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dupuy_C/0/1/0/all/0/1"&gt;Christophe Dupuy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arava_R/0/1/0/all/0/1"&gt;Radhika Arava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1"&gt;Rahul Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1"&gt;Anna Rumshisky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporation of Deep Neural Network & Reinforcement Learning with Domain Knowledge. (arXiv:2107.14613v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14613</id>
        <link href="http://arxiv.org/abs/2107.14613"/>
        <updated>2021-08-02T01:58:24.616Z</updated>
        <summary type="html"><![CDATA[We present a study of the manners by which Domain information has been
incorporated when building models with Neural Networks. Integrating space data
is uniquely important to the development of Knowledge understanding model, as
well as other fields that aid in understanding information by utilizing the
human-machine interface and Reinforcement Learning. On numerous such occasions,
machine-based model development may profit essentially from the human
information on the world encoded in an adequately exact structure. This paper
inspects expansive ways to affect encode such information as sensible and
mathematical limitations and portrays methods and results that came to a couple
of subcategories under all of those methodologies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karn_A/0/1/0/all/0/1"&gt;Aryan Karn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1"&gt;Ashutosh Acharya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Tree Aggregation of Layers for Neural Machine Translation. (arXiv:2107.14590v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.14590</id>
        <link href="http://arxiv.org/abs/2107.14590"/>
        <updated>2021-08-02T01:58:24.609Z</updated>
        <summary type="html"><![CDATA[Although attention-based Neural Machine Translation has achieved remarkable
progress in recent layers, it still suffers from issue of making insufficient
use of the output of each layer. In transformer, it only uses the top layer of
encoder and decoder in the subsequent process, which makes it impossible to
take advantage of the useful information in other layers. To address this
issue, we propose a residual tree aggregation of layers for Transformer(RTAL),
which helps to fuse information across layers. Specifically, we try to fuse the
information across layers by constructing a post-order binary tree. In
additional to the last node, we add the residual connection to the process of
generating child nodes. Our model is based on the Neural Machine Translation
model Transformer and we conduct our experiments on WMT14 English-to-German and
WMT17 English-to-France translation tasks. Experimental results across language
pairs show that the proposed approach outperforms the strong baseline model
significantly]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;GuoLiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yiyang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Framework for Adversarial Streaming via Differential Privacy and Difference Estimators. (arXiv:2107.14527v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2107.14527</id>
        <link href="http://arxiv.org/abs/2107.14527"/>
        <updated>2021-08-02T01:58:24.593Z</updated>
        <summary type="html"><![CDATA[Streaming algorithms are algorithms for processing large data streams, using
only a limited amount of memory. Classical streaming algorithms operate under
the assumption that the input stream is fixed in advance. Recently, there is a
growing interest in studying streaming algorithms that provide provable
guarantees even when the input stream is chosen by an adaptive adversary. Such
streaming algorithms are said to be {\em adversarially-robust}. We propose a
novel framework for adversarial streaming that hybrids two recently suggested
frameworks by Hassidim et al. (2020) and by Woodruff and Zhou (2021). These
recently suggested frameworks rely on very different ideas, each with its own
strengths and weaknesses. We combine these two frameworks (in a non-trivial
way) into a single hybrid framework that gains from both approaches to obtain
superior performances for turnstile streams.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Attias_I/0/1/0/all/0/1"&gt;Idan Attias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_E/0/1/0/all/0/1"&gt;Edith Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shechner_M/0/1/0/all/0/1"&gt;Moshe Shechner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stemmer_U/0/1/0/all/0/1"&gt;Uri Stemmer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Urdu & Hindi Poetry Generation using Neural Networks. (arXiv:2107.14587v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.14587</id>
        <link href="http://arxiv.org/abs/2107.14587"/>
        <updated>2021-08-02T01:58:24.587Z</updated>
        <summary type="html"><![CDATA[One of the major problems writers and poets face is the writer's block. It is
a condition in which an author loses the ability to produce new work or
experiences a creative slowdown. The problem is more difficult in the context
of poetry than prose, as in the latter case authors need not be very concise
while expressing their ideas, also the various aspects such as rhyme, poetic
meters are not relevant for prose. One of the most effective ways to overcome
this writing block for poets can be, to have a prompt system, which would help
their imagination and open their minds for new ideas. A prompt system can
possibly generate one liner, two liner or full ghazals. The purpose of this
work is to give an ode to the Urdu, Hindi poets, and helping them start their
next line of poetry, a couplet or a complete ghazal considering various factors
like rhymes, refrain, and meters. The result will help aspiring poets to get
new ideas and help them overcome writer's block by auto-generating pieces of
poetry using Deep Learning techniques. A concern with creative works like this,
especially in the literary context, is to ensure that the output is not
plagiarized. This work also addresses the concern and makes sure that the
resulting odes are not exact match with input data using parameters like
temperature and manual plagiarism check against input corpus. To the best of
our knowledge, although the automatic text generation problem has been studied
quite extensively in the literature, the specific problem of Urdu, Hindi poetry
generation has not been explored much. Apart from developing system to
auto-generate Urdu, Hindi poetry, another key contribution of our work is to
create a cleaned and preprocessed corpus of Urdu, Hindi poetry (derived from
authentic resources) and making it freely available for researchers in the
area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mukhtar_S/0/1/0/all/0/1"&gt;Shakeeb A. M. Mukhtar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joglekar_P/0/1/0/all/0/1"&gt;Pushkar S. Joglekar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervision for health insurance claims data: a Covid-19 use case. (arXiv:2107.14591v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.14591</id>
        <link href="http://arxiv.org/abs/2107.14591"/>
        <updated>2021-08-02T01:58:24.580Z</updated>
        <summary type="html"><![CDATA[In this work, we modify and apply self-supervision techniques to the domain
of medical health insurance claims. We model patients' healthcare claims
history analogous to free-text narratives, and introduce pre-trained `prior
knowledge', later utilized for patient outcome predictions on a challenging
task: predicting Covid-19 hospitalization, given a patient's pre-Covid-19
insurance claims history. Results suggest that pre-training on insurance claims
not only produces better prediction performance, but, more importantly,
improves the model's `clinical trustworthiness' and model
stability/reliability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Apostolova_E/0/1/0/all/0/1"&gt;Emilia Apostolova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karim_F/0/1/0/all/0/1"&gt;Fazle Karim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muscioni_G/0/1/0/all/0/1"&gt;Guido Muscioni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rana_A/0/1/0/all/0/1"&gt;Anubhav Rana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clyman_J/0/1/0/all/0/1"&gt;Jeffrey Clyman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeuralDP Differentially private neural networks by design. (arXiv:2107.14582v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14582</id>
        <link href="http://arxiv.org/abs/2107.14582"/>
        <updated>2021-08-02T01:58:24.574Z</updated>
        <summary type="html"><![CDATA[The application of differential privacy to the training of deep neural
networks holds the promise of allowing large-scale (decentralized) use of
sensitive data while providing rigorous privacy guarantees to the individual.
The predominant approach to differentially private training of neural networks
is DP-SGD, which relies on norm-based gradient clipping as a method for
bounding sensitivity, followed by the addition of appropriately calibrated
Gaussian noise. In this work we propose NeuralDP, a technique for privatising
activations of some layer within a neural network, which by the post-processing
properties of differential privacy yields a differentially private network. We
experimentally demonstrate on two datasets (MNIST and Pediatric Pneumonia
Dataset (PPD)) that our method offers substantially improved privacy-utility
trade-offs compared to DP-SGD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Knolle_M/0/1/0/all/0/1"&gt;Moritz Knolle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usynin_D/0/1/0/all/0/1"&gt;Dmitrii Usynin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1"&gt;Alexander Ziller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1"&gt;Marcus R. Makowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1"&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1"&gt;Georgios Kaissis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artist Similarity with Graph Neural Networks. (arXiv:2107.14541v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.14541</id>
        <link href="http://arxiv.org/abs/2107.14541"/>
        <updated>2021-08-02T01:58:24.568Z</updated>
        <summary type="html"><![CDATA[Artist similarity plays an important role in organizing, understanding, and
subsequently, facilitating discovery in large collections of music. In this
paper, we present a hybrid approach to computing similarity between artists
using graph neural networks trained with triplet loss. The novelty of using a
graph neural network architecture is to combine the topology of a graph of
artist connections with content features to embed artists into a vector space
that encodes similarity. To evaluate the proposed method, we compile the new
OLGA dataset, which contains artist similarities from AllMusic, together with
content features from AcousticBrainz. With 17,673 artists, this is the largest
academic artist similarity dataset that includes content-based features to
date. Moreover, we also showcase the scalability of our approach by
experimenting with a much larger proprietary dataset. Results show the
superiority of the proposed approach over current state-of-the-art methods for
music similarity. Finally, we hope that the OLGA dataset will facilitate
research on data-driven models for artist similarity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Korzeniowski_F/0/1/0/all/0/1"&gt;Filip Korzeniowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oramas_S/0/1/0/all/0/1"&gt;Sergio Oramas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gouyon_F/0/1/0/all/0/1"&gt;Fabien Gouyon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the interpretation of linear Riemannian tangent space model parameters in M/EEG. (arXiv:2107.14398v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14398</id>
        <link href="http://arxiv.org/abs/2107.14398"/>
        <updated>2021-08-02T01:58:24.561Z</updated>
        <summary type="html"><![CDATA[Riemannian tangent space methods offer state-of-the-art performance in
magnetoencephalography (MEG) and electroencephalography (EEG) based
applications such as brain-computer interfaces and biomarker development. One
limitation, particularly relevant for biomarker development, is limited model
interpretability compared to established component-based methods. Here, we
propose a method to transform the parameters of linear tangent space models
into interpretable patterns. Using typical assumptions, we show that this
approach identifies the true patterns of latent sources, encoding a target
signal. In simulations and two real MEG and EEG datasets, we demonstrate the
validity of the proposed approach and investigate its behavior when the model
assumptions are violated. Our results confirm that Riemannian tangent space
methods are robust to differences in the source patterns across observations.
We found that this robustness property also transfers to the associated
patterns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kobler_R/0/1/0/all/0/1"&gt;Reinmar J. Kobler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hirayama_J/0/1/0/all/0/1"&gt;Jun-Ichiro Hirayama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lopes_Dias_L/0/1/0/all/0/1"&gt;Lea Hehenberger Catarina Lopes-Dias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_Putz_G/0/1/0/all/0/1"&gt;Gernot R. M&amp;#xfc;ller-Putz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawanabe_M/0/1/0/all/0/1"&gt;Motoaki Kawanabe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deciphering Cryptic Behavior in Bimetallic Transition Metal Complexes with Machine Learning. (arXiv:2107.14280v1 [cond-mat.mtrl-sci])]]></title>
        <id>http://arxiv.org/abs/2107.14280</id>
        <link href="http://arxiv.org/abs/2107.14280"/>
        <updated>2021-08-02T01:58:24.551Z</updated>
        <summary type="html"><![CDATA[The rational tailoring of transition metal complexes is necessary to address
outstanding challenges in energy utilization and storage. Heterobimetallic
transition metal complexes that exhibit metal-metal bonding in stacked "double
decker" ligand structures are an emerging, attractive platform for catalysis,
but their properties are challenging to predict prior to laborious synthetic
efforts. We demonstrate an alternative, data-driven approach to uncovering
structure-property relationships for rational bimetallic complex design. We
tailor graph-based representations of the metal-local environment for these
heterobimetallic complexes for use in training of multiple linear regression
and kernel ridge regression (KRR) models. Focusing on oxidation potentials, we
obtain a set of 28 experimentally characterized complexes to develop a multiple
linear regression model. On this training set, we achieve good accuracy (mean
absolute error, MAE, of 0.25 V) and preserve transferability to unseen
experimental data with a new ligand structure. We trained a KRR model on a
subset of 330 structurally characterized heterobimetallics to predict the
degree of metal-metal bonding. This KRR model predicts relative metal-metal
bond lengths in the test set to within 5%, and analysis of key features reveals
the fundamental atomic contributions (e.g., the valence electron configuration)
that most strongly influence the behavior of complexes. Our work provides
guidance for rational bimetallic design, suggesting that properties including
the formal shortness ratio should be transferable from one period to another.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Taylor_M/0/1/0/all/0/1"&gt;Michael G. Taylor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Nandy_A/0/1/0/all/0/1"&gt;Aditya Nandy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Lu_C/0/1/0/all/0/1"&gt;Connie C. Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Kulik_H/0/1/0/all/0/1"&gt;Heather J. Kulik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trusted-Maximizers Entropy Search for Efficient Bayesian Optimization. (arXiv:2107.14465v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14465</id>
        <link href="http://arxiv.org/abs/2107.14465"/>
        <updated>2021-08-02T01:58:24.545Z</updated>
        <summary type="html"><![CDATA[Information-based Bayesian optimization (BO) algorithms have achieved
state-of-the-art performance in optimizing a black-box objective function.
However, they usually require several approximations or simplifying assumptions
(without clearly understanding their effects on the BO performance) and/or
their generalization to batch BO is computationally unwieldy, especially with
an increasing batch size. To alleviate these issues, this paper presents a
novel trusted-maximizers entropy search (TES) acquisition function: It measures
how much an input query contributes to the information gain on the maximizer
over a finite set of trusted maximizers, i.e., inputs optimizing functions that
are sampled from the Gaussian process posterior belief of the objective
function. Evaluating TES requires either only a stochastic approximation with
sampling or a deterministic approximation with expectation propagation, both of
which are investigated and empirically evaluated using synthetic benchmark
objective functions and real-world optimization problems, e.g., hyperparameter
tuning of a convolutional neural network and synthesizing 'physically
realizable' faces to fool a black-box face recognition system. Though TES can
naturally be generalized to a batch variant with either approximation, the
latter is amenable to be scaled to a much larger batch size in our experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1"&gt;Quoc Phong Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhaoxuan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Low_B/0/1/0/all/0/1"&gt;Bryan Kian Hsiang Low&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaillet_P/0/1/0/all/0/1"&gt;Patrick Jaillet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating the COVID-19 Identification ResNet (CIdeR) on the INTERSPEECH COVID-19 from Audio Challenges. (arXiv:2107.14549v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.14549</id>
        <link href="http://arxiv.org/abs/2107.14549"/>
        <updated>2021-08-02T01:58:24.525Z</updated>
        <summary type="html"><![CDATA[We report on cross-running the recent COVID-19 Identification ResNet (CIdeR)
on the two Interspeech 2021 COVID-19 diagnosis from cough and speech audio
challenges: ComParE and DiCOVA. CIdeR is an end-to-end deep learning neural
network originally designed to classify whether an individual is COVID-positive
or COVID-negative based on coughing and breathing audio recordings from a
published crowdsourced dataset. In the current study, we demonstrate the
potential of CIdeR at binary COVID-19 diagnosis from both the COVID-19 Cough
and Speech Sub-Challenges of INTERSPEECH 2021, ComParE and DiCOVA. CIdeR
achieves significant improvements over several baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akman_A/0/1/0/all/0/1"&gt;Alican Akman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coppock_H/0/1/0/all/0/1"&gt;Harry Coppock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaskell_A/0/1/0/all/0/1"&gt;Alexander Gaskell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tzirakis_P/0/1/0/all/0/1"&gt;Panagiotis Tzirakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jones_L/0/1/0/all/0/1"&gt;Lyn Jones&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn W. Schuller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TASK3 DCASE2021 Challenge: Sound event localization and detection using squeeze-excitation residual CNNs. (arXiv:2107.14561v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.14561</id>
        <link href="http://arxiv.org/abs/2107.14561"/>
        <updated>2021-08-02T01:58:24.506Z</updated>
        <summary type="html"><![CDATA[Sound event localisation and detection (SELD) is a problem in the field of
automatic listening that aims at the temporal detection and localisation
(direction of arrival estimation) of sound events within an audio clip, usually
of long duration. Due to the amount of data present in the datasets related to
this problem, solutions based on deep learning have positioned themselves at
the top of the state of the art. Most solutions are based on 2D representations
of the audio (different spectrograms) that are processed by a
convolutional-recurrent network. The motivation of this submission is to study
the squeeze-excitation technique in the convolutional part of the network and
how it improves the performance of the system. This study is based on the one
carried out by the same team last year. This year, it has been decided to study
how this technique improves each of the datasets (last year only the MIC
dataset was studied). This modification shows an improvement in the performance
of the system compared to the baseline using MIC dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naranjo_Alcazar_J/0/1/0/all/0/1"&gt;Javier Naranjo-Alcazar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Castanos_S/0/1/0/all/0/1"&gt;Sergi Perez-Castanos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuccarello_P/0/1/0/all/0/1"&gt;Pedro Zuccarello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferri_F/0/1/0/all/0/1"&gt;Francesc J. Ferri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cobos_M/0/1/0/all/0/1"&gt;Maximo Cobos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Creating Powerful and Interpretable Models withRegression Networks. (arXiv:2107.14417v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14417</id>
        <link href="http://arxiv.org/abs/2107.14417"/>
        <updated>2021-08-02T01:58:24.479Z</updated>
        <summary type="html"><![CDATA[As the discipline has evolved, research in machine learning has been focused
more and more on creating more powerful neural networks, without regard for the
interpretability of these networks. Such "black-box models" yield
state-of-the-art results, but we cannot understand why they make a particular
decision or prediction. Sometimes this is acceptable, but often it is not.

We propose a novel architecture, Regression Networks, which combines the
power of neural networks with the understandability of regression analysis.
While some methods for combining these exist in the literature, our
architecture generalizes these approaches by taking interactions into account,
offering the power of a dense neural network without forsaking
interpretability. We demonstrate that the models exceed the state-of-the-art
performance of interpretable models on several benchmark datasets, matching the
power of a dense neural network. Finally, we discuss how these techniques can
be generalized to other neural architectures, such as convolutional and
recurrent neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+ONeill_L/0/1/0/all/0/1"&gt;Lachlan O&amp;#x27;Neill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Angus_S/0/1/0/all/0/1"&gt;Simon Angus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borgohain_S/0/1/0/all/0/1"&gt;Satya Borgohain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chmait_N/0/1/0/all/0/1"&gt;Nader Chmait&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dowe_D/0/1/0/all/0/1"&gt;David L. Dowe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Otimizacao de pesos e funcoes de ativacao de redes neurais aplicadas na previsao de series temporais. (arXiv:2107.14370v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14370</id>
        <link href="http://arxiv.org/abs/2107.14370"/>
        <updated>2021-08-02T01:58:24.469Z</updated>
        <summary type="html"><![CDATA[Neural Networks have been applied for time series prediction with good
experimental results that indicate the high capacity to approximate functions
with good precision. Most neural models used in these applications use
activation functions with fixed parameters. However, it is known that the
choice of activation function strongly influences the complexity and
performance of the neural network and that a limited number of activation
functions have been used. In this work, we propose the use of a family of free
parameter asymmetric activation functions for neural networks and show that
this family of defined activation functions satisfies the requirements of the
universal approximation theorem. A methodology for the global optimization of
this family of activation functions with free parameter and the weights of the
connections between the processing units of the neural network is used. The
central idea of the proposed methodology is to simultaneously optimize the
weights and the activation function used in a multilayer perceptron network
(MLP), through an approach that combines the advantages of simulated annealing,
tabu search and a local learning algorithm, with the purpose of improving
performance in the adjustment and forecasting of time series. We chose two
learning algorithms: backpropagation with the term momentum (BPM) and
LevenbergMarquardt (LM).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gomes_G/0/1/0/all/0/1"&gt;Gecynalda Gomes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1"&gt;Teresa Ludermir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Generative Learning via Schr\"{o}dinger Bridge. (arXiv:2106.10410v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10410</id>
        <link href="http://arxiv.org/abs/2106.10410"/>
        <updated>2021-08-02T01:58:24.455Z</updated>
        <summary type="html"><![CDATA[We propose to learn a generative model via entropy interpolation with a
Schr\"{o}dinger Bridge. The generative learning task can be formulated as
interpolating between a reference distribution and a target distribution based
on the Kullback-Leibler divergence. At the population level, this entropy
interpolation is characterized via an SDE on $[0,1]$ with a time-varying drift
term. At the sample level, we derive our Schr\"{o}dinger Bridge algorithm by
plugging the drift term estimated by a deep score estimator and a deep density
ratio estimator into the Euler-Maruyama method. Under some mild smoothness
assumptions of the target distribution, we prove the consistency of both the
score estimator and the density ratio estimator, and then establish the
consistency of the proposed Schr\"{o}dinger Bridge approach. Our theoretical
results guarantee that the distribution learned by our approach converges to
the target distribution. Experimental results on multimodal synthetic data and
benchmark data support our theoretical findings and indicate that the
generative model via Schr\"{o}dinger Bridge is comparable with state-of-the-art
GANs, suggesting a new formulation of generative learning. We demonstrate its
usefulness in image interpolation and image inpainting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Gefei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1"&gt;Yuling Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1"&gt;Qian Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Can Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the relation between statistical learning and perceptual distances. (arXiv:2106.04427v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04427</id>
        <link href="http://arxiv.org/abs/2106.04427"/>
        <updated>2021-08-02T01:58:24.435Z</updated>
        <summary type="html"><![CDATA[It has been demonstrated many times that the behavior of the human visual
system is connected to the statistics of natural images. Since machine learning
relies on the statistics of training data as well, the above connection has
interesting implications when using perceptual distances (which mimic the
behavior of the human visual system) as a loss function. In this paper, we aim
to unravel the non-trivial relationship between the probability distribution of
the data, perceptual distances, and unsupervised machine learning. To this end,
we show that perceptual sensitivity is correlated with the probability of an
image in its close neighborhood. We also explore the relation between distances
induced by autoencoders and the probability distribution of the data used for
training them, as well as how these induced distances are correlated with human
perception. Finally, we discuss why perceptual distances might not lead to
noticeable gains in performance over standard Euclidean distances in common
image processing tasks except when data is scarce and the perceptual distance
provides regularization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hepburn_A/0/1/0/all/0/1"&gt;Alexander Hepburn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laparra_V/0/1/0/all/0/1"&gt;Valero Laparra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_Rodriguez_R/0/1/0/all/0/1"&gt;Raul Santos-Rodriguez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balle_J/0/1/0/all/0/1"&gt;Johannes Ball&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malo_J/0/1/0/all/0/1"&gt;Jes&amp;#xfa;s Malo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards the Unification and Data-Driven Synthesis of Autonomous Vehicle Safety Concepts. (arXiv:2107.14412v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.14412</id>
        <link href="http://arxiv.org/abs/2107.14412"/>
        <updated>2021-08-02T01:58:24.416Z</updated>
        <summary type="html"><![CDATA[As safety-critical autonomous vehicles (AVs) will soon become pervasive in
our society, a number of safety concepts for trusted AV deployment have been
recently proposed throughout industry and academia. Yet, agreeing upon an
"appropriate" safety concept is still an elusive task. In this paper, we
advocate for the use of Hamilton Jacobi (HJ) reachability as a unifying
mathematical framework for comparing existing safety concepts, and propose ways
to expand its modeling premises in a data-driven fashion. Specifically, we show
that (i) existing predominant safety concepts can be embedded in the HJ
reachability framework, thereby enabling a common language for comparing and
contrasting modeling assumptions, and (ii) HJ reachability can serve as an
inductive bias to effectively reason, in a data-driven context, about two
critical, yet often overlooked aspects of safety: responsibility and
context-dependency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bajcsy_A/0/1/0/all/0/1"&gt;Andrea Bajcsy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leung_K/0/1/0/all/0/1"&gt;Karen Leung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmerling_E/0/1/0/all/0/1"&gt;Edward Schmerling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1"&gt;Marco Pavone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards robust vision by multi-task learning on monkey visual cortex. (arXiv:2107.14344v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14344</id>
        <link href="http://arxiv.org/abs/2107.14344"/>
        <updated>2021-08-02T01:58:24.397Z</updated>
        <summary type="html"><![CDATA[Deep neural networks set the state-of-the-art across many tasks in computer
vision, but their generalization ability to image distortions is surprisingly
fragile. In contrast, the mammalian visual system is robust to a wide range of
perturbations. Recent work suggests that this generalization ability can be
explained by useful inductive biases encoded in the representations of visual
stimuli throughout the visual cortex. Here, we successfully leveraged these
inductive biases with a multi-task learning approach: we jointly trained a deep
network to perform image classification and to predict neural activity in
macaque primary visual cortex (V1). We measured the out-of-distribution
generalization abilities of our network by testing its robustness to image
distortions. We found that co-training on monkey V1 data leads to increased
robustness despite the absence of those distortions during training.
Additionally, we showed that our network's robustness is very close to that of
an Oracle network where parts of the architecture are directly trained on noisy
images. Our results also demonstrated that the network's representations become
more brain-like as their robustness improves. Using a novel constrained
reconstruction analysis, we investigated what makes our brain-regularized
network more robust. We found that our co-trained network is more sensitive to
content than noise when compared to a Baseline network that we trained for
image classification alone. Using DeepGaze-predicted saliency maps for ImageNet
images, we found that our monkey co-trained network tends to be more sensitive
to salient regions in a scene, reminiscent of existing theories on the role of
V1 in the detection of object borders and bottom-up saliency. Overall, our work
expands the promising research avenue of transferring inductive biases from the
brain, and provides a novel analysis of the effects of our transfer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Safarani_S/0/1/0/all/0/1"&gt;Shahd Safarani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nix_A/0/1/0/all/0/1"&gt;Arne Nix&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Willeke_K/0/1/0/all/0/1"&gt;Konstantin Willeke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cadena_S/0/1/0/all/0/1"&gt;Santiago A. Cadena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Restivo_K/0/1/0/all/0/1"&gt;Kelli Restivo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Denfield_G/0/1/0/all/0/1"&gt;George Denfield&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tolias_A/0/1/0/all/0/1"&gt;Andreas S. Tolias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sinz_F/0/1/0/all/0/1"&gt;Fabian H. Sinz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continental-Scale Building Detection from High Resolution Satellite Imagery. (arXiv:2107.12283v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12283</id>
        <link href="http://arxiv.org/abs/2107.12283"/>
        <updated>2021-08-02T01:58:24.389Z</updated>
        <summary type="html"><![CDATA[Identifying the locations and footprints of buildings is vital for many
practical and scientific purposes. Such information can be particularly useful
in developing regions where alternative data sources may be scarce. In this
work, we describe a model training pipeline for detecting buildings across the
entire continent of Africa, using 50 cm satellite imagery. Starting with the
U-Net model, widely used in satellite image analysis, we study variations in
architecture, loss functions, regularization, pre-training, self-training and
post-processing that increase instance segmentation performance. Experiments
were carried out using a dataset of 100k satellite images across Africa
containing 1.75M manually labelled building instances, and further datasets for
pre-training and self-training. We report novel methods for improving
performance of building detection with this type of model, including the use of
mixup (mAP +0.12) and self-training with soft KL loss (mAP +0.06). The
resulting pipeline obtains good results even on a wide variety of challenging
rural and urban contexts, and was used to create the Open Buildings dataset of
516M Africa-wide detected footprints.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sirko_W/0/1/0/all/0/1"&gt;Wojciech Sirko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kashubin_S/0/1/0/all/0/1"&gt;Sergii Kashubin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ritter_M/0/1/0/all/0/1"&gt;Marvin Ritter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Annkah_A/0/1/0/all/0/1"&gt;Abigail Annkah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouchareb_Y/0/1/0/all/0/1"&gt;Yasser Salah Eddine Bouchareb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dauphin_Y/0/1/0/all/0/1"&gt;Yann Dauphin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keysers_D/0/1/0/all/0/1"&gt;Daniel Keysers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neumann_M/0/1/0/all/0/1"&gt;Maxim Neumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cisse_M/0/1/0/all/0/1"&gt;Moustapha Cisse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quinn_J/0/1/0/all/0/1"&gt;John Quinn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ManiSkill: Learning-from-Demonstrations Benchmark for Generalizable Manipulation Skills. (arXiv:2107.14483v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14483</id>
        <link href="http://arxiv.org/abs/2107.14483"/>
        <updated>2021-08-02T01:58:24.381Z</updated>
        <summary type="html"><![CDATA[Learning generalizable manipulation skills is central for robots to achieve
task automation in environments with endless scene and object variations.
However, existing robot learning environments are limited in both scale and
diversity of 3D assets (especially of articulated objects), making it difficult
to train and evaluate the generalization ability of agents over novel objects.
In this work, we focus on object-level generalization and propose SAPIEN
Manipulation Skill Benchmark (abbreviated as ManiSkill), a large-scale
learning-from-demonstrations benchmark for articulated object manipulation with
visual input (point cloud and image). ManiSkill supports object-level
variations by utilizing a rich and diverse set of articulated objects, and each
task is carefully designed for learning manipulations on a single category of
objects. We equip ManiSkill with high-quality demonstrations to facilitate
learning-from-demonstrations approaches and perform evaluations on common
baseline algorithms. We believe ManiSkill can encourage the robot learning
community to explore more on learning generalizable object manipulation skills.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1"&gt;Tongzhou Mu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1"&gt;Zhan Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_F/0/1/0/all/0/1"&gt;Fanbo Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1"&gt;Derek Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuanlin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1"&gt;Stone Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhiao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1"&gt;Zhiwei Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hao Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EEG multipurpose eye blink detector using convolutional neural network. (arXiv:2107.14235v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2107.14235</id>
        <link href="http://arxiv.org/abs/2107.14235"/>
        <updated>2021-08-02T01:58:24.374Z</updated>
        <summary type="html"><![CDATA[The electrical signal emitted by the eyes movement produces a very strong
artifact on EEG signaldue to its close proximity to the sensors and abundance
of occurrence. In the context of detectingeye blink artifacts in EEG waveforms
for further removal and signal purification, multiple strategieswhere proposed
in the literature. Most commonly applied methods require the use of a large
numberof electrodes, complex equipment for sampling and processing data. The
goal of this work is to createa reliable and user independent algorithm for
detecting and removing eye blink in EEG signals usingCNN (convolutional neural
network). For training and validation, three sets of public EEG data wereused.
All three sets contain samples obtained while the recruited subjects performed
assigned tasksthat included blink voluntarily in specific moments, watch a
video and read an article. The modelused in this study was able to have an
embracing understanding of all the features that distinguish atrivial EEG
signal from a signal contaminated with eye blink artifacts without being
overfitted byspecific features that only occurred in the situations when the
signals were registered.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Iaquinta_A/0/1/0/all/0/1"&gt;Amanda Ferrari Iaquinta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Silva_A/0/1/0/all/0/1"&gt;Ana Carolina de Sousa Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Junior_A/0/1/0/all/0/1"&gt;Aldrumont Ferraz J&amp;#xfa;nior&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Toledo_J/0/1/0/all/0/1"&gt;Jessica Monique de Toledo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Atzingen_G/0/1/0/all/0/1"&gt;Gustavo Voltani von Atzingen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Optimizers with Sparse Group Lasso for Neural Networks in CTR Prediction. (arXiv:2107.14432v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14432</id>
        <link href="http://arxiv.org/abs/2107.14432"/>
        <updated>2021-08-02T01:58:24.368Z</updated>
        <summary type="html"><![CDATA[We develop a novel framework that adds the regularizers of the sparse group
lasso to a family of adaptive optimizers in deep learning, such as Momentum,
Adagrad, Adam, AMSGrad, AdaHessian, and create a new class of optimizers, which
are named Group Momentum, Group Adagrad, Group Adam, Group AMSGrad and Group
AdaHessian, etc., accordingly. We establish theoretically proven convergence
guarantees in the stochastic convex settings, based on primal-dual methods. We
evaluate the regularized effect of our new optimizers on three large-scale
real-world ad click datasets with state-of-the-art deep learning models. The
experimental results reveal that compared with the original optimizers with the
post-processing procedure which uses the magnitude pruning method, the
performance of the models can be significantly improved on the same sparsity
level. Furthermore, in comparison to the cases without magnitude pruning, our
methods can achieve extremely high sparsity with significantly better or highly
competitive performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1"&gt;Yun Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yongchao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1"&gt;Suo Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Minghao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1"&gt;Chunyang Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1"&gt;Huanjun Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1"&gt;Lihong Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jinjie Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1"&gt;Yixiang Mu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manipulating Identical Filter Redundancy for Efficient Pruning on Deep and Complicated CNN. (arXiv:2107.14444v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14444</id>
        <link href="http://arxiv.org/abs/2107.14444"/>
        <updated>2021-08-02T01:58:24.348Z</updated>
        <summary type="html"><![CDATA[The existence of redundancy in Convolutional Neural Networks (CNNs) enables
us to remove some filters/channels with acceptable performance drops. However,
the training objective of CNNs usually tends to minimize an accuracy-related
loss function without any attention paid to the redundancy, making the
redundancy distribute randomly on all the filters, such that removing any of
them may trigger information loss and accuracy drop, necessitating a following
finetuning step for recovery. In this paper, we propose to manipulate the
redundancy during training to facilitate network pruning. To this end, we
propose a novel Centripetal SGD (C-SGD) to make some filters identical,
resulting in ideal redundancy patterns, as such filters become purely redundant
due to their duplicates; hence removing them does not harm the network. As
shown on CIFAR and ImageNet, C-SGD delivers better performance because the
redundancy is better organized, compared to the existing methods. The
efficiency also characterizes C-SGD because it is as fast as regular SGD,
requires no finetuning, and can be conducted simultaneously on all the layers
even in very deep CNNs. Besides, C-SGD can improve the accuracy of CNNs by
first training a model with the same architecture but wider layers then
squeezing it into the original width.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1"&gt;Xiaohan Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_T/0/1/0/all/0/1"&gt;Tianxiang Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jungong Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yuchen Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1"&gt;Guiguang Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FloMo: Tractable Motion Prediction with Normalizing Flows. (arXiv:2103.03614v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03614</id>
        <link href="http://arxiv.org/abs/2103.03614"/>
        <updated>2021-08-02T01:58:24.339Z</updated>
        <summary type="html"><![CDATA[The future motion of traffic participants is inherently uncertain. To plan
safely, therefore, an autonomous agent must take into account multiple possible
trajectory outcomes and prioritize them. Recently, this problem has been
addressed with generative neural networks. However, most generative models
either do not learn the true underlying trajectory distribution reliably, or do
not allow predictions to be associated with likelihoods. In our work, we model
motion prediction directly as a density estimation problem with a normalizing
flow between a noise distribution and the future motion distribution. Our
model, named FloMo, allows likelihoods to be computed in a single network pass
and can be trained directly with maximum likelihood estimation. Furthermore, we
propose a method to stabilize training flows on trajectory datasets and a new
data augmentation transformation that improves the performance and
generalization of our model. Our method achieves state-of-the-art performance
on three popular prediction datasets, with a significant gap to most competing
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Scholler_C/0/1/0/all/0/1"&gt;Christoph Sch&amp;#xf6;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1"&gt;Alois Knoll&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DCT2net: an interpretable shallow CNN for image denoising. (arXiv:2107.14803v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.14803</id>
        <link href="http://arxiv.org/abs/2107.14803"/>
        <updated>2021-08-02T01:58:24.331Z</updated>
        <summary type="html"><![CDATA[This work tackles the issue of noise removal from images, focusing on the
well-known DCT image denoising algorithm. The latter, stemming from signal
processing, has been well studied over the years. Though very simple, it is
still used in crucial parts of state-of-the-art "traditional" denoising
algorithms such as BM3D. Since a few years however, deep convolutional neural
networks (CNN) have outperformed their traditional counterparts, making signal
processing methods less attractive. In this paper, we demonstrate that a DCT
denoiser can be seen as a shallow CNN and thereby its original linear transform
can be tuned through gradient descent in a supervised manner, improving
considerably its performance. This gives birth to a fully interpretable CNN
called DCT2net. To deal with remaining artifacts induced by DCT2net, an
original hybrid solution between DCT and DCT2net is proposed combining the best
that these two methods can offer; DCT2net is selected to process non-stationary
image patches while DCT is optimal for piecewise smooth patches. Experiments on
artificially noisy images demonstrate that two-layer DCT2net provides
comparable results to BM3D and is as fast as DnCNN algorithm composed of more
than a dozen of layers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Herbreteau_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Herbreteau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kervrann_C/0/1/0/all/0/1"&gt;Charles Kervrann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distribution free optimality intervals for clustering. (arXiv:2107.14442v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.14442</id>
        <link href="http://arxiv.org/abs/2107.14442"/>
        <updated>2021-08-02T01:58:24.323Z</updated>
        <summary type="html"><![CDATA[We address the problem of validating the ouput of clustering algorithms.
Given data $\mathcal{D}$ and a partition $\mathcal{C}$ of these data into $K$
clusters, when can we say that the clusters obtained are correct or meaningful
for the data? This paper introduces a paradigm in which a clustering
$\mathcal{C}$ is considered meaningful if it is good with respect to a loss
function such as the K-means distortion, and stable, i.e. the only good
clustering up to small perturbations. Furthermore, we present a generic method
to obtain post-inference guarantees of near-optimality and stability for a
clustering $\mathcal{C}$. The method can be instantiated for a variety of
clustering criteria (also called loss functions) for which convex relaxations
exist. Obtaining the guarantees amounts to solving a convex optimization
problem. We demonstrate the practical relevance of this method by obtaining
guarantees for the K-means and the Normalized Cut clustering criteria on
realistic data sets. We also prove that asymptotic instability implies finite
sample instability w.h.p., allowing inferences about the population
clusterability from a sample. The guarantees do not depend on any
distributional assumptions, but they depend on the data set $\mathcal{D}$
admitting a stable clustering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Meila_M/0/1/0/all/0/1"&gt;Marina Meil&amp;#x103;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hanyu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SiamAPN++: Siamese Attentional Aggregation Network for Real-Time UAV Tracking. (arXiv:2106.08816v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08816</id>
        <link href="http://arxiv.org/abs/2106.08816"/>
        <updated>2021-08-02T01:58:24.317Z</updated>
        <summary type="html"><![CDATA[Recently, the Siamese-based method has stood out from multitudinous tracking
methods owing to its state-of-the-art (SOTA) performance. Nevertheless, due to
various special challenges in UAV tracking, \textit{e.g.}, severe occlusion and
fast motion, most existing Siamese-based trackers hardly combine superior
performance with high efficiency. To this concern, in this paper, a novel
attentional Siamese tracker (SiamAPN++) is proposed for real-time UAV tracking.
By virtue of the attention mechanism, we conduct a special attentional
aggregation network (AAN) consisting of self-AAN and cross-AAN for raising the
representation ability of features eventually. The former AAN aggregates and
models the self-semantic interdependencies of the single feature map via
spatial and channel dimensions. The latter aims to aggregate the
cross-interdependencies of two different semantic features including the
location information of anchors. In addition, the anchor proposal network based
on dual features is proposed to raise its robustness of tracking objects with
various scales. Experiments on two well-known authoritative benchmarks are
conducted, where SiamAPN++ outperforms its baseline SiamAPN and other SOTA
trackers. Besides, real-world tests onboard a typical embedded platform
demonstrate that SiamAPN++ achieves promising tracking results with real-time
speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1"&gt;Ziang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Changhong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Junjie Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bowen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yiming Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Developing Open Source Educational Resources for Machine Learning and Data Science. (arXiv:2107.14330v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2107.14330</id>
        <link href="http://arxiv.org/abs/2107.14330"/>
        <updated>2021-08-02T01:58:24.310Z</updated>
        <summary type="html"><![CDATA[Education should not be a privilege but a common good. It should be openly
accessible to everyone, with as few barriers as possible; even more so for key
technologies such as Machine Learning (ML) and Data Science (DS). Open
Educational Resources (OER) are a crucial factor for greater educational
equity. In this paper, we describe the specific requirements for OER in ML and
DS and argue that it is especially important for these fields to make source
files publicly available, leading to Open Source Educational Resources (OSER).
We present our view on the collaborative development of OSER, the challenges
this poses, and first steps towards their solutions. We outline how OSER can be
used for blended learning scenarios and share our experiences in university
education. Finally, we discuss additional challenges such as credit assignment
or granting certificates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bothmann_L/0/1/0/all/0/1"&gt;Ludwig Bothmann&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Strickroth_S/0/1/0/all/0/1"&gt;Sven Strickroth&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Casalicchio_G/0/1/0/all/0/1"&gt;Giuseppe Casalicchio&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Rugamer_D/0/1/0/all/0/1"&gt;David R&amp;#xfc;gamer&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Lindauer_M/0/1/0/all/0/1"&gt;Marius Lindauer&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Scheipl_F/0/1/0/all/0/1"&gt;Fabian Scheipl&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1"&gt;Bernd Bischl&lt;/a&gt; (1) ((1) Department of Statistics, Ludwig-Maximilians-Universit&amp;#xe4;t M&amp;#xfc;nchen, Germany, (2) Institute of Computer Science, Ludwig-Maximilians-Universit&amp;#xe4;t M&amp;#xfc;nchen, Germany, (3) Institute of Information Process, Leibniz University Hannover, Germany)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IGibson 1.0: a Simulation Environment for Interactive Tasks in Large Realistic Scenes. (arXiv:2012.02924v5 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02924</id>
        <link href="http://arxiv.org/abs/2012.02924"/>
        <updated>2021-08-02T01:58:24.292Z</updated>
        <summary type="html"><![CDATA[We present iGibson 1.0, a novel simulation environment to develop robotic
solutions for interactive tasks in large-scale realistic scenes. Our
environment contains 15 fully interactive home-sized scenes with 108 rooms
populated with rigid and articulated objects. The scenes are replicas of
real-world homes, with distribution and the layout of objects aligned to those
of the real world. iGibson 1.0 integrates several key features to facilitate
the study of interactive tasks: i) generation of high-quality virtual sensor
signals (RGB, depth, segmentation, LiDAR, flow and so on), ii) domain
randomization to change the materials of the objects (both visual and physical)
and/or their shapes, iii) integrated sampling-based motion planners to generate
collision-free trajectories for robot bases and arms, and iv) intuitive
human-iGibson interface that enables efficient collection of human
demonstrations. Through experiments, we show that the full interactivity of the
scenes enables agents to learn useful visual representations that accelerate
the training of downstream manipulation tasks. We also show that iGibson 1.0
features enable the generalization of navigation agents, and that the
human-iGibson interface and integrated motion planners facilitate efficient
imitation learning of human demonstrated (mobile) manipulation behaviors.
iGibson 1.0 is open-source, equipped with comprehensive examples and
documentation. For more information, visit our project website:
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1"&gt;Bokui Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1"&gt;Fei Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chengshu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1"&gt;Roberto Mart&amp;#xed;n-Mart&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1"&gt;Linxi Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guanzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_DArpino_C/0/1/0/all/0/1"&gt;Claudia P&amp;#xe9;rez-D&amp;#x27;Arpino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buch_S/0/1/0/all/0/1"&gt;Shyamal Buch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1"&gt;Sanjana Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tchapmi_L/0/1/0/all/0/1"&gt;Lyne P. Tchapmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tchapmi_M/0/1/0/all/0/1"&gt;Micael E. Tchapmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1"&gt;Kent Vainio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1"&gt;Josiah Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1"&gt;Silvio Savarese&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying Uncertainty for Machine Learning Based Diagnostic. (arXiv:2107.14261v1 [physics.acc-ph])]]></title>
        <id>http://arxiv.org/abs/2107.14261</id>
        <link href="http://arxiv.org/abs/2107.14261"/>
        <updated>2021-08-02T01:58:24.285Z</updated>
        <summary type="html"><![CDATA[Virtual Diagnostic (VD) is a deep learning tool that can be used to predict a
diagnostic output. VDs are especially useful in systems where measuring the
output is invasive, limited, costly or runs the risk of damaging the output.
Given a prediction, it is necessary to relay how reliable that prediction is.
This is known as 'uncertainty quantification' of a prediction. In this paper,
we use ensemble methods and quantile regression neural networks to explore
different ways of creating and analyzing prediction's uncertainty on
experimental data from the Linac Coherent Light Source at SLAC. We aim to
accurately and confidently predict the current profile or longitudinal phase
space images of the electron beam. The ability to make informed decisions under
uncertainty is crucial for reliable deployment of deep learning tools on
safety-critical systems as particle accelerators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Convery_O/0/1/0/all/0/1"&gt;Owen Convery&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Smith_L/0/1/0/all/0/1"&gt;Lewis Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Hanuka_A/0/1/0/all/0/1"&gt;Adi Hanuka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed Identification of Contracting and/or Monotone Network Dynamics. (arXiv:2107.14309v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2107.14309</id>
        <link href="http://arxiv.org/abs/2107.14309"/>
        <updated>2021-08-02T01:58:24.279Z</updated>
        <summary type="html"><![CDATA[This paper proposes methods for identification of large-scale networked
systems with guarantees that the resulting model will be contracting -- a
strong form of nonlinear stability -- and/or monotone, i.e. order relations
between states are preserved. The main challenges that we address are:
simultaneously searching for model parameters and a certificate of stability,
and scalability to networks with hundreds or thousands of nodes. We propose a
model set that admits convex constraints for stability and monotonicity, and
has a separable structure that allows distributed identification via the
alternating directions method of multipliers (ADMM). The performance and
scalability of the approach is illustrated on a variety of linear and
non-linear case studies, including a nonlinear traffic network with a
200-dimensional state space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Revay_M/0/1/0/all/0/1"&gt;Max Revay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Umenberger_J/0/1/0/all/0/1"&gt;Jack Umenberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Manchester_I/0/1/0/all/0/1"&gt;Ian R. Manchester&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Siamese Anchor Proposal Network for High-Speed Aerial Tracking. (arXiv:2012.10706v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10706</id>
        <link href="http://arxiv.org/abs/2012.10706"/>
        <updated>2021-08-02T01:58:24.273Z</updated>
        <summary type="html"><![CDATA[In the domain of visual tracking, most deep learning-based trackers highlight
the accuracy but casting aside efficiency. Therefore, their real-world
deployment on mobile platforms like the unmanned aerial vehicle (UAV) is
impeded. In this work, a novel two-stage Siamese network-based method is
proposed for aerial tracking, \textit{i.e.}, stage-1 for high-quality anchor
proposal generation, stage-2 for refining the anchor proposal. Different from
anchor-based methods with numerous pre-defined fixed-sized anchors, our
no-prior method can 1) increase the robustness and generalization to different
objects with various sizes, especially to small, occluded, and fast-moving
objects, under complex scenarios in light of the adaptive anchor generation, 2)
make calculation feasible due to the substantial decrease of anchor numbers. In
addition, compared to anchor-free methods, our framework has better performance
owing to refinement at stage-2. Comprehensive experiments on three benchmarks
have proven the superior performance of our approach, with a speed of around
200 frames/s.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Changhong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1"&gt;Ziang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yiming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Junjie Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1"&gt;Chen Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synth-by-Reg (SbR): Contrastive learning for synthesis-based registration of paired images. (arXiv:2107.14449v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14449</id>
        <link href="http://arxiv.org/abs/2107.14449"/>
        <updated>2021-08-02T01:58:24.267Z</updated>
        <summary type="html"><![CDATA[Nonlinear inter-modality registration is often challenging due to the lack of
objective functions that are good proxies for alignment. Here we propose a
synthesis-by-registration method to convert this problem into an easier
intra-modality task. We introduce a registration loss for weakly supervised
image translation between domains that does not require perfectly aligned
training data. This loss capitalises on a registration U-Net with frozen
weights, to drive a synthesis CNN towards the desired translation. We
complement this loss with a structure preserving constraint based on
contrastive learning, which prevents blurring and content shifts due to
overfitting. We apply this method to the registration of histological sections
to MRI slices, a key step in 3D histology reconstruction. Results on two
different public datasets show improvements over registration based on mutual
information (13% reduction in landmark error) and synthesis-based algorithms
such as CycleGAN (11% reduction), and are comparable to a registration CNN with
label supervision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Casamitjana_A/0/1/0/all/0/1"&gt;Adri&amp;#xe0; Casamitjana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mancini_M/0/1/0/all/0/1"&gt;Matteo Mancini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iglesias_J/0/1/0/all/0/1"&gt;Juan Eugenio Iglesias&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fooling LiDAR Perception via Adversarial Trajectory Perturbation. (arXiv:2103.15326v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15326</id>
        <link href="http://arxiv.org/abs/2103.15326"/>
        <updated>2021-08-02T01:58:24.248Z</updated>
        <summary type="html"><![CDATA[LiDAR point clouds collected from a moving vehicle are functions of its
trajectories, because the sensor motion needs to be compensated to avoid
distortions. When autonomous vehicles are sending LiDAR point clouds to deep
networks for perception and planning, could the motion compensation
consequently become a wide-open backdoor in those networks, due to both the
adversarial vulnerability of deep learning and GPS-based vehicle trajectory
estimation that is susceptible to wireless spoofing? We demonstrate such
possibilities for the first time: instead of directly attacking point cloud
coordinates which requires tampering with the raw LiDAR readings, only
adversarial spoofing of a self-driving car's trajectory with small
perturbations is enough to make safety-critical objects undetectable or
detected with incorrect positions. Moreover, polynomial trajectory perturbation
is developed to achieve a temporally-smooth and highly-imperceptible attack.
Extensive experiments on 3D object detection have shown that such attacks not
only lower the performance of the state-of-the-art detectors effectively, but
also transfer to other detectors, raising a red flag for the community. The
code is available on https://ai4ce.github.io/FLAT/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yiming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1"&gt;Congcong Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1"&gt;Felix Juefei-Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1"&gt;Chen Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis. (arXiv:2103.14910v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14910</id>
        <link href="http://arxiv.org/abs/2103.14910"/>
        <updated>2021-08-02T01:58:24.240Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose MINE to perform novel view synthesis and depth
estimation via dense 3D reconstruction from a single image. Our approach is a
continuous depth generalization of the Multiplane Images (MPI) by introducing
the NEural radiance fields (NeRF). Given a single image as input, MINE predicts
a 4-channel image (RGB and volume density) at arbitrary depth values to jointly
reconstruct the camera frustum and fill in occluded contents. The reconstructed
and inpainted frustum can then be easily rendered into novel RGB or depth views
using differentiable rendering. Extensive experiments on RealEstate10K, KITTI
and Flowers Light Fields show that our MINE outperforms state-of-the-art by a
large margin in novel view synthesis. We also achieve competitive results in
depth estimation on iBims-1 and NYU-v2 without annotated depth supervision. Our
source code is available at https://github.com/vincentfung13/MINE]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiaxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zijian Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1"&gt;Qi She&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1"&gt;Henghui Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Changhu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1"&gt;Gim Hee Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Autofocus for Synthetic Aperture Sonar. (arXiv:2010.15687v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.15687</id>
        <link href="http://arxiv.org/abs/2010.15687"/>
        <updated>2021-08-02T01:58:24.219Z</updated>
        <summary type="html"><![CDATA[Synthetic aperture sonar (SAS) requires precise positional and environmental
information to produce well-focused output during the image reconstruction
step. However, errors in these measurements are commonly present resulting in
defocused imagery. To overcome these issues, an \emph{autofocus} algorithm is
employed as a post-processing step after image reconstruction for the purpose
of improving image quality using the image content itself. These algorithms are
usually iterative and metric-based in that they seek to optimize an image
sharpness metric. In this letter, we demonstrate the potential of machine
learning, specifically deep learning, to address the autofocus problem. We
formulate the problem as a self-supervised, phase error estimation task using a
deep network we call Deep Autofocus. Our formulation has the advantages of
being non-iterative (and thus fast) and not requiring ground truth
focused-defocused images pairs as often required by other deblurring deep
learning methods. We compare our technique against a set of common sharpness
metrics optimized using gradient descent over a real-world dataset. Our results
demonstrate Deep Autofocus can produce imagery that is perceptually as good as
benchmark iterative techniques but at a substantially lower computational cost.
We conclude that our proposed Deep Autofocus can provide a more favorable
cost-quality trade-off than state-of-the-art alternatives with significant
potential of future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gerg_I/0/1/0/all/0/1"&gt;Isaac Gerg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Monga_V/0/1/0/all/0/1"&gt;Vishal Monga&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling User Empathy Elicited by a Robot Storyteller. (arXiv:2107.14345v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.14345</id>
        <link href="http://arxiv.org/abs/2107.14345"/>
        <updated>2021-08-02T01:58:24.212Z</updated>
        <summary type="html"><![CDATA[Virtual and robotic agents capable of perceiving human empathy have the
potential to participate in engaging and meaningful human-machine interactions
that support human well-being. Prior research in computational empathy has
focused on designing empathic agents that use verbal and nonverbal behaviors to
simulate empathy and attempt to elicit empathic responses from humans. The
challenge of developing agents with the ability to automatically perceive
elicited empathy in humans remains largely unexplored. Our paper presents the
first approach to modeling user empathy elicited during interactions with a
robotic agent. We collected a new dataset from the novel interaction context of
participants listening to a robot storyteller (46 participants, 6.9 hours of
video). After each storytelling interaction, participants answered a
questionnaire that assessed their level of elicited empathy during the
interaction with the robot. We conducted experiments with 8 classical machine
learning models and 2 deep learning models (long short-term memory networks and
temporal convolutional networks) to detect empathy by leveraging patterns in
participants' visual behaviors while they were listening to the robot
storyteller. Our highest-performing approach, based on XGBoost, achieved an
accuracy of 69% and AUC of 72% when detecting empathy in videos. We contribute
insights regarding modeling approaches and visual features for automated
empathy detection. Our research informs and motivates future development of
empathy perception models that can be leveraged by virtual and robotic agents
during human-machine interactions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mathur_L/0/1/0/all/0/1"&gt;Leena Mathur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spitale_M/0/1/0/all/0/1"&gt;Micol Spitale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xi_H/0/1/0/all/0/1"&gt;Hao Xi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jieyun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mataric_M/0/1/0/all/0/1"&gt;Maja J Matari&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Efficacy of Small Self-Supervised Contrastive Models without Distillation Signals. (arXiv:2107.14762v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14762</id>
        <link href="http://arxiv.org/abs/2107.14762"/>
        <updated>2021-08-02T01:58:24.206Z</updated>
        <summary type="html"><![CDATA[It is a consensus that small models perform quite poorly under the paradigm
of self-supervised contrastive learning. Existing methods usually adopt a large
off-the-shelf model to transfer knowledge to the small one via knowledge
distillation. Despite their effectiveness, distillation-based methods may not
be suitable for some resource-restricted scenarios due to the huge
computational expenses of deploying a large model. In this paper, we study the
issue of training self-supervised small models without distillation signals. We
first evaluate the representation spaces of the small models and make two
non-negligible observations: (i) small models can complete the pretext task
without overfitting despite its limited capacity; (ii) small models universally
suffer the problem of over-clustering. Then we verify multiple assumptions that
are considered to alleviate the over-clustering phenomenon. Finally, we combine
the validated techniques and improve the baseline of five small architectures
with considerable margins, which indicates that training small self-supervised
contrastive models is feasible even without distillation signals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1"&gt;Haizhou Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Youcai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1"&gt;Siliang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1"&gt;Wenjie Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yaqian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yandong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1"&gt;Yueting Zhuang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Perceiver IO: A General Architecture for Structured Inputs & Outputs. (arXiv:2107.14795v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14795</id>
        <link href="http://arxiv.org/abs/2107.14795"/>
        <updated>2021-08-02T01:58:24.128Z</updated>
        <summary type="html"><![CDATA[The recently-proposed Perceiver model obtains good results on several domains
(images, audio, multimodal, point clouds) while scaling linearly in compute and
memory with the input size. While the Perceiver supports many kinds of inputs,
it can only produce very simple outputs such as class scores. Perceiver IO
overcomes this limitation without sacrificing the original's appealing
properties by learning to flexibly query the model's latent space to produce
outputs of arbitrary size and semantics. Perceiver IO still decouples model
depth from data size and still scales linearly with data size, but now with
respect to both input and output sizes. The full Perceiver IO model achieves
strong results on tasks with highly structured output spaces, such as natural
language and visual understanding, StarCraft II, and multi-task and multi-modal
domains. As highlights, Perceiver IO matches a Transformer-based BERT baseline
on the GLUE language benchmark without the need for input tokenization and
achieves state-of-the-art performance on Sintel optical flow estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1"&gt;Andrew Jaegle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borgeaud_S/0/1/0/all/0/1"&gt;Sebastian Borgeaud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alayrac_J/0/1/0/all/0/1"&gt;Jean-Baptiste Alayrac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doersch_C/0/1/0/all/0/1"&gt;Carl Doersch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ionescu_C/0/1/0/all/0/1"&gt;Catalin Ionescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1"&gt;David Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koppula_S/0/1/0/all/0/1"&gt;Skanda Koppula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brock_A/0/1/0/all/0/1"&gt;Andrew Brock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shelhamer_E/0/1/0/all/0/1"&gt;Evan Shelhamer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henaff_O/0/1/0/all/0/1"&gt;Olivier H&amp;#xe9;naff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Botvinick_M/0/1/0/all/0/1"&gt;Matthew M. Botvinick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1"&gt;Andrew Zisserman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1"&gt;Oriol Vinyals&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Carreira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maximum Entropy Dueling Network Architecture. (arXiv:2107.14457v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14457</id>
        <link href="http://arxiv.org/abs/2107.14457"/>
        <updated>2021-08-02T01:58:24.116Z</updated>
        <summary type="html"><![CDATA[In recent years, there have been many deep structures for Reinforcement
Learning, mainly for value function estimation and representations. These
methods achieved great success in Atari 2600 domain. In this paper, we propose
an improved architecture based upon Dueling Networks, in this architecture,
there are two separate estimators, one approximate the state value function and
the other, state advantage function. This improvement based on Maximum Entropy,
shows better policy evaluation compared to the original network and other
value-based architectures in Atari domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nadali_A/0/1/0/all/0/1"&gt;Alireza Nadali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ebadzadeh_M/0/1/0/all/0/1"&gt;Mohammad Mehdi Ebadzadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Adaptive Multi-Factor Model and the Financial Market. (arXiv:2107.14410v1 [q-fin.ST])]]></title>
        <id>http://arxiv.org/abs/2107.14410</id>
        <link href="http://arxiv.org/abs/2107.14410"/>
        <updated>2021-08-02T01:58:24.110Z</updated>
        <summary type="html"><![CDATA[Modern evolvements of the technologies have been leading to a profound
influence on the financial market. The introduction of constituents like
Exchange-Traded Funds, and the wide-use of advanced technologies such as
algorithmic trading, results in a boom of the data which provides more
opportunities to reveal deeper insights. However, traditional statistical
methods always suffer from the high-dimensional, high-correlation, and
time-varying instinct of the financial data. In this dissertation, we focus on
developing techniques to stress these difficulties. With the proposed
methodologies, we can have more interpretable models, clearer explanations, and
better predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Liao Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpenSync: An opensource platform for synchronizing multiple measures in neuroscience experiments. (arXiv:2107.14367v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14367</id>
        <link href="http://arxiv.org/abs/2107.14367"/>
        <updated>2021-08-02T01:58:24.103Z</updated>
        <summary type="html"><![CDATA[Background: The human mind is multimodal. Yet most behavioral studies rely on
century-old measures such as task accuracy and latency. To create a better
understanding of human behavior and brain functionality, we should introduce
other measures and analyze behavior from various aspects. However, it is
technically complex and costly to design and implement the experiments that
record multiple measures. To address this issue, a platform that allows
synchronizing multiple measures from human behavior is needed. Method: This
paper introduces an opensource platform named OpenSync, which can be used to
synchronize multiple measures in neuroscience experiments. This platform helps
to automatically integrate, synchronize and record physiological measures
(e.g., electroencephalogram (EEG), galvanic skin response (GSR), eye-tracking,
body motion, etc.), user input response (e.g., from mouse, keyboard, joystick,
etc.), and task-related information (stimulus markers). In this paper, we
explain the structure and details of OpenSync, provide two case studies in
PsychoPy and Unity. Comparison with existing tools: Unlike proprietary systems
(e.g., iMotions), OpenSync is free and it can be used inside any opensource
experiment design software (e.g., PsychoPy, OpenSesame, Unity, etc.,
https://pypi.org/project/OpenSync/ and
https://github.com/moeinrazavi/OpenSync_Unity). Results: Our experimental
results show that the OpenSync platform is able to synchronize multiple
measures with microsecond resolution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Razavi_M/0/1/0/all/0/1"&gt;Moein Razavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Janfaza_V/0/1/0/all/0/1"&gt;Vahid Janfaza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamauchi_T/0/1/0/all/0/1"&gt;Takashi Yamauchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leontyev_A/0/1/0/all/0/1"&gt;Anton Leontyev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Longmire_Monford_S/0/1/0/all/0/1"&gt;Shanle Longmire-Monford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orr_J/0/1/0/all/0/1"&gt;Joseph Orr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Networks Provably Classify Data on Curves. (arXiv:2107.14324v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.14324</id>
        <link href="http://arxiv.org/abs/2107.14324"/>
        <updated>2021-08-02T01:58:24.096Z</updated>
        <summary type="html"><![CDATA[Data with low-dimensional nonlinear structure are ubiquitous in engineering
and scientific problems. We study a model problem with such structure -- a
binary classification task that uses a deep fully-connected neural network to
classify data drawn from two disjoint smooth curves on the unit sphere. Aside
from mild regularity conditions, we place no restrictions on the configuration
of the curves. We prove that when (i) the network depth is large relative to
certain geometric properties that set the difficulty of the problem and (ii)
the network width and number of samples is polynomial in the depth,
randomly-initialized gradient descent quickly learns to correctly classify all
points on the two curves with high probability. To our knowledge, this is the
first generalization guarantee for deep networks with nonlinear data that
depends only on intrinsic data properties. Our analysis proceeds by a reduction
to dynamics in the neural tangent kernel (NTK) regime, where the network depth
plays the role of a fitting resource in solving the classification problem. In
particular, via fine-grained control of the decay properties of the NTK, we
demonstrate that when the network is sufficiently deep, the NTK can be locally
approximated by a translationally invariant operator on the manifolds and
stably inverted over smooth functions, which guarantees convergence and
generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tingran Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Buchanan_S/0/1/0/all/0/1"&gt;Sam Buchanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gilboa_D/0/1/0/all/0/1"&gt;Dar Gilboa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wright_J/0/1/0/all/0/1"&gt;John Wright&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[T-SVDNet: Exploring High-Order Prototypical Correlations for Multi-Source Domain Adaptation. (arXiv:2107.14447v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14447</id>
        <link href="http://arxiv.org/abs/2107.14447"/>
        <updated>2021-08-02T01:58:24.077Z</updated>
        <summary type="html"><![CDATA[Most existing domain adaptation methods focus on adaptation from only one
source domain, however, in practice there are a number of relevant sources that
could be leveraged to help improve performance on target domain. We propose a
novel approach named T-SVDNet to address the task of Multi-source Domain
Adaptation (MDA), which is featured by incorporating Tensor Singular Value
Decomposition (T-SVD) into a neural network's training pipeline. Overall,
high-order correlations among multiple domains and categories are fully
explored so as to better bridge the domain gap. Specifically, we impose
Tensor-Low-Rank (TLR) constraint on a tensor obtained by stacking up a group of
prototypical similarity matrices, aiming at capturing consistent data structure
across different domains. Furthermore, to avoid negative transfer brought by
noisy source data, we propose a novel uncertainty-aware weighting strategy to
adaptively assign weights to different source domains and samples based on the
result of uncertainty estimation. Extensive experiments conducted on public
benchmarks demonstrate the superiority of our model in addressing the task of
MDA compared to state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruihuang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xu Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jianzhong He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shuaijun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1"&gt;Qinghua Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random vector functional link neural network based ensemble deep learning for short-term load forecasting. (arXiv:2107.14385v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14385</id>
        <link href="http://arxiv.org/abs/2107.14385"/>
        <updated>2021-08-02T01:58:24.071Z</updated>
        <summary type="html"><![CDATA[Electricity load forecasting is crucial for the power systems' planning and
maintenance. However, its un-stationary and non-linear characteristics impose
significant difficulties in anticipating future demand. This paper proposes a
novel ensemble deep Random Vector Functional Link (edRVFL) network for
electricity load forecasting. The weights of hidden layers are randomly
initialized and kept fixed during the training process. The hidden layers are
stacked to enforce deep representation learning. Then, the model generates the
forecasts by ensembling the outputs of each layer. Moreover, we also propose to
augment the random enhancement features by empirical wavelet transformation
(EWT). The raw load data is decomposed by EWT in a walk-forward fashion, not
introducing future data leakage problems in the decomposition process. Finally,
all the sub-series generated by the EWT, including raw data, are fed into the
edRVFL for forecasting purposes. The proposed model is evaluated on twenty
publicly available time series from the Australian Energy Market Operator of
the year 2020. The simulation results demonstrate the proposed model's superior
performance over eleven forecasting methods in three error metrics and
statistical tests on electricity load forecasting tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1"&gt;Ruobin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1"&gt;Liang Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suganthan_P/0/1/0/all/0/1"&gt;P.N. Suganthan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1"&gt;Qin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuen_K/0/1/0/all/0/1"&gt;Kum Fai Yuen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Quantized Representation for Enhanced Reconstruction. (arXiv:2107.14368v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.14368</id>
        <link href="http://arxiv.org/abs/2107.14368"/>
        <updated>2021-08-02T01:58:24.064Z</updated>
        <summary type="html"><![CDATA[While machine learning approaches have shown remarkable performance in
biomedical image analysis, most of these methods rely on high-quality and
accurate imaging data. However, collecting such data requires intensive and
careful manual effort. One of the major challenges in imaging the Shoot Apical
Meristem (SAM) of Arabidopsis thaliana, is that the deeper slices in the
z-stack suffer from different perpetual quality-related problems like poor
contrast and blurring. These quality-related issues often lead to the disposal
of the painstakingly collected data with little to no control on quality while
collecting the data. Therefore, it becomes necessary to employ and design
techniques that can enhance the images to make them more suitable for further
analysis. In this paper, we propose a data-driven Deep Quantized Latent
Representation (DQLR) methodology for high-quality image reconstruction in the
Shoot Apical Meristem (SAM) of Arabidopsis thaliana. Our proposed framework
utilizes multiple consecutive slices in the z-stack to learn a low dimensional
latent space, quantize it and subsequently perform reconstruction using the
quantized representation to obtain sharper images. Experiments on a publicly
available dataset validate our methodology showing promising results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Akash Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Aich_A/0/1/0/all/0/1"&gt;Abhishek Aich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rodriguez_K/0/1/0/all/0/1"&gt;Kevin Rodriguez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Reddy_G/0/1/0/all/0/1"&gt;G. Venugopala Reddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1"&gt;Amit K. Roy-Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using transfer learning to study burned area dynamics: A case study of refugee settlements in West Nile, Northern Uganda. (arXiv:2107.14372v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14372</id>
        <link href="http://arxiv.org/abs/2107.14372"/>
        <updated>2021-08-02T01:58:24.055Z</updated>
        <summary type="html"><![CDATA[With the global refugee crisis at a historic high, there is a growing need to
assess the impact of refugee settlements on their hosting countries and
surrounding environments. Because fires are an important land management
practice in smallholder agriculture in sub-Saharan Africa, burned area (BA)
mappings can help provide information about the impacts of land management
practices on local environments. However, a lack of BA ground-truth data in
much of sub-Saharan Africa limits the use of highly scalable deep learning (DL)
techniques for such BA mappings. In this work, we propose a scalable transfer
learning approach to study BA dynamics in areas with little to no ground-truth
data such as the West Nile region in Northern Uganda. We train a deep learning
model on BA ground-truth data in Portugal and propose the application of that
model on refugee-hosting districts in West Nile between 2015 and 2020. By
comparing the district-level BA dynamic with the wider West Nile region, we aim
to add understanding of the land management impacts of refugee settlements on
their surrounding environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huppertz_R/0/1/0/all/0/1"&gt;Robert Huppertz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakalembe_C/0/1/0/all/0/1"&gt;Catherine Nakalembe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kerner_H/0/1/0/all/0/1"&gt;Hannah Kerner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lachyan_R/0/1/0/all/0/1"&gt;Ramani Lachyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rischard_M/0/1/0/all/0/1"&gt;Maxime Rischard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transductive image segmentation: Self-training and effect of uncertainty estimation. (arXiv:2107.08964v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08964</id>
        <link href="http://arxiv.org/abs/2107.08964"/>
        <updated>2021-08-02T01:58:24.048Z</updated>
        <summary type="html"><![CDATA[Semi-supervised learning (SSL) uses unlabeled data during training to learn
better models. Previous studies on SSL for medical image segmentation focused
mostly on improving model generalization to unseen data. In some applications,
however, our primary interest is not generalization but to obtain optimal
predictions on a specific unlabeled database that is fully available during
model development. Examples include population studies for extracting imaging
phenotypes. This work investigates an often overlooked aspect of SSL,
transduction. It focuses on the quality of predictions made on the unlabeled
data of interest when they are included for optimization during training,
rather than improving generalization. We focus on the self-training framework
and explore its potential for transduction. We analyze it through the lens of
Information Gain and reveal that learning benefits from the use of calibrated
or under-confident models. Our extensive experiments on a large MRI database
for multi-class segmentation of traumatic brain lesions shows promising results
when comparing transductive with inductive predictions. We believe this study
will inspire further research on transductive learning, a well-suited paradigm
for medical image analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kamnitsas_K/0/1/0/all/0/1"&gt;Konstantinos Kamnitsas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winzeck_S/0/1/0/all/0/1"&gt;Stefan Winzeck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kornaropoulos_E/0/1/0/all/0/1"&gt;Evgenios N. Kornaropoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whitehouse_D/0/1/0/all/0/1"&gt;Daniel Whitehouse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Englman_C/0/1/0/all/0/1"&gt;Cameron Englman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phyu_P/0/1/0/all/0/1"&gt;Poe Phyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pao_N/0/1/0/all/0/1"&gt;Norman Pao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menon_D/0/1/0/all/0/1"&gt;David K. Menon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1"&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_T/0/1/0/all/0/1"&gt;Tilak Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Newcombe_V/0/1/0/all/0/1"&gt;Virginia F.J. Newcombe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1"&gt;Ben Glocker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Survey of Recent Multi-Agent Reinforcement Learning Algorithms Utilizing Centralized Training. (arXiv:2107.14316v1 [cs.MA])]]></title>
        <id>http://arxiv.org/abs/2107.14316</id>
        <link href="http://arxiv.org/abs/2107.14316"/>
        <updated>2021-08-02T01:58:24.039Z</updated>
        <summary type="html"><![CDATA[Much work has been dedicated to the exploration of Multi-Agent Reinforcement
Learning (MARL) paradigms implementing a centralized learning with
decentralized execution (CLDE) approach to achieve human-like collaboration in
cooperative tasks. Here, we discuss variations of centralized training and
describe a recent survey of algorithmic approaches. The goal is to explore how
different implementations of information sharing mechanism in centralized
learning may give rise to distinct group coordinated behaviors in multi-agent
systems performing cooperative tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1"&gt;Piyush K. Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandez_R/0/1/0/all/0/1"&gt;Rolando Fernandez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaroukian_E/0/1/0/all/0/1"&gt;Erin Zaroukian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dorothy_M/0/1/0/all/0/1"&gt;Michael Dorothy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basak_A/0/1/0/all/0/1"&gt;Anjon Basak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asher_D/0/1/0/all/0/1"&gt;Derrik E. Asher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Batch Active Learning at Scale. (arXiv:2107.14263v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14263</id>
        <link href="http://arxiv.org/abs/2107.14263"/>
        <updated>2021-08-02T01:58:24.017Z</updated>
        <summary type="html"><![CDATA[The ability to train complex and highly effective models often requires an
abundance of training data, which can easily become a bottleneck in cost, time,
and computational resources. Batch active learning, which adaptively issues
batched queries to a labeling oracle, is a common approach for addressing this
problem. The practical benefits of batch sampling come with the downside of
less adaptivity and the risk of sampling redundant examples within a batch -- a
risk that grows with the batch size. In this work, we analyze an efficient
active learning algorithm, which focuses on the large batch setting. In
particular, we show that our sampling method, which combines notions of
uncertainty and diversity, easily scales to batch sizes (100K-1M) several
orders of magnitude larger than used in previous studies and provides
significant improvements in model training efficiency compared to recent
baselines. Finally, we provide an initial theoretical analysis, proving label
complexity guarantees for a related sampling method, which we show is
approximately equivalent to our sampling method in specific settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Citovsky_G/0/1/0/all/0/1"&gt;Gui Citovsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DeSalvo_G/0/1/0/all/0/1"&gt;Giulia DeSalvo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gentile_C/0/1/0/all/0/1"&gt;Claudio Gentile&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karydas_L/0/1/0/all/0/1"&gt;Lazaros Karydas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajagopalan_A/0/1/0/all/0/1"&gt;Anand Rajagopalan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rostamizadeh_A/0/1/0/all/0/1"&gt;Afshin Rostamizadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sanjiv Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MLMOD Package: Machine Learning Methods for Data-Driven Modeling in LAMMPS. (arXiv:2107.14362v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14362</id>
        <link href="http://arxiv.org/abs/2107.14362"/>
        <updated>2021-08-02T01:58:24.007Z</updated>
        <summary type="html"><![CDATA[We discuss a software package for incorporating into simulations data-driven
models trained using machine learning methods. These can be used for (i)
modeling dynamics and time-step integration, (ii) modeling interactions between
system components, and (iii) computing quantities of interest characterizing
system state. The package allows for use of machine learning methods with
general model classes including Neural Networks, Gaussian Process Regression,
Kernel Models, and other approaches. We discuss in this whitepaper our
prototype C++ package, aims, and example usage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Atzberger_P/0/1/0/all/0/1"&gt;Paul J. Atzberger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[No-Reference Quality Assessment for 3D Colored Point Cloud and Mesh Models. (arXiv:2107.02041v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02041</id>
        <link href="http://arxiv.org/abs/2107.02041"/>
        <updated>2021-08-02T01:58:24.001Z</updated>
        <summary type="html"><![CDATA[To improve the viewer's Quality of Experience (QoE) and optimize computer
graphics applications, 3D model quality assessment (3D-QA) has become an
important task in the multimedia area. Point cloud and mesh are the two most
widely used digital representation formats of 3D models, the visual quality of
which is quite sensitive to lossy operations like simplification and
compression. Therefore, many related studies such as point cloud quality
assessment (PCQA) and mesh quality assessment (MQA) have been carried out to
measure the caused visual quality degradations. However, a large part of
previous studies utilizes full-reference (FR) metrics, which means they may
fail to predict the quality level with the absence of the reference 3D model.
Furthermore, few 3D-QA metrics are carried out to consider color information,
which significantly restricts the effectiveness and scope of application. In
this paper, we propose a no-reference (NR) quality assessment metric for
colored 3D models represented by both point cloud and mesh. First, we project
the 3D models from 3D space into quality-related geometry and color feature
domains. Then, the natural scene statistics (NSS) and entropy are utilized to
extract quality-aware features. Finally, the Support Vector Regressor (SVR) is
employed to regress the quality-aware features into quality scores. Our method
is mainly validated on the colored point cloud quality assessment database
(SJTU-PCQA) and the colored mesh quality assessment database (CMDM). The
experimental results show that the proposed method outperforms all the
state-of-art NR 3D-QA metrics and obtains an acceptable gap with the
state-of-art FR 3D-QA metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zicheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1"&gt;Xiongkuo Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wei Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1"&gt;Guangtao Zhai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting Few-Shot Classification with View-Learnable Contrastive Learning. (arXiv:2107.09242v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.09242</id>
        <link href="http://arxiv.org/abs/2107.09242"/>
        <updated>2021-08-02T01:58:23.993Z</updated>
        <summary type="html"><![CDATA[The goal of few-shot classification is to classify new categories with few
labeled examples within each class. Nowadays, the excellent performance in
handling few-shot classification problems is shown by metric-based
meta-learning methods. However, it is very hard for previous methods to
discriminate the fine-grained sub-categories in the embedding space without
fine-grained labels. This may lead to unsatisfactory generalization to
fine-grained subcategories, and thus affects model interpretation. To tackle
this problem, we introduce the contrastive loss into few-shot classification
for learning latent fine-grained structure in the embedding space. Furthermore,
to overcome the drawbacks of random image transformation used in current
contrastive learning in producing noisy and inaccurate image pairs (i.e.,
views), we develop a learning-to-learn algorithm to automatically generate
different views of the same image. Extensive experiments on standard few-shot
learning benchmarks demonstrate the superiority of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xu Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuxuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1"&gt;Liangjian Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1"&gt;Lili Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Transformer for Multivariate Clinical Time-Series with Missing Values. (arXiv:2107.14293v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14293</id>
        <link href="http://arxiv.org/abs/2107.14293"/>
        <updated>2021-08-02T01:58:23.987Z</updated>
        <summary type="html"><![CDATA[Multivariate time-series (MVTS) data are frequently observed in critical care
settings and are typically characterized by excessive missingness and irregular
time intervals. Existing approaches for learning representations in this domain
handle such issues by either aggregation or imputation of values, which in-turn
suppresses the fine-grained information and adds undesirable noise/overhead
into the machine learning model. To tackle this challenge, we propose STraTS
(Self-supervised Transformer for TimeSeries) model which bypasses these
pitfalls by treating time-series as a set of observation triplets instead of
using the traditional dense matrix representation. It employs a novel
Continuous Value Embedding (CVE) technique to encode continuous time and
variable values without the need for discretization. It is composed of a
Transformer component with Multi-head attention layers which enables it to
learn contextual triplet embeddings while avoiding problems of recurrence and
vanishing gradients that occur in recurrent architectures. Many healthcare
datasets also suffer from the limited availability of labeled data. Our model
utilizes self-supervision by leveraging unlabeled data to learn better
representations by performing time-series forecasting as a self-supervision
task. Experiments on real-world multivariate clinical time-series benchmark
datasets show that STraTS shows better prediction performance than
state-of-the-art methods for mortality prediction, especially when labeled data
is limited. Finally, we also present an interpretable version of STraTS which
can identify important measurements in the time-series data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tipirneni_S/0/1/0/all/0/1"&gt;Sindhu Tipirneni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1"&gt;Chandan K. Reddy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal Dependencies in Feature Importance for Time Series Predictions. (arXiv:2107.14317v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14317</id>
        <link href="http://arxiv.org/abs/2107.14317"/>
        <updated>2021-08-02T01:58:23.970Z</updated>
        <summary type="html"><![CDATA[Explanation methods applied to sequential models for multivariate time series
prediction are receiving more attention in machine learning literature. While
current methods perform well at providing instance-wise explanations, they
struggle to efficiently and accurately make attributions over long periods of
time and with complex feature interactions. We propose WinIT, a framework for
evaluating feature importance in time series prediction settings by quantifying
the shift in predictive distribution over multiple instances in a windowed
setting. Comprehensive empirical evidence shows our method improves on the
previous state-of-the-art, FIT, by capturing temporal dependencies in feature
importance. We also demonstrate how the solution improves the appropriate
attribution of features within time steps, which existing interpretability
methods often fail to do. We compare with baselines on simulated and real-world
clinical data. WinIT achieves 2.47x better performance than FIT and other
feature importance methods on real-world clinical MIMIC-mortality task. The
code for this work is available at https://github.com/layer6ai-labs/WinIT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rooke_C/0/1/0/all/0/1"&gt;Clayton Rooke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1"&gt;Jonathan Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leung_K/0/1/0/all/0/1"&gt;Kin Kwan Leung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Volkovs_M/0/1/0/all/0/1"&gt;Maksims Volkovs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuberi_S/0/1/0/all/0/1"&gt;Saba Zuberi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Show to Tell: A Survey on Image Captioning. (arXiv:2107.06912v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06912</id>
        <link href="http://arxiv.org/abs/2107.06912"/>
        <updated>2021-08-02T01:58:23.964Z</updated>
        <summary type="html"><![CDATA[Connecting Vision and Language plays an essential role in Generative
Intelligence. For this reason, large research efforts have been devoted to
image captioning, i.e. describing images with syntactically and semantically
meaningful sentences. Starting from 2015 the task has generally been addressed
with pipelines composed of a visual encoder and a language model for text
generation. During these years, both components have evolved considerably
through the exploitation of object regions, attributes, the introduction of
multi-modal connections, fully-attentive approaches, and BERT-like early-fusion
strategies. However, regardless of the impressive results, research in image
captioning has not reached a conclusive answer yet. This work aims at providing
a comprehensive overview of image captioning approaches, from visual encoding
and text generation to training strategies, datasets, and evaluation metrics.
In this respect, we quantitatively compare many relevant state-of-the-art
approaches to identify the most impactful technical innovations in
architectures and training strategies. Moreover, many variants of the problem
and its open challenges are discussed. The final goal of this work is to serve
as a tool for understanding the existing literature and highlighting the future
directions for a research area where Computer Vision and Natural Language
Processing can find an optimal synergy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stefanini_M/0/1/0/all/0/1"&gt;Matteo Stefanini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1"&gt;Marcella Cornia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1"&gt;Lorenzo Baraldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cascianelli_S/0/1/0/all/0/1"&gt;Silvia Cascianelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1"&gt;Giuseppe Fiameni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1"&gt;Rita Cucchiara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Verifiable Fingerprinting Scheme for Generative Adversarial Networks. (arXiv:2106.11760v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11760</id>
        <link href="http://arxiv.org/abs/2106.11760"/>
        <updated>2021-08-02T01:58:23.958Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel fingerprinting scheme for the Intellectual
Property (IP) protection of Generative Adversarial Networks (GANs). Prior
solutions for classification models adopt adversarial examples as the
fingerprints, which can raise stealthiness and robustness problems when they
are applied to the GAN models. Our scheme constructs a composite deep learning
model from the target GAN and a classifier. Then we generate stealthy
fingerprint samples from this composite model, and register them to the
classifier for effective ownership verification. This scheme inspires three
concrete methodologies to practically protect the modern GAN models.
Theoretical analysis proves that these methods can satisfy different security
requirements necessary for IP protection. We also conduct extensive experiments
to show that our solutions outperform existing strategies in terms of
stealthiness, functionality-preserving and unremovability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guanlin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Guowen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1"&gt;Han Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1"&gt;Shangwei Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Run Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiwei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianwei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling and Optimizing Laser-Induced Graphene. (arXiv:2107.14257v1 [physics.app-ph])]]></title>
        <id>http://arxiv.org/abs/2107.14257</id>
        <link href="http://arxiv.org/abs/2107.14257"/>
        <updated>2021-08-02T01:58:23.952Z</updated>
        <summary type="html"><![CDATA[A lot of technological advances depend on next-generation materials, such as
graphene, which enables a raft of new applications, for example better
electronics. Manufacturing such materials is often difficult; in particular,
producing graphene at scale is an open problem. We provide a series of datasets
that describe the optimization of the production of laser-induced graphene, an
established manufacturing method that has shown great promise. We pose three
challenges based on the datasets we provide -- modeling the behavior of
laser-induced graphene production with respect to parameters of the production
process, transferring models and knowledge between different precursor
materials, and optimizing the outcome of the transformation over the space of
possible production parameters. We present illustrative results, along with the
code used to generate them, as a starting point for interested users. The data
we provide represents an important real-world application of machine learning;
to the best of our knowledge, no similar datasets are available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Kotthoff_L/0/1/0/all/0/1"&gt;Lars Kotthoff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Dey_S/0/1/0/all/0/1"&gt;Sourin Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Jain_V/0/1/0/all/0/1"&gt;Vivek Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Tyrrell_A/0/1/0/all/0/1"&gt;Alexander Tyrrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wahab_H/0/1/0/all/0/1"&gt;Hud Wahab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Johnson_P/0/1/0/all/0/1"&gt;Patrick Johnson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VerSe: A Vertebrae Labelling and Segmentation Benchmark for Multi-detector CT Images. (arXiv:2001.09193v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.09193</id>
        <link href="http://arxiv.org/abs/2001.09193"/>
        <updated>2021-08-02T01:58:23.945Z</updated>
        <summary type="html"><![CDATA[Vertebral labelling and segmentation are two fundamental tasks in an
automated spine processing pipeline. Reliable and accurate processing of spine
images is expected to benefit clinical decision-support systems for diagnosis,
surgery planning, and population-based analysis on spine and bone health.
However, designing automated algorithms for spine processing is challenging
predominantly due to considerable variations in anatomy and acquisition
protocols and due to a severe shortage of publicly available data. Addressing
these limitations, the Large Scale Vertebrae Segmentation Challenge (VerSe) was
organised in conjunction with the International Conference on Medical Image
Computing and Computer Assisted Intervention (MICCAI) in 2019 and 2020, with a
call for algorithms towards labelling and segmentation of vertebrae. Two
datasets containing a total of 374 multi-detector CT scans from 355 patients
were prepared and 4505 vertebrae have individually been annotated at
voxel-level by a human-machine hybrid algorithm (https://osf.io/nqjyw/,
https://osf.io/t98fz/). A total of 25 algorithms were benchmarked on these
datasets. In this work, we present the the results of this evaluation and
further investigate the performance-variation at vertebra-level, scan-level,
and at different fields-of-view. We also evaluate the generalisability of the
approaches to an implicit domain shift in data by evaluating the top performing
algorithms of one challenge iteration on data from the other iteration. The
principal takeaway from VerSe: the performance of an algorithm in labelling and
segmenting a spine scan hinges on its ability to correctly identify vertebrae
in cases of rare anatomical variations. The content and code concerning VerSe
can be accessed at: https://github.com/anjany/verse.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sekuboyina_A/0/1/0/all/0/1"&gt;Anjany Sekuboyina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Husseini_M/0/1/0/all/0/1"&gt;Malek E. Husseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bayat_A/0/1/0/all/0/1"&gt;Amirhossein Bayat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loffler_M/0/1/0/all/0/1"&gt;Maximilian L&amp;#xf6;ffler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liebl_H/0/1/0/all/0/1"&gt;Hans Liebl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongwei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tetteh_G/0/1/0/all/0/1"&gt;Giles Tetteh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kukacka_J/0/1/0/all/0/1"&gt;Jan Kuka&amp;#x10d;ka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Payer_C/0/1/0/all/0/1"&gt;Christian Payer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stern_D/0/1/0/all/0/1"&gt;Darko &amp;#x160;tern&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Urschler_M/0/1/0/all/0/1"&gt;Martin Urschler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Maodong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1"&gt;Dalong Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lessmann_N/0/1/0/all/0/1"&gt;Nikolas Lessmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yujin Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianfu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1"&gt;Dong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Daguang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ambellan_F/0/1/0/all/0/1"&gt;Felix Ambellan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amiranashvili_T/0/1/0/all/0/1"&gt;Tamaz Amiranashvili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ehlke_M/0/1/0/all/0/1"&gt;Moritz Ehlke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lamecker_H/0/1/0/all/0/1"&gt;Hans Lamecker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehnert_S/0/1/0/all/0/1"&gt;Sebastian Lehnert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lirio_M/0/1/0/all/0/1"&gt;Marilia Lirio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olaguer_N/0/1/0/all/0/1"&gt;Nicol&amp;#xe1;s P&amp;#xe9;rez de Olaguer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramm_H/0/1/0/all/0/1"&gt;Heiko Ramm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahu_M/0/1/0/all/0/1"&gt;Manish Sahu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tack_A/0/1/0/all/0/1"&gt;Alexander Tack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zachow_S/0/1/0/all/0/1"&gt;Stefan Zachow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1"&gt;Tao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xinjun Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Angerman_C/0/1/0/all/0/1"&gt;Christoph Angerman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_K/0/1/0/all/0/1"&gt;Kevin Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirszenberg_A/0/1/0/all/0/1"&gt;Alexandre Kirszenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Puybareau_E/0/1/0/all/0/1"&gt;&amp;#xc9;lodie Puybareau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Di Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yiwei Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rapazzo_B/0/1/0/all/0/1"&gt;Brandon H. Rapazzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeah_T/0/1/0/all/0/1"&gt;Timyoas Yeah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Amber Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shangliang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_F/0/1/0/all/0/1"&gt;Feng Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zhiqiang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1"&gt;Chan Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiangshang_Z/0/1/0/all/0/1"&gt;Zheng Xiangshang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liming_X/0/1/0/all/0/1"&gt;Xu Liming&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Netherton_T/0/1/0/all/0/1"&gt;Tucker J. Netherton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mumme_R/0/1/0/all/0/1"&gt;Raymond P. Mumme&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Court_L/0/1/0/all/0/1"&gt;Laurence E. Court&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zixun Huang&lt;/a&gt;, et al. (18 additional authors not shown)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FINT: Field-aware INTeraction Neural Network For CTR Prediction. (arXiv:2107.01999v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01999</id>
        <link href="http://arxiv.org/abs/2107.01999"/>
        <updated>2021-08-02T01:58:23.925Z</updated>
        <summary type="html"><![CDATA[As a critical component for online advertising and marking, click-through
rate (CTR) prediction has draw lots of attentions from both industry and
academia field. Recently, the deep learning has become the mainstream
methodological choice for CTR. Despite of sustainable efforts have been made,
existing approaches still pose several challenges. On the one hand, high-order
interaction between the features is under-explored. On the other hand,
high-order interactions may neglect the semantic information from the low-order
fields. In this paper, we proposed a novel prediction method, named FINT, that
employs the Field-aware INTeraction layer which captures high-order feature
interactions while retaining the low-order field information. To empirically
investigate the effectiveness and robustness of the FINT, we perform extensive
experiments on the three realistic databases: KDD2012, Criteo and Avazu. The
obtained results demonstrate that the FINT can significantly improve the
performance compared to the existing methods, without increasing the amount of
computation required. Moreover, the proposed method brought about 2.72\%
increase to the advertising revenue of a big online video app through A/B
testing. To better promote the research in CTR field, we released our code as
well as reference implementation at: https://github.com/zhishan01/FINT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhishan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Sen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guohui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1"&gt;Dawei Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kele Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When Deep Learners Change Their Mind: Learning Dynamics for Active Learning. (arXiv:2107.14707v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14707</id>
        <link href="http://arxiv.org/abs/2107.14707"/>
        <updated>2021-08-02T01:58:23.917Z</updated>
        <summary type="html"><![CDATA[Active learning aims to select samples to be annotated that yield the largest
performance improvement for the learning algorithm. Many methods approach this
problem by measuring the informativeness of samples and do this based on the
certainty of the network predictions for samples. However, it is well-known
that neural networks are overly confident about their prediction and are
therefore an untrustworthy source to assess sample informativeness. In this
paper, we propose a new informativeness-based active learning method. Our
measure is derived from the learning dynamics of a neural network. More
precisely we track the label assignment of the unlabeled data pool during the
training of the algorithm. We capture the learning dynamics with a metric
called label-dispersion, which is low when the network consistently assigns the
same label to the sample during the training of the network and high when the
assigned label changes frequently. We show that label-dispersion is a promising
predictor of the uncertainty of the network, and show on two benchmark datasets
that an active learning algorithm based on label-dispersion obtains excellent
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bengar_J/0/1/0/all/0/1"&gt;Javad Zolfaghari Bengar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raducanu_B/0/1/0/all/0/1"&gt;Bogdan Raducanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1"&gt;Joost van de Weijer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse-to-dense Feature Matching: Intra and Inter domain Cross-modal Learning in Domain Adaptation for 3D Semantic Segmentation. (arXiv:2107.14724v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14724</id>
        <link href="http://arxiv.org/abs/2107.14724"/>
        <updated>2021-08-02T01:58:23.910Z</updated>
        <summary type="html"><![CDATA[Domain adaptation is critical for success when confronting with the lack of
annotations in a new domain. As the huge time consumption of labeling process
on 3D point cloud, domain adaptation for 3D semantic segmentation is of great
expectation. With the rise of multi-modal datasets, large amount of 2D images
are accessible besides 3D point clouds. In light of this, we propose to further
leverage 2D data for 3D domain adaptation by intra and inter domain cross modal
learning. As for intra-domain cross modal learning, most existing works sample
the dense 2D pixel-wise features into the same size with sparse 3D point-wise
features, resulting in the abandon of numerous useful 2D features. To address
this problem, we propose Dynamic sparse-to-dense Cross Modal Learning (DsCML)
to increase the sufficiency of multi-modality information interaction for
domain adaptation. For inter-domain cross modal learning, we further advance
Cross Modal Adversarial Learning (CMAL) on 2D and 3D data which contains
different semantic content aiming to promote high-level modal complementarity.
We evaluate our model under various multi-modality domain adaptation settings
including day-to-night, country-to-country and dataset-to-dataset, brings large
improvements over both uni-modal and multi-modal domain adaptation methods on
all settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1"&gt;Duo Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1"&gt;Yinjie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pingping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yulan Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seeing poverty from space, how much can it be tuned?. (arXiv:2107.14700v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14700</id>
        <link href="http://arxiv.org/abs/2107.14700"/>
        <updated>2021-08-02T01:58:23.903Z</updated>
        <summary type="html"><![CDATA[Since the United Nations launched the Sustainable Development Goals (SDG) in
2015, numerous universities, NGOs and other organizations have attempted to
develop tools for monitoring worldwide progress in achieving them. Led by
advancements in the fields of earth observation techniques, data sciences and
the emergence of artificial intelligence, a number of research teams have
developed innovative tools for highlighting areas of vulnerability and tracking
the implementation of SDG targets. In this paper we demonstrate that
individuals with no organizational affiliation and equipped only with common
hardware, publicly available datasets and cloud-based computing services can
participate in the improvement of predicting machine-learning-based approaches
to predicting local poverty levels in a given agro-ecological environment. The
approach builds upon several pioneering efforts over the last five years
related to mapping poverty by deep learning to process satellite imagery and
"ground-truth" data from the field to link features with incidence of poverty
in a particular context. The approach employs new methods for object
identification in order to optimize the modeled results and achieve
significantly high accuracy. A key goal of the project was to intentionally
keep costs as low as possible - by using freely available resources - so that
citizen scientists, students and organizations could replicate the method in
other areas of interest. Moreover, for simplicity, the input data used were
derived from just a handful of sources (involving only earth observation and
population headcounts). The results of the project could therefore certainly be
strengthened further through the integration of proprietary data from social
networks, mobile phone providers, and other sources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sako_T/0/1/0/all/0/1"&gt;Tomas Sako&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martinez_A/0/1/0/all/0/1"&gt;Arturo Jr M. Martinez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CAM-GAN: Continual Adaptation Modules for Generative Adversarial Networks. (arXiv:2103.04032v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04032</id>
        <link href="http://arxiv.org/abs/2103.04032"/>
        <updated>2021-08-02T01:58:23.897Z</updated>
        <summary type="html"><![CDATA[We present a continual learning approach for generative adversarial networks
(GANs), by designing and leveraging parameter-efficient feature map
transformations. Our approach is based on learning a set of global and
task-specific parameters. The global parameters are fixed across tasks whereas
the task-specific parameters act as local adapters for each task, and help in
efficiently obtaining task-specific feature maps. Moreover, we propose an
element-wise addition of residual bias in the transformed feature space, which
further helps stabilize GAN training in such settings. Our approach also
leverages task similarity information based on the Fisher information matrix.
Leveraging this knowledge from previous tasks significantly improves the model
performance. In addition, the similarity measure also helps reduce the
parameter growth in continual adaptation and helps to learn a compact model. In
contrast to the recent approaches for continually-learned GANs, the proposed
approach provides a memory-efficient way to perform effective continual data
generation. Through extensive experiments on challenging and diverse datasets,
we show that the feature-map-transformation approach outperforms
state-of-the-art methods for continually-learned GANs, with substantially fewer
parameters. The proposed method generates high-quality samples that can also
improve the generative-replay-based continual learning for discriminative
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Varshney_S/0/1/0/all/0/1"&gt;Sakshi Varshney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_V/0/1/0/all/0/1"&gt;Vinay Kumar Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+K_S/0/1/0/all/0/1"&gt;Srijith P K&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1"&gt;Lawrence Carin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rai_P/0/1/0/all/0/1"&gt;Piyush Rai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can non-specialists provide high quality gold standard labels in challenging modalities?. (arXiv:2107.14682v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14682</id>
        <link href="http://arxiv.org/abs/2107.14682"/>
        <updated>2021-08-02T01:58:23.880Z</updated>
        <summary type="html"><![CDATA[Probably yes. -- Supervised Deep Learning dominates performance scores for
many computer vision tasks and defines the state-of-the-art. However, medical
image analysis lags behind natural image applications. One of the many reasons
is the lack of well annotated medical image data available to researchers. One
of the first things researchers are told is that we require significant
expertise to reliably and accurately interpret and label such data. We see
significant inter- and intra-observer variability between expert annotations of
medical images. Still, it is a widely held assumption that novice annotators
are unable to provide useful annotations for use by clinical Deep Learning
models. In this work we challenge this assumption and examine the implications
of using a minimally trained novice labelling workforce to acquire annotations
for a complex medical image dataset. We study the time and cost implications of
using novice annotators, the raw performance of novice annotators compared to
gold-standard expert annotators, and the downstream effects on a trained Deep
Learning segmentation model's performance for detecting a specific congenital
heart disease (hypoplastic left heart syndrome) in fetal ultrasound imaging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Budd_S/0/1/0/all/0/1"&gt;Samuel Budd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Day_T/0/1/0/all/0/1"&gt;Thomas Day&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simpson_J/0/1/0/all/0/1"&gt;John Simpson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lloyd_K/0/1/0/all/0/1"&gt;Karen Lloyd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matthew_J/0/1/0/all/0/1"&gt;Jacqueline Matthew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Skelton_E/0/1/0/all/0/1"&gt;Emily Skelton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Razavi_R/0/1/0/all/0/1"&gt;Reza Razavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1"&gt;Bernhard Kainz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Minimum Edit Arborescence Problem and Its Use in Compressing Graph Collections [Extended Version]. (arXiv:2107.14525v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14525</id>
        <link href="http://arxiv.org/abs/2107.14525"/>
        <updated>2021-08-02T01:58:23.872Z</updated>
        <summary type="html"><![CDATA[The inference of minimum spanning arborescences within a set of objects is a
general problem which translates into numerous application-specific
unsupervised learning tasks. We introduce a unified and generic structure
called edit arborescence that relies on edit paths between data in a
collection, as well as the Min Edit Arborescence Problem, which asks for an
edit arborescence that minimizes the sum of costs of its inner edit paths.
Through the use of suitable cost functions, this generic framework allows to
model a variety of problems. In particular, we show that by introducing
encoding size preserving edit costs, it can be used as an efficient method for
compressing collections of labeled graphs. Experiments on various graph
datasets, with comparisons to standard compression tools, show the potential of
our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gnecco_L/0/1/0/all/0/1"&gt;Lucas Gnecco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boria_N/0/1/0/all/0/1"&gt;Nicolas Boria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bougleux_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Bougleux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yger_F/0/1/0/all/0/1"&gt;Florian Yger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blumenthal_D/0/1/0/all/0/1"&gt;David B. Blumenthal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out-of-Core Surface Reconstruction via Global $TGV$ Minimization. (arXiv:2107.14790v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14790</id>
        <link href="http://arxiv.org/abs/2107.14790"/>
        <updated>2021-08-02T01:58:23.865Z</updated>
        <summary type="html"><![CDATA[We present an out-of-core variational approach for surface reconstruction
from a set of aligned depth maps. Input depth maps are supposed to be
reconstructed from regular photos or/and can be a representation of terrestrial
LIDAR point clouds. Our approach is based on surface reconstruction via total
generalized variation minimization ($TGV$) because of its strong
visibility-based noise-filtering properties and GPU-friendliness. Our main
contribution is an out-of-core OpenCL-accelerated adaptation of this numerical
algorithm which can handle arbitrarily large real-world scenes with scale
diversity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Poliarnyi_N/0/1/0/all/0/1"&gt;Nikolai Poliarnyi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the importance of cross-task features for class-incremental learning. (arXiv:2106.11930v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11930</id>
        <link href="http://arxiv.org/abs/2106.11930"/>
        <updated>2021-08-02T01:58:23.859Z</updated>
        <summary type="html"><![CDATA[In class-incremental learning, an agent with limited resources needs to learn
a sequence of classification tasks, forming an ever growing classification
problem, with the constraint of not being able to access data from previous
tasks. The main difference with task-incremental learning, where a task-ID is
available at inference time, is that the learner also needs to perform
cross-task discrimination, i.e. distinguish between classes that have not been
seen together. Approaches to tackle this problem are numerous and mostly make
use of an external memory (buffer) of non-negligible size. In this paper, we
ablate the learning of cross-task features and study its influence on the
performance of basic replay strategies used for class-IL. We also define a new
forgetting measure for class-incremental learning, and see that forgetting is
not the principal cause of low performance. Our experimental results show that
future algorithms for class-incremental learning should not only prevent
forgetting, but also aim to improve the quality of the cross-task features, and
the knowledge transfer between tasks. This is especially important when tasks
contain limited amount of data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soutif__Cormerais_A/0/1/0/all/0/1"&gt;Albin Soutif--Cormerais&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masana_M/0/1/0/all/0/1"&gt;Marc Masana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1"&gt;Joost Van de Weijer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Twardowski_B/0/1/0/all/0/1"&gt;Bart&amp;#x142;omiej Twardowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Resolution Depth Maps Based on TOF-Stereo Fusion. (arXiv:2107.14688v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14688</id>
        <link href="http://arxiv.org/abs/2107.14688"/>
        <updated>2021-08-02T01:58:23.851Z</updated>
        <summary type="html"><![CDATA[The combination of range sensors with color cameras can be very useful for
robot navigation, semantic perception, manipulation, and telepresence. Several
methods of combining range- and color-data have been investigated and
successfully used in various robotic applications. Most of these systems suffer
from the problems of noise in the range-data and resolution mismatch between
the range sensor and the color cameras, since the resolution of current range
sensors is much less than the resolution of color cameras. High-resolution
depth maps can be obtained using stereo matching, but this often fails to
construct accurate depth maps of weakly/repetitively textured scenes, or if the
scene exhibits complex self-occlusions. Range sensors provide coarse depth
information regardless of presence/absence of texture. The use of a calibrated
system, composed of a time-of-flight (TOF) camera and of a stereoscopic camera
pair, allows data fusion thus overcoming the weaknesses of both individual
sensors. We propose a novel TOF-stereo fusion method based on an efficient
seed-growing algorithm which uses the TOF data projected onto the stereo image
pair as an initial set of correspondences. These initial "seeds" are then
propagated based on a Bayesian model which combines an image similarity score
with rough depth priors computed from the low-resolution range data. The
overall result is a dense and accurate depth map at the resolution of the color
cameras at hand. We show that the proposed algorithm outperforms 2D image-based
stereo algorithms and that the results are of higher resolution than
off-the-shelf color-range sensors, e.g., Kinect. Moreover, the algorithm
potentially exhibits real-time performance on a single CPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gandhi_V/0/1/0/all/0/1"&gt;Vineet Gandhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cech_J/0/1/0/all/0/1"&gt;Jan Cech&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horaud_R/0/1/0/all/0/1"&gt;Radu Horaud&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Attention Networks for Low-Level Vision-and-Language Navigation. (arXiv:1911.12377v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.12377</id>
        <link href="http://arxiv.org/abs/1911.12377"/>
        <updated>2021-08-02T01:58:23.845Z</updated>
        <summary type="html"><![CDATA[Vision-and-Language Navigation (VLN) is a challenging task in which an agent
needs to follow a language-specified path to reach a target destination. The
goal gets even harder as the actions available to the agent get simpler and
move towards low-level, atomic interactions with the environment. This setting
takes the name of low-level VLN. In this paper, we strive for the creation of
an agent able to tackle three key issues: multi-modality, long-term
dependencies, and adaptability towards different locomotive settings. To that
end, we devise "Perceive, Transform, and Act" (PTA): a fully-attentive VLN
architecture that leaves the recurrent approach behind and the first
Transformer-like architecture incorporating three different modalities -
natural language, images, and low-level actions for the agent control. In
particular, we adopt an early fusion strategy to merge lingual and visual
information efficiently in our encoder. We then propose to refine the decoding
phase with a late fusion extension between the agent's history of actions and
the perceptual modalities. We experimentally validate our model on two
datasets: PTA achieves promising results in low-level VLN on R2R and achieves
good performance in the recently proposed R4R benchmark. Our code is publicly
available at https://github.com/aimagelab/perceive-transform-and-act.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Landi_F/0/1/0/all/0/1"&gt;Federico Landi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1"&gt;Lorenzo Baraldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1"&gt;Marcella Cornia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Corsini_M/0/1/0/all/0/1"&gt;Massimiliano Corsini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1"&gt;Rita Cucchiara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SNE-RoadSeg+: Rethinking Depth-Normal Translation and Deep Supervision for Freespace Detection. (arXiv:2107.14599v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14599</id>
        <link href="http://arxiv.org/abs/2107.14599"/>
        <updated>2021-08-02T01:58:23.822Z</updated>
        <summary type="html"><![CDATA[Freespace detection is a fundamental component of autonomous driving
perception. Recently, deep convolutional neural networks (DCNNs) have achieved
impressive performance for this task. In particular, SNE-RoadSeg, our
previously proposed method based on a surface normal estimator (SNE) and a
data-fusion DCNN (RoadSeg), has achieved impressive performance in freespace
detection. However, SNE-RoadSeg is computationally intensive, and it is
difficult to execute in real time. To address this problem, we introduce
SNE-RoadSeg+, an upgraded version of SNE-RoadSeg. SNE-RoadSeg+ consists of 1)
SNE+, a module for more accurate surface normal estimation, and 2) RoadSeg+, a
data-fusion DCNN that can greatly minimize the trade-off between accuracy and
efficiency with the use of deep supervision. Extensive experimental results
have demonstrated the effectiveness of our SNE+ for surface normal estimation
and the superior performance of our SNE-RoadSeg+ over all other freespace
detection approaches. Specifically, our SNE-RoadSeg+ runs in real time, and
meanwhile, achieves the state-of-the-art performance on the KITTI road
benchmark. Our project page is at
https://www.sne-roadseg.site/sne-roadseg-plus.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hengli Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1"&gt;Rui Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1"&gt;Peide Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Ming Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relightable Neural Video Portrait. (arXiv:2107.14735v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14735</id>
        <link href="http://arxiv.org/abs/2107.14735"/>
        <updated>2021-08-02T01:58:23.815Z</updated>
        <summary type="html"><![CDATA[Photo-realistic facial video portrait reenactment benefits virtual production
and numerous VR/AR experiences. The task remains challenging as the portrait
should maintain high realism and consistency with the target environment. In
this paper, we present a relightable neural video portrait, a simultaneous
relighting and reenactment scheme that transfers the head pose and facial
expressions from a source actor to a portrait video of a target actor with
arbitrary new backgrounds and lighting conditions. Our approach combines 4D
reflectance field learning, model-based facial performance capture and
target-aware neural rendering. Specifically, we adopt a rendering-to-video
translation network to first synthesize high-quality OLAT imagesets and alpha
mattes from hybrid facial performance capture results. We then design a
semantic-aware facial normalization scheme to enable reliable explicit control
as well as a multi-frame multi-task learning strategy to encode content,
segmentation and temporal information simultaneously for high-quality
reflectance field inference. After training, our approach further enables
photo-realistic and controllable video portrait editing of the target
performer. Reliable face poses and expression editing is obtained by applying
the same hybrid facial capture and normalization scheme to the source video
input, while our explicit alpha and OLAT output enable high-quality relit and
background editing. With the ability to achieve simultaneous relighting and
reenactment, we are able to improve the realism in a variety of virtual
production and video rewrite applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Youjia Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1"&gt;Taotao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Minzhang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1"&gt;Teng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1"&gt;Minye Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jingyi Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video Based Fall Detection Using Human Poses. (arXiv:2107.14633v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14633</id>
        <link href="http://arxiv.org/abs/2107.14633"/>
        <updated>2021-08-02T01:58:23.807Z</updated>
        <summary type="html"><![CDATA[Video based fall detection accuracy has been largely improved due to the
recent progress on deep convolutional neural networks. However, there still
exists some challenges, such as lighting variation, complex background, which
degrade the accuracy and generalization ability of these approaches. Meanwhile,
large computation cost limits the application of existing fall detection
approaches. To alleviate these problems, a video based fall detection approach
using human poses is proposed in this paper. First, a lightweight pose
estimator extracts 2D poses from video sequences and then 2D poses are lifted
to 3D poses. Second, we introduce a robust fall detection network to recognize
fall events using estimated 3D poses, which increases respective filed and
maintains low computation cost by dilated convolutions. The experimental
results show that the proposed fall detection approach achieves a high accuracy
of 99.83% on large benchmark action recognition dataset NTU RGB+D and real-time
performance of 18 FPS on a non-GPU platform and 63 FPS on a GPU platform.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Ziwei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yiye Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wankou Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Iterative, Deep, and Unsupervised Synthetic Aperture Sonar Image Segmentation. (arXiv:2107.14563v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14563</id>
        <link href="http://arxiv.org/abs/2107.14563"/>
        <updated>2021-08-02T01:58:23.801Z</updated>
        <summary type="html"><![CDATA[Deep learning has not been routinely employed for semantic segmentation of
seabed environment for synthetic aperture sonar (SAS) imagery due to the
implicit need of abundant training data such methods necessitate. Abundant
training data, specifically pixel-level labels for all images, is usually not
available for SAS imagery due to the complex logistics (e.g., diver survey,
chase boat, precision position information) needed for obtaining accurate
ground-truth. Many hand-crafted feature based algorithms have been proposed to
segment SAS in an unsupervised fashion. However, there is still room for
improvement as the feature extraction step of these methods is fixed. In this
work, we present a new iterative unsupervised algorithm for learning deep
features for SAS image segmentation. Our proposed algorithm alternates between
clustering superpixels and updating the parameters of a convolutional neural
network (CNN) so that the feature extraction for image segmentation can be
optimized. We demonstrate the efficacy of our method on a realistic benchmark
dataset. Our results show that the performance of our proposed method is
considerably better than current state-of-the-art methods in SAS image
segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yung-Chen Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gerg_I/0/1/0/all/0/1"&gt;Isaac D. Gerg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Monga_V/0/1/0/all/0/1"&gt;Vishal Monga&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recognizing Emotions evoked by Movies using Multitask Learning. (arXiv:2107.14529v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14529</id>
        <link href="http://arxiv.org/abs/2107.14529"/>
        <updated>2021-08-02T01:58:23.794Z</updated>
        <summary type="html"><![CDATA[Understanding the emotional impact of movies has become important for
affective movie analysis, ranking, and indexing. Methods for recognizing evoked
emotions are usually trained on human annotated data. Concretely, viewers watch
video clips and have to manually annotate the emotions they experienced while
watching the videos. Then, the common practice is to aggregate the different
annotations, by computing average scores or majority voting, and train and test
models on these aggregated annotations. With this procedure a single aggregated
evoked emotion annotation is obtained per each video. However, emotions
experienced while watching a video are subjective: different individuals might
experience different emotions. In this paper, we model the emotions evoked by
videos in a different manner: instead of modeling the aggregated value we
jointly model the emotions experienced by each viewer and the aggregated value
using a multi-task learning approach. Concretely, we propose two deep learning
architectures: a Single-Task (ST) architecture and a Multi-Task (MT)
architecture. Our results show that the MT approach can more accurately model
each viewer and the aggregated annotation when compared to methods that are
directly trained on the aggregated annotations. Furthermore, our approach
outperforms the current state-of-the-art results on the COGNIMUSE benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hayat_H/0/1/0/all/0/1"&gt;Hassan Hayat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ventura_C/0/1/0/all/0/1"&gt;Carles Ventura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapedriza_A/0/1/0/all/0/1"&gt;Agata Lapedriza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shadow Art Revisited: A Differentiable Rendering Based Approach. (arXiv:2107.14539v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14539</id>
        <link href="http://arxiv.org/abs/2107.14539"/>
        <updated>2021-08-02T01:58:23.778Z</updated>
        <summary type="html"><![CDATA[While recent learning based methods have been observed to be superior for
several vision-related applications, their potential in generating artistic
effects has not been explored much. One such interesting application is Shadow
Art - a unique form of sculptural art where 2D shadows cast by a 3D sculpture
produce artistic effects. In this work, we revisit shadow art using
differentiable rendering based optimization frameworks to obtain the 3D
sculpture from a set of shadow (binary) images and their corresponding
projection information. Specifically, we discuss shape optimization through
voxel as well as mesh-based differentiable renderers. Our choice of using
differentiable rendering for generating shadow art sculptures can be attributed
to its ability to learn the underlying 3D geometry solely from image data, thus
reducing the dependence on 3D ground truth. The qualitative and quantitative
results demonstrate the potential of the proposed framework in generating
complex 3D sculptures that go beyond those seen in contemporary art pieces
using just a set of shadow images as input. Further, we demonstrate the
generation of 3D sculptures to cast shadows of faces, animated movie
characters, and applicability of the framework to sketch-based 3D
reconstruction of underlying shapes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sadekar_K/0/1/0/all/0/1"&gt;Kaustubh Sadekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tiwari_A/0/1/0/all/0/1"&gt;Ashish Tiwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raman_S/0/1/0/all/0/1"&gt;Shanmuganathan Raman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Medical Instrument Segmentation in 3D US by Hybrid Constrained Semi-Supervised Learning. (arXiv:2107.14476v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14476</id>
        <link href="http://arxiv.org/abs/2107.14476"/>
        <updated>2021-08-02T01:58:23.770Z</updated>
        <summary type="html"><![CDATA[Medical instrument segmentation in 3D ultrasound is essential for
image-guided intervention. However, to train a successful deep neural network
for instrument segmentation, a large number of labeled images are required,
which is expensive and time-consuming to obtain. In this article, we propose a
semi-supervised learning (SSL) framework for instrument segmentation in 3D US,
which requires much less annotation effort than the existing methods. To
achieve the SSL learning, a Dual-UNet is proposed to segment the instrument.
The Dual-UNet leverages unlabeled data using a novel hybrid loss function,
consisting of uncertainty and contextual constraints. Specifically, the
uncertainty constraints leverage the uncertainty estimation of the predictions
of the UNet, and therefore improve the unlabeled information for SSL training.
In addition, contextual constraints exploit the contextual information of the
training images, which are used as the complementary information for voxel-wise
uncertainty estimation. Extensive experiments on multiple ex-vivo and in-vivo
datasets show that our proposed method achieves Dice score of about 68.6%-69.1%
and the inference time of about 1 sec. per volume. These results are better
than the state-of-the-art SSL methods and the inference time is comparable to
the supervised approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_C/0/1/0/all/0/1"&gt;Caifeng Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouwman_R/0/1/0/all/0/1"&gt;R. Arthur Bouwman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dekker_L/0/1/0/all/0/1"&gt;Lukas R. C. Dekker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolen_A/0/1/0/all/0/1"&gt;Alexander F. Kolen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+With_P/0/1/0/all/0/1"&gt;Peter H. N. de With&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synth-by-Reg (SbR): Contrastive learning for synthesis-based registration of paired images. (arXiv:2107.14449v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14449</id>
        <link href="http://arxiv.org/abs/2107.14449"/>
        <updated>2021-08-02T01:58:23.762Z</updated>
        <summary type="html"><![CDATA[Nonlinear inter-modality registration is often challenging due to the lack of
objective functions that are good proxies for alignment. Here we propose a
synthesis-by-registration method to convert this problem into an easier
intra-modality task. We introduce a registration loss for weakly supervised
image translation between domains that does not require perfectly aligned
training data. This loss capitalises on a registration U-Net with frozen
weights, to drive a synthesis CNN towards the desired translation. We
complement this loss with a structure preserving constraint based on
contrastive learning, which prevents blurring and content shifts due to
overfitting. We apply this method to the registration of histological sections
to MRI slices, a key step in 3D histology reconstruction. Results on two
different public datasets show improvements over registration based on mutual
information (13% reduction in landmark error) and synthesis-based algorithms
such as CycleGAN (11% reduction), and are comparable to a registration CNN with
label supervision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Casamitjana_A/0/1/0/all/0/1"&gt;Adri&amp;#xe0; Casamitjana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mancini_M/0/1/0/all/0/1"&gt;Matteo Mancini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iglesias_J/0/1/0/all/0/1"&gt;Juan Eugenio Iglesias&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Instant Visual Odometry Initialization for Mobile AR. (arXiv:2107.14659v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14659</id>
        <link href="http://arxiv.org/abs/2107.14659"/>
        <updated>2021-08-02T01:58:23.752Z</updated>
        <summary type="html"><![CDATA[Mobile AR applications benefit from fast initialization to display
world-locked effects instantly. However, standard visual odometry or SLAM
algorithms require motion parallax to initialize (see Figure 1) and, therefore,
suffer from delayed initialization. In this paper, we present a 6-DoF monocular
visual odometry that initializes instantly and without motion parallax. Our
main contribution is a pose estimator that decouples estimating the 5-DoF
relative rotation and translation direction from the 1-DoF translation
magnitude. While scale is not observable in a monocular vision-only setting, it
is still paramount to estimate a consistent scale over the whole trajectory
(even if not physically accurate) to avoid AR effects moving erroneously along
depth. In our approach, we leverage the fact that depth errors are not
perceivable to the user during rotation-only motion. However, as the user
starts translating the device, depth becomes perceivable and so does the
capability to estimate consistent scale. Our proposed algorithm naturally
transitions between these two modes. We perform extensive validations of our
contributions with both a publicly available dataset and synthetic data. We
show that the proposed pose estimator outperforms the classical approaches for
6-DoF pose estimation used in the literature in low-parallax configurations. We
release a dataset for the relative pose problem using real data to facilitate
the comparison with future solutions for the relative pose problem. Our
solution is either used as a full odometry or as a preSLAM component of any
supported SLAM system (ARKit, ARCore) in world-locked AR effects on platforms
such as Instagram and Facebook.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Concha_A/0/1/0/all/0/1"&gt;Alejo Concha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burri_M/0/1/0/all/0/1"&gt;Michael Burri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Briales_J/0/1/0/all/0/1"&gt;Jes&amp;#xfa;s Briales&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forster_C/0/1/0/all/0/1"&gt;Christian Forster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oth_L/0/1/0/all/0/1"&gt;Luc Oth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Vocabulary and Graph Verification for Accurate Loop Closure Detection. (arXiv:2107.14611v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14611</id>
        <link href="http://arxiv.org/abs/2107.14611"/>
        <updated>2021-08-02T01:58:23.746Z</updated>
        <summary type="html"><![CDATA[Localizing pre-visited places during long-term simultaneous localization and
mapping, i.e. loop closure detection (LCD), is a crucial technique to correct
accumulated inconsistencies. As one of the most effective and efficient
solutions, Bag-of-Words (BoW) builds a visual vocabulary to associate features
and then detect loops. Most existing approaches that build vocabularies
off-line determine scales of the vocabulary by trial-and-error, which often
results in unreasonable feature association. Moreover, the accuracy of the
algorithm usually declines due to perceptual aliasing, as the BoW-based method
ignores the positions of visual features. To overcome these disadvantages, we
propose a natural convergence criterion based on the comparison between the
radii of nodes and the drifts of feature descriptors, which is then utilized to
build the optimal vocabulary automatically. Furthermore, we present a novel
topological graph verification method for validating candidate loops so that
geometrical positions of the words can be involved with a negligible increase
in complexity, which can significantly improve the accuracy of LCD. Experiments
on various public datasets and comparisons against several state-of-the-art
algorithms verify the performance of our proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yue_H/0/1/0/all/0/1"&gt;Haosong Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_J/0/1/0/all/0/1"&gt;Jinyu Miao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weihai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1"&gt;Fanghong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhengguo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Product1M: Towards Weakly Supervised Instance-Level Product Retrieval via Cross-modal Pretraining. (arXiv:2107.14572v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14572</id>
        <link href="http://arxiv.org/abs/2107.14572"/>
        <updated>2021-08-02T01:58:23.738Z</updated>
        <summary type="html"><![CDATA[Nowadays, customer's demands for E-commerce are more diversified, which
introduces more complications to the product retrieval industry. Previous
methods are either subject to single-modal input or perform supervised
image-level product retrieval, thus fail to accommodate real-life scenarios
where enormous weakly annotated multi-modal data are present. In this paper, we
investigate a more realistic setting that aims to perform weakly-supervised
multi-modal instance-level product retrieval among fine-grained product
categories. To promote the study of this challenging task, we contribute
Product1M, one of the largest multi-modal cosmetic datasets for real-world
instance-level retrieval. Notably, Product1M contains over 1 million
image-caption pairs and consists of two sample types, i.e., single-product and
multi-product samples, which encompass a wide variety of cosmetics brands. In
addition to the great diversity, Product1M enjoys several appealing
characteristics including fine-grained categories, complex combinations, and
fuzzy correspondence that well mimic the real-world scenes. Moreover, we
propose a novel model named Cross-modal contrAstive Product Transformer for
instance-level prodUct REtrieval (CAPTURE), that excels in capturing the
potential synergy between multi-modal inputs via a hybrid-stream transformer in
a self-supervised manner.CAPTURE generates discriminative instance features via
masked multi-modal learning as well as cross-modal contrastive pretraining and
it outperforms several SOTA cross-modal baselines. Extensive ablation studies
well demonstrate the effectiveness and the generalization capacity of our
model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1"&gt;Xunlin Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yangxin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xiao Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yunchao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1"&gt;Minlong Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yichi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DPT: Deformable Patch-based Transformer for Visual Recognition. (arXiv:2107.14467v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14467</id>
        <link href="http://arxiv.org/abs/2107.14467"/>
        <updated>2021-08-02T01:58:23.720Z</updated>
        <summary type="html"><![CDATA[Transformer has achieved great success in computer vision, while how to split
patches in an image remains a problem. Existing methods usually use a
fixed-size patch embedding which might destroy the semantics of objects. To
address this problem, we propose a new Deformable Patch (DePatch) module which
learns to adaptively split the images into patches with different positions and
scales in a data-driven way rather than using predefined fixed patches. In this
way, our method can well preserve the semantics in patches. The DePatch module
can work as a plug-and-play module, which can easily be incorporated into
different transformers to achieve an end-to-end training. We term this
DePatch-embedded transformer as Deformable Patch-based Transformer (DPT) and
conduct extensive evaluations of DPT on image classification and object
detection. Results show DPT can achieve 81.9% top-1 accuracy on ImageNet
classification, and 43.7% box mAP with RetinaNet, 44.3% with Mask R-CNN on
MSCOCO object detection. Code has been made available at:
https://github.com/CASIA-IVA-Lab/DPT .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhiyang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yousong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1"&gt;Chaoyang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1"&gt;Guosheng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wei Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jinqiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1"&gt;Ming Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topological Similarity Index and Loss Function for Blood Vessel Segmentation. (arXiv:2107.14531v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.14531</id>
        <link href="http://arxiv.org/abs/2107.14531"/>
        <updated>2021-08-02T01:58:23.713Z</updated>
        <summary type="html"><![CDATA[Blood vessel segmentation is one of the most studied topics in computer
vision, due to its relevance in daily clinical practice. Despite the evolution
the field has been facing, especially after the dawn of deep learning,
important challenges are still not solved. One of them concerns the consistency
of the topological properties of the vascular trees, given that the best
performing methodologies do not directly penalize mistakes such as broken
segments and end up producing predictions with disconnected trees. This is
particularly relevant in graph-like structures, such as blood vessel trees,
given that it puts at risk the characterization steps that follow the
segmentation task. In this paper, we propose a similarity index which captures
the topological consistency of the predicted segmentations having as reference
the ground truth. We also design a novel loss function based on the
morphological closing operator and show how it allows to learn deep neural
network models which produce more topologically coherent masks. Our experiments
target well known retinal benchmarks and a coronary angiogram database.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Araujo_R/0/1/0/all/0/1"&gt;R. J. Ara&amp;#xfa;jo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cardoso_J/0/1/0/all/0/1"&gt;J. S. Cardoso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oliveira_H/0/1/0/all/0/1"&gt;H. P. Oliveira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single image deep defocus estimation and its applications. (arXiv:2107.14443v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.14443</id>
        <link href="http://arxiv.org/abs/2107.14443"/>
        <updated>2021-08-02T01:58:23.692Z</updated>
        <summary type="html"><![CDATA[The depth information is useful in many image processing applications.
However, since taking a picture is a process of projection of a 3D scene onto a
2D imaging sensor, the depth information is embedded in the image. Extracting
the depth information from the image is a challenging task. A guiding principle
is that the level of blurriness due to defocus is related to the distance
between the object and the focal plane. Based on this principle and the widely
used assumption that Gaussian blur is a good model for defocus blur, we
formulate the problem of estimating the spatially varying defocus blurriness as
a Gaussian blur classification problem. We solved the problem by training a
deep neural network to classify image patches into one of the 20 levels of
blurriness. We have created a dataset of more than 500000 image patches of size
32x32 which are used to train and test several well-known network models. We
find that MobileNetV2 is suitable for this application due to its low memory
requirement and high accuracy. The trained model is used to determine the patch
blurriness which is then refined by applying an iterative weighted guided
filter. The result is a defocus map that carries the information of the degree
of blurriness for each pixel. We compare the proposed method with
state-of-the-art techniques and we demonstrate its successful applications in
adaptive image enhancement, defocus magnification, and multi-focus image
fusion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Galetto_F/0/1/0/all/0/1"&gt;Fernando J. Galetto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Deng_G/0/1/0/all/0/1"&gt;Guang Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpenForensics: Large-Scale Challenging Dataset For Multi-Face Forgery Detection And Segmentation In-The-Wild. (arXiv:2107.14480v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14480</id>
        <link href="http://arxiv.org/abs/2107.14480"/>
        <updated>2021-08-02T01:58:23.652Z</updated>
        <summary type="html"><![CDATA[The proliferation of deepfake media is raising concerns among the public and
relevant authorities. It has become essential to develop countermeasures
against forged faces in social media. This paper presents a comprehensive study
on two new countermeasure tasks: multi-face forgery detection and segmentation
in-the-wild. Localizing forged faces among multiple human faces in unrestricted
natural scenes is far more challenging than the traditional deepfake
recognition task. To promote these new tasks, we have created the first
large-scale dataset posing a high level of challenges that is designed with
face-wise rich annotations explicitly for face forgery detection and
segmentation, namely OpenForensics. With its rich annotations, our
OpenForensics dataset has great potentials for research in both deepfake
prevention and general human face detection. We have also developed a suite of
benchmarks for these tasks by conducting an extensive evaluation of
state-of-the-art instance detection and segmentation methods on our newly
constructed dataset in various scenarios. The dataset, benchmark results,
codes, and supplementary materials will be publicly available on our project
page: https://sites.google.com/view/ltnghia/research/openforensics]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1"&gt;Trung-Nghia Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Huy H. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamagishi_J/0/1/0/all/0/1"&gt;Junichi Yamagishi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Echizen_I/0/1/0/all/0/1"&gt;Isao Echizen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time Streaming Perception System for Autonomous Driving. (arXiv:2107.14388v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14388</id>
        <link href="http://arxiv.org/abs/2107.14388"/>
        <updated>2021-08-02T01:58:23.644Z</updated>
        <summary type="html"><![CDATA[Nowadays, plenty of deep learning technologies are being applied to all
aspects of autonomous driving with promising results. Among them, object
detection is the key to improve the ability of an autonomous agent to perceive
its environment so that it can (re)act. However, previous vision-based object
detectors cannot achieve satisfactory performance under real-time driving
scenarios. To remedy this, we present the real-time steaming perception system
in this paper, which is also the 2nd Place solution of Streaming Perception
Challenge (Workshop on Autonomous Driving at CVPR 2021) for the detection-only
track. Unlike traditional object detection challenges, which focus mainly on
the absolute performance, streaming perception task requires achieving a
balance of accuracy and latency, which is crucial for real-time autonomous
driving. We adopt YOLOv5 as our basic framework, data augmentation,
Bag-of-Freebies, and Transformer are adopted to improve streaming object
detection performance with negligible extra inference cost. On the Argoverse-HD
test set, our method achieves 33.2 streaming AP (34.6 streaming AP verified by
the organizer) under the required hardware. Its performance significantly
surpasses the fixed baseline of 13.6 (host team), demonstrating the
potentiality of application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yongxiang Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qianlei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1"&gt;Xiaolin Qin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Social Relation Inference with Concise Interaction Graph and Discriminative Scene Representation. (arXiv:2107.14425v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14425</id>
        <link href="http://arxiv.org/abs/2107.14425"/>
        <updated>2021-08-02T01:58:23.636Z</updated>
        <summary type="html"><![CDATA[There has been a recent surge of research interest in attacking the problem
of social relation inference based on images. Existing works classify social
relations mainly by creating complicated graphs of human interactions, or
learning the foreground and/or background information of persons and objects,
but ignore holistic scene context. The holistic scene refers to the
functionality of a place in images, such as dinning room, playground and
office. In this paper, by mimicking human understanding on images, we propose
an approach of \textbf{PR}actical \textbf{I}nference in \textbf{S}ocial
r\textbf{E}lation (PRISE), which concisely learns interactive features of
persons and discriminative features of holistic scenes. Technically, we develop
a simple and fast relational graph convolutional network to capture interactive
features of all persons in one image. To learn the holistic scene feature, we
elaborately design a contrastive learning task based on image scene
classification. To further boost the performance in social relation inference,
we collect and distribute a new large-scale dataset, which consists of about
240 thousand unlabeled images. The extensive experimental results show that our
novel learning framework significantly beats the state-of-the-art methods,
e.g., PRISE achieves 6.8$\%$ improvement for domain classification in PIPA
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xiaotian Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_H/0/1/0/all/0/1"&gt;Hanling Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yi Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1"&gt;Ling Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shiliang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaoyu Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fourier Series Expansion Based Filter Parametrization for Equivariant Convolutions. (arXiv:2107.14519v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14519</id>
        <link href="http://arxiv.org/abs/2107.14519"/>
        <updated>2021-08-02T01:58:23.629Z</updated>
        <summary type="html"><![CDATA[It has been shown that equivariant convolution is very helpful for many types
of computer vision tasks. Recently, the 2D filter parametrization technique
plays an important role when designing equivariant convolutions. However, the
current filter parametrization method still has its evident drawbacks, where
the most critical one lies in the accuracy problem of filter representation.
Against this issue, in this paper we modify the classical Fourier series
expansion for 2D filters, and propose a new set of atomic basis functions for
filter parametrization. The proposed filter parametrization method not only
finely represents 2D filters with zero error when the filter is not rotated,
but also substantially alleviates the fence-effect-caused quality degradation
when the filter is rotated. Accordingly, we construct a new equivariant
convolution method based on the proposed filter parametrization method, named
F-Conv. We prove that the equivariance of the proposed F-Conv is exact in the
continuous domain, which becomes approximate only after discretization.
Extensive experiments show the superiority of the proposed method.
Particularly, we adopt rotation equivariant convolution methods to image
super-resolution task, and F-Conv evidently outperforms previous filter
parametrization based method in this task, reflecting its intrinsic capability
of faithfully preserving rotation symmetries in local image features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1"&gt;Qi Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qian Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zongben Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1"&gt;Deyu Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pix2Point: Learning Outdoor 3D Using Sparse Point Clouds and Optimal Transport. (arXiv:2107.14498v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14498</id>
        <link href="http://arxiv.org/abs/2107.14498"/>
        <updated>2021-08-02T01:58:23.621Z</updated>
        <summary type="html"><![CDATA[Good quality reconstruction and comprehension of a scene rely on 3D
estimation methods. The 3D information was usually obtained from images by
stereo-photogrammetry, but deep learning has recently provided us with
excellent results for monocular depth estimation. Building up a sufficiently
large and rich training dataset to achieve these results requires onerous
processing. In this paper, we address the problem of learning outdoor 3D point
cloud from monocular data using a sparse ground-truth dataset. We propose
Pix2Point, a deep learning-based approach for monocular 3D point cloud
prediction, able to deal with complete and challenging outdoor scenes. Our
method relies on a 2D-3D hybrid neural network architecture, and a supervised
end-to-end minimisation of an optimal transport divergence between point
clouds. We show that, when trained on sparse point clouds, our simple promising
approach achieves a better coverage of 3D outdoor scenes than efficient
monocular depth methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leroy_R/0/1/0/all/0/1"&gt;R&amp;#xe9;my Leroy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trouve_Peloux_P/0/1/0/all/0/1"&gt;Pauline Trouv&amp;#xe9;-Peloux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Champagnat_F/0/1/0/all/0/1"&gt;Fr&amp;#xe9;d&amp;#xe9;ric Champagnat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saux_B/0/1/0/all/0/1"&gt;Bertrand Le Saux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carvalho_M/0/1/0/all/0/1"&gt;Marcela Carvalho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection. (arXiv:2107.14391v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14391</id>
        <link href="http://arxiv.org/abs/2107.14391"/>
        <updated>2021-08-02T01:58:23.612Z</updated>
        <summary type="html"><![CDATA[As an emerging data modal with precise distance sensing, LiDAR point clouds
have been placed great expectations on 3D scene understanding. However, point
clouds are always sparsely distributed in the 3D space, and with unstructured
storage, which makes it difficult to represent them for effective 3D object
detection. To this end, in this work, we regard point clouds as hollow-3D data
and propose a new architecture, namely Hallucinated Hollow-3D R-CNN
($\text{H}^2$3D R-CNN), to address the problem of 3D object detection. In our
approach, we first extract the multi-view features by sequentially projecting
the point clouds into the perspective view and the bird-eye view. Then, we
hallucinate the 3D representation by a novel bilaterally guided multi-view
fusion block. Finally, the 3D objects are detected via a box refinement module
with a novel Hierarchical Voxel RoI Pooling operation. The proposed
$\text{H}^2$3D R-CNN provides a new angle to take full advantage of
complementary information in the perspective view and the bird-eye view with an
efficient framework. We evaluate our approach on the public KITTI Dataset and
Waymo Open Dataset. Extensive experiments demonstrate the superiority of our
method over the state-of-the-art algorithms with respect to both effectiveness
and efficiency. The code will be made available at
\url{https://github.com/djiajunustc/H-23D_R-CNN}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Jiajun Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wengang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yanyong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Houqiang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Neural Representational Decoders for High-Resolution Semantic Segmentation. (arXiv:2107.14428v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14428</id>
        <link href="http://arxiv.org/abs/2107.14428"/>
        <updated>2021-08-02T01:58:23.605Z</updated>
        <summary type="html"><![CDATA[Semantic segmentation requires per-pixel prediction for a given image.
Typically, the output resolution of a segmentation network is severely reduced
due to the downsampling operations in the CNN backbone. Most previous methods
employ upsampling decoders to recover the spatial resolution. Various decoders
were designed in the literature. Here, we propose a novel decoder, termed
dynamic neural representational decoder (NRD), which is simple yet
significantly more efficient. As each location on the encoder's output
corresponds to a local patch of the semantic labels, in this work, we represent
these local patches of labels with compact neural networks. This neural
representation enables our decoder to leverage the smoothness prior in the
semantic label space, and thus makes our decoder more efficient. Furthermore,
these neural representations are dynamically generated and conditioned on the
outputs of the encoder networks. The desired semantic labels can be efficiently
decoded from the neural representations, resulting in high-resolution semantic
segmentation predictions. We empirically show that our proposed decoder can
outperform the decoder in DeeplabV3+ with only 30% computational complexity,
and achieve competitive performance with the methods using dilated encoders
with only 15% computation. Experiments on the Cityscapes, ADE20K, and PASCAL
Context datasets demonstrate the effectiveness and efficiency of our proposed
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bowen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yifan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1"&gt;Zhi Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chunhua Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DarkLighter: Light Up the Darkness for UAV Tracking. (arXiv:2107.14389v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14389</id>
        <link href="http://arxiv.org/abs/2107.14389"/>
        <updated>2021-08-02T01:58:23.597Z</updated>
        <summary type="html"><![CDATA[Recent years have witnessed the fast evolution and promising performance of
the convolutional neural network (CNN)-based trackers, which aim at imitating
biological visual systems. However, current CNN-based trackers can hardly
generalize well to low-light scenes that are commonly lacked in the existing
training set. In indistinguishable night scenarios frequently encountered in
unmanned aerial vehicle (UAV) tracking-based applications, the robustness of
the state-of-the-art (SOTA) trackers drops significantly. To facilitate aerial
tracking in the dark through a general fashion, this work proposes a low-light
image enhancer namely DarkLighter, which dedicates to alleviate the impact of
poor illumination and noise iteratively. A lightweight map estimation network,
i.e., ME-Net, is trained to efficiently estimate illumination maps and noise
maps jointly. Experiments are conducted with several SOTA trackers on numerous
UAV dark tracking scenes. Exhaustive evaluations demonstrate the reliability
and universality of DarkLighter, with high efficiency. Moreover, DarkLighter
has further been implemented on a typical UAV system. Real-world tests at night
scenes have verified its practicability and dependability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Junjie Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Changhong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1"&gt;Guangze Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1"&gt;Ziang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bowen Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Multi-Stain Registration of Whole Slide Images in Histopathology. (arXiv:2107.14292v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.14292</id>
        <link href="http://arxiv.org/abs/2107.14292"/>
        <updated>2021-08-02T01:58:23.581Z</updated>
        <summary type="html"><![CDATA[Joint analysis of multiple biomarker images and tissue morphology is
important for disease diagnosis, treatment planning and drug development. It
requires cross-staining comparison among Whole Slide Images (WSIs) of
immuno-histochemical and hematoxylin and eosin (H&E) microscopic slides.
However, automatic, and fast cross-staining alignment of enormous gigapixel
WSIs at single-cell precision is challenging. In addition to morphological
deformations introduced during slide preparation, there are large variations in
cell appearance and tissue morphology across different staining. In this paper,
we propose a two-step automatic feature-based cross-staining WSI alignment to
assist localization of even tiny metastatic foci in the assessment of lymph
node. Image pairs were aligned allowing for translation, rotation, and scaling.
The registration was performed automatically by first detecting landmarks in
both images, using the scale-invariant image transform (SIFT), followed by the
fast sample consensus (FSC) protocol for finding point correspondences and
finally aligned the images. The Registration results were evaluated using both
visual and quantitative criteria using the Jaccard index. The average Jaccard
similarity index of the results produced by the proposed system is 0.942 when
compared with the manual registration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shafique_A/0/1/0/all/0/1"&gt;Abubakr Shafique&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/eess/1/au:+Babaie_M/0/1/0/all/0/1"&gt;Morteza Babaie&lt;/a&gt; (1 and 3), &lt;a href="http://arxiv.org/find/eess/1/au:+Sajadi_M/0/1/0/all/0/1"&gt;Mahjabin Sajadi&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/eess/1/au:+Batten_A/0/1/0/all/0/1"&gt;Adrian Batten&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/eess/1/au:+Skdar_S/0/1/0/all/0/1"&gt;Soma Skdar&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/eess/1/au:+Tizhoosh_H/0/1/0/all/0/1"&gt;H.R. Tizhoosh&lt;/a&gt; (1 and 3) ((1) Kimia Lab, University of Waterloo, Waterloo, ON, Canada., (2) Department of Pathology, Grand River Hospital, Kitchener, ON, Canada., and (3) Vector Institute, MaRS Centre, Toronto, Canada.)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ManiSkill: Learning-from-Demonstrations Benchmark for Generalizable Manipulation Skills. (arXiv:2107.14483v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14483</id>
        <link href="http://arxiv.org/abs/2107.14483"/>
        <updated>2021-08-02T01:58:23.571Z</updated>
        <summary type="html"><![CDATA[Learning generalizable manipulation skills is central for robots to achieve
task automation in environments with endless scene and object variations.
However, existing robot learning environments are limited in both scale and
diversity of 3D assets (especially of articulated objects), making it difficult
to train and evaluate the generalization ability of agents over novel objects.
In this work, we focus on object-level generalization and propose SAPIEN
Manipulation Skill Benchmark (abbreviated as ManiSkill), a large-scale
learning-from-demonstrations benchmark for articulated object manipulation with
visual input (point cloud and image). ManiSkill supports object-level
variations by utilizing a rich and diverse set of articulated objects, and each
task is carefully designed for learning manipulations on a single category of
objects. We equip ManiSkill with high-quality demonstrations to facilitate
learning-from-demonstrations approaches and perform evaluations on common
baseline algorithms. We believe ManiSkill can encourage the robot learning
community to explore more on learning generalizable object manipulation skills.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1"&gt;Tongzhou Mu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1"&gt;Zhan Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_F/0/1/0/all/0/1"&gt;Fanbo Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1"&gt;Derek Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuanlin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1"&gt;Stone Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhiao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1"&gt;Zhiwei Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hao Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[T-SVDNet: Exploring High-Order Prototypical Correlations for Multi-Source Domain Adaptation. (arXiv:2107.14447v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14447</id>
        <link href="http://arxiv.org/abs/2107.14447"/>
        <updated>2021-08-02T01:58:23.561Z</updated>
        <summary type="html"><![CDATA[Most existing domain adaptation methods focus on adaptation from only one
source domain, however, in practice there are a number of relevant sources that
could be leveraged to help improve performance on target domain. We propose a
novel approach named T-SVDNet to address the task of Multi-source Domain
Adaptation (MDA), which is featured by incorporating Tensor Singular Value
Decomposition (T-SVD) into a neural network's training pipeline. Overall,
high-order correlations among multiple domains and categories are fully
explored so as to better bridge the domain gap. Specifically, we impose
Tensor-Low-Rank (TLR) constraint on a tensor obtained by stacking up a group of
prototypical similarity matrices, aiming at capturing consistent data structure
across different domains. Furthermore, to avoid negative transfer brought by
noisy source data, we propose a novel uncertainty-aware weighting strategy to
adaptively assign weights to different source domains and samples based on the
result of uncertainty estimation. Extensive experiments conducted on public
benchmarks demonstrate the superiority of our model in addressing the task of
MDA compared to state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruihuang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xu Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jianzhong He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shuaijun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1"&gt;Qinghua Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERT Embeddings for Automatic Readability Assessment. (arXiv:2106.07935v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07935</id>
        <link href="http://arxiv.org/abs/2106.07935"/>
        <updated>2021-08-02T01:58:23.538Z</updated>
        <summary type="html"><![CDATA[Automatic readability assessment (ARA) is the task of evaluating the level of
ease or difficulty of text documents for a target audience. For researchers,
one of the many open problems in the field is to make such models trained for
the task show efficacy even for low-resource languages. In this study, we
propose an alternative way of utilizing the information-rich embeddings of BERT
models with handcrafted linguistic features through a combined method for
readability assessment. Results show that the proposed method outperforms
classical approaches in readability assessment using English and Filipino
datasets, obtaining as high as 12.4% increase in F1 performance. We also show
that the general information encoded in BERT embeddings can be used as a
substitute feature set for low-resource languages like Filipino with limited
semantic and syntactic NLP tools to explicitly extract feature values for the
task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Imperial_J/0/1/0/all/0/1"&gt;Joseph Marvin Imperial&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ADeLA: Automatic Dense Labeling with Attention for Viewpoint Adaptation in Semantic Segmentation. (arXiv:2107.14285v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14285</id>
        <link href="http://arxiv.org/abs/2107.14285"/>
        <updated>2021-08-02T01:58:23.521Z</updated>
        <summary type="html"><![CDATA[We describe an unsupervised domain adaptation method for image content shift
caused by viewpoint changes for a semantic segmentation task. Most existing
methods perform domain alignment in a shared space and assume that the mapping
from the aligned space to the output is transferable. However, the novel
content induced by viewpoint changes may nullify such a space for effective
alignments, thus resulting in negative adaptation. Our method works without
aligning any statistics of the images between the two domains. Instead, it
utilizes a view transformation network trained only on color images to
hallucinate the semantic images for the target. Despite the lack of
supervision, the view transformation network can still generalize to semantic
images thanks to the inductive bias introduced by the attention mechanism.
Furthermore, to resolve ambiguities in converting the semantic images to
semantic labels, we treat the view transformation network as a functional
representation of an unknown mapping implied by the color images and propose
functional label hallucination to generate pseudo-labels in the target domain.
Our method surpasses baselines built on state-of-the-art correspondence
estimation and view synthesis methods. Moreover, it outperforms the
state-of-the-art unsupervised domain adaptation methods that utilize
self-training and adversarial domain alignment. Our code and dataset will be
made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yanchao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1"&gt;Hanxiang Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;He Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1"&gt;Bokui Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1"&gt;Qingnan Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Youyi Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;C. Karen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1"&gt;Leonidas Guibas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-19 sentiment analysis via deep learning during the rise of novel cases. (arXiv:2104.10662v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10662</id>
        <link href="http://arxiv.org/abs/2104.10662"/>
        <updated>2021-08-02T01:58:23.508Z</updated>
        <summary type="html"><![CDATA[Social scientists and psychologists take interest in understanding how people
express emotions and sentiments when dealing with catastrophic events such as
natural disasters, political unrest, and terrorism. The COVID-19 pandemic is a
catastrophic event that has raised a number of psychological issues such as
depression given abrupt social changes and lack of employment. Advancements of
deep learning-based language models have been promising for sentiment analysis
with data from social networks such as Twitter. Given the situation with
COVID-19 pandemic, different countries had different peaks where the rise and
fall of new cases affected lock-downs which directly affected the economy and
employment. During the rise of COVID-19 cases with stricter lock-downs, people
have been expressing their sentiments in social media. This can provide a deep
understanding of human psychology during catastrophic events. In this paper, we
present a framework that employs deep learning-based language models via long
short-term memory (LSTM) recurrent neural networks for sentiment analysis
during the rise of novel COVID-19 cases in India. The framework features LSTM
language model with a global vector embedding and state-of-art BERT language
model. We review the sentiments expressed for selective months in 2020 which
covers the first major peak of novel cases in India. Our framework utilises
multi-label sentiment classification where more than one sentiment can be
expressed at once. Our results indicate that the majority of the tweets have
been positive with high levels of optimism during the rise of the novel
COVID-19 cases and the number of tweets significantly lowered towards the peak.
The predictions generally indicate that although the majority have been
optimistic, a significant group of population has been annoyed towards the way
the pandemic was handled by the authorities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1"&gt;Rohitash Chandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishna_A/0/1/0/all/0/1"&gt;Aswin Krishna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PiBase: An IoT-based Security System using Raspberry Pi and Google Firebase. (arXiv:2107.14325v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.14325</id>
        <link href="http://arxiv.org/abs/2107.14325"/>
        <updated>2021-08-02T01:58:23.489Z</updated>
        <summary type="html"><![CDATA[Smart environments are environments where digital devices are connected to
each other over the Internet and operate in sync. Security is of paramount
importance in such environments. This paper addresses aspects of authorized
access and intruder detection for smart environments. Proposed is PiBase, an
Internet of Things (IoT)-based app that aids in detecting intruders and
providing security. The hardware for the application consists of a Raspberry
Pi, a PIR motion sensor to detect motion from infrared radiation in the
environment, an Android mobile phone and a camera. The software for the
application is written in Java, Python and NodeJS. The PIR sensor and Pi camera
module connected to the Raspberry Pi aid in detecting human intrusion. Machine
learning algorithms, namely Haar-feature based cascade classifiers and Linear
Binary Pattern Histograms (LBPH), are used for face detection and face
recognition, respectively. The app lets the user create a list of non-intruders
and anyone that is not on the list is identified as an intruder. The app alerts
the user only in the event of an intrusion by using the Google Firebase Cloud
Messaging service to trigger a notification to the app. The user may choose to
add the detected intruder to the list of non-intruders through the app to avoid
further detections as intruder. Face detection by the Haar Cascade algorithm
yields a recall of 94.6%. Thus, the system is both highly effective and
relatively low cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Margapuri_V/0/1/0/all/0/1"&gt;Venkat Margapuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Penumajji_N/0/1/0/all/0/1"&gt;Niketa Penumajji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neilsen_M/0/1/0/all/0/1"&gt;Mitchell Neilsen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Functionals in the Clouds: An abstract architecture of serverless Cloud-Native Apps. (arXiv:2105.10362v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10362</id>
        <link href="http://arxiv.org/abs/2105.10362"/>
        <updated>2021-08-02T01:58:23.482Z</updated>
        <summary type="html"><![CDATA[Cloud Native Application CNApp (as a distributed system) is a collection of
independent components (micro-services) interacting via communication
protocols. This gives rise to present an abstract architecture of CNApp as
dynamically re-configurable acyclic directed multi graph where vertices are
microservices, and edges are the protocols. Generic mechanisms for such
reconfigurations evidently correspond to higher-level functions (functionals).
This implies also internal abstract architecture of microservice as a
collection of event-triggered serverless functions (including functions
implementing the protocols) that are dynamically composed into event-dependent
data-flow graphs. Again, generic mechanisms for such compositions correspond to
calculus of functionals and relations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ambroszkiewicz_S/0/1/0/all/0/1"&gt;Stanislaw Ambroszkiewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bartyna_W/0/1/0/all/0/1"&gt;Waldemar Bartyna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bylka_S/0/1/0/all/0/1"&gt;Stanislaw Bylka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Regional and Temporal Auxiliary Tasks for Facial Action Unit Recognition. (arXiv:2107.14399v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14399</id>
        <link href="http://arxiv.org/abs/2107.14399"/>
        <updated>2021-08-02T01:58:23.463Z</updated>
        <summary type="html"><![CDATA[Automatic facial action unit (AU) recognition is a challenging task due to
the scarcity of manual annotations. To alleviate this problem, a large amount
of efforts has been dedicated to exploiting various methods which leverage
numerous unlabeled data. However, many aspects with regard to some unique
properties of AUs, such as the regional and relational characteristics, are not
sufficiently explored in previous works. Motivated by this, we take the AU
properties into consideration and propose two auxiliary AU related tasks to
bridge the gap between limited annotations and the model performance in a
self-supervised manner via the unlabeled data. Specifically, to enhance the
discrimination of regional features with AU relation embedding, we design a
task of RoI inpainting to recover the randomly cropped AU patches. Meanwhile, a
single image based optical flow estimation task is proposed to leverage the
dynamic change of facial muscles and encode the motion information into the
global feature representation. Based on these two self-supervised auxiliary
tasks, local features, mutual relation and motion cues of AUs are better
captured in the backbone network with the proposed regional and temporal based
auxiliary task learning (RTATL) framework. Extensive experiments on BP4D and
DISFA demonstrate the superiority of our method and new state-of-the-art
performances are achieved.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Jingwei Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jingjing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chunmao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1"&gt;Shiliang Pu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-Time Anchor-Free Single-Stage 3D Detection with IoU-Awareness. (arXiv:2107.14342v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14342</id>
        <link href="http://arxiv.org/abs/2107.14342"/>
        <updated>2021-08-02T01:58:23.457Z</updated>
        <summary type="html"><![CDATA[In this report, we introduce our winning solution to the Real-time 3D
Detection and also the "Most Efficient Model" in the Waymo Open Dataset
Challenges at CVPR 2021. Extended from our last year's award-winning model
AFDet, we have made a handful of modifications to the base model, to improve
the accuracy and at the same time to greatly reduce the latency. The modified
model, named as AFDetV2, is featured with a lite 3D Feature Extractor, an
improved RPN with extended receptive field and an added sub-head that produces
an IoU-aware confidence score. These model enhancements, together with enriched
data augmentation, stochastic weights averaging, and a GPU-based implementation
of voxelization, lead to a winning accuracy of 73.12 mAPH/L2 for our AFDetV2
with a latency of 60.06 ms, and an accuracy of 72.57 mAPH/L2 for our
AFDetV2-base, entitled as the "Most Efficient Model" by the challenge sponsor,
with a winning latency of 55.86 ms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1"&gt;Runzhou Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1"&gt;Zhuangzhuang Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yihan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1"&gt;Wenxin Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Li Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Kun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qiang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards robust vision by multi-task learning on monkey visual cortex. (arXiv:2107.14344v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14344</id>
        <link href="http://arxiv.org/abs/2107.14344"/>
        <updated>2021-08-02T01:58:23.450Z</updated>
        <summary type="html"><![CDATA[Deep neural networks set the state-of-the-art across many tasks in computer
vision, but their generalization ability to image distortions is surprisingly
fragile. In contrast, the mammalian visual system is robust to a wide range of
perturbations. Recent work suggests that this generalization ability can be
explained by useful inductive biases encoded in the representations of visual
stimuli throughout the visual cortex. Here, we successfully leveraged these
inductive biases with a multi-task learning approach: we jointly trained a deep
network to perform image classification and to predict neural activity in
macaque primary visual cortex (V1). We measured the out-of-distribution
generalization abilities of our network by testing its robustness to image
distortions. We found that co-training on monkey V1 data leads to increased
robustness despite the absence of those distortions during training.
Additionally, we showed that our network's robustness is very close to that of
an Oracle network where parts of the architecture are directly trained on noisy
images. Our results also demonstrated that the network's representations become
more brain-like as their robustness improves. Using a novel constrained
reconstruction analysis, we investigated what makes our brain-regularized
network more robust. We found that our co-trained network is more sensitive to
content than noise when compared to a Baseline network that we trained for
image classification alone. Using DeepGaze-predicted saliency maps for ImageNet
images, we found that our monkey co-trained network tends to be more sensitive
to salient regions in a scene, reminiscent of existing theories on the role of
V1 in the detection of object borders and bottom-up saliency. Overall, our work
expands the promising research avenue of transferring inductive biases from the
brain, and provides a novel analysis of the effects of our transfer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Safarani_S/0/1/0/all/0/1"&gt;Shahd Safarani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nix_A/0/1/0/all/0/1"&gt;Arne Nix&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Willeke_K/0/1/0/all/0/1"&gt;Konstantin Willeke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cadena_S/0/1/0/all/0/1"&gt;Santiago A. Cadena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Restivo_K/0/1/0/all/0/1"&gt;Kelli Restivo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Denfield_G/0/1/0/all/0/1"&gt;George Denfield&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tolias_A/0/1/0/all/0/1"&gt;Andreas S. Tolias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sinz_F/0/1/0/all/0/1"&gt;Fabian H. Sinz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manipulating Identical Filter Redundancy for Efficient Pruning on Deep and Complicated CNN. (arXiv:2107.14444v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14444</id>
        <link href="http://arxiv.org/abs/2107.14444"/>
        <updated>2021-08-02T01:58:23.439Z</updated>
        <summary type="html"><![CDATA[The existence of redundancy in Convolutional Neural Networks (CNNs) enables
us to remove some filters/channels with acceptable performance drops. However,
the training objective of CNNs usually tends to minimize an accuracy-related
loss function without any attention paid to the redundancy, making the
redundancy distribute randomly on all the filters, such that removing any of
them may trigger information loss and accuracy drop, necessitating a following
finetuning step for recovery. In this paper, we propose to manipulate the
redundancy during training to facilitate network pruning. To this end, we
propose a novel Centripetal SGD (C-SGD) to make some filters identical,
resulting in ideal redundancy patterns, as such filters become purely redundant
due to their duplicates; hence removing them does not harm the network. As
shown on CIFAR and ImageNet, C-SGD delivers better performance because the
redundancy is better organized, compared to the existing methods. The
efficiency also characterizes C-SGD because it is as fast as regular SGD,
requires no finetuning, and can be conducted simultaneously on all the layers
even in very deep CNNs. Besides, C-SGD can improve the accuracy of CNNs by
first training a model with the same architecture but wider layers then
squeezing it into the original width.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1"&gt;Xiaohan Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_T/0/1/0/all/0/1"&gt;Tianxiang Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jungong Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yuchen Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1"&gt;Guiguang Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Low-light Object Detection Techniques. (arXiv:2107.14382v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14382</id>
        <link href="http://arxiv.org/abs/2107.14382"/>
        <updated>2021-08-02T01:58:23.426Z</updated>
        <summary type="html"><![CDATA[Images acquired by computer vision systems under low light conditions have
multiple characteristics like high noise, lousy illumination, reflectance, and
bad contrast, which make object detection tasks difficult. Much work has been
done to enhance images using various pixel manipulation techniques, as well as
deep neural networks - some focused on improving the illumination, while some
on reducing the noise. Similarly, considerable research has been done in object
detection neural network models. In our work, we break down the problem into
two phases: 1)First, we explore which image enhancement algorithm is more
suited for object detection tasks, where accurate feature retrieval is more
important than good image quality. Specifically, we look at basic histogram
equalization techniques and unpaired image translation techniques. 2)In the
second phase, we explore different object detection models that can be applied
to the enhanced image. We conclude by comparing all results, calculating mean
average precisions (mAP), and giving some directions for future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Winston Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_T/0/1/0/all/0/1"&gt;Tejas Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal Feature Warping for Video Shadow Detection. (arXiv:2107.14287v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14287</id>
        <link href="http://arxiv.org/abs/2107.14287"/>
        <updated>2021-08-02T01:58:23.418Z</updated>
        <summary type="html"><![CDATA[While single image shadow detection has been improving rapidly in recent
years, video shadow detection remains a challenging task due to data scarcity
and the difficulty in modelling temporal consistency. The current video shadow
detection method achieves this goal via co-attention, which mostly exploits
information that is temporally coherent but is not robust in detecting moving
shadows and small shadow regions. In this paper, we propose a simple but
powerful method to better aggregate information temporally. We use an optical
flow based warping module to align and then combine features between frames. We
apply this warping module across multiple deep-network layers to retrieve
information from neighboring frames including both local details and high-level
semantic information. We train and test our framework on the ViSha dataset.
Experimental results show that our model outperforms the state-of-the-art video
shadow detection method by 28%, reducing BER from 16.7 to 12.0.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shilin Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1"&gt;Hieu Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samaras_D/0/1/0/all/0/1"&gt;Dimitris Samaras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Show to Tell: A Survey on Image Captioning. (arXiv:2107.06912v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06912</id>
        <link href="http://arxiv.org/abs/2107.06912"/>
        <updated>2021-08-02T01:58:23.400Z</updated>
        <summary type="html"><![CDATA[Connecting Vision and Language plays an essential role in Generative
Intelligence. For this reason, large research efforts have been devoted to
image captioning, i.e. describing images with syntactically and semantically
meaningful sentences. Starting from 2015 the task has generally been addressed
with pipelines composed of a visual encoder and a language model for text
generation. During these years, both components have evolved considerably
through the exploitation of object regions, attributes, the introduction of
multi-modal connections, fully-attentive approaches, and BERT-like early-fusion
strategies. However, regardless of the impressive results, research in image
captioning has not reached a conclusive answer yet. This work aims at providing
a comprehensive overview of image captioning approaches, from visual encoding
and text generation to training strategies, datasets, and evaluation metrics.
In this respect, we quantitatively compare many relevant state-of-the-art
approaches to identify the most impactful technical innovations in
architectures and training strategies. Moreover, many variants of the problem
and its open challenges are discussed. The final goal of this work is to serve
as a tool for understanding the existing literature and highlighting the future
directions for a research area where Computer Vision and Natural Language
Processing can find an optimal synergy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stefanini_M/0/1/0/all/0/1"&gt;Matteo Stefanini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1"&gt;Marcella Cornia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1"&gt;Lorenzo Baraldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cascianelli_S/0/1/0/all/0/1"&gt;Silvia Cascianelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1"&gt;Giuseppe Fiameni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1"&gt;Rita Cucchiara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[O2D2: Out-Of-Distribution Detector to Capture Undecidable Trials in Authorship Verification. (arXiv:2106.15825v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15825</id>
        <link href="http://arxiv.org/abs/2106.15825"/>
        <updated>2021-08-02T01:58:23.394Z</updated>
        <summary type="html"><![CDATA[The PAN 2021 authorship verification (AV) challenge is part of a three-year
strategy, moving from a cross-topic/closed-set AV task to a
cross-topic/open-set AV task over a collection of fanfiction texts. In this
work, we present a novel hybrid neural-probabilistic framework that is designed
to tackle the challenges of the 2021 task. Our system is based on our 2020
winning submission, with updates to significantly reduce sensitivities to
topical variations and to further improve the system's calibration by means of
an uncertainty-adaptation layer. Our framework additionally includes an
out-of-distribution detector (O2D2) for defining non-responses. Our proposed
system outperformed all other systems that participated in the PAN 2021 AV
task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boenninghoff_B/0/1/0/all/0/1"&gt;Benedikt Boenninghoff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nickel_R/0/1/0/all/0/1"&gt;Robert M. Nickel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolossa_D/0/1/0/all/0/1"&gt;Dorothea Kolossa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UIT-ISE-NLP at SemEval-2021 Task 5: Toxic Spans Detection with BiLSTM-CRF and ToxicBERT Comment Classification. (arXiv:2104.10100v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10100</id>
        <link href="http://arxiv.org/abs/2104.10100"/>
        <updated>2021-08-02T01:58:23.371Z</updated>
        <summary type="html"><![CDATA[We present our works on SemEval-2021 Task 5 about Toxic Spans Detection. This
task aims to build a model for identifying toxic words in whole posts. We use
the BiLSTM-CRF model combining with ToxicBERT Classification to train the
detection model for identifying toxic words in posts. Our model achieves 62.23%
by F1-score on the Toxic Spans Detection task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1"&gt;Son T. Luu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1"&gt;Ngan Luu-Thuy Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dispatcher: A Message-Passing Approach To Language Modelling. (arXiv:2105.03994v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03994</id>
        <link href="http://arxiv.org/abs/2105.03994"/>
        <updated>2021-08-02T01:58:23.363Z</updated>
        <summary type="html"><![CDATA[This paper proposes a message-passing mechanism to address language
modelling. A new layer type is introduced that aims to substitute
self-attention for unidirectional sequence generation tasks. The system is
shown to be competitive with existing methods: Given N tokens, the
computational complexity is O(N logN) and the memory complexity is O(N) under
reasonable assumptions. In the end, the Dispatcher layer is seen to achieve
comparable perplexity to prior results while being more efficient.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cetoli_A/0/1/0/all/0/1"&gt;Alberto Cetoli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comprehensive Exploration of Pre-training Language Models. (arXiv:2106.11483v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11483</id>
        <link href="http://arxiv.org/abs/2106.11483"/>
        <updated>2021-08-02T01:58:23.356Z</updated>
        <summary type="html"><![CDATA[Recently, the development of pre-trained language models has brought natural
language processing (NLP) tasks to the new state-of-the-art. In this paper we
explore the efficiency of various pre-trained language models. We pre-train a
list of transformer-based models with the same amount of text and the same
training steps. The experimental results shows that the most improvement upon
the origin BERT is adding the RNN-layer to capture more contextual information
for the transformer-encoder layers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1"&gt;Tong Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Time-Aware Evidence Ranking for Fact-Checking. (arXiv:2009.06402v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06402</id>
        <link href="http://arxiv.org/abs/2009.06402"/>
        <updated>2021-08-02T01:58:23.320Z</updated>
        <summary type="html"><![CDATA[Truth can vary over time. Fact-checking decisions on claim veracity should
therefore take into account temporal information of both the claim and
supporting or refuting evidence. In this work, we investigate the hypothesis
that the timestamp of a Web page is crucial to how it should be ranked for a
given claim. We delineate four temporal ranking methods that constrain evidence
ranking differently and simulate hypothesis-specific evidence rankings given
the evidence timestamps as gold standard. Evidence ranking in three
fact-checking models is ultimately optimized using a learning-to-rank loss
function. Our study reveals that time-aware evidence ranking not only surpasses
relevance assumptions based purely on semantic similarity or position in a
search results list, but also improves veracity predictions of time-sensitive
claims in particular.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Allein_L/0/1/0/all/0/1"&gt;Liesbeth Allein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1"&gt;Isabelle Augenstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1"&gt;Marie-Francine Moens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ChrEnTranslate: Cherokee-English Machine Translation Demo with Quality Estimation and Corrective Feedback. (arXiv:2107.14800v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.14800</id>
        <link href="http://arxiv.org/abs/2107.14800"/>
        <updated>2021-08-02T01:58:23.313Z</updated>
        <summary type="html"><![CDATA[We introduce ChrEnTranslate, an online machine translation demonstration
system for translation between English and an endangered language Cherokee. It
supports both statistical and neural translation models as well as provides
quality estimation to inform users of reliability, two user feedback interfaces
for experts and common users respectively, example inputs to collect human
translations for monolingual data, word alignment visualization, and relevant
terms from the Cherokee-English dictionary. The quantitative evaluation
demonstrates that our backbone translation models achieve state-of-the-art
translation performance and our quality estimation well correlates with both
BLEU and human judgment. By analyzing 216 pieces of expert feedback, we find
that NMT is preferable because it copies less than SMT, and, in general,
current models can translate fragments of the source sentence but make major
mistakes. When we add these 216 expert-corrected parallel texts into the
training set and retrain models, equal or slightly better performance is
observed, which demonstrates indicates the potential of human-in-the-loop
learning. Our online demo is at https://chren.cs.unc.edu/; our code is
open-sourced at https://github.com/ZhangShiyue/ChrEnTranslate; and our data is
available at https://github.com/ZhangShiyue/ChrEn.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shiyue Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frey_B/0/1/0/all/0/1"&gt;Benjamin Frey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Perceiver IO: A General Architecture for Structured Inputs & Outputs. (arXiv:2107.14795v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14795</id>
        <link href="http://arxiv.org/abs/2107.14795"/>
        <updated>2021-08-02T01:58:23.302Z</updated>
        <summary type="html"><![CDATA[The recently-proposed Perceiver model obtains good results on several domains
(images, audio, multimodal, point clouds) while scaling linearly in compute and
memory with the input size. While the Perceiver supports many kinds of inputs,
it can only produce very simple outputs such as class scores. Perceiver IO
overcomes this limitation without sacrificing the original's appealing
properties by learning to flexibly query the model's latent space to produce
outputs of arbitrary size and semantics. Perceiver IO still decouples model
depth from data size and still scales linearly with data size, but now with
respect to both input and output sizes. The full Perceiver IO model achieves
strong results on tasks with highly structured output spaces, such as natural
language and visual understanding, StarCraft II, and multi-task and multi-modal
domains. As highlights, Perceiver IO matches a Transformer-based BERT baseline
on the GLUE language benchmark without the need for input tokenization and
achieves state-of-the-art performance on Sintel optical flow estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1"&gt;Andrew Jaegle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borgeaud_S/0/1/0/all/0/1"&gt;Sebastian Borgeaud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alayrac_J/0/1/0/all/0/1"&gt;Jean-Baptiste Alayrac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doersch_C/0/1/0/all/0/1"&gt;Carl Doersch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ionescu_C/0/1/0/all/0/1"&gt;Catalin Ionescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1"&gt;David Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koppula_S/0/1/0/all/0/1"&gt;Skanda Koppula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brock_A/0/1/0/all/0/1"&gt;Andrew Brock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shelhamer_E/0/1/0/all/0/1"&gt;Evan Shelhamer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henaff_O/0/1/0/all/0/1"&gt;Olivier H&amp;#xe9;naff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Botvinick_M/0/1/0/all/0/1"&gt;Matthew M. Botvinick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1"&gt;Andrew Zisserman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1"&gt;Oriol Vinyals&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Carreira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EmailSum: Abstractive Email Thread Summarization. (arXiv:2107.14691v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.14691</id>
        <link href="http://arxiv.org/abs/2107.14691"/>
        <updated>2021-08-02T01:58:23.296Z</updated>
        <summary type="html"><![CDATA[Recent years have brought about an interest in the challenging task of
summarizing conversation threads (meetings, online discussions, etc.). Such
summaries help analysis of the long text to quickly catch up with the decisions
made and thus improve our work or communication efficiency. To spur research in
thread summarization, we have developed an abstractive Email Thread
Summarization (EmailSum) dataset, which contains human-annotated short (<30
words) and long (<100 words) summaries of 2549 email threads (each containing 3
to 10 emails) over a wide variety of topics. We perform a comprehensive
empirical study to explore different summarization techniques (including
extractive and abstractive methods, single-document and hierarchical models, as
well as transfer and semisupervised learning) and conduct human evaluations on
both short and long summary generation tasks. Our results reveal the key
challenges of current abstractive summarization models in this task, such as
understanding the sender's intent and identifying the roles of sender and
receiver. Furthermore, we find that widely used automatic evaluation metrics
(ROUGE, BERTScore) are weakly correlated with human judgments on this email
thread summarization task. Hence, we emphasize the importance of human
evaluation and the development of better metrics by the community. Our code and
summary data have been made available at:
https://github.com/ZhangShiyue/EmailSum]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shiyue Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1"&gt;Asli Celikyilmaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An automated domain-independent text reading, interpreting and extracting approach for reviewing the scientific literature. (arXiv:2107.14638v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.14638</id>
        <link href="http://arxiv.org/abs/2107.14638"/>
        <updated>2021-08-02T01:58:23.266Z</updated>
        <summary type="html"><![CDATA[It is presented here a machine learning-based (ML) natural language
processing (NLP) approach capable to automatically recognize and extract
categorical and numerical parameters from a corpus of articles. The approach
(named a.RIX) operates with a concomitant/interchangeable use of ML models such
as neuron networks (NNs), latent semantic analysis (LSA) and naive-Bayes
classifiers (NBC), and a pattern recognition model using regular expression
(REGEX). To demonstrate the efficiency of the a.RIX engine, it was processed a
corpus of 7,873 scientific articles dealing with natural products (NPs). The
engine automatically extracts categorical and numerical parameters such as (i)
the plant species from which active molecules are extracted, (ii) the
microorganisms species for which active molecules can act against, and (iii)
the values of minimum inhibitory concentration (MIC) against these
microorganisms. The parameters are extracted without part-of-speech tagging
(POS) and named entity recognition (NER) approaches (i.e. without the need of
text annotation), and the models training is performed with unsupervised
approaches. In this way, a.RIX can be essentially used on articles from any
scientific field. Finally, it has a potential to make obsolete the currently
used articles reviewing process in some areas, specially those in which texts
structure, text semantics and latent knowledge is captured by machine learning
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paula_A/0/1/0/all/0/1"&gt;Amauri J Paula&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FINT: Field-aware INTeraction Neural Network For CTR Prediction. (arXiv:2107.01999v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01999</id>
        <link href="http://arxiv.org/abs/2107.01999"/>
        <updated>2021-08-02T01:58:23.232Z</updated>
        <summary type="html"><![CDATA[As a critical component for online advertising and marking, click-through
rate (CTR) prediction has draw lots of attentions from both industry and
academia field. Recently, the deep learning has become the mainstream
methodological choice for CTR. Despite of sustainable efforts have been made,
existing approaches still pose several challenges. On the one hand, high-order
interaction between the features is under-explored. On the other hand,
high-order interactions may neglect the semantic information from the low-order
fields. In this paper, we proposed a novel prediction method, named FINT, that
employs the Field-aware INTeraction layer which captures high-order feature
interactions while retaining the low-order field information. To empirically
investigate the effectiveness and robustness of the FINT, we perform extensive
experiments on the three realistic databases: KDD2012, Criteo and Avazu. The
obtained results demonstrate that the FINT can significantly improve the
performance compared to the existing methods, without increasing the amount of
computation required. Moreover, the proposed method brought about 2.72\%
increase to the advertising revenue of a big online video app through A/B
testing. To better promote the research in CTR field, we released our code as
well as reference implementation at: https://github.com/zhishan01/FINT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhishan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Sen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guohui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1"&gt;Dawei Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kele Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Attention Networks for Low-Level Vision-and-Language Navigation. (arXiv:1911.12377v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.12377</id>
        <link href="http://arxiv.org/abs/1911.12377"/>
        <updated>2021-08-02T01:58:23.225Z</updated>
        <summary type="html"><![CDATA[Vision-and-Language Navigation (VLN) is a challenging task in which an agent
needs to follow a language-specified path to reach a target destination. The
goal gets even harder as the actions available to the agent get simpler and
move towards low-level, atomic interactions with the environment. This setting
takes the name of low-level VLN. In this paper, we strive for the creation of
an agent able to tackle three key issues: multi-modality, long-term
dependencies, and adaptability towards different locomotive settings. To that
end, we devise "Perceive, Transform, and Act" (PTA): a fully-attentive VLN
architecture that leaves the recurrent approach behind and the first
Transformer-like architecture incorporating three different modalities -
natural language, images, and low-level actions for the agent control. In
particular, we adopt an early fusion strategy to merge lingual and visual
information efficiently in our encoder. We then propose to refine the decoding
phase with a late fusion extension between the agent's history of actions and
the perceptual modalities. We experimentally validate our model on two
datasets: PTA achieves promising results in low-level VLN on R2R and achieves
good performance in the recently proposed R4R benchmark. Our code is publicly
available at https://github.com/aimagelab/perceive-transform-and-act.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Landi_F/0/1/0/all/0/1"&gt;Federico Landi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1"&gt;Lorenzo Baraldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1"&gt;Marcella Cornia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Corsini_M/0/1/0/all/0/1"&gt;Massimiliano Corsini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1"&gt;Rita Cucchiara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM. (arXiv:2104.04473v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04473</id>
        <link href="http://arxiv.org/abs/2104.04473"/>
        <updated>2021-08-02T01:58:23.215Z</updated>
        <summary type="html"><![CDATA[Large language models have led to state-of-the-art accuracies across a range
of tasks. However, training these models efficiently is challenging for two
reasons: a) GPU memory capacity is limited, making it impossible to fit large
models on even a multi-GPU server; b) the number of compute operations required
to train these models can result in unrealistically long training times.
Consequently, new methods of model parallelism such as tensor and pipeline
parallelism have been proposed. Unfortunately, naive usage of these methods
leads to fundamental scaling issues at thousands of GPUs, e.g., due to
expensive cross-node communication or devices spending significant time waiting
on other devices to make progress.

In this paper, we show how different types of parallelism methods (tensor,
pipeline, and data parallelism) can be composed to scale to thousands of GPUs
and models with trillions of parameters. We survey techniques for pipeline
parallelism and propose a novel interleaved pipeline parallelism schedule that
can improve throughput by 10+% with memory footprint comparable to existing
approaches. We quantitatively study the trade-offs between tensor, pipeline,
and data parallelism, and provide intuition as to how to configure distributed
training of a large model. Our approach allows us to perform training
iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs
with achieved per-GPU throughput of 52% of theoretical peak. Our code is open
sourced at https://github.com/nvidia/megatron-lm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_D/0/1/0/all/0/1"&gt;Deepak Narayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1"&gt;Mohammad Shoeybi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casper_J/0/1/0/all/0/1"&gt;Jared Casper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+LeGresley_P/0/1/0/all/0/1"&gt;Patrick LeGresley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patwary_M/0/1/0/all/0/1"&gt;Mostofa Patwary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korthikanti_V/0/1/0/all/0/1"&gt;Vijay Anand Korthikanti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vainbrand_D/0/1/0/all/0/1"&gt;Dmitri Vainbrand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kashinkunti_P/0/1/0/all/0/1"&gt;Prethvi Kashinkunti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bernauer_J/0/1/0/all/0/1"&gt;Julie Bernauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1"&gt;Bryan Catanzaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phanishayee_A/0/1/0/all/0/1"&gt;Amar Phanishayee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1"&gt;Matei Zaharia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-stage Pre-training over Simplified Multimodal Pre-training Models. (arXiv:2107.14596v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.14596</id>
        <link href="http://arxiv.org/abs/2107.14596"/>
        <updated>2021-08-02T01:58:23.204Z</updated>
        <summary type="html"><![CDATA[Multimodal pre-training models, such as LXMERT, have achieved excellent
results in downstream tasks. However, current pre-trained models require large
amounts of training data and have huge model sizes, which make them difficult
to apply in low-resource situations. How to obtain similar or even better
performance than a larger model under the premise of less pre-training data and
smaller model size has become an important problem. In this paper, we propose a
new Multi-stage Pre-training (MSP) method, which uses information at different
granularities from word, phrase to sentence in both texts and images to
pre-train the model in stages. We also design several different pre-training
tasks suitable for the information granularity in different stage in order to
efficiently capture the diverse knowledge from a limited corpus. We take a
Simplified LXMERT (LXMERT- S), which has only 45.9% parameters of the original
LXMERT model and 11.76% of the original pre-training data as the testbed of our
MSP method. Experimental results show that our method achieves comparable
performance to the original LXMERT model in all downstream tasks, and even
outperforms the original model in Image-Text Retrieval task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongtong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1"&gt;Fangxiang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaojie Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WiC = TSV = WSD: On the Equivalence of Three Semantic Tasks. (arXiv:2107.14352v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.14352</id>
        <link href="http://arxiv.org/abs/2107.14352"/>
        <updated>2021-08-02T01:58:23.189Z</updated>
        <summary type="html"><![CDATA[The WiC task has attracted considerable attention in the NLP community, as
demonstrated by the popularity of the recent MCL-WiC SemEval task. WSD systems
and lexical resources have been used for the WiC task, as well as for WiC
dataset construction. TSV is another task related to both WiC and WSD. We aim
to establish the exact relationship between WiC, TSV, and WSD. We demonstrate
that these semantic classification problems can be pairwise reduced to each
other, and so they are theoretically equivalent. We analyze the existing WiC
datasets to validate this equivalence hypothesis. We conclude that our
understanding of semantic tasks can be increased through the applications of
tools from theoretical computer science. Our findings also suggests that more
efficient and simpler methods for one of these tasks could be successfully
applied in the other two.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hauer_B/0/1/0/all/0/1"&gt;Bradley Hauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kondrak_G/0/1/0/all/0/1"&gt;Grzegorz Kondrak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Universality in Multilingual Text Rewriting. (arXiv:2107.14749v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.14749</id>
        <link href="http://arxiv.org/abs/2107.14749"/>
        <updated>2021-08-02T01:58:23.174Z</updated>
        <summary type="html"><![CDATA[In this work, we take the first steps towards building a universal rewriter:
a model capable of rewriting text in any language to exhibit a wide variety of
attributes, including styles and languages, while preserving as much of the
original semantics as possible. In addition to obtaining state-of-the-art
results on unsupervised translation, we also demonstrate the ability to do
zero-shot sentiment transfer in non-English languages using only English
exemplars for sentiment. We then show that our model is able to modify multiple
attributes at once, for example adjusting both language and sentiment jointly.
Finally, we show that our model is capable of performing zero-shot
formality-sensitive translation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1"&gt;Xavier Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1"&gt;Noah Constant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Mandy Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1"&gt;Orhan Firat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text Classification and Clustering with Annealing Soft Nearest Neighbor Loss. (arXiv:2107.14597v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14597</id>
        <link href="http://arxiv.org/abs/2107.14597"/>
        <updated>2021-08-02T01:58:23.167Z</updated>
        <summary type="html"><![CDATA[We define disentanglement as how far class-different data points from each
other are, relative to the distances among class-similar data points. When
maximizing disentanglement during representation learning, we obtain a
transformed feature representation where the class memberships of the data
points are preserved. If the class memberships of the data points are
preserved, we would have a feature representation space in which a nearest
neighbour classifier or a clustering algorithm would perform well. We take
advantage of this method to learn better natural language representation, and
employ it on text classification and text clustering tasks. Through
disentanglement, we obtain text representations with better-defined clusters
and improve text classification performance. Our approach had a test
classification accuracy of as high as 90.11% and test clustering accuracy of
88% on the AG News dataset, outperforming our baseline models -- without any
other training tricks or regularization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agarap_A/0/1/0/all/0/1"&gt;Abien Fred Agarap&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Quantum-like Contextuality of Ambiguous Phrases. (arXiv:2107.14589v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.14589</id>
        <link href="http://arxiv.org/abs/2107.14589"/>
        <updated>2021-08-02T01:58:23.159Z</updated>
        <summary type="html"><![CDATA[Language is contextual as meanings of words are dependent on their contexts.
Contextuality is, concomitantly, a well-defined concept in quantum mechanics
where it is considered a major resource for quantum computations. We
investigate whether natural language exhibits any of the quantum mechanics'
contextual features. We show that meaning combinations in ambiguous phrases can
be modelled in the sheaf-theoretic framework for quantum contextuality, where
they can become possibilistically contextual. Using the framework of
Contextuality-by-Default (CbD), we explore the probabilistic variants of these
and show that CbD-contextuality is also possible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Daphne Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadrzadeh_M/0/1/0/all/0/1"&gt;Mehrnoosh Sadrzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abramsky_S/0/1/0/all/0/1"&gt;Samson Abramsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cervantes_V/0/1/0/all/0/1"&gt;Victor H. Cervantes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Difficulty-Aware Machine Translation Evaluation. (arXiv:2107.14402v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.14402</id>
        <link href="http://arxiv.org/abs/2107.14402"/>
        <updated>2021-08-02T01:58:23.149Z</updated>
        <summary type="html"><![CDATA[The high-quality translation results produced by machine translation (MT)
systems still pose a huge challenge for automatic evaluation. Current MT
evaluation pays the same attention to each sentence component, while the
questions of real-world examinations (e.g., university examinations) have
different difficulties and weightings. In this paper, we propose a novel
difficulty-aware MT evaluation metric, expanding the evaluation dimension by
taking translation difficulty into consideration. A translation that fails to
be predicted by most MT systems will be treated as a difficult one and assigned
a large weight in the final score function, and conversely. Experimental
results on the WMT19 English-German Metrics shared tasks show that our proposed
method outperforms commonly used MT metrics in terms of human correlation. In
particular, our proposed method performs well even when all the MT systems are
very competitive, which is when most existing metrics fail to distinguish
between them. The source code is freely available at
https://github.com/NLP2CT/Difficulty-Aware-MT-Evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_R/0/1/0/all/0/1"&gt;Runzhe Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xuebo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1"&gt;Derek F. Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1"&gt;Lidia S. Chao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Urdu & Hindi Poetry Generation using Neural Networks. (arXiv:2107.14587v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.14587</id>
        <link href="http://arxiv.org/abs/2107.14587"/>
        <updated>2021-08-02T01:58:23.142Z</updated>
        <summary type="html"><![CDATA[One of the major problems writers and poets face is the writer's block. It is
a condition in which an author loses the ability to produce new work or
experiences a creative slowdown. The problem is more difficult in the context
of poetry than prose, as in the latter case authors need not be very concise
while expressing their ideas, also the various aspects such as rhyme, poetic
meters are not relevant for prose. One of the most effective ways to overcome
this writing block for poets can be, to have a prompt system, which would help
their imagination and open their minds for new ideas. A prompt system can
possibly generate one liner, two liner or full ghazals. The purpose of this
work is to give an ode to the Urdu, Hindi poets, and helping them start their
next line of poetry, a couplet or a complete ghazal considering various factors
like rhymes, refrain, and meters. The result will help aspiring poets to get
new ideas and help them overcome writer's block by auto-generating pieces of
poetry using Deep Learning techniques. A concern with creative works like this,
especially in the literary context, is to ensure that the output is not
plagiarized. This work also addresses the concern and makes sure that the
resulting odes are not exact match with input data using parameters like
temperature and manual plagiarism check against input corpus. To the best of
our knowledge, although the automatic text generation problem has been studied
quite extensively in the literature, the specific problem of Urdu, Hindi poetry
generation has not been explored much. Apart from developing system to
auto-generate Urdu, Hindi poetry, another key contribution of our work is to
create a cleaned and preprocessed corpus of Urdu, Hindi poetry (derived from
authentic resources) and making it freely available for researchers in the
area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mukhtar_S/0/1/0/all/0/1"&gt;Shakeeb A. M. Mukhtar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joglekar_P/0/1/0/all/0/1"&gt;Pushkar S. Joglekar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring Disagreement in Science. (arXiv:2107.14641v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2107.14641</id>
        <link href="http://arxiv.org/abs/2107.14641"/>
        <updated>2021-08-02T01:58:23.132Z</updated>
        <summary type="html"><![CDATA[Disagreement is essential to scientific progress. However, the extent of
disagreement in science, its evolution over time, and the fields in which it
happens, remains largely unknown. Leveraging a massive collection of scientific
texts, we develop a cue-phrase based approach to identify instances of
disagreement citations across more than four million scientific articles. Using
this method, we construct an indicator of disagreement across scientific fields
over the 2000-2015 period. In contrast with black-box text classification
methods, our framework is transparent and easily interpretable. We reveal a
disciplinary spectrum of disagreement, with higher disagreement in the social
sciences and lower disagreement in physics and mathematics. However, detailed
disciplinary analysis demonstrates heterogeneity across sub-fields, revealing
the importance of local disciplinary cultures and epistemic characteristics of
disagreement. Paper-level analysis reveals notable episodes of disagreement in
science, and illustrates how methodological artefacts can confound analyses of
scientific texts. These findings contribute to a broader understanding of
disagreement and establish a foundation for future research to understanding
key processes underlying scientific progress.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lamers_W/0/1/0/all/0/1"&gt;Wout S. Lamers&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Boyack_K/0/1/0/all/0/1"&gt;Kevin Boyack&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Lariviere_V/0/1/0/all/0/1"&gt;Vincent Larivi&amp;#xe8;re&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Sugimoto_C/0/1/0/all/0/1"&gt;Cassidy R. Sugimoto&lt;/a&gt; (4), &lt;a href="http://arxiv.org/find/cs/1/au:+Eck_N/0/1/0/all/0/1"&gt;Nees Jan van Eck&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Waltman_L/0/1/0/all/0/1"&gt;Ludo Waltman&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Murray_D/0/1/0/all/0/1"&gt;Dakota Murray&lt;/a&gt; (4) ((1) Centre for Science and Technology Studies, Leiden University, Leiden, Netherlands, (2) SciTech Strategies, Inc., Albuquerque, NM, USA, (3) &amp;#xc9;cole de biblioth&amp;#xe9;conomie et des sciences de l&amp;#x27;information, Universit&amp;#xe9; de Montr&amp;#xe9;al, Canada, (4) School of Informatics, Computing, and Engineering, Indiana University Bloomington, IN, USA)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Variational Learning for Grounded Language Acquisition. (arXiv:2107.14593v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.14593</id>
        <link href="http://arxiv.org/abs/2107.14593"/>
        <updated>2021-08-02T01:58:23.110Z</updated>
        <summary type="html"><![CDATA[We propose a learning system in which language is grounded in visual percepts
without specific pre-defined categories of terms. We present a unified
generative method to acquire a shared semantic/visual embedding that enables
the learning of language about a wide range of real-world objects. We evaluate
the efficacy of this learning by predicting the semantics of objects and
comparing the performance with neural and non-neural inputs. We show that this
generative approach exhibits promising results in language grounding without
pre-specifying visual categories under low resource settings. Our experiments
demonstrate that this approach is generalizable to multilingual, highly varied
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pillai_N/0/1/0/all/0/1"&gt;Nisha Pillai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matuszek_C/0/1/0/all/0/1"&gt;Cynthia Matuszek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferraro_F/0/1/0/all/0/1"&gt;Francis Ferraro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Claim Review for Climate Science via Explanation Generation. (arXiv:2107.14740v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.14740</id>
        <link href="http://arxiv.org/abs/2107.14740"/>
        <updated>2021-08-02T01:58:23.097Z</updated>
        <summary type="html"><![CDATA[There is unison is the scientific community about human induced climate
change. Despite this, we see the web awash with claims around climate change
scepticism, thus driving the need for fact checking them but at the same time
providing an explanation and justification for the fact check. Scientists and
experts have been trying to address it by providing manually written feedback
for these claims. In this paper, we try to aid them by automating generating
explanation for a predicted veracity label for a claim by deploying the
approach used in open domain question answering of a fusion in decoder
augmented with retrieved supporting passages from an external knowledge. We
experiment with different knowledge sources, retrievers, retriever depths and
demonstrate that even a small number of high quality manually written
explanations can help us in generating good explanations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1"&gt;Shraey Bhatia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1"&gt;Jey Han Lau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1"&gt;Timothy Baldwin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IIITG-ADBU@HASOC-Dravidian-CodeMix-FIRE2020: Offensive Content Detection in Code-Mixed Dravidian Text. (arXiv:2107.14336v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.14336</id>
        <link href="http://arxiv.org/abs/2107.14336"/>
        <updated>2021-08-02T01:58:23.087Z</updated>
        <summary type="html"><![CDATA[This paper presents the results obtained by our SVM and XLM-RoBERTa based
classifiers in the shared task Dravidian-CodeMix-HASOC 2020. The SVM classifier
trained using TF-IDF features of character and word n-grams performed the best
on the code-mixed Malayalam text. It obtained a weighted F1 score of 0.95 (1st
Rank) and 0.76 (3rd Rank) on the YouTube and Twitter dataset respectively. The
XLM-RoBERTa based classifier performed the best on the code-mixed Tamil text.
It obtained a weighted F1 score of 0.87 (3rd Rank) on the code-mixed Tamil
Twitter dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baruah_A/0/1/0/all/0/1"&gt;Arup Baruah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_K/0/1/0/all/0/1"&gt;Kaushik Amar Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barbhuiya_F/0/1/0/all/0/1"&gt;Ferdous Ahmed Barbhuiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dey_K/0/1/0/all/0/1"&gt;Kuntal Dey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Adversarial Networks for Spatio-temporal Data: A Survey. (arXiv:2008.08903v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.08903</id>
        <link href="http://arxiv.org/abs/2008.08903"/>
        <updated>2021-08-02T01:58:23.075Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) have shown remarkable success in
producing realistic-looking images in the computer vision area. Recently,
GAN-based techniques are shown to be promising for spatio-temporal-based
applications such as trajectory prediction, events generation and time-series
data imputation. While several reviews for GANs in computer vision have been
presented, no one has considered addressing the practical applications and
challenges relevant to spatio-temporal data. In this paper, we have conducted a
comprehensive review of the recent developments of GANs for spatio-temporal
data. We summarise the application of popular GAN architectures for
spatio-temporal data and the common practices for evaluating the performance of
spatio-temporal applications with GANs. Finally, we point out future research
directions to benefit researchers in this area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1"&gt;Nan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1"&gt;Hao Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1"&gt;Wei Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Sichen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_K/0/1/0/all/0/1"&gt;Kyle Kai Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prabowo_A/0/1/0/all/0/1"&gt;Arian Prabowo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1"&gt;Mohammad Saiedur Rahaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1"&gt;Flora D. Salim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Tree Aggregation of Layers for Neural Machine Translation. (arXiv:2107.14590v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.14590</id>
        <link href="http://arxiv.org/abs/2107.14590"/>
        <updated>2021-08-02T01:58:23.066Z</updated>
        <summary type="html"><![CDATA[Although attention-based Neural Machine Translation has achieved remarkable
progress in recent layers, it still suffers from issue of making insufficient
use of the output of each layer. In transformer, it only uses the top layer of
encoder and decoder in the subsequent process, which makes it impossible to
take advantage of the useful information in other layers. To address this
issue, we propose a residual tree aggregation of layers for Transformer(RTAL),
which helps to fuse information across layers. Specifically, we try to fuse the
information across layers by constructing a post-order binary tree. In
additional to the last node, we add the residual connection to the process of
generating child nodes. Our model is based on the Neural Machine Translation
model Transformer and we conduct our experiments on WMT14 English-to-German and
WMT17 English-to-France translation tasks. Experimental results across language
pairs show that the proposed approach outperforms the strong baseline model
significantly]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;GuoLiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yiyang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervision for health insurance claims data: a Covid-19 use case. (arXiv:2107.14591v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.14591</id>
        <link href="http://arxiv.org/abs/2107.14591"/>
        <updated>2021-08-02T01:58:23.044Z</updated>
        <summary type="html"><![CDATA[In this work, we modify and apply self-supervision techniques to the domain
of medical health insurance claims. We model patients' healthcare claims
history analogous to free-text narratives, and introduce pre-trained `prior
knowledge', later utilized for patient outcome predictions on a challenging
task: predicting Covid-19 hospitalization, given a patient's pre-Covid-19
insurance claims history. Results suggest that pre-training on insurance claims
not only produces better prediction performance, but, more importantly,
improves the model's `clinical trustworthiness' and model
stability/reliability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Apostolova_E/0/1/0/all/0/1"&gt;Emilia Apostolova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karim_F/0/1/0/all/0/1"&gt;Fazle Karim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muscioni_G/0/1/0/all/0/1"&gt;Guido Muscioni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rana_A/0/1/0/all/0/1"&gt;Anubhav Rana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clyman_J/0/1/0/all/0/1"&gt;Jeffrey Clyman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MDQE: A More Accurate Direct Pretraining for Machine Translation Quality Estimation. (arXiv:2107.14600v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.14600</id>
        <link href="http://arxiv.org/abs/2107.14600"/>
        <updated>2021-08-02T01:58:23.031Z</updated>
        <summary type="html"><![CDATA[It is expensive to evaluate the results of Machine Translation(MT), which
usually requires manual translation as a reference. Machine Translation Quality
Estimation (QE) is a task of predicting the quality of machine translations
without relying on any reference. Recently, the emergence of
predictor-estimator framework which trains the predictor as a feature extractor
and estimator as a QE predictor, and pre-trained language models(PLM) have
achieved promising QE performance. However, we argue that there are still gaps
between the predictor and the estimator in both data quality and training
objectives, which preclude QE models from benefiting from a large number of
parallel corpora more directly. Based on previous related work that have
alleviated gaps to some extent, we propose a novel framework that provides a
more accurate direct pretraining for QE tasks. In this framework, a generator
is trained to produce pseudo data that is closer to the real QE data, and a
estimator is pretrained on these data with novel objectives that are the same
as the QE task. Experiments on widely used benchmarks show that our proposed
framework outperforms existing methods, without using any pretraining models
such as BERT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1"&gt;Lei Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient DP-SGD Mechanism for Large Scale NLP Models. (arXiv:2107.14586v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.14586</id>
        <link href="http://arxiv.org/abs/2107.14586"/>
        <updated>2021-08-02T01:58:23.017Z</updated>
        <summary type="html"><![CDATA[Recent advances in deep learning have drastically improved performance on
many Natural Language Understanding (NLU) tasks. However, the data used to
train NLU models may contain private information such as addresses or phone
numbers, particularly when drawn from human subjects. It is desirable that
underlying models do not expose private information contained in the training
data. Differentially Private Stochastic Gradient Descent (DP-SGD) has been
proposed as a mechanism to build privacy-preserving models. However, DP-SGD can
be prohibitively slow to train. In this work, we propose a more efficient
DP-SGD for training using a GPU infrastructure and apply it to fine-tuning
models based on LSTM and transformer architectures. We report faster training
times, alongside accuracy, theoretical privacy guarantees and success of
Membership inference attacks for our models and observe that fine-tuning with
proposed variant of DP-SGD can yield competitive models without significant
degradation in training time and improvement in privacy protection. We also
make observations such as looser theoretical $\epsilon, \delta$ can translate
into significant practical privacy gains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dupuy_C/0/1/0/all/0/1"&gt;Christophe Dupuy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arava_R/0/1/0/all/0/1"&gt;Radhika Arava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1"&gt;Rahul Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1"&gt;Anna Rumshisky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Debiased Explainable Pairwise Ranking from Implicit Feedback. (arXiv:2107.14768v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.14768</id>
        <link href="http://arxiv.org/abs/2107.14768"/>
        <updated>2021-08-02T01:58:23.001Z</updated>
        <summary type="html"><![CDATA[Recent work in recommender systems has emphasized the importance of fairness,
with a particular interest in bias and transparency, in addition to predictive
accuracy. In this paper, we focus on the state of the art pairwise ranking
model, Bayesian Personalized Ranking (BPR), which has previously been found to
outperform pointwise models in predictive accuracy, while also being able to
handle implicit feedback. Specifically, we address two limitations of BPR: (1)
BPR is a black box model that does not explain its outputs, thus limiting the
user's trust in the recommendations, and the analyst's ability to scrutinize a
model's outputs; and (2) BPR is vulnerable to exposure bias due to the data
being Missing Not At Random (MNAR). This exposure bias usually translates into
an unfairness against the least popular items because they risk being
under-exposed by the recommender system. In this work, we first propose a novel
explainable loss function and a corresponding Matrix Factorization-based model
called Explainable Bayesian Personalized Ranking (EBPR) that generates
recommendations along with item-based explanations. Then, we theoretically
quantify additional exposure bias resulting from the explainability, and use it
as a basis to propose an unbiased estimator for the ideal EBPR loss. The result
is a ranking model that aptly captures both debiased and explainable user
preferences. Finally, we perform an empirical study on three real-world
datasets that demonstrate the advantages of our proposed models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Damak_K/0/1/0/all/0/1"&gt;Khalil Damak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khenissi_S/0/1/0/all/0/1"&gt;Sami Khenissi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasraoui_O/0/1/0/all/0/1"&gt;Olfa Nasraoui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[USC: An Open-Source Uzbek Speech Corpus and Initial Speech Recognition Experiments. (arXiv:2107.14419v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.14419</id>
        <link href="http://arxiv.org/abs/2107.14419"/>
        <updated>2021-08-02T01:58:22.987Z</updated>
        <summary type="html"><![CDATA[We present a freely available speech corpus for the Uzbek language and report
preliminary automatic speech recognition (ASR) results using both the deep
neural network hidden Markov model (DNN-HMM) and end-to-end (E2E)
architectures. The Uzbek speech corpus (USC) comprises 958 different speakers
with a total of 105 hours of transcribed audio recordings. To the best of our
knowledge, this is the first open-source Uzbek speech corpus dedicated to the
ASR task. To ensure high quality, the USC has been manually checked by native
speakers. We first describe the design and development procedures of the USC,
and then explain the conducted ASR experiments in detail. The experimental
results demonstrate promising results for the applicability of the USC for ASR.
Specifically, 18.1% and 17.4% word error rates were achieved on the validation
and test sets, respectively. To enable experiment reproducibility, we share the
USC dataset, pre-trained models, and training recipes in our GitHub repository.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Musaev_M/0/1/0/all/0/1"&gt;Muhammadjon Musaev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mussakhojayeva_S/0/1/0/all/0/1"&gt;Saida Mussakhojayeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khujayorov_I/0/1/0/all/0/1"&gt;Ilyos Khujayorov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khassanov_Y/0/1/0/all/0/1"&gt;Yerbolat Khassanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ochilov_M/0/1/0/all/0/1"&gt;Mannon Ochilov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Varol_H/0/1/0/all/0/1"&gt;Huseyin Atakan Varol&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differences in Chinese and Western tourists faced with Japanese hospitality: A natural language processing approach. (arXiv:2107.14681v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.14681</id>
        <link href="http://arxiv.org/abs/2107.14681"/>
        <updated>2021-08-02T01:58:22.752Z</updated>
        <summary type="html"><![CDATA[Since culture influences expectations, perceptions, and satisfaction, a
cross-culture study is necessary to understand the differences between Japan's
biggest tourist populations, Chinese and Western tourists. However, with
ever-increasing customer populations, this is hard to accomplish without
extensive customer base studies. There is a need for an automated method for
identifying these expectations at a large scale. For this, we used a
data-driven approach to our analysis. Our study analyzed their satisfaction
factors comparing soft attributes, such as service, with hard attributes, such
as location and facilities, and studied different price ranges. We collected
hotel reviews and extracted keywords to classify the sentiment of sentences
with an SVC. We then used dependency parsing and part-of-speech tagging to
extract nouns tied to positive adjectives. We found that Chinese tourists
consider room quality more than hospitality, whereas Westerners are delighted
more by staff behavior. Furthermore, the lack of a Chinese-friendly environment
for Chinese customers and cigarette smell for Western ones can be disappointing
factors of their stay. As one of the first studies in the tourism field to use
the high-standard Japanese hospitality environment for this analysis, our
cross-cultural study contributes to both the theoretical understanding of
satisfaction and suggests practical applications and strategies for hotel
managers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carreon_E/0/1/0/all/0/1"&gt;Elisa Claire Alem&amp;#xe1;n Carre&amp;#xf3;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Espana_H/0/1/0/all/0/1"&gt;Hugo Alberto Mendoza Espa&amp;#xf1;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nonaka_H/0/1/0/all/0/1"&gt;Hirofumi Nonaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hiraoka_T/0/1/0/all/0/1"&gt;Toru Hiraoka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artist Similarity with Graph Neural Networks. (arXiv:2107.14541v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.14541</id>
        <link href="http://arxiv.org/abs/2107.14541"/>
        <updated>2021-08-02T01:58:22.703Z</updated>
        <summary type="html"><![CDATA[Artist similarity plays an important role in organizing, understanding, and
subsequently, facilitating discovery in large collections of music. In this
paper, we present a hybrid approach to computing similarity between artists
using graph neural networks trained with triplet loss. The novelty of using a
graph neural network architecture is to combine the topology of a graph of
artist connections with content features to embed artists into a vector space
that encodes similarity. To evaluate the proposed method, we compile the new
OLGA dataset, which contains artist similarities from AllMusic, together with
content features from AcousticBrainz. With 17,673 artists, this is the largest
academic artist similarity dataset that includes content-based features to
date. Moreover, we also showcase the scalability of our approach by
experimenting with a much larger proprietary dataset. Results show the
superiority of the proposed approach over current state-of-the-art methods for
music similarity. Finally, we hope that the OLGA dataset will facilitate
research on data-driven models for artist similarity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Korzeniowski_F/0/1/0/all/0/1"&gt;Filip Korzeniowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oramas_S/0/1/0/all/0/1"&gt;Sergio Oramas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gouyon_F/0/1/0/all/0/1"&gt;Fabien Gouyon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learnable Compression Network with Transformer for Approximate Nearest Neighbor Search. (arXiv:2107.14415v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.14415</id>
        <link href="http://arxiv.org/abs/2107.14415"/>
        <updated>2021-08-02T01:58:22.680Z</updated>
        <summary type="html"><![CDATA[Approximate Nearest neighbor search (ANNS) plays a crucial role in
information retrieval, which has a wide range of application scenarios.
Therefore, during past several years, a lot of fast ANNS approaches have been
proposed. Among these approaches, graph-based methods are one of the most
popular type, as they have shown attractive theoretical guarantees and low
query latency. In this paper, we propose a learnable compression network with
transformer (LCNT), which projects feature vectors from high dimensional space
onto low dimensional space, while preserving neighbor relationship. The
proposed model can be generalized to existing graph-based methods to accelerate
the process of building indexing graph and further reduce query latency.
Specifically, the proposed LCNT contains two major parts, projection part and
harmonizing part. In the projection part, input vectors are projected into a
sequence of subspaces via multi channel sparse projection network. In the
harmonizing part, a modified Transformer network is employed to harmonize
features in subspaces and combine them to get a new feature. To evaluate the
effectiveness of the proposed model, we conduct experiments on two
million-scale databases, GIST1M and Deep1M. Experimental results show that the
proposed model can improve the speed of building indexing graph to 2-3 times
its original speed without sacrificing accuracy significantly. The query
latency is reduced by a factor of 1.3 to 2.0. In addition, the proposed model
can also be combined with other popular quantization methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haokui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Wenze Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaoyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1"&gt;Buzhou Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse Feature Factorization for Recommender Systems with Knowledge Graphs. (arXiv:2107.14290v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.14290</id>
        <link href="http://arxiv.org/abs/2107.14290"/>
        <updated>2021-08-02T01:58:22.651Z</updated>
        <summary type="html"><![CDATA[Deep Learning and factorization-based collaborative filtering recommendation
models have undoubtedly dominated the scene of recommender systems in recent
years. However, despite their outstanding performance, these methods require a
training time proportional to the size of the embeddings and it further
increases when also side information is considered for the computation of the
recommendation list. In fact, in these cases we have that with a large number
of high-quality features, the resulting models are more complex and difficult
to train. This paper addresses this problem by presenting KGFlex: a sparse
factorization approach that grants an even greater degree of expressiveness. To
achieve this result, KGFlex analyzes the historical data to understand the
dimensions the user decisions depend on (e.g., movie direction, musical genre,
nationality of book writer). KGFlex represents each item feature as an
embedding and it models user-item interactions as a factorized entropy-driven
combination of the item attributes relevant to the user. KGFlex facilitates the
training process by letting users update only those relevant features on which
they base their decisions. In other words, the user-item prediction is mediated
by the user's personal view that considers only relevant features. An extensive
experimental evaluation shows the approach's effectiveness, considering the
recommendation results' accuracy, diversity, and induced bias. The public
implementation of KGFlex is available at https://split.to/kgflex.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anelli_V/0/1/0/all/0/1"&gt;Vito Walter Anelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noia_T/0/1/0/all/0/1"&gt;Tommaso Di Noia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sciascio_E/0/1/0/all/0/1"&gt;Eugenio Di Sciascio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferrara_A/0/1/0/all/0/1"&gt;Antonio Ferrara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mancino_A/0/1/0/all/0/1"&gt;Alberto Carlo Maria Mancino&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Approximation for Online Tensorial Independent Component Analysis. (arXiv:2012.14415v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14415</id>
        <link href="http://arxiv.org/abs/2012.14415"/>
        <updated>2021-07-30T02:13:30.636Z</updated>
        <summary type="html"><![CDATA[Independent component analysis (ICA) has been a popular dimension reduction
tool in statistical machine learning and signal processing. In this paper, we
present a convergence analysis for an online tensorial ICA algorithm, by
viewing the problem as a nonconvex stochastic approximation problem. For
estimating one component, we provide a dynamics-based analysis to prove that
our online tensorial ICA algorithm with a specific choice of stepsize achieves
a sharp finite-sample error bound. In particular, under a mild assumption on
the data-generating distribution and a scaling condition such that $d^4/T$ is
sufficiently small up to a polylogarithmic factor of data dimension $d$ and
sample size $T$, a sharp finite-sample error bound of $\tilde{O}(\sqrt{d/T})$
can be obtained.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chris Junchi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modular Deep Reinforcement Learning for Continuous Motion Planning with Temporal Logic. (arXiv:2102.12855v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12855</id>
        <link href="http://arxiv.org/abs/2102.12855"/>
        <updated>2021-07-30T02:13:30.616Z</updated>
        <summary type="html"><![CDATA[This paper investigates the motion planning of autonomous dynamical systems
modeled by Markov decision processes (MDP) with unknown transition
probabilities over continuous state and action spaces. Linear temporal logic
(LTL) is used to specify high-level tasks over infinite horizon, which can be
converted into a limit deterministic generalized B\"uchi automaton (LDGBA) with
several accepting sets. The novelty is to design an embedded product MDP
(EP-MDP) between the LDGBA and the MDP by incorporating a synchronous
tracking-frontier function to record unvisited accepting sets of the automaton,
and to facilitate the satisfaction of the accepting conditions. The proposed
LDGBA-based reward shaping and discounting schemes for the model-free
reinforcement learning (RL) only depend on the EP-MDP states and can overcome
the issues of sparse rewards. Rigorous analysis shows that any RL method that
optimizes the expected discounted return is guaranteed to find an optimal
policy whose traces maximize the satisfaction probability. A modular deep
deterministic policy gradient (DDPG) is then developed to generate such
policies over continuous state and action spaces. The performance of our
framework is evaluated via an array of OpenAI gym environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1"&gt;Mingyu Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasanbeig_M/0/1/0/all/0/1"&gt;Mohammadhosein Hasanbeig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1"&gt;Shaoping Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abate_A/0/1/0/all/0/1"&gt;Alessandro Abate&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kan_Z/0/1/0/all/0/1"&gt;Zhen Kan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Reinforcement Learning Policies for Multi-Agent Control. (arXiv:2011.08055v2 [cs.MA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08055</id>
        <link href="http://arxiv.org/abs/2011.08055"/>
        <updated>2021-07-30T02:13:30.599Z</updated>
        <summary type="html"><![CDATA[We develop a Multi-Agent Reinforcement Learning (MARL) method to learn
scalable control policies for target tracking. Our method can handle an
arbitrary number of pursuers and targets; we show results for tasks consisting
up to 1000 pursuers tracking 1000 targets. We use a decentralized,
partially-observable Markov Decision Process framework to model pursuers as
agents receiving partial observations (range and bearing) about targets which
move using fixed, unknown policies. An attention mechanism is used to
parameterize the value function of the agents; this mechanism allows us to
handle an arbitrary number of targets. Entropy-regularized off-policy RL
methods are used to train a stochastic policy, and we discuss how it enables a
hedging behavior between pursuers that leads to a weak form of cooperation in
spite of completely decentralized control execution. We further develop a
masking heuristic that allows training on smaller problems with few
pursuers-targets and execution on much larger problems. Thorough simulation
experiments, ablation studies, and comparisons to state of the art algorithms
are performed to study the scalability of the approach and robustness of
performance to varying numbers of agents and targets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1"&gt;Christopher D. Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1"&gt;Heejin Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pappas_G/0/1/0/all/0/1"&gt;George J. Pappas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhari_P/0/1/0/all/0/1"&gt;Pratik Chaudhari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TELESTO: A Graph Neural Network Model for Anomaly Classification in Cloud Services. (arXiv:2102.12877v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12877</id>
        <link href="http://arxiv.org/abs/2102.12877"/>
        <updated>2021-07-30T02:13:30.516Z</updated>
        <summary type="html"><![CDATA[Deployment, operation and maintenance of large IT systems becomes
increasingly complex and puts human experts under extreme stress when problems
occur. Therefore, utilization of machine learning (ML) and artificial
intelligence (AI) is applied on IT system operation and maintenance -
summarized in the term AIOps. One specific direction aims at the recognition of
re-occurring anomaly types to enable remediation automation. However, due to IT
system specific properties, especially their frequent changes (e.g. software
updates, reconfiguration or hardware modernization), recognition of reoccurring
anomaly types is challenging. Current methods mainly assume a static
dimensionality of provided data. We propose a method that is invariant to
dimensionality changes of given data. Resource metric data such as CPU
utilization, allocated memory and others are modelled as multivariate time
series. The extraction of temporal and spatial features together with the
subsequent anomaly classification is realized by utilizing TELESTO, our novel
graph convolutional neural network (GCNN) architecture. The experimental
evaluation is conducted in a real-world cloud testbed deployment that is
hosting two applications. Classification results of injected anomalies on a
cassandra database node show that TELESTO outperforms the alternative GCNNs and
achieves an overall classification accuracy of 85.1%. Classification results
for the other nodes show accuracy values between 85% and 60%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Scheinert_D/0/1/0/all/0/1"&gt;Dominik Scheinert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Acker_A/0/1/0/all/0/1"&gt;Alexander Acker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modern Non-Linear Function-on-Function Regression. (arXiv:2107.14151v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2107.14151</id>
        <link href="http://arxiv.org/abs/2107.14151"/>
        <updated>2021-07-30T02:13:30.509Z</updated>
        <summary type="html"><![CDATA[We introduce a new class of non-linear function-on-function regression models
for functional data using neural networks. We propose a framework using a
hidden layer consisting of continuous neurons, called a continuous hidden
layer, for functional response modeling and give two model fitting strategies,
Functional Direct Neural Network (FDNN) and Functional Basis Neural Network
(FBNN). Both are designed explicitly to exploit the structure inherent in
functional data and capture the complex relations existing between the
functional predictors and the functional response. We fit these models by
deriving functional gradients and implement regularization techniques for more
parsimonious results. We demonstrate the power and flexibility of our proposed
method in handling complex functional models through extensive simulation
studies as well as real data examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Rao_A/0/1/0/all/0/1"&gt;Aniruddha Rajendra Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Reimherr_M/0/1/0/all/0/1"&gt;Matthew Reimherr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReLearn: A Robust Machine Learning Framework in Presence of Missing Data for Multimodal Stress Detection from Physiological Signals. (arXiv:2104.14278v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14278</id>
        <link href="http://arxiv.org/abs/2104.14278"/>
        <updated>2021-07-30T02:13:30.484Z</updated>
        <summary type="html"><![CDATA[Continuous and multimodal stress detection has been performed recently
through wearable devices and machine learning algorithms. However, a well-known
and important challenge of working on physiological signals recorded by
conventional monitoring devices is missing data due to sensors insufficient
contact and interference by other equipment. This challenge becomes more
problematic when the user/patient is mentally or physically active or stressed
because of more frequent conscious or subconscious movements. In this paper, we
propose ReLearn, a robust machine learning framework for stress detection from
biomarkers extracted from multimodal physiological signals. ReLearn effectively
copes with missing data and outliers both at training and inference phases.
ReLearn, composed of machine learning models for feature selection, outlier
detection, data imputation, and classification, allows us to classify all
samples, including those with missing values at inference. In particular,
according to our experiments and stress database, while by discarding all
missing data, as a simplistic yet common approach, no prediction can be made
for 34% of the data at inference, our approach can achieve accurate
predictions, as high as 78%, for missing samples. Also, our experiments show
that the proposed framework obtains a cross-validation accuracy of 86.8% even
if more than 50% of samples within the features are missing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Iranfar_A/0/1/0/all/0/1"&gt;Arman Iranfar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arza_A/0/1/0/all/0/1"&gt;Adriana Arza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atienza_D/0/1/0/all/0/1"&gt;David Atienza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How To Make the Gradients Small Stochastically: Even Faster Convex and Nonconvex SGD. (arXiv:1801.02982v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1801.02982</id>
        <link href="http://arxiv.org/abs/1801.02982"/>
        <updated>2021-07-30T02:13:30.469Z</updated>
        <summary type="html"><![CDATA[Stochastic gradient descent (SGD) gives an optimal convergence rate when
minimizing convex stochastic objectives $f(x)$. However, in terms of making the
gradients small, the original SGD does not give an optimal rate, even when
$f(x)$ is convex.

If $f(x)$ is convex, to find a point with gradient norm $\varepsilon$, we
design an algorithm SGD3 with a near-optimal rate
$\tilde{O}(\varepsilon^{-2})$, improving the best known rate
$O(\varepsilon^{-8/3})$ of [18].

If $f(x)$ is nonconvex, to find its $\varepsilon$-approximate local minimum,
we design an algorithm SGD5 with rate $\tilde{O}(\varepsilon^{-3.5})$, where
previously SGD variants only achieve $\tilde{O}(\varepsilon^{-4})$ [6, 15, 33].
This is no slower than the best known stochastic version of Newton's method in
all parameter regimes [30].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Allen_Zhu_Z/0/1/0/all/0/1"&gt;Zeyuan Allen-Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Baseline Values for Shapley Values. (arXiv:2105.10719v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10719</id>
        <link href="http://arxiv.org/abs/2105.10719"/>
        <updated>2021-07-30T02:13:30.463Z</updated>
        <summary type="html"><![CDATA[This paper aims to formulate the problem of estimating the optimal baseline
values for the Shapley value in game theory. The Shapley value measures the
attribution of each input variable of a complex model, which is computed as the
marginal benefit from the presence of this variable w.r.t.its absence under
different contexts. To this end, people usually set the input variable to its
baseline value to represent the absence of this variable (i.e.the no-signal
state of this variable). Previous studies usually determine the baseline values
in an empirical manner, which hurts the trustworthiness of the Shapley value.
In this paper, we revisit the feature representation of a deep model from the
perspective of game theory, and define the multi-variate interaction patterns
of input variables to define the no-signal state of an input variable. Based on
the multi-variate interaction, we learn the optimal baseline value of each
input variable. Experimental results have demonstrated the effectiveness of our
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jie Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhanpeng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qirui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Quanshi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RECOWNs: Probabilistic Circuits for Trustworthy Time Series Forecasting. (arXiv:2106.04148v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04148</id>
        <link href="http://arxiv.org/abs/2106.04148"/>
        <updated>2021-07-30T02:13:30.458Z</updated>
        <summary type="html"><![CDATA[Time series forecasting is a relevant task that is performed in several
real-world scenarios such as product sales analysis and prediction of energy
demand. Given their accuracy performance, currently, Recurrent Neural Networks
(RNNs) are the models of choice for this task. Despite their success in time
series forecasting, less attention has been paid to make the RNNs trustworthy.
For example, RNNs can not naturally provide an uncertainty measure to their
predictions. This could be extremely useful in practice in several cases e.g.
to detect when a prediction might be completely wrong due to an unusual pattern
in the time series. Whittle Sum-Product Networks (WSPNs), prominent deep
tractable probabilistic circuits (PCs) for time series, can assist an RNN with
providing meaningful probabilities as uncertainty measure. With this aim, we
propose RECOWN, a novel architecture that employs RNNs and a discriminant
variant of WSPNs called Conditional WSPNs (CWSPNs). We also formulate a
Log-Likelihood Ratio Score as better estimation of uncertainty that is tailored
to time series and Whittle likelihoods. In our experiments, we show that
RECOWNs are accurate and trustworthy time series predictors, able to "know when
they do not know".]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thoma_N/0/1/0/all/0/1"&gt;Nils Thoma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhongjie Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ventola_F/0/1/0/all/0/1"&gt;Fabrizio Ventola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1"&gt;Kristian Kersting&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Sample Complexity Upper and Lower Bounds for Exact Ranking from Noisy Comparisons. (arXiv:1909.03194v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.03194</id>
        <link href="http://arxiv.org/abs/1909.03194"/>
        <updated>2021-07-30T02:13:30.452Z</updated>
        <summary type="html"><![CDATA[This paper studies the problem of finding the exact ranking from noisy
comparisons. A comparison over a set of $m$ items produces a noisy outcome
about the most preferred item, and reveals some information about the ranking.
By repeatedly and adaptively choosing items to compare, we want to fully rank
the items with a certain confidence, and use as few comparisons as possible.
Different from most previous works, in this paper, we have three main
novelties: (i) compared to prior works, our upper bounds (algorithms) and lower
bounds on the sample complexity (aka number of comparisons) require the minimal
assumptions on the instances, and are not restricted to specific models; (ii)
we give lower bounds and upper bounds on instances with unequal noise levels;
and (iii) this paper aims at the exact ranking without knowledge on the
instances, while most of the previous works either focus on approximate
rankings or study exact ranking but require prior knowledge. We first derive
lower bounds for pairwise ranking (i.e., compare two items each time), and then
propose (nearly) optimal pairwise ranking algorithms. We further make
extensions to listwise ranking (i.e., comparing multiple items each time).
Numerical results also show our improvements against the state of the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1"&gt;Wenbo Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jia Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shroff_N/0/1/0/all/0/1"&gt;Ness B. Shroff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Merge of k-NN Graph. (arXiv:1908.00814v6 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.00814</id>
        <link href="http://arxiv.org/abs/1908.00814"/>
        <updated>2021-07-30T02:13:30.446Z</updated>
        <summary type="html"><![CDATA[k-nearest neighbor graph is a fundamental data structure in many disciplines
such as information retrieval, data-mining, pattern recognition, and machine
learning, etc. In the literature, considerable research has been focusing on
how to efficiently build an approximate k-nearest neighbor graph (k-NN graph)
for a fixed dataset. Unfortunately, a closely related issue of how to merge two
existing k-NN graphs has been overlooked. In this paper, we address the issue
of k-NN graph merging in two different scenarios. In the first scenario, a
symmetric merge algorithm is proposed to combine two approximate k-NN graphs.
The algorithm facilitates large-scale processing by the efficient merging of
k-NN graphs that are produced in parallel. In the second scenario, a joint
merge algorithm is proposed to expand an existing k-NN graph with a raw
dataset. The algorithm enables the incremental construction of a hierarchical
approximate k-NN graph. Superior performance is attained when leveraging the
hierarchy for NN search of various data types, dimensionality, and distance
measures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wan-Lei Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1"&gt;Peng-Cheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1"&gt;Chong-Wah Ngo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spot What Matters: Learning Context Using Graph Convolutional Networks for Weakly-Supervised Action Detection. (arXiv:2107.13648v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13648</id>
        <link href="http://arxiv.org/abs/2107.13648"/>
        <updated>2021-07-30T02:13:30.432Z</updated>
        <summary type="html"><![CDATA[The dominant paradigm in spatiotemporal action detection is to classify
actions using spatiotemporal features learned by 2D or 3D Convolutional
Networks. We argue that several actions are characterized by their context,
such as relevant objects and actors present in the video. To this end, we
introduce an architecture based on self-attention and Graph Convolutional
Networks in order to model contextual cues, such as actor-actor and
actor-object interactions, to improve human action detection in video. We are
interested in achieving this in a weakly-supervised setting, i.e. using as less
annotations as possible in terms of action bounding boxes. Our model aids
explainability by visualizing the learned context as an attention map, even for
actions and objects unseen during training. We evaluate how well our model
highlights the relevant context by introducing a quantitative metric based on
recall of objects retrieved by attention maps. Our model relies on a 3D
convolutional RGB stream, and does not require expensive optical flow
computation. We evaluate our models on the DALY dataset, which consists of
human-object interaction actions. Experimental results show that our
contextualized approach outperforms a baseline action detection approach by
more than 2 points in Video-mAP. Code is available at
\url{https://github.com/micts/acgcn}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsiaousis_M/0/1/0/all/0/1"&gt;Michail Tsiaousis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burghouts_G/0/1/0/all/0/1"&gt;Gertjan Burghouts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hillerstrom_F/0/1/0/all/0/1"&gt;Fieke Hillerstr&amp;#xf6;m&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Putten_P/0/1/0/all/0/1"&gt;Peter van der Putten&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral Leakage and Rethinking the Kernel Size in CNNs. (arXiv:2101.10143v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10143</id>
        <link href="http://arxiv.org/abs/2101.10143"/>
        <updated>2021-07-30T02:13:30.426Z</updated>
        <summary type="html"><![CDATA[Convolutional layers in CNNs implement linear filters which decompose the
input into different frequency bands. However, most modern architectures
neglect standard principles of filter design when optimizing their model
choices regarding the size and shape of the convolutional kernel. In this work,
we consider the well-known problem of spectral leakage caused by windowing
artifacts in filtering operations in the context of CNNs. We show that the
small size of CNN kernels make them susceptible to spectral leakage, which may
induce performance-degrading artifacts. To address this issue, we propose the
use of larger kernel sizes along with the Hamming window function to alleviate
leakage in CNN architectures. We demonstrate improved classification accuracy
on multiple benchmark datasets including Fashion-MNIST, CIFAR-10, CIFAR-100 and
ImageNet with the simple use of a standard window function in convolutional
layers. Finally, we show that CNNs employing the Hamming window display
increased robustness against various adversarial attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tomen_N/0/1/0/all/0/1"&gt;Nergis Tomen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1"&gt;Jan van Gemert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FATNN: Fast and Accurate Ternary Neural Networks. (arXiv:2008.05101v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.05101</id>
        <link href="http://arxiv.org/abs/2008.05101"/>
        <updated>2021-07-30T02:13:30.421Z</updated>
        <summary type="html"><![CDATA[Ternary Neural Networks (TNNs) have received much attention due to being
potentially orders of magnitude faster in inference, as well as more power
efficient, than full-precision counterparts. However, 2 bits are required to
encode the ternary representation with only 3 quantization levels leveraged. As
a result, conventional TNNs have similar memory consumption and speed compared
with the standard 2-bit models, but have worse representational capability.
Moreover, there is still a significant gap in accuracy between TNNs and
full-precision networks, hampering their deployment to real applications. To
tackle these two challenges, in this work, we first show that, under some mild
constraints, computational complexity of the ternary inner product can be
reduced by a factor of 2. Second, to mitigate the performance gap, we
elaborately design an implementation-dependent ternary quantization algorithm.
The proposed framework is termed Fast and Accurate Ternary Neural Networks
(FATNN). Experiments on image classification demonstrate that our FATNN
surpasses the state-of-the-arts by a significant margin in accuracy. More
importantly, speedup evaluation compared with various precisions is analyzed on
several platforms, which serves as a strong benchmark for further research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Peng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1"&gt;Bohan Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chunhua Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relational Graph Neural Networks for Fraud Detection in a Super-Appe nvironment. (arXiv:2107.13673v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13673</id>
        <link href="http://arxiv.org/abs/2107.13673"/>
        <updated>2021-07-30T02:13:30.415Z</updated>
        <summary type="html"><![CDATA[Large digital platforms create environments where different types of user
interactions are captured, these relationships offer a novel source of
information for fraud detection problems. In this paper we propose a framework
of relational graph convolutional networks methods for fraudulent behaviour
prevention in the financial services of a Super-App. To this end, we apply the
framework on different heterogeneous graphs of users, devices, and credit
cards; and finally use an interpretability algorithm for graph neural networks
to determine the most important relations to the classification task of the
users. Our results show that there is an added value when considering models
that take advantage of the alternative data of the Super-App and the
interactions found in their high connectivity, further proofing how they can
leverage that into better decisions and fraud detection strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Acevedo_Viloria_J/0/1/0/all/0/1"&gt;Jaime D. Acevedo-Viloria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roa_L/0/1/0/all/0/1"&gt;Luisa Roa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adeshina_S/0/1/0/all/0/1"&gt;Soji Adeshina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olazo_C/0/1/0/all/0/1"&gt;Cesar Charalla Olazo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_Rey_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9;s Rodr&amp;#xed;guez-Rey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramos_J/0/1/0/all/0/1"&gt;Jose Alberto Ramos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Correa_Bahnsen_A/0/1/0/all/0/1"&gt;Alejandro Correa-Bahnsen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the combined effect of class imbalance and concept complexity in deep learning. (arXiv:2107.14194v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14194</id>
        <link href="http://arxiv.org/abs/2107.14194"/>
        <updated>2021-07-30T02:13:30.410Z</updated>
        <summary type="html"><![CDATA[Structural concept complexity, class overlap, and data scarcity are some of
the most important factors influencing the performance of classifiers under
class imbalance conditions. When these effects were uncovered in the early
2000s, understandably, the classifiers on which they were demonstrated belonged
to the classical rather than Deep Learning categories of approaches. As Deep
Learning is gaining ground over classical machine learning and is beginning to
be used in critical applied settings, it is important to assess systematically
how well they respond to the kind of challenges their classical counterparts
have struggled with in the past two decades. The purpose of this paper is to
study the behavior of deep learning systems in settings that have previously
been deemed challenging to classical machine learning systems to find out
whether the depth of the systems is an asset in such settings. The results in
both artificial and real-world image datasets (MNIST Fashion, CIFAR-10) show
that these settings remain mostly challenging for Deep Learning systems and
that deeper architectures seem to help with structural concept complexity but
not with overlap challenges in simple artificial domains. Data scarcity is not
overcome by deeper layers, either. In the real-world image domains, where
overfitting is a greater concern than in the artificial domains, the advantage
of deeper architectures is less obvious: while it is observed in certain cases,
it is quickly cancelled as models get deeper and perform worse than their
shallower counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_K/0/1/0/all/0/1"&gt;Kushankur Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bellinger_C/0/1/0/all/0/1"&gt;Colin Bellinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Corizzo_R/0/1/0/all/0/1"&gt;Roberto Corizzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krawczyk_B/0/1/0/all/0/1"&gt;Bartosz Krawczyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Japkowicz_N/0/1/0/all/0/1"&gt;Nathalie Japkowicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Day-to-day and seasonal regularity of network passenger delay for metro networks. (arXiv:2107.14094v1 [physics.soc-ph])]]></title>
        <id>http://arxiv.org/abs/2107.14094</id>
        <link href="http://arxiv.org/abs/2107.14094"/>
        <updated>2021-07-30T02:13:30.393Z</updated>
        <summary type="html"><![CDATA[In an effort to improve user satisfaction and transit image, transit service
providers worldwide offer delay compensations. Smart card data enables the
estimation of passenger delays throughout the network and aid in monitoring
service performance. Notwithstanding, in order to prioritize measures for
improving service reliability and hence reducing passenger delays, it is
paramount to identify the system components - stations and track segments -
where most passenger delay occurs. To this end, we propose a novel method for
estimating network passenger delay from individual trajectories. We decompose
the delay along a passenger trajectory into its corresponding track segment
delay, initial waiting time and transfer delay. We distinguish between two
different types of passenger delay in relation to the public transit network:
average passenger delay and total passenger delay. We employ temporal
clustering on these two quantities to reveal daily and seasonal regularity in
delay patterns of the transit network. The estimation and clustering methods
are demonstrated on one year of data from Washington metro network. The data
consists of schedule information and smart card data which includes
passenger-train assignment of the metro network for the months of August 2017
to August 2018. Our findings show that the average passenger delay is
relatively stable throughout the day. The temporal clustering reveals
pronounced and recurrent and thus predictable daily and weekly patterns with
distinct characteristics for certain months.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Krishnakumari_P/0/1/0/all/0/1"&gt;Panchamy Krishnakumari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Cats_O/0/1/0/all/0/1"&gt;Oded Cats&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Lint_H/0/1/0/all/0/1"&gt;Hans van Lint&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning more skills through optimistic exploration. (arXiv:2107.14226v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14226</id>
        <link href="http://arxiv.org/abs/2107.14226"/>
        <updated>2021-07-30T02:13:30.387Z</updated>
        <summary type="html"><![CDATA[Unsupervised skill learning objectives (Gregor et al., 2016, Eysenbach et
al., 2018) allow agents to learn rich repertoires of behavior in the absence of
extrinsic rewards. They work by simultaneously training a policy to produce
distinguishable latent-conditioned trajectories, and a discriminator to
evaluate distinguishability by trying to infer latents from trajectories. The
hope is for the agent to explore and master the environment by encouraging each
skill (latent) to reliably reach different states. However, an inherent
exploration problem lingers: when a novel state is actually encountered, the
discriminator will necessarily not have seen enough training data to produce
accurate and confident skill classifications, leading to low intrinsic reward
for the agent and effective penalization of the sort of exploration needed to
actually maximize the objective. To combat this inherent pessimism towards
exploration, we derive an information gain auxiliary objective that involves
training an ensemble of discriminators and rewarding the policy for their
disagreement. Our objective directly estimates the epistemic uncertainty that
comes from the discriminator not having seen enough training examples, thus
providing an intrinsic reward more tailored to the true objective compared to
pseudocount-based methods (Burda et al., 2019). We call this exploration bonus
discriminator disagreement intrinsic reward, or DISDAIN. We demonstrate
empirically that DISDAIN improves skill learning both in a tabular grid world
(Four Rooms) and the 57 games of the Atari Suite (from pixels). Thus, we
encourage researchers to treat pessimism with DISDAIN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Strouse_D/0/1/0/all/0/1"&gt;DJ Strouse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baumli_K/0/1/0/all/0/1"&gt;Kate Baumli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Warde_Farley_D/0/1/0/all/0/1"&gt;David Warde-Farley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mnih_V/0/1/0/all/0/1"&gt;Vlad Mnih&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hansen_S/0/1/0/all/0/1"&gt;Steven Hansen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Fairness Testing of Neural Classifiers through Adversarial Sampling. (arXiv:2107.08176v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08176</id>
        <link href="http://arxiv.org/abs/2107.08176"/>
        <updated>2021-07-30T02:13:30.380Z</updated>
        <summary type="html"><![CDATA[Although deep learning has demonstrated astonishing performance in many
applications, there are still concerns about its dependability. One desirable
property of deep learning applications with societal impact is fairness (i.e.,
non-discrimination). Unfortunately, discrimination might be intrinsically
embedded into the models due to the discrimination in the training data. As a
countermeasure, fairness testing systemically identifies discriminatory
samples, which can be used to retrain the model and improve the model's
fairness. Existing fairness testing approaches however have two major
limitations. Firstly, they only work well on traditional machine learning
models and have poor performance (e.g., effectiveness and efficiency) on deep
learning models. Secondly, they only work on simple structured (e.g., tabular)
data and are not applicable for domains such as text. In this work, we bridge
the gap by proposing a scalable and effective approach for systematically
searching for discriminatory samples while extending existing fairness testing
approaches to address a more challenging domain, i.e., text classification.
Compared with state-of-the-art methods, our approach only employs lightweight
procedures like gradient computation and clustering, which is significantly
more scalable and effective. Experimental results show that on average, our
approach explores the search space much more effectively (9.62 and 2.38 times
more than the state-of-the-art methods respectively on tabular and text
datasets) and generates much more discriminatory samples (24.95 and 2.68 times)
within a same reasonable time. Moreover, the retrained models reduce
discrimination by 57.2% and 60.2% respectively on average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Peixin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jingyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jun Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1"&gt;Guoliang Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xingen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1"&gt;Ting Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Jin Song Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Convergence of Reinforcement Learning in Nonlinear Continuous State Space Problems. (arXiv:2011.10829v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10829</id>
        <link href="http://arxiv.org/abs/2011.10829"/>
        <updated>2021-07-30T02:13:30.374Z</updated>
        <summary type="html"><![CDATA[We consider the problem of Reinforcement Learning for nonlinear stochastic
dynamical systems. We show that in the RL setting, there is an inherent ``Curse
of Variance" in addition to Bellman's infamous ``Curse of Dimensionality", in
particular, we show that the variance in the solution grows
factorial-exponentially in the order of the approximation. A fundamental
consequence is that this precludes the search for anything other than ``local"
feedback solutions in RL, in order to control the explosive variance growth,
and thus, ensure accuracy. We further show that the deterministic optimal
control has a perturbation structure, in that the higher order terms do not
affect the calculation of lower order terms, which can be utilized in RL to get
accurate local solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_R/0/1/0/all/0/1"&gt;Raman Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakravorty_S/0/1/0/all/0/1"&gt;Suman Chakravorty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ran Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_M/0/1/0/all/0/1"&gt;Mohamed Naveed Gul Mohamed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modifications of FastICA in Convolutive Blind Source Separation. (arXiv:2107.14135v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.14135</id>
        <link href="http://arxiv.org/abs/2107.14135"/>
        <updated>2021-07-30T02:13:30.369Z</updated>
        <summary type="html"><![CDATA[Convolutive blind source separation (BSS) is intended to recover the unknown
components from their convolutive mixtures. Contrary to the contrast functions
used in instantaneous cases, the spatial-temporal prewhitening stage and the
para-unitary filters constraint are difficult to implement in a convolutive
context. In this paper, we propose several modifications of FastICA to
alleviate these difficulties. Our method performs the simple prewhitening step
on convolutive mixtures prior to the separation and optimizes the contrast
function under the diagonalization constraint implemented by single value
decomposition (SVD). Numerical simulations are implemented to verify the
performance of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;YunPeng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpolating Classifiers Make Few Mistakes. (arXiv:2101.11815v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11815</id>
        <link href="http://arxiv.org/abs/2101.11815"/>
        <updated>2021-07-30T02:13:30.350Z</updated>
        <summary type="html"><![CDATA[This paper provides elementary analyses of the regret and generalization of
minimum-norm interpolating classifiers (MNIC). The MNIC is the function of
smallest Reproducing Kernel Hilbert Space norm that perfectly interpolates a
label pattern on a finite data set. We derive a mistake bound for MNIC and a
regularized variant that holds for all data sets. This bound follows from
elementary properties of matrix inverses. Under the assumption that the data is
independently and identically distributed, the mistake bound implies that MNIC
generalizes at a rate proportional to the norm of the interpolating solution
and inversely proportional to the number of data points. This rate matches
similar rates derived for margin classifiers and perceptrons. We derive several
plausible generative models where the norm of the interpolating classifier is
bounded or grows at a rate sublinear in $n$. We also show that as long as the
population class conditional distributions are sufficiently separable in total
variation, then MNIC generalizes with a fast rate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Liang_T/0/1/0/all/0/1"&gt;Tengyuan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Recht_B/0/1/0/all/0/1"&gt;Benjamin Recht&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Complexity of Finding Stationary Points with Stochastic Gradient Descent. (arXiv:1910.01845v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.01845</id>
        <link href="http://arxiv.org/abs/1910.01845"/>
        <updated>2021-07-30T02:13:30.345Z</updated>
        <summary type="html"><![CDATA[We study the iteration complexity of stochastic gradient descent (SGD) for
minimizing the gradient norm of smooth, possibly nonconvex functions. We
provide several results, implying that the $\mathcal{O}(\epsilon^{-4})$ upper
bound of Ghadimi and Lan~\cite{ghadimi2013stochastic} (for making the average
gradient norm less than $\epsilon$) cannot be improved upon, unless a
combination of additional assumptions is made. Notably, this holds even if we
limit ourselves to convex quadratic functions. We also show that for nonconvex
functions, the feasibility of minimizing gradients with SGD is surprisingly
sensitive to the choice of optimality criteria.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Drori_Y/0/1/0/all/0/1"&gt;Yoel Drori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1"&gt;Ohad Shamir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-Markovian Reinforcement Learning using Fractional Dynamics. (arXiv:2107.13790v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13790</id>
        <link href="http://arxiv.org/abs/2107.13790"/>
        <updated>2021-07-30T02:13:30.336Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning (RL) is a technique to learn the control policy for an
agent that interacts with a stochastic environment. In any given state, the
agent takes some action, and the environment determines the probability
distribution over the next state as well as gives the agent some reward. Most
RL algorithms typically assume that the environment satisfies Markov
assumptions (i.e. the probability distribution over the next state depends only
on the current state). In this paper, we propose a model-based RL technique for
a system that has non-Markovian dynamics. Such environments are common in many
real-world applications such as in human physiology, biological systems,
material science, and population dynamics. Model-based RL (MBRL) techniques
typically try to simultaneously learn a model of the environment from the data,
as well as try to identify an optimal policy for the learned model. We propose
a technique where the non-Markovianity of the system is modeled through a
fractional dynamical system. We show that we can quantify the difference in the
performance of an MBRL algorithm that uses bounded horizon model predictive
control from the optimal policy. Finally, we demonstrate our proposed framework
on a pharmacokinetic model of human blood glucose dynamics and show that our
fractional models can capture distant correlations on real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1"&gt;Gaurav Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1"&gt;Chenzhong Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deshmukh_J/0/1/0/all/0/1"&gt;Jyotirmoy V. Deshmukh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bogdan_P/0/1/0/all/0/1"&gt;Paul Bogdan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Translatotron 2: Robust direct speech-to-speech translation. (arXiv:2107.08661v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08661</id>
        <link href="http://arxiv.org/abs/2107.08661"/>
        <updated>2021-07-30T02:13:30.331Z</updated>
        <summary type="html"><![CDATA[We present Translatotron 2, a neural direct speech-to-speech translation
model that can be trained end-to-end. Translatotron 2 consists of a speech
encoder, a phoneme decoder, a mel-spectrogram synthesizer, and an attention
module that connects all the previous three components. Experimental results
suggest that Translatotron 2 outperforms the original Translatotron by a large
margin in terms of translation quality and predicted speech naturalness, and
drastically improves the robustness of the predicted speech by mitigating
over-generation, such as babbling or long pause. We also propose a new method
for retaining the source speaker's voice in the translated speech. The trained
model is restricted to retain the source speaker's voice, and unlike the
original Translatotron, it is not able to generate speech in a different
speaker's voice, making the model more robust for production deployment, by
mitigating potential misuse for creating spoofing audio artifacts. When the new
method is used together with a simple concatenation-based data augmentation,
the trained Translatotron 2 model is able to retain each speaker's voice for
input with speaker turns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1"&gt;Ye Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramanovich_M/0/1/0/all/0/1"&gt;Michelle Tadmor Ramanovich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1"&gt;Tal Remez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pomerantz_R/0/1/0/all/0/1"&gt;Roi Pomerantz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Did the Model Change? Efficiently Assessing Machine Learning API Shifts. (arXiv:2107.14203v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.14203</id>
        <link href="http://arxiv.org/abs/2107.14203"/>
        <updated>2021-07-30T02:13:30.326Z</updated>
        <summary type="html"><![CDATA[Machine learning (ML) prediction APIs are increasingly widely used. An ML API
can change over time due to model updates or retraining. This presents a key
challenge in the usage of the API because it is often not clear to the user if
and how the ML model has changed. Model shifts can affect downstream
application performance and also create oversight issues (e.g. if consistency
is desired). In this paper, we initiate a systematic investigation of ML API
shifts. We first quantify the performance shifts from 2020 to 2021 of popular
ML APIs from Google, Microsoft, Amazon, and others on a variety of datasets. We
identified significant model shifts in 12 out of 36 cases we investigated.
Interestingly, we found several datasets where the API's predictions became
significantly worse over time. This motivated us to formulate the API shift
assessment problem at a more fine-grained level as estimating how the API
model's confusion matrix changes over time when the data distribution is
constant. Monitoring confusion matrix shifts using standard random sampling can
require a large number of samples, which is expensive as each API call costs a
fee. We propose a principled adaptive sampling algorithm, MASA, to efficiently
estimate confusion matrix shifts. MASA can accurately estimate the confusion
matrix shifts in commercial ML APIs using up to 90% fewer samples compared to
random sampling. This work establishes ML API shifts as an important problem to
study and provides a cost-effective approach to monitor such shifts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lingjiao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cai_T/0/1/0/all/0/1"&gt;Tracy Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zaharia_M/0/1/0/all/0/1"&gt;Matei Zaharia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zou_J/0/1/0/all/0/1"&gt;James Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Overview of Human Activity Recognition Using Wearable Sensors: Healthcare and Artificial Intelligence. (arXiv:2103.15990v4 [cs.HC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15990</id>
        <link href="http://arxiv.org/abs/2103.15990"/>
        <updated>2021-07-30T02:13:30.313Z</updated>
        <summary type="html"><![CDATA[With the rapid development of the internet of things (IoT) and artificial
intelligence (AI) technologies, human activity recognition (HAR) has been
applied in a variety of domains such as security and surveillance, human-robot
interaction, and entertainment. Even though a number of surveys and review
papers have been published, there is a lack of HAR overview papers focusing on
healthcare applications that use wearable sensors. Therefore, we fill in the
gap by presenting this overview paper. In particular, we present our projects
to illustrate the system design of HAR applications for healthcare. Our
projects include early mobility identification of human activities for
intensive care unit (ICU) patients and gait analysis of Duchenne muscular
dystrophy (DMD) patients. We cover essential components of designing HAR
systems including sensor factors (e.g., type, number, and placement location),
AI model selection (e.g., classical machine learning models versus deep
learning models), and feature engineering. In addition, we highlight the
challenges of such healthcare-oriented HAR systems and propose several research
opportunities for both the medical and the computer science community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Rex Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramli_A/0/1/0/all/0/1"&gt;Albara Ah Ramli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huanle Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Datta_E/0/1/0/all/0/1"&gt;Esha Datta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henricson_E/0/1/0/all/0/1"&gt;Erik Henricson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xin Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReconVAT: A Semi-Supervised Automatic Music Transcription Framework for Low-Resource Real-World Data. (arXiv:2107.04954v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04954</id>
        <link href="http://arxiv.org/abs/2107.04954"/>
        <updated>2021-07-30T02:13:30.307Z</updated>
        <summary type="html"><![CDATA[Most of the current supervised automatic music transcription (AMT) models
lack the ability to generalize. This means that they have trouble transcribing
real-world music recordings from diverse musical genres that are not presented
in the labelled training data. In this paper, we propose a semi-supervised
framework, ReconVAT, which solves this issue by leveraging the huge amount of
available unlabelled music recordings. The proposed ReconVAT uses
reconstruction loss and virtual adversarial training. When combined with
existing U-net models for AMT, ReconVAT achieves competitive results on common
benchmark datasets such as MAPS and MusicNet. For example, in the few-shot
setting for the string part version of MusicNet, ReconVAT achieves F1-scores of
61.0% and 41.6% for the note-wise and note-with-offset-wise metrics
respectively, which translates into an improvement of 22.2% and 62.5% compared
to the supervised baseline model. Our proposed framework also demonstrates the
potential of continual learning on new data, which could be useful in
real-world applications whereby new data is constantly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheuk_K/0/1/0/all/0/1"&gt;Kin Wai Cheuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herremans_D/0/1/0/all/0/1"&gt;Dorien Herremans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1"&gt;Li Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Adversarial Robustness via Test-time Transformation Ensembling. (arXiv:2107.14110v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14110</id>
        <link href="http://arxiv.org/abs/2107.14110"/>
        <updated>2021-07-30T02:13:30.302Z</updated>
        <summary type="html"><![CDATA[Deep learning models are prone to being fooled by imperceptible perturbations
known as adversarial attacks. In this work, we study how equipping models with
Test-time Transformation Ensembling (TTE) can work as a reliable defense
against such attacks. While transforming the input data, both at train and test
times, is known to enhance model performance, its effects on adversarial
robustness have not been studied. Here, we present a comprehensive empirical
study of the impact of TTE, in the form of widely-used image transforms, on
adversarial robustness. We show that TTE consistently improves model robustness
against a variety of powerful attacks without any need for re-training, and
that this improvement comes at virtually no trade-off with accuracy on clean
samples. Finally, we show that the benefits of TTE transfer even to the
certified robustness domain, in which TTE provides sizable and consistent
improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perez_J/0/1/0/all/0/1"&gt;Juan C. P&amp;#xe9;rez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1"&gt;Motasem Alfarra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeanneret_G/0/1/0/all/0/1"&gt;Guillaume Jeanneret&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rueda_L/0/1/0/all/0/1"&gt;Laura Rueda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1"&gt;Ali Thabet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1"&gt;Bernard Ghanem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1"&gt;Pablo Arbel&amp;#xe1;ez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning second order coupled differential equations that are subject to non-conservative forces. (arXiv:2010.11270v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11270</id>
        <link href="http://arxiv.org/abs/2010.11270"/>
        <updated>2021-07-30T02:13:30.296Z</updated>
        <summary type="html"><![CDATA[In this article we address the question whether it is possible to learn the
differential equations describing the physical properties of a dynamical
system, subject to non-conservative forces, from observations of its realspace
trajectory(ies) only. We introduce a network that incorporates a difference
approximation for the second order derivative in terms of residual connections
between convolutional blocks, whose shared weights represent the coefficients
of a second order ordinary differential equation. We further combine this
solver-like architecture with a convolutional network, capable of learning the
relation between trajectories of coupled oscillators and therefore allows us to
make a stable forecast even if the system is only partially observed. We
optimize this map together with the solver network, while sharing their
weights, to form a powerful framework capable of learning the complex physical
properties of a dissipative dynamical system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Muller_R/0/1/0/all/0/1"&gt;Roger Alexander M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laflamme_Janssen_J/0/1/0/all/0/1"&gt;Jonathan Laflamme-Janssen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camacaro_J/0/1/0/all/0/1"&gt;Jaime Camacaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bessega_C/0/1/0/all/0/1"&gt;Carolina Bessega&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Guided Disentanglement in Generative Networks. (arXiv:2107.14229v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14229</id>
        <link href="http://arxiv.org/abs/2107.14229"/>
        <updated>2021-07-30T02:13:30.290Z</updated>
        <summary type="html"><![CDATA[Image-to-image translation (i2i) networks suffer from entanglement effects in
presence of physics-related phenomena in target domain (such as occlusions,
fog, etc), thus lowering the translation quality and variability. In this
paper, we present a comprehensive method for disentangling physics-based traits
in the translation, guiding the learning process with neural or physical
models. For the latter, we integrate adversarial estimation and genetic
algorithms to correctly achieve disentanglement. The results show our approach
dramatically increase performances in many challenging scenarios for image
translation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pizzati_F/0/1/0/all/0/1"&gt;Fabio Pizzati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cerri_P/0/1/0/all/0/1"&gt;Pietro Cerri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1"&gt;Raoul de Charette&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Machine Learning: Moving From Mythos to Diagnostics. (arXiv:2103.06254v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06254</id>
        <link href="http://arxiv.org/abs/2103.06254"/>
        <updated>2021-07-30T02:13:30.276Z</updated>
        <summary type="html"><![CDATA[Despite increasing interest in the field of Interpretable Machine Learning
(IML), a significant gap persists between the technical objectives targeted by
researchers' methods and the high-level goals of consumers' use cases. In this
work, we synthesize foundational work on IML methods and evaluation into an
actionable taxonomy. This taxonomy serves as a tool to conceptualize the gap
between researchers and consumers, illustrated by the lack of connections
between its methods and use cases components. It also provides the foundation
from which we describe a three-step workflow to better enable researchers and
consumers to work together to discover what types of methods are useful for
what use cases. Eventually, by building on the results generated from this
workflow, a more complete version of the taxonomy will increasingly allow
consumers to find relevant methods for their target use cases and researchers
to identify applicable use cases for their proposed methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_V/0/1/0/all/0/1"&gt;Valerie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jeffrey Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Joon Sik Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plumb_G/0/1/0/all/0/1"&gt;Gregory Plumb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1"&gt;Ameet Talwalkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot and Continual Learning with Attentive Independent Mechanisms. (arXiv:2107.14053v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14053</id>
        <link href="http://arxiv.org/abs/2107.14053"/>
        <updated>2021-07-30T02:13:30.270Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) are known to perform well when deployed to test
distributions that shares high similarity with the training distribution.
Feeding DNNs with new data sequentially that were unseen in the training
distribution has two major challenges -- fast adaptation to new tasks and
catastrophic forgetting of old tasks. Such difficulties paved way for the
on-going research on few-shot learning and continual learning. To tackle these
problems, we introduce Attentive Independent Mechanisms (AIM). We incorporate
the idea of learning using fast and slow weights in conjunction with the
decoupling of the feature extraction and higher-order conceptual learning of a
DNN. AIM is designed for higher-order conceptual learning, modeled by a mixture
of experts that compete to learn independent concepts to solve a new task. AIM
is a modular component that can be inserted into existing deep learning
frameworks. We demonstrate its capability for few-shot learning by adding it to
SIB and trained on MiniImageNet and CIFAR-FS, showing significant improvement.
AIM is also applied to ANML and OML trained on Omniglot, CIFAR-100 and
MiniImageNet to demonstrate its capability in continual learning. Code made
publicly available at https://github.com/huang50213/AIM-Fewshot-Continual.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1"&gt;Eugene Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Cheng-Han Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chen-Yi Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Point Cloud Audio Processing. (arXiv:2105.02469v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02469</id>
        <link href="http://arxiv.org/abs/2105.02469"/>
        <updated>2021-07-30T02:13:30.177Z</updated>
        <summary type="html"><![CDATA[Most audio processing pipelines involve transformations that act on
fixed-dimensional input representations of audio. For example, when using the
Short Time Fourier Transform (STFT) the DFT size specifies a fixed dimension
for the input representation. As a consequence, most audio machine learning
models are designed to process fixed-size vector inputs which often prohibits
the repurposing of learned models on audio with different sampling rates or
alternative representations. We note, however, that the intrinsic spectral
information in the audio signal is invariant to the choice of the input
representation or the sampling rate. Motivated by this, we introduce a novel
way of processing audio signals by treating them as a collection of points in
feature space, and we use point cloud machine learning models that give us
invariance to the choice of representation parameters, such as DFT size or the
sampling rate. Additionally, we observe that these methods result in smaller
models, and allow us to significantly subsample the input representation with
minimal effects to a trained model performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Subramani_K/0/1/0/all/0/1"&gt;Krishna Subramani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Smaragdis_P/0/1/0/all/0/1"&gt;Paris Smaragdis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-supervised Learning for Data-driven Soft-sensing of Biological and Chemical Processes. (arXiv:2107.13822v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2107.13822</id>
        <link href="http://arxiv.org/abs/2107.13822"/>
        <updated>2021-07-30T02:13:30.166Z</updated>
        <summary type="html"><![CDATA[Continuously operated (bio-)chemical processes increasingly suffer from
external disturbances, such as feed fluctuations or changes in market
conditions. Product quality often hinges on control of rarely measured
concentrations, which are expensive to measure. Semi-supervised regression is a
possible building block and method from machine learning to construct
soft-sensors for such infrequently measured states. Using two case studies,
i.e., the Williams-Otto process and a bioethanol production process,
semi-supervised regression is compared against standard regression to evaluate
its merits and its possible scope of application for process control in the
(bio-)chemical industry.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Esche_E/0/1/0/all/0/1"&gt;Erik Esche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Talis_T/0/1/0/all/0/1"&gt;Torben Talis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Weigert_J/0/1/0/all/0/1"&gt;Joris Weigert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Brand_Rihm_G/0/1/0/all/0/1"&gt;Gerardo Brand-Rihm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+You_B/0/1/0/all/0/1"&gt;Byungjun You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hoffmann_C/0/1/0/all/0/1"&gt;Christian Hoffmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Repke_J/0/1/0/all/0/1"&gt;Jens-Uwe Repke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coarse-to-Fine for Sim-to-Real: Sub-Millimetre Precision Across Wide Task Spaces. (arXiv:2105.11283v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11283</id>
        <link href="http://arxiv.org/abs/2105.11283"/>
        <updated>2021-07-30T02:13:30.159Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the problem of zero-shot sim-to-real when the task
requires both highly precise control with sub-millimetre error tolerance, and
wide task space generalisation. Our framework involves a coarse-to-fine
controller, where trajectories begin with classical motion planning using
ICP-based pose estimation, and transition to a learned end-to-end controller
which maps images to actions and is trained in simulation with domain
randomisation. In this way, we achieve precise control whilst also generalising
the controller across wide task spaces, and keeping the robustness of
vision-based, end-to-end control. Real-world experiments on a range of
different tasks show that, by exploiting the best of both worlds, our framework
significantly outperforms purely motion planning methods, and purely
learning-based methods. Furthermore, we answer a range of questions on best
practices for precise sim-to-real transfer, such as how different image sensor
modalities and image feature representations perform.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Valassakis_E/0/1/0/all/0/1"&gt;Eugene Valassakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palo_N/0/1/0/all/0/1"&gt;Norman Di Palo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1"&gt;Edward Johns&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Progressive Correspondence Pruning by Consensus Learning. (arXiv:2101.00591v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00591</id>
        <link href="http://arxiv.org/abs/2101.00591"/>
        <updated>2021-07-30T02:13:30.129Z</updated>
        <summary type="html"><![CDATA[Correspondence selection aims to correctly select the consistent matches
(inliers) from an initial set of putative correspondences. The selection is
challenging since putative matches are typically extremely unbalanced, largely
dominated by outliers, and the random distribution of such outliers further
complicates the learning process for learning-based methods. To address this
issue, we propose to progressively prune the correspondences via a
local-to-global consensus learning procedure. We introduce a ``pruning'' block
that lets us identify reliable candidates among the initial matches according
to consensus scores estimated using local-to-global dynamic graphs. We then
achieve progressive pruning by stacking multiple pruning blocks sequentially.
Our method outperforms state-of-the-arts on robust line fitting, camera pose
estimation and retrieval-based image localization benchmarks by significant
margins and shows promising generalization ability to different datasets and
detector/descriptor combinations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1"&gt;Chen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1"&gt;Yixiao Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1"&gt;Feng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1"&gt;Rui Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongsheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1"&gt;Mathieu Salzmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Active Learning with Temporal Output Discrepancy. (arXiv:2107.14153v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14153</id>
        <link href="http://arxiv.org/abs/2107.14153"/>
        <updated>2021-07-30T02:13:30.124Z</updated>
        <summary type="html"><![CDATA[While deep learning succeeds in a wide range of tasks, it highly depends on
the massive collection of annotated data which is expensive and time-consuming.
To lower the cost of data annotation, active learning has been proposed to
interactively query an oracle to annotate a small proportion of informative
samples in an unlabeled dataset. Inspired by the fact that the samples with
higher loss are usually more informative to the model than the samples with
lower loss, in this paper we present a novel deep active learning approach that
queries the oracle for data annotation when the unlabeled sample is believed to
incorporate high loss. The core of our approach is a measurement Temporal
Output Discrepancy (TOD) that estimates the sample loss by evaluating the
discrepancy of outputs given by models at different optimization steps. Our
theoretical investigation shows that TOD lower-bounds the accumulated sample
loss thus it can be used to select informative unlabeled samples. On basis of
TOD, we further develop an effective unlabeled data sampling strategy as well
as an unsupervised learning criterion that enhances model performance by
incorporating the unlabeled data. Due to the simplicity of TOD, our active
learning approach is efficient, flexible, and task-agnostic. Extensive
experimental results demonstrate that our approach achieves superior
performances than the state-of-the-art active learning methods on image
classification and semantic segmentation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Siyu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Haoyi Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huan_J/0/1/0/all/0/1"&gt;Jun Huan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1"&gt;Dejing Dou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do CNNs Encode Data Augmentations?. (arXiv:2003.08773v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.08773</id>
        <link href="http://arxiv.org/abs/2003.08773"/>
        <updated>2021-07-30T02:13:30.095Z</updated>
        <summary type="html"><![CDATA[Data augmentations are important ingredients in the recipe for training
robust neural networks, especially in computer vision. A fundamental question
is whether neural network features encode data augmentation transformations. To
answer this question, we introduce a systematic approach to investigate which
layers of neural networks are the most predictive of augmentation
transformations. Our approach uses features in pre-trained vision models with
minimal additional processing to predict common properties transformed by
augmentation (scale, aspect ratio, hue, saturation, contrast, and brightness).
Surprisingly, neural network features not only predict data augmentation
transformations, but they predict many transformations with high accuracy.
After validating that neural networks encode features corresponding to
augmentation transformations, we show that these features are encoded in the
early layers of modern CNNs, though the augmentation signal fades in deeper
layers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_E/0/1/0/all/0/1"&gt;Eddie Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yanping Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Fair and Ethical Healthcare Artificial Intelligence System for Monitoring Driver Behavior and Preventing Road Accidents. (arXiv:2107.14077v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2107.14077</id>
        <link href="http://arxiv.org/abs/2107.14077"/>
        <updated>2021-07-30T02:13:30.090Z</updated>
        <summary type="html"><![CDATA[This paper presents a new approach to prevent transportation accidents and
monitor driver's behavior using a healthcare AI system that incorporates
fairness and ethics. Dangerous medical cases and unusual behavior of the driver
are detected. Fairness algorithm is approached in order to improve
decision-making and address ethical issues such as privacy issues, and to
consider challenges that appear in the wild within AI in healthcare and
driving. A healthcare professional will be alerted about any unusual activity,
and the driver's location when necessary, is provided in order to enable the
healthcare professional to immediately help to the unstable driver. Therefore,
using the healthcare AI system allows for accidents to be predicted and thus
prevented and lives may be saved based on the built-in AI system inside the
vehicle which interacts with the ER system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oueida_S/0/1/0/all/0/1"&gt;Soraia Oueida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hossain_S/0/1/0/all/0/1"&gt;Soaad Hossain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kotb_Y/0/1/0/all/0/1"&gt;Yehia Kotb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1"&gt;Syed Ishtiaque Ahmed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Tale of Two Efficient and Informative Negative Sampling Distributions. (arXiv:2012.15843v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15843</id>
        <link href="http://arxiv.org/abs/2012.15843"/>
        <updated>2021-07-30T02:13:30.077Z</updated>
        <summary type="html"><![CDATA[Softmax classifiers with a very large number of classes naturally occur in
many applications such as natural language processing and information
retrieval. The calculation of full softmax is costly from the computational and
energy perspective. There have been various sampling approaches to overcome
this challenge, popularly known as negative sampling (NS). Ideally, NS should
sample negative classes from a distribution that is dependent on the input
data, the current parameters, and the correct positive class. Unfortunately,
due to the dynamically updated parameters and data samples, there is no
sampling scheme that is provably adaptive and samples the negative classes
efficiently. Therefore, alternative heuristics like random sampling, static
frequency-based sampling, or learning-based biased sampling, which primarily
trade either the sampling cost or the adaptivity of samples per iteration are
adopted. In this paper, we show two classes of distributions where the sampling
scheme is truly adaptive and provably generates negative samples in
near-constant time. Our implementation in C++ on CPU is significantly superior,
both in terms of wall-clock time and accuracy, compared to the most optimized
TensorFlow implementations of other popular negative sampling approaches on
powerful NVIDIA V100 GPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Daghaghi_S/0/1/0/all/0/1"&gt;Shabnam Daghaghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Medini_T/0/1/0/all/0/1"&gt;Tharun Medini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meisburger_N/0/1/0/all/0/1"&gt;Nicholas Meisburger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Beidi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1"&gt;Mengnan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Anshumali Shrivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrent U-net for automatic pelvic floor muscle segmentation on 3D ultrasound. (arXiv:2107.13833v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.13833</id>
        <link href="http://arxiv.org/abs/2107.13833"/>
        <updated>2021-07-30T02:13:30.062Z</updated>
        <summary type="html"><![CDATA[The prevalance of pelvic floor problems is high within the female population.
Transperineal ultrasound (TPUS) is the main imaging modality used to
investigate these problems. Automating the analysis of TPUS data will help in
growing our understanding of pelvic floor related problems. In this study we
present a U-net like neural network with some convolutional long short term
memory (CLSTM) layers to automate the 3D segmentation of the levator ani muscle
(LAM) in TPUS volumes. The CLSTM layers are added to preserve the inter-slice
3D information. We reach human level performance on this segmentation task.
Therefore, we conclude that we successfully automated the segmentation of the
LAM on 3D TPUS data. This paves the way towards automatic in-vivo analysis of
the LAM mechanics in the context of large study populations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Noort_F/0/1/0/all/0/1"&gt;Frieda van den Noort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sirmacek_B/0/1/0/all/0/1"&gt;Beril Sirmacek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Slump_C/0/1/0/all/0/1"&gt;Cornelis H. Slump&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-end Learning for OFDM: From Neural Receivers to Pilotless Communication. (arXiv:2009.05261v3 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.05261</id>
        <link href="http://arxiv.org/abs/2009.05261"/>
        <updated>2021-07-30T02:13:30.056Z</updated>
        <summary type="html"><![CDATA[Previous studies have demonstrated that end-to-end learning enables
significant shaping gains over additive white Gaussian noise (AWGN) channels.
However, its benefits have not yet been quantified over realistic wireless
channel models. This work aims to fill this gap by exploring the gains of
end-to-end learning over a frequency- and time-selective fading channel using
orthogonal frequency division multiplexing (OFDM). With imperfect channel
knowledge at the receiver, the shaping gains observed on AWGN channels vanish.
Nonetheless, we identify two other sources of performance improvements. The
first comes from a neural network (NN)-based receiver operating over a large
number of subcarriers and OFDM symbols which allows to significantly reduce the
number of orthogonal pilots without loss of bit error rate (BER). The second
comes from entirely eliminating orthognal pilots by jointly learning a neural
receiver together with either superimposed pilots (SIPs), linearly combined
with conventional quadrature amplitude modulation (QAM), or an optimized
constellation geometry. The learned geometry works for a wide range of
signal-to-noise ratios (SNRs), Doppler and delay spreads, has zero mean and
does hence not contain any form of superimposed pilots. Both schemes achieve
the same BER as the pilot-based baseline with around 7% higher throughput.
Thus, we believe that a jointly learned transmitter and receiver are a very
interesting component for beyond-5G communication systems which could remove
the need and associated control overhead for demodulation reference signals
(DMRSs).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1"&gt;Fay&amp;#xe7;al Ait Aoudia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1"&gt;Jakob Hoydis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Distributed Discrete Gaussian Mechanism for Federated Learning with Secure Aggregation. (arXiv:2102.06387v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06387</id>
        <link href="http://arxiv.org/abs/2102.06387"/>
        <updated>2021-07-30T02:13:30.006Z</updated>
        <summary type="html"><![CDATA[We consider training models on private data that are distributed across user
devices. To ensure privacy, we add on-device noise and use secure aggregation
so that only the noisy sum is revealed to the server. We present a
comprehensive end-to-end system, which appropriately discretizes the data and
adds discrete Gaussian noise before performing secure aggregation. We provide a
novel privacy analysis for sums of discrete Gaussians and carefully analyze the
effects of data quantization and modular summation arithmetic. Our theoretical
guarantees highlight the complex tension between communication, privacy, and
accuracy. Our extensive experimental results demonstrate that our solution is
essentially able to match the accuracy to central differential privacy with
less than 16 bits of precision per value.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kairouz_P/0/1/0/all/0/1"&gt;Peter Kairouz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steinke_T/0/1/0/all/0/1"&gt;Thomas Steinke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Variational Gradient Descent. (arXiv:2107.10731v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.10731</id>
        <link href="http://arxiv.org/abs/2107.10731"/>
        <updated>2021-07-30T02:13:29.987Z</updated>
        <summary type="html"><![CDATA[Particle-based approximate Bayesian inference approaches such as Stein
Variational Gradient Descent (SVGD) combine the flexibility and convergence
guarantees of sampling methods with the computational benefits of variational
inference. In practice, SVGD relies on the choice of an appropriate kernel
function, which impacts its ability to model the target distribution -- a
challenging problem with only heuristic solutions. We propose Neural
Variational Gradient Descent (NVGD), which is based on parameterizing the
witness function of the Stein discrepancy by a deep neural network whose
parameters are learned in parallel to the inference, mitigating the necessity
to make any kernel choices whatsoever. We empirically evaluate our method on
popular synthetic inference problems, real-world Bayesian linear regression,
and Bayesian neural network inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Langosco_L/0/1/0/all/0/1"&gt;Lauro Langosco di Langosco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1"&gt;Vincent Fortuin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strathmann_H/0/1/0/all/0/1"&gt;Heiko Strathmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Optimization for Min Max Optimization. (arXiv:2107.13772v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13772</id>
        <link href="http://arxiv.org/abs/2107.13772"/>
        <updated>2021-07-30T02:13:29.981Z</updated>
        <summary type="html"><![CDATA[A solution that is only reliable under favourable conditions is hardly a safe
solution. Min Max Optimization is an approach that returns optima that are
robust against worst case conditions. We propose algorithms that perform Min
Max Optimization in a setting where the function that should be optimized is
not known a priori and hence has to be learned by experiments. Therefore we
extend the Bayesian Optimization setting, which is tailored to maximization
problems, to Min Max Optimization problems. While related work extends the two
acquisition functions Expected Improvement and Gaussian Process Upper
Confidence Bound; we extend the two acquisition functions Entropy Search and
Knowledge Gradient. These acquisition functions are able to gain knowledge
about the optimum instead of just looking for points that are supposed to be
optimal. In our evaluation we show that these acquisition functions allow for
better solutions - converging faster to the optimum than the benchmark
settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weichert_D/0/1/0/all/0/1"&gt;Dorina Weichert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kister_A/0/1/0/all/0/1"&gt;Alexander Kister&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Equivariant Energy Based Models with Equivariant Stein Variational Gradient Descent. (arXiv:2106.07832v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07832</id>
        <link href="http://arxiv.org/abs/2106.07832"/>
        <updated>2021-07-30T02:13:29.922Z</updated>
        <summary type="html"><![CDATA[We focus on the problem of efficient sampling and learning of probability
densities by incorporating symmetries in probabilistic models. We first
introduce Equivariant Stein Variational Gradient Descent algorithm -- an
equivariant sampling method based on Stein's identity for sampling from
densities with symmetries. Equivariant SVGD explicitly incorporates symmetry
information in a density through equivariant kernels which makes the resultant
sampler efficient both in terms of sample complexity and the quality of
generated samples. Subsequently, we define equivariant energy based models to
model invariant densities that are learned using contrastive divergence. By
utilizing our equivariant SVGD for training equivariant EBMs, we propose new
ways of improving and scaling up training of energy based models. We apply
these equivariant energy models for modelling joint densities in regression and
classification tasks for image datasets, many-body particle systems and
molecular structure generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jaini_P/0/1/0/all/0/1"&gt;Priyank Jaini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holdijk_L/0/1/0/all/0/1"&gt;Lars Holdijk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1"&gt;Max Welling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Advances aiding Recognition and Classification of Indian Monuments and Landmarks. (arXiv:2107.14070v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14070</id>
        <link href="http://arxiv.org/abs/2107.14070"/>
        <updated>2021-07-30T02:13:29.905Z</updated>
        <summary type="html"><![CDATA[Tourism in India plays a quintessential role in the country's economy with an
estimated 9.2% GDP share for the year 2018. With a yearly growth rate of 6.2%,
the industry holds a huge potential for being the primary driver of the economy
as observed in the nations of the Middle East like the United Arab Emirates.
The historical and cultural diversity exhibited throughout the geography of the
nation is a unique spectacle for people around the world and therefore serves
to attract tourists in tens of millions in number every year. Traditionally,
tour guides or academic professionals who study these heritage monuments were
responsible for providing information to the visitors regarding their
architectural and historical significance. However, unfortunately this system
has several caveats when considered on a large scale such as unavailability of
sufficient trained people, lack of accurate information, failure to convey the
richness of details in an attractive format etc. Recently, machine learning
approaches revolving around the usage of monument pictures have been shown to
be useful for rudimentary analysis of heritage sights. This paper serves as a
survey of the research endeavors undertaken in this direction which would
eventually provide insights for building an automated decision system that
could be utilized to make the experience of tourism in India more modernized
for visitors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1"&gt;Aditya Jyoti Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1"&gt;Smaranjit Ghose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_K/0/1/0/all/0/1"&gt;Kanishka Aggarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nethaji_N/0/1/0/all/0/1"&gt;Niketha Nethaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1"&gt;Shivam Pal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Purkayastha_A/0/1/0/all/0/1"&gt;Arnab Dutta Purkayastha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bellamy: Reusing Performance Models for Distributed Dataflow Jobs Across Contexts. (arXiv:2107.13921v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2107.13921</id>
        <link href="http://arxiv.org/abs/2107.13921"/>
        <updated>2021-07-30T02:13:29.887Z</updated>
        <summary type="html"><![CDATA[Distributed dataflow systems enable the use of clusters for scalable data
analytics. However, selecting appropriate cluster resources for a processing
job is often not straightforward. Performance models trained on historical
executions of a concrete job are helpful in such situations, yet they are
usually bound to a specific job execution context (e.g. node type, software
versions, job parameters) due to the few considered input parameters. Even in
case of slight context changes, such supportive models need to be retrained and
cannot benefit from historical execution data from related contexts.

This paper presents Bellamy, a novel modeling approach that combines
scale-outs, dataset sizes, and runtimes with additional descriptive properties
of a dataflow job. It is thereby able to capture the context of a job
execution. Moreover, Bellamy is realizing a two-step modeling approach. First,
a general model is trained on all the available data for a specific scalable
analytics algorithm, hereby incorporating data from different contexts.
Subsequently, the general model is optimized for the specific situation at
hand, based on the available data for the concrete context. We evaluate our
approach on two publicly available datasets consisting of execution data from
various dataflow jobs carried out in different environments, showing that
Bellamy outperforms state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Scheinert_D/0/1/0/all/0/1"&gt;Dominik Scheinert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thamsen_L/0/1/0/all/0/1"&gt;Lauritz Thamsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Houkun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Will_J/0/1/0/all/0/1"&gt;Jonathan Will&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Acker_A/0/1/0/all/0/1"&gt;Alexander Acker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wittkopp_T/0/1/0/all/0/1"&gt;Thorsten Wittkopp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kao_O/0/1/0/all/0/1"&gt;Odej Kao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometry of Similarity Comparisons. (arXiv:2006.09858v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09858</id>
        <link href="http://arxiv.org/abs/2006.09858"/>
        <updated>2021-07-30T02:13:29.875Z</updated>
        <summary type="html"><![CDATA[Many data analysis problems can be cast as distance geometry problems in
\emph{space forms} -- Euclidean, spherical, or hyperbolic spaces. Often,
absolute distance measurements are often unreliable or simply unavailable and
only proxies to absolute distances in the form of similarities are available.
Hence we ask the following: Given only \emph{comparisons} of similarities
amongst a set of entities, what can be said about the geometry of the
underlying space form? To study this question, we introduce the notions of the
\textit{ordinal capacity} of a target space form and \emph{ordinal spread} of
the similarity measurements. The latter is an indicator of complex patterns in
the measurements, while the former quantifies the capacity of a space form to
accommodate a set of measurements with a specific ordinal spread profile. We
prove that the ordinal capacity of a space form is related to its dimension and
the sign of its curvature. This leads to a lower bound on the Euclidean and
spherical embedding dimension of what we term similarity graphs. More
importantly, we show that the statistical behavior of the ordinal spread random
variables defined on a similarity graph can be used to identify its underlying
space form. We support our theoretical claims with experiments on weighted
trees, single-cell RNA expression data and spherical cartographic measurements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tabaghi_P/0/1/0/all/0/1"&gt;Puoya Tabaghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jianhao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milenkovic_O/0/1/0/all/0/1"&gt;Olgica Milenkovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dokmanic_I/0/1/0/all/0/1"&gt;Ivan Dokmani&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-objective optimization and explanation for stroke risk assessment in Shanxi province. (arXiv:2107.14060v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14060</id>
        <link href="http://arxiv.org/abs/2107.14060"/>
        <updated>2021-07-30T02:13:29.869Z</updated>
        <summary type="html"><![CDATA[Stroke is the top leading causes of death in China (Zhou et al. The Lancet
2019). A dataset from Shanxi Province is used to identify the risk of each
patient's at four states low/medium/high/attack and provide the state
transition tendency through a SHAP DeepExplainer. To improve the accuracy on an
imbalance sample set, the Quadratic Interactive Deep Neural Network (QIDNN)
model is first proposed by flexible selecting and appending of quadratic
interactive features. The experimental results showed that the QIDNN model with
7 interactive features achieve the state-of-art accuracy $83.25\%$. Blood
pressure, physical inactivity, smoking, weight and total cholesterol are the
top five important features. Then, for the sake of high recall on the most
urgent state, attack state, the stroke occurrence prediction is taken as an
auxiliary objective to benefit from multi-objective optimization. The
prediction accuracy was promoted, meanwhile the recall of the attack state was
improved by $24.9\%$ (to $84.83\%$) compared to QIDNN (from $67.93\%$) with
same features. The prediction model and analysis tool in this paper not only
gave the theoretical optimized prediction method, but also provided the
attribution explanation of risk states and transition direction of each
patient, which provided a favorable tool for doctors to analyze and diagnose
the disease.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_i/0/1/0/all/0/1"&gt;ing Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yiyang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Junjie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Huaxiong Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xiaoshuang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shixin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Malware Classification Using Transfer Learning. (arXiv:2107.13743v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.13743</id>
        <link href="http://arxiv.org/abs/2107.13743"/>
        <updated>2021-07-30T02:13:29.853Z</updated>
        <summary type="html"><![CDATA[With the rapid growth of the number of devices on the Internet, malware poses
a threat not only to the affected devices but also their ability to use said
devices to launch attacks on the Internet ecosystem. Rapid malware
classification is an important tools to combat that threat. One of the
successful approaches to classification is based on malware images and deep
learning. While many deep learning architectures are very accurate they usually
take a long time to train. In this work we perform experiments on multiple well
known, pre-trained, deep network architectures in the context of transfer
learning. We show that almost all them classify malware accurately with a very
short training period.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farhat_H/0/1/0/all/0/1"&gt;Hikmat Farhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rammouz_V/0/1/0/all/0/1"&gt;Veronica Rammouz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning the temporal evolution of multivariate densities via normalizing flows. (arXiv:2107.13735v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.13735</id>
        <link href="http://arxiv.org/abs/2107.13735"/>
        <updated>2021-07-30T02:13:29.848Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a method to learn probability distributions using
sample path data from stochastic differential equations. Specifically, we
consider temporally evolving probability distributions (e.g., those produced by
integrating local or nonlocal Fokker-Planck equations). We analyze this
evolution through machine learning assisted construction of a time-dependent
mapping that takes a reference distribution (say, a Gaussian) to each and every
instance of our evolving distribution. If the reference distribution is the
initial condition of a Fokker-Planck equation, what we learn is the time-T map
of the corresponding solution. Specifically, the learned map is a normalizing
flow that deforms the support of the reference density to the support of each
and every density snapshot in time. We demonstrate that this approach can learn
solutions to non-local Fokker-Planck equations, such as those arising in
systems driven by both Brownian and L\'evy noise. We present examples with two-
and three-dimensional, uni- and multimodal distributions to validate the
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yubin Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Maulik_R/0/1/0/all/0/1"&gt;Romit Maulik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gao_T/0/1/0/all/0/1"&gt;Ting Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dietrich_F/0/1/0/all/0/1"&gt;Felix Dietrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kevrekidis_I/0/1/0/all/0/1"&gt;Ioannis G. Kevrekidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Duan_J/0/1/0/all/0/1"&gt;Jinqiao Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Experience Report on Machine Learning Reproducibility: Guidance for Practitioners and TensorFlow Model Garden Contributors. (arXiv:2107.00821v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00821</id>
        <link href="http://arxiv.org/abs/2107.00821"/>
        <updated>2021-07-30T02:13:29.842Z</updated>
        <summary type="html"><![CDATA[Machine learning techniques are becoming a fundamental tool for scientific
and engineering progress. These techniques are applied in contexts as diverse
as astronomy and spam filtering. However, correctly applying these techniques
requires careful engineering. Much attention has been paid to the technical
potential; relatively little attention has been paid to the software
engineering process required to bring research-based machine learning
techniques into practical utility. Technology companies have supported the
engineering community through machine learning frameworks such as TensorFLow
and PyTorch, but the details of how to engineer complex machine learning models
in these frameworks have remained hidden.

To promote best practices within the engineering community, academic
institutions and Google have partnered to launch a Special Interest Group on
Machine Learning Models (SIGMODELS) whose goal is to develop exemplary
implementations of prominent machine learning models in community locations
such as the TensorFlow Model Garden (TFMG). The purpose of this report is to
define a process for reproducing a state-of-the-art machine learning model at a
level of quality suitable for inclusion in the TFMG. We define the engineering
process and elaborate on each step, from paper analysis to model release. We
report on our experiences implementing the YOLO model family with a team of 26
student researchers, share the tools we developed, and describe the lessons we
learned along the way.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Banna_V/0/1/0/all/0/1"&gt;Vishnu Banna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chinnakotla_A/0/1/0/all/0/1"&gt;Akhil Chinnakotla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zhengxin Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vegesana_A/0/1/0/all/0/1"&gt;Anirudh Vegesana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vivek_N/0/1/0/all/0/1"&gt;Naveen Vivek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnappa_K/0/1/0/all/0/1"&gt;Kruthi Krishnappa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1"&gt;Wenxin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yung-Hsiang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thiruvathukal_G/0/1/0/all/0/1"&gt;George K. Thiruvathukal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1"&gt;James C. Davis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Asymmetric Loss For Multi-Label Classification. (arXiv:2009.14119v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.14119</id>
        <link href="http://arxiv.org/abs/2009.14119"/>
        <updated>2021-07-30T02:13:29.837Z</updated>
        <summary type="html"><![CDATA[In a typical multi-label setting, a picture contains on average few positive
labels, and many negative ones. This positive-negative imbalance dominates the
optimization process, and can lead to under-emphasizing gradients from positive
labels during training, resulting in poor accuracy. In this paper, we introduce
a novel asymmetric loss ("ASL"), which operates differently on positive and
negative samples. The loss enables to dynamically down-weights and
hard-thresholds easy negative samples, while also discarding possibly
mislabeled samples. We demonstrate how ASL can balance the probabilities of
different samples, and how this balancing is translated to better mAP scores.
With ASL, we reach state-of-the-art results on multiple popular multi-label
datasets: MS-COCO, Pascal-VOC, NUS-WIDE and Open Images. We also demonstrate
ASL applicability for other tasks, such as single-label classification and
object detection. ASL is effective, easy to implement, and does not increase
the training time or complexity.

Implementation is available at: https://github.com/Alibaba-MIIL/ASL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1"&gt;Emanuel Ben-Baruch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1"&gt;Tal Ridnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamir_N/0/1/0/all/0/1"&gt;Nadav Zamir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1"&gt;Asaf Noy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Friedman_I/0/1/0/all/0/1"&gt;Itamar Friedman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Protter_M/0/1/0/all/0/1"&gt;Matan Protter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zelnik_Manor_L/0/1/0/all/0/1"&gt;Lihi Zelnik-Manor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Need and Status of Sea Turtle Conservation and Survey of Associated Computer Vision Advances. (arXiv:2107.14061v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14061</id>
        <link href="http://arxiv.org/abs/2107.14061"/>
        <updated>2021-07-30T02:13:29.830Z</updated>
        <summary type="html"><![CDATA[For over hundreds of millions of years, sea turtles and their ancestors have
swum in the vast expanses of the ocean. They have undergone a number of
evolutionary changes, leading to speciation and sub-speciation. However, in the
past few decades, some of the most notable forces driving the genetic variance
and population decline have been global warming and anthropogenic impact
ranging from large-scale poaching, collecting turtle eggs for food, besides
dumping trash including plastic waste into the ocean. This leads to severe
detrimental effects in the sea turtle population, driving them to extinction.
This research focusses on the forces causing the decline in sea turtle
population, the necessity for the global conservation efforts along with its
successes and failures, followed by an in-depth analysis of the modern advances
in detection and recognition of sea turtles, involving Machine Learning and
Computer Vision systems, aiding the conservation efforts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1"&gt;Aditya Jyoti Paul&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gated recurrent units viewed through the lens of continuous time dynamical systems. (arXiv:1906.01005v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.01005</id>
        <link href="http://arxiv.org/abs/1906.01005"/>
        <updated>2021-07-30T02:13:29.816Z</updated>
        <summary type="html"><![CDATA[Gated recurrent units (GRUs) are specialized memory elements for building
recurrent neural networks. Despite their incredible success on various tasks,
including extracting dynamics underlying neural data, little is understood
about the specific dynamics representable in a GRU network. As a result, it is
both difficult to know a priori how successful a GRU network will perform on a
given task, and also their capacity to mimic the underlying behavior of their
biological counterparts. Using a continuous time analysis, we gain intuition on
the inner workings of GRU networks. We restrict our presentation to low
dimensions, allowing for a comprehensive visualization. We found a surprisingly
rich repertoire of dynamical features that includes stable limit cycles
(nonlinear oscillations), multi-stable dynamics with various topologies, and
homoclinic bifurcations. At the same time we were unable to train GRU networks
to produce continuous attractors, which are hypothesized to exist in biological
neural networks. We contextualize the usefulness of different kinds of observed
dynamics and support our claims experimentally.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_I/0/1/0/all/0/1"&gt;Ian D. Jordan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sokol_P/0/1/0/all/0/1"&gt;Piotr Aleksander Sokol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_I/0/1/0/all/0/1"&gt;Il Memming Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Gaussian DAGs from Network Data. (arXiv:1905.10848v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.10848</id>
        <link href="http://arxiv.org/abs/1905.10848"/>
        <updated>2021-07-30T02:13:29.810Z</updated>
        <summary type="html"><![CDATA[Structural learning of directed acyclic graphs (DAGs) or Bayesian networks
has been studied extensively under the assumption that data are independent. We
propose a new Gaussian DAG model for dependent data which assumes the
observations are correlated according to an undirected network. Under this
model, we develop a method to estimate the DAG structure given a topological
ordering of the nodes. The proposed method jointly estimates the Bayesian
network and the correlations among observations by optimizing a scoring
function based on penalized likelihood. We show that under some mild
conditions, the proposed method produces consistent estimators after one
iteration. Extensive numerical experiments also demonstrate that by jointly
estimating the DAG structure and the sample correlation, our method achieves
much higher accuracy in structure learning. When the node ordering is unknown,
through experiments on synthetic and real data, we show that our algorithm can
be used to estimate the correlations between samples, with which we can
de-correlate the dependent data to significantly improve the performance of
classical DAG learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Li_H/0/1/0/all/0/1"&gt;Hangjian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Padilla_O/0/1/0/all/0/1"&gt;Oscar Hernan Madrid Padilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_Q/0/1/0/all/0/1"&gt;Qing Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed Deep Convolutional Neural Networks for the Internet-of-Things. (arXiv:1908.01656v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.01656</id>
        <link href="http://arxiv.org/abs/1908.01656"/>
        <updated>2021-07-30T02:13:29.804Z</updated>
        <summary type="html"><![CDATA[Severe constraints on memory and computation characterizing the
Internet-of-Things (IoT) units may prevent the execution of Deep Learning
(DL)-based solutions, which typically demand large memory and high processing
load. In order to support a real-time execution of the considered DL model at
the IoT unit level, DL solutions must be designed having in mind constraints on
memory and processing capability exposed by the chosen IoT technology. In this
paper, we introduce a design methodology aiming at allocating the execution of
Convolutional Neural Networks (CNNs) on a distributed IoT application. Such a
methodology is formalized as an optimization problem where the latency between
the data-gathering phase and the subsequent decision-making one is minimized,
within the given constraints on memory and processing load at the units level.
The methodology supports multiple sources of data as well as multiple CNNs in
execution on the same IoT system allowing the design of CNN-based applications
demanding autonomy, low decision-latency, and high Quality-of-Service.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Disabato_S/0/1/0/all/0/1"&gt;Simone Disabato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roveri_M/0/1/0/all/0/1"&gt;Manuel Roveri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alippi_C/0/1/0/all/0/1"&gt;Cesare Alippi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deterministic tensor completion with hypergraph expanders. (arXiv:1910.10692v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.10692</id>
        <link href="http://arxiv.org/abs/1910.10692"/>
        <updated>2021-07-30T02:13:29.798Z</updated>
        <summary type="html"><![CDATA[We provide a novel analysis of low-rank tensor completion based on hypergraph
expanders. As a proxy for rank, we minimize the max-quasinorm of the tensor,
which generalizes the max-norm for matrices. Our analysis is deterministic and
shows that the number of samples required to approximately recover an order-$t$
tensor with at most $n$ entries per dimension is linear in $n$, under the
assumption that the rank and order of the tensor are $O(1)$. As steps in our
proof, we find a new expander mixing lemma for a $t$-partite, $t$-uniform
regular hypergraph model, and prove several new properties about tensor
max-quasinorm. To the best of our knowledge, this is the first deterministic
analysis of tensor completion. We develop a practical algorithm that solves a
relaxed version of the max-quasinorm minimization problem, and we demonstrate
its efficacy with numerical experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Harris_K/0/1/0/all/0/1"&gt;Kameron Decker Harris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yizhe Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Open-World Entity Segmentation. (arXiv:2107.14228v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14228</id>
        <link href="http://arxiv.org/abs/2107.14228"/>
        <updated>2021-07-30T02:13:29.792Z</updated>
        <summary type="html"><![CDATA[We introduce a new image segmentation task, termed Entity Segmentation (ES)
with the aim to segment all visual entities in an image without considering
semantic category labels. It has many practical applications in image
manipulation/editing where the segmentation mask quality is typically crucial
but category labels are less important. In this setting, all
semantically-meaningful segments are equally treated as categoryless entities
and there is no thing-stuff distinction. Based on our unified entity
representation, we propose a center-based entity segmentation framework with
two novel modules to improve mask quality. Experimentally, both our new task
and framework demonstrate superior advantages as against existing work. In
particular, ES enables the following: (1) merging multiple datasets to form a
large training set without the need to resolve label conflicts; (2) any model
trained on one dataset can generalize exceptionally well to other datasets with
unseen domains. Our code is made publicly available at
https://github.com/dvlab-research/Entity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1"&gt;Lu Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuen_J/0/1/0/all/0/1"&gt;Jason Kuen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jiuxiang Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hengshuang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhe Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1"&gt;Jiaya Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Social Processes: Self-Supervised Forecasting of Nonverbal Cues in Social Conversations. (arXiv:2107.13576v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13576</id>
        <link href="http://arxiv.org/abs/2107.13576"/>
        <updated>2021-07-30T02:13:29.711Z</updated>
        <summary type="html"><![CDATA[The default paradigm for the forecasting of human behavior in social
conversations is characterized by top-down approaches. These involve
identifying predictive relationships between low level nonverbal cues and
future semantic events of interest (e.g. turn changes, group leaving). A common
hurdle however, is the limited availability of labeled data for supervised
learning. In this work, we take the first step in the direction of a bottom-up
self-supervised approach in the domain. We formulate the task of Social Cue
Forecasting to leverage the larger amount of unlabeled low-level behavior cues,
and characterize the modeling challenges involved. To address these, we take a
meta-learning approach and propose the Social Process (SP) models--socially
aware sequence-to-sequence (Seq2Seq) models within the Neural Process (NP)
family. SP models learn extractable representations of non-semantic future cues
for each participant, while capturing global uncertainty by jointly reasoning
about the future for all members of the group. Evaluation on synthesized and
real-world behavior data shows that our SP models achieve higher log-likelihood
than the NP baselines, and also highlights important considerations for
applying such techniques within the domain of social human interactions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raman_C/0/1/0/all/0/1"&gt;Chirag Raman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hung_H/0/1/0/all/0/1"&gt;Hayley Hung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loog_M/0/1/0/all/0/1"&gt;Marco Loog&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Amplitude Mean of Functional Data on $\mathbb{S}^2$. (arXiv:2107.13721v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.13721</id>
        <link href="http://arxiv.org/abs/2107.13721"/>
        <updated>2021-07-30T02:13:29.706Z</updated>
        <summary type="html"><![CDATA[Mainfold-valued functional data analysis (FDA) recently becomes an active
area of research motivated by the raising availability of trajectories or
longitudinal data observed on non-linear manifolds. The challenges of analyzing
such data comes from many aspects, including infinite dimensionality and
nonlinearity, as well as time domain or phase variability. In this paper, we
study the amplitude part of manifold-valued functions on $\S^2$, which is
invariant to random time warping or re-parameterization of the function.
Utilizing the nice geometry of $\S^2$, we develop a set of efficient and
accurate tools for temporal alignment of functions, geodesic and sample mean
calculation. At the heart of these tools, they rely on gradient descent
algorithms with carefully derived gradients. We show the advantages of these
newly developed tools over its competitors with extensive simulations and real
data, and demonstrate the importance of considering the amplitude part of
functions instead of mixing it with phase variability in mainfold-valued FDA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhengwu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Saparbayeva_B/0/1/0/all/0/1"&gt;Bayan Saparbayeva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalizing Fairness: Discovery and Mitigation of Unknown Sensitive Attributes. (arXiv:2107.13625v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13625</id>
        <link href="http://arxiv.org/abs/2107.13625"/>
        <updated>2021-07-30T02:13:29.700Z</updated>
        <summary type="html"><![CDATA[When deploying artificial intelligence (AI) in the real world, being able to
trust the operation of the AI by characterizing how it performs is an
ever-present and important topic. An important and still largely unexplored
task in this characterization is determining major factors within the real
world that affect the AI's behavior, such as weather conditions or lighting,
and either a) being able to give justification for why it may have failed or b)
eliminating the influence the factor has. Determining these sensitive factors
heavily relies on collected data that is diverse enough to cover numerous
combinations of these factors, which becomes more onerous when having many
potential sensitive factors or operating in complex environments. This paper
investigates methods that discover and separate out individual semantic
sensitive factors from a given dataset to conduct this characterization as well
as addressing mitigation of these factors' sensitivity. We also broaden
remediation of fairness, which normally only addresses socially relevant
factors, and widen it to deal with the desensitization of AI with regard to all
possible aspects of variation in the domain. The proposed methods which
discover these major factors reduce the potentially onerous demands of
collecting a sufficiently diverse dataset. In experiments using the road sign
(GTSRB) and facial imagery (CelebA) datasets, we show the promise of using this
scheme to perform this characterization and remediation and demonstrate that
our approach outperforms state of the art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1"&gt;William Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1"&gt;Philippe Burlina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ranking Micro-Influencers: a Novel Multi-Task Learning and Interpretable Framework. (arXiv:2107.13943v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13943</id>
        <link href="http://arxiv.org/abs/2107.13943"/>
        <updated>2021-07-30T02:13:29.659Z</updated>
        <summary type="html"><![CDATA[With the rise in use of social media to promote branded products, the demand
for effective influencer marketing has increased. Brands are looking for
improved ways to identify valuable influencers among a vast catalogue; this is
even more challenging with "micro-influencers", which are more affordable than
mainstream ones but difficult to discover. In this paper, we propose a novel
multi-task learning framework to improve the state of the art in
micro-influencer ranking based on multimedia content. Moreover, since the
visual congruence between a brand and influencer has been shown to be good
measure of compatibility, we provide an effective visual method for
interpreting our models' decisions, which can also be used to inform brands'
media strategies. We compare with the current state-of-the-art on a recently
constructed public dataset and we show significant improvement both in terms of
accuracy and model complexity. The techniques for ranking and interpretation
presented in this work can be generalised to arbitrary multimedia ranking tasks
that have datasets with a similar structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elwood_A/0/1/0/all/0/1"&gt;Adam Elwood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gasparin_A/0/1/0/all/0/1"&gt;Alberto Gasparin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rozza_A/0/1/0/all/0/1"&gt;Alessandro Rozza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Addressing materials' microstructure diversity using transfer learning. (arXiv:2107.13841v1 [cond-mat.mtrl-sci])]]></title>
        <id>http://arxiv.org/abs/2107.13841</id>
        <link href="http://arxiv.org/abs/2107.13841"/>
        <updated>2021-07-30T02:13:29.652Z</updated>
        <summary type="html"><![CDATA[Materials' microstructures are signatures of their alloying composition and
processing history. Therefore, microstructures exist in a wide variety. As
materials become increasingly complex to comply with engineering demands,
advanced computer vision (CV) approaches such as deep learning (DL) inevitably
gain relevance for quantifying microstrucutures' constituents from micrographs.
While DL can outperform classical CV techniques for many tasks, shortcomings
are poor data efficiency and generalizability across datasets. This is
inherently in conflict with the expense associated with annotating materials
data through experts and extensive materials diversity. To tackle poor domain
generalizability and the lack of labeled data simultaneously, we propose to
apply a sub-class of transfer learning methods called unsupervised domain
adaptation (UDA). These algorithms address the task of finding domain-invariant
features when supplied with annotated source data and unannotated target data,
such that performance on the latter distribution is optimized despite the
absence of annotations. Exemplarily, this study is conducted on a lath-shaped
bainite segmentation task in complex phase steel micrographs. Here, the domains
to bridge are selected to be different metallographic specimen preparations
(surface etchings) and distinct imaging modalities. We show that a
state-of-the-art UDA approach surpasses the na\"ive application of source
domain trained models on the target domain (generalization baseline) to a large
extent. This holds true independent of the domain shift, despite using little
data, and even when the baseline models were pre-trained or employed data
augmentation. Through UDA, mIoU was improved over generalization baselines from
82.2%, 61.0%, 49.7% to 84.7%, 67.3%, 73.3% on three target datasets,
respectively. This underlines this techniques' potential to cope with materials
variance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Goetz_A/0/1/0/all/0/1"&gt;Aur&amp;#xe8;le Goetz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Durmaz_A/0/1/0/all/0/1"&gt;Ali Riza Durmaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Muller_M/0/1/0/all/0/1"&gt;Martin M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Thomas_A/0/1/0/all/0/1"&gt;Akhil Thomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Britz_D/0/1/0/all/0/1"&gt;Dominik Britz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Kerfriden_P/0/1/0/all/0/1"&gt;Pierre Kerfriden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Eberl_C/0/1/0/all/0/1"&gt;Chris Eberl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Point-Cloud Deep Learning of Porous Media for Permeability Prediction. (arXiv:2107.14038v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.14038</id>
        <link href="http://arxiv.org/abs/2107.14038"/>
        <updated>2021-07-30T02:13:29.632Z</updated>
        <summary type="html"><![CDATA[We propose a novel deep learning framework for predicting permeability of
porous media from their digital images. Unlike convolutional neural networks,
instead of feeding the whole image volume as inputs to the network, we model
the boundary between solid matrix and pore spaces as point clouds and feed them
as inputs to a neural network based on the PointNet architecture. This approach
overcomes the challenge of memory restriction of graphics processing units and
its consequences on the choice of batch size, and convergence. Compared to
convolutional neural networks, the proposed deep learning methodology provides
freedom to select larger batch sizes, due to reducing significantly the size of
network inputs. Specifically, we use the classification branch of PointNet and
adjust it for a regression task. As a test case, two and three dimensional
synthetic digital rock images are considered. We investigate the effect of
different components of our neural network on its performance. We compare our
deep learning strategy with a convolutional neural network from various
perspectives, specifically for maximum possible batch size. We inspect the
generalizability of our network by predicting the permeability of real-world
rock samples as well as synthetic digital rocks that are statistically
different from the samples used during training. The network predicts the
permeability of digital rocks a few thousand times faster than a Lattice
Boltzmann solver with a high level of prediction accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kashefi_A/0/1/0/all/0/1"&gt;Ali Kashefi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mukerji_T/0/1/0/all/0/1"&gt;Tapan Mukerji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tianshou: a Highly Modularized Deep Reinforcement Learning Library. (arXiv:2107.14171v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14171</id>
        <link href="http://arxiv.org/abs/2107.14171"/>
        <updated>2021-07-30T02:13:29.614Z</updated>
        <summary type="html"><![CDATA[We present Tianshou, a highly modularized python library for deep
reinforcement learning (DRL) that uses PyTorch as its backend. Tianshou aims to
provide building blocks to replicate common RL experiments and has officially
supported more than 15 classic algorithms succinctly. To facilitate related
research and prove Tianshou's reliability, we release Tianshou's benchmark of
MuJoCo environments, covering 9 classic algorithms and 9/13 Mujoco tasks with
state-of-the-art performance. We open-sourced Tianshou at
https://github.com/thu-ml/tianshou/, which has received over 3k stars and
become one of the most popular PyTorch-based DRL libraries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weng_J/0/1/0/all/0/1"&gt;Jiayi Weng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Huayu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1"&gt;Dong Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_K/0/1/0/all/0/1"&gt;Kaichao You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duburcq_A/0/1/0/all/0/1"&gt;Alexis Duburcq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Minghao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hang Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Intelligence Hybrid Deep Learning Model for Groundwater Level Prediction Using MLP-ADAM. (arXiv:2107.13870v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13870</id>
        <link href="http://arxiv.org/abs/2107.13870"/>
        <updated>2021-07-30T02:13:29.589Z</updated>
        <summary type="html"><![CDATA[Groundwater is the largest storage of freshwater resources, which serves as
the major inventory for most of the human consumption through agriculture,
industrial, and domestic water supply. In the fields of hydrological, some
researchers applied a neural network to forecast rainfall intensity in
space-time and introduced the advantages of neural networks compared to
numerical models. Then, many researches have been conducted applying
data-driven models. Some of them extended an Artificial Neural Networks (ANNs)
model to forecast groundwater level in semi-confined glacial sand and gravel
aquifer under variable state, pumping extraction and climate conditions with
significant accuracy. In this paper, a multi-layer perceptron is applied to
simulate groundwater level. The adaptive moment estimation optimization
algorithm is also used to this matter. The root mean squared error, mean
absolute error, mean squared error and the coefficient of determination ( ) are
used to evaluate the accuracy of the simulated groundwater level. Total value
of and RMSE are 0.9458 and 0.7313 respectively which are obtained from the
model output. Results indicate that deep learning algorithms can demonstrate a
high accuracy prediction. Although the optimization of parameters is
insignificant in numbers, but due to the value of time in modelling setup, it
is highly recommended to apply an optimization algorithm in modelling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zarafshan_P/0/1/0/all/0/1"&gt;Pejman Zarafshan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javadi_S/0/1/0/all/0/1"&gt;Saman Javadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roozbahani_A/0/1/0/all/0/1"&gt;Abbas Roozbahani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashemy_S/0/1/0/all/0/1"&gt;Seyed Mehdi Hashemy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zarafshan_P/0/1/0/all/0/1"&gt;Payam Zarafshan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Etezadi_H/0/1/0/all/0/1"&gt;Hamed Etezadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lyapunov-based uncertainty-aware safe reinforcement learning. (arXiv:2107.13944v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13944</id>
        <link href="http://arxiv.org/abs/2107.13944"/>
        <updated>2021-07-30T02:13:29.584Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning (RL) has shown a promising performance in learning
optimal policies for a variety of sequential decision-making tasks. However, in
many real-world RL problems, besides optimizing the main objectives, the agent
is expected to satisfy a certain level of safety (e.g., avoiding collisions in
autonomous driving). While RL problems are commonly formalized as Markov
decision processes (MDPs), safety constraints are incorporated via constrained
Markov decision processes (CMDPs). Although recent advances in safe RL have
enabled learning safe policies in CMDPs, these safety requirements should be
satisfied during both training and in the deployment process. Furthermore, it
is shown that in memory-based and partially observable environments, these
methods fail to maintain safety over unseen out-of-distribution observations.
To address these limitations, we propose a Lyapunov-based uncertainty-aware
safe RL model. The introduced model adopts a Lyapunov function that converts
trajectory-based constraints to a set of local linear constraints. Furthermore,
to ensure the safety of the agent in highly uncertain environments, an
uncertainty quantification method is developed that enables identifying
risk-averse actions through estimating the probability of constraint
violations. Moreover, a Transformers model is integrated to provide the agent
with memory to process long time horizons of information via the self-attention
mechanism. The proposed model is evaluated in grid-world navigation tasks where
safety is defined as avoiding static and dynamic obstacles in fully and
partially observable environments. The results of these experiments show a
significant improvement in the performance of the agent both in achieving
optimality and satisfying safety constraints.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jeddi_A/0/1/0/all/0/1"&gt;Ashkan B. Jeddi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dehghani_N/0/1/0/all/0/1"&gt;Nariman L. Dehghani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafieezadeh_A/0/1/0/all/0/1"&gt;Abdollah Shafieezadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[To Boost or not to Boost: On the Limits of Boosted Neural Networks. (arXiv:2107.13600v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13600</id>
        <link href="http://arxiv.org/abs/2107.13600"/>
        <updated>2021-07-30T02:13:29.572Z</updated>
        <summary type="html"><![CDATA[Boosting is a method for finding a highly accurate hypothesis by linearly
combining many ``weak" hypotheses, each of which may be only moderately
accurate. Thus, boosting is a method for learning an ensemble of classifiers.
While boosting has been shown to be very effective for decision trees, its
impact on neural networks has not been extensively studied. We prove one
important difference between sums of decision trees compared to sums of
convolutional neural networks (CNNs) which is that a sum of decision trees
cannot be represented by a single decision tree with the same number of
parameters while a sum of CNNs can be represented by a single CNN. Next, using
standard object recognition datasets, we verify experimentally the well-known
result that a boosted ensemble of decision trees usually generalizes much
better on testing data than a single decision tree with the same number of
parameters. In contrast, using the same datasets and boosting algorithms, our
experiments show the opposite to be true when using neural networks (both CNNs
and multilayer perceptrons (MLPs)). We find that a single neural network
usually generalizes better than a boosted ensemble of smaller neural networks
with the same total number of parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rambhatla_S/0/1/0/all/0/1"&gt;Sai Saketh Rambhatla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jones_M/0/1/0/all/0/1"&gt;Michael Jones&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1"&gt;Rama Chellappa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coarse-to-Fine for Sim-to-Real: Sub-Millimetre Precision Across Wide Task Spaces. (arXiv:2105.11283v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11283</id>
        <link href="http://arxiv.org/abs/2105.11283"/>
        <updated>2021-07-30T02:13:29.558Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the problem of zero-shot sim-to-real when the task
requires both highly precise control with sub-millimetre error tolerance, and
wide task space generalisation. Our framework involves a coarse-to-fine
controller, where trajectories begin with classical motion planning using
ICP-based pose estimation, and transition to a learned end-to-end controller
which maps images to actions and is trained in simulation with domain
randomisation. In this way, we achieve precise control whilst also generalising
the controller across wide task spaces, and keeping the robustness of
vision-based, end-to-end control. Real-world experiments on a range of
different tasks show that, by exploiting the best of both worlds, our framework
significantly outperforms purely motion planning methods, and purely
learning-based methods. Furthermore, we answer a range of questions on best
practices for precise sim-to-real transfer, such as how different image sensor
modalities and image feature representations perform.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Valassakis_E/0/1/0/all/0/1"&gt;Eugene Valassakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palo_N/0/1/0/all/0/1"&gt;Norman Di Palo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1"&gt;Edward Johns&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Co-learning: Challenges, Applications with Datasets, Recent Advances and Future Directions. (arXiv:2107.13782v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13782</id>
        <link href="http://arxiv.org/abs/2107.13782"/>
        <updated>2021-07-30T02:13:29.552Z</updated>
        <summary type="html"><![CDATA[Multimodal deep learning systems which employ multiple modalities like text,
image, audio, video, etc., are showing better performance in comparison with
individual modalities (i.e., unimodal) systems. Multimodal machine learning
involves multiple aspects: representation, translation, alignment, fusion, and
co-learning. In the current state of multimodal machine learning, the
assumptions are that all modalities are present, aligned, and noiseless during
training and testing time. However, in real-world tasks, typically, it is
observed that one or more modalities are missing, noisy, lacking annotated
data, have unreliable labels, and are scarce in training or testing and or
both. This challenge is addressed by a learning paradigm called multimodal
co-learning. The modeling of a (resource-poor) modality is aided by exploiting
knowledge from another (resource-rich) modality using transfer of knowledge
between modalities, including their representations and predictive models.
Co-learning being an emerging area, there are no dedicated reviews explicitly
focusing on all challenges addressed by co-learning. To that end, in this work,
we provide a comprehensive survey on the emerging area of multimodal
co-learning that has not been explored in its entirety yet. We review
implementations that overcome one or more co-learning challenges without
explicitly considering them as co-learning challenges. We present the
comprehensive taxonomy of multimodal co-learning based on the challenges
addressed by co-learning and associated implementations. The various techniques
employed to include the latest ones are reviewed along with some of the
applications and datasets. Our final goal is to discuss challenges and
perspectives along with the important ideas and directions for future work that
we hope to be beneficial for the entire research community focusing on this
exciting domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahate_A/0/1/0/all/0/1"&gt;Anil Rahate&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walambe_R/0/1/0/all/0/1"&gt;Rahee Walambe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramanna_S/0/1/0/all/0/1"&gt;Sheela Ramanna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kotecha_K/0/1/0/all/0/1"&gt;Ketan Kotecha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning and Deep Learning Methods for Building Intelligent Systems in Medicine and Drug Discovery: A Comprehensive Survey. (arXiv:2107.14037v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14037</id>
        <link href="http://arxiv.org/abs/2107.14037"/>
        <updated>2021-07-30T02:13:29.521Z</updated>
        <summary type="html"><![CDATA[With the advancements in computer technology, there is a rapid development of
intelligent systems to understand the complex relationships in data to make
predictions and classifications. Artificail Intelligence based framework is
rapidly revolutionizing the healthcare industry. These intelligent systems are
built with machine learning and deep learning based robust models for early
diagnosis of diseases and demonstrates a promising supplementary diagnostic
method for frontline clinical doctors and surgeons. Machine Learning and Deep
Learning based systems can streamline and simplify the steps involved in
diagnosis of diseases from clinical and image-based data, thus providing
significant clinician support and workflow optimization. They mimic human
cognition and are even capable of diagnosing diseases that cannot be diagnosed
with human intelligence. This paper focuses on the survey of machine learning
and deep learning applications in across 16 medical specialties, namely Dental
medicine, Haematology, Surgery, Cardiology, Pulmonology, Orthopedics,
Radiology, Oncology, General medicine, Psychiatry, Endocrinology, Neurology,
Dermatology, Hepatology, Nephrology, Ophthalmology, and Drug discovery. In this
paper along with the survey, we discuss the advancements of medical practices
with these systems and also the impact of these systems on medical
professionals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chowdary_G/0/1/0/all/0/1"&gt;G Jignesh Chowdary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+G_S/0/1/0/all/0/1"&gt;Suganya G&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+M_P/0/1/0/all/0/1"&gt;Premalatha M&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Y_A/0/1/0/all/0/1"&gt;Asnath Victy Phamila Y&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+K_K/0/1/0/all/0/1"&gt;Karunamurthy K&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[QuPeD: Quantized Personalization via Distillation with Applications to Federated Learning. (arXiv:2107.13892v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13892</id>
        <link href="http://arxiv.org/abs/2107.13892"/>
        <updated>2021-07-30T02:13:29.461Z</updated>
        <summary type="html"><![CDATA[Traditionally, federated learning (FL) aims to train a single global model
while collaboratively using multiple clients and a server. Two natural
challenges that FL algorithms face are heterogeneity in data across clients and
collaboration of clients with {\em diverse resources}. In this work, we
introduce a \textit{quantized} and \textit{personalized} FL algorithm QuPeD
that facilitates collective (personalized model compression) training via
\textit{knowledge distillation} (KD) among clients who have access to
heterogeneous data and resources. For personalization, we allow clients to
learn \textit{compressed personalized models} with different quantization
parameters and model dimensions/structures. Towards this, first we propose an
algorithm for learning quantized models through a relaxed optimization problem,
where quantization values are also optimized over. When each client
participating in the (federated) learning process has different requirements
for the compressed model (both in model dimension and precision), we formulate
a compressed personalization framework by introducing knowledge distillation
loss for local client objectives collaborating through a global model. We
develop an alternating proximal gradient update for solving this compressed
personalization problem, and analyze its convergence properties. Numerically,
we validate that QuPeD outperforms competing personalized FL methods, FedAvg,
and local training of clients in various heterogeneous settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozkara_K/0/1/0/all/0/1"&gt;Kaan Ozkara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1"&gt;Navjot Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Data_D/0/1/0/all/0/1"&gt;Deepesh Data&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diggavi_S/0/1/0/all/0/1"&gt;Suhas Diggavi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ProtoTransformer: A Meta-Learning Approach to Providing Student Feedback. (arXiv:2107.14035v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2107.14035</id>
        <link href="http://arxiv.org/abs/2107.14035"/>
        <updated>2021-07-30T02:13:29.453Z</updated>
        <summary type="html"><![CDATA[High-quality computer science education is limited by the difficulty of
providing instructor feedback to students at scale. While this feedback could
in principle be automated, supervised approaches to predicting the correct
feedback are bottlenecked by the intractability of annotating large quantities
of student code. In this paper, we instead frame the problem of providing
feedback as few-shot classification, where a meta-learner adapts to give
feedback to student code on a new programming question from just a few examples
annotated by instructors. Because data for meta-training is limited, we propose
a number of amendments to the typical few-shot learning framework, including
task augmentation to create synthetic tasks, and additional side information to
build stronger priors about each task. These additions are combined with a
transformer architecture to embed discrete sequences (e.g. code) to a
prototypical representation of a feedback class label. On a suite of few-shot
natural language processing tasks, we match or outperform state-of-the-art
performance. Then, on a collection of student solutions to exam questions from
an introductory university course, we show that our approach reaches an average
precision of 88% on unseen questions, surpassing the 82% precision of teaching
assistants. Our approach was successfully deployed to deliver feedback to
16,000 student exam-solutions in a programming course offered by a tier 1
university. This is, to the best of our knowledge, the first successful
deployment of a machine learning based feedback to open-ended student code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1"&gt;Mike Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1"&gt;Noah Goodman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piech_C/0/1/0/all/0/1"&gt;Chris Piech&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1"&gt;Chelsea Finn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structure and Performance of Fully Connected Neural Networks: Emerging Complex Network Properties. (arXiv:2107.14062v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14062</id>
        <link href="http://arxiv.org/abs/2107.14062"/>
        <updated>2021-07-30T02:13:29.434Z</updated>
        <summary type="html"><![CDATA[Understanding the behavior of Artificial Neural Networks is one of the main
topics in the field recently, as black-box approaches have become usual since
the widespread of deep learning. Such high-dimensional models may manifest
instabilities and weird properties that resemble complex systems. Therefore, we
propose Complex Network (CN) techniques to analyze the structure and
performance of fully connected neural networks. For that, we build a dataset
with 4 thousand models and their respective CN properties. They are employed in
a supervised classification setup considering four vision benchmarks. Each
neural network is approached as a weighted and undirected graph of neurons and
synapses, and centrality measures are computed after training. Results show
that these measures are highly related to the network classification
performance. We also propose the concept of Bag-Of-Neurons (BoN), a CN-based
approach for finding topological signatures linking similar neurons. Results
suggest that six neuronal types emerge in such networks, independently of the
target domain, and are distributed differently according to classification
accuracy. We also tackle specific CN properties related to performance, such as
higher subgraph centrality on lower-performing models. Our findings suggest
that CN properties play a critical role in the performance of fully connected
neural networks, with topological patterns emerging independently on a wide
range of models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Scabini_L/0/1/0/all/0/1"&gt;Leonardo F. S. Scabini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruno_O/0/1/0/all/0/1"&gt;Odemir M. Bruno&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HAFLO: GPU-Based Acceleration for Federated Logistic Regression. (arXiv:2107.13797v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13797</id>
        <link href="http://arxiv.org/abs/2107.13797"/>
        <updated>2021-07-30T02:13:29.428Z</updated>
        <summary type="html"><![CDATA[In recent years, federated learning (FL) has been widely applied for
supporting decentralized collaborative learning scenarios. Among existing FL
models, federated logistic regression (FLR) is a widely used statistic model
and has been used in various industries. To ensure data security and user
privacy, FLR leverages homomorphic encryption (HE) to protect the exchanged
data among different collaborative parties. However, HE introduces significant
computational overhead (i.e., the cost of data encryption/decryption and
calculation over encrypted data), which eventually becomes the performance
bottleneck of the whole system. In this paper, we propose HAFLO, a GPU-based
solution to improve the performance of FLR. The core idea of HAFLO is to
summarize a set of performance-critical homomorphic operators (HO) used by FLR
and accelerate the execution of these operators through a joint optimization of
storage, IO, and computation. The preliminary results show that our
acceleration on FATE, a popular FL framework, achieves a 49.9$\times$ speedup
for heterogeneous LR and 88.4$\times$ for homogeneous LR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xiaodian Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wanhang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xinyang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shuihai Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kai Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatio-temporal graph neural networks for multi-site PV power forecasting. (arXiv:2107.13875v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13875</id>
        <link href="http://arxiv.org/abs/2107.13875"/>
        <updated>2021-07-30T02:13:29.421Z</updated>
        <summary type="html"><![CDATA[Accurate forecasting of solar power generation with fine temporal and spatial
resolution is vital for the operation of the power grid. However,
state-of-the-art approaches that combine machine learning with numerical
weather predictions (NWP) have coarse resolution. In this paper, we take a
graph signal processing perspective and model multi-site photovoltaic (PV)
production time series as signals on a graph to capture their spatio-temporal
dependencies and achieve higher spatial and temporal resolution forecasts. We
present two novel graph neural network models for deterministic multi-site PV
forecasting dubbed the graph-convolutional long short term memory (GCLSTM) and
the graph-convolutional transformer (GCTrafo) models. These methods rely solely
on production data and exploit the intuition that PV systems provide a dense
network of virtual weather stations. The proposed methods were evaluated in two
data sets for an entire year: 1) production data from 304 real PV systems, and
2) simulated production of 1000 PV systems, both distributed over Switzerland.
The proposed models outperform state-of-the-art multi-site forecasting methods
for prediction horizons of six hours ahead. Furthermore, the proposed models
outperform state-of-the-art single-site methods with NWP as inputs on horizons
up to four hours ahead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Simeunovic_J/0/1/0/all/0/1"&gt;Jelena Simeunovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schubnel_B/0/1/0/all/0/1"&gt;Baptiste Schubnel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alet_P/0/1/0/all/0/1"&gt;Pierre-Jean Alet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carrillo_R/0/1/0/all/0/1"&gt;Rafael E. Carrillo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Competitive Control. (arXiv:2107.13657v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2107.13657</id>
        <link href="http://arxiv.org/abs/2107.13657"/>
        <updated>2021-07-30T02:13:29.415Z</updated>
        <summary type="html"><![CDATA[We consider control from the perspective of competitive analysis. Unlike much
prior work on learning-based control, which focuses on minimizing regret
against the best controller selected in hindsight from some specific class, we
focus on designing an online controller which competes against a clairvoyant
offline optimal controller. A natural performance metric in this setting is
competitive ratio, which is the ratio between the cost incurred by the online
controller and the cost incurred by the offline optimal controller. Using
operator-theoretic techniques from robust control, we derive a computationally
efficient state-space description of the the controller with optimal
competitive ratio in both finite-horizon and infinite-horizon settings. We
extend competitive control to nonlinear systems using Model Predictive Control
(MPC) and present numerical experiments which show that our competitive
controller can significantly outperform standard $H_2$ and $H_{\infty}$
controllers in the MPC setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Goel_G/0/1/0/all/0/1"&gt;Gautam Goel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hassibi_B/0/1/0/all/0/1"&gt;Babak Hassibi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Internet-of-Things Devices and Assistive Technologies for Healthcare: Applications, Challenges, and Opportunities. (arXiv:2107.14112v1 [physics.soc-ph])]]></title>
        <id>http://arxiv.org/abs/2107.14112</id>
        <link href="http://arxiv.org/abs/2107.14112"/>
        <updated>2021-07-30T02:13:29.409Z</updated>
        <summary type="html"><![CDATA[Medical conditions and cases are growing at a rapid pace, where physical
space is starting to be constrained. Hospitals and clinics no longer have the
ability to accommodate large numbers of incoming patients. It is clear that the
current state of the health industry needs to improve its valuable and limited
resources. The evolution of the Internet of Things (IoT) devices along with
assistive technologies can alleviate the problem in healthcare, by being a
convenient and easy means of accessing healthcare services wirelessly. There is
a plethora of IoT devices and potential applications that can take advantage of
the unique characteristics that these technologies can offer. However, at the
same time, these services pose novel challenges that need to be properly
addressed. In this article, we review some popular categories of IoT-based
applications for healthcare along with their devices. Then, we describe the
challenges and discuss how research can properly address the open issues and
improve the already existing implementations in healthcare. Further possible
solutions are also discussed to show their potential in being viable solutions
for future healthcare applications]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Baucas_M/0/1/0/all/0/1"&gt;Marc Jayson Baucas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Spachos_P/0/1/0/all/0/1"&gt;Petros Spachos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Gregori_S/0/1/0/all/0/1"&gt;Stefano Gregori&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. (arXiv:2107.13586v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.13586</id>
        <link href="http://arxiv.org/abs/2107.13586"/>
        <updated>2021-07-30T02:13:29.395Z</updated>
        <summary type="html"><![CDATA[This paper surveys and organizes research works in a new paradigm in natural
language processing, which we dub "prompt-based learning". Unlike traditional
supervised learning, which trains a model to take in an input x and predict an
output y as P(y|x), prompt-based learning is based on language models that
model the probability of text directly. To use these models to perform
prediction tasks, the original input x is modified using a template into a
textual string prompt x' that has some unfilled slots, and then the language
model is used to probabilistically fill the unfilled information to obtain a
final string x, from which the final output y can be derived. This framework is
powerful and attractive for a number of reasons: it allows the language model
to be pre-trained on massive amounts of raw text, and by defining a new
prompting function the model is able to perform few-shot or even zero-shot
learning, adapting to new scenarios with few or no labeled data. In this paper
we introduce the basics of this promising paradigm, describe a unified set of
mathematical notations that can cover a wide variety of existing work, and
organize existing work along several dimensions, e.g.the choice of pre-trained
models, prompts, and tuning strategies. To make the field more accessible to
interested beginners, we not only make a systematic review of existing works
and a highly structured typology of prompt-based concepts, but also release
other resources, e.g., a website this http URL including
constantly-updated survey, and paperlist.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Pengfei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1"&gt;Weizhe Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1"&gt;Jinlan Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhengbao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayashi_H/0/1/0/all/0/1"&gt;Hiroaki Hayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1"&gt;Graham Neubig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deeper Learning By Doing: Integrating Hands-On Research Projects Into a Machine Learning Course. (arXiv:2107.13671v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2107.13671</id>
        <link href="http://arxiv.org/abs/2107.13671"/>
        <updated>2021-07-30T02:13:29.389Z</updated>
        <summary type="html"><![CDATA[Machine learning has seen a vast increase of interest in recent years, along
with an abundance of learning resources. While conventional lectures provide
students with important information and knowledge, we also believe that
additional project-based learning components can motivate students to engage in
topics more deeply. In addition to incorporating project-based learning in our
courses, we aim to develop project-based learning components aligned with
real-world tasks, including experimental design and execution, report writing,
oral presentation, and peer-reviewing. This paper describes the organization of
our project-based machine learning courses with a particular emphasis on the
class project components and shares our resources with instructors who would
like to include similar elements in their courses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raschka_S/0/1/0/all/0/1"&gt;Sebastian Raschka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Excavating AI" Re-excavated: Debunking a Fallacious Account of the JAFFE Dataset. (arXiv:2107.13998v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2107.13998</id>
        <link href="http://arxiv.org/abs/2107.13998"/>
        <updated>2021-07-30T02:13:29.381Z</updated>
        <summary type="html"><![CDATA[Twenty-five years ago, my colleagues Miyuki Kamachi and Jiro Gyoba and I
designed and photographed JAFFE, a set of facial expression images intended for
use in a study of face perception. In 2019, without seeking permission or
informing us, Kate Crawford and Trevor Paglen exhibited JAFFE in two widely
publicized art shows. In addition, they published a nonfactual account of the
images in the essay "Excavating AI: The Politics of Images in Machine Learning
Training Sets." The present article recounts the creation of the JAFFE dataset
and unravels each of Crawford and Paglen's fallacious statements. I also
discuss JAFFE more broadly in connection with research on facial expression,
affective computing, and human-computer interaction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lyons_M/0/1/0/all/0/1"&gt;Michael J. Lyons&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pitch-Informed Instrument Assignment Using a Deep Convolutional Network with Multiple Kernel Shapes. (arXiv:2107.13617v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.13617</id>
        <link href="http://arxiv.org/abs/2107.13617"/>
        <updated>2021-07-30T02:13:29.376Z</updated>
        <summary type="html"><![CDATA[This paper proposes a deep convolutional neural network for performing
note-level instrument assignment. Given a polyphonic multi-instrumental music
signal along with its ground truth or predicted notes, the objective is to
assign an instrumental source for each note. This problem is addressed as a
pitch-informed classification task where each note is analysed individually. We
also propose to utilise several kernel shapes in the convolutional layers in
order to facilitate learning of efficient timbre-discriminative feature maps.
Experiments on the MusicNet dataset using 7 instrument classes show that our
approach is able to achieve an average F-score of 0.904 when the original
multi-pitch annotations are used as the pitch information for the system, and
that it also excels if the note information is provided using third-party
multi-pitch estimation algorithms. We also include ablation studies
investigating the effects of the use of multiple kernel shapes and comparing
different input representations for the audio and the note-related information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lordelo_C/0/1/0/all/0/1"&gt;Carlos Lordelo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benetos_E/0/1/0/all/0/1"&gt;Emmanouil Benetos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dixon_S/0/1/0/all/0/1"&gt;Simon Dixon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahlback_S/0/1/0/all/0/1"&gt;Sven Ahlb&amp;#xe4;ck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Demand Forecasting in Smart Grid Using Long Short-Term Memory. (arXiv:2107.13653v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13653</id>
        <link href="http://arxiv.org/abs/2107.13653"/>
        <updated>2021-07-30T02:13:29.370Z</updated>
        <summary type="html"><![CDATA[Demand forecasting in power sector has become an important part of modern
demand management and response systems with the rise of smart metering enabled
grids. Long Short-Term Memory (LSTM) shows promising results in predicting time
series data which can also be applied to power load demand in smart grids. In
this paper, an LSTM based model using neural network architecture is proposed
to forecast power demand. The model is trained with hourly energy and power
usage data of four years from a smart grid. After training and prediction, the
accuracy of the model is compared against the traditional statistical time
series analysis algorithms, such as Auto-Regressive (AR), to determine the
efficiency. The mean absolute percentile error is found to be 1.22 in the
proposed LSTM model, which is the lowest among the other models. From the
findings, it is clear that the inclusion of neural network in predicting power
demand reduces the error of prediction significantly. Thus, the application of
LSTM can enable a more efficient demand response system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1"&gt;Koushik Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ishmam_A/0/1/0/all/0/1"&gt;Abtahi Ishmam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taher_K/0/1/0/all/0/1"&gt;Kazi Abu Taher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Graph Reinforcement Learning Model for Improving User Experience in Live Video Streaming. (arXiv:2107.13619v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.13619</id>
        <link href="http://arxiv.org/abs/2107.13619"/>
        <updated>2021-07-30T02:13:29.355Z</updated>
        <summary type="html"><![CDATA[In this paper we present a deep graph reinforcement learning model to predict
and improve the user experience during a live video streaming event,
orchestrated by an agent/tracker. We first formulate the user experience
prediction problem as a classification task, accounting for the fact that most
of the viewers at the beginning of an event have poor quality of experience due
to low-bandwidth connections and limited interactions with the tracker. In our
model we consider different factors that influence the quality of user
experience and train the proposed model on diverse state-action transitions
when viewers interact with the tracker. In addition, provided that past events
have various user experience characteristics we follow a gradient boosting
strategy to compute a global model that learns from different events. Our
experiments with three real-world datasets of live video streaming events
demonstrate the superiority of the proposed model against several baseline
strategies. Moreover, as the majority of the viewers at the beginning of an
event has poor experience, we show that our model can significantly increase
the number of viewers with high quality experience by at least 75% over the
first streaming minutes. Our evaluation datasets and implementation are
publicly available at https://publicresearch.z13.web.core.windows.net]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Antaris_S/0/1/0/all/0/1"&gt;Stefanos Antaris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rafailidis_D/0/1/0/all/0/1"&gt;Dimitrios Rafailidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Girdzijauskas_S/0/1/0/all/0/1"&gt;Sarunas Girdzijauskas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Secure Bayesian Federated Analytics for Privacy-Preserving Trend Detection. (arXiv:2107.13640v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.13640</id>
        <link href="http://arxiv.org/abs/2107.13640"/>
        <updated>2021-07-30T02:13:29.348Z</updated>
        <summary type="html"><![CDATA[Federated analytics has many applications in edge computing, its use can lead
to better decision making for service provision, product development, and user
experience. We propose a Bayesian approach to trend detection in which the
probability of a keyword being trendy, given a dataset, is computed via Bayes'
Theorem; the probability of a dataset, given that a keyword is trendy, is
computed through secure aggregation of such conditional probabilities over
local datasets of users. We propose a protocol, named SAFE, for Bayesian
federated analytics that offers sufficient privacy for production grade use
cases and reduces the computational burden of users and an aggregator. We
illustrate this approach with a trend detection experiment and discuss how this
approach could be extended further to make it production-ready.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chaulwar_A/0/1/0/all/0/1"&gt;Amit Chaulwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huth_M/0/1/0/all/0/1"&gt;Michael Huth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the Effects of Adversarial Personalized Ranking Optimization Method on Recommendation Quality. (arXiv:2107.13876v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.13876</id>
        <link href="http://arxiv.org/abs/2107.13876"/>
        <updated>2021-07-30T02:13:29.341Z</updated>
        <summary type="html"><![CDATA[Recommender systems (RSs) employ user-item feedback, e.g., ratings, to match
customers to personalized lists of products. Approaches to top-k recommendation
mainly rely on Learning-To-Rank algorithms and, among them, the most widely
adopted is Bayesian Personalized Ranking (BPR), which bases on a pair-wise
optimization approach. Recently, BPR has been found vulnerable against
adversarial perturbations of its model parameters. Adversarial Personalized
Ranking (APR) mitigates this issue by robustifying BPR via an adversarial
training procedure. The empirical improvements of APR's accuracy performance on
BPR have led to its wide use in several recommender models. However, a key
overlooked aspect has been the beyond-accuracy performance of APR, i.e.,
novelty, coverage, and amplification of popularity bias, considering that
recent results suggest that BPR, the building block of APR, is sensitive to the
intensification of biases and reduction of recommendation novelty. In this
work, we model the learning characteristics of the BPR and APR optimization
frameworks to give mathematical evidence that, when the feedback data have a
tailed distribution, APR amplifies the popularity bias more than BPR due to an
unbalanced number of received positive updates from short-head items. Using
matrix factorization (MF), we empirically validate the theoretical results by
performing preliminary experiments on two public datasets to compare BPR-MF and
APR-MF performance on accuracy and beyond-accuracy metrics. The experimental
results consistently show the degradation of novelty and coverage measures and
a worrying amplification of bias.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anelli_V/0/1/0/all/0/1"&gt;Vito Walter Anelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deldjoo_Y/0/1/0/all/0/1"&gt;Yashar Deldjoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noia_T/0/1/0/all/0/1"&gt;Tommaso Di Noia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merra_F/0/1/0/all/0/1"&gt;Felice Antonio Merra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models. (arXiv:2107.13686v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.13686</id>
        <link href="http://arxiv.org/abs/2107.13686"/>
        <updated>2021-07-30T02:13:29.334Z</updated>
        <summary type="html"><![CDATA[Pre-trained language models (PLMs) have achieved great success in natural
language processing. Most of PLMs follow the default setting of architecture
hyper-parameters (e.g., the hidden dimension is a quarter of the intermediate
dimension in feed-forward sub-networks) in BERT (Devlin et al., 2019). Few
studies have been conducted to explore the design of architecture
hyper-parameters in BERT, especially for the more efficient PLMs with tiny
sizes, which are essential for practical deployment on resource-constrained
devices. In this paper, we adopt the one-shot Neural Architecture Search (NAS)
to automatically search architecture hyper-parameters. Specifically, we
carefully design the techniques of one-shot learning and the search space to
provide an adaptive and efficient development way of tiny PLMs for various
latency constraints. We name our method AutoTinyBERT and evaluate its
effectiveness on the GLUE and SQuAD benchmarks. The extensive experiments show
that our method outperforms both the SOTA search-based baseline (NAS-BERT) and
the SOTA distillation-based methods (such as DistilBERT, TinyBERT, MiniLM and
MobileBERT). In addition, based on the obtained architectures, we propose a
more efficient development method that is even faster than the development of a
single PLM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1"&gt;Yichun Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Cheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1"&gt;Lifeng Shang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qun Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting battery end of life from solar off-grid system field data using machine learning. (arXiv:2107.13856v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13856</id>
        <link href="http://arxiv.org/abs/2107.13856"/>
        <updated>2021-07-30T02:13:29.328Z</updated>
        <summary type="html"><![CDATA[Hundreds of millions of people lack access to electricity. Decentralised
solar-battery systems are key for addressing this whilst avoiding carbon
emissions and air pollution, but are hindered by relatively high costs and
rural locations that inhibit timely preventative maintenance. Accurate
diagnosis of battery health and prediction of end of life from operational data
improves user experience and reduces costs. But lack of controlled validation
tests and variable data quality mean existing lab-based techniques fail to
work. We apply a scaleable probabilistic machine learning approach to diagnose
health in 1027 solar-connected lead-acid batteries, each running for 400-760
days, totalling 620 million data rows. We demonstrate 73% accurate prediction
of end of life, eight weeks in advance, rising to 82% at the point of failure.
This work highlights the opportunity to estimate health from existing
measurements using `big data' techniques, without additional equipment,
extending lifetime and improving performance in real-world applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aitio_A/0/1/0/all/0/1"&gt;Antti Aitio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Howey_D/0/1/0/all/0/1"&gt;David A. Howey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Imbalanced Adversarial Training with Reweighting. (arXiv:2107.13639v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13639</id>
        <link href="http://arxiv.org/abs/2107.13639"/>
        <updated>2021-07-30T02:13:29.313Z</updated>
        <summary type="html"><![CDATA[Adversarial training has been empirically proven to be one of the most
effective and reliable defense methods against adversarial attacks. However,
almost all existing studies about adversarial training are focused on balanced
datasets, where each class has an equal amount of training examples. Research
on adversarial training with imbalanced training datasets is rather limited. As
the initial effort to investigate this problem, we reveal the facts that
adversarially trained models present two distinguished behaviors from naturally
trained models in imbalanced datasets: (1) Compared to natural training,
adversarially trained models can suffer much worse performance on
under-represented classes, when the training dataset is extremely imbalanced.
(2) Traditional reweighting strategies may lose efficacy to deal with the
imbalance issue for adversarial training. For example, upweighting the
under-represented classes will drastically hurt the model's performance on
well-represented classes, and as a result, finding an optimal reweighting value
can be tremendously challenging. In this paper, to further understand our
observations, we theoretically show that the poor data separability is one key
reason causing this strong tension between under-represented and
well-represented classes. Motivated by this finding, we propose Separable
Reweighted Adversarial Training (SRAT) to facilitate adversarial training under
imbalanced scenarios, by learning more separable features for different
classes. Extensive experiments on various datasets verify the effectiveness of
the proposed framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wentao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Han Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaorui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yaxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thuraisingham_B/0/1/0/all/0/1"&gt;Bhavani Thuraisingham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jiliang Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Concept for a Technical Infrastructure for Management of Predictive Models in Industrial Applications. (arXiv:2107.13821v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13821</id>
        <link href="http://arxiv.org/abs/2107.13821"/>
        <updated>2021-07-30T02:13:29.306Z</updated>
        <summary type="html"><![CDATA[With the increasing number of created and deployed prediction models and the
complexity of machine learning workflows we require so called model management
systems to support data scientists in their tasks. In this work we describe our
technological concept for such a model management system. This concept includes
versioned storage of data, support for different machine learning algorithms,
fine tuning of models, subsequent deployment of models and monitoring of model
performance after deployment. We describe this concept with a close focus on
model lifecycle requirements stemming from our industry application cases, but
generalize key features that are relevant for all applications of machine
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bachinger_F/0/1/0/all/0/1"&gt;Florian Bachinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kronberger_G/0/1/0/all/0/1"&gt;Gabriel Kronberger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Transformer based Dual Discriminator Generative Adversarial Networks for Video Anomaly Detection. (arXiv:2107.13720v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13720</id>
        <link href="http://arxiv.org/abs/2107.13720"/>
        <updated>2021-07-30T02:13:29.299Z</updated>
        <summary type="html"><![CDATA[Detecting abnormal activities in real-world surveillance videos is an
important yet challenging task as the prior knowledge about video anomalies is
usually limited or unavailable. Despite that many approaches have been
developed to resolve this problem, few of them can capture the normal
spatio-temporal patterns effectively and efficiently. Moreover, existing works
seldom explicitly consider the local consistency at frame level and global
coherence of temporal dynamics in video sequences. To this end, we propose
Convolutional Transformer based Dual Discriminator Generative Adversarial
Networks (CT-D2GAN) to perform unsupervised video anomaly detection.
Specifically, we first present a convolutional transformer to perform future
frame prediction. It contains three key components, i.e., a convolutional
encoder to capture the spatial information of the input video clips, a temporal
self-attention module to encode the temporal dynamics, and a convolutional
decoder to integrate spatio-temporal features and predict the future frame.
Next, a dual discriminator based adversarial training procedure, which jointly
considers an image discriminator that can maintain the local consistency at
frame-level and a video discriminator that can enforce the global coherence of
temporal dynamics, is employed to enhance the future frame prediction. Finally,
the prediction error is used to identify abnormal video frames. Thoroughly
empirical studies on three public video anomaly detection datasets, i.e., UCSD
Ped2, CUHK Avenue, and Shanghai Tech Campus, demonstrate the effectiveness of
the proposed adversarial spatio-temporal modeling framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1"&gt;Xinyang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1"&gt;Dongjin Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuncong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhengzhang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1"&gt;Jingchao Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Intelligence in Achieving Sustainable Development Goals. (arXiv:2107.13966v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2107.13966</id>
        <link href="http://arxiv.org/abs/2107.13966"/>
        <updated>2021-07-30T02:13:29.293Z</updated>
        <summary type="html"><![CDATA[This perspective illustrates some of the AI applications that can accelerate
the achievement of SDGs and also highlights some of the considerations that
could hinder the efforts towards them. This emphasizes the importance of
establishing standard AI guidelines and regulations for the beneficial
applications of AI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goh_H/0/1/0/all/0/1"&gt;Hoe-Han Goh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Characterizing the Generalization Error of Gibbs Algorithm with Symmetrized KL information. (arXiv:2107.13656v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13656</id>
        <link href="http://arxiv.org/abs/2107.13656"/>
        <updated>2021-07-30T02:13:29.277Z</updated>
        <summary type="html"><![CDATA[Bounding the generalization error of a supervised learning algorithm is one
of the most important problems in learning theory, and various approaches have
been developed. However, existing bounds are often loose and lack of
guarantees. As a result, they may fail to characterize the exact generalization
ability of a learning algorithm. Our main contribution is an exact
characterization of the expected generalization error of the well-known Gibbs
algorithm in terms of symmetrized KL information between the input training
samples and the output hypothesis. Such a result can be applied to tighten
existing expected generalization error bound. Our analysis provides more
insight on the fundamental role the symmetrized KL information plays in
controlling the generalization error of the Gibbs algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aminian_G/0/1/0/all/0/1"&gt;Gholamali Aminian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bu_Y/0/1/0/all/0/1"&gt;Yuheng Bu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toni_L/0/1/0/all/0/1"&gt;Laura Toni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodrigues_M/0/1/0/all/0/1"&gt;Miguel R. D. Rodrigues&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wornell_G/0/1/0/all/0/1"&gt;Gregory Wornell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mind the Performance Gap: Examining Dataset Shift During Prospective Validation. (arXiv:2107.13964v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2107.13964</id>
        <link href="http://arxiv.org/abs/2107.13964"/>
        <updated>2021-07-30T02:13:29.247Z</updated>
        <summary type="html"><![CDATA[Once integrated into clinical care, patient risk stratification models may
perform worse compared to their retrospective performance. To date, it is
widely accepted that performance will degrade over time due to changes in care
processes and patient populations. However, the extent to which this occurs is
poorly understood, in part because few researchers report prospective
validation performance. In this study, we compare the 2020-2021 ('20-'21)
prospective performance of a patient risk stratification model for predicting
healthcare-associated infections to a 2019-2020 ('19-'20) retrospective
validation of the same model. We define the difference in retrospective and
prospective performance as the performance gap. We estimate how i) "temporal
shift", i.e., changes in clinical workflows and patient populations, and ii)
"infrastructure shift", i.e., changes in access, extraction and transformation
of data, both contribute to the performance gap. Applied prospectively to
26,864 hospital encounters during a twelve-month period from July 2020 to June
2021, the model achieved an area under the receiver operating characteristic
curve (AUROC) of 0.767 (95% confidence interval (CI): 0.737, 0.801) and a Brier
score of 0.189 (95% CI: 0.186, 0.191). Prospective performance decreased
slightly compared to '19-'20 retrospective performance, in which the model
achieved an AUROC of 0.778 (95% CI: 0.744, 0.815) and a Brier score of 0.163
(95% CI: 0.161, 0.165). The resulting performance gap was primarily due to
infrastructure shift and not temporal shift. So long as we continue to develop
and validate models using data stored in large research data warehouses, we
must consider differences in how and when data are accessed, measure how these
differences may affect prospective performance, and work to mitigate those
differences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Otles_E/0/1/0/all/0/1"&gt;Erkin &amp;#xd6;tle&amp;#x15f;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1"&gt;Jeeheh Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Benjamin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bochinski_M/0/1/0/all/0/1"&gt;Michelle Bochinski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joo_H/0/1/0/all/0/1"&gt;Hyeon Joo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ortwine_J/0/1/0/all/0/1"&gt;Justin Ortwine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shenoy_E/0/1/0/all/0/1"&gt;Erica Shenoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Washer_L/0/1/0/all/0/1"&gt;Laraine Washer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Young_V/0/1/0/all/0/1"&gt;Vincent B. Young&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1"&gt;Krishna Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wiens_J/0/1/0/all/0/1"&gt;Jenna Wiens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning with Noisy Labels for Robust Point Cloud Segmentation. (arXiv:2107.14230v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14230</id>
        <link href="http://arxiv.org/abs/2107.14230"/>
        <updated>2021-07-30T02:13:29.197Z</updated>
        <summary type="html"><![CDATA[Point cloud segmentation is a fundamental task in 3D. Despite recent progress
on point cloud segmentation with the power of deep networks, current deep
learning methods based on the clean label assumptions may fail with noisy
labels. Yet, object class labels are often mislabeled in real-world point cloud
datasets. In this work, we take the lead in solving this issue by proposing a
novel Point Noise-Adaptive Learning (PNAL) framework. Compared to existing
noise-robust methods on image tasks, our PNAL is noise-rate blind, to cope with
the spatially variant noise rate problem specific to point clouds.
Specifically, we propose a novel point-wise confidence selection to obtain
reliable labels based on the historical predictions of each point. A novel
cluster-wise label correction is proposed with a voting strategy to generate
the best possible label taking the neighbor point correlations into
consideration. We conduct extensive experiments to demonstrate the
effectiveness of PNAL on both synthetic and real-world noisy datasets. In
particular, even with $60\%$ symmetric noisy labels, our proposed method
produces much better results than its baseline counterpart without PNAL and is
comparable to the ideal upper bound trained on a completely clean dataset.
Moreover, we fully re-labeled the test set of a popular but noisy real-world
scene dataset ScanNetV2 to make it clean, for rigorous experiment and future
research. Our code and data will be available at
\url{https://shuquanye.com/PNAL_website/}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1"&gt;Shuquan Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dongdong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Songfang Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1"&gt;Jing Liao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Relaxations of Logic for Neural Networks: A Comprehensive Study. (arXiv:2107.13646v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.13646</id>
        <link href="http://arxiv.org/abs/2107.13646"/>
        <updated>2021-07-30T02:13:29.191Z</updated>
        <summary type="html"><![CDATA[Symbolic knowledge can provide crucial inductive bias for training neural
models, especially in low data regimes. A successful strategy for incorporating
such knowledge involves relaxing logical statements into sub-differentiable
losses for optimization. In this paper, we study the question of how best to
relax logical expressions that represent labeled examples and knowledge about a
problem; we focus on sub-differentiable t-norm relaxations of logic. We present
theoretical and empirical criteria for characterizing which relaxation would
perform best in various scenarios. In our theoretical study driven by the goal
of preserving tautologies, the Lukasiewicz t-norm performs best. However, in
our empirical analysis on the text chunking and digit recognition tasks, the
product t-norm achieves best predictive performance. We analyze this apparent
discrepancy, and conclude with a list of best practices for defining loss
functions via logic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grespan_M/0/1/0/all/0/1"&gt;Mattia Medina Grespan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Ashim Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1"&gt;Vivek Srikumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large sample spectral analysis of graph-based multi-manifold clustering. (arXiv:2107.13610v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13610</id>
        <link href="http://arxiv.org/abs/2107.13610"/>
        <updated>2021-07-30T02:13:29.165Z</updated>
        <summary type="html"><![CDATA[In this work we study statistical properties of graph-based algorithms for
multi-manifold clustering (MMC). In MMC the goal is to retrieve the
multi-manifold structure underlying a given Euclidean data set when this one is
assumed to be obtained by sampling a distribution on a union of manifolds
$\mathcal{M} = \mathcal{M}_1 \cup\dots \cup \mathcal{M}_N$ that may intersect
with each other and that may have different dimensions. We investigate
sufficient conditions that similarity graphs on data sets must satisfy in order
for their corresponding graph Laplacians to capture the right geometric
information to solve the MMC problem. Precisely, we provide high probability
error bounds for the spectral approximation of a tensorized Laplacian on
$\mathcal{M}$ with a suitable graph Laplacian built from the observations; the
recovered tensorized Laplacian contains all geometric information of all the
individual underlying manifolds. We provide an example of a family of
similarity graphs, which we call annular proximity graphs with angle
constraints, satisfying these sufficient conditions. We contrast our family of
graphs with other constructions in the literature based on the alignment of
tangent planes. Extensive numerical experiments expand the insights that our
theory provides on the MMC problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trillos_N/0/1/0/all/0/1"&gt;Nicolas Garcia Trillos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1"&gt;Pengfei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chenghui Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimating Respiratory Rate From Breath Audio Obtained Through Wearable Microphones. (arXiv:2107.14028v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.14028</id>
        <link href="http://arxiv.org/abs/2107.14028"/>
        <updated>2021-07-30T02:13:29.158Z</updated>
        <summary type="html"><![CDATA[Respiratory rate (RR) is a clinical metric used to assess overall health and
physical fitness. An individual's RR can change from their baseline due to
chronic illness symptoms (e.g., asthma, congestive heart failure), acute
illness (e.g., breathlessness due to infection), and over the course of the day
due to physical exhaustion during heightened exertion. Remote estimation of RR
can offer a cost-effective method to track disease progression and
cardio-respiratory fitness over time. This work investigates a model-driven
approach to estimate RR from short audio segments obtained after physical
exertion in healthy adults. Data was collected from 21 individuals using
microphone-enabled, near-field headphones before, during, and after strenuous
exercise. RR was manually annotated by counting perceived inhalations and
exhalations. A multi-task Long-Short Term Memory (LSTM) network with
convolutional layers was implemented to process mel-filterbank energies,
estimate RR in varying background noise conditions, and predict heavy
breathing, indicated by an RR of more than 25 breaths per minute. The
multi-task model performs both classification and regression tasks and
leverages a mixture of loss functions. It was observed that RR can be estimated
with a concordance correlation coefficient (CCC) of 0.76 and a mean squared
error (MSE) of 0.2, demonstrating that audio can be a viable signal for
approximating RR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Agni Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_V/0/1/0/all/0/1"&gt;Vikramjit Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliver_C/0/1/0/all/0/1"&gt;Carolyn Oliver&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullal_A/0/1/0/all/0/1"&gt;Adeeti Ullal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biddulph_M/0/1/0/all/0/1"&gt;Matt Biddulph&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mance_I/0/1/0/all/0/1"&gt;Irida Mance&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Significance of Speaker Embeddings and Temporal Context for Depression Detection. (arXiv:2107.13969v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2107.13969</id>
        <link href="http://arxiv.org/abs/2107.13969"/>
        <updated>2021-07-30T02:13:29.151Z</updated>
        <summary type="html"><![CDATA[Depression detection from speech has attracted a lot of attention in recent
years. However, the significance of speaker-specific information in depression
detection has not yet been explored. In this work, we analyze the significance
of speaker embeddings for the task of depression detection from speech.
Experimental results show that the speaker embeddings provide important cues to
achieve state-of-the-art performance in depression detection. We also show that
combining conventional OpenSMILE and COVAREP features, which carry
complementary information, with speaker embeddings further improves the
depression detection performance. The significance of temporal context in the
training of deep learning models for depression detection is also analyzed in
this paper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dumpala_S/0/1/0/all/0/1"&gt;Sri Harsha Dumpala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_S/0/1/0/all/0/1"&gt;Sebastian Rodriguez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rempel_S/0/1/0/all/0/1"&gt;Sheri Rempel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uher_R/0/1/0/all/0/1"&gt;Rudolf Uher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oore_S/0/1/0/all/0/1"&gt;Sageev Oore&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Underwater inspection and intervention dataset. (arXiv:2107.13628v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13628</id>
        <link href="http://arxiv.org/abs/2107.13628"/>
        <updated>2021-07-30T02:13:29.140Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel dataset for the development of visual navigation
and simultaneous localisation and mapping (SLAM) algorithms as well as for
underwater intervention tasks. It differs from existing datasets as it contains
ground truth for the vehicle's position captured by an underwater motion
tracking system. The dataset contains distortion-free and rectified stereo
images along with the calibration parameters of the stereo camera setup.
Furthermore, the experiments were performed and recorded in a controlled
environment, where current and waves could be generated allowing the dataset to
cover a wide range of conditions - from calm water to waves and currents of
significant strength.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luczynski_T/0/1/0/all/0/1"&gt;Tomasz Luczynski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Willners_J/0/1/0/all/0/1"&gt;Jonatan Scharff Willners&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vargas_E/0/1/0/all/0/1"&gt;Elizabeth Vargas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roe_J/0/1/0/all/0/1"&gt;Joshua Roe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shida Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yu Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petillot_Y/0/1/0/all/0/1"&gt;Yvan Petillot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sen Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[United We Learn Better: Harvesting Learning Improvements From Class Hierarchies Across Tasks. (arXiv:2107.13627v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13627</id>
        <link href="http://arxiv.org/abs/2107.13627"/>
        <updated>2021-07-30T02:13:29.104Z</updated>
        <summary type="html"><![CDATA[Attempts of learning from hierarchical taxonomies in computer vision have
been mostly focusing on image classification. Though ways of best harvesting
learning improvements from hierarchies in classification are far from being
solved, there is a need to target these problems in other vision tasks such as
object detection. As progress on the classification side is often dependent on
hierarchical cross-entropy losses, novel detection architectures using sigmoid
as an output function instead of softmax cannot easily apply these advances,
requiring novel methods in detection. In this work we establish a theoretical
framework based on probability and set theory for extracting parent predictions
and a hierarchical loss that can be used across tasks, showing results across
classification and detection benchmarks and opening up the possibility of
hierarchical learning for sigmoid-based detection architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shkodrani_S/0/1/0/all/0/1"&gt;Sindi Shkodrani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manfredi_M/0/1/0/all/0/1"&gt;Marco Manfredi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baka_N/0/1/0/all/0/1"&gt;N&amp;#xf3;ra Baka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning for Fine-Grained Image Classification. (arXiv:2107.13973v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13973</id>
        <link href="http://arxiv.org/abs/2107.13973"/>
        <updated>2021-07-30T02:13:29.096Z</updated>
        <summary type="html"><![CDATA[Fine-grained image classification involves identifying different
subcategories of a class which possess very subtle discriminatory features.
Fine-grained datasets usually provide bounding box annotations along with class
labels to aid the process of classification. However, building large scale
datasets with such annotations is a mammoth task. Moreover, this extensive
annotation is time-consuming and often requires expertise, which is a huge
bottleneck in building large datasets. On the other hand, self-supervised
learning (SSL) exploits the freely available data to generate supervisory
signals which act as labels. The features learnt by performing some pretext
tasks on huge unlabelled data proves to be very helpful for multiple downstream
tasks.

Our idea is to leverage self-supervision such that the model learns useful
representations of fine-grained image classes. We experimented with 3 kinds of
models: Jigsaw solving as pretext task, adversarial learning (SRGAN) and
contrastive learning based (SimCLR) model. The learned features are used for
downstream tasks such as fine-grained image classification. Our code is
available at
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Breiki_F/0/1/0/all/0/1"&gt;Farha Al Breiki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ridzuan_M/0/1/0/all/0/1"&gt;Muhammad Ridzuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grandhe_R/0/1/0/all/0/1"&gt;Rushali Grandhe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic and Geometric Depth: Detecting Objects in Perspective. (arXiv:2107.14160v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14160</id>
        <link href="http://arxiv.org/abs/2107.14160"/>
        <updated>2021-07-30T02:13:29.036Z</updated>
        <summary type="html"><![CDATA[3D object detection is an important capability needed in various practical
applications such as driver assistance systems. Monocular 3D detection, as an
economical solution compared to conventional settings relying on binocular
vision or LiDAR, has drawn increasing attention recently but still yields
unsatisfactory results. This paper first presents a systematic study on this
problem and observes that the current monocular 3D detection problem can be
simplified as an instance depth estimation problem: The inaccurate instance
depth blocks all the other 3D attribute predictions from improving the overall
detection performance. However, recent methods directly estimate the depth
based on isolated instances or pixels while ignoring the geometric relations
across different objects, which can be valuable constraints as the key
information about depth is not directly manifest in the monocular image.
Therefore, we construct geometric relation graphs across predicted objects and
use the graph to facilitate depth estimation. As the preliminary depth
estimation of each instance is usually inaccurate in this ill-posed setting, we
incorporate a probabilistic representation to capture the uncertainty. It
provides an important indicator to identify confident predictions and further
guide the depth propagation. Despite the simplicity of the basic idea, our
method obtains significant improvements on KITTI and nuScenes benchmarks,
achieving the 1st place out of all monocular vision-only methods while still
maintaining real-time efficiency. Code and models will be released at
https://github.com/open-mmlab/mmdetection3d.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xinge Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1"&gt;Jiangmiao Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1"&gt;Dahua Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-shot action recognition in challenging therapy scenarios. (arXiv:2102.08997v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08997</id>
        <link href="http://arxiv.org/abs/2102.08997"/>
        <updated>2021-07-30T02:13:28.989Z</updated>
        <summary type="html"><![CDATA[One-shot action recognition aims to recognize new action categories from a
single reference example, typically referred to as the anchor example. This
work presents a novel approach for one-shot action recognition in the wild that
computes motion representations robust to variable kinematic conditions.
One-shot action recognition is then performed by evaluating anchor and target
motion representations. We also develop a set of complementary steps that boost
the action recognition performance in the most challenging scenarios. Our
approach is evaluated on the public NTU-120 one-shot action recognition
benchmark, outperforming previous action recognition models. Besides, we
evaluate our framework on a real use-case of therapy with autistic people.
These recordings are particularly challenging due to high-level artifacts from
the patient motion. Our results provide not only quantitative but also online
qualitative measures, essential for the patient evaluation and monitoring
during the actual therapy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sabater_A/0/1/0/all/0/1"&gt;Alberto Sabater&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_L/0/1/0/all/0/1"&gt;Laura Santos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_Victor_J/0/1/0/all/0/1"&gt;Jose Santos-Victor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bernardino_A/0/1/0/all/0/1"&gt;Alexandre Bernardino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Montesano_L/0/1/0/all/0/1"&gt;Luis Montesano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murillo_A/0/1/0/all/0/1"&gt;Ana C. Murillo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RigNet: Repetitive Image Guided Network for Depth Completion. (arXiv:2107.13802v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13802</id>
        <link href="http://arxiv.org/abs/2107.13802"/>
        <updated>2021-07-30T02:13:28.982Z</updated>
        <summary type="html"><![CDATA[Depth completion deals with the problem of recovering dense depth maps from
sparse ones, where color images are often used to facilitate this completion.
Recent approaches mainly focus on image guided learning to predict dense
results. However, blurry image guidance and object structures in depth still
impede the performance of image guided frameworks. To tackle these problems, we
explore a repetitive design in our image guided network to sufficiently and
gradually recover depth values. Specifically, the repetition is embodied in a
color image guidance branch and a depth generation branch. In the former
branch, we design a repetitive hourglass network to extract higher-level image
features of complex environments, which can provide powerful context guidance
for depth prediction. In the latter branch, we design a repetitive guidance
module based on dynamic convolution where the convolution factorization is
applied to simultaneously reduce its complexity and progressively model
high-frequency structures, e.g., boundaries. Further, in this module, we
propose an adaptive fusion mechanism to effectively aggregate multi-step depth
features. Extensive experiments show that our method achieves state-of-the-art
result on the NYUv2 dataset and ranks 1st on the KITTI benchmark at the time of
submission.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zhiqiang Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhenyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1"&gt;Baobei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jian Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal-Relational Hypergraph Tri-Attention Networks for Stock Trend Prediction. (arXiv:2107.14033v1 [q-fin.ST])]]></title>
        <id>http://arxiv.org/abs/2107.14033</id>
        <link href="http://arxiv.org/abs/2107.14033"/>
        <updated>2021-07-30T02:13:28.966Z</updated>
        <summary type="html"><![CDATA[Predicting the future price trends of stocks is a challenging yet intriguing
problem given its critical role to help investors make profitable decisions. In
this paper, we present a collaborative temporal-relational modeling framework
for end-to-end stock trend prediction. The temporal dynamics of stocks is
firstly captured with an attention-based recurrent neural network. Then,
different from existing studies relying on the pairwise correlations between
stocks, we argue that stocks are naturally connected as a collective group, and
introduce the hypergraph structures to jointly characterize the stock
group-wise relationships of industry-belonging and fund-holding. A novel
hypergraph tri-attention network (HGTAN) is proposed to augment the hypergraph
convolutional networks with a hierarchical organization of intra-hyperedge,
inter-hyperedge, and inter-hypergraph attention modules. In this manner, HGTAN
adaptively determines the importance of nodes, hyperedges, and hypergraphs
during the information propagation among stocks, so that the potential
synergies between stock movements can be fully exploited. Extensive experiments
on real-world data demonstrate the effectiveness of our approach. Also, the
results of investment simulation show that our approach can achieve a more
desirable risk-adjusted return. The data and codes of our work have been
released at https://github.com/lixiaojieff/HGTAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Cui_C/0/1/0/all/0/1"&gt;Chaoran Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaojie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Du_J/0/1/0/all/0/1"&gt;Juan Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chunyun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Nie_X/0/1/0/all/0/1"&gt;Xiushan Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Wang_M/0/1/0/all/0/1"&gt;Meng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Yin_Y/0/1/0/all/0/1"&gt;Yilong Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VMNet: Voxel-Mesh Network for Geodesic-Aware 3D Semantic Segmentation. (arXiv:2107.13824v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13824</id>
        <link href="http://arxiv.org/abs/2107.13824"/>
        <updated>2021-07-30T02:13:28.957Z</updated>
        <summary type="html"><![CDATA[In recent years, sparse voxel-based methods have become the state-of-the-arts
for 3D semantic segmentation of indoor scenes, thanks to the powerful 3D CNNs.
Nevertheless, being oblivious to the underlying geometry, voxel-based methods
suffer from ambiguous features on spatially close objects and struggle with
handling complex and irregular geometries due to the lack of geodesic
information. In view of this, we present Voxel-Mesh Network (VMNet), a novel 3D
deep architecture that operates on the voxel and mesh representations
leveraging both the Euclidean and geodesic information. Intuitively, the
Euclidean information extracted from voxels can offer contextual cues
representing interactions between nearby objects, while the geodesic
information extracted from meshes can help separate objects that are spatially
close but have disconnected surfaces. To incorporate such information from the
two domains, we design an intra-domain attentive module for effective feature
aggregation and an inter-domain attentive module for adaptive feature fusion.
Experimental results validate the effectiveness of VMNet: specifically, on the
challenging ScanNet dataset for large-scale segmentation of indoor scenes, it
outperforms the state-of-the-art SparseConvNet and MinkowskiNet (74.6% vs 72.5%
and 73.6% in mIoU) with a simpler network structure (17M vs 30M and 38M
parameters). Code release: https://github.com/hzykent/VMNet]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zeyu Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1"&gt;Xuyang Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1"&gt;Jiaxiang Shang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Runze Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Jiayu Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1"&gt;Guangyuan Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1"&gt;Hongbo Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tai_C/0/1/0/all/0/1"&gt;Chiew-Lan Tai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FATNN: Fast and Accurate Ternary Neural Networks. (arXiv:2008.05101v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.05101</id>
        <link href="http://arxiv.org/abs/2008.05101"/>
        <updated>2021-07-30T02:13:28.948Z</updated>
        <summary type="html"><![CDATA[Ternary Neural Networks (TNNs) have received much attention due to being
potentially orders of magnitude faster in inference, as well as more power
efficient, than full-precision counterparts. However, 2 bits are required to
encode the ternary representation with only 3 quantization levels leveraged. As
a result, conventional TNNs have similar memory consumption and speed compared
with the standard 2-bit models, but have worse representational capability.
Moreover, there is still a significant gap in accuracy between TNNs and
full-precision networks, hampering their deployment to real applications. To
tackle these two challenges, in this work, we first show that, under some mild
constraints, computational complexity of the ternary inner product can be
reduced by a factor of 2. Second, to mitigate the performance gap, we
elaborately design an implementation-dependent ternary quantization algorithm.
The proposed framework is termed Fast and Accurate Ternary Neural Networks
(FATNN). Experiments on image classification demonstrate that our FATNN
surpasses the state-of-the-arts by a significant margin in accuracy. More
importantly, speedup evaluation compared with various precisions is analyzed on
several platforms, which serves as a strong benchmark for further research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Peng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1"&gt;Bohan Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chunhua Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Object Recognition in Indoor Environments Using Topologically Persistent Features. (arXiv:2010.03196v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03196</id>
        <link href="http://arxiv.org/abs/2010.03196"/>
        <updated>2021-07-30T02:13:28.943Z</updated>
        <summary type="html"><![CDATA[Object recognition in unseen indoor environments remains a challenging
problem for visual perception of mobile robots. In this letter, we propose the
use of topologically persistent features, which rely on the objects' shape
information, to address this challenge. In particular, we extract two kinds of
features, namely, sparse persistence image (PI) and amplitude, by applying
persistent homology to multi-directional height function-based filtrations of
the cubical complexes representing the object segmentation maps. The features
are then used to train a fully connected network for recognition. For
performance evaluation, in addition to a widely used shape dataset and a
benchmark indoor scenes dataset, we collect a new dataset, comprising scene
images from two different environments, namely, a living room and a mock
warehouse. The scenes are captured using varying camera poses under different
illumination conditions and include up to five different objects from a given
set of fourteen objects. On the benchmark indoor scenes dataset, sparse PI
features show better recognition performance in unseen environments than the
features learned using the widely used ResNetV2-56 and EfficientNet-B4 models.
Further, they provide slightly higher recall and accuracy values than Faster
R-CNN, an end-to-end object detection method, and its state-of-the-art variant,
Domain Adaptive Faster R-CNN. The performance of our methods also remains
relatively unchanged from the training environment (living room) to the unseen
environment (mock warehouse) in the new dataset. In contrast, the performance
of the object detection methods drops substantially. We also implement the
proposed method on a real-world robot to demonstrate its usefulness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samani_E/0/1/0/all/0/1"&gt;Ekta U. Samani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xingjian Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1"&gt;Ashis G. Banerjee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[P2-Net: Joint Description and Detection of Local Features for Pixel and Point Matching. (arXiv:2103.01055v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01055</id>
        <link href="http://arxiv.org/abs/2103.01055"/>
        <updated>2021-07-30T02:13:28.934Z</updated>
        <summary type="html"><![CDATA[Accurately describing and detecting 2D and 3D keypoints is crucial to
establishing correspondences across images and point clouds. Despite a plethora
of learning-based 2D or 3D local feature descriptors and detectors having been
proposed, the derivation of a shared descriptor and joint keypoint detector
that directly matches pixels and points remains under-explored by the
community. This work takes the initiative to establish fine-grained
correspondences between 2D images and 3D point clouds. In order to directly
match pixels and points, a dual fully convolutional framework is presented that
maps 2D and 3D inputs into a shared latent representation space to
simultaneously describe and detect keypoints. Furthermore, an ultra-wide
reception mechanism in combination with a novel loss function are designed to
mitigate the intrinsic information variations between pixel and point local
regions. Extensive experimental results demonstrate that our framework shows
competitive performance in fine-grained matching between images and point
clouds and achieves state-of-the-art results for the task of indoor visual
localization. Our source code will be available at [no-name-for-blind-review].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Changhao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1"&gt;Zhaopeng Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1"&gt;Jie Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Chris Xiaoxuan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhengdi Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Peijun Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1"&gt;Zhen Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1"&gt;Fan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trigoni_N/0/1/0/all/0/1"&gt;Niki Trigoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markham_A/0/1/0/all/0/1"&gt;Andrew Markham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Walk in the Cloud: Learning Curves for Point Clouds Shape Analysis. (arXiv:2105.01288v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01288</id>
        <link href="http://arxiv.org/abs/2105.01288"/>
        <updated>2021-07-30T02:13:28.914Z</updated>
        <summary type="html"><![CDATA[Discrete point cloud objects lack sufficient shape descriptors of 3D
geometries. In this paper, we present a novel method for aggregating
hypothetical curves in point clouds. Sequences of connected points (curves) are
initially grouped by taking guided walks in the point clouds, and then
subsequently aggregated back to augment their point-wise features. We provide
an effective implementation of the proposed aggregation strategy including a
novel curve grouping operator followed by a curve aggregation operator. Our
method was benchmarked on several point cloud analysis tasks where we achieved
the state-of-the-art classification accuracy of 94.2% on the ModelNet40
classification task, instance IoU of 86.8 on the ShapeNetPart segmentation
task, and cosine error of 0.11 on the ModelNet40 normal estimation task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1"&gt;Tiange Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chaoyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jianhui Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1"&gt;Weidong Cai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic Monocular 3D Human Pose Estimation with Normalizing Flows. (arXiv:2107.13788v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13788</id>
        <link href="http://arxiv.org/abs/2107.13788"/>
        <updated>2021-07-30T02:13:28.908Z</updated>
        <summary type="html"><![CDATA[3D human pose estimation from monocular images is a highly ill-posed problem
due to depth ambiguities and occlusions. Nonetheless, most existing works
ignore these ambiguities and only estimate a single solution. In contrast, we
generate a diverse set of hypotheses that represents the full posterior
distribution of feasible 3D poses. To this end, we propose a normalizing flow
based method that exploits the deterministic 3D-to-2D mapping to solve the
ambiguous inverse 2D-to-3D problem. Additionally, uncertain detections and
occlusions are effectively modeled by incorporating uncertainty information of
the 2D detector as condition. Further keys to success are a learned 3D pose
prior and a generalization of the best-of-M loss. We evaluate our approach on
the two benchmark datasets Human3.6M and MPI-INF-3DHP, outperforming all
comparable methods in most metrics. The implementation is available on GitHub.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wehrbein_T/0/1/0/all/0/1"&gt;Tom Wehrbein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudolph_M/0/1/0/all/0/1"&gt;Marco Rudolph&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1"&gt;Bodo Rosenhahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wandt_B/0/1/0/all/0/1"&gt;Bastian Wandt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CI-Net: Contextual Information for Joint Semantic Segmentation and Depth Estimation. (arXiv:2107.13800v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13800</id>
        <link href="http://arxiv.org/abs/2107.13800"/>
        <updated>2021-07-30T02:13:28.902Z</updated>
        <summary type="html"><![CDATA[Monocular depth estimation and semantic segmentation are two fundamental
goals of scene understanding. Due to the advantages of task interaction, many
works study the joint task learning algorithm. However, most existing methods
fail to fully leverage the semantic labels, ignoring the provided context
structures and only using them to supervise the prediction of segmentation
split. In this paper, we propose a network injected with contextual information
(CI-Net) to solve the problem. Specifically, we introduce self-attention block
in the encoder to generate attention map. With supervision from the ground
truth created by semantic labels, the network is embedded with contextual
information so that it could understand the scene better, utilizing dependent
features to make accurate prediction. Besides, a feature sharing module is
constructed to make the task-specific features deeply fused and a consistency
loss is devised to make the features mutually guided. We evaluate the proposed
CI-Net on the NYU-Depth-v2 and SUN-RGBD datasets. The experimental results
validate that our proposed CI-Net is competitive with the state-of-the-arts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1"&gt;Tianxiao Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1"&gt;Wu Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhongbin Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1"&gt;Zhun Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1"&gt;Shane Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinmei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1"&gt;Qiuda Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anchor-Based Spatio-Temporal Attention 3D Convolutional Networks for Dynamic 3D Point Cloud Sequences. (arXiv:2012.10860v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10860</id>
        <link href="http://arxiv.org/abs/2012.10860"/>
        <updated>2021-07-30T02:13:28.895Z</updated>
        <summary type="html"><![CDATA[With the rapid development of measurement technology, LiDAR and depth cameras
are widely used in the perception of the 3D environment. Recent learning based
methods for robot perception most focus on the image or video, but deep
learning methods for dynamic 3D point cloud sequences are underexplored.
Therefore, developing efficient and accurate perception method compatible with
these advanced instruments is pivotal to autonomous driving and service robots.
An Anchor-based Spatio-Temporal Attention 3D Convolution operation (ASTA3DConv)
is proposed in this paper to process dynamic 3D point cloud sequences. The
proposed convolution operation builds a regular receptive field around each
point by setting several virtual anchors around each point. The features of
neighborhood points are firstly aggregated to each anchor based on the
spatio-temporal attention mechanism. Then, anchor-based 3D convolution is
adopted to aggregate these anchors' features to the core points. The proposed
method makes better use of the structured information within the local region
and learns spatio-temporal embedding features from dynamic 3D point cloud
sequences. Anchor-based Spatio-Temporal Attention 3D Convolutional Neural
Networks (ASTA3DCNNs) are built for classification and segmentation tasks based
on the proposed ASTA3DConv and evaluated on action recognition and semantic
segmentation tasks. The experiments and ablation studies on MSRAction3D and
Synthia datasets demonstrate the superior performance and effectiveness of our
method for dynamic 3D point cloud sequences. Our method achieves the
state-of-the-art performance among the methods with dynamic 3D point cloud
sequences as input on MSRAction3D and Synthia datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guangming Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Muyao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hanwen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yehui Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhe Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hesheng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[InstanceRefer: Cooperative Holistic Understanding for Visual Grounding on Point Clouds through Instance Multi-level Contextual Referring. (arXiv:2103.01128v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01128</id>
        <link href="http://arxiv.org/abs/2103.01128"/>
        <updated>2021-07-30T02:13:28.879Z</updated>
        <summary type="html"><![CDATA[Compared with the visual grounding on 2D images, the natural-language-guided
3D object localization on point clouds is more challenging. In this paper, we
propose a new model, named InstanceRefer, to achieve a superior 3D visual
grounding through the grounding-by-matching strategy. In practice, our model
first predicts the target category from the language descriptions using a
simple language classification model. Then, based on the category, our model
sifts out a small number of instance candidates (usually less than 20) from the
panoptic segmentation of point clouds. Thus, the non-trivial 3D visual
grounding task has been effectively re-formulated as a simplified
instance-matching problem, considering that instance-level candidates are more
rational than the redundant 3D object proposals. Subsequently, for each
candidate, we perform the multi-level contextual inference, i.e., referring
from instance attribute perception, instance-to-instance relation perception,
and instance-to-background global localization perception, respectively.
Eventually, the most relevant candidate is selected and localized by ranking
confidence scores, which are obtained by the cooperative holistic
visual-language feature matching. Experiments confirm that our method
outperforms previous state-of-the-arts on ScanRefer online benchmark and
Nr3D/Sr3D datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1"&gt;Zhihao Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1"&gt;Xu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1"&gt;Yinghong Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruimao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Shuguang Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RaidaR: A Rich Annotated Image Dataset of Rainy Street Scenes. (arXiv:2104.04606v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04606</id>
        <link href="http://arxiv.org/abs/2104.04606"/>
        <updated>2021-07-30T02:13:28.873Z</updated>
        <summary type="html"><![CDATA[We introduce RaidaR, a rich annotated image dataset of rainy street scenes,
to support autonomous driving research. The new dataset contains the largest
number of rainy images (58,542) to date, 5,000 of which provide semantic
segmentations and 3,658 provide object instance segmentations. The RaidaR
images cover a wide range of realistic rain-induced artifacts, including fog,
droplets, and road reflections, which can effectively augment existing street
scene datasets to improve data-driven machine perception during rainy weather.
To facilitate efficient annotation of a large volume of images, we develop a
semi-automatic scheme combining manual segmentation and an automated processing
akin to cross validation, resulting in 10-20 fold reduction on annotation time.
We demonstrate the utility of our new dataset by showing how data augmentation
with RaidaR can elevate the accuracy of existing segmentation algorithms. We
also present a novel unpaired image-to-image translation algorithm for
adding/removing rain artifacts, which directly benefits from RaidaR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1"&gt;Jiongchao Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fatemi_A/0/1/0/all/0/1"&gt;Arezou Fatemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lira_W/0/1/0/all/0/1"&gt;Wallace Lira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1"&gt;Fenggen Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leng_B/0/1/0/all/0/1"&gt;Biao Leng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1"&gt;Rui Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahdavi_Amiri_A/0/1/0/all/0/1"&gt;Ali Mahdavi-Amiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hao Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Embeddings for Few-Shot Open World Recognition. (arXiv:2107.13682v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13682</id>
        <link href="http://arxiv.org/abs/2107.13682"/>
        <updated>2021-07-30T02:13:28.867Z</updated>
        <summary type="html"><![CDATA[As autonomous decision-making agents move from narrow operating environments
to unstructured worlds, learning systems must move from a closed-world
formulation to an open-world and few-shot setting in which agents continuously
learn new classes from small amounts of information. This stands in stark
contrast to modern machine learning systems that are typically designed with a
known set of classes and a large number of examples for each class. In this
work we extend embedding-based few-shot learning algorithms to the open-world
recognition setting. We combine Bayesian non-parametric class priors with an
embedding-based pre-training scheme to yield a highly flexible framework which
we refer to as few-shot learning for open world recognition (FLOWR). We
benchmark our framework on open-world extensions of the common MiniImageNet and
TieredImageNet few-shot learning datasets. Our results show, compared to prior
methods, strong classification accuracy performance and up to a 12% improvement
in H-measure (a measure of novel class detection) from our non-parametric
open-world few-shot learning scheme.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Willes_J/0/1/0/all/0/1"&gt;John Willes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harrison_J/0/1/0/all/0/1"&gt;James Harrison&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harakeh_A/0/1/0/all/0/1"&gt;Ali Harakeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1"&gt;Chelsea Finn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1"&gt;Marco Pavone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waslander_S/0/1/0/all/0/1"&gt;Steven Waslander&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Blind Room Parameter Estimation Using Multiple-Multichannel Speech Recordings. (arXiv:2107.13832v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.13832</id>
        <link href="http://arxiv.org/abs/2107.13832"/>
        <updated>2021-07-30T02:13:28.859Z</updated>
        <summary type="html"><![CDATA[Knowing the geometrical and acoustical parameters of a room may benefit
applications such as audio augmented reality, speech dereverberation or audio
forensics. In this paper, we study the problem of jointly estimating the total
surface area, the volume, as well as the frequency-dependent reverberation time
and mean surface absorption of a room in a blind fashion, based on two-channel
noisy speech recordings from multiple, unknown source-receiver positions. A
novel convolutional neural network architecture leveraging both single- and
inter-channel cues is proposed and trained on a large, realistic simulated
dataset. Results on both simulated and real data show that using multiple
observations in one room significantly reduces estimation errors and variances
on all target quantities, and that using two channels helps the estimation of
surface and volume. The proposed model outperforms a recently proposed blind
volume estimation method on the considered datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_P/0/1/0/all/0/1"&gt;Prerak Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deleforge_A/0/1/0/all/0/1"&gt;Antoine Deleforge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vincent_E/0/1/0/all/0/1"&gt;Emmanuel Vincent&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Continuity to Editability: Inverting GANs with Consecutive Images. (arXiv:2107.13812v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13812</id>
        <link href="http://arxiv.org/abs/2107.13812"/>
        <updated>2021-07-30T02:13:28.852Z</updated>
        <summary type="html"><![CDATA[Existing GAN inversion methods are stuck in a paradox that the inverted codes
can either achieve high-fidelity reconstruction, or retain the editing
capability. Having only one of them clearly cannot realize real image editing.
In this paper, we resolve this paradox by introducing consecutive images (\eg,
video frames or the same person with different poses) into the inversion
process. The rationale behind our solution is that the continuity of
consecutive images leads to inherent editable directions. This inborn property
is used for two unique purposes: 1) regularizing the joint inversion process,
such that each of the inverted code is semantically accessible from one of the
other and fastened in a editable domain; 2) enforcing inter-image coherence,
such that the fidelity of each inverted code can be maximized with the
complement of other images. Extensive experiments demonstrate that our
alternative significantly outperforms state-of-the-art methods in terms of
reconstruction fidelity and editability on both the real image dataset and
synthesis dataset. Furthermore, our method provides the first support of
video-based GAN inversion, and an interesting application of unsupervised
semantic transfer from consecutive images. Source code can be found at:
\url{https://github.com/Qingyang-Xu/InvertingGANs_with_ConsecutiveImgs}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yangyang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yong Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1"&gt;Wenpeng Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xuemiao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shengfeng He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Social Processes: Self-Supervised Forecasting of Nonverbal Cues in Social Conversations. (arXiv:2107.13576v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13576</id>
        <link href="http://arxiv.org/abs/2107.13576"/>
        <updated>2021-07-30T02:13:28.836Z</updated>
        <summary type="html"><![CDATA[The default paradigm for the forecasting of human behavior in social
conversations is characterized by top-down approaches. These involve
identifying predictive relationships between low level nonverbal cues and
future semantic events of interest (e.g. turn changes, group leaving). A common
hurdle however, is the limited availability of labeled data for supervised
learning. In this work, we take the first step in the direction of a bottom-up
self-supervised approach in the domain. We formulate the task of Social Cue
Forecasting to leverage the larger amount of unlabeled low-level behavior cues,
and characterize the modeling challenges involved. To address these, we take a
meta-learning approach and propose the Social Process (SP) models--socially
aware sequence-to-sequence (Seq2Seq) models within the Neural Process (NP)
family. SP models learn extractable representations of non-semantic future cues
for each participant, while capturing global uncertainty by jointly reasoning
about the future for all members of the group. Evaluation on synthesized and
real-world behavior data shows that our SP models achieve higher log-likelihood
than the NP baselines, and also highlights important considerations for
applying such techniques within the domain of social human interactions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raman_C/0/1/0/all/0/1"&gt;Chirag Raman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hung_H/0/1/0/all/0/1"&gt;Hayley Hung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loog_M/0/1/0/all/0/1"&gt;Marco Loog&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I2UV-HandNet: Image-to-UV Prediction Network for Accurate and High-fidelity 3D Hand Mesh Modeling. (arXiv:2102.03725v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03725</id>
        <link href="http://arxiv.org/abs/2102.03725"/>
        <updated>2021-07-30T02:13:28.830Z</updated>
        <summary type="html"><![CDATA[Reconstructing a high-precision and high-fidelity 3D human hand from a color
image plays a central role in replicating a realistic virtual hand in
human-computer interaction and virtual reality applications. The results of
current methods are lacking in accuracy and fidelity due to various hand poses
and severe occlusions. In this study, we propose an I2UV-HandNet model for
accurate hand pose and shape estimation as well as 3D hand super-resolution
reconstruction. Specifically, we present the first UV-based 3D hand shape
representation. To recover a 3D hand mesh from an RGB image, we design an
AffineNet to predict a UV position map from the input in an image-to-image
translation fashion. To obtain a higher fidelity shape, we exploit an
additional SRNet to transform the low-resolution UV map outputted by AffineNet
into a high-resolution one. For the first time, we demonstrate the
characterization capability of the UV-based hand shape representation. Our
experiments show that the proposed method achieves state-of-the-art performance
on several challenging benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Ping Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yujin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1"&gt;Dong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fangyin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Q/0/1/0/all/0/1"&gt;Qingpei Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1"&gt;Yong Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stereo Plane SLAM Based on Intersecting Lines. (arXiv:2008.08218v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.08218</id>
        <link href="http://arxiv.org/abs/2008.08218"/>
        <updated>2021-07-30T02:13:28.808Z</updated>
        <summary type="html"><![CDATA[Plane feature is a kind of stable landmark to reduce drift error in SLAM
system. It is easy and fast to extract planes from dense point cloud, which is
commonly acquired from RGB-D camera or lidar. But for stereo camera, it is hard
to compute dense point cloud accurately and efficiently. In this paper, we
propose a novel method to compute plane parameters using intersecting lines
which are extracted from the stereo image. The plane features commonly exist on
the surface of man-made objects and structure, which have regular shape and
straight edge lines. In 3D space, two intersecting lines can determine such a
plane. Thus we extract line segments from both stereo left and right image. By
stereo matching, we compute the endpoints and line directions in 3D space, and
then the planes from two intersecting lines. We discard those inaccurate plane
features in the frame tracking. Adding such plane features in stereo SLAM
system reduces the drift error and refines the performance. We test our
proposed system on public datasets and demonstrate its robust and accurate
estimation results, compared with state-of-the-art SLAM systems. To benefit the
research of plane-based SLAM, we release our codes at
https://github.com/fishmarch/Stereo-Plane-SLAM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaoyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1"&gt;Xianyu Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1"&gt;Ziwei Liao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for Multi-View Stereo via Plane Sweep: A Survey. (arXiv:2106.15328v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15328</id>
        <link href="http://arxiv.org/abs/2106.15328"/>
        <updated>2021-07-30T02:13:28.801Z</updated>
        <summary type="html"><![CDATA[3D reconstruction has lately attracted increasing attention due to its wide
application in many areas, such as autonomous driving, robotics and virtual
reality. As a dominant technique in artificial intelligence, deep learning has
been successfully adopted to solve various computer vision problems. However,
deep learning for 3D reconstruction is still at its infancy due to its unique
challenges and varying pipelines. To stimulate future research, this paper
presents a review of recent progress in deep learning methods for Multi-view
Stereo (MVS), which is considered as a crucial task of image-based 3D
reconstruction. It also presents comparative results on several publicly
available datasets, with insightful observations and inspiring future research
directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Qingtian Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Min_C/0/1/0/all/0/1"&gt;Chen Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1"&gt;Zizhuang Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yisong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guoping Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Open-World Entity Segmentation. (arXiv:2107.14228v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14228</id>
        <link href="http://arxiv.org/abs/2107.14228"/>
        <updated>2021-07-30T02:13:28.778Z</updated>
        <summary type="html"><![CDATA[We introduce a new image segmentation task, termed Entity Segmentation (ES)
with the aim to segment all visual entities in an image without considering
semantic category labels. It has many practical applications in image
manipulation/editing where the segmentation mask quality is typically crucial
but category labels are less important. In this setting, all
semantically-meaningful segments are equally treated as categoryless entities
and there is no thing-stuff distinction. Based on our unified entity
representation, we propose a center-based entity segmentation framework with
two novel modules to improve mask quality. Experimentally, both our new task
and framework demonstrate superior advantages as against existing work. In
particular, ES enables the following: (1) merging multiple datasets to form a
large training set without the need to resolve label conflicts; (2) any model
trained on one dataset can generalize exceptionally well to other datasets with
unseen domains. Our code is made publicly available at
https://github.com/dvlab-research/Entity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1"&gt;Lu Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuen_J/0/1/0/all/0/1"&gt;Jason Kuen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jiuxiang Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hengshuang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhe Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1"&gt;Jiaya Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Asymmetric Loss For Multi-Label Classification. (arXiv:2009.14119v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.14119</id>
        <link href="http://arxiv.org/abs/2009.14119"/>
        <updated>2021-07-30T02:13:28.754Z</updated>
        <summary type="html"><![CDATA[In a typical multi-label setting, a picture contains on average few positive
labels, and many negative ones. This positive-negative imbalance dominates the
optimization process, and can lead to under-emphasizing gradients from positive
labels during training, resulting in poor accuracy. In this paper, we introduce
a novel asymmetric loss ("ASL"), which operates differently on positive and
negative samples. The loss enables to dynamically down-weights and
hard-thresholds easy negative samples, while also discarding possibly
mislabeled samples. We demonstrate how ASL can balance the probabilities of
different samples, and how this balancing is translated to better mAP scores.
With ASL, we reach state-of-the-art results on multiple popular multi-label
datasets: MS-COCO, Pascal-VOC, NUS-WIDE and Open Images. We also demonstrate
ASL applicability for other tasks, such as single-label classification and
object detection. ASL is effective, easy to implement, and does not increase
the training time or complexity.

Implementation is available at: https://github.com/Alibaba-MIIL/ASL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1"&gt;Emanuel Ben-Baruch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1"&gt;Tal Ridnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamir_N/0/1/0/all/0/1"&gt;Nadav Zamir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1"&gt;Asaf Noy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Friedman_I/0/1/0/all/0/1"&gt;Itamar Friedman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Protter_M/0/1/0/all/0/1"&gt;Matan Protter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zelnik_Manor_L/0/1/0/all/0/1"&gt;Lihi Zelnik-Manor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Guided Disentanglement in Generative Networks. (arXiv:2107.14229v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14229</id>
        <link href="http://arxiv.org/abs/2107.14229"/>
        <updated>2021-07-30T02:13:28.747Z</updated>
        <summary type="html"><![CDATA[Image-to-image translation (i2i) networks suffer from entanglement effects in
presence of physics-related phenomena in target domain (such as occlusions,
fog, etc), thus lowering the translation quality and variability. In this
paper, we present a comprehensive method for disentangling physics-based traits
in the translation, guiding the learning process with neural or physical
models. For the latter, we integrate adversarial estimation and genetic
algorithms to correctly achieve disentanglement. The results show our approach
dramatically increase performances in many challenging scenarios for image
translation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pizzati_F/0/1/0/all/0/1"&gt;Fabio Pizzati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cerri_P/0/1/0/all/0/1"&gt;Pietro Cerri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1"&gt;Raoul de Charette&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Trajectory Prediction via Distribution Discrimination. (arXiv:2107.14204v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14204</id>
        <link href="http://arxiv.org/abs/2107.14204"/>
        <updated>2021-07-30T02:13:28.729Z</updated>
        <summary type="html"><![CDATA[Trajectory prediction is confronted with the dilemma to capture the
multi-modal nature of future dynamics with both diversity and accuracy. In this
paper, we present a distribution discrimination (DisDis) method to predict
personalized motion patterns by distinguishing the potential distributions.
Motivated by that the motion pattern of each person is personalized due to
his/her habit, our DisDis learns the latent distribution to represent different
motion patterns and optimize it by the contrastive discrimination. This
distribution discrimination encourages latent distributions to be more
discriminative. Our method can be integrated with existing multi-modal
stochastic predictive models as a plug-and-play module to learn the more
discriminative latent distribution. To evaluate the latent distribution, we
further propose a new metric, probability cumulative minimum distance (PCMD)
curve, which cumulatively calculates the minimum distance on the sorted
probabilities. Experimental results on the ETH and UCY datasets show the
effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guangyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Junlong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_N/0/1/0/all/0/1"&gt;Nuoxing Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1"&gt;Liangliang Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiwen Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do CNNs Encode Data Augmentations?. (arXiv:2003.08773v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.08773</id>
        <link href="http://arxiv.org/abs/2003.08773"/>
        <updated>2021-07-30T02:13:28.721Z</updated>
        <summary type="html"><![CDATA[Data augmentations are important ingredients in the recipe for training
robust neural networks, especially in computer vision. A fundamental question
is whether neural network features encode data augmentation transformations. To
answer this question, we introduce a systematic approach to investigate which
layers of neural networks are the most predictive of augmentation
transformations. Our approach uses features in pre-trained vision models with
minimal additional processing to predict common properties transformed by
augmentation (scale, aspect ratio, hue, saturation, contrast, and brightness).
Surprisingly, neural network features not only predict data augmentation
transformations, but they predict many transformations with high accuracy.
After validating that neural networks encode features corresponding to
augmentation transformations, we show that these features are encoded in the
early layers of modern CNNs, though the augmentation signal fades in deeper
layers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_E/0/1/0/all/0/1"&gt;Eddie Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yanping Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Visual Anomaly Detection for Task Execution Monitoring. (arXiv:2107.14206v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.14206</id>
        <link href="http://arxiv.org/abs/2107.14206"/>
        <updated>2021-07-30T02:13:28.698Z</updated>
        <summary type="html"><![CDATA[Execution monitoring is essential for robots to detect and respond to
failures. Since it is impossible to enumerate all failures for a given task, we
learn from successful executions of the task to detect visual anomalies during
runtime. Our method learns to predict the motions that occur during the nominal
execution of a task, including camera and robot body motion. A probabilistic
U-Net architecture is used to learn to predict optical flow, and the robot's
kinematics and 3D model are used to model camera and body motion. The errors
between the observed and predicted motion are used to calculate an anomaly
score. We evaluate our method on a dataset of a robot placing a book on a
shelf, which includes anomalies such as falling books, camera occlusions, and
robot disturbances. We find that modeling camera and body motion, in addition
to the learning-based optical flow prediction, results in an improvement of the
area under the receiver operating characteristic curve from 0.752 to 0.804, and
the area under the precision-recall curve from 0.467 to 0.549.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thoduka_S/0/1/0/all/0/1"&gt;Santosh Thoduka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1"&gt;Juergen Gall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ploger_P/0/1/0/all/0/1"&gt;Paul G. Pl&amp;#xf6;ger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Selective Feature Compression for Efficient Activity Recognition Inference. (arXiv:2104.00179v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00179</id>
        <link href="http://arxiv.org/abs/2104.00179"/>
        <updated>2021-07-30T02:13:28.681Z</updated>
        <summary type="html"><![CDATA[Most action recognition solutions rely on dense sampling to precisely cover
the informative temporal clip. Extensively searching temporal region is
expensive for a real-world application. In this work, we focus on improving the
inference efficiency of current action recognition backbones on trimmed videos,
and illustrate that one action model can also cover then informative region by
dropping non-informative features. We present Selective Feature Compression
(SFC), an action recognition inference strategy that greatly increase model
inference efficiency without any accuracy compromise. Differently from previous
works that compress kernel sizes and decrease the channel dimension, we propose
to compress feature flow at spatio-temporal dimension without changing any
backbone parameters. Our experiments on Kinetics-400, UCF101 and ActivityNet
show that SFC is able to reduce inference speed by 6-7x and memory usage by
5-6x compared with the commonly used 30 crops dense sampling procedure, while
also slightly improving Top1 Accuracy. We thoroughly quantitatively and
qualitatively evaluate SFC and all its components and show how does SFC learn
to attend to important video regions and to drop temporal features that are
uninformative for the task of action recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chunhui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xinyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Modolo_D/0/1/0/all/0/1"&gt;Davide Modolo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1"&gt;Joseph Tighe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain and View-point Agnostic Hand Action Recognition. (arXiv:2103.02303v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02303</id>
        <link href="http://arxiv.org/abs/2103.02303"/>
        <updated>2021-07-30T02:13:28.658Z</updated>
        <summary type="html"><![CDATA[Hand action recognition is a special case of action recognition with
applications in human-robot interaction, virtual reality or life-logging
systems. Building action classifiers able to work for such heterogeneous action
domains is very challenging. There are very subtle changes across different
actions from a given application but also large variations across domains (e.g.
virtual reality vs life-logging). This work introduces a novel skeleton-based
hand motion representation model that tackles this problem. The framework we
propose is agnostic to the application domain or camera recording view-point.
When working on a single domain (intra-domain action classification) our
approach performs better or similar to current state-of-the-art methods on
well-known hand action recognition benchmarks. And, more importantly, when
performing hand action recognition for action domains and camera perspectives
which our approach has not been trained for (cross-domain action
classification), our proposed framework achieves comparable performance to
intra-domain state-of-the-art methods. These experiments show the robustness
and generalization capabilities of our framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sabater_A/0/1/0/all/0/1"&gt;Alberto Sabater&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alonso_I/0/1/0/all/0/1"&gt;I&amp;#xf1;igo Alonso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Montesano_L/0/1/0/all/0/1"&gt;Luis Montesano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murillo_A/0/1/0/all/0/1"&gt;Ana C. Murillo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Advances aiding Recognition and Classification of Indian Monuments and Landmarks. (arXiv:2107.14070v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14070</id>
        <link href="http://arxiv.org/abs/2107.14070"/>
        <updated>2021-07-30T02:13:28.638Z</updated>
        <summary type="html"><![CDATA[Tourism in India plays a quintessential role in the country's economy with an
estimated 9.2% GDP share for the year 2018. With a yearly growth rate of 6.2%,
the industry holds a huge potential for being the primary driver of the economy
as observed in the nations of the Middle East like the United Arab Emirates.
The historical and cultural diversity exhibited throughout the geography of the
nation is a unique spectacle for people around the world and therefore serves
to attract tourists in tens of millions in number every year. Traditionally,
tour guides or academic professionals who study these heritage monuments were
responsible for providing information to the visitors regarding their
architectural and historical significance. However, unfortunately this system
has several caveats when considered on a large scale such as unavailability of
sufficient trained people, lack of accurate information, failure to convey the
richness of details in an attractive format etc. Recently, machine learning
approaches revolving around the usage of monument pictures have been shown to
be useful for rudimentary analysis of heritage sights. This paper serves as a
survey of the research endeavors undertaken in this direction which would
eventually provide insights for building an automated decision system that
could be utilized to make the experience of tourism in India more modernized
for visitors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1"&gt;Aditya Jyoti Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1"&gt;Smaranjit Ghose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_K/0/1/0/all/0/1"&gt;Kanishka Aggarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nethaji_N/0/1/0/all/0/1"&gt;Niketha Nethaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1"&gt;Shivam Pal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Purkayastha_A/0/1/0/all/0/1"&gt;Arnab Dutta Purkayastha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReFormer: The Relational Transformer for Image Captioning. (arXiv:2107.14178v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14178</id>
        <link href="http://arxiv.org/abs/2107.14178"/>
        <updated>2021-07-30T02:13:28.600Z</updated>
        <summary type="html"><![CDATA[Image captioning is shown to be able to achieve a better performance by using
scene graphs to represent the relations of objects in the image. The current
captioning encoders generally use a Graph Convolutional Net (GCN) to represent
the relation information and merge it with the object region features via
concatenation or convolution to get the final input for sentence decoding.
However, the GCN-based encoders in the existing methods are less effective for
captioning due to two reasons. First, using the image captioning as the
objective (i.e., Maximum Likelihood Estimation) rather than a relation-centric
loss cannot fully explore the potential of the encoder. Second, using a
pre-trained model instead of the encoder itself to extract the relationships is
not flexible and cannot contribute to the explainability of the model. To
improve the quality of image captioning, we propose a novel architecture
ReFormer -- a RElational transFORMER to generate features with relation
information embedded and to explicitly express the pair-wise relationships
between objects in the image. ReFormer incorporates the objective of scene
graph generation with that of image captioning using one modified Transformer
model. This design allows ReFormer to generate not only better image captions
with the bene-fit of extracting strong relational image features, but also
scene graphs to explicitly describe the pair-wise relation-ships. Experiments
on publicly available datasets show that our model significantly outperforms
state-of-the-art methods on image captioning and scene graph generation]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xuewen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yingru Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot and Continual Learning with Attentive Independent Mechanisms. (arXiv:2107.14053v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14053</id>
        <link href="http://arxiv.org/abs/2107.14053"/>
        <updated>2021-07-30T02:13:28.594Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) are known to perform well when deployed to test
distributions that shares high similarity with the training distribution.
Feeding DNNs with new data sequentially that were unseen in the training
distribution has two major challenges -- fast adaptation to new tasks and
catastrophic forgetting of old tasks. Such difficulties paved way for the
on-going research on few-shot learning and continual learning. To tackle these
problems, we introduce Attentive Independent Mechanisms (AIM). We incorporate
the idea of learning using fast and slow weights in conjunction with the
decoupling of the feature extraction and higher-order conceptual learning of a
DNN. AIM is designed for higher-order conceptual learning, modeled by a mixture
of experts that compete to learn independent concepts to solve a new task. AIM
is a modular component that can be inserted into existing deep learning
frameworks. We demonstrate its capability for few-shot learning by adding it to
SIB and trained on MiniImageNet and CIFAR-FS, showing significant improvement.
AIM is also applied to ANML and OML trained on Omniglot, CIFAR-100 and
MiniImageNet to demonstrate its capability in continual learning. Code made
publicly available at https://github.com/huang50213/AIM-Fewshot-Continual.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1"&gt;Eugene Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Cheng-Han Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chen-Yi Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral Leakage and Rethinking the Kernel Size in CNNs. (arXiv:2101.10143v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10143</id>
        <link href="http://arxiv.org/abs/2101.10143"/>
        <updated>2021-07-30T02:13:28.535Z</updated>
        <summary type="html"><![CDATA[Convolutional layers in CNNs implement linear filters which decompose the
input into different frequency bands. However, most modern architectures
neglect standard principles of filter design when optimizing their model
choices regarding the size and shape of the convolutional kernel. In this work,
we consider the well-known problem of spectral leakage caused by windowing
artifacts in filtering operations in the context of CNNs. We show that the
small size of CNN kernels make them susceptible to spectral leakage, which may
induce performance-degrading artifacts. To address this issue, we propose the
use of larger kernel sizes along with the Hamming window function to alleviate
leakage in CNN architectures. We demonstrate improved classification accuracy
on multiple benchmark datasets including Fashion-MNIST, CIFAR-10, CIFAR-100 and
ImageNet with the simple use of a standard window function in convolutional
layers. Finally, we show that CNNs employing the Hamming window display
increased robustness against various adversarial attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tomen_N/0/1/0/all/0/1"&gt;Nergis Tomen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1"&gt;Jan van Gemert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Adversarial Robustness via Test-time Transformation Ensembling. (arXiv:2107.14110v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14110</id>
        <link href="http://arxiv.org/abs/2107.14110"/>
        <updated>2021-07-30T02:13:28.493Z</updated>
        <summary type="html"><![CDATA[Deep learning models are prone to being fooled by imperceptible perturbations
known as adversarial attacks. In this work, we study how equipping models with
Test-time Transformation Ensembling (TTE) can work as a reliable defense
against such attacks. While transforming the input data, both at train and test
times, is known to enhance model performance, its effects on adversarial
robustness have not been studied. Here, we present a comprehensive empirical
study of the impact of TTE, in the form of widely-used image transforms, on
adversarial robustness. We show that TTE consistently improves model robustness
against a variety of powerful attacks without any need for re-training, and
that this improvement comes at virtually no trade-off with accuracy on clean
samples. Finally, we show that the benefits of TTE transfer even to the
certified robustness domain, in which TTE provides sizable and consistent
improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perez_J/0/1/0/all/0/1"&gt;Juan C. P&amp;#xe9;rez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1"&gt;Motasem Alfarra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeanneret_G/0/1/0/all/0/1"&gt;Guillaume Jeanneret&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rueda_L/0/1/0/all/0/1"&gt;Laura Rueda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1"&gt;Ali Thabet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1"&gt;Bernard Ghanem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1"&gt;Pablo Arbel&amp;#xe1;ez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking and Improving Relative Position Encoding for Vision Transformer. (arXiv:2107.14222v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14222</id>
        <link href="http://arxiv.org/abs/2107.14222"/>
        <updated>2021-07-30T02:13:28.486Z</updated>
        <summary type="html"><![CDATA[Relative position encoding (RPE) is important for transformer to capture
sequence ordering of input tokens. General efficacy has been proven in natural
language processing. However, in computer vision, its efficacy is not well
studied and even remains controversial, e.g., whether relative position
encoding can work equally well as absolute position? In order to clarify this,
we first review existing relative position encoding methods and analyze their
pros and cons when applied in vision transformers. We then propose new relative
position encoding methods dedicated to 2D images, called image RPE (iRPE). Our
methods consider directional relative distance modeling as well as the
interactions between queries and relative position embeddings in self-attention
mechanism. The proposed iRPE methods are simple and lightweight. They can be
easily plugged into transformer blocks. Experiments demonstrate that solely due
to the proposed encoding methods, DeiT and DETR obtain up to 1.5% (top-1 Acc)
and 1.3% (mAP) stable improvements over their original versions on ImageNet and
COCO respectively, without tuning any extra hyperparameters such as learning
rate and weight decay. Our ablation and analysis also yield interesting
findings, some of which run counter to previous understanding. Code and models
are open-sourced at https://github.com/microsoft/Cream/tree/main/iRPE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1"&gt;Kan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Houwen Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Minghao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1"&gt;Jianlong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_H/0/1/0/all/0/1"&gt;Hongyang Chao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fully-Automatic Pipeline for Document Signature Analysis to Detect Money Laundering Activities. (arXiv:2107.14091v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14091</id>
        <link href="http://arxiv.org/abs/2107.14091"/>
        <updated>2021-07-30T02:13:28.479Z</updated>
        <summary type="html"><![CDATA[Signatures present on corporate documents are often used in investigations of
relationships between persons of interest, and prior research into the task of
offline signature verification has evaluated a wide range of methods on
standard signature datasets. However, such tasks often benefit from prior human
supervision in the collection, adjustment and labelling of isolated signature
images from which all real-world context has been removed. Signatures found in
online document repositories such as the United Kingdom Companies House
regularly contain high variation in location, size, quality and degrees of
obfuscation under stamps. We propose an integrated pipeline of signature
extraction and curation, with no human assistance from the obtaining of company
documents to the clustering of individual signatures. We use a sequence of
heuristic methods, convolutional neural networks, generative adversarial
networks and convolutional Siamese networks for signature extraction,
filtering, cleaning and embedding respectively. We evaluate both the
effectiveness of the pipeline at matching obscured same-author signature pairs
and the effectiveness of the entire pipeline against a human baseline for
document signature analysis, as well as presenting uses for such a pipeline in
the field of real-world anti-money laundering investigation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Woodruff_N/0/1/0/all/0/1"&gt;Nikhil Woodruff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Enshaei_A/0/1/0/all/0/1"&gt;Amir Enshaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_B/0/1/0/all/0/1"&gt;Bashar Awwad Shiekh Hasan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Efficient Pyramid Transformer for Semantic Segmentation. (arXiv:2107.14209v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14209</id>
        <link href="http://arxiv.org/abs/2107.14209"/>
        <updated>2021-07-30T02:13:28.472Z</updated>
        <summary type="html"><![CDATA[Semantic segmentation is a challenging problem due to difficulties in
modeling context in complex scenes and class confusions along boundaries. Most
literature either focuses on context modeling or boundary refinement, which is
less generalizable in open-world scenarios. In this work, we advocate a unified
framework(UN-EPT) to segment objects by considering both context information
and boundary artifacts. We first adapt a sparse sampling strategy to
incorporate the transformer-based attention mechanism for efficient context
modeling. In addition, a separate spatial branch is introduced to capture image
details for boundary refinement. The whole model can be trained in an
end-to-end manner. We demonstrate promising performance on three popular
benchmarks for semantic segmentation with low memory footprint. Code will be
released soon.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1"&gt;Fangrui Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Li Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chongruo Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yanwei Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mu Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-View Tracking for Multi-Human 3D Pose Estimation at over 100 FPS. (arXiv:2003.03972v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.03972</id>
        <link href="http://arxiv.org/abs/2003.03972"/>
        <updated>2021-07-30T02:13:28.456Z</updated>
        <summary type="html"><![CDATA[Estimating 3D poses of multiple humans in real-time is a classic but still
challenging task in computer vision. Its major difficulty lies in the ambiguity
in cross-view association of 2D poses and the huge state space when there are
multiple people in multiple views. In this paper, we present a novel solution
for multi-human 3D pose estimation from multiple calibrated camera views. It
takes 2D poses in different camera coordinates as inputs and aims for the
accurate 3D poses in the global coordinate. Unlike previous methods that
associate 2D poses among all pairs of views from scratch at every frame, we
exploit the temporal consistency in videos to match the 2D inputs with 3D poses
directly in 3-space. More specifically, we propose to retain the 3D pose for
each person and update them iteratively via the cross-view multi-human
tracking. This novel formulation improves both accuracy and efficiency, as we
demonstrated on widely-used public datasets. To further verify the scalability
of our method, we propose a new large-scale multi-human dataset with 12 to 28
camera views. Without bells and whistles, our solution achieves 154 FPS on 12
cameras and 34 FPS on 28 cameras, indicating its ability to handle large-scale
real-world applications. The proposed dataset is released at
https://github.com/longcw/crossview_3d_pose_tracking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Long Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ai_H/0/1/0/all/0/1"&gt;Haizhou Ai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1"&gt;Rui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1"&gt;Zijie Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shuang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Swap-Free Fat-Water Separation in Dixon MRI using Conditional Generative Adversarial Networks. (arXiv:2107.14175v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.14175</id>
        <link href="http://arxiv.org/abs/2107.14175"/>
        <updated>2021-07-30T02:13:28.449Z</updated>
        <summary type="html"><![CDATA[Dixon MRI is widely used for body composition studies. Current processing
methods associated with large whole-body volumes are time intensive and prone
to artifacts during fat-water separation performed on the scanner, making the
data difficult to analyse. The most common artifact are fat-water swaps, where
the labels are inverted at the voxel level. It is common for researchers to
discard swapped data (generally around 10%), which can be wasteful and lead to
unintended biases. The UK Biobank is acquiring Dixon MRI for over 100,000
participants, and thousands of swaps will occur. If those go undetected, errors
will propagate into processes such as abdominal organ segmentation and dilute
the results in population-based analyses. There is a clear need for a fast and
robust method to accurately separate fat and water channels. In this work we
propose such a method based on style transfer using a conditional generative
adversarial network. We also introduce a new Dixon loss function for the
generator model. Using data from the UK Biobank Dixon MRI, our model is able to
predict highly accurate fat and water channels that are free from artifacts. We
show that the model separates fat and water channels using either single input
(in-phase) or dual input (in-phase and opposed-phase), with the latter
producing improved results. Our proposed method enables faster and more
accurate downstream analysis of body composition from Dixon MRI in population
studies by eliminating the need for visual inspection or discarding data due to
fat-water swaps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Basty_N/0/1/0/all/0/1"&gt;Nicolas Basty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thanaj_M/0/1/0/all/0/1"&gt;Marjola Thanaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cule_M/0/1/0/all/0/1"&gt;Madeleine Cule&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sorokin_E/0/1/0/all/0/1"&gt;Elena P. Sorokin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bell_J/0/1/0/all/0/1"&gt;Jimmy D. Bell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thomas_E/0/1/0/all/0/1"&gt;E. Louise Thomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Whitcher_B/0/1/0/all/0/1"&gt;Brandon Whitcher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovering 3D Parts from Image Collections. (arXiv:2107.13629v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13629</id>
        <link href="http://arxiv.org/abs/2107.13629"/>
        <updated>2021-07-30T02:13:28.442Z</updated>
        <summary type="html"><![CDATA[Reasoning 3D shapes from 2D images is an essential yet challenging task,
especially when only single-view images are at our disposal. While an object
can have a complicated shape, individual parts are usually close to geometric
primitives and thus are easier to model. Furthermore, parts provide a mid-level
representation that is robust to appearance variations across objects in a
particular category. In this work, we tackle the problem of 3D part discovery
from only 2D image collections. Instead of relying on manually annotated parts
for supervision, we propose a self-supervised approach, latent part discovery
(LPD). Our key insight is to learn a novel part shape prior that allows each
part to fit an object shape faithfully while constrained to have simple
geometry. Extensive experiments on the synthetic ShapeNet, PartNet, and
real-world Pascal 3D+ datasets show that our method discovers consistent object
parts and achieves favorable reconstruction accuracy compared to the existing
methods with the same level of supervision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1"&gt;Chun-Han Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hung_W/0/1/0/all/0/1"&gt;Wei-Chih Hung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1"&gt;Varun Jampani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Ming-Hsuan Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FREE: Feature Refinement for Generalized Zero-Shot Learning. (arXiv:2107.13807v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13807</id>
        <link href="http://arxiv.org/abs/2107.13807"/>
        <updated>2021-07-30T02:13:28.436Z</updated>
        <summary type="html"><![CDATA[Generalized zero-shot learning (GZSL) has achieved significant progress, with
many efforts dedicated to overcoming the problems of visual-semantic domain gap
and seen-unseen bias. However, most existing methods directly use feature
extraction models trained on ImageNet alone, ignoring the cross-dataset bias
between ImageNet and GZSL benchmarks. Such a bias inevitably results in
poor-quality visual features for GZSL tasks, which potentially limits the
recognition performance on both seen and unseen classes. In this paper, we
propose a simple yet effective GZSL method, termed feature refinement for
generalized zero-shot learning (FREE), to tackle the above problem. FREE
employs a feature refinement (FR) module that incorporates
\textit{semantic$\rightarrow$visual} mapping into a unified generative model to
refine the visual features of seen and unseen class samples. Furthermore, we
propose a self-adaptive margin center loss (SAMC-loss) that cooperates with a
semantic cycle-consistency loss to guide FR to learn class- and
semantically-relevant representations, and concatenate the features in FR to
extract the fully refined features. Extensive experiments on five benchmark
datasets demonstrate the significant performance gain of FREE over its baseline
and current state-of-the-art methods. Our codes are available at
https://github.com/shiming-chen/FREE .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shiming Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenjie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1"&gt;Beihao Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1"&gt;Qinmu Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1"&gt;Xinge You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1"&gt;Feng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Does TERRA-REF's High Resolution, Multi Sensor Plant Sensing Public Domain Data Offer the Computer Vision Community?. (arXiv:2107.14072v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14072</id>
        <link href="http://arxiv.org/abs/2107.14072"/>
        <updated>2021-07-30T02:13:28.427Z</updated>
        <summary type="html"><![CDATA[A core objective of the TERRA-REF project was to generate an open-access
reference dataset for the study of evaluation of sensing technology to study
plants under field conditions. The TERRA-REF program deployed a suite of high
resolution, cutting edge technology sensors on a gantry system with the aim of
scanning 1 hectare (~$10^4$ m) at around $1 mm^2$ spatial resolution multiple
times per week. The system contains co-located sensors including a stereo-pair
RGB camera, a thermal imager, a laser scanner to capture 3D structure, and two
hyperspectral cameras covering wavelengths of 300-2500nm. This sensor data is
provided alongside over sixty types of traditional plant measurements that can
be used to train new machine learning models. Associated weather and
environmental measurements, information about agronomic management and
experimental design, and the genomic sequences of hundreds of plant varieties
have been collected and are available alongside the sensor and plant trait
(phenotype) data.

Over the course of four years and ten growing seasons, the TERRA-REF system
generated over 1 PB of sensor data and almost 45 million files. The subset that
has been released to the public domain accounts for two seasons and about half
of the total data volume. This provides an unprecedented opportunity for
investigations far beyond the core biological scope of the project.

This focus of this paper is to provide the Computer Vision and Machine
Learning communities an overview of the available data and some potential
applications of this one of a kind data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+LeBauer_D/0/1/0/all/0/1"&gt;David LeBauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnette_M/0/1/0/all/0/1"&gt;Max Burnette&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fahlgren_N/0/1/0/all/0/1"&gt;Noah Fahlgren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kooper_R/0/1/0/all/0/1"&gt;Rob Kooper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McHenry_K/0/1/0/all/0/1"&gt;Kenton McHenry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stylianou_A/0/1/0/all/0/1"&gt;Abby Stylianou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy-Preserving Portrait Matting. (arXiv:2104.14222v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14222</id>
        <link href="http://arxiv.org/abs/2104.14222"/>
        <updated>2021-07-30T02:13:28.420Z</updated>
        <summary type="html"><![CDATA[Recently, there has been an increasing concern about the privacy issue raised
by using personally identifiable information in machine learning. However,
previous portrait matting methods were all based on identifiable portrait
images. To fill the gap, we present P3M-10k in this paper, which is the first
large-scale anonymized benchmark for Privacy-Preserving Portrait Matting.
P3M-10k consists of 10,000 high-resolution face-blurred portrait images along
with high-quality alpha mattes. We systematically evaluate both trimap-free and
trimap-based matting methods on P3M-10k and find that existing matting methods
show different generalization capabilities when following the
Privacy-Preserving Training (PPT) setting, i.e., training on face-blurred
images and testing on arbitrary images. To devise a better trimap-free portrait
matting model, we propose P3M-Net, which leverages the power of a unified
framework for both semantic perception and detail matting, and specifically
emphasizes the interaction between them and the encoder to facilitate the
matting process. Extensive experiments on P3M-10k demonstrate that P3M-Net
outperforms the state-of-the-art methods in terms of both objective metrics and
subjective visual quality. Besides, it shows good generalization capacity under
the PPT setting, confirming the value of P3M-10k for facilitating future
research and enabling potential real-world applications. The source code and
dataset are available at https://github.com/JizhiziLi/P3M]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jizhizi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Sihan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PPT Fusion: Pyramid Patch Transformerfor a Case Study in Image Fusion. (arXiv:2107.13967v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13967</id>
        <link href="http://arxiv.org/abs/2107.13967"/>
        <updated>2021-07-30T02:13:28.404Z</updated>
        <summary type="html"><![CDATA[The Transformer architecture has achieved rapiddevelopment in recent years,
outperforming the CNN archi-tectures in many computer vision tasks, such as the
VisionTransformers (ViT) for image classification. However, existingvisual
transformer models aim to extract semantic informationfor high-level tasks such
as classification and detection, distortingthe spatial resolution of the input
image, thus sacrificing thecapacity in reconstructing the input or generating
high-resolutionimages. In this paper, therefore, we propose a Patch
PyramidTransformer(PPT) to effectively address the above issues. Specif-ically,
we first design a Patch Transformer to transform theimage into a sequence of
patches, where transformer encodingis performed for each patch to extract local
representations.In addition, we construct a Pyramid Transformer to
effectivelyextract the non-local information from the entire image.
Afterobtaining a set of multi-scale, multi-dimensional, and multi-anglefeatures
of the original image, we design the image reconstructionnetwork to ensure that
the features can be reconstructed intothe original input. To validate the
effectiveness, we apply theproposed Patch Pyramid Transformer to the image
fusion task.The experimental results demonstrate its superior
performanceagainst the state-of-the-art fusion approaches, achieving the
bestresults on several evaluation indicators. The underlying capacityof the PPT
network is reflected by its universal power in featureextraction and image
reconstruction, which can be directlyapplied to different image fusion tasks
without redesigning orretraining the network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yu Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1"&gt;TianYang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;XiaoJun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kittler_J/0/1/0/all/0/1"&gt;Josef Kittler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cascaded Residual Density Network for Crowd Counting. (arXiv:2107.13718v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13718</id>
        <link href="http://arxiv.org/abs/2107.13718"/>
        <updated>2021-07-30T02:13:28.394Z</updated>
        <summary type="html"><![CDATA[Crowd counting is a challenging task due to the issues such as scale
variation and perspective variation in real crowd scenes. In this paper, we
propose a novel Cascaded Residual Density Network (CRDNet) in a coarse-to-fine
approach to generate the high-quality density map for crowd counting more
accurately. (1) We estimate the residual density maps by multi-scale pyramidal
features through cascaded residual density modules. It can improve the quality
of density map layer by layer effectively. (2) A novel additional local count
loss is presented to refine the accuracy of crowd counting, which reduces the
errors of pixel-wise Euclidean loss by restricting the number of people in the
local crowd areas. Experiments on two public benchmark datasets show that the
proposed method achieves effective improvement compared with the
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1"&gt;Kun Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"&gt;Luchuan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1"&gt;Qi Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1"&gt;Nenghai Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Image Semantic Segmentation. (arXiv:2107.13978v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13978</id>
        <link href="http://arxiv.org/abs/2107.13978"/>
        <updated>2021-07-30T02:13:28.375Z</updated>
        <summary type="html"><![CDATA[Semantic segmentation models trained on public datasets have achieved great
success in recent years. However, these models didn't consider the
personalization issue of segmentation though it is important in practice. In
this paper, we address the problem of personalized image segmentation. The
objective is to generate more accurate segmentation results on unlabeled
personalized images by investigating the data's personalized traits. To open up
future research in this area, we collect a large dataset containing various
users' personalized images called PIS (Personalized Image Semantic
Segmentation). We also survey some recent researches related to this problem
and report their performance on our dataset. Furthermore, by observing the
correlation among a user's personalized images, we propose a baseline method
that incorporates the inter-image context when segmenting certain images.
Extensive experiments show that our method outperforms the existing methods on
the proposed dataset. The code and the PIS dataset will be made publicly
available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chang-Bin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1"&gt;Peng-Tao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_F/0/1/0/all/0/1"&gt;Feng Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1"&gt;Ming-Ming Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structure and Performance of Fully Connected Neural Networks: Emerging Complex Network Properties. (arXiv:2107.14062v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.14062</id>
        <link href="http://arxiv.org/abs/2107.14062"/>
        <updated>2021-07-30T02:13:28.368Z</updated>
        <summary type="html"><![CDATA[Understanding the behavior of Artificial Neural Networks is one of the main
topics in the field recently, as black-box approaches have become usual since
the widespread of deep learning. Such high-dimensional models may manifest
instabilities and weird properties that resemble complex systems. Therefore, we
propose Complex Network (CN) techniques to analyze the structure and
performance of fully connected neural networks. For that, we build a dataset
with 4 thousand models and their respective CN properties. They are employed in
a supervised classification setup considering four vision benchmarks. Each
neural network is approached as a weighted and undirected graph of neurons and
synapses, and centrality measures are computed after training. Results show
that these measures are highly related to the network classification
performance. We also propose the concept of Bag-Of-Neurons (BoN), a CN-based
approach for finding topological signatures linking similar neurons. Results
suggest that six neuronal types emerge in such networks, independently of the
target domain, and are distributed differently according to classification
accuracy. We also tackle specific CN properties related to performance, such as
higher subgraph centrality on lower-performing models. Our findings suggest
that CN properties play a critical role in the performance of fully connected
neural networks, with topological patterns emerging independently on a wide
range of models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Scabini_L/0/1/0/all/0/1"&gt;Leonardo F. S. Scabini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruno_O/0/1/0/all/0/1"&gt;Odemir M. Bruno&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Importance-aware Transferable Adversarial Attacks. (arXiv:2107.14185v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14185</id>
        <link href="http://arxiv.org/abs/2107.14185"/>
        <updated>2021-07-30T02:13:28.353Z</updated>
        <summary type="html"><![CDATA[Transferability of adversarial examples is of central importance for
attacking an unknown model, which facilitates adversarial attacks in more
practical scenarios, e.g., blackbox attacks. Existing transferable attacks tend
to craft adversarial examples by indiscriminately distorting features to
degrade prediction accuracy in a source model without aware of intrinsic
features of objects in the images. We argue that such brute-force degradation
would introduce model-specific local optimum into adversarial examples, thus
limiting the transferability. By contrast, we propose the Feature
Importance-aware Attack (FIA), which disrupts important object-aware features
that dominate model decisions consistently. More specifically, we obtain
feature importance by introducing the aggregate gradient, which averages the
gradients with respect to feature maps of the source model, computed on a batch
of random transforms of the original clean image. The gradients will be highly
correlated to objects of interest, and such correlation presents invariance
across different models. Besides, the random transforms will preserve intrinsic
features of objects and suppress model-specific information. Finally, the
feature importance guides to search for adversarial examples towards disrupting
critical features, achieving stronger transferability. Extensive experimental
evaluation demonstrates the effectiveness and superior performance of the
proposed FIA, i.e., improving the success rate by 8.4% against normally trained
models and 11.7% against defense models as compared to the state-of-the-art
transferable attacks. Code is available at: https://github.com/hcguoO0/FIA]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhibo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Hengchang Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhifei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wenxin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1"&gt;Zhan Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1"&gt;Kui Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video Generation from Text Employing Latent Path Construction for Temporal Modeling. (arXiv:2107.13766v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13766</id>
        <link href="http://arxiv.org/abs/2107.13766"/>
        <updated>2021-07-30T02:13:28.342Z</updated>
        <summary type="html"><![CDATA[Video generation is one of the most challenging tasks in Machine Learning and
Computer Vision fields of study. In this paper, we tackle the text to video
generation problem, which is a conditional form of video generation. Humans can
listen/read natural language sentences, and can imagine or visualize what is
being described; therefore, we believe that video generation from natural
language sentences will have an important impact on Artificial Intelligence.
Video generation is relatively a new field of study in Computer Vision, which
is far from being solved. The majority of recent works deal with synthetic
datasets or real datasets with very limited types of objects, scenes, and
emotions. To the best of our knowledge, this is the very first work on the text
(free-form sentences) to video generation on more realistic video datasets like
Actor and Action Dataset (A2D) or UCF101. We tackle the complicated problem of
video generation by regressing the latent representations of the first and last
frames and employing a context-aware interpolation method to build the latent
representations of in-between frames. We propose a stacking ``upPooling'' block
to sequentially generate RGB frames out of each latent representation and
progressively increase the resolution. Moreover, our proposed Discriminator
encodes videos based on single and multiple frames. We provide quantitative and
qualitative results to support our arguments and show the superiority of our
method over well-known baselines like Recurrent Neural Network (RNN) and
Deconvolution (as known as Convolutional Transpose) based video generation
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mazaheri_A/0/1/0/all/0/1"&gt;Amir Mazaheri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Mubarak Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mapping Vulnerable Populations with AI. (arXiv:2107.14123v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14123</id>
        <link href="http://arxiv.org/abs/2107.14123"/>
        <updated>2021-07-30T02:13:28.335Z</updated>
        <summary type="html"><![CDATA[Humanitarian actions require accurate information to efficiently delegate
support operations. Such information can be maps of building footprints,
building functions, and population densities. While the access to this
information is comparably easy in industrialized countries thanks to reliable
census data and national geo-data infrastructures, this is not the case for
developing countries, where that data is often incomplete or outdated. Building
maps derived from remote sensing images may partially remedy this challenge in
such countries, but are not always accurate due to different landscape
configurations and lack of validation data. Even when they exist, building
footprint layers usually do not reveal more fine-grained building properties,
such as the number of stories or the building's function (e.g., office,
residential, school, etc.). In this project we aim to automate building
footprint and function mapping using heterogeneous data sources. In a first
step, we intend to delineate buildings from satellite data, using deep learning
models for semantic image segmentation. Building functions shall be retrieved
by parsing social media data like for instance tweets, as well as ground-based
imagery, to automatically identify different buildings functions and retrieve
further information such as the number of building stories. Building maps
augmented with those additional attributes make it possible to derive more
accurate population density maps, needed to support the targeted provision of
humanitarian aid.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kellenberger_B/0/1/0/all/0/1"&gt;Benjamin Kellenberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vargas_Munoz_J/0/1/0/all/0/1"&gt;John E. Vargas-Mu&amp;#xf1;oz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuia_D/0/1/0/all/0/1"&gt;Devis Tuia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daudt_R/0/1/0/all/0/1"&gt;Rodrigo C. Daudt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1"&gt;Konrad Schindler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whelan_T/0/1/0/all/0/1"&gt;Thao T-T Whelan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayo_B/0/1/0/all/0/1"&gt;Brenda Ayo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ofli_F/0/1/0/all/0/1"&gt;Ferda Ofli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Imran_M/0/1/0/all/0/1"&gt;Muhammad Imran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Camera Feature Prediction for Intra-Camera Supervised Person Re-identification across Distant Scenes. (arXiv:2107.13904v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13904</id>
        <link href="http://arxiv.org/abs/2107.13904"/>
        <updated>2021-07-30T02:13:28.325Z</updated>
        <summary type="html"><![CDATA[Person re-identification (Re-ID) aims to match person images across
non-overlapping camera views. The majority of Re-ID methods focus on
small-scale surveillance systems in which each pedestrian is captured in
different camera views of adjacent scenes. However, in large-scale surveillance
systems that cover larger areas, it is required to track a pedestrian of
interest across distant scenes (e.g., a criminal suspect escapes from one city
to another). Since most pedestrians appear in limited local areas, it is
difficult to collect training data with cross-camera pairs of the same person.
In this work, we study intra-camera supervised person re-identification across
distant scenes (ICS-DS Re-ID), which uses cross-camera unpaired data with
intra-camera identity labels for training. It is challenging as cross-camera
paired data plays a crucial role for learning camera-invariant features in most
existing Re-ID methods. To learn camera-invariant representation from
cross-camera unpaired training data, we propose a cross-camera feature
prediction method to mine cross-camera self supervision information from
camera-specific feature distribution by transforming fake cross-camera positive
feature pairs and minimize the distances of the fake pairs. Furthermore, we
automatically localize and extract local-level feature by a transformer. Joint
learning of global-level and local-level features forms a global-local
cross-camera feature prediction scheme for mining fine-grained cross-camera
self supervision information. Finally, cross-camera self supervision and
intra-camera supervision are aggregated in a framework. The experiments are
conducted in the ICS-DS setting on Market-SCT, Duke-SCT and MSMT17-SCT
datasets. The evaluation results demonstrate the superiority of our method,
which gains significant improvements of 15.4 Rank-1 and 22.3 mAP on Market-SCT
as compared to the second best method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1"&gt;Wenhang Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1"&gt;Chunyan Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1"&gt;Ancong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Hongwei Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1"&gt;Wei-Shi Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrent U-net for automatic pelvic floor muscle segmentation on 3D ultrasound. (arXiv:2107.13833v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.13833</id>
        <link href="http://arxiv.org/abs/2107.13833"/>
        <updated>2021-07-30T02:13:28.318Z</updated>
        <summary type="html"><![CDATA[The prevalance of pelvic floor problems is high within the female population.
Transperineal ultrasound (TPUS) is the main imaging modality used to
investigate these problems. Automating the analysis of TPUS data will help in
growing our understanding of pelvic floor related problems. In this study we
present a U-net like neural network with some convolutional long short term
memory (CLSTM) layers to automate the 3D segmentation of the levator ani muscle
(LAM) in TPUS volumes. The CLSTM layers are added to preserve the inter-slice
3D information. We reach human level performance on this segmentation task.
Therefore, we conclude that we successfully automated the segmentation of the
LAM on 3D TPUS data. This paves the way towards automatic in-vivo analysis of
the LAM mechanics in the context of large study populations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Noort_F/0/1/0/all/0/1"&gt;Frieda van den Noort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sirmacek_B/0/1/0/all/0/1"&gt;Beril Sirmacek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Slump_C/0/1/0/all/0/1"&gt;Cornelis H. Slump&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Abnormal Behavior Detection Based on Target Analysis. (arXiv:2107.13706v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13706</id>
        <link href="http://arxiv.org/abs/2107.13706"/>
        <updated>2021-07-30T02:13:28.312Z</updated>
        <summary type="html"><![CDATA[Abnormal behavior detection in surveillance video is a pivotal part of the
intelligent city. Most existing methods only consider how to detect anomalies,
with less considering to explain the reason of the anomalies. We investigate an
orthogonal perspective based on the reason of these abnormal behaviors. To this
end, we propose a multivariate fusion method that analyzes each target through
three branches: object, action and motion. The object branch focuses on the
appearance information, the motion branch focuses on the distribution of the
motion features, and the action branch focuses on the action category of the
target. The information that these branches focus on is different, and they can
complement each other and jointly detect abnormal behavior. The final abnormal
score can then be obtained by combining the abnormal scores of the three
branches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"&gt;Luchuan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Huihui Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1"&gt;Qi Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1"&gt;Nenghai Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Excavating AI" Re-excavated: Debunking a Fallacious Account of the JAFFE Dataset. (arXiv:2107.13998v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2107.13998</id>
        <link href="http://arxiv.org/abs/2107.13998"/>
        <updated>2021-07-30T02:13:28.297Z</updated>
        <summary type="html"><![CDATA[Twenty-five years ago, my colleagues Miyuki Kamachi and Jiro Gyoba and I
designed and photographed JAFFE, a set of facial expression images intended for
use in a study of face perception. In 2019, without seeking permission or
informing us, Kate Crawford and Trevor Paglen exhibited JAFFE in two widely
publicized art shows. In addition, they published a nonfactual account of the
images in the essay "Excavating AI: The Politics of Images in Machine Learning
Training Sets." The present article recounts the creation of the JAFFE dataset
and unravels each of Crawford and Paglen's fallacious statements. I also
discuss JAFFE more broadly in connection with research on facial expression,
affective computing, and human-computer interaction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lyons_M/0/1/0/all/0/1"&gt;Michael J. Lyons&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why You Should Try the Real Data for the Scene Text Recognition. (arXiv:2107.13938v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13938</id>
        <link href="http://arxiv.org/abs/2107.13938"/>
        <updated>2021-07-30T02:13:28.291Z</updated>
        <summary type="html"><![CDATA[Recent works in the text recognition area have pushed forward the recognition
results to the new horizons. But for a long time a lack of large human-labeled
natural text recognition datasets has been forcing researchers to use synthetic
data for training text recognition models. Even though synthetic datasets are
very large (MJSynth and SynthTest, two most famous synthetic datasets, have
several million images each), their diversity could be insufficient, compared
to natural datasets like ICDAR and others. Fortunately, the recently released
text-recognition annotation for OpenImages V5 dataset has comparable with
synthetic dataset number of instances and more diverse examples. We have used
this annotation with a Text Recognition head architecture from the Yet Another
Mask Text Spotter and got comparable to the SOTA results. On some datasets we
have even outperformed previous SOTA models. In this paper we also introduce a
text recognition model. The model's code is available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Loginov_V/0/1/0/all/0/1"&gt;Vladimir Loginov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Need and Status of Sea Turtle Conservation and Survey of Associated Computer Vision Advances. (arXiv:2107.14061v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14061</id>
        <link href="http://arxiv.org/abs/2107.14061"/>
        <updated>2021-07-30T02:13:28.233Z</updated>
        <summary type="html"><![CDATA[For over hundreds of millions of years, sea turtles and their ancestors have
swum in the vast expanses of the ocean. They have undergone a number of
evolutionary changes, leading to speciation and sub-speciation. However, in the
past few decades, some of the most notable forces driving the genetic variance
and population decline have been global warming and anthropogenic impact
ranging from large-scale poaching, collecting turtle eggs for food, besides
dumping trash including plastic waste into the ocean. This leads to severe
detrimental effects in the sea turtle population, driving them to extinction.
This research focusses on the forces causing the decline in sea turtle
population, the necessity for the global conservation efforts along with its
successes and failures, followed by an in-depth analysis of the modern advances
in detection and recognition of sea turtles, involving Machine Learning and
Computer Vision systems, aiding the conservation efforts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1"&gt;Aditya Jyoti Paul&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Active Learning with Temporal Output Discrepancy. (arXiv:2107.14153v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14153</id>
        <link href="http://arxiv.org/abs/2107.14153"/>
        <updated>2021-07-30T02:13:28.218Z</updated>
        <summary type="html"><![CDATA[While deep learning succeeds in a wide range of tasks, it highly depends on
the massive collection of annotated data which is expensive and time-consuming.
To lower the cost of data annotation, active learning has been proposed to
interactively query an oracle to annotate a small proportion of informative
samples in an unlabeled dataset. Inspired by the fact that the samples with
higher loss are usually more informative to the model than the samples with
lower loss, in this paper we present a novel deep active learning approach that
queries the oracle for data annotation when the unlabeled sample is believed to
incorporate high loss. The core of our approach is a measurement Temporal
Output Discrepancy (TOD) that estimates the sample loss by evaluating the
discrepancy of outputs given by models at different optimization steps. Our
theoretical investigation shows that TOD lower-bounds the accumulated sample
loss thus it can be used to select informative unlabeled samples. On basis of
TOD, we further develop an effective unlabeled data sampling strategy as well
as an unsupervised learning criterion that enhances model performance by
incorporating the unlabeled data. Due to the simplicity of TOD, our active
learning approach is efficient, flexible, and task-agnostic. Extensive
experimental results demonstrate that our approach achieves superior
performances than the state-of-the-art active learning methods on image
classification and semantic segmentation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Siyu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Haoyi Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huan_J/0/1/0/all/0/1"&gt;Jun Huan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1"&gt;Dejing Dou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Paced Contrastive Learning for Semi-supervisedMedical Image Segmentation with Meta-labels. (arXiv:2107.13741v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13741</id>
        <link href="http://arxiv.org/abs/2107.13741"/>
        <updated>2021-07-30T02:13:28.205Z</updated>
        <summary type="html"><![CDATA[Pre-training a recognition model with contrastive learning on a large dataset
of unlabeled data has shown great potential to boost the performance of a
downstream task, e.g., image classification. However, in domains such as
medical imaging, collecting unlabeled data can be challenging and expensive. In
this work, we propose to adapt contrastive learning to work with meta-label
annotations, for improving the model's performance in medical image
segmentation even when no additional unlabeled data is available. Meta-labels
such as the location of a 2D slice in a 3D MRI scan or the type of device used,
often come for free during the acquisition process. We use the meta-labels for
pre-training the image encoder as well as to regularize a semi-supervised
training, in which a reduced set of annotated data is used for training.
Finally, to fully exploit the weak annotations, a self-paced learning approach
is used to help the learning and discriminate useful labels from noise. Results
on three different medical image segmentation datasets show that our approach:
i) highly boosts the performance of a model trained on a few scans, ii)
outperforms previous contrastive and semi-supervised approaches, and iii)
reaches close to the performance of a model trained on the full data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jizong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Ping Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desrosiers_C/0/1/0/all/0/1"&gt;Chrisitian Desrosiers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1"&gt;Marco Pedersoli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human Trajectory Prediction via Counterfactual Analysis. (arXiv:2107.14202v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14202</id>
        <link href="http://arxiv.org/abs/2107.14202"/>
        <updated>2021-07-30T02:13:28.151Z</updated>
        <summary type="html"><![CDATA[Forecasting human trajectories in complex dynamic environments plays a
critical role in autonomous vehicles and intelligent robots. Most existing
methods learn to predict future trajectories by behavior clues from history
trajectories and interaction clues from environments. However, the inherent
bias between training and deployment environments is ignored. Hence, we propose
a counterfactual analysis method for human trajectory prediction to investigate
the causality between the predicted trajectories and input clues and alleviate
the negative effects brought by environment bias. We first build a causal graph
for trajectory forecasting with history trajectory, future trajectory, and the
environment interactions. Then, we cut off the inference from environment to
trajectory by constructing the counterfactual intervention on the trajectory
itself. Finally, we compare the factual and counterfactual trajectory clues to
alleviate the effects of environment bias and highlight the trajectory clues.
Our counterfactual analysis is a plug-and-play module that can be applied to
any baseline prediction methods including RNN- and CNN-based ones. We show that
our method achieves consistent improvement for different baselines and obtains
the state-of-the-art results on public pedestrian trajectory forecasting
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guangyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Junlong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiwen Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The interpretation of endobronchial ultrasound image using 3D convolutional neural network for differentiating malignant and benign mediastinal lesions. (arXiv:2107.13820v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.13820</id>
        <link href="http://arxiv.org/abs/2107.13820"/>
        <updated>2021-07-30T02:13:28.133Z</updated>
        <summary type="html"><![CDATA[The purpose of this study is to differentiate malignant and benign
mediastinal lesions by using the three-dimensional convolutional neural network
through the endobronchial ultrasound (EBUS) image. Compared with previous
study, our proposed model is robust to noise and able to fuse various imaging
features and spatiotemporal features of EBUS videos. Endobronchial
ultrasound-guided transbronchial needle aspiration (EBUS-TBNA) is a diagnostic
tool for intrathoracic lymph nodes. Physician can observe the characteristics
of the lesion using grayscale mode, doppler mode, and elastography during the
procedure. To process the EBUS data in the form of a video and appropriately
integrate the features of multiple imaging modes, we used a time-series
three-dimensional convolutional neural network (3D CNN) to learn the
spatiotemporal features and design a variety of architectures to fuse each
imaging mode. Our model (Res3D_UDE) took grayscale mode, Doppler mode, and
elastography as training data and achieved an accuracy of 82.00% and area under
the curve (AUC) of 0.83 on the validation set. Compared with previous study, we
directly used videos recorded during procedure as training and validation data,
without additional manual selection, which might be easier for clinical
application. In addition, model designed with 3D CNN can also effectively learn
spatiotemporal features and improve accuracy. In the future, our model may be
used to guide physicians to quickly and correctly find the target lesions for
slice sampling during the inspection process, reduce the number of slices of
benign lesions, and shorten the inspection time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ching/0/1/0/all/0/1"&gt;Ching&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lin_K/0/1/0/all/0/1"&gt;Kai Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shao/0/1/0/all/0/1"&gt;Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chang_J/0/1/0/all/0/1"&gt;Jerry Chang&lt;/a&gt;, Yun, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_C/0/1/0/all/0/1"&gt;Chien Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improvement of image classification by multiple optical scattering. (arXiv:2107.14051v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.14051</id>
        <link href="http://arxiv.org/abs/2107.14051"/>
        <updated>2021-07-30T02:13:27.993Z</updated>
        <summary type="html"><![CDATA[Multiple optical scattering occurs when light propagates in a non-uniform
medium. During the multiple scattering, images were distorted and the spatial
information they carried became scrambled. However, the image information is
not lost but presents in the form of speckle patterns (SPs). In this study, we
built up an optical random scattering system based on an LCD and an RGB laser
source. We found that the image classification can be improved by the help of
random scattering which is considered as a feedforward neural network to
extracts features from image. Along with the ridge classification deployed on
computer, we achieved excellent classification accuracy higher than 94%, for a
variety of data sets covering medical, agricultural, environmental protection
and other fields. In addition, the proposed optical scattering system has the
advantages of high speed, low power consumption, and miniaturization, which is
suitable for deploying in edge computing applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xinyu Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1"&gt;Yanqing Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_B/0/1/0/all/0/1"&gt;Bangning Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Miaogen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1"&gt;Yanlong Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1"&gt;Chunliu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1"&gt;Juan Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Changyu Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometry Uncertainty Projection Network for Monocular 3D Object Detection. (arXiv:2107.13774v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13774</id>
        <link href="http://arxiv.org/abs/2107.13774"/>
        <updated>2021-07-30T02:13:27.987Z</updated>
        <summary type="html"><![CDATA[Geometry Projection is a powerful depth estimation method in monocular 3D
object detection. It estimates depth dependent on heights, which introduces
mathematical priors into the deep model. But projection process also introduces
the error amplification problem, in which the error of the estimated height
will be amplified and reflected greatly at the output depth. This property
leads to uncontrollable depth inferences and also damages the training
efficiency. In this paper, we propose a Geometry Uncertainty Projection Network
(GUP Net) to tackle the error amplification problem at both inference and
training stages. Specifically, a GUP module is proposed to obtains the
geometry-guided uncertainty of the inferred depth, which not only provides high
reliable confidence for each depth but also benefits depth learning.
Furthermore, at the training stage, we propose a Hierarchical Task Learning
strategy to reduce the instability caused by error amplification. This learning
algorithm monitors the learning situation of each task by a proposed indicator
and adaptively assigns the proper loss weights for different tasks according to
their pre-tasks situation. Based on that, each task starts learning only when
its pre-tasks are learned well, which can significantly improve the stability
and efficiency of the training process. Extensive experiments demonstrate the
effectiveness of the proposed method. The overall model can infer more reliable
object depth than existing methods and outperforms the state-of-the-art
image-based monocular 3D detectors by 3.74% and 4.7% AP40 of the car and
pedestrian categories on the KITTI benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xinzhu Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianzhu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yating Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1"&gt;Qi Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junjie Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1"&gt;Wanli Ouyang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Egyptian Sign Language Recognition Using CNN and LSTM. (arXiv:2107.13647v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13647</id>
        <link href="http://arxiv.org/abs/2107.13647"/>
        <updated>2021-07-30T02:13:27.981Z</updated>
        <summary type="html"><![CDATA[Sign language is a set of gestures that deaf people use to communicate.
Unfortunately, normal people don't understand it, which creates a communication
gap that needs to be filled. Because of the variations in (Egyptian Sign
Language) ESL from one region to another, ESL provides a challenging research
problem. In this work, we are providing applied research with its video-based
Egyptian sign language recognition system that serves the local community of
deaf people in Egypt, with a moderate and reasonable accuracy. We present a
computer vision system with two different neural networks architectures. The
first is a Convolutional Neural Network (CNN) for extracting spatial features.
The CNN model was retrained on the inception mod. The second architecture is a
CNN followed by a Long Short-Term Memory (LSTM) for extracting both spatial and
temporal features. The two models achieved an accuracy of 90% and 72%,
respectively. We examined the power of these two architectures to distinguish
between 9 common words (with similar signs) among some deaf people community in
Egypt.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elhagry_A/0/1/0/all/0/1"&gt;Ahmed Elhagry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gla_R/0/1/0/all/0/1"&gt;Rawan Gla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Similarity Measure of Histopathology Images by Deep Embeddings. (arXiv:2107.13703v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.13703</id>
        <link href="http://arxiv.org/abs/2107.13703"/>
        <updated>2021-07-30T02:13:27.974Z</updated>
        <summary type="html"><![CDATA[Histopathology digital scans are large-size images that contain valuable
information at the pixel level. Content-based comparison of these images is a
challenging task. This study proposes a content-based similarity measure for
high-resolution gigapixel histopathology images. The proposed similarity
measure is an expansion of cosine vector similarity to a matrix. Each image is
divided into same-size patches with a meaningful amount of information (i.e.,
contained enough tissue). The similarity is measured by the extraction of
patch-level deep embeddings of the last pooling layer of a pre-trained deep
model at four different magnification levels, namely, 1x, 2.5x, 5x, and 10x
magnifications. In addition, for faster measurement, embedding reduction is
investigated. Finally, to assess the proposed method, an image search method is
implemented. Results show that the similarity measure represents the slide
labels with a maximum accuracy of 93.18\% for top-5 search at 5x magnification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Afshari_M/0/1/0/all/0/1"&gt;Mehdi Afshari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tizhoosh_H/0/1/0/all/0/1"&gt;H.R. Tizhoosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Robustness and Accuracy via Relative Information Encoding in 3D Human Pose Estimation. (arXiv:2107.13994v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13994</id>
        <link href="http://arxiv.org/abs/2107.13994"/>
        <updated>2021-07-30T02:13:27.957Z</updated>
        <summary type="html"><![CDATA[Most of the existing 3D human pose estimation approaches mainly focus on
predicting 3D positional relationships between the root joint and other human
joints (local motion) instead of the overall trajectory of the human body
(global motion). Despite the great progress achieved by these approaches, they
are not robust to global motion, and lack the ability to accurately predict
local motion with a small movement range. To alleviate these two problems, we
propose a relative information encoding method that yields positional and
temporal enhanced representations. Firstly, we encode positional information by
utilizing relative coordinates of 2D poses to enhance the consistency between
the input and output distribution. The same posture with different absolute 2D
positions can be mapped to a common representation. It is beneficial to resist
the interference of global motion on the prediction results. Second, we encode
temporal information by establishing the connection between the current pose
and other poses of the same person within a period of time. More attention will
be paid to the movement changes before and after the current pose, resulting in
better prediction performance on local motion with a small movement range. The
ablation studies validate the effectiveness of the proposed relative
information encoding method. Besides, we introduce a multi-stage optimization
method to the whole framework to further exploit the positional and temporal
enhanced representations. Our method outperforms state-of-the-art methods on
two public datasets. Code is available at
https://github.com/paTRICK-swk/Pose3D-RIE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shan_W/0/1/0/all/0/1"&gt;Wenkang Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Haopeng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shanshe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xinfeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1"&gt;Wen Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalizing Gaze Estimation with Outlier-guided Collaborative Adaptation. (arXiv:2107.13780v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13780</id>
        <link href="http://arxiv.org/abs/2107.13780"/>
        <updated>2021-07-30T02:13:27.942Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have significantly improved appearance-based gaze
estimation accuracy. However, it still suffers from unsatisfactory performance
when generalizing the trained model to new domains, e.g., unseen environments
or persons. In this paper, we propose a plug-and-play gaze adaptation framework
(PnP-GA), which is an ensemble of networks that learn collaboratively with the
guidance of outliers. Since our proposed framework does not require
ground-truth labels in the target domain, the existing gaze estimation networks
can be directly plugged into PnP-GA and generalize the algorithms to new
domains. We test PnP-GA on four gaze domain adaptation tasks, ETH-to-MPII,
ETH-to-EyeDiap, Gaze360-to-MPII, and Gaze360-to-EyeDiap. The experimental
results demonstrate that the PnP-GA framework achieves considerable performance
improvements of 36.9%, 31.6%, 19.4%, and 11.8% over the baseline system. The
proposed framework also outperforms the state-of-the-art domain adaptation
approaches on gaze domain adaptation tasks. Code has been released at
https://github.com/DreamtaleCore/PnP-GA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yunfei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Ruicong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haofei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1"&gt;Feng Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Self-supervised Augmented Knowledge Distillation. (arXiv:2107.13715v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13715</id>
        <link href="http://arxiv.org/abs/2107.13715"/>
        <updated>2021-07-30T02:13:27.915Z</updated>
        <summary type="html"><![CDATA[Knowledge distillation often involves how to define and transfer knowledge
from teacher to student effectively. Although recent self-supervised
contrastive knowledge achieves the best performance, forcing the network to
learn such knowledge may damage the representation learning of the original
class recognition task. We therefore adopt an alternative self-supervised
augmented task to guide the network to learn the joint distribution of the
original recognition task and self-supervised auxiliary task. It is
demonstrated as a richer knowledge to improve the representation power without
losing the normal classification capability. Moreover, it is incomplete that
previous methods only transfer the probabilistic knowledge between the final
layers. We propose to append several auxiliary classifiers to hierarchical
intermediate feature maps to generate diverse self-supervised knowledge and
perform the one-to-one transfer to teach the student network thoroughly. Our
method significantly surpasses the previous SOTA SSKD with an average
improvement of 2.56\% on CIFAR-100 and an improvement of 0.77\% on ImageNet
across widely used network pairs. Codes are available at
https://github.com/winycg/HSAKD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chuanguang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_Z/0/1/0/all/0/1"&gt;Zhulin An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1"&gt;Linhang Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yongjun Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Similarity and symmetry measures based on fuzzy descriptors of image objects` composition. (arXiv:2107.13651v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13651</id>
        <link href="http://arxiv.org/abs/2107.13651"/>
        <updated>2021-07-30T02:13:27.909Z</updated>
        <summary type="html"><![CDATA[The paper describes a method for measuring the similarity and symmetry of an
image annotated with bounding boxes indicating image objects. The latter
representation became popular recently due to the rapid development of fast and
efficient deep-learning-based object-detection methods. The proposed approach
allows for comparing sets of bounding boxes to estimate the degree of
similarity of their underlying images. It is based on the fuzzy approach that
uses the fuzzy mutual position (FMP) matrix to describe spatial composition and
relations between bounding boxes within an image. A method of computing the
similarity of two images described by their FMP matrices is proposed and the
algorithm of its computation. It outputs the single scalar value describing the
degree of content-based image similarity. By modifying the method`s parameters,
instead of similarity, the reflectional symmetry of object composition may also
be measured. The proposed approach allows for measuring differences in objects`
composition of various intensities. It is also invariant to translation and
scaling and - in case of symmetry detection - position and orientation of the
symmetry axis. A couple of examples illustrate the method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Iwanowski_M/0/1/0/all/0/1"&gt;Marcin Iwanowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grzabka_M/0/1/0/all/0/1"&gt;Marcin Grzabka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Addressing Barriers to Reproducible Named Entity Recognition Evaluation. (arXiv:2107.14154v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.14154</id>
        <link href="http://arxiv.org/abs/2107.14154"/>
        <updated>2021-07-30T02:13:27.900Z</updated>
        <summary type="html"><![CDATA[To address what we believe is a looming crisis of unreproducible evaluation
for named entity recognition tasks, we present guidelines for reproducible
evaluation. The guidelines we propose are extremely simple, focusing on
transparency regarding how chunks are encoded and scored, but very few papers
currently being published fully comply with them. We demonstrate that despite
the apparent simplicity of NER evaluation, unreported differences in the
scoring procedure can result in changes to scores that are both of noticeable
magnitude and are statistically significant. We provide SeqScore, an open
source toolkit that addresses many of the issues that cause replication
failures and makes following our guidelines easy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Palen_Michel_C/0/1/0/all/0/1"&gt;Chester Palen-Michel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holley_N/0/1/0/all/0/1"&gt;Nolan Holley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lignos_C/0/1/0/all/0/1"&gt;Constantine Lignos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UIBert: Learning Generic Multimodal Representations for UI Understanding. (arXiv:2107.13731v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13731</id>
        <link href="http://arxiv.org/abs/2107.13731"/>
        <updated>2021-07-30T02:13:27.894Z</updated>
        <summary type="html"><![CDATA[To improve the accessibility of smart devices and to simplify their usage,
building models which understand user interfaces (UIs) and assist users to
complete their tasks is critical. However, unique challenges are proposed by
UI-specific characteristics, such as how to effectively leverage multimodal UI
features that involve image, text, and structural metadata and how to achieve
good performance when high-quality labeled data is unavailable. To address such
challenges we introduce UIBert, a transformer-based joint image-text model
trained through novel pre-training tasks on large-scale unlabeled UI data to
learn generic feature representations for a UI and its components. Our key
intuition is that the heterogeneous features in a UI are self-aligned, i.e.,
the image and text features of UI components, are predictive of each other. We
propose five pretraining tasks utilizing this self-alignment among different
features of a UI component and across various components in the same UI. We
evaluate our method on nine real-world downstream UI tasks where UIBert
outperforms strong multimodal baselines by up to 9.26% accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bai_C/0/1/0/all/0/1"&gt;Chongyang Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zang_X/0/1/0/all/0/1"&gt;Xiaoxue Zang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Ying Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sunkara_S/0/1/0/all/0/1"&gt;Srinivas Sunkara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1"&gt;Abhinav Rastogi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jindong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arcas_B/0/1/0/all/0/1"&gt;Blaise Aguera y Arcas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Profile to Frontal Face Recognition in the Wild Using Coupled Conditional GAN. (arXiv:2107.13742v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13742</id>
        <link href="http://arxiv.org/abs/2107.13742"/>
        <updated>2021-07-30T02:13:27.888Z</updated>
        <summary type="html"><![CDATA[In recent years, with the advent of deep-learning, face recognition has
achieved exceptional success. However, many of these deep face recognition
models perform much better in handling frontal faces compared to profile faces.
The major reason for poor performance in handling of profile faces is that it
is inherently difficult to learn pose-invariant deep representations that are
useful for profile face recognition. In this paper, we hypothesize that the
profile face domain possesses a latent connection with the frontal face domain
in a latent feature subspace. We look to exploit this latent connection by
projecting the profile faces and frontal faces into a common latent subspace
and perform verification or retrieval in the latent domain. We leverage a
coupled conditional generative adversarial network (cpGAN) structure to find
the hidden relationship between the profile and frontal images in a latent
common embedding subspace. Specifically, the cpGAN framework consists of two
conditional GAN-based sub-networks, one dedicated to the frontal domain and the
other dedicated to the profile domain. Each sub-network tends to find a
projection that maximizes the pair-wise correlation between the two feature
domains in a common embedding feature subspace. The efficacy of our approach
compared with the state-of-the-art is demonstrated using the CFP, CMU
Multi-PIE, IJB-A, and IJB-C datasets. Additionally, we have also implemented a
coupled convolutional neural network (cpCNN) and an adversarial discriminative
domain adaptation network (ADDA) for profile to frontal face recognition. We
have evaluated the performance of cpCNN and ADDA and compared it with the
proposed cpGAN. Finally, we have also evaluated our cpGAN for reconstruction of
frontal faces from input profile faces contained in the VGGFace2 dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Taherkhani_F/0/1/0/all/0/1"&gt;Fariborz Taherkhani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talreja_V/0/1/0/all/0/1"&gt;Veeru Talreja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1"&gt;Jeremy Dawson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valenti_M/0/1/0/all/0/1"&gt;Matthew C. Valenti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1"&gt;Nasser M. Nasrabadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Corridor for new mobility Aachen-D\"usseldorf: Methods and concepts of the research project ACCorD. (arXiv:2107.14048v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2107.14048</id>
        <link href="http://arxiv.org/abs/2107.14048"/>
        <updated>2021-07-30T02:13:27.880Z</updated>
        <summary type="html"><![CDATA[With the Corridor for New Mobility Aachen - D\"usseldorf, an integrated
development environment is created, incorporating existing test capabilities,
to systematically test and validate automated vehicles in interaction with
connected Intelligent Transport Systems Stations (ITS-Ss). This is achieved
through a time- and cost-efficient toolchain and methodology, in which
simulation, closed test sites as well as test fields in public transport are
linked in the best possible way. By implementing a digital twin, the recorded
traffic events can be visualized in real-time and driving functions can be
tested in the simulation based on real data. In order to represent diverse
traffic scenarios, the corridor contains a highway section, a rural area, and
urban areas. First, this paper outlines the project goals before describing the
individual project contents in more detail. These include the concepts of
traffic detection, driving function development, digital twin development, and
public involvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kloeker_L/0/1/0/all/0/1"&gt;Laurent Kloeker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kloeker_A/0/1/0/all/0/1"&gt;Amarin Kloeker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thomsen_F/0/1/0/all/0/1"&gt;Fabian Thomsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erraji_A/0/1/0/all/0/1"&gt;Armin Erraji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eckstein_L/0/1/0/all/0/1"&gt;Lutz Eckstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lamberty_S/0/1/0/all/0/1"&gt;Serge Lamberty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fazekas_A/0/1/0/all/0/1"&gt;Adrian Fazekas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kallo_E/0/1/0/all/0/1"&gt;Eszter Kall&amp;#xf3;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oeser_M/0/1/0/all/0/1"&gt;Markus Oeser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flechon_C/0/1/0/all/0/1"&gt;Charlotte Fl&amp;#xe9;chon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lohmiller_J/0/1/0/all/0/1"&gt;Jochen Lohmiller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfeiffer_P/0/1/0/all/0/1"&gt;Pascal Pfeiffer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sommer_M/0/1/0/all/0/1"&gt;Martin Sommer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winter_H/0/1/0/all/0/1"&gt;Helen Winter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Easy and Efficient Transformer : Scalable Inference Solution For large NLP model. (arXiv:2104.12470v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12470</id>
        <link href="http://arxiv.org/abs/2104.12470"/>
        <updated>2021-07-30T02:13:27.864Z</updated>
        <summary type="html"><![CDATA[Recently, large-scale transformer-based models have been proven to be
effective over a variety of tasks across many domains. Nevertheless, putting
them into production is very expensive, requiring comprehensive optimization
techniques to reduce inference costs. This paper introduces a series of
transformer inference optimization techniques that are both in algorithm level
and hardware level. These techniques include a pre-padding decoding mechanism
that improves token parallelism for text generation, and highly optimized
kernels designed for very long input length and large hidden size. On this
basis, we propose a transformer inference acceleration library -- Easy and
Efficient Transformer (EET), which has a significant performance improvement
over existing libraries. Compared to Faster Transformer v4.0's implementation
for GPT-2 layer on A100, EET achieves a 1.5-4.5x state-of-art speedup varying
with different context lengths. EET is available at
https://github.com/NetEase-FuXi/EET. A demo video is available at
https://youtu.be/22UPcNGcErg.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Gongzheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1"&gt;Yadong Xi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1"&gt;Jingzhen Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Duan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bai Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1"&gt;Changjie Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1"&gt;Xiaoxi Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zeng Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Human Pose Estimation by Maximizing Fusion and High-Level Spatial Attention. (arXiv:2107.13693v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13693</id>
        <link href="http://arxiv.org/abs/2107.13693"/>
        <updated>2021-07-30T02:13:27.858Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an efficient human pose estimation network -- SFM
(slender fusion model) by fusing multi-level features and adding lightweight
attention blocks -- HSA (High-Level Spatial Attention). Many existing methods
on efficient network have already taken feature fusion into consideration,
which largely boosts the performance. However, its performance is far inferior
to large network such as ResNet and HRNet due to its limited fusion operation
in the network. Specifically, we expand the number of fusion operation by
building bridges between two pyramid frameworks without adding layers.
Meanwhile, to capture long-range dependency, we propose a lightweight attention
block -- HSA, which computes second-order attention map. In summary, SFM
maximizes the number of feature fusion in a limited number of layers. HSA
learns high precise spatial information by computing the attention of spatial
attention map. With the help of SFM and HSA, our network is able to generate
multi-level feature and extract precise global spatial information with little
computing resource. Thus, our method achieve comparable or even better accuracy
with less parameters and computational cost. Our SFM achieve 89.0 in PCKh@0.5,
42.0 in PCKh@0.1 on MPII validation set and 71.7 in AP, 90.7 in AP@0.5 on COCO
validation with only 1.7G FLOPs and 1.5M parameters. The source code will be
public soon.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1"&gt;Zhiyuan Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yaohai Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yizhe Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1"&gt;Ruisong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yayu Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Break, Perturb, Build: Automatic Perturbation of Reasoning Paths through Question Decomposition. (arXiv:2107.13935v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.13935</id>
        <link href="http://arxiv.org/abs/2107.13935"/>
        <updated>2021-07-30T02:13:27.852Z</updated>
        <summary type="html"><![CDATA[Recent efforts to create challenge benchmarks that test the abilities of
natural language understanding models have largely depended on human
annotations. In this work, we introduce the "Break, Perturb, Build" (BPB)
framework for automatic reasoning-oriented perturbation of question-answer
pairs. BPB represents a question by decomposing it into the reasoning steps
that are required to answer it, symbolically perturbs the decomposition, and
then generates new question-answer pairs. We demonstrate the effectiveness of
BPB by creating evaluation sets for three reading comprehension (RC)
benchmarks, generating thousands of high-quality examples without human
intervention. We evaluate a range of RC models on our evaluation sets, which
reveals large performance gaps on generated examples compared to the original
data. Moreover, symbolic perturbations enable fine-grained analysis of the
strengths and limitations of models. Last, augmenting the training data with
examples generated by BPB helps close performance gaps, without any drop on the
original data distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1"&gt;Mor Geva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolfson_T/0/1/0/all/0/1"&gt;Tomer Wolfson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1"&gt;Jonathan Berant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Viewpoint-Invariant Exercise Repetition Counting. (arXiv:2107.13760v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13760</id>
        <link href="http://arxiv.org/abs/2107.13760"/>
        <updated>2021-07-30T02:13:27.846Z</updated>
        <summary type="html"><![CDATA[Counting the repetition of human exercise and physical rehabilitation is a
common task in rehabilitation and exercise training. The existing vision-based
repetition counting methods less emphasize the concurrent motions in the same
video. This work presents a vision-based human motion repetition counting
applicable to counting concurrent motions through the skeleton location
extracted from various pose estimation methods. The presented method was
validated on the University of Idaho Physical Rehabilitation Movements Data Set
(UI-PRMD), and MM-fit dataset. The overall mean absolute error (MAE) for mm-fit
was 0.06 with off-by-one Accuracy (OBOA) 0.94. Overall MAE for UI-PRMD dataset
was 0.06 with OBOA 0.95. We have also tested the performance in a variety of
camera locations and concurrent motions with conveniently collected video with
overall MAE 0.06 and OBOA 0.88. The proposed method provides a view-angle and
motion agnostic concurrent motion counting. This method can potentially use in
large-scale remote rehabilitation and exercise training with only one camera.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1"&gt;Yu Cheng Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qingpeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsougenis_E/0/1/0/all/0/1"&gt;Efstratios Tsougenis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsui_K/0/1/0/all/0/1"&gt;Kwok-Leung Tsui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lighter Stacked Hourglass Human Pose Estimation. (arXiv:2107.13643v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13643</id>
        <link href="http://arxiv.org/abs/2107.13643"/>
        <updated>2021-07-30T02:13:27.840Z</updated>
        <summary type="html"><![CDATA[Human pose estimation (HPE) is one of the most challenging tasks in computer
vision as humans are deformable by nature and thus their pose has so much
variance. HPE aims to correctly identify the main joint locations of a single
person or multiple people in a given image or video. Locating joints of a
person in images or videos is an important task that can be applied in action
recognition and object tracking. As have many computer vision tasks, HPE has
advanced massively with the introduction of deep learning to the field. In this
paper, we focus on one of the deep learning-based approaches of HPE proposed by
Newell et al., which they named the stacked hourglass network. Their approach
is widely used in many applications and is regarded as one of the best works in
this area. The main focus of their approach is to capture as much information
as it can at all possible scales so that a coherent understanding of the local
features and full-body location is achieved. Their findings demonstrate that
important cues such as orientation of a person, arrangement of limbs, and
adjacent joints' relative location can be identified from multiple scales at
different resolutions. To do so, they makes use of a single pipeline to process
images in multiple resolutions, which comprises a skip layer to not lose
spatial information at each resolution. The resolution of the images stretches
as lower as 4x4 to make sure that a smaller spatial feature is included. In
this study, we study the effect of architectural modifications on the
computational speed and accuracy of the network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elhagry_A/0/1/0/all/0/1"&gt;Ahmed Elhagry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saeed_M/0/1/0/all/0/1"&gt;Mohamed Saeed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Araia_M/0/1/0/all/0/1"&gt;Musie Araia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Perturbed Length-aware Positional Encoding for Non-autoregressive Neural Machine Translation. (arXiv:2107.13689v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.13689</id>
        <link href="http://arxiv.org/abs/2107.13689"/>
        <updated>2021-07-30T02:13:27.825Z</updated>
        <summary type="html"><![CDATA[Non-autoregressive neural machine translation (NAT) usually employs
sequence-level knowledge distillation using autoregressive neural machine
translation (AT) as its teacher model. However, a NAT model often outputs
shorter sentences than an AT model. In this work, we propose sequence-level
knowledge distillation (SKD) using perturbed length-aware positional encoding
and apply it to a student model, the Levenshtein Transformer. Our method
outperformed a standard Levenshtein Transformer by 2.5 points in bilingual
evaluation understudy (BLEU) at maximum in a WMT14 German to English
translation. The NAT model output longer sentences than the baseline NAT
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oka_Y/0/1/0/all/0/1"&gt;Yui Oka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sudoh_K/0/1/0/all/0/1"&gt;Katsuhito Sudoh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakamura_S/0/1/0/all/0/1"&gt;Satoshi Nakamura&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recognizing Emotion Cause in Conversations. (arXiv:2012.11820v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11820</id>
        <link href="http://arxiv.org/abs/2012.11820"/>
        <updated>2021-07-30T02:13:27.819Z</updated>
        <summary type="html"><![CDATA[We address the problem of recognizing emotion cause in conversations, define
two novel sub-tasks of this problem, and provide a corresponding dialogue-level
dataset, along with strong Transformer-based baselines. The dataset is
available at https://github.com/declare-lab/RECCON.

Introduction: Recognizing the cause behind emotions in text is a fundamental
yet under-explored area of research in NLP. Advances in this area hold the
potential to improve interpretability and performance in affect-based models.
Identifying emotion causes at the utterance level in conversations is
particularly challenging due to the intermingling dynamics among the
interlocutors.

Method: We introduce the task of Recognizing Emotion Cause in CONversations
with an accompanying dataset named RECCON, containing over 1,000 dialogues and
10,000 utterance cause-effect pairs. Furthermore, we define different cause
types based on the source of the causes, and establish strong Transformer-based
baselines to address two different sub-tasks on this dataset: causal span
extraction and causal emotion entailment.

Result: Our Transformer-based baselines, which leverage contextual
pre-trained embeddings, such as RoBERTa, outperform the state-of-the-art
emotion cause extraction approaches

Conclusion: We introduce a new task highly relevant for (explainable)
emotion-aware artificial intelligence: recognizing emotion cause in
conversations, provide a new highly challenging publicly available
dialogue-level dataset for this task, and give strong baseline results on this
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1"&gt;Soujanya Poria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1"&gt;Navonil Majumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hazarika_D/0/1/0/all/0/1"&gt;Devamanyu Hazarika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1"&gt;Deepanway Ghosal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhardwaj_R/0/1/0/all/0/1"&gt;Rishabh Bhardwaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jian_S/0/1/0/all/0/1"&gt;Samson Yu Bai Jian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1"&gt;Pengfei Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1"&gt;Romila Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1"&gt;Abhinaba Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chhaya_N/0/1/0/all/0/1"&gt;Niyati Chhaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1"&gt;Alexander Gelbukh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1"&gt;Rada Mihalcea&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Transformer based Dual Discriminator Generative Adversarial Networks for Video Anomaly Detection. (arXiv:2107.13720v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13720</id>
        <link href="http://arxiv.org/abs/2107.13720"/>
        <updated>2021-07-30T02:13:27.812Z</updated>
        <summary type="html"><![CDATA[Detecting abnormal activities in real-world surveillance videos is an
important yet challenging task as the prior knowledge about video anomalies is
usually limited or unavailable. Despite that many approaches have been
developed to resolve this problem, few of them can capture the normal
spatio-temporal patterns effectively and efficiently. Moreover, existing works
seldom explicitly consider the local consistency at frame level and global
coherence of temporal dynamics in video sequences. To this end, we propose
Convolutional Transformer based Dual Discriminator Generative Adversarial
Networks (CT-D2GAN) to perform unsupervised video anomaly detection.
Specifically, we first present a convolutional transformer to perform future
frame prediction. It contains three key components, i.e., a convolutional
encoder to capture the spatial information of the input video clips, a temporal
self-attention module to encode the temporal dynamics, and a convolutional
decoder to integrate spatio-temporal features and predict the future frame.
Next, a dual discriminator based adversarial training procedure, which jointly
considers an image discriminator that can maintain the local consistency at
frame-level and a video discriminator that can enforce the global coherence of
temporal dynamics, is employed to enhance the future frame prediction. Finally,
the prediction error is used to identify abnormal video frames. Thoroughly
empirical studies on three public video anomaly detection datasets, i.e., UCSD
Ped2, CUHK Avenue, and Shanghai Tech Campus, demonstrate the effectiveness of
the proposed adversarial spatio-temporal modeling framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1"&gt;Xinyang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1"&gt;Dongjin Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuncong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhengzhang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1"&gt;Jingchao Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast and Scalable Image Search For Histology. (arXiv:2107.13587v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13587</id>
        <link href="http://arxiv.org/abs/2107.13587"/>
        <updated>2021-07-30T02:13:27.802Z</updated>
        <summary type="html"><![CDATA[The expanding adoption of digital pathology has enabled the curation of large
repositories of histology whole slide images (WSIs), which contain a wealth of
information. Similar pathology image search offers the opportunity to comb
through large historical repositories of gigapixel WSIs to identify cases with
similar morphological features and can be particularly useful for diagnosing
rare diseases, identifying similar cases for predicting prognosis, treatment
outcomes, and potential clinical trial success. A critical challenge in
developing a WSI search and retrieval system is scalability, which is uniquely
challenging given the need to search a growing number of slides that each can
consist of billions of pixels and are several gigabytes in size. Such systems
are typically slow and retrieval speed often scales with the size of the
repository they search through, making their clinical adoption tedious and are
not feasible for repositories that are constantly growing. Here we present Fast
Image Search for Histopathology (FISH), a histology image search pipeline that
is infinitely scalable and achieves constant search speed that is independent
of the image database size while being interpretable and without requiring
detailed annotations. FISH uses self-supervised deep learning to encode
meaningful representations from WSIs and a Van Emde Boas tree for fast search,
followed by an uncertainty-based ranking algorithm to retrieve similar WSIs. We
evaluated FISH on multiple tasks and datasets with over 22,000 patient cases
spanning 56 disease subtypes. We additionally demonstrate that FISH can be used
to assist with the diagnosis of rare cancer types where sufficient cases may
not be available to train traditional supervised deep models. FISH is available
as an easy-to-use, open-source software package
(https://github.com/mahmoodlab/FISH).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chengkuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1"&gt;Ming Y. Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williamson_D/0/1/0/all/0/1"&gt;Drew F. K. Williamson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tiffany Y. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schaumberg_A/0/1/0/all/0/1"&gt;Andrew J. Schaumberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_F/0/1/0/all/0/1"&gt;Faisal Mahmood&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sign and Search: Sign Search Functionality for Sign Language Lexica. (arXiv:2107.13637v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13637</id>
        <link href="http://arxiv.org/abs/2107.13637"/>
        <updated>2021-07-30T02:13:27.788Z</updated>
        <summary type="html"><![CDATA[Sign language lexica are a useful resource for researchers and people
learning sign languages. Current implementations allow a user to search a sign
either by its gloss or by selecting its primary features such as handshape and
location. This study focuses on exploring a reverse search functionality where
a user can sign a query sign in front of a webcam and retrieve a set of
matching signs. By extracting different body joints combinations (upper body,
dominant hand's arm and wrist) using the pose estimation framework OpenPose, we
compare four techniques (PCA, UMAP, DTW and Euclidean distance) as distance
metrics between 20 query signs, each performed by eight participants on a 1200
sign lexicon. The results show that UMAP and DTW can predict a matching sign
with an 80\% and 71\% accuracy respectively at the top-20 retrieved signs using
the movement of the dominant hand arm. Using DTW and adding more sign instances
from other participants in the lexicon, the accuracy can be raised to 90\% at
the top-10 ranking. Our results suggest that our methodology can be used with
no training in any sign language lexicon regardless of its size.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fragkiadakis_M/0/1/0/all/0/1"&gt;Manolis Fragkiadakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Putten_P/0/1/0/all/0/1"&gt;Peter van der Putten&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Applying Occam's Razor to Transformer-Based Dependency Parsing: What Works, What Doesn't, and What is Really Necessary. (arXiv:2010.12699v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12699</id>
        <link href="http://arxiv.org/abs/2010.12699"/>
        <updated>2021-07-30T02:13:27.771Z</updated>
        <summary type="html"><![CDATA[The introduction of pre-trained transformer-based contextualized word
embeddings has led to considerable improvements in the accuracy of graph-based
parsers for frameworks such as Universal Dependencies (UD). However, previous
works differ in various dimensions, including their choice of pre-trained
language models and whether they use LSTM layers. With the aims of
disentangling the effects of these choices and identifying a simple yet widely
applicable architecture, we introduce STEPS, a new modular graph-based
dependency parser. Using STEPS, we perform a series of analyses on the UD
corpora of a diverse set of languages. We find that the choice of pre-trained
embeddings has by far the greatest impact on parser performance and identify
XLM-R as a robust choice across the languages in our study. Adding LSTM layers
provides no benefits when using transformer-based embeddings. A multi-task
training setup outputting additional UD features may contort results. Taking
these insights together, we propose a simple but widely applicable parser
architecture and configuration, achieving new state-of-the-art results (in
terms of LAS) for 10 out of 12 diverse languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grunewald_S/0/1/0/all/0/1"&gt;Stefan Gr&amp;#xfc;newald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Friedrich_A/0/1/0/all/0/1"&gt;Annemarie Friedrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuhn_J/0/1/0/all/0/1"&gt;Jonas Kuhn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging Gap between Image Pixels and Semantics via Supervision: A Survey. (arXiv:2107.13757v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13757</id>
        <link href="http://arxiv.org/abs/2107.13757"/>
        <updated>2021-07-30T02:13:27.763Z</updated>
        <summary type="html"><![CDATA[The fact that there exists a gap between low-level features and semantic
meanings of images, called the semantic gap, is known for decades. Resolution
of the semantic gap is a long standing problem. The semantic gap problem is
reviewed and a survey on recent efforts in bridging the gap is made in this
work. Most importantly, we claim that the semantic gap is primarily bridged
through supervised learning today. Experiences are drawn from two application
domains to illustrate this point: 1) object detection and 2) metric learning
for content-based image retrieval (CBIR). To begin with, this paper offers a
historical retrospective on supervision, makes a gradual transition to the
modern data-driven methodology and introduces commonly used datasets. Then, it
summarizes various supervision methods to bridge the semantic gap in the
context of object detection and metric learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1"&gt;Jiali Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1"&gt;C.-C. Jay Kuo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rare Disease Identification from Clinical Notes with Ontologies and Weak Supervision. (arXiv:2105.01995v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01995</id>
        <link href="http://arxiv.org/abs/2105.01995"/>
        <updated>2021-07-30T02:13:27.757Z</updated>
        <summary type="html"><![CDATA[The identification of rare diseases from clinical notes with Natural Language
Processing (NLP) is challenging due to the few cases available for machine
learning and the need of data annotation from clinical experts. We propose a
method using ontologies and weak supervision. The approach includes two steps:
(i) Text-to-UMLS, linking text mentions to concepts in Unified Medical Language
System (UMLS), with a named entity linking tool (e.g. SemEHR) and weak
supervision based on customised rules and Bidirectional Encoder Representations
from Transformers (BERT) based contextual representations, and (ii)
UMLS-to-ORDO, matching UMLS concepts to rare diseases in Orphanet Rare Disease
Ontology (ORDO). Using MIMIC-III US intensive care discharge summaries as a
case study, we show that the Text-to-UMLS process can be greatly improved with
weak supervision, without any annotated data from domain experts. Our analysis
shows that the overall pipeline processing discharge summaries can surface rare
disease cases, which are mostly uncaptured in manual ICD codes of the hospital
admissions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Hang Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suarez_Paniagua_V/0/1/0/all/0/1"&gt;V&amp;#xed;ctor Su&amp;#xe1;rez-Paniagua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huayu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Minhong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whitfield_E/0/1/0/all/0/1"&gt;Emma Whitfield&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Honghan Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning for Fine-Grained Image Classification. (arXiv:2107.13973v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13973</id>
        <link href="http://arxiv.org/abs/2107.13973"/>
        <updated>2021-07-30T02:13:27.750Z</updated>
        <summary type="html"><![CDATA[Fine-grained image classification involves identifying different
subcategories of a class which possess very subtle discriminatory features.
Fine-grained datasets usually provide bounding box annotations along with class
labels to aid the process of classification. However, building large scale
datasets with such annotations is a mammoth task. Moreover, this extensive
annotation is time-consuming and often requires expertise, which is a huge
bottleneck in building large datasets. On the other hand, self-supervised
learning (SSL) exploits the freely available data to generate supervisory
signals which act as labels. The features learnt by performing some pretext
tasks on huge unlabelled data proves to be very helpful for multiple downstream
tasks.

Our idea is to leverage self-supervision such that the model learns useful
representations of fine-grained image classes. We experimented with 3 kinds of
models: Jigsaw solving as pretext task, adversarial learning (SRGAN) and
contrastive learning based (SimCLR) model. The learned features are used for
downstream tasks such as fine-grained image classification. Our code is
available at
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Breiki_F/0/1/0/all/0/1"&gt;Farha Al Breiki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ridzuan_M/0/1/0/all/0/1"&gt;Muhammad Ridzuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grandhe_R/0/1/0/all/0/1"&gt;Rushali Grandhe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Translatotron 2: Robust direct speech-to-speech translation. (arXiv:2107.08661v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08661</id>
        <link href="http://arxiv.org/abs/2107.08661"/>
        <updated>2021-07-30T02:13:27.734Z</updated>
        <summary type="html"><![CDATA[We present Translatotron 2, a neural direct speech-to-speech translation
model that can be trained end-to-end. Translatotron 2 consists of a speech
encoder, a phoneme decoder, a mel-spectrogram synthesizer, and an attention
module that connects all the previous three components. Experimental results
suggest that Translatotron 2 outperforms the original Translatotron by a large
margin in terms of translation quality and predicted speech naturalness, and
drastically improves the robustness of the predicted speech by mitigating
over-generation, such as babbling or long pause. We also propose a new method
for retaining the source speaker's voice in the translated speech. The trained
model is restricted to retain the source speaker's voice, and unlike the
original Translatotron, it is not able to generate speech in a different
speaker's voice, making the model more robust for production deployment, by
mitigating potential misuse for creating spoofing audio artifacts. When the new
method is used together with a simple concatenation-based data augmentation,
the trained Translatotron 2 model is able to retain each speaker's voice for
input with speaker turns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1"&gt;Ye Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramanovich_M/0/1/0/all/0/1"&gt;Michelle Tadmor Ramanovich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1"&gt;Tal Remez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pomerantz_R/0/1/0/all/0/1"&gt;Roi Pomerantz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalizing Fairness: Discovery and Mitigation of Unknown Sensitive Attributes. (arXiv:2107.13625v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13625</id>
        <link href="http://arxiv.org/abs/2107.13625"/>
        <updated>2021-07-30T02:13:27.728Z</updated>
        <summary type="html"><![CDATA[When deploying artificial intelligence (AI) in the real world, being able to
trust the operation of the AI by characterizing how it performs is an
ever-present and important topic. An important and still largely unexplored
task in this characterization is determining major factors within the real
world that affect the AI's behavior, such as weather conditions or lighting,
and either a) being able to give justification for why it may have failed or b)
eliminating the influence the factor has. Determining these sensitive factors
heavily relies on collected data that is diverse enough to cover numerous
combinations of these factors, which becomes more onerous when having many
potential sensitive factors or operating in complex environments. This paper
investigates methods that discover and separate out individual semantic
sensitive factors from a given dataset to conduct this characterization as well
as addressing mitigation of these factors' sensitivity. We also broaden
remediation of fairness, which normally only addresses socially relevant
factors, and widen it to deal with the desensitization of AI with regard to all
possible aspects of variation in the domain. The proposed methods which
discover these major factors reduce the potentially onerous demands of
collecting a sufficiently diverse dataset. In experiments using the road sign
(GTSRB) and facial imagery (CelebA) datasets, we show the promise of using this
scheme to perform this characterization and remediation and demonstrate that
our approach outperforms state of the art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1"&gt;William Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1"&gt;Philippe Burlina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Geometry-Guided Depth via Projective Modeling for Monocular 3D Object Detection. (arXiv:2107.13931v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13931</id>
        <link href="http://arxiv.org/abs/2107.13931"/>
        <updated>2021-07-30T02:13:27.720Z</updated>
        <summary type="html"><![CDATA[As a crucial task of autonomous driving, 3D object detection has made great
progress in recent years. However, monocular 3D object detection remains a
challenging problem due to the unsatisfactory performance in depth estimation.
Most existing monocular methods typically directly regress the scene depth
while ignoring important relationships between the depth and various geometric
elements (e.g. bounding box sizes, 3D object dimensions, and object poses). In
this paper, we propose to learn geometry-guided depth estimation with
projective modeling to advance monocular 3D object detection. Specifically, a
principled geometry formula with projective modeling of 2D and 3D depth
predictions in the monocular 3D object detection network is devised. We further
implement and embed the proposed formula to enable geometry-aware deep
representation learning, allowing effective 2D and 3D interactions for boosting
the depth estimation. Moreover, we provide a strong baseline through addressing
substantial misalignment between 2D annotation and projected boxes to ensure
robust learning with the proposed geometric formula. Experiments on the KITTI
dataset show that our method remarkably improves the detection performance of
the state-of-the-art monocular-based method without extra data by 2.80% on the
moderate test setting. The model and code will be released at
https://github.com/YinminZhang/MonoGeo.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yinmin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xinzhu Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1"&gt;Shuai Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1"&gt;Jun Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhihui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1"&gt;Wanli Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Dan Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Demystifying Neural Language Models' Insensitivity to Word-Order. (arXiv:2107.13955v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.13955</id>
        <link href="http://arxiv.org/abs/2107.13955"/>
        <updated>2021-07-30T02:13:27.560Z</updated>
        <summary type="html"><![CDATA[Recent research analyzing the sensitivity of natural language understanding
models to word-order perturbations have shown that the state-of-the-art models
in several language tasks may have a unique way to understand the text that
could seldom be explained with conventional syntax and semantics. In this
paper, we investigate the insensitivity of natural language models to
word-order by quantifying perturbations and analysing their effect on neural
models' performance on language understanding tasks in GLUE benchmark. Towards
that end, we propose two metrics - the Direct Neighbour Displacement (DND) and
the Index Displacement Count (IDC) - that score the local and global ordering
of tokens in the perturbed texts and observe that perturbation functions found
in prior literature affect only the global ordering while the local ordering
remains relatively unperturbed. We propose perturbations at the granularity of
sub-words and characters to study the correlation between DND, IDC and the
performance of neural language models on natural language tasks. We find that
neural language models - pretrained and non-pretrained Transformers, LSTMs, and
Convolutional architectures - require local ordering more so than the global
ordering of tokens. The proposed metrics and the suite of perturbations allow a
systematic way to study the (in)sensitivity of neural language understanding
models to varying degree of perturbations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Clouatre_L/0/1/0/all/0/1"&gt;Louis Clouatre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parthasarathi_P/0/1/0/all/0/1"&gt;Prasanna Parthasarathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zouaq_A/0/1/0/all/0/1"&gt;Amal Zouaq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1"&gt;Sarath Chandar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Term Expansion and FinBERT fine-tuning for Hypernym and Synonym Ranking of Financial Terms. (arXiv:2107.13764v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.13764</id>
        <link href="http://arxiv.org/abs/2107.13764"/>
        <updated>2021-07-30T02:13:27.536Z</updated>
        <summary type="html"><![CDATA[Hypernym and synonym matching are one of the mainstream Natural Language
Processing (NLP) tasks. In this paper, we present systems that attempt to solve
this problem. We designed these systems to participate in the FinSim-3, a
shared task of FinNLP workshop at IJCAI-2021. The shared task is focused on
solving this problem for the financial domain. We experimented with various
transformer based pre-trained embeddings by fine-tuning these for either
classification or phrase similarity tasks. We also augmented the provided
dataset with abbreviations derived from prospectus provided by the organizers
and definitions of the financial terms from DBpedia [Auer et al., 2007],
Investopedia, and the Financial Industry Business Ontology (FIBO). Our best
performing system uses both FinBERT [Araci, 2019] and data augmentation from
the afore-mentioned sources. We observed that term expansion using data
augmentation in conjunction with semantic similarity is beneficial for this
task and could be useful for the other tasks that deal with short phrases. Our
best performing model (Accuracy: 0.917, Rank: 1.156) was developed by
fine-tuning SentenceBERT [Reimers et al., 2019] (with FinBERT at the backend)
over an extended labelled set created using the hierarchy of labels present in
FIBO.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chopra_A/0/1/0/all/0/1"&gt;Ankush Chopra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1"&gt;Sohom Ghosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Direct multimodal few-shot learning of speech and images. (arXiv:2012.05680v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05680</id>
        <link href="http://arxiv.org/abs/2012.05680"/>
        <updated>2021-07-30T02:13:27.511Z</updated>
        <summary type="html"><![CDATA[We propose direct multimodal few-shot models that learn a shared embedding
space of spoken words and images from only a few paired examples. Imagine an
agent is shown an image along with a spoken word describing the object in the
picture, e.g. pen, book and eraser. After observing a few paired examples of
each class, the model is asked to identify the "book" in a set of unseen
pictures. Previous work used a two-step indirect approach relying on learned
unimodal representations: speech-speech and image-image comparisons are
performed across the support set of given speech-image pairs. We propose two
direct models which instead learn a single multimodal space where inputs from
different modalities are directly comparable: a multimodal triplet network
(MTriplet) and a multimodal correspondence autoencoder (MCAE). To train these
direct models, we mine speech-image pairs: the support set is used to pair up
unlabelled in-domain speech and images. In a speech-to-image digit matching
task, direct models outperform indirect models, with the MTriplet achieving the
best multimodal five-shot accuracy. We show that the improvements are due to
the combination of unsupervised and transfer learning in the direct models, and
the absence of two-step compounding errors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nortje_L/0/1/0/all/0/1"&gt;Leanne Nortje&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamper_H/0/1/0/all/0/1"&gt;Herman Kamper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CogniFNN: A Fuzzy Neural Network Framework for Cognitive Word Embedding Evaluation. (arXiv:2009.11485v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.11485</id>
        <link href="http://arxiv.org/abs/2009.11485"/>
        <updated>2021-07-30T02:13:27.470Z</updated>
        <summary type="html"><![CDATA[Word embeddings can reflect the semantic representations, and the embedding
qualities can be comprehensively evaluated with human natural reading-related
cognitive data sources. In this paper, we proposed the CogniFNN framework,
which is the first attempt at using fuzzy neural networks to extract non-linear
and non-stationary characteristics for evaluations of English word embeddings
against the corresponding cognitive datasets. In our experiment, we used 15
human cognitive datasets across three modalities: EEG, fMRI, and eye-tracking,
and selected the mean square error and multiple hypotheses testing as metrics
to evaluate our proposed CogniFNN framework. Compared to the recent pioneer
framework, our proposed CogniFNN showed smaller prediction errors of both
context-independent (GloVe) and context-sensitive (BERT) word embeddings, and
achieved higher significant ratios with randomly generated word embeddings. Our
findings suggested that the CogniFNN framework could provide a more accurate
and comprehensive evaluation of cognitive word embeddings. It will potentially
be beneficial to the further word embeddings evaluation on extrinsic natural
language processing tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xinping Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1"&gt;Zehong Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1"&gt;Son Tran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis of User Preferences for Robot Motions in Immersive Telepresence. (arXiv:2103.03496v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03496</id>
        <link href="http://arxiv.org/abs/2103.03496"/>
        <updated>2021-07-30T02:13:27.261Z</updated>
        <summary type="html"><![CDATA[This paper considers how the motions of a telepresence robot moving
autonomously affect a person immersed in the robot through a head-mounted
display. In particular, we explore the preference, comfort, and naturalness of
elements of piecewise linear paths compared to the same elements on a smooth
path. In a user study, thirty-six subjects watched panoramic videos of three
different paths through a simulated museum in virtual reality and responded to
questionnaires regarding each path. Preference for a particular path was
influenced the most by comfort, forward speed, and characteristics of the
turns. Preference was also strongly associated with the users' perceived
naturalness, which was primarily determined by the ability to see salient
objects, the distance to the walls and objects, as well as the turns.
Participants favored the paths that had a one meter per second forward speed
and rated the path with the least amount of turns as the most comfortable]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mimnaugh_K/0/1/0/all/0/1"&gt;Katherine J. Mimnaugh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suomalainen_M/0/1/0/all/0/1"&gt;Markku Suomalainen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Becerra_I/0/1/0/all/0/1"&gt;Israel Becerra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lozano_E/0/1/0/all/0/1"&gt;Eliezer Lozano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murrieta_Cid_R/0/1/0/all/0/1"&gt;Rafael Murrieta-Cid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+LaValle_S/0/1/0/all/0/1"&gt;Steven M. LaValle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pitch-Informed Instrument Assignment Using a Deep Convolutional Network with Multiple Kernel Shapes. (arXiv:2107.13617v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.13617</id>
        <link href="http://arxiv.org/abs/2107.13617"/>
        <updated>2021-07-30T02:13:27.230Z</updated>
        <summary type="html"><![CDATA[This paper proposes a deep convolutional neural network for performing
note-level instrument assignment. Given a polyphonic multi-instrumental music
signal along with its ground truth or predicted notes, the objective is to
assign an instrumental source for each note. This problem is addressed as a
pitch-informed classification task where each note is analysed individually. We
also propose to utilise several kernel shapes in the convolutional layers in
order to facilitate learning of efficient timbre-discriminative feature maps.
Experiments on the MusicNet dataset using 7 instrument classes show that our
approach is able to achieve an average F-score of 0.904 when the original
multi-pitch annotations are used as the pitch information for the system, and
that it also excels if the note information is provided using third-party
multi-pitch estimation algorithms. We also include ablation studies
investigating the effects of the use of multiple kernel shapes and comparing
different input representations for the audio and the note-related information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lordelo_C/0/1/0/all/0/1"&gt;Carlos Lordelo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benetos_E/0/1/0/all/0/1"&gt;Emmanouil Benetos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dixon_S/0/1/0/all/0/1"&gt;Simon Dixon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahlback_S/0/1/0/all/0/1"&gt;Sven Ahlb&amp;#xe4;ck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Transformer based Dual Discriminator Generative Adversarial Networks for Video Anomaly Detection. (arXiv:2107.13720v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13720</id>
        <link href="http://arxiv.org/abs/2107.13720"/>
        <updated>2021-07-30T02:13:27.224Z</updated>
        <summary type="html"><![CDATA[Detecting abnormal activities in real-world surveillance videos is an
important yet challenging task as the prior knowledge about video anomalies is
usually limited or unavailable. Despite that many approaches have been
developed to resolve this problem, few of them can capture the normal
spatio-temporal patterns effectively and efficiently. Moreover, existing works
seldom explicitly consider the local consistency at frame level and global
coherence of temporal dynamics in video sequences. To this end, we propose
Convolutional Transformer based Dual Discriminator Generative Adversarial
Networks (CT-D2GAN) to perform unsupervised video anomaly detection.
Specifically, we first present a convolutional transformer to perform future
frame prediction. It contains three key components, i.e., a convolutional
encoder to capture the spatial information of the input video clips, a temporal
self-attention module to encode the temporal dynamics, and a convolutional
decoder to integrate spatio-temporal features and predict the future frame.
Next, a dual discriminator based adversarial training procedure, which jointly
considers an image discriminator that can maintain the local consistency at
frame-level and a video discriminator that can enforce the global coherence of
temporal dynamics, is employed to enhance the future frame prediction. Finally,
the prediction error is used to identify abnormal video frames. Thoroughly
empirical studies on three public video anomaly detection datasets, i.e., UCSD
Ped2, CUHK Avenue, and Shanghai Tech Campus, demonstrate the effectiveness of
the proposed adversarial spatio-temporal modeling framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1"&gt;Xinyang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1"&gt;Dongjin Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuncong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhengzhang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1"&gt;Jingchao Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ranking Micro-Influencers: a Novel Multi-Task Learning and Interpretable Framework. (arXiv:2107.13943v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13943</id>
        <link href="http://arxiv.org/abs/2107.13943"/>
        <updated>2021-07-30T02:13:27.216Z</updated>
        <summary type="html"><![CDATA[With the rise in use of social media to promote branded products, the demand
for effective influencer marketing has increased. Brands are looking for
improved ways to identify valuable influencers among a vast catalogue; this is
even more challenging with "micro-influencers", which are more affordable than
mainstream ones but difficult to discover. In this paper, we propose a novel
multi-task learning framework to improve the state of the art in
micro-influencer ranking based on multimedia content. Moreover, since the
visual congruence between a brand and influencer has been shown to be good
measure of compatibility, we provide an effective visual method for
interpreting our models' decisions, which can also be used to inform brands'
media strategies. We compare with the current state-of-the-art on a recently
constructed public dataset and we show significant improvement both in terms of
accuracy and model complexity. The techniques for ranking and interpretation
presented in this work can be generalised to arbitrary multimedia ranking tasks
that have datasets with a similar structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elwood_A/0/1/0/all/0/1"&gt;Adam Elwood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gasparin_A/0/1/0/all/0/1"&gt;Alberto Gasparin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rozza_A/0/1/0/all/0/1"&gt;Alessandro Rozza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. (arXiv:2107.13586v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.13586</id>
        <link href="http://arxiv.org/abs/2107.13586"/>
        <updated>2021-07-30T02:13:27.208Z</updated>
        <summary type="html"><![CDATA[This paper surveys and organizes research works in a new paradigm in natural
language processing, which we dub "prompt-based learning". Unlike traditional
supervised learning, which trains a model to take in an input x and predict an
output y as P(y|x), prompt-based learning is based on language models that
model the probability of text directly. To use these models to perform
prediction tasks, the original input x is modified using a template into a
textual string prompt x' that has some unfilled slots, and then the language
model is used to probabilistically fill the unfilled information to obtain a
final string x, from which the final output y can be derived. This framework is
powerful and attractive for a number of reasons: it allows the language model
to be pre-trained on massive amounts of raw text, and by defining a new
prompting function the model is able to perform few-shot or even zero-shot
learning, adapting to new scenarios with few or no labeled data. In this paper
we introduce the basics of this promising paradigm, describe a unified set of
mathematical notations that can cover a wide variety of existing work, and
organize existing work along several dimensions, e.g.the choice of pre-trained
models, prompts, and tuning strategies. To make the field more accessible to
interested beginners, we not only make a systematic review of existing works
and a highly structured typology of prompt-based concepts, but also release
other resources, e.g., a website this http URL including
constantly-updated survey, and paperlist.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Pengfei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1"&gt;Weizhe Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1"&gt;Jinlan Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhengbao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayashi_H/0/1/0/all/0/1"&gt;Hiroaki Hayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1"&gt;Graham Neubig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReconVAT: A Semi-Supervised Automatic Music Transcription Framework for Low-Resource Real-World Data. (arXiv:2107.04954v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04954</id>
        <link href="http://arxiv.org/abs/2107.04954"/>
        <updated>2021-07-30T02:13:27.197Z</updated>
        <summary type="html"><![CDATA[Most of the current supervised automatic music transcription (AMT) models
lack the ability to generalize. This means that they have trouble transcribing
real-world music recordings from diverse musical genres that are not presented
in the labelled training data. In this paper, we propose a semi-supervised
framework, ReconVAT, which solves this issue by leveraging the huge amount of
available unlabelled music recordings. The proposed ReconVAT uses
reconstruction loss and virtual adversarial training. When combined with
existing U-net models for AMT, ReconVAT achieves competitive results on common
benchmark datasets such as MAPS and MusicNet. For example, in the few-shot
setting for the string part version of MusicNet, ReconVAT achieves F1-scores of
61.0% and 41.6% for the note-wise and note-with-offset-wise metrics
respectively, which translates into an improvement of 22.2% and 62.5% compared
to the supervised baseline model. Our proposed framework also demonstrates the
potential of continual learning on new data, which could be useful in
real-world applications whereby new data is constantly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheuk_K/0/1/0/all/0/1"&gt;Kin Wai Cheuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herremans_D/0/1/0/all/0/1"&gt;Dorien Herremans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1"&gt;Li Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models. (arXiv:2107.13686v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.13686</id>
        <link href="http://arxiv.org/abs/2107.13686"/>
        <updated>2021-07-30T02:13:27.181Z</updated>
        <summary type="html"><![CDATA[Pre-trained language models (PLMs) have achieved great success in natural
language processing. Most of PLMs follow the default setting of architecture
hyper-parameters (e.g., the hidden dimension is a quarter of the intermediate
dimension in feed-forward sub-networks) in BERT (Devlin et al., 2019). Few
studies have been conducted to explore the design of architecture
hyper-parameters in BERT, especially for the more efficient PLMs with tiny
sizes, which are essential for practical deployment on resource-constrained
devices. In this paper, we adopt the one-shot Neural Architecture Search (NAS)
to automatically search architecture hyper-parameters. Specifically, we
carefully design the techniques of one-shot learning and the search space to
provide an adaptive and efficient development way of tiny PLMs for various
latency constraints. We name our method AutoTinyBERT and evaluate its
effectiveness on the GLUE and SQuAD benchmarks. The extensive experiments show
that our method outperforms both the SOTA search-based baseline (NAS-BERT) and
the SOTA distillation-based methods (such as DistilBERT, TinyBERT, MiniLM and
MobileBERT). In addition, based on the obtained architectures, we propose a
more efficient development method that is even faster than the development of a
single PLM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1"&gt;Yichun Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Cheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1"&gt;Lifeng Shang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qun Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the Effects of Adversarial Personalized Ranking Optimization Method on Recommendation Quality. (arXiv:2107.13876v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.13876</id>
        <link href="http://arxiv.org/abs/2107.13876"/>
        <updated>2021-07-30T02:13:27.172Z</updated>
        <summary type="html"><![CDATA[Recommender systems (RSs) employ user-item feedback, e.g., ratings, to match
customers to personalized lists of products. Approaches to top-k recommendation
mainly rely on Learning-To-Rank algorithms and, among them, the most widely
adopted is Bayesian Personalized Ranking (BPR), which bases on a pair-wise
optimization approach. Recently, BPR has been found vulnerable against
adversarial perturbations of its model parameters. Adversarial Personalized
Ranking (APR) mitigates this issue by robustifying BPR via an adversarial
training procedure. The empirical improvements of APR's accuracy performance on
BPR have led to its wide use in several recommender models. However, a key
overlooked aspect has been the beyond-accuracy performance of APR, i.e.,
novelty, coverage, and amplification of popularity bias, considering that
recent results suggest that BPR, the building block of APR, is sensitive to the
intensification of biases and reduction of recommendation novelty. In this
work, we model the learning characteristics of the BPR and APR optimization
frameworks to give mathematical evidence that, when the feedback data have a
tailed distribution, APR amplifies the popularity bias more than BPR due to an
unbalanced number of received positive updates from short-head items. Using
matrix factorization (MF), we empirically validate the theoretical results by
performing preliminary experiments on two public datasets to compare BPR-MF and
APR-MF performance on accuracy and beyond-accuracy metrics. The experimental
results consistently show the degradation of novelty and coverage measures and
a worrying amplification of bias.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anelli_V/0/1/0/all/0/1"&gt;Vito Walter Anelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deldjoo_Y/0/1/0/all/0/1"&gt;Yashar Deldjoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noia_T/0/1/0/all/0/1"&gt;Tommaso Di Noia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merra_F/0/1/0/all/0/1"&gt;Felice Antonio Merra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video-based Point Cloud Compression Artifact Removal. (arXiv:2107.14179v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.14179</id>
        <link href="http://arxiv.org/abs/2107.14179"/>
        <updated>2021-07-30T02:13:27.160Z</updated>
        <summary type="html"><![CDATA[Photo-realistic point cloud capture and transmission are the fundamental
enablers for immersive visual communication. The coding process of dynamic
point clouds, especially video-based point cloud compression (V-PCC) developed
by the MPEG standardization group, is now delivering state-of-the-art
performance in compression efficiency. V-PCC is based on the projection of the
point cloud patches to 2D planes and encoding the sequence as 2D texture and
geometry patch sequences. However, the resulting quantization errors from
coding can introduce compression artifacts, which can be very unpleasant for
the quality of experience (QoE). In this work, we developed a novel
out-of-the-loop point cloud geometry artifact removal solution that can
significantly improve reconstruction quality without additional bandwidth cost.
Our novel framework consists of a point cloud sampling scheme, an artifact
removal network, and an aggregation scheme. The point cloud sampling scheme
employs a cube-based neighborhood patch extraction to divide the point cloud
into patches. The geometry artifact removal network then processes these
patches to obtain artifact-removed patches. The artifact-removed patches are
then merged together using an aggregation scheme to obtain the final
artifact-removed point cloud. We employ 3D deep convolutional feature learning
for geometry artifact removal that jointly recovers both the quantization
direction and the quantization noise level by exploiting projection and
quantization prior. The simulation results demonstrate that the proposed method
is highly effective and can considerably improve the quality of the
reconstructed point cloud.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akhtar_A/0/1/0/all/0/1"&gt;Anique Akhtar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1"&gt;Wen Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Li Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1"&gt;Wei Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-matched Pre-training Tasks for Dense Retrieval. (arXiv:2107.13602v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.13602</id>
        <link href="http://arxiv.org/abs/2107.13602"/>
        <updated>2021-07-30T02:13:27.152Z</updated>
        <summary type="html"><![CDATA[Pre-training on larger datasets with ever increasing model size is now a
proven recipe for increased performance across almost all NLP tasks. A notable
exception is information retrieval, where additional pre-training has so far
failed to produce convincing results. We show that, with the right pre-training
setup, this barrier can be overcome. We demonstrate this by pre-training large
bi-encoder models on 1) a recently released set of 65 million synthetically
generated questions, and 2) 200 million post-comment pairs from a preexisting
dataset of Reddit conversations made available by pushshift.io. We evaluate on
a set of information retrieval and dialogue retrieval benchmarks, showing
substantial improvements over supervised baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1"&gt;Barlas O&amp;#x11f;uz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1"&gt;Kushal Lakhotia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Anchit Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1"&gt;Patrick Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karpukhin_V/0/1/0/all/0/1"&gt;Vladimir Karpukhin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piktus_A/0/1/0/all/0/1"&gt;Aleksandra Piktus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xilun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1"&gt;Sebastian Riedel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1"&gt;Wen-tau Yih&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Sonal Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1"&gt;Yashar Mehdad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PAD: a graphical and numerical enhancement of structural coding to facilitate thematic analysis of a literature corpus. (arXiv:2107.13983v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2107.13983</id>
        <link href="http://arxiv.org/abs/2107.13983"/>
        <updated>2021-07-30T02:13:27.143Z</updated>
        <summary type="html"><![CDATA[We suggest an enhancement to structural coding through the use of (a)
causally bound codes, (b) basic constructs of graph theory and (c) statistics.
As is the norm with structural coding, the codes are collected into categories.
The categories are represented by nodes (graph theory). The causality is
illustrated through links (graph theory) between the nodes and the entire set
of linked nodes is collected into a single directed acyclic graph. The number
of occurrences of the nodes and the links provide the input required to analyze
relative frequency of occurrence, as well as opening a scope for further
statistical analysis. While our raw data was a corpus of literature from a
specific discipline, this enhancement is accessible to any qualitative analysis
that recognizes causality in its structural codes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Depasquale_E/0/1/0/all/0/1"&gt;Etienne-Victor Depasquale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salam_H/0/1/0/all/0/1"&gt;Humaira Abdul Salam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davoli_F/0/1/0/all/0/1"&gt;Franco Davoli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Cross-Lingual Arabic Information REtrieval (CLAIRE) System. (arXiv:2107.13751v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.13751</id>
        <link href="http://arxiv.org/abs/2107.13751"/>
        <updated>2021-07-30T02:13:27.126Z</updated>
        <summary type="html"><![CDATA[Despite advances in neural machine translation, cross-lingual retrieval tasks
in which queries and documents live in different natural language spaces remain
challenging. Although neural translation models may provide an intuitive
approach to tackle the cross-lingual problem, their resource-consuming training
and advanced model structures may complicate the overall retrieval pipeline and
reduce users engagement. In this paper, we build our end-to-end Cross-Lingual
Arabic Information REtrieval (CLAIRE) system based on the cross-lingual word
embedding where searchers are assumed to have a passable passive understanding
of Arabic and various supporting information in English is provided to aid
retrieval experience. The proposed system has three major advantages: (1) The
usage of English-Arabic word embedding simplifies the overall pipeline and
avoids the potential mistakes caused by machine translation. (2) Our CLAIRE
system can incorporate arbitrary word embedding-based neural retrieval models
without structural modification. (3) Early empirical results on an Arabic news
collection show promising performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhizhong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1"&gt;Carsten Eickhoff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Merge of k-NN Graph. (arXiv:1908.00814v6 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.00814</id>
        <link href="http://arxiv.org/abs/1908.00814"/>
        <updated>2021-07-30T02:13:27.117Z</updated>
        <summary type="html"><![CDATA[k-nearest neighbor graph is a fundamental data structure in many disciplines
such as information retrieval, data-mining, pattern recognition, and machine
learning, etc. In the literature, considerable research has been focusing on
how to efficiently build an approximate k-nearest neighbor graph (k-NN graph)
for a fixed dataset. Unfortunately, a closely related issue of how to merge two
existing k-NN graphs has been overlooked. In this paper, we address the issue
of k-NN graph merging in two different scenarios. In the first scenario, a
symmetric merge algorithm is proposed to combine two approximate k-NN graphs.
The algorithm facilitates large-scale processing by the efficient merging of
k-NN graphs that are produced in parallel. In the second scenario, a joint
merge algorithm is proposed to expand an existing k-NN graph with a raw
dataset. The algorithm enables the incremental construction of a hierarchical
approximate k-NN graph. Superior performance is attained when leveraging the
hierarchy for NN search of various data types, dimensionality, and distance
measures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wan-Lei Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1"&gt;Peng-Cheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1"&gt;Chong-Wah Ngo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Investigating Text Simplification Evaluation. (arXiv:2107.13662v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.13662</id>
        <link href="http://arxiv.org/abs/2107.13662"/>
        <updated>2021-07-30T02:13:27.101Z</updated>
        <summary type="html"><![CDATA[Modern text simplification (TS) heavily relies on the availability of gold
standard data to build machine learning models. However, existing studies show
that parallel TS corpora contain inaccurate simplifications and incorrect
alignments. Additionally, evaluation is usually performed by using metrics such
as BLEU or SARI to compare system output to the gold standard. A major
limitation is that these metrics do not match human judgements and the
performance on different datasets and linguistic phenomena vary greatly.
Furthermore, our research shows that the test and training subsets of parallel
datasets differ significantly. In this work, we investigate existing TS
corpora, providing new insights that will motivate the improvement of existing
state-of-the-art TS evaluation methods. Our contributions include the analysis
of TS corpora based on existing modifications used for simplification and an
empirical study on TS models performance by using better-distributed datasets.
We demonstrate that by improving the distribution of TS datasets, we can build
more robust TS models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vasquez_Rodriguez_L/0/1/0/all/0/1"&gt;Laura V&amp;#xe1;squez-Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shardlow_M/0/1/0/all/0/1"&gt;Matthew Shardlow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Przybyla_P/0/1/0/all/0/1"&gt;Piotr Przyby&amp;#x142;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1"&gt;Sophia Ananiadou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-matched Pre-training Tasks for Dense Retrieval. (arXiv:2107.13602v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.13602</id>
        <link href="http://arxiv.org/abs/2107.13602"/>
        <updated>2021-07-30T02:13:27.087Z</updated>
        <summary type="html"><![CDATA[Pre-training on larger datasets with ever increasing model size is now a
proven recipe for increased performance across almost all NLP tasks. A notable
exception is information retrieval, where additional pre-training has so far
failed to produce convincing results. We show that, with the right pre-training
setup, this barrier can be overcome. We demonstrate this by pre-training large
bi-encoder models on 1) a recently released set of 65 million synthetically
generated questions, and 2) 200 million post-comment pairs from a preexisting
dataset of Reddit conversations made available by pushshift.io. We evaluate on
a set of information retrieval and dialogue retrieval benchmarks, showing
substantial improvements over supervised baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1"&gt;Barlas O&amp;#x11f;uz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1"&gt;Kushal Lakhotia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Anchit Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1"&gt;Patrick Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karpukhin_V/0/1/0/all/0/1"&gt;Vladimir Karpukhin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piktus_A/0/1/0/all/0/1"&gt;Aleksandra Piktus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xilun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1"&gt;Sebastian Riedel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1"&gt;Wen-tau Yih&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Sonal Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1"&gt;Yashar Mehdad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elliot: a Comprehensive and Rigorous Framework for Reproducible Recommender Systems Evaluation. (arXiv:2103.02590v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02590</id>
        <link href="http://arxiv.org/abs/2103.02590"/>
        <updated>2021-07-30T02:13:27.069Z</updated>
        <summary type="html"><![CDATA[Recommender Systems have shown to be an effective way to alleviate the
over-choice problem and provide accurate and tailored recommendations. However,
the impressive number of proposed recommendation algorithms, splitting
strategies, evaluation protocols, metrics, and tasks, has made rigorous
experimental evaluation particularly challenging. Puzzled and frustrated by the
continuous recreation of appropriate evaluation benchmarks, experimental
pipelines, hyperparameter optimization, and evaluation procedures, we have
developed an exhaustive framework to address such needs. Elliot is a
comprehensive recommendation framework that aims to run and reproduce an entire
experimental pipeline by processing a simple configuration file. The framework
loads, filters, and splits the data considering a vast set of strategies (13
splitting methods and 8 filtering approaches, from temporal training-test
splitting to nested K-folds Cross-Validation). Elliot optimizes hyperparameters
(51 strategies) for several recommendation algorithms (50), selects the best
models, compares them with the baselines providing intra-model statistics,
computes metrics (36) spanning from accuracy to beyond-accuracy, bias, and
fairness, and conducts statistical analysis (Wilcoxon and Paired t-test). The
aim is to provide the researchers with a tool to ease (and make them
reproducible) all the experimental evaluation phases, from data reading to
results collection. Elliot is available on GitHub
(https://github.com/sisinflab/elliot).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anelli_V/0/1/0/all/0/1"&gt;Vito Walter Anelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bellogin_A/0/1/0/all/0/1"&gt;Alejandro Bellog&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferrara_A/0/1/0/all/0/1"&gt;Antonio Ferrara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malitesta_D/0/1/0/all/0/1"&gt;Daniele Malitesta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merra_F/0/1/0/all/0/1"&gt;Felice Antonio Merra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pomo_C/0/1/0/all/0/1"&gt;Claudio Pomo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donini_F/0/1/0/all/0/1"&gt;Francesco Maria Donini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noia_T/0/1/0/all/0/1"&gt;Tommaso Di Noia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Tale of Two Efficient and Informative Negative Sampling Distributions. (arXiv:2012.15843v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15843</id>
        <link href="http://arxiv.org/abs/2012.15843"/>
        <updated>2021-07-30T02:13:27.055Z</updated>
        <summary type="html"><![CDATA[Softmax classifiers with a very large number of classes naturally occur in
many applications such as natural language processing and information
retrieval. The calculation of full softmax is costly from the computational and
energy perspective. There have been various sampling approaches to overcome
this challenge, popularly known as negative sampling (NS). Ideally, NS should
sample negative classes from a distribution that is dependent on the input
data, the current parameters, and the correct positive class. Unfortunately,
due to the dynamically updated parameters and data samples, there is no
sampling scheme that is provably adaptive and samples the negative classes
efficiently. Therefore, alternative heuristics like random sampling, static
frequency-based sampling, or learning-based biased sampling, which primarily
trade either the sampling cost or the adaptivity of samples per iteration are
adopted. In this paper, we show two classes of distributions where the sampling
scheme is truly adaptive and provably generates negative samples in
near-constant time. Our implementation in C++ on CPU is significantly superior,
both in terms of wall-clock time and accuracy, compared to the most optimized
TensorFlow implementations of other popular negative sampling approaches on
powerful NVIDIA V100 GPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Daghaghi_S/0/1/0/all/0/1"&gt;Shabnam Daghaghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Medini_T/0/1/0/all/0/1"&gt;Tharun Medini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meisburger_N/0/1/0/all/0/1"&gt;Nicholas Meisburger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Beidi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1"&gt;Mengnan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Anshumali Shrivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting Abusive Albanian. (arXiv:2107.13592v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.13592</id>
        <link href="http://arxiv.org/abs/2107.13592"/>
        <updated>2021-07-30T02:13:27.042Z</updated>
        <summary type="html"><![CDATA[The ever growing usage of social media in the recent years has had a direct
impact on the increased presence of hate speech and offensive speech in online
platforms. Research on effective detection of such content has mainly focused
on English and a few other widespread languages, while the leftover majority
fail to have the same work put into them and thus cannot benefit from the
steady advancements made in the field. In this paper we present \textsc{Shaj},
an annotated Albanian dataset for hate speech and offensive speech that has
been constructed from user-generated content on various social media platforms.
Its annotation follows the hierarchical schema introduced in OffensEval. The
dataset is tested using three different classification models, the best of
which achieves an F1 score of 0.77 for the identification of offensive
language, 0.64 F1 score for the automatic categorization of offensive types and
lastly, 0.52 F1 score for the offensive language target identification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nurce_E/0/1/0/all/0/1"&gt;Erida Nurce&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keci_J/0/1/0/all/0/1"&gt;Jorgel Keci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Derczynski_L/0/1/0/all/0/1"&gt;Leon Derczynski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interactive GIS Web-Atlas for Twelve Pacific Islands Countries. (arXiv:2107.14041v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2107.14041</id>
        <link href="http://arxiv.org/abs/2107.14041"/>
        <updated>2021-07-30T02:13:27.025Z</updated>
        <summary type="html"><![CDATA[This article deals with the development of an interactive up-to-date Pacific
Islands Web GIS Atlas. It focuses on the compilation of spatial data from the
twelve member countries of the University of the South Pacific (Cook Islands,
Fiji Islands, Kiribati Islands, Marshall Islands, Nauru, Niue, Tonga, Tuvalu,
Tokelau, Solomon Islands, Vanuatu, and Western Samoa). A previous bitmap web
Atlas was created in 1996, and was a pilot activity investigating the potential
for using Geographical Information Systems (GIS) in the South Pacific. The
objective of the new atlas is to provide sets of spatial and attributive data
and maps for use of educators, students, researchers, policy makers and other
relevant user groups and the public. GIS is a highly flexible and dynamic
technology that allows the construction and analysis of maps and data sets from
a variety of sources and formats. Nowadays, GIS application has moved from
local and client-server applications to a three-tier architecture: Client (Web
Browser) -- Application Web Map Server -- Spatial Data Warehouses. The
objective of this project is to produce an Atlas that will include interactive
maps and data on an Application Web Map Server. Intergraph products such as
GeoMedia Professional, Web Map and Web Publisher have been selected for the web
atlas production and design. In an interactive environment, an atlas will be
composed from a series of maps and data profiles, which will be based on legend
entries, queries, hot spots and cartographic tools. Only the first stage of
development of the atlas and related technological solutions are outlined in
this article.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lartigou_F/0/1/0/all/0/1"&gt;Fabrice Lartigou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Govorov_M/0/1/0/all/0/1"&gt;Michael Govorov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aisake_T/0/1/0/all/0/1"&gt;Tofiga Aisake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1"&gt;Pankajeshwara N. Sharma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ExpertRank: A Multi-level Coarse-grained Expert-based Listwise Ranking Loss. (arXiv:2107.13752v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.13752</id>
        <link href="http://arxiv.org/abs/2107.13752"/>
        <updated>2021-07-30T02:13:27.009Z</updated>
        <summary type="html"><![CDATA[The goal of information retrieval is to recommend a list of document
candidates that are most relevant to a given query. Listwise learning trains
neural retrieval models by comparing various candidates simultaneously on a
large scale, offering much more competitive performance than pairwise and
pointwise schemes. Existing listwise ranking losses treat the candidate
document list as a whole unit without further inspection. Some candidates with
moderate semantic prominence may be ignored by the noisy similarity signals or
overshadowed by a few especially pronounced candidates. As a result, existing
ranking losses fail to exploit the full potential of neural retrieval models.
To address these concerns, we apply the classic pooling technique to conduct
multi-level coarse graining and propose ExpertRank, a novel expert-based
listwise ranking loss. The proposed scheme has three major advantages: (1)
ExpertRank introduces the profound physics concept of coarse graining to
information retrieval by selecting prominent candidates at various local levels
based on model prediction and inter-document comparison. (2) ExpertRank applies
the mixture of experts (MoE) technique to combine different experts effectively
by extending the traditional ListNet. (3) Compared to other existing listwise
learning approaches, ExpertRank produces much more reliable and competitive
performance for various neural retrieval models with different complexities,
from traditional models, such as KNRM, ConvKNRM, MatchPyramid, to sophisticated
BERT/ALBERT-based retrieval models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhizhong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1"&gt;Carsten Eickhoff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sign and Search: Sign Search Functionality for Sign Language Lexica. (arXiv:2107.13637v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13637</id>
        <link href="http://arxiv.org/abs/2107.13637"/>
        <updated>2021-07-30T02:13:26.993Z</updated>
        <summary type="html"><![CDATA[Sign language lexica are a useful resource for researchers and people
learning sign languages. Current implementations allow a user to search a sign
either by its gloss or by selecting its primary features such as handshape and
location. This study focuses on exploring a reverse search functionality where
a user can sign a query sign in front of a webcam and retrieve a set of
matching signs. By extracting different body joints combinations (upper body,
dominant hand's arm and wrist) using the pose estimation framework OpenPose, we
compare four techniques (PCA, UMAP, DTW and Euclidean distance) as distance
metrics between 20 query signs, each performed by eight participants on a 1200
sign lexicon. The results show that UMAP and DTW can predict a matching sign
with an 80\% and 71\% accuracy respectively at the top-20 retrieved signs using
the movement of the dominant hand arm. Using DTW and adding more sign instances
from other participants in the lexicon, the accuracy can be raised to 90\% at
the top-10 ranking. Our results suggest that our methodology can be used with
no training in any sign language lexicon regardless of its size.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fragkiadakis_M/0/1/0/all/0/1"&gt;Manolis Fragkiadakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Putten_P/0/1/0/all/0/1"&gt;Peter van der Putten&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sign and Search: Sign Search Functionality for Sign Language Lexica. (arXiv:2107.13637v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13637</id>
        <link href="http://arxiv.org/abs/2107.13637"/>
        <updated>2021-07-30T02:13:26.946Z</updated>
        <summary type="html"><![CDATA[Sign language lexica are a useful resource for researchers and people
learning sign languages. Current implementations allow a user to search a sign
either by its gloss or by selecting its primary features such as handshape and
location. This study focuses on exploring a reverse search functionality where
a user can sign a query sign in front of a webcam and retrieve a set of
matching signs. By extracting different body joints combinations (upper body,
dominant hand's arm and wrist) using the pose estimation framework OpenPose, we
compare four techniques (PCA, UMAP, DTW and Euclidean distance) as distance
metrics between 20 query signs, each performed by eight participants on a 1200
sign lexicon. The results show that UMAP and DTW can predict a matching sign
with an 80\% and 71\% accuracy respectively at the top-20 retrieved signs using
the movement of the dominant hand arm. Using DTW and adding more sign instances
from other participants in the lexicon, the accuracy can be raised to 90\% at
the top-10 ranking. Our results suggest that our methodology can be used with
no training in any sign language lexicon regardless of its size.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fragkiadakis_M/0/1/0/all/0/1"&gt;Manolis Fragkiadakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Putten_P/0/1/0/all/0/1"&gt;Peter van der Putten&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Plant Cover Estimation with Convolutional Neural Networks. (arXiv:2106.11154v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11154</id>
        <link href="http://arxiv.org/abs/2106.11154"/>
        <updated>2021-07-29T02:00:11.252Z</updated>
        <summary type="html"><![CDATA[Monitoring the responses of plants to environmental changes is essential for
plant biodiversity research. This, however, is currently still being done
manually by botanists in the field. This work is very laborious, and the data
obtained is, though following a standardized method to estimate plant coverage,
usually subjective and has a coarse temporal resolution. To remedy these
caveats, we investigate approaches using convolutional neural networks (CNNs)
to automatically extract the relevant data from images, focusing on plant
community composition and species coverages of 9 herbaceous plant species. To
this end, we investigate several standard CNN architectures and different
pretraining methods. We find that we outperform our previous approach at higher
image resolutions using a custom CNN with a mean absolute error of 5.16%. In
addition to these investigations, we also conduct an error analysis based on
the temporal aspect of the plant cover images. This analysis gives insight into
where problems for automatic approaches lie, like occlusion and likely
misclassifications caused by temporal changes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Korschens_M/0/1/0/all/0/1"&gt;Matthias K&amp;#xf6;rschens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bodesheim_P/0/1/0/all/0/1"&gt;Paul Bodesheim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romermann_C/0/1/0/all/0/1"&gt;Christine R&amp;#xf6;mermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bucher_S/0/1/0/all/0/1"&gt;Solveig Franziska Bucher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Migliavacca_M/0/1/0/all/0/1"&gt;Mirco Migliavacca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ulrich_J/0/1/0/all/0/1"&gt;Josephine Ulrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Denzler_J/0/1/0/all/0/1"&gt;Joachim Denzler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Robust Method for Image Stitching. (arXiv:2004.03860v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.03860</id>
        <link href="http://arxiv.org/abs/2004.03860"/>
        <updated>2021-07-29T02:00:11.244Z</updated>
        <summary type="html"><![CDATA[We propose a novel method for large-scale image stitching that is robust
against repetitive patterns and featureless regions in the imagery. In such
cases, state-of-the-art image stitching methods easily produce image alignment
artifacts, since they may produce false pairwise image registrations that are
in conflict within the global connectivity graph. Our method augments the
current methods by collecting all the plausible pairwise image registration
candidates, among which globally consistent candidates are chosen. This enables
the stitching process to determine the correct pairwise registrations by
utilizing all the available information from the whole imagery, such as
unambiguous registrations outside the repeating pattern and featureless
regions. We formalize the method as a weighted multigraph whose nodes represent
the individual image transformations from the composite image, and whose sets
of multiple edges between two nodes represent all the plausible transformations
between the pixel coordinates of the two images. The edge weights represent the
plausibility of the transformations. The image transformations and the edge
weights are solved from a non-linear minimization problem with linear
constraints, for which a projection method is used. As an example, we apply the
method in a large-scale scanning application where the transformations are
primarily translations with only slight rotation and scaling component. Despite
these simplifications, the state-of-the-art methods do not produce adequate
results in such applications, since the image overlap is small, which can be
featureless or repetitive, and misalignment artifacts and their concealment are
unacceptable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pellikka_M/0/1/0/all/0/1"&gt;Matti Pellikka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lahtinen_V/0/1/0/all/0/1"&gt;Valtteri Lahtinen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward Drug-Target Interaction Prediction via Ensemble Modeling and Transfer Learning. (arXiv:2107.00719v2 [q-bio.BM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00719</id>
        <link href="http://arxiv.org/abs/2107.00719"/>
        <updated>2021-07-29T02:00:11.235Z</updated>
        <summary type="html"><![CDATA[Drug-target interaction (DTI) prediction plays a crucial role in drug
discovery, and deep learning approaches have achieved state-of-the-art
performance in this field. We introduce an ensemble of deep learning models
(EnsembleDLM) for DTI prediction. EnsembleDLM only uses the sequence
information of chemical compounds and proteins, and it aggregates the
predictions from multiple deep neural networks. This approach not only achieves
state-of-the-art performance in Davis and KIBA datasets but also reaches
cutting-edge performance in the cross-domain applications across different
bio-activity types and different protein classes. We also demonstrate that
EnsembleDLM achieves a good performance (Pearson correlation coefficient and
concordance index > 0.8) in the new domain with approximately 50% transfer
learning data, i.e., the training set has twice as much data as the test set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Kao_P/0/1/0/all/0/1"&gt;Po-Yu Kao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kao_S/0/1/0/all/0/1"&gt;Shu-Min Kao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Huang_N/0/1/0/all/0/1"&gt;Nan-Lan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yen-Chu Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confluence: A Robust Non-IoU Alternative to Non-Maxima Suppression in Object Detection. (arXiv:2012.00257v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00257</id>
        <link href="http://arxiv.org/abs/2012.00257"/>
        <updated>2021-07-29T02:00:11.228Z</updated>
        <summary type="html"><![CDATA[Confluence is a novel non-Intersection over Union (IoU) alternative to
Non-Maxima Suppression (NMS) in bounding box post-processing in object
detection. It overcomes the inherent limitations of IoU-based NMS variants to
provide a more stable, consistent predictor of bounding box clustering by using
a normalized Manhattan Distance inspired proximity metric to represent bounding
box clustering. Unlike Greedy and Soft NMS, it does not rely solely on
classification confidence scores to select optimal bounding boxes, instead
selecting the box which is closest to every other box within a given cluster
and removing highly confluent neighboring boxes. Confluence is experimentally
validated on the MS COCO and CrowdHuman benchmarks, improving Average Precision
by up to 2.3-3.8% and Average Recall by up to 5.3-7.2% when compared against
de-facto standard and state of the art NMS variants. Quantitative results are
supported by extensive qualitative analysis and threshold sensitivity analysis
experiments support the conclusion that Confluence is more robust than NMS
variants. Confluence represents a paradigm shift in bounding box processing,
with potential to replace IoU in bounding box regression processes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shepley_A/0/1/0/all/0/1"&gt;Andrew Shepley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Falzon_G/0/1/0/all/0/1"&gt;Greg Falzon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwan_P/0/1/0/all/0/1"&gt;Paul Kwan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Tale Of Two Long Tails. (arXiv:2107.13098v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13098</id>
        <link href="http://arxiv.org/abs/2107.13098"/>
        <updated>2021-07-29T02:00:11.209Z</updated>
        <summary type="html"><![CDATA[As machine learning models are increasingly employed to assist human
decision-makers, it becomes critical to communicate the uncertainty associated
with these model predictions. However, the majority of work on uncertainty has
focused on traditional probabilistic or ranking approaches - where the model
assigns low probabilities or scores to uncertain examples. While this captures
what examples are challenging for the model, it does not capture the underlying
source of the uncertainty. In this work, we seek to identify examples the model
is uncertain about and characterize the source of said uncertainty. We explore
the benefits of designing a targeted intervention - targeted data augmentation
of the examples where the model is uncertain over the course of training. We
investigate whether the rate of learning in the presence of additional
information differs between atypical and noisy examples? Our results show that
this is indeed the case, suggesting that well-designed interventions over the
course of training can be an effective way to characterize and distinguish
between different sources of uncertainty.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dsouza_D/0/1/0/all/0/1"&gt;Daniel D&amp;#x27;souza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nussbaum_Z/0/1/0/all/0/1"&gt;Zach Nussbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1"&gt;Chirag Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1"&gt;Sara Hooker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Know Thyself: Transferable Visuomotor Control Through Robot-Awareness. (arXiv:2107.09047v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.09047</id>
        <link href="http://arxiv.org/abs/2107.09047"/>
        <updated>2021-07-29T02:00:11.202Z</updated>
        <summary type="html"><![CDATA[Training visuomotor robot controllers from scratch on a new robot typically
requires generating large amounts of robot-specific data. Could we leverage
data previously collected on another robot to reduce or even completely remove
this need for robot-specific data? We propose a "robot-aware" solution paradigm
that exploits readily available robot "self-knowledge" such as proprioception,
kinematics, and camera calibration to achieve this. First, we learn modular
dynamics models that pair a transferable, robot-agnostic world dynamics module
with a robot-specific, analytical robot dynamics module. Next, we set up visual
planning costs that draw a distinction between the robot self and the world.
Our experiments on tabletop manipulation tasks in simulation and on real robots
demonstrate that these plug-in improvements dramatically boost the
transferability of visuomotor controllers, even permitting zero-shot transfer
onto new robots for the very first time. Project website:
https://hueds.github.io/rac/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1"&gt;Edward S. Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rybkin_O/0/1/0/all/0/1"&gt;Oleh Rybkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jayaraman_D/0/1/0/all/0/1"&gt;Dinesh Jayaraman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inferring bias and uncertainty in camera calibration. (arXiv:2107.13484v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13484</id>
        <link href="http://arxiv.org/abs/2107.13484"/>
        <updated>2021-07-29T02:00:11.192Z</updated>
        <summary type="html"><![CDATA[Accurate camera calibration is a precondition for many computer vision
applications. Calibration errors, such as wrong model assumptions or imprecise
parameter estimation, can deteriorate a system's overall performance, making
the reliable detection and quantification of these errors critical. In this
work, we introduce an evaluation scheme to capture the fundamental error
sources in camera calibration: systematic errors (biases) and uncertainty
(variance). The proposed bias detection method uncovers smallest systematic
errors and thereby reveals imperfections of the calibration setup and provides
the basis for camera model selection. A novel resampling-based uncertainty
estimator enables uncertainty estimation under non-ideal conditions and thereby
extends the classical covariance estimator. Furthermore, we derive a simple
uncertainty metric that is independent of the camera model. In combination, the
proposed methods can be used to assess the accuracy of individual calibrations,
but also to benchmark new calibration algorithms, camera models, or calibration
setups. We evaluate the proposed methods with simulations and real cameras.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hagemann_A/0/1/0/all/0/1"&gt;Annika Hagemann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knorr_M/0/1/0/all/0/1"&gt;Moritz Knorr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Janssen_H/0/1/0/all/0/1"&gt;Holger Janssen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stiller_C/0/1/0/all/0/1"&gt;Christoph Stiller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Unsupervised Domain Adaptation with Conditional and Label Shift: Infer, Align and Iterate. (arXiv:2107.13469v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13469</id>
        <link href="http://arxiv.org/abs/2107.13469"/>
        <updated>2021-07-29T02:00:11.185Z</updated>
        <summary type="html"><![CDATA[In this work, we propose an adversarial unsupervised domain adaptation (UDA)
approach with the inherent conditional and label shifts, in which we aim to
align the distributions w.r.t. both $p(x|y)$ and $p(y)$. Since the label is
inaccessible in the target domain, the conventional adversarial UDA assumes
$p(y)$ is invariant across domains, and relies on aligning $p(x)$ as an
alternative to the $p(x|y)$ alignment. To address this, we provide a thorough
theoretical and empirical analysis of the conventional adversarial UDA methods
under both conditional and label shifts, and propose a novel and practical
alternative optimization scheme for adversarial UDA. Specifically, we infer the
marginal $p(y)$ and align $p(x|y)$ iteratively in the training, and precisely
align the posterior $p(y|x)$ in testing. Our experimental results demonstrate
its effectiveness on both classification and segmentation UDA, and partial UDA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1"&gt;Zhenhua Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Site Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1"&gt;Fangxu Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1"&gt;Jane You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1"&gt;C.-C. Jay Kuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1"&gt;Georges El Fakhri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1"&gt;Jonghye Woo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Attributed Graph Representations with Communicative Message Passing Transformer. (arXiv:2107.08773v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08773</id>
        <link href="http://arxiv.org/abs/2107.08773"/>
        <updated>2021-07-29T02:00:11.168Z</updated>
        <summary type="html"><![CDATA[Constructing appropriate representations of molecules lies at the core of
numerous tasks such as material science, chemistry and drug designs. Recent
researches abstract molecules as attributed graphs and employ graph neural
networks (GNN) for molecular representation learning, which have made
remarkable achievements in molecular graph modeling. Albeit powerful, current
models either are based on local aggregation operations and thus miss
higher-order graph properties or focus on only node information without fully
using the edge information. For this sake, we propose a Communicative Message
Passing Transformer (CoMPT) neural network to improve the molecular graph
representation by reinforcing message interactions between nodes and edges
based on the Transformer architecture. Unlike the previous transformer-style
GNNs that treat molecules as fully connected graphs, we introduce a message
diffusion mechanism to leverage the graph connectivity inductive bias and
reduce the message enrichment explosion. Extensive experiments demonstrated
that the proposed model obtained superior performances (around 4$\%$ on
average) against state-of-the-art baselines on seven chemical property datasets
(graph-level tasks) and two chemical shift datasets (node-level tasks). Further
visualization studies also indicated a better representation capacity achieved
by our model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jianwen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Shuangjia Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Ying Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1"&gt;Jiahua Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yuedong Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stable deep neural network architectures for mitochondria segmentation on electron microscopy volumes. (arXiv:2104.03577v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03577</id>
        <link href="http://arxiv.org/abs/2104.03577"/>
        <updated>2021-07-29T02:00:11.149Z</updated>
        <summary type="html"><![CDATA[Electron microscopy (EM) allows the identification of intracellular
organelles such as mitochondria, providing insights for clinical and scientific
studies. In recent years, a number of novel deep learning architectures have
been published reporting superior performance, or even human-level accuracy,
compared to previous approaches on public mitochondria segmentation datasets.
Unfortunately, many of these publications do not make neither the code nor the
full training details public to support the results obtained, leading to
reproducibility issues and dubious model comparisons. For that reason, and
following a recent code of best practices for reporting experimental results,
we present an extensive study of the state-of-the-art deep learning
architectures for the segmentation of mitochondria on EM volumes, and evaluate
the impact in performance of different variations of 2D and 3D U-Net-like
models for this task. To better understand the contribution of each component,
a common set of pre- and post-processing operations has been implemented and
tested with each approach. Moreover, an exhaustive sweep of hyperparameters
values for all architectures have been performed and each configuration has
been run multiple times to report the mean and standard deviation values of the
evaluation metrics. Using this methodology, we found very stable architectures
and hyperparameter configurations that consistently obtain state-of-the-art
results in the well-known EPFL Hippocampus mitochondria segmentation dataset.
Furthermore, we have benchmarked our proposed models on two other available
datasets, Lucchi++ and Kasthuri++, where they outperform all previous works.
The code derived from this research and its documentation are publicly
available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Franco_Barranco_D/0/1/0/all/0/1"&gt;Daniel Franco-Barranco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Munoz_Barrutia_A/0/1/0/all/0/1"&gt;Arrate Mu&amp;#xf1;oz-Barrutia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arganda_Carreras_I/0/1/0/all/0/1"&gt;Ignacio Arganda-Carreras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CiwGAN and fiwGAN: Encoding information in acoustic data to model lexical learning with Generative Adversarial Networks. (arXiv:2006.02951v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.02951</id>
        <link href="http://arxiv.org/abs/2006.02951"/>
        <updated>2021-07-29T02:00:11.091Z</updated>
        <summary type="html"><![CDATA[How can deep neural networks encode information that corresponds to words in
human speech into raw acoustic data? This paper proposes two neural network
architectures for modeling unsupervised lexical learning from raw acoustic
inputs, ciwGAN (Categorical InfoWaveGAN) and fiwGAN (Featural InfoWaveGAN),
that combine a Deep Convolutional GAN architecture for audio data (WaveGAN;
arXiv:1705.07904) with an information theoretic extension of GAN -- InfoGAN
(arXiv:1606.03657), and propose a new latent space structure that can model
featural learning simultaneously with a higher level classification and allows
for a very low-dimension vector representation of lexical items. Lexical
learning is modeled as emergent from an architecture that forces a deep neural
network to output data such that unique information is retrievable from its
acoustic outputs. The networks trained on lexical items from TIMIT learn to
encode unique information corresponding to lexical items in the form of
categorical variables in their latent space. By manipulating these variables,
the network outputs specific lexical items. The network occasionally outputs
innovative lexical items that violate training data, but are linguistically
interpretable and highly informative for cognitive modeling and neural network
interpretability. Innovative outputs suggest that phonetic and phonological
representations learned by the network can be productively recombined and
directly paralleled to productivity in human speech: a fiwGAN network trained
on `suit' and `dark' outputs innovative `start', even though it never saw
`start' or even a [st] sequence in the training data. We also argue that
setting latent featural codes to values well beyond training range results in
almost categorical generation of prototypical lexical items and reveals
underlying values of each latent code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1"&gt;Ga&amp;#x161;per Begu&amp;#x161;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flood Segmentation on Sentinel-1 SAR Imagery with Semi-Supervised Learning. (arXiv:2107.08369v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08369</id>
        <link href="http://arxiv.org/abs/2107.08369"/>
        <updated>2021-07-29T02:00:11.083Z</updated>
        <summary type="html"><![CDATA[Floods wreak havoc throughout the world, causing billions of dollars in
damages, and uprooting communities, ecosystems and economies. Accurate and
robust flood detection including delineating open water flood areas and
identifying flood levels can aid in disaster response and mitigation. However,
estimating flood levels remotely is of essence as physical access to flooded
areas is limited and the ability to deploy instruments in potential flood zones
can be dangerous. Aligning flood extent mapping with local topography can
provide a plan-of-action that the disaster response team can consider. Thus,
remote flood level estimation via satellites like Sentinel-1 can prove to be
remedial. The Emerging Techniques in Computational Intelligence (ETCI)
competition on Flood Detection tasked participants with predicting flooded
pixels after training with synthetic aperture radar (SAR) images in a
supervised setting. We use a cyclical approach involving two stages (1)
training an ensemble model of multiple UNet architectures with available high
and low confidence labeled data and, (2) generating pseudo labels or low
confidence labels on the unlabeled test dataset, and then, combining the
generated labels with the previously available high confidence labeled dataset.
This assimilated dataset is used for the next round of training ensemble
models. This cyclical process is repeated until the performance improvement
plateaus. Additionally, we post process our results with Conditional Random
Fields. Our approach sets a high score on the public leaderboard for the ETCI
competition with 0.7654 IoU. Our method, which we release with all the code
including trained models, can also be used as an open science benchmark for the
Sentinel-1 released dataset on GitHub. To the best of our knowledge we believe
this the first works to try out semi-supervised learning to improve flood
segmentation models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1"&gt;Sayak Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1"&gt;Siddha Ganju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On automatic extraction of on-street parking spaces using park-out events data. (arXiv:2102.06758v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06758</id>
        <link href="http://arxiv.org/abs/2102.06758"/>
        <updated>2021-07-29T02:00:11.075Z</updated>
        <summary type="html"><![CDATA[This article proposes two different approaches to automatically create a map
for valid on-street car parking spaces. For this, we use car sharing park-out
events data. The first one uses spatial aggregation and the second a machine
learning algorithm. For the former, we chose rasterization and road sectioning;
for the latter we chose decision trees. We compare the results of these
approaches and discuss their advantages and disadvantages. Furthermore, we show
our results for a neighborhood in the city of Berlin and report a
classification accuracy of 91.6\% on the original imbalanced data. Finally, we
discuss further work; from gathering more data over a longer period of time to
fitting spatial Gaussian densities to the data and the usage of apps for manual
validation and annotation of parking spaces to improve ground truth data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Navarro_B_J/0/1/0/all/0/1"&gt;J.-Emeterio Navarro-B&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gebert_M/0/1/0/all/0/1"&gt;Martin Gebert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bielig_R/0/1/0/all/0/1"&gt;Ralf Bielig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximate Bayesian Computation for an Explicit-Duration Hidden Markov Model of COVID-19 Hospital Trajectories. (arXiv:2105.00773v2 [stat.AP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00773</id>
        <link href="http://arxiv.org/abs/2105.00773"/>
        <updated>2021-07-29T02:00:11.068Z</updated>
        <summary type="html"><![CDATA[We address the problem of modeling constrained hospital resources in the
midst of the COVID-19 pandemic in order to inform decision-makers of future
demand and assess the societal value of possible interventions. For broad
applicability, we focus on the common yet challenging scenario where
patient-level data for a region of interest are not available. Instead, given
daily admissions counts, we model aggregated counts of observed resource use,
such as the number of patients in the general ward, in the intensive care unit,
or on a ventilator. In order to explain how individual patient trajectories
produce these counts, we propose an aggregate count explicit-duration hidden
Markov model, nicknamed the ACED-HMM, with an interpretable, compact
parameterization. We develop an Approximate Bayesian Computation approach that
draws samples from the posterior distribution over the model's transition and
duration parameters given aggregate counts from a specific location, thus
adapting the model to a region or individual hospital site of interest. Samples
from this posterior can then be used to produce future forecasts of any counts
of interest. Using data from the United States and the United Kingdom, we show
our mechanistic approach provides competitive probabilistic forecasts for the
future even as the dynamics of the pandemic shift. Furthermore, we show how our
model provides insight about recovery probabilities or length of stay
distributions, and we suggest its potential to answer challenging what-if
questions about the societal value of possible interventions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Visani_G/0/1/0/all/0/1"&gt;Gian Marco Visani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lee_A/0/1/0/all/0/1"&gt;Alexandra Hope Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nguyen_C/0/1/0/all/0/1"&gt;Cuong Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kent_D/0/1/0/all/0/1"&gt;David M. Kent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wong_J/0/1/0/all/0/1"&gt;John B. Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cohen_J/0/1/0/all/0/1"&gt;Joshua T. Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hughes_M/0/1/0/all/0/1"&gt;Michael C. Hughes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Partial Recovery in the Graph Alignment Problem. (arXiv:2007.00533v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.00533</id>
        <link href="http://arxiv.org/abs/2007.00533"/>
        <updated>2021-07-29T02:00:11.061Z</updated>
        <summary type="html"><![CDATA[In this paper, we consider the graph alignment problem, which is the problem
of recovering, given two graphs, a one-to-one mapping between nodes that
maximizes edge overlap. This problem can be viewed as a noisy version of the
well-known graph isomorphism problem and appears in many applications,
including social network deanonymization and cellular biology. Our focus here
is on partial recovery, i.e., we look for a one-to-one mapping which is correct
on a fraction of the nodes of the graph rather than on all of them, and we
assume that the two input graphs to the problem are correlated
Erd\H{o}s-R\'enyi graphs of parameters $(n,q,s)$. Our main contribution is then
to give necessary and sufficient conditions on $(n,q,s)$ under which partial
recovery is possible with high probability as the number of nodes $n$ goes to
infinity. In particular, we show that it is possible to achieve partial
recovery in the $nqs=\Theta(1)$ regime under certain additional assumptions. An
interesting byproduct of the analysis techniques we develop to obtain the
sufficiency result in the partial recovery setting is a tighter analysis of the
maximum likelihood estimator for the graph alignment problem, which leads to
improved sufficient conditions for exact recovery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hall_G/0/1/0/all/0/1"&gt;Georgina Hall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Massoulie_L/0/1/0/all/0/1"&gt;Laurent Massouli&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gym-$\mu$RTS: Toward Affordable Full Game Real-time Strategy Games Research with Deep Reinforcement Learning. (arXiv:2105.13807v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13807</id>
        <link href="http://arxiv.org/abs/2105.13807"/>
        <updated>2021-07-29T02:00:11.038Z</updated>
        <summary type="html"><![CDATA[In recent years, researchers have achieved great success in applying Deep
Reinforcement Learning (DRL) algorithms to Real-time Strategy (RTS) games,
creating strong autonomous agents that could defeat professional players in
StarCraft~II. However, existing approaches to tackle full games have high
computational costs, usually requiring the use of thousands of GPUs and CPUs
for weeks. This paper has two main contributions to address this issue: 1) We
introduce Gym-$\mu$RTS (pronounced "gym-micro-RTS") as a fast-to-run RL
environment for full-game RTS research and 2) we present a collection of
techniques to scale DRL to play full-game $\mu$RTS as well as ablation studies
to demonstrate their empirical importance. Our best-trained bot can defeat
every $\mu$RTS bot we tested from the past $\mu$RTS competitions when working
in a single-map setting, resulting in a state-of-the-art DRL agent while only
taking about 60 hours of training using a single machine (one GPU, three vCPU,
16GB RAM). See the blog post at
https://wandb.ai/vwxyzjn/gym-microrts-paper/reports/Gym-RTS-Toward-Affordable-Deep-Reinforcement-Learning-Research-in-Real-Time-Strategy-Games--Vmlldzo2MDIzMTg
and the source code at https://github.com/vwxyzjn/gym-microrts-paper]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shengyi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1"&gt;Santiago Onta&amp;#xf1;&amp;#xf3;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bamford_C/0/1/0/all/0/1"&gt;Chris Bamford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grela_L/0/1/0/all/0/1"&gt;Lukasz Grela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consensus-Based Optimization on the Sphere: Convergence to Global Minimizers and Machine Learning. (arXiv:2001.11988v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.11988</id>
        <link href="http://arxiv.org/abs/2001.11988"/>
        <updated>2021-07-29T02:00:11.020Z</updated>
        <summary type="html"><![CDATA[We investigate the implementation of a new stochastic Kuramoto-Vicsek-type
model for global optimization of nonconvex functions on the sphere. This model
belongs to the class of Consensus-Based Optimization. In fact, particles move
on the sphere driven by a drift towards an instantaneous consensus point, which
is computed as a convex combination of particle locations, weighted by the cost
function according to Laplace's principle, and it represents an approximation
to a global minimizer. The dynamics is further perturbed by a random vector
field to favor exploration, whose variance is a function of the distance of the
particles to the consensus point. In particular, as soon as the consensus is
reached the stochastic component vanishes. The main results of this paper are
about the proof of convergence of the numerical scheme to global minimizers
provided conditions of well-preparation of the initial datum. The proof
combines previous results of mean-field limit with a novel asymptotic analysis,
and classical convergence results of numerical methods for SDE. We present
several numerical experiments, which show that the algorithm proposed in the
present paper scales well with the dimension and is extremely versatile. To
quantify the performances of the new approach, we show that the algorithm is
able to perform essentially as good as ad hoc state of the art methods in
challenging problems in signal processing and machine learning, namely the
phase retrieval problem and the robust subspace detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fornasier_M/0/1/0/all/0/1"&gt;Massimo Fornasier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hui Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pareschi_L/0/1/0/all/0/1"&gt;Lorenzo Pareschi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sunnen_P/0/1/0/all/0/1"&gt;Philippe S&amp;#xfc;nnen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Defensive Approximation: Enhancing CNNs Security through Approximate Computing. (arXiv:2006.07700v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07700</id>
        <link href="http://arxiv.org/abs/2006.07700"/>
        <updated>2021-07-29T02:00:11.003Z</updated>
        <summary type="html"><![CDATA[In the past few years, an increasing number of machine-learning and deep
learning structures, such as Convolutional Neural Networks (CNNs), have been
applied to solving a wide range of real-life problems. However, these
architectures are vulnerable to adversarial attacks. In this paper, we propose
for the first time to use hardware-supported approximate computing to improve
the robustness of machine learning classifiers. We show that our approximate
computing implementation achieves robustness across a wide range of attack
scenarios. Specifically, for black-box and grey-box attack scenarios, we show
that successful adversarial attacks against the exact classifier have poor
transferability to the approximate implementation. Surprisingly, the robustness
advantages also apply to white-box attacks where the attacker has access to the
internal implementation of the approximate classifier. We explain some of the
possible reasons for this robustness through analysis of the internal operation
of the approximate implementation. Furthermore, our approximate computing model
maintains the same level in terms of classification accuracy, does not require
retraining, and reduces resource utilization and energy consumption of the CNN.
We conducted extensive experiments on a set of strong adversarial attacks; We
empirically show that the proposed implementation increases the robustness of a
LeNet-5 and an Alexnet CNNs by up to 99% and 87%, respectively for strong
grey-box adversarial attacks along with up to 67% saving in energy consumption
due to the simpler nature of the approximate logic. We also show that a
white-box attack requires a remarkably higher noise budget to fool the
approximate classifier, causing an average of 4db degradation of the PSNR of
the input image relative to the images that succeed in fooling the exact
classifier]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guesmi_A/0/1/0/all/0/1"&gt;Amira Guesmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alouani_I/0/1/0/all/0/1"&gt;Ihsen Alouani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khasawneh_K/0/1/0/all/0/1"&gt;Khaled Khasawneh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baklouti_M/0/1/0/all/0/1"&gt;Mouna Baklouti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frikha_T/0/1/0/all/0/1"&gt;Tarek Frikha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abid_M/0/1/0/all/0/1"&gt;Mohamed Abid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abu_Ghazaleh_N/0/1/0/all/0/1"&gt;Nael Abu-Ghazaleh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring the Limits of Out-of-Distribution Detection. (arXiv:2106.03004v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03004</id>
        <link href="http://arxiv.org/abs/2106.03004"/>
        <updated>2021-07-29T02:00:10.994Z</updated>
        <summary type="html"><![CDATA[Near out-of-distribution detection (OOD) is a major challenge for deep neural
networks. We demonstrate that large-scale pre-trained transformers can
significantly improve the state-of-the-art (SOTA) on a range of near OOD tasks
across different data modalities. For instance, on CIFAR-100 vs CIFAR-10 OOD
detection, we improve the AUROC from 85% (current SOTA) to more than 96% using
Vision Transformers pre-trained on ImageNet-21k. On a challenging genomics OOD
detection benchmark, we improve the AUROC from 66% to 77% using transformers
and unsupervised pre-training. To further improve performance, we explore the
few-shot outlier exposure setting where a few examples from outlier classes may
be available; we show that pre-trained transformers are particularly
well-suited for outlier exposure, and that the AUROC of OOD detection on
CIFAR-100 vs CIFAR-10 can be improved to 98.7% with just 1 image per OOD class,
and 99.46% with 10 images per OOD class. For multi-modal image-text pre-trained
transformers such as CLIP, we explore a new way of using just the names of
outlier classes as a sole source of information without any accompanying
images, and show that this outperforms previous SOTA on standard vision OOD
benchmark tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1"&gt;Stanislav Fort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jie Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1"&gt;Balaji Lakshminarayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Long-Short Transformer: Efficient Transformers for Language and Vision. (arXiv:2107.02192v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02192</id>
        <link href="http://arxiv.org/abs/2107.02192"/>
        <updated>2021-07-29T02:00:10.967Z</updated>
        <summary type="html"><![CDATA[Transformers have achieved success in both language and vision domains.
However, it is prohibitively expensive to scale them to long sequences such as
long documents or high-resolution images, because self-attention mechanism has
quadratic time and memory complexities with respect to the input sequence
length. In this paper, we propose Long-Short Transformer (Transformer-LS), an
efficient self-attention mechanism for modeling long sequences with linear
complexity for both language and vision tasks. It aggregates a novel long-range
attention with dynamic projection to model distant correlations and a
short-term attention to capture fine-grained local correlations. We propose a
dual normalization strategy to account for the scale mismatch between the two
attention mechanisms. Transformer-LS can be applied to both autoregressive and
bidirectional models without additional complexity. Our method outperforms the
state-of-the-art models on multiple tasks in language and vision domains,
including the Long Range Arena benchmark, autoregressive language modeling, and
ImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on
enwik8 using half the number of parameters than previous method, while being
faster and is able to handle 3x as long sequences compared to its
full-attention version on the same hardware. On ImageNet, it can obtain the
state-of-the-art results (e.g., a moderate size of 55.8M model solely trained
on 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more
scalable on high-resolution images. The source code and models are released at
https://github.com/NVIDIA/transformer-ls .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1"&gt;Chen Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1"&gt;Wei Ping&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Chaowei Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1"&gt;Mohammad Shoeybi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1"&gt;Tom Goldstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1"&gt;Bryan Catanzaro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A novel Time-frequency Transformer and its Application in Fault Diagnosis of Rolling Bearings. (arXiv:2104.09079v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09079</id>
        <link href="http://arxiv.org/abs/2104.09079"/>
        <updated>2021-07-29T02:00:10.915Z</updated>
        <summary type="html"><![CDATA[The scope of data-driven fault diagnosis models is greatly improved through
deep learning (DL). However, the classical convolution and recurrent structure
have their defects in computational efficiency and feature representation,
while the latest Transformer architecture based on attention mechanism has not
been applied in this field. To solve these problems, we propose a novel
time-frequency Transformer (TFT) model inspired by the massive success of
standard Transformer in sequence processing. Specially, we design a fresh
tokenizer and encoder module to extract effective abstractions from the
time-frequency representation (TFR) of vibration signals. On this basis, a new
end-to-end fault diagnosis framework based on time-frequency Transformer is
presented in this paper. Through the case studies on bearing experimental
datasets, we constructed the optimal Transformer structure and verified the
performance of the diagnostic method. The superiority of the proposed method is
demonstrated in comparison with the benchmark model and other state-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yifei Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1"&gt;Minping Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_Q/0/1/0/all/0/1"&gt;Qiuhua Miao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yudong Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Molecular Embeddings in QSAR Modeling: Does it Make a Difference?. (arXiv:2104.02604v2 [q-bio.BM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02604</id>
        <link href="http://arxiv.org/abs/2104.02604"/>
        <updated>2021-07-29T02:00:10.908Z</updated>
        <summary type="html"><![CDATA[With the consolidation of deep learning in drug discovery, several novel
algorithms for learning molecular representations have been proposed. Despite
the interest of the community in developing new methods for learning molecular
embeddings and their theoretical benefits, comparing molecular embeddings with
each other and with traditional representations is not straightforward, which
in turn hinders the process of choosing a suitable representation for QSAR
modeling. A reason behind this issue is the difficulty of conducting a fair and
thorough comparison of the different existing embedding approaches, which
requires numerous experiments on various datasets and training scenarios. To
close this gap, we reviewed the literature on methods for molecular embeddings
and reproduced three unsupervised and two supervised molecular embedding
techniques recently proposed in the literature. We compared these five methods
concerning their performance in QSAR scenarios using different classification
and regression datasets. We also compared these representations to traditional
molecular representations, namely molecular descriptors and fingerprints. As
opposed to the expected outcome, our experimental setup consisting of over
25,000 trained models and statistical tests revealed that the predictive
performance using molecular embeddings did not significantly surpass that of
traditional representations. While supervised embeddings yielded competitive
results compared to those using traditional molecular representations,
unsupervised embeddings tended to perform worse than traditional
representations. Our results highlight the need for conducting a careful
comparison and analysis of the different embedding techniques prior to using
them in drug design tasks, and motivate a discussion about the potential of
molecular embeddings in computer-aided drug design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Sabando_M/0/1/0/all/0/1"&gt;Mar&amp;#xed;a Virginia Sabando&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ponzoni_I/0/1/0/all/0/1"&gt;Ignacio Ponzoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Milios_E/0/1/0/all/0/1"&gt;Evangelos E. Milios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Soto_A/0/1/0/all/0/1"&gt;Axel J. Soto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Certifiable Machine Unlearning for Linear Models. (arXiv:2106.15093v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15093</id>
        <link href="http://arxiv.org/abs/2106.15093"/>
        <updated>2021-07-29T02:00:10.886Z</updated>
        <summary type="html"><![CDATA[Machine unlearning is the task of updating machine learning (ML) models after
a subset of the training data they were trained on is deleted. Methods for the
task are desired to combine effectiveness and efficiency, i.e., they should
effectively "unlearn" deleted data, but in a way that does not require
excessive computation effort (e.g., a full retraining) for a small amount of
deletions. Such a combination is typically achieved by tolerating some amount
of approximation in the unlearning. In addition, laws and regulations in the
spirit of "the right to be forgotten" have given rise to requirements for
certifiability, i.e., the ability to demonstrate that the deleted data has
indeed been unlearned by the ML model.

In this paper, we present an experimental study of the three state-of-the-art
approximate unlearning methods for linear models and demonstrate the trade-offs
between efficiency, effectiveness and certifiability offered by each method. In
implementing the study, we extend some of the existing works and describe a
common ML pipeline to compare and evaluate the unlearning methods on six
real-world datasets and a variety of settings. We provide insights into the
effect of the quantity and distribution of the deleted data on ML models and
the performance of each unlearning method in different settings. We also
propose a practical online strategy to determine when the accumulated error
from approximate unlearning is large enough to warrant a full retrain of the ML
model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahadevan_A/0/1/0/all/0/1"&gt;Ananth Mahadevan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathioudakis_M/0/1/0/all/0/1"&gt;Michael Mathioudakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A method to integrate and classify normal distributions. (arXiv:2012.14331v7 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14331</id>
        <link href="http://arxiv.org/abs/2012.14331"/>
        <updated>2021-07-29T02:00:10.854Z</updated>
        <summary type="html"><![CDATA[Univariate and multivariate normal probability distributions are widely used
when modeling decisions under uncertainty. Computing the performance of such
models requires integrating these distributions over specific domains, which
can vary widely across models. Besides some special cases where these integrals
are easy to calculate, there exist no general analytical expressions, standard
numerical methods or software for these integrals. Here we present mathematical
results and open-source software that provide (i) the probability in any domain
of a normal in any dimensions with any parameters, (ii) the probability
density, cumulative distribution, and inverse cumulative distribution of any
function of a normal vector, (iii) the classification errors among any number
of normal distributions, the Bayes-optimal discriminability index and relation
to the operating characteristic, (iv) dimension reduction and visualizations
for such problems, and (v) tests for how reliably these methods may be used on
given data. We demonstrate these tools with vision research applications of
detecting occluding objects in natural scenes, and detecting camouflage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Das_A/0/1/0/all/0/1"&gt;Abhranil Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Geisler_W/0/1/0/all/0/1"&gt;Wilson S Geisler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking ResNets: Improved Stacking Strategies With High Order Schemes. (arXiv:2103.15244v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15244</id>
        <link href="http://arxiv.org/abs/2103.15244"/>
        <updated>2021-07-29T02:00:10.846Z</updated>
        <summary type="html"><![CDATA[Various deep neural network architectures (DNNs) maintain massive vital
records in computer vision. While drawing attention worldwide, the design of
the overall structure lacks general guidance. Based on the relationship between
DNN design and numerical differential equations, we performed a fair comparison
of the residual design with higher-order perspectives. We show that the widely
used DNN design strategy, constantly stacking a small design (usually 2-3
layers), could be easily improved, supported by solid theoretical knowledge and
with no extra parameters needed. We reorganise the residual design in
higher-order ways, which is inspired by the observation that many effective
networks can be interpreted as different numerical discretisations of
differential equations. The design of ResNet follows a relatively simple
scheme, which is Euler forward; however, the situation becomes complicated
rapidly while stacking. We suppose that stacked ResNet is somehow equalled to a
higher-order scheme; then, the current method of forwarding propagation might
be relatively weak compared with a typical high-order method such as
Runge-Kutta. We propose HO-ResNet to verify the hypothesis of widely used CV
benchmarks with sufficient experiments. Stable and noticeable increases in
performance are observed, and convergence and robustness are also improved. Our
stacking strategy improved ResNet-30 by 2.15 per cent and ResNet-58 by 2.35 per
cent on CIFAR-10, with the same settings and parameters. The proposed strategy
is fundamental and theoretical and can therefore be applied to any network as a
general guideline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1"&gt;Zhengbo Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zitang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Weilian Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zizhang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamata_S/0/1/0/all/0/1"&gt;Sei-ichiro Kamata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Understanding Learning in Neural Networks with Linear Teachers. (arXiv:2101.02533v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02533</id>
        <link href="http://arxiv.org/abs/2101.02533"/>
        <updated>2021-07-29T02:00:10.759Z</updated>
        <summary type="html"><![CDATA[Can a neural network minimizing cross-entropy learn linearly separable data?
Despite progress in the theory of deep learning, this question remains
unsolved. Here we prove that SGD globally optimizes this learning problem for a
two-layer network with Leaky ReLU activations. The learned network can in
principle be very complex. However, empirical evidence suggests that it often
turns out to be approximately linear. We provide theoretical support for this
phenomenon by proving that if network weights converge to two weight clusters,
this will imply an approximately linear decision boundary. Finally, we show a
condition on the optimization that leads to weight clustering. We provide
empirical results that validate our theoretical analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sarussi_R/0/1/0/all/0/1"&gt;Roei Sarussi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brutzkus_A/0/1/0/all/0/1"&gt;Alon Brutzkus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1"&gt;Amir Globerson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stronger Privacy for Federated Collaborative Filtering with Implicit Feedback. (arXiv:2105.03941v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03941</id>
        <link href="http://arxiv.org/abs/2105.03941"/>
        <updated>2021-07-29T02:00:10.747Z</updated>
        <summary type="html"><![CDATA[Recommender systems are commonly trained on centrally collected user
interaction data like views or clicks. This practice however raises serious
privacy concerns regarding the recommender's collection and handling of
potentially sensitive data. Several privacy-aware recommender systems have been
proposed in recent literature, but comparatively little attention has been
given to systems at the intersection of implicit feedback and privacy. To
address this shortcoming, we propose a practical federated recommender system
for implicit data under user-level local differential privacy (LDP). The
privacy-utility trade-off is controlled by parameters $\epsilon$ and $k$,
regulating the per-update privacy budget and the number of $\epsilon$-LDP
gradient updates sent by each user respectively. To further protect the user's
privacy, we introduce a proxy network to reduce the fingerprinting surface by
anonymizing and shuffling the reports before forwarding them to the
recommender. We empirically demonstrate the effectiveness of our framework on
the MovieLens dataset, achieving up to Hit Ratio with K=10 (HR@10) 0.68 on 50k
users with 5k items. Even on the full dataset, we show that it is possible to
achieve reasonable utility with HR@10>0.5 without compromising user privacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Minto_L/0/1/0/all/0/1"&gt;Lorenzo Minto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haller_M/0/1/0/all/0/1"&gt;Moritz Haller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haddadi_H/0/1/0/all/0/1"&gt;Hamed Haddadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Livshits_B/0/1/0/all/0/1"&gt;Benjamin Livshits&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning for Decision-Making and Control in Power Systems: Tutorial, Review, and Vision. (arXiv:2102.01168v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01168</id>
        <link href="http://arxiv.org/abs/2102.01168"/>
        <updated>2021-07-29T02:00:10.704Z</updated>
        <summary type="html"><![CDATA[With large-scale integration of renewable generation and distributed energy
resources (DERs), modern power systems are confronted with new operational
challenges, such as growing complexity, increasing uncertainty, and aggravating
volatility. Meanwhile, more and more data are becoming available owing to the
widespread deployment of smart meters, smart sensors, and upgraded
communication networks. As a result, data-driven control techniques, especially
reinforcement learning (RL), have attracted surging attention in recent years.
In this paper, we provide a tutorial on various RL techniques and how they can
be applied to decision-making in power systems. We illustrate RL-based models
and solutions in three key applications, frequency regulation, voltage control,
and energy management. We conclude with three critical issues in the
application of RL, i.e., safety, scalability, and data. Several potential
future directions are discussed as well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_G/0/1/0/all/0/1"&gt;Guannan Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yujie Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Low_S/0/1/0/all/0/1"&gt;Steven Low&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1"&gt;Na Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SPINN: Sparse, Physics-based, and partially Interpretable Neural Networks for PDEs. (arXiv:2102.13037v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13037</id>
        <link href="http://arxiv.org/abs/2102.13037"/>
        <updated>2021-07-29T02:00:10.684Z</updated>
        <summary type="html"><![CDATA[We introduce a class of Sparse, Physics-based, and partially Interpretable
Neural Networks (SPINN) for solving ordinary and partial differential equations
(PDEs). By reinterpreting a traditional meshless representation of solutions of
PDEs we develop a class of sparse neural network architectures that are
partially interpretable. The SPINN model we propose here serves as a seamless
bridge between two extreme modeling tools for PDEs, namely dense neural network
based methods like Physics Informed Neural Networks (PINNs) and traditional
mesh-free numerical methods, thereby providing a novel means to develop a new
class of hybrid algorithms that build on the best of both these viewpoints. A
unique feature of the SPINN model that distinguishes it from other neural
network based approximations proposed earlier is that it is (i) interpretable,
in a particular sense made precise in the work, and (ii) sparse in the sense
that it has much fewer connections than typical dense neural networks used for
PDEs. Further, the SPINN algorithm implicitly encodes mesh adaptivity and is
able to handle discontinuities in the solutions. In addition, we demonstrate
that Fourier series representations can also be expressed as a special class of
SPINN and propose generalized neural network analogues of Fourier
representations. We illustrate the utility of the proposed method with a
variety of examples involving ordinary differential equations, elliptic,
parabolic, hyperbolic and nonlinear partial differential equations, and an
example in fluid dynamics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ramabathiran_A/0/1/0/all/0/1"&gt;Amuthan A. Ramabathiran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramachandran_P/0/1/0/all/0/1"&gt;Prabhu Ramachandran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning based CVD Virtual Metrology in Mass Produced Semiconductor Process. (arXiv:2107.05071v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05071</id>
        <link href="http://arxiv.org/abs/2107.05071"/>
        <updated>2021-07-29T02:00:10.616Z</updated>
        <summary type="html"><![CDATA[A cross-benchmark has been done on three critical aspects, data imputing,
feature selection and regression algorithms, for machine learning based
chemical vapor deposition (CVD) virtual metrology (VM). The result reveals that
linear feature selection regression algorithm would extensively under-fit the
VM data. Data imputing is also necessary to achieve a higher prediction
accuracy as the data availability is only ~70% when optimal accuracy is
obtained. This work suggests a nonlinear feature selection and regression
algorithm combined with nearest data imputing algorithm would provide a
prediction accuracy as high as 0.7. This would lead to 70% reduced CVD
processing variation, which is believed to will lead to reduced frequency of
physical metrology as well as more reliable mass-produced wafer with improved
quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yunsong Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stearrett_R/0/1/0/all/0/1"&gt;Ryan Stearrett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classifying Textual Data with Pre-trained Vision Models through Transfer Learning and Data Transformations. (arXiv:2106.12479v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12479</id>
        <link href="http://arxiv.org/abs/2106.12479"/>
        <updated>2021-07-29T02:00:10.346Z</updated>
        <summary type="html"><![CDATA[Knowledge is acquired by humans through experience, and no boundary is set
between the kinds of knowledge or skill levels we can achieve on different
tasks at the same time. When it comes to Neural Networks, that is not the case,
the major breakthroughs in the field are extremely task and domain specific.
Vision and language are dealt with in separate manners, using separate methods
and different datasets. In this work, we propose to use knowledge acquired by
benchmark Vision Models which are trained on ImageNet to help a much smaller
architecture learn to classify text. After transforming the textual data
contained in the IMDB dataset to gray scale images. An analysis of different
domains and the Transfer Learning method is carried out. Despite the challenge
posed by the very different datasets, promising results are achieved. The main
contribution of this work is a novel approach which links large pretrained
models on both language and vision to achieve state-of-the-art results in
different sub-fields from the original task. Without needing high compute
capacity resources. Specifically, Sentiment Analysis is achieved after
transferring knowledge between vision and language models. BERT embeddings are
transformed into grayscale images, these images are then used as training
examples for pre-trained vision models such as VGG16 and ResNet

Index Terms: BERT, Convolutional Neural Networks, Domain Adaptation, image
classification, Natural Language Processing, t-SNE, text classification,
Transfer Learning]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Benarab_C/0/1/0/all/0/1"&gt;Charaf Eddine Benarab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Board Volcanic Eruption Detection through CNNs and Satellite Multispectral Imagery. (arXiv:2106.15281v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15281</id>
        <link href="http://arxiv.org/abs/2106.15281"/>
        <updated>2021-07-29T02:00:10.339Z</updated>
        <summary type="html"><![CDATA[In recent years, the growth of Machine Learning (ML) algorithms has raised
the number of studies including their applicability in a variety of different
scenarios. Among all, one of the hardest ones is the aerospace, due to its
peculiar physical requirements. In this context, a feasibility study and a
first prototype for an Artificial Intelligence (AI) model to be deployed on
board satellites are presented in this work. As a case study, the detection of
volcanic eruptions has been investigated as a method to swiftly produce alerts
and allow immediate interventions. Two Convolutional Neural Networks (CNNs)
have been proposed and designed, showing how to efficiently implement them for
identifying the eruptions and at the same time adapting their complexity in
order to fit on board requirements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosso_M/0/1/0/all/0/1"&gt;Maria Pia Del Rosso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebastianelli_A/0/1/0/all/0/1"&gt;Alessandro Sebastianelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spiller_D/0/1/0/all/0/1"&gt;Dario Spiller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathieu_P/0/1/0/all/0/1"&gt;Pierre Philippe Mathieu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullo_S/0/1/0/all/0/1"&gt;Silvia Liberata Ullo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying social organization and political polarization in online platforms. (arXiv:2010.00590v3 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.00590</id>
        <link href="http://arxiv.org/abs/2010.00590"/>
        <updated>2021-07-29T02:00:10.318Z</updated>
        <summary type="html"><![CDATA[Optimism about the Internet's potential to bring the world together has been
tempered by concerns about its role in inflaming the 'culture wars'. Via mass
selection into like-minded groups, online society may be becoming more
fragmented and polarized, particularly with respect to partisan differences.
However, our ability to measure the social makeup of online communities, and in
turn understand the social organization of online platforms, is limited by the
pseudonymous, unstructured, and large-scale nature of digital discussion. We
develop a neural embedding methodology to quantify the positioning of online
communities along social dimensions by leveraging large-scale patterns of
aggregate behaviour. Applying our methodology to 5.1B Reddit comments made in
10K communities over 14 years, we measure how the macroscale community
structure is organized with respect to age, gender, and U.S. political
partisanship. Examining political content, we find Reddit underwent a
significant polarization event around the 2016 U.S. presidential election, and
remained highly polarized for years afterward. Contrary to conventional wisdom,
however, individual-level polarization is rare; the system-level shift in 2016
was disproportionately driven by the arrival of new and newly political users.
Political polarization on Reddit is unrelated to previous activity on the
platform, and is instead temporally aligned with external events. We also
observe a stark ideological asymmetry, with the sharp increase in 2016 being
entirely attributable to changes in right-wing activity. Our methodology is
broadly applicable to the study of online interaction, and our findings have
implications for the design of online platforms, understanding the social
contexts of online behaviour, and quantifying the dynamics and mechanisms of
online polarization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Waller_I/0/1/0/all/0/1"&gt;Isaac Waller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_A/0/1/0/all/0/1"&gt;Ashton Anderson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Process Model Forecasting Using Time Series Analysis of Event Sequence Data. (arXiv:2105.01092v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01092</id>
        <link href="http://arxiv.org/abs/2105.01092"/>
        <updated>2021-07-29T02:00:10.311Z</updated>
        <summary type="html"><![CDATA[Process analytics is an umbrella of data-driven techniques which includes
making predictions for individual process instances or overall process models.
At the instance level, various novel techniques have been recently devised,
tackling next activity, remaining time, and outcome prediction. At the model
level, there is a notable void. It is the ambition of this paper to fill this
gap. To this end, we develop a technique to forecast the entire process model
from historical event data. A forecasted model is a will-be process model
representing a probable future state of the overall process. Such a forecast
helps to investigate the consequences of drift and emerging bottlenecks. Our
technique builds on a representation of event data as multiple time series,
each capturing the evolution of a behavioural aspect of the process model, such
that corresponding forecasting techniques can be applied. Our implementation
demonstrates the accuracy of our technique on real-world event log data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smedt_J/0/1/0/all/0/1"&gt;Johannes De Smedt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeshchenko_A/0/1/0/all/0/1"&gt;Anton Yeshchenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polyvyanyy_A/0/1/0/all/0/1"&gt;Artem Polyvyanyy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weerdt_J/0/1/0/all/0/1"&gt;Jochen De Weerdt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mendling_J/0/1/0/all/0/1"&gt;Jan Mendling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local and non-local dependency learning and emergence of rule-like representations in speech data by Deep Convolutional Generative Adversarial Networks. (arXiv:2009.12711v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.12711</id>
        <link href="http://arxiv.org/abs/2009.12711"/>
        <updated>2021-07-29T02:00:10.300Z</updated>
        <summary type="html"><![CDATA[This paper argues that training GANs on local and non-local dependencies in
speech data offers insights into how deep neural networks discretize continuous
data and how symbolic-like rule-based morphophonological processes emerge in a
deep convolutional architecture. Acquisition of speech has recently been
modeled as a dependency between latent space and data generated by GANs in
Begu\v{s} (2020b; arXiv:2006.03965), who models learning of a simple local
allophonic distribution. We extend this approach to test learning of local and
non-local phonological processes that include approximations of morphological
processes. We further parallel outputs of the model to results of a behavioral
experiment where human subjects are trained on the data used for training the
GAN network. Four main conclusions emerge: (i) the networks provide useful
information for computational models of speech acquisition even if trained on a
comparatively small dataset of an artificial grammar learning experiment; (ii)
local processes are easier to learn than non-local processes, which matches
both behavioral data in human subjects and typology in the world's languages.
This paper also proposes (iii) how we can actively observe the network's
progress in learning and explore the effect of training steps on learning
representations by keeping latent space constant across different training
steps. Finally, this paper shows that (iv) the network learns to encode the
presence of a prefix with a single latent variable; by interpolating this
variable, we can actively observe the operation of a non-local phonological
process. The proposed technique for retrieving learning representations has
general implications for our understanding of how GANs discretize continuous
speech data and suggests that rule-like generalizations in the training data
are represented as an interaction between variables in the network's latent
space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1"&gt;Ga&amp;#x161;per Begu&amp;#x161;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Entropic alternatives to initialization. (arXiv:2107.07757v2 [cond-mat.dis-nn] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.07757</id>
        <link href="http://arxiv.org/abs/2107.07757"/>
        <updated>2021-07-29T02:00:10.291Z</updated>
        <summary type="html"><![CDATA[Local entropic loss functions provide a versatile framework to define
architecture-aware regularization procedures. Besides the possibility of being
anisotropic in the synaptic space, the local entropic smoothening of the loss
function can vary during training, thus yielding a tunable model complexity. A
scoping protocol where the regularization is strong in the early-stage of the
training and then fades progressively away constitutes an alternative to
standard initialization procedures for deep convolutional neural networks,
nonetheless, it has wider applicability. We analyze anisotropic, local entropic
smoothenings in the language of statistical physics and information theory,
providing insight into both their interpretation and workings. We comment some
aspects related to the physics of renormalization and the spacetime structure
of convolutional networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Musso_D/0/1/0/all/0/1"&gt;Daniele Musso&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Realistic River Image Synthesis using Deep Generative Adversarial Networks. (arXiv:2003.00826v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.00826</id>
        <link href="http://arxiv.org/abs/2003.00826"/>
        <updated>2021-07-29T02:00:10.284Z</updated>
        <summary type="html"><![CDATA[In this paper, we demonstrated a practical application of realistic river
image generation using deep learning. Specifically, we explored a generative
adversarial network (GAN) model capable of generating high-resolution and
realistic river images that can be used to support modeling and analysis in
surface water estimation, river meandering, wetland loss, and other
hydrological research studies. First, we have created an extensive repository
of overhead river images to be used in training. Second, we incorporated the
Progressive Growing GAN (PGGAN), a network architecture that iteratively trains
smaller-resolution GANs to gradually build up to a very high resolution to
generate high quality (i.e., 1024x1024) synthetic river imagery. With simpler
GAN architectures, difficulties arose in terms of exponential increase of
training time and vanishing/exploding gradient issues, which the PGGAN
implementation seemed to significantly reduce. The results presented in this
study show great promise in generating high-quality images and capturing the
details of river structure and flow to support hydrological research, which
often requires extensive imagery for model performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gautam_A/0/1/0/all/0/1"&gt;Akshat Gautam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sit_M/0/1/0/all/0/1"&gt;Muhammed Sit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demir_I/0/1/0/all/0/1"&gt;Ibrahim Demir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[User Acceptance of Gender Stereotypes in Automated Career Recommendations. (arXiv:2106.07112v2 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07112</id>
        <link href="http://arxiv.org/abs/2106.07112"/>
        <updated>2021-07-29T02:00:10.232Z</updated>
        <summary type="html"><![CDATA[Currently, there is a surge of interest in fair Artificial Intelligence (AI)
and Machine Learning (ML) research which aims to mitigate discriminatory bias
in AI algorithms, e.g. along lines of gender, age, and race. While most
research in this domain focuses on developing fair AI algorithms, in this work,
we show that a fair AI algorithm on its own may be insufficient to achieve its
intended results in the real world. Using career recommendation as a case
study, we build a fair AI career recommender by employing gender debiasing
machine learning techniques. Our offline evaluation showed that the debiased
recommender makes fairer career recommendations without sacrificing its
accuracy. Nevertheless, an online user study of more than 200 college students
revealed that participants on average prefer the original biased system over
the debiased system. Specifically, we found that perceived gender disparity is
a determining factor for the acceptance of a recommendation. In other words,
our results demonstrate we cannot fully address the gender bias issue in AI
recommendations without addressing the gender bias in humans.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Clarice Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kathryn Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bian_A/0/1/0/all/0/1"&gt;Andrew Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islam_R/0/1/0/all/0/1"&gt;Rashidul Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keya_K/0/1/0/all/0/1"&gt;Kamrun Naher Keya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foulds_J/0/1/0/all/0/1"&gt;James Foulds&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Shimei Pan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Numerical issues in maximum likelihood parameter estimation for Gaussian process interpolation. (arXiv:2101.09747v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09747</id>
        <link href="http://arxiv.org/abs/2101.09747"/>
        <updated>2021-07-29T02:00:10.225Z</updated>
        <summary type="html"><![CDATA[This article investigates the origin of numerical issues in maximum
likelihood parameter estimation for Gaussian process (GP) interpolation and
investigates simple but effective strategies for improving commonly used
open-source software implementations. This work targets a basic problem but a
host of studies, particularly in the literature of Bayesian optimization, rely
on off-the-shelf GP implementations. For the conclusions of these studies to be
reliable and reproducible, robust GP implementations are critical.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Basak_S/0/1/0/all/0/1"&gt;Subhasish Basak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Petit_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Petit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bect_J/0/1/0/all/0/1"&gt;Julien Bect&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vazquez_E/0/1/0/all/0/1"&gt;Emmanuel Vazquez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cooperative Online Learning. (arXiv:2106.04982v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04982</id>
        <link href="http://arxiv.org/abs/2106.04982"/>
        <updated>2021-07-29T02:00:10.218Z</updated>
        <summary type="html"><![CDATA[In this preliminary (and unpolished) version of the paper, we study an
asynchronous online learning setting with a network of agents. At each time
step, some of the agents are activated, requested to make a prediction, and pay
the corresponding loss. Some feedback is then revealed to these agents and is
later propagated through the network. We consider the case of full, bandit, and
semi-bandit feedback. In particular, we construct a reduction to delayed
single-agent learning that applies to both the full and the bandit feedback
case and allows to obtain regret guarantees for both settings. We complement
these results with a near-matching lower bound.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cesari_T/0/1/0/all/0/1"&gt;Tommaso R. Cesari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vecchia_R/0/1/0/all/0/1"&gt;Riccardo Della Vecchia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HDR Environment Map Estimation for Real-Time Augmented Reality. (arXiv:2011.10687v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10687</id>
        <link href="http://arxiv.org/abs/2011.10687"/>
        <updated>2021-07-29T02:00:10.197Z</updated>
        <summary type="html"><![CDATA[We present a method to estimate an HDR environment map from a narrow
field-of-view LDR camera image in real-time. This enables perceptually
appealing reflections and shading on virtual objects of any material finish,
from mirror to diffuse, rendered into a real physical environment using
augmented reality. Our method is based on our efficient convolutional neural
network architecture, EnvMapNet, trained end-to-end with two novel losses,
ProjectionLoss for the generated image, and ClusterLoss for adversarial
training. Through qualitative and quantitative comparison to state-of-the-art
methods, we demonstrate that our algorithm reduces the directional error of
estimated light sources by more than 50%, and achieves 3.7 times lower Frechet
Inception Distance (FID). We further showcase a mobile application that is able
to run our neural network model in under 9 ms on an iPhone XS, and render in
real-time, visually coherent virtual objects in previously unseen real-world
environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Somanath_G/0/1/0/all/0/1"&gt;Gowri Somanath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurz_D/0/1/0/all/0/1"&gt;Daniel Kurz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Inertial Newton Algorithm for Deep Learning. (arXiv:1905.12278v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.12278</id>
        <link href="http://arxiv.org/abs/1905.12278"/>
        <updated>2021-07-29T02:00:10.190Z</updated>
        <summary type="html"><![CDATA[We introduce a new second-order inertial optimization method for machine
learning called INNA. It exploits the geometry of the loss function while only
requiring stochastic approximations of the function values and the generalized
gradients. This makes INNA fully implementable and adapted to large-scale
optimization problems such as the training of deep neural networks. The
algorithm combines both gradient-descent and Newton-like behaviors as well as
inertia. We prove the convergence of INNA for most deep learning problems. To
do so, we provide a well-suited framework to analyze deep learning loss
functions involving tame optimization in which we study a continuous dynamical
system together with its discrete stochastic approximations. We prove sublinear
convergence for the continuous-time differential inclusion which underlies our
algorithm. Additionally, we also show how standard optimization mini-batch
methods applied to non-smooth non-convex problems can yield a certain type of
spurious stationary points never discussed before. We address this issue by
providing a theoretical framework around the new idea of $D$-criticality; we
then give a simple asymptotic analysis of INNA. Our algorithm allows for using
an aggressive learning rate of $o(1/\log k)$. From an empirical viewpoint, we
show that INNA returns competitive results with respect to state of the art
(stochastic gradient descent, ADAGRAD, ADAM) on popular deep learning benchmark
problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Castera_C/0/1/0/all/0/1"&gt;Camille Castera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bolte_J/0/1/0/all/0/1"&gt;J&amp;#xe9;r&amp;#xf4;me Bolte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fevotte_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric F&amp;#xe9;votte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pauwels_E/0/1/0/all/0/1"&gt;Edouard Pauwels&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel CropdocNet for Automated Potato Late Blight Disease Detection from the Unmanned Aerial Vehicle-based Hyperspectral Imagery. (arXiv:2107.13277v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13277</id>
        <link href="http://arxiv.org/abs/2107.13277"/>
        <updated>2021-07-29T02:00:10.166Z</updated>
        <summary type="html"><![CDATA[Late blight disease is one of the most destructive diseases in potato crop,
leading to serious yield losses globally. Accurate diagnosis of the disease at
early stage is critical for precision disease control and management. Current
farm practices in crop disease diagnosis are based on manual visual inspection,
which is costly, time consuming, subject to individual bias. Recent advances in
imaging sensors (e.g. RGB, multiple spectral and hyperspectral cameras), remote
sensing and machine learning offer the opportunity to address this challenge.
Particularly, hyperspectral imagery (HSI) combining with machine learning/deep
learning approaches is preferable for accurately identifying specific plant
diseases because the HSI consists of a wide range of high-quality reflectance
information beyond human vision, capable of capturing both spectral-spatial
information. The proposed method considers the potential disease specific
reflectance radiation variance caused by the canopy structural diversity,
introduces the multiple capsule layers to model the hierarchical structure of
the spectral-spatial disease attributes with the encapsulated features to
represent the various classes and the rotation invariance of the disease
attributes in the feature space. We have evaluated the proposed method with the
real UAV-based HSI data under the controlled field conditions. The
effectiveness of the hierarchical features has been quantitatively assessed and
compared with the existing representative machine learning/deep learning
methods. The experiment results show that the proposed model significantly
improves the accuracy performance when considering hierarchical-structure of
spectral-spatial features, comparing to the existing methods only using
spectral, or spatial or spectral-spatial features without consider
hierarchical-structure of spectral-spatial features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yue Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1"&gt;Liangxiu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kleerekoper_A/0/1/0/all/0/1"&gt;Anthony Kleerekoper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Sheng Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1"&gt;Tongle Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search. (arXiv:1907.01845v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1907.01845</id>
        <link href="http://arxiv.org/abs/1907.01845"/>
        <updated>2021-07-29T02:00:10.158Z</updated>
        <summary type="html"><![CDATA[One of the most critical problems in weight-sharing neural architecture
search is the evaluation of candidate models within a predefined search space.
In practice, a one-shot supernet is trained to serve as an evaluator. A
faithful ranking certainly leads to more accurate searching results. However,
current methods are prone to making misjudgments. In this paper, we prove that
their biased evaluation is due to inherent unfairness in the supernet training.
In view of this, we propose two levels of constraints: expectation fairness and
strict fairness. Particularly, strict fairness ensures equal optimization
opportunities for all choice blocks throughout the training, which neither
overestimates nor underestimates their capacity. We demonstrate that this is
crucial for improving the confidence of models' ranking. Incorporating the
one-shot supernet trained under the proposed fairness constraints with a
multi-objective evolutionary search algorithm, we obtain various
state-of-the-art models, e.g., FairNAS-A attains 77.5% top-1 validation
accuracy on ImageNet. The models and their evaluation codes are made publicly
available online this http URL .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1"&gt;Xiangxiang Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Ruijun Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating the Use of Reconstruction Error for Novelty Localization. (arXiv:2107.13379v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13379</id>
        <link href="http://arxiv.org/abs/2107.13379"/>
        <updated>2021-07-29T02:00:10.151Z</updated>
        <summary type="html"><![CDATA[The pixelwise reconstruction error of deep autoencoders is often utilized for
image novelty detection and localization under the assumption that pixels with
high error indicate which parts of the input image are unfamiliar and therefore
likely to be novel. This assumed correlation between pixels with high
reconstruction error and novel regions of input images has not been verified
and may limit the accuracy of these methods. In this paper we utilize saliency
maps to evaluate whether this correlation exists. Saliency maps reveal directly
how much a change in each input pixel would affect reconstruction loss, while
each pixel's reconstruction error may be attributed to many input pixels when
layers are fully connected. We compare saliency maps to reconstruction error
maps via qualitative visualizations as well as quantitative correspondence
between the top K elements of the maps for both novel and normal images. Our
results indicate that reconstruction error maps do not closely correlate with
the importance of pixels in the input images, making them insufficient for
novelty localization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feeney_P/0/1/0/all/0/1"&gt;Patrick Feeney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1"&gt;Michael C. Hughes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WaveCNet: Wavelet Integrated CNNs to Suppress Aliasing Effect for Noise-Robust Image Classification. (arXiv:2107.13335v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13335</id>
        <link href="http://arxiv.org/abs/2107.13335"/>
        <updated>2021-07-29T02:00:10.141Z</updated>
        <summary type="html"><![CDATA[Though widely used in image classification, convolutional neural networks
(CNNs) are prone to noise interruptions, i.e. the CNN output can be drastically
changed by small image noise. To improve the noise robustness, we try to
integrate CNNs with wavelet by replacing the common down-sampling (max-pooling,
strided-convolution, and average pooling) with discrete wavelet transform
(DWT). We firstly propose general DWT and inverse DWT (IDWT) layers applicable
to various orthogonal and biorthogonal discrete wavelets like Haar, Daubechies,
and Cohen, etc., and then design wavelet integrated CNNs (WaveCNets) by
integrating DWT into the commonly used CNNs (VGG, ResNets, and DenseNet).
During the down-sampling, WaveCNets apply DWT to decompose the feature maps
into the low-frequency and high-frequency components. Containing the main
information including the basic object structures, the low-frequency component
is transmitted into the following layers to generate robust high-level
features. The high-frequency components are dropped to remove most of the data
noises. The experimental results show that %wavelet accelerates the CNN
training, and WaveCNets achieve higher accuracy on ImageNet than various
vanilla CNNs. We have also tested the performance of WaveCNets on the noisy
version of ImageNet, ImageNet-C and six adversarial attacks, the results
suggest that the proposed DWT/IDWT layers could provide better noise-robustness
and adversarial robustness. When applying WaveCNets as backbones, the
performance of object detectors (i.e., faster R-CNN and RetinaNet) on COCO
detection dataset are consistently improved. We believe that suppression of
aliasing effect, i.e. separation of low frequency and high frequency
information, is the main advantages of our approach. The code of our DWT/IDWT
layer and different WaveCNets are available at
https://github.com/CVI-SZU/WaveCNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qiufu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Linlin Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1"&gt;Sheng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_Z/0/1/0/all/0/1"&gt;Zhihui Lai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subjective evaluation of traditional and learning-based image coding methods. (arXiv:2107.13122v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13122</id>
        <link href="http://arxiv.org/abs/2107.13122"/>
        <updated>2021-07-29T02:00:10.118Z</updated>
        <summary type="html"><![CDATA[We conduct a subjective experiment to compare the performance of traditional
image coding methods and learning-based image coding methods. HEVC and VVC, the
state-of-the-art traditional coding methods, are used as the representative
traditional methods. The learning-based methods used contain not only CNN-based
methods, but also a GAN-based method, all of which are advanced or typical.
Single Stimuli (SS), which is also called Absolute Category Rating (ACR), is
adopted as the methodology of the experiment to obtain perceptual quality of
images. Additionally, we utilize some typical and frequently used objective
quality metrics to evaluate the coding methods in the experiment as comparison.
The experiment shows that CNN-based and GAN-based methods can perform better
than traditional methods in low bit-rates. In high bit-rates, however, it is
hard to verify whether CNN-based methods are superior to traditional methods.
Because the GAN method does not provide models with high target bit-rates, we
cannot exactly tell the performance of the GAN method in high bit-rates.
Furthermore, some popular objective quality metrics have not shown the ability
well to measure quality of images generated by learning-based coding methods,
especially the GAN-based one.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1"&gt;Zhigao Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiaqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1"&gt;Lu Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yin Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shape Controllable Virtual Try-on for Underwear Models. (arXiv:2107.13156v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13156</id>
        <link href="http://arxiv.org/abs/2107.13156"/>
        <updated>2021-07-29T02:00:10.105Z</updated>
        <summary type="html"><![CDATA[Image virtual try-on task has abundant applications and has become a hot
research topic recently. Existing 2D image-based virtual try-on methods aim to
transfer a target clothing image onto a reference person, which has two main
disadvantages: cannot control the size and length precisely; unable to
accurately estimate the user's figure in the case of users wearing thick
clothes, resulting in inaccurate dressing effect. In this paper, we put forward
an akin task that aims to dress clothing for underwear models. %, which is also
an urgent need in e-commerce scenarios. To solve the above drawbacks, we
propose a Shape Controllable Virtual Try-On Network (SC-VTON), where a graph
attention network integrates the information of model and clothing to generate
the warped clothing image. In addition, the control points are incorporated
into SC-VTON for the desired clothing shape. Furthermore, by adding a Splitting
Network and a Synthesis Network, we can use clothing/model pair data to help
optimize the deformation module and generalize the task to the typical virtual
try-on task. Extensive experiments show that the proposed method can achieve
accurate shape control. Meanwhile, compared with other methods, our method can
generate high-resolution results with detailed textures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xin Gao&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhenjiang Liu&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zunlei Feng&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chengji Shen&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Ou_K/0/1/0/all/0/1"&gt;Kairi Ou&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Haihong Tang&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1"&gt;Mingli Song&lt;/a&gt; (2) ((1) Alibaba Group, (2) Zhejiang University)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty-Aware Credit Card Fraud Detection Using Deep Learning. (arXiv:2107.13508v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13508</id>
        <link href="http://arxiv.org/abs/2107.13508"/>
        <updated>2021-07-29T02:00:10.077Z</updated>
        <summary type="html"><![CDATA[Countless research works of deep neural networks (DNNs) in the task of credit
card fraud detection have focused on improving the accuracy of point
predictions and mitigating unwanted biases by building different network
architectures or learning models. Quantifying uncertainty accompanied by point
estimation is essential because it mitigates model unfairness and permits
practitioners to develop trustworthy systems which abstain from suboptimal
decisions due to low confidence. Explicitly, assessing uncertainties associated
with DNNs predictions is critical in real-world card fraud detection settings
for characteristic reasons, including (a) fraudsters constantly change their
strategies, and accordingly, DNNs encounter observations that are not generated
by the same process as the training distribution, (b) owing to the
time-consuming process, very few transactions are timely checked by
professional experts to update DNNs. Therefore, this study proposes three
uncertainty quantification (UQ) techniques named Monte Carlo dropout, ensemble,
and ensemble Monte Carlo dropout for card fraud detection applied on
transaction data. Moreover, to evaluate the predictive uncertainty estimates,
UQ confusion matrix and several performance metrics are utilized. Through
experimental results, we show that the ensemble is more effective in capturing
uncertainty corresponding to generated predictions. Additionally, we
demonstrate that the proposed UQ methods provide extra insight to the point
predictions, leading to elevate the fraud prevention process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Habibpour_M/0/1/0/all/0/1"&gt;Maryam Habibpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gharoun_H/0/1/0/all/0/1"&gt;Hassan Gharoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehdipour_M/0/1/0/all/0/1"&gt;Mohammadreza Mehdipour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tajally_A/0/1/0/all/0/1"&gt;AmirReza Tajally&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asgharnezhad_H/0/1/0/all/0/1"&gt;Hamzeh Asgharnezhad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamsi_A/0/1/0/all/0/1"&gt;Afshar Shamsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khosravi_A/0/1/0/all/0/1"&gt;Abbas Khosravi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafie_Khah_M/0/1/0/all/0/1"&gt;Miadreza Shafie-Khah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1"&gt;Saeid Nahavandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Catalao_J/0/1/0/all/0/1"&gt;Joao P.S. Catalao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Supervised Learning and the Finite-Temperature String Method for Computing Committor Functions and Reaction Rates. (arXiv:2107.13522v1 [cond-mat.stat-mech])]]></title>
        <id>http://arxiv.org/abs/2107.13522</id>
        <link href="http://arxiv.org/abs/2107.13522"/>
        <updated>2021-07-29T02:00:10.028Z</updated>
        <summary type="html"><![CDATA[A central object in the computational studies of rare events is the committor
function. Though costly to compute, the committor function encodes complete
mechanistic information of the processes involving rare events, including
reaction rates and transition-state ensembles. Under the framework of
transition path theory (TPT), recent work [1] proposes an algorithm where a
feedback loop couples a neural network that models the committor function with
importance sampling, mainly umbrella sampling, which collects data needed for
adaptive training. In this work, we show additional modifications are needed to
improve the accuracy of the algorithm. The first modification adds elements of
supervised learning, which allows the neural network to improve its prediction
by fitting to sample-mean estimates of committor values obtained from short
molecular dynamics trajectories. The second modification replaces the
committor-based umbrella sampling with the finite-temperature string (FTS)
method, which enables homogeneous sampling in regions where transition pathways
are located. We test our modifications on low-dimensional systems with
non-convex potential energy where reference solutions can be found via
analytical or the finite element methods, and show how combining supervised
learning and the FTS method yields accurate computation of committor functions
and reaction rates. We also provide an error analysis for algorithms that use
the FTS method, using which reaction rates can be accurately estimated during
training with a small number of samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Hasyim_M/0/1/0/all/0/1"&gt;Muhammad R. Hasyim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Batton_C/0/1/0/all/0/1"&gt;Clay H. Batton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Mandadapu_K/0/1/0/all/0/1"&gt;Kranthi K. Mandadapu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Recurrent Semi-Supervised EEG Representation Learning for Emotion Recognition. (arXiv:2107.13505v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13505</id>
        <link href="http://arxiv.org/abs/2107.13505"/>
        <updated>2021-07-29T02:00:10.020Z</updated>
        <summary type="html"><![CDATA[EEG-based emotion recognition often requires sufficient labeled training
samples to build an effective computational model. Labeling EEG data, on the
other hand, is often expensive and time-consuming. To tackle this problem and
reduce the need for output labels in the context of EEG-based emotion
recognition, we propose a semi-supervised pipeline to jointly exploit both
unlabeled and labeled data for learning EEG representations. Our
semi-supervised framework consists of both unsupervised and supervised
components. The unsupervised part maximizes the consistency between original
and reconstructed input data using an autoencoder, while simultaneously the
supervised part minimizes the cross-entropy between the input and output
labels. We evaluate our framework using both a stacked autoencoder and an
attention-based recurrent autoencoder. We test our framework on the large-scale
SEED EEG dataset and compare our results with several other popular
semi-supervised methods. Our semi-supervised framework with a deep
attention-based recurrent autoencoder consistently outperforms the benchmark
methods, even when small sub-sets (3\%, 5\% and 10\%) of the output labels are
available during training, achieving a new state-of-the-art semi-supervised
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guangyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1"&gt;Ali Etemad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatial Uncertainty-Aware Semi-Supervised Crowd Counting. (arXiv:2107.13271v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13271</id>
        <link href="http://arxiv.org/abs/2107.13271"/>
        <updated>2021-07-29T02:00:10.004Z</updated>
        <summary type="html"><![CDATA[Semi-supervised approaches for crowd counting attract attention, as the fully
supervised paradigm is expensive and laborious due to its request for a large
number of images of dense crowd scenarios and their annotations. This paper
proposes a spatial uncertainty-aware semi-supervised approach via regularized
surrogate task (binary segmentation) for crowd counting problems. Different
from existing semi-supervised learning-based crowd counting methods, to exploit
the unlabeled data, our proposed spatial uncertainty-aware teacher-student
framework focuses on high confident regions' information while addressing the
noisy supervision from the unlabeled data in an end-to-end manner.
Specifically, we estimate the spatial uncertainty maps from the teacher model's
surrogate task to guide the feature learning of the main task (density
regression) and the surrogate task of the student model at the same time.
Besides, we introduce a simple yet effective differential transformation layer
to enforce the inherent spatial consistency regularization between the main
task and the surrogate task in the student model, which helps the surrogate
task to yield more reliable predictions and generates high-quality uncertainty
maps. Thus, our model can also address the task-level perturbation problems
that occur spatial inconsistency between the primary and surrogate tasks in the
student model. Experimental results on four challenging crowd counting datasets
demonstrate that our method achieves superior performance to the
state-of-the-art semi-supervised methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1"&gt;Yanda Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongrun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yitian Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaoyun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1"&gt;Xuesheng Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaowei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yalin Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Reasonable Crowd: Towards evidence-based and interpretable models of driving behavior. (arXiv:2107.13507v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13507</id>
        <link href="http://arxiv.org/abs/2107.13507"/>
        <updated>2021-07-29T02:00:09.974Z</updated>
        <summary type="html"><![CDATA[Autonomous vehicles must balance a complex set of objectives. There is no
consensus on how they should do so, nor on a model for specifying a desired
driving behavior. We created a dataset to help address some of these questions
in a limited operating domain. The data consists of 92 traffic scenarios, with
multiple ways of traversing each scenario. Multiple annotators expressed their
preference between pairs of scenario traversals. We used the data to compare an
instance of a rulebook, carefully hand-crafted independently of the dataset,
with several interpretable machine learning models such as Bayesian networks,
decision trees, and logistic regression trained on the dataset. To compare
driving behavior, these models use scores indicating by how much different
scenario traversals violate each of 14 driving rules. The rules are
interpretable and designed by subject-matter experts. First, we found that
these rules were enough for these models to achieve a high classification
accuracy on the dataset. Second, we found that the rulebook provides high
interpretability without excessively sacrificing performance. Third, the data
pointed to possible improvements in the rulebook and the rules, and to
potential new rules. Fourth, we explored the interpretability vs performance
trade-off by also training non-interpretable models such as a random forest.
Finally, we make the dataset publicly available to encourage a discussion from
the wider community on behavior specification for AVs. Please find it at
github.com/bassam-motional/Reasonable-Crowd.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Helou_B/0/1/0/all/0/1"&gt;Bassam Helou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dusi_A/0/1/0/all/0/1"&gt;Aditya Dusi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collin_A/0/1/0/all/0/1"&gt;Anne Collin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehdipour_N/0/1/0/all/0/1"&gt;Noushin Mehdipour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhiliang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lizarazo_C/0/1/0/all/0/1"&gt;Cristhian Lizarazo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belta_C/0/1/0/all/0/1"&gt;Calin Belta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wongpiromsarn_T/0/1/0/all/0/1"&gt;Tichakorn Wongpiromsarn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tebbens_R/0/1/0/all/0/1"&gt;Radboud Duintjer Tebbens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beijbom_O/0/1/0/all/0/1"&gt;Oscar Beijbom&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quasi-Newton Methods for Machine Learning: Forget the Past, Just Sample. (arXiv:1901.09997v5 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1901.09997</id>
        <link href="http://arxiv.org/abs/1901.09997"/>
        <updated>2021-07-29T02:00:09.967Z</updated>
        <summary type="html"><![CDATA[We present two sampled quasi-Newton methods (sampled LBFGS and sampled LSR1)
for solving empirical risk minimization problems that arise in machine
learning. Contrary to the classical variants of these methods that sequentially
build Hessian or inverse Hessian approximations as the optimization progresses,
our proposed methods sample points randomly around the current iterate at every
iteration to produce these approximations. As a result, the approximations
constructed make use of more reliable (recent and local) information, and do
not depend on past iterate information that could be significantly stale. Our
proposed algorithms are efficient in terms of accessed data points (epochs) and
have enough concurrency to take advantage of parallel/distributed computing
environments. We provide convergence guarantees for our proposed methods.
Numerical tests on a toy classification problem as well as on popular
benchmarking binary classification and neural network training tasks reveal
that the methods outperform their classical variants.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Berahas_A/0/1/0/all/0/1"&gt;Albert S. Berahas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jahani_M/0/1/0/all/0/1"&gt;Majid Jahani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Richtarik_P/0/1/0/all/0/1"&gt;Peter Richt&amp;#xe1;rik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Takac_M/0/1/0/all/0/1"&gt;Martin Tak&amp;#xe1;&amp;#x10d;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReLMM: Practical RL for Learning Mobile Manipulation Skills Using Only Onboard Sensors. (arXiv:2107.13545v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13545</id>
        <link href="http://arxiv.org/abs/2107.13545"/>
        <updated>2021-07-29T02:00:09.959Z</updated>
        <summary type="html"><![CDATA[In this paper, we study how robots can autonomously learn skills that require
a combination of navigation and grasping. Learning robotic skills in the real
world remains challenging without large-scale data collection and supervision.
Our aim is to devise a robotic reinforcement learning system for learning
navigation and manipulation together, in an \textit{autonomous} way without
human intervention, enabling continual learning under realistic assumptions.
Specifically, our system, ReLMM, can learn continuously on a real-world
platform without any environment instrumentation, without human intervention,
and without access to privileged information, such as maps, objects positions,
or a global view of the environment. Our method employs a modularized policy
with components for manipulation and navigation, where uncertainty over the
manipulation success drives exploration for the navigation controller, and the
manipulation module provides rewards for navigation. We evaluate our method on
a room cleanup task, where the robot must navigate to and pick up items of
scattered on the floor. After a grasp curriculum training phase, ReLMM can
learn navigation and grasping together fully automatically, in around 40 hours
of real-world training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Charles Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orbik_J/0/1/0/all/0/1"&gt;J&amp;#x119;drzej Orbik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Devin_C/0/1/0/all/0/1"&gt;Coline Devin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Brian Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Abhishek Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berseth_G/0/1/0/all/0/1"&gt;Glen Berseth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Portiloop: a deep learning-based open science tool for closed-loop brain stimulation. (arXiv:2107.13473v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.13473</id>
        <link href="http://arxiv.org/abs/2107.13473"/>
        <updated>2021-07-29T02:00:09.840Z</updated>
        <summary type="html"><![CDATA[Electroencephalography (EEG) is a method of measuring the brain's electrical
activity, using non-invasive scalp electrodes. In this article, we propose the
Portiloop, a deep learning-based portable and low-cost device enabling the
neuroscience community to capture EEG, process it in real time, detect patterns
of interest, and respond with precisely-timed stimulation. The core of the
Portiloop is a System on Chip composed of an Analog to Digital Converter (ADC)
and a Field-Programmable Gate Array (FPGA). After being converted to digital by
the ADC, the EEG signal is processed in the FPGA. The FPGA contains an ad-hoc
Artificial Neural Network (ANN) with convolutional and recurrent units,
directly implemented in hardware. The output of the ANN is then used to trigger
the user-defined feedback. We use the Portiloop to develop a real-time sleep
spindle stimulating application, as a case study. Sleep spindles are a specific
type of transient oscillation ($\sim$2.5 s, 12-16 Hz) that are observed in EEG
recordings, and are related to memory consolidation during sleep. We tested the
Portiloop's capacity to detect and stimulate sleep spindles in real time using
an existing database of EEG sleep recordings. With 71% for both precision and
recall as compared with expert labels, the system is able to stimulate spindles
within $\sim$300 ms of their onset, enabling experimental manipulation of early
the entire spindle. The Portiloop can be extended to detect and stimulate other
neural events in EEG. It is fully available to the research community as an
open science project.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Valenchon_N/0/1/0/all/0/1"&gt;Nicolas Valenchon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bouteiller_Y/0/1/0/all/0/1"&gt;Yann Bouteiller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jourde_H/0/1/0/all/0/1"&gt;Hugo R. Jourde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Coffey_E/0/1/0/all/0/1"&gt;Emily B.J. Coffey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Beltrame_G/0/1/0/all/0/1"&gt;Giovanni Beltrame&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manifold Coordinates with Physical Meaning. (arXiv:1811.11891v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1811.11891</id>
        <link href="http://arxiv.org/abs/1811.11891"/>
        <updated>2021-07-29T02:00:09.832Z</updated>
        <summary type="html"><![CDATA[Manifold embedding algorithms map high-dimensional data down to coordinates
in a much lower-dimensional space. One of the aims of dimension reduction is to
find intrinsic coordinates that describe the data manifold. The coordinates
returned by the embedding algorithm are abstract, and finding their physical or
domain-related meaning is not formalized and often left to domain experts. This
paper studies the problem of recovering the meaning of the new low-dimensional
representation in an automatic, principled fashion. We propose a method to
explain embedding coordinates of a manifold as non-linear compositions of
functions from a user-defined dictionary. We show that this problem can be set
up as a sparse linear Group Lasso recovery problem, find sufficient recovery
conditions, and demonstrate its effectiveness on data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Koelle_S/0/1/0/all/0/1"&gt;Samson Koelle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hanyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Meila_M/0/1/0/all/0/1"&gt;Marina Meila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yu-Chia Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clustering by Orthogonal NMF Model and Non-Convex Penalty Optimization. (arXiv:1906.00570v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.00570</id>
        <link href="http://arxiv.org/abs/1906.00570"/>
        <updated>2021-07-29T02:00:09.825Z</updated>
        <summary type="html"><![CDATA[The non-negative matrix factorization (NMF) model with an additional
orthogonality constraint on one of the factor matrices, called the orthogonal
NMF (ONMF), has been found a promising clustering model and can outperform the
classical K-means. However, solving the ONMF model is a challenging
optimization problem because the coupling of the orthogonality and
non-negativity constraints introduces a mixed combinatorial aspect into the
problem due to the determination of the correct status of the variables
(positive or zero). Most of the existing methods directly deal with the
orthogonality constraint in its original form via various optimization
techniques, but are not scalable for large-scale problems. In this paper, we
propose a new ONMF based clustering formulation that equivalently transforms
the orthogonality constraint into a set of norm-based non-convex equality
constraints. We then apply a non-convex penalty (NCP) approach to add them to
the objective as penalty terms, leading to a problem that is efficiently
solvable. One smooth penalty formulation and one non-smooth penalty formulation
are respectively studied. We build theoretical conditions for the penalized
problems to provide feasible stationary solutions to the ONMF based clustering
problem, as well as proposing efficient algorithms for solving the penalized
problems of the two NCP methods. Experimental results based on both synthetic
and real datasets are presented to show that the proposed NCP methods are
computationally time efficient, and either match or outperform the existing
K-means and ONMF based methods in terms of the clustering performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1"&gt;Tsung-Hui Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1"&gt;Ying Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1"&gt;Jong-Shi Pang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Satisfiability and Synthesis Modulo Oracles. (arXiv:2107.13477v1 [cs.LO])]]></title>
        <id>http://arxiv.org/abs/2107.13477</id>
        <link href="http://arxiv.org/abs/2107.13477"/>
        <updated>2021-07-29T02:00:09.813Z</updated>
        <summary type="html"><![CDATA[In classic program synthesis algorithms, such as counterexample-guided
inductive synthesis (CEGIS), the algorithms alternate between a synthesis phase
and an oracle (verification) phase. Many synthesis algorithms use a white-box
oracle based on satisfiability modulo theory (SMT) solvers to provide
counterexamples. But what if a white-box oracle is either not available or not
easy to work with? We present a framework for solving a general class of
oracle-guided synthesis problems which we term synthesis modulo oracles. In
this setting, oracles may be black boxes with a query-response interface
defined by the synthesis problem. As a necessary component of this framework,
we also formalize the problem of satisfiability modulo theories and oracles,
and present an algorithm for solving this problem. We implement a prototype
solver for satisfiability and synthesis modulo oracles and demonstrate that, by
using oracles that execute functions not easily modeled in SMT-constraints,
such as recursive functions or oracles that incorporate compilation and
execution of code, SMTO and SyMO are able to solve problems beyond the
abilities of standard SMT and synthesis solvers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Polgreen_E/0/1/0/all/0/1"&gt;Elizabeth Polgreen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reynolds_A/0/1/0/all/0/1"&gt;Andrew Reynolds&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seshia_S/0/1/0/all/0/1"&gt;Sanjit A. Seshia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MARViN -- Multiple Arithmetic Resolutions Vacillating in Neural Networks. (arXiv:2107.13490v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13490</id>
        <link href="http://arxiv.org/abs/2107.13490"/>
        <updated>2021-07-29T02:00:09.804Z</updated>
        <summary type="html"><![CDATA[Quantization is a technique for reducing deep neural networks (DNNs) training
and inference times, which is crucial for training in resource constrained
environments or time critical inference applications. State-of-the-art (SOTA)
quantization approaches focus on post-training quantization, i.e. quantization
of pre-trained DNNs for speeding up inference. Very little work on quantized
training exists, which neither al-lows dynamic intra-epoch precision switches
nor em-ploys an information theory based switching heuristic. Usually, existing
approaches require full precision refinement afterwards and enforce a global
word length across the whole DNN. This leads to suboptimal quantization
mappings and resource usage. Recognizing these limits, we introduce MARViN, a
new quantized training strategy using information theory-based intra-epoch
precision switching, which decides on a per-layer basis which precision should
be used in order to minimize quantization-induced information loss. Note that
any quantization must leave enough precision such that future learning steps do
not suffer from vanishing gradients. We achieve an average speedup of 1.86
compared to a float32 basis while limiting mean accuracy degradation on
AlexNet/ResNet to only -0.075%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kummer_L/0/1/0/all/0/1"&gt;Lorenz Kummer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sidak_K/0/1/0/all/0/1"&gt;Kevin Sidak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reichmann_T/0/1/0/all/0/1"&gt;Tabea Reichmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gansterer_W/0/1/0/all/0/1"&gt;Wilfried Gansterer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings. (arXiv:2106.03143v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03143</id>
        <link href="http://arxiv.org/abs/2106.03143"/>
        <updated>2021-07-29T02:00:09.699Z</updated>
        <summary type="html"><![CDATA[Without positional information, attention-based transformer neural networks
are permutation-invariant. Absolute or relative positional embeddings are the
most popular ways to feed transformer models positional information. Absolute
positional embeddings are simple to implement, but suffer from generalization
issues when evaluating on sequences of different length than those seen at
training time. Relative positions are more robust to length change, but are
more complex to implement and yield inferior model throughput. In this paper,
we propose an augmentation-based approach (CAPE) for absolute positional
embeddings, which keeps the advantages of both absolute (simplicity and speed)
and relative position embeddings (better generalization). In addition, our
empirical evaluation on state-of-the-art models in machine translation, image
and speech recognition demonstrates that CAPE leads to better generalization
performance as well as increased stability with respect to training
hyper-parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1"&gt;Tatiana Likhomanenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1"&gt;Qiantong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1"&gt;Ronan Collobert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1"&gt;Gabriel Synnaeve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rogozhnikov_A/0/1/0/all/0/1"&gt;Alex Rogozhnikov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistically Significant Stopping of Neural Network Training. (arXiv:2103.01205v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01205</id>
        <link href="http://arxiv.org/abs/2103.01205"/>
        <updated>2021-07-29T02:00:09.681Z</updated>
        <summary type="html"><![CDATA[The general approach taken when training deep learning classifiers is to save
the parameters after every few iterations, train until either a human observer
or a simple metric-based heuristic decides the network isn't learning anymore,
and then backtrack and pick the saved parameters with the best validation
accuracy. Simple methods are used to determine if a neural network isn't
learning anymore because, as long as it's well after the optimal values are
found, the condition doesn't impact the final accuracy of the model. However
from a runtime perspective, this is of great significance to the many cases
where numerous neural networks are trained simultaneously (e.g. hyper-parameter
tuning). Motivated by this, we introduce a statistical significance test to
determine if a neural network has stopped learning. This stopping criterion
appears to represent a happy medium compared to other popular stopping
criterions, achieving comparable accuracy to the criterions that achieve the
highest final accuracies in 77% or fewer epochs, while the criterions which
stop sooner do so with an appreciable loss to final accuracy. Additionally, we
use this as the basis of a new learning rate scheduler, removing the need to
manually choose learning rate schedules and acting as a quasi-line search,
achieving superior or comparable empirical performance to existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Terry_J/0/1/0/all/0/1"&gt;J. K. Terry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jayakumar_M/0/1/0/all/0/1"&gt;Mario Jayakumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alwis_K/0/1/0/all/0/1"&gt;Kusal De Alwis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When and how do CNNs generalize to out-of-distribution category-viewpoint combinations?. (arXiv:2007.08032v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.08032</id>
        <link href="http://arxiv.org/abs/2007.08032"/>
        <updated>2021-07-29T02:00:09.674Z</updated>
        <summary type="html"><![CDATA[Object recognition and viewpoint estimation lie at the heart of visual
understanding. Recent works suggest that convolutional neural networks (CNNs)
fail to generalize to out-of-distribution (OOD) category-viewpoint
combinations, ie. combinations not seen during training. In this paper, we
investigate when and how such OOD generalization may be possible by evaluating
CNNs trained to classify both object category and 3D viewpoint on OOD
combinations, and identifying the neural mechanisms that facilitate such OOD
generalization. We show that increasing the number of in-distribution
combinations (ie. data diversity) substantially improves generalization to OOD
combinations, even with the same amount of training data. We compare learning
category and viewpoint in separate and shared network architectures, and
observe starkly different trends on in-distribution and OOD combinations, ie.
while shared networks are helpful in-distribution, separate networks
significantly outperform shared ones at OOD combinations. Finally, we
demonstrate that such OOD generalization is facilitated by the neural mechanism
of specialization, ie. the emergence of two types of neurons -- neurons
selective to category and invariant to viewpoint, and vice versa.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Madan_S/0/1/0/all/0/1"&gt;Spandan Madan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henry_T/0/1/0/all/0/1"&gt;Timothy Henry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dozier_J/0/1/0/all/0/1"&gt;Jamell Dozier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_H/0/1/0/all/0/1"&gt;Helen Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhandari_N/0/1/0/all/0/1"&gt;Nishchal Bhandari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1"&gt;Tomotake Sasaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1"&gt;Fr&amp;#xe9;do Durand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1"&gt;Hanspeter Pfister&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1"&gt;Xavier Boix&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-driven effective model shows a liquid-like deep learning. (arXiv:2007.08093v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.08093</id>
        <link href="http://arxiv.org/abs/2007.08093"/>
        <updated>2021-07-29T02:00:09.666Z</updated>
        <summary type="html"><![CDATA[The geometric structure of an optimization landscape is argued to be
fundamentally important to support the success of deep neural network learning.
A direct computation of the landscape beyond two layers is hard. Therefore, to
capture the global view of the landscape, an interpretable model of the
network-parameter (or weight) space must be established. However, the model is
lacking so far. Furthermore, it remains unknown what the landscape looks like
for deep networks of binary synapses, which plays a key role in robust and
energy efficient neuromorphic computation. Here, we propose a statistical
mechanics framework by directly building a least structured model of the
high-dimensional weight space, considering realistic structured data,
stochastic gradient descent training, and the computational depth of neural
networks. We also consider whether the number of network parameters outnumbers
the number of supplied training data, namely, over- or under-parametrization.
Our least structured model reveals that the weight spaces of the
under-parametrization and over-parameterization cases belong to the same class,
in the sense that these weight spaces are well-connected without any
hierarchical clustering structure. In contrast, the shallow-network has a
broken weight space, characterized by a discontinuous phase transition, thereby
clarifying the benefit of depth in deep learning from the angle of high
dimensional geometry. Our effective model also reveals that inside a deep
network, there exists a liquid-like central part of the architecture in the
sense that the weights in this part behave as randomly as possible, providing
algorithmic implications. Our data-driven model thus provides a statistical
mechanics insight about why deep learning is unreasonably effective in terms of
the high-dimensional weight space, and how deep networks are different from
shallow ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1"&gt;Wenxuan Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Haiping Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Board Volcanic Eruption Detection through CNNs and Satellite Multispectral Imagery. (arXiv:2106.15281v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15281</id>
        <link href="http://arxiv.org/abs/2106.15281"/>
        <updated>2021-07-29T02:00:09.647Z</updated>
        <summary type="html"><![CDATA[In recent years, the growth of Machine Learning (ML) algorithms has raised
the number of studies including their applicability in a variety of different
scenarios. Among all, one of the hardest ones is the aerospace, due to its
peculiar physical requirements. In this context, a feasibility study and a
first prototype for an Artificial Intelligence (AI) model to be deployed on
board satellites are presented in this work. As a case study, the detection of
volcanic eruptions has been investigated as a method to swiftly produce alerts
and allow immediate interventions. Two Convolutional Neural Networks (CNNs)
have been proposed and designed, showing how to efficiently implement them for
identifying the eruptions and at the same time adapting their complexity in
order to fit on board requirements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosso_M/0/1/0/all/0/1"&gt;Maria Pia Del Rosso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebastianelli_A/0/1/0/all/0/1"&gt;Alessandro Sebastianelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spiller_D/0/1/0/all/0/1"&gt;Dario Spiller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathieu_P/0/1/0/all/0/1"&gt;Pierre Philippe Mathieu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullo_S/0/1/0/all/0/1"&gt;Silvia Liberata Ullo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proximal boosting and variants. (arXiv:1808.09670v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1808.09670</id>
        <link href="http://arxiv.org/abs/1808.09670"/>
        <updated>2021-07-29T02:00:09.640Z</updated>
        <summary type="html"><![CDATA[Gradient boosting is a prediction method that iteratively combines weak
learners to produce a complex and accurate model. From an optimization point of
view, the learning procedure of gradient boosting mimics a gradient descent on
a functional variable. This paper proposes to build upon the proximal point
algorithm, when the empirical risk to minimize is not differentiable, in order
to introduce a novel boosting approach, called proximal boosting. Besides being
motivated by non-differentiable optimization, the proposed algorithm benefits
from algorithmic improvements such as controlling the approximation error and
Nesterov's acceleration, in the same way as gradient boosting [Grubb and
Bagnell, 2011, Biau et al., 2018]. This leads to two variants, respectively
called residual proximal boosting and accelerated proximal boosting.
Theoretical convergence is proved for the first two procedures under different
hypotheses on the empirical risk and advantages of leveraging proximal methods
for boosting are illustrated by numerical experiments on simulated and
real-world data. In particular, we exhibit a favorable comparison over gradient
boosting regarding convergence rate and prediction accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fouillen_E/0/1/0/all/0/1"&gt;Erwan Fouillen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boyer_C/0/1/0/all/0/1"&gt;Claire Boyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sangnier_M/0/1/0/all/0/1"&gt;Maxime Sangnier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Reflection on Learning from Data: Epistemology Issues and Limitations. (arXiv:2107.13270v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13270</id>
        <link href="http://arxiv.org/abs/2107.13270"/>
        <updated>2021-07-29T02:00:09.633Z</updated>
        <summary type="html"><![CDATA[Although learning from data is effective and has achieved significant
milestones, it has many challenges and limitations. Learning from data starts
from observations and then proceeds to broader generalizations. This framework
is controversial in science, yet it has achieved remarkable engineering
successes. This paper reflects on some epistemological issues and some of the
limitations of the knowledge discovered in data. The document discusses the
common perception that getting more data is the key to achieving better machine
learning models from theoretical and practical perspectives. The paper sheds
some light on the shortcomings of using generic mathematical theories to
describe the process. It further highlights the need for theories specialized
in learning from data. While more data leverages the performance of machine
learning models in general, the relation in practice is shown to be logarithmic
at its best; After a specific limit, more data stabilize or degrade the machine
learning models. Recent work in reinforcement learning showed that the trend is
shifting away from data-oriented approaches and relying more on algorithms. The
paper concludes that learning from data is hindered by many limitations. Hence
an approach that has an intensional orientation is needed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hammoudeh_A/0/1/0/all/0/1"&gt;Ahmad Hammoudeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tedmori_S/0/1/0/all/0/1"&gt;Sara Tedmori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Obeid_N/0/1/0/all/0/1"&gt;Nadim Obeid&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Navigation Turing Test (NTT): Learning to Evaluate Human-Like Navigation. (arXiv:2105.09637v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09637</id>
        <link href="http://arxiv.org/abs/2105.09637"/>
        <updated>2021-07-29T02:00:09.626Z</updated>
        <summary type="html"><![CDATA[A key challenge on the path to developing agents that learn complex
human-like behavior is the need to quickly and accurately quantify
human-likeness. While human assessments of such behavior can be highly
accurate, speed and scalability are limited. We address these limitations
through a novel automated Navigation Turing Test (ANTT) that learns to predict
human judgments of human-likeness. We demonstrate the effectiveness of our
automated NTT on a navigation task in a complex 3D environment. We investigate
six classification models to shed light on the types of architectures best
suited to this task, and validate them against data collected through a human
NTT. Our best models achieve high accuracy when distinguishing true human and
agent behavior. At the same time, we show that predicting finer-grained human
assessment of agents' progress towards human-like behavior remains unsolved.
Our work takes an important step towards agents that more effectively learn
complex human-like behavior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Devlin_S/0/1/0/all/0/1"&gt;Sam Devlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Georgescu_R/0/1/0/all/0/1"&gt;Raluca Georgescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Momennejad_I/0/1/0/all/0/1"&gt;Ida Momennejad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rzepecki_J/0/1/0/all/0/1"&gt;Jaroslaw Rzepecki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuniga_E/0/1/0/all/0/1"&gt;Evelyn Zuniga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Costello_G/0/1/0/all/0/1"&gt;Gavin Costello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leroy_G/0/1/0/all/0/1"&gt;Guy Leroy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaw_A/0/1/0/all/0/1"&gt;Ali Shaw&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1"&gt;Katja Hofmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Payload Optimization Method for Federated Recommender Systems. (arXiv:2107.13078v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13078</id>
        <link href="http://arxiv.org/abs/2107.13078"/>
        <updated>2021-07-29T02:00:09.606Z</updated>
        <summary type="html"><![CDATA[We introduce the payload optimization method for federated recommender
systems (FRS). In federated learning (FL), the global model payload that is
moved between the server and users depends on the number of items to recommend.
The model payload grows when there is an increasing number of items. This
becomes challenging for an FRS if it is running in production mode. To tackle
the payload challenge, we formulated a multi-arm bandit solution that selected
part of the global model and transmitted it to all users. The selection process
was guided by a novel reward function suitable for FL systems. So far as we are
aware, this is the first optimization method that seeks to address item
dependent payloads. The method was evaluated using three benchmark
recommendation datasets. The empirical validation confirmed that the proposed
method outperforms the simpler methods that do not benefit from the bandits for
the purpose of item selection. In addition, we have demonstrated the usefulness
of our proposed method by rigorously evaluating the effects of a payload
reduction on the recommendation performance degradation. Our method achieved up
to a 90\% reduction in model payload, yielding only a $\sim$4\% - 8\% loss in
the recommendation performance for highly sparse datasets]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1"&gt;Farwa K. Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flanagan_A/0/1/0/all/0/1"&gt;Adrian Flanagan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1"&gt;Kuan E. Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alamgir_Z/0/1/0/all/0/1"&gt;Zareen Alamgir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ammad_Ud_Din_M/0/1/0/all/0/1"&gt;Muhammad Ammad-Ud-Din&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating the Use of Reconstruction Error for Novelty Localization. (arXiv:2107.13379v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13379</id>
        <link href="http://arxiv.org/abs/2107.13379"/>
        <updated>2021-07-29T02:00:09.599Z</updated>
        <summary type="html"><![CDATA[The pixelwise reconstruction error of deep autoencoders is often utilized for
image novelty detection and localization under the assumption that pixels with
high error indicate which parts of the input image are unfamiliar and therefore
likely to be novel. This assumed correlation between pixels with high
reconstruction error and novel regions of input images has not been verified
and may limit the accuracy of these methods. In this paper we utilize saliency
maps to evaluate whether this correlation exists. Saliency maps reveal directly
how much a change in each input pixel would affect reconstruction loss, while
each pixel's reconstruction error may be attributed to many input pixels when
layers are fully connected. We compare saliency maps to reconstruction error
maps via qualitative visualizations as well as quantitative correspondence
between the top K elements of the maps for both novel and normal images. Our
results indicate that reconstruction error maps do not closely correlate with
the importance of pixels in the input images, making them insufficient for
novelty localization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feeney_P/0/1/0/all/0/1"&gt;Patrick Feeney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1"&gt;Michael C. Hughes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel CropdocNet for Automated Potato Late Blight Disease Detection from the Unmanned Aerial Vehicle-based Hyperspectral Imagery. (arXiv:2107.13277v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13277</id>
        <link href="http://arxiv.org/abs/2107.13277"/>
        <updated>2021-07-29T02:00:09.592Z</updated>
        <summary type="html"><![CDATA[Late blight disease is one of the most destructive diseases in potato crop,
leading to serious yield losses globally. Accurate diagnosis of the disease at
early stage is critical for precision disease control and management. Current
farm practices in crop disease diagnosis are based on manual visual inspection,
which is costly, time consuming, subject to individual bias. Recent advances in
imaging sensors (e.g. RGB, multiple spectral and hyperspectral cameras), remote
sensing and machine learning offer the opportunity to address this challenge.
Particularly, hyperspectral imagery (HSI) combining with machine learning/deep
learning approaches is preferable for accurately identifying specific plant
diseases because the HSI consists of a wide range of high-quality reflectance
information beyond human vision, capable of capturing both spectral-spatial
information. The proposed method considers the potential disease specific
reflectance radiation variance caused by the canopy structural diversity,
introduces the multiple capsule layers to model the hierarchical structure of
the spectral-spatial disease attributes with the encapsulated features to
represent the various classes and the rotation invariance of the disease
attributes in the feature space. We have evaluated the proposed method with the
real UAV-based HSI data under the controlled field conditions. The
effectiveness of the hierarchical features has been quantitatively assessed and
compared with the existing representative machine learning/deep learning
methods. The experiment results show that the proposed model significantly
improves the accuracy performance when considering hierarchical-structure of
spectral-spatial features, comparing to the existing methods only using
spectral, or spatial or spectral-spatial features without consider
hierarchical-structure of spectral-spatial features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yue Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1"&gt;Liangxiu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kleerekoper_A/0/1/0/all/0/1"&gt;Anthony Kleerekoper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Sheng Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1"&gt;Tongle Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Wireless Sensor Anomaly Detection based on Data Stream in Edge Computing Enabled Smart Greenhouse. (arXiv:2107.13353v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13353</id>
        <link href="http://arxiv.org/abs/2107.13353"/>
        <updated>2021-07-29T02:00:09.585Z</updated>
        <summary type="html"><![CDATA[Edge computing enabled smart greenhouse is a representative application of
Internet of Things technology, which can monitor the environmental information
in real time and employ the information to contribute to intelligent
decision-making. In the process, anomaly detection for wireless sensor data
plays an important role. However, traditional anomaly detection algorithms
originally designed for anomaly detection in static data have not properly
considered the inherent characteristics of data stream produced by wireless
sensor such as infiniteness, correlations and concept drift, which may pose a
considerable challenge on anomaly detection based on data stream, and lead to
low detection accuracy and efficiency. First, data stream usually generates
quickly which means that it is infinite and enormous, so any traditional
off-line anomaly detection algorithm that attempts to store the whole dataset
or to scan the dataset multiple times for anomaly detection will run out of
memory space. Second, there exist correlations among different data streams,
which traditional algorithms hardly consider. Third, the underlying data
generation process or data distribution may change over time. Thus, traditional
anomaly detection algorithms with no model update will lose their effects.
Considering these issues, a novel method (called DLSHiForest) on basis of
Locality-Sensitive Hashing and time window technique in this paper is proposed
to solve these problems while achieving accurate and efficient detection.
Comprehensive experiments are executed using real-world agricultural greenhouse
dataset to demonstrate the feasibility of our approach. Experimental results
show that our proposal is practicable in addressing challenges of traditional
anomaly detection while ensuring accuracy and efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yihong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1"&gt;Sheng Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuwen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_S/0/1/0/all/0/1"&gt;Shunmei Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_X/0/1/0/all/0/1"&gt;Xiaoxiao Chi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1"&gt;Rui Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1"&gt;Chao Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Models of Computational Profiles to Study the Likelihood of DNN Metamorphic Test Cases. (arXiv:2107.13491v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13491</id>
        <link href="http://arxiv.org/abs/2107.13491"/>
        <updated>2021-07-29T02:00:09.561Z</updated>
        <summary type="html"><![CDATA[Neural network test cases are meant to exercise different reasoning paths in
an architecture and used to validate the prediction outcomes. In this paper, we
introduce "computational profiles" as vectors of neuron activation levels. We
investigate the distribution of computational profile likelihood of metamorphic
test cases with respect to the likelihood distributions of training, test and
error control cases. We estimate the non-parametric probability densities of
neuron activation levels for each distinct output class. Probabilities are
inferred using training cases only, without any additional knowledge about
metamorphic test cases. Experiments are performed by training a network on the
MNIST Fashion library of images and comparing prediction likelihoods with those
obtained from error control-data and from metamorphic test cases. Experimental
results show that the distributions of computational profile likelihood for
training and test cases are somehow similar, while the distribution of the
random-noise control-data is always remarkably lower than the observed one for
the training and testing sets. In contrast, metamorphic test cases show a
prediction likelihood that lies in an extended range with respect to training,
tests, and random noise. Moreover, the presented approach allows the
independent assessment of different training classes and experiments to show
that some of the classes are more sensitive to misclassifying metamorphic test
cases than other classes. In conclusion, metamorphic test cases represent very
aggressive tests for neural network architectures. Furthermore, since
metamorphic test cases force a network to misclassify those inputs whose
likelihood is similar to that of training cases, they could also be considered
as adversarial attacks that evade defenses based on computational profile
likelihood evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Merlo_E/0/1/0/all/0/1"&gt;Ettore Merlo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marhaba_M/0/1/0/all/0/1"&gt;Mira Marhaba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1"&gt;Foutse Khomh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Braiek_H/0/1/0/all/0/1"&gt;Houssem Ben Braiek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antoniol_G/0/1/0/all/0/1"&gt;Giuliano Antoniol&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual-wav2vec2: an Application of Continual Learning for Self-Supervised Automatic Speech Recognition. (arXiv:2107.13530v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.13530</id>
        <link href="http://arxiv.org/abs/2107.13530"/>
        <updated>2021-07-29T02:00:09.543Z</updated>
        <summary type="html"><![CDATA[We present a method for continual learning of speech representations for
multiple languages using self-supervised learning (SSL) and applying these for
automatic speech recognition. There is an abundance of unannotated speech, so
creating self-supervised representations from raw audio and finetuning on a
small annotated datasets is a promising direction to build speech recognition
systems. Wav2vec models perform SSL on raw audio in a pretraining phase and
then finetune on a small fraction of annotated data. SSL models have produced
state of the art results for ASR. However, these models are very expensive to
pretrain with self-supervision. We tackle the problem of learning new language
representations continually from audio without forgetting a previous language
representation. We use ideas from continual learning to transfer knowledge from
a previous task to speed up pretraining a new language task. Our
continual-wav2vec2 model can decrease pretraining times by 32% when learning a
new language task, and learn this new audio-language representation without
forgetting previous language representation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kessler_S/0/1/0/all/0/1"&gt;Samuel Kessler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thomas_B/0/1/0/all/0/1"&gt;Bethan Thomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Karout_S/0/1/0/all/0/1"&gt;Salah Karout&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incentivizing Compliance with Algorithmic Instruments. (arXiv:2107.10093v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.10093</id>
        <link href="http://arxiv.org/abs/2107.10093"/>
        <updated>2021-07-29T02:00:09.515Z</updated>
        <summary type="html"><![CDATA[Randomized experiments can be susceptible to selection bias due to potential
non-compliance by the participants. While much of the existing work has studied
compliance as a static behavior, we propose a game-theoretic model to study
compliance as dynamic behavior that may change over time. In rounds, a social
planner interacts with a sequence of heterogeneous agents who arrive with their
unobserved private type that determines both their prior preferences across the
actions (e.g., control and treatment) and their baseline rewards without taking
any treatment. The planner provides each agent with a randomized recommendation
that may alter their beliefs and their action selection. We develop a novel
recommendation mechanism that views the planner's recommendation as a form of
instrumental variable (IV) that only affects an agents' action selection, but
not the observed rewards. We construct such IVs by carefully mapping the
history -- the interactions between the planner and the previous agents -- to a
random recommendation. Even though the initial agents may be completely
non-compliant, our mechanism can incentivize compliance over time, thereby
enabling the estimation of the treatment effect of each treatment, and
minimizing the cumulative regret of the planner whose goal is to identify the
optimal treatment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_D/0/1/0/all/0/1"&gt;Daniel Ngo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stapleton_L/0/1/0/all/0/1"&gt;Logan Stapleton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Syrgkanis_V/0/1/0/all/0/1"&gt;Vasilis Syrgkanis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhiwei Steven Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effective Eigendecomposition based Graph Adaptation for Heterophilic Networks. (arXiv:2107.13312v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13312</id>
        <link href="http://arxiv.org/abs/2107.13312"/>
        <updated>2021-07-29T02:00:09.507Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) exhibit excellent performance when graphs have
strong homophily property, i.e. connected nodes have the same labels. However,
they perform poorly on heterophilic graphs. Several approaches address the
issue of heterophily by proposing models that adapt the graph by optimizing
task-specific loss function using labelled data. These adaptations are made
either via attention or by attenuating or enhancing various
low-frequency/high-frequency signals, as needed for the task at hand. More
recent approaches adapt the eigenvalues of the graph. One important
interpretation of this adaptation is that these models select/weigh the
eigenvectors of the graph. Based on this interpretation, we present an
eigendecomposition based approach and propose EigenNetwork models that improve
the performance of GNNs on heterophilic graphs. Performance improvement is
achieved by learning flexible graph adaptation functions that modulate the
eigenvalues of the graph. Regularization of these functions via parameter
sharing helps to improve the performance even more. Our approach achieves up to
11% improvement in performance over the state-of-the-art methods on
heterophilic graphs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lingam_V/0/1/0/all/0/1"&gt;Vijay Lingam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ragesh_R/0/1/0/all/0/1"&gt;Rahul Ragesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyer_A/0/1/0/all/0/1"&gt;Arun Iyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sellamanickam_S/0/1/0/all/0/1"&gt;Sundararajan Sellamanickam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Retinal Microvasculature as Biomarker for Diabetes and Cardiovascular Diseases. (arXiv:2107.13157v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.13157</id>
        <link href="http://arxiv.org/abs/2107.13157"/>
        <updated>2021-07-29T02:00:09.498Z</updated>
        <summary type="html"><![CDATA[Purpose: To demonstrate that retinal microvasculature per se is a reliable
biomarker for Diabetic Retinopathy (DR) and, by extension, cardiovascular
diseases. Methods: Deep Learning Convolutional Neural Networks (CNN) applied to
color fundus images for semantic segmentation of the blood vessels and severity
classification on both vascular and full images. Vessel reconstruction through
harmonic descriptors is also used as a smoothing and de-noising tool. The
mathematical background of the theory is also outlined. Results: For diabetic
patients, at least 93.8% of DR No-Refer vs. Refer classification can be related
to vasculature defects. As for the Non-Sight Threatening vs. Sight Threatening
case, the ratio is as high as 96.7%. Conclusion: In the case of DR, most of the
disease biomarkers are related topologically to the vasculature. Translational
Relevance: Experiments conducted on eye blood vasculature reconstruction as a
biomarker shows a strong correlation between vasculature shape and later stages
of DR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Trivedi_A/0/1/0/all/0/1"&gt;Anusua Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Desbiens_J/0/1/0/all/0/1"&gt;Jocelyn Desbiens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gross_R/0/1/0/all/0/1"&gt;Ron Gross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Sunil Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dodhia_R/0/1/0/all/0/1"&gt;Rahul Dodhia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ferres_J/0/1/0/all/0/1"&gt;Juan Lavista Ferres&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SimROD: A Simple Adaptation Method for Robust Object Detection. (arXiv:2107.13389v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13389</id>
        <link href="http://arxiv.org/abs/2107.13389"/>
        <updated>2021-07-29T02:00:09.477Z</updated>
        <summary type="html"><![CDATA[This paper presents a Simple and effective unsupervised adaptation method for
Robust Object Detection (SimROD). To overcome the challenging issues of domain
shift and pseudo-label noise, our method integrates a novel domain-centric
augmentation method, a gradual self-labeling adaptation procedure, and a
teacher-guided fine-tuning mechanism. Using our method, target domain samples
can be leveraged to adapt object detection models without changing the model
architecture or generating synthetic data. When applied to image corruptions
and high-level cross-domain adaptation benchmarks, our method outperforms prior
baselines on multiple domain adaptation benchmarks. SimROD achieves new
state-of-the-art on standard real-to-synthetic and cross-camera setup
benchmarks. On the image corruption benchmark, models adapted with our method
achieved a relative robustness improvement of 15-25% AP50 on Pascal-C and 5-6%
AP on COCO-C and Cityscapes-C. On the cross-domain benchmark, our method
outperformed the best baseline performance by up to 8% AP50 on Comic dataset
and up to 4% on Watercolor dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ramamonjison_R/0/1/0/all/0/1"&gt;Rindra Ramamonjison&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banitalebi_Dehkordi_A/0/1/0/all/0/1"&gt;Amin Banitalebi-Dehkordi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1"&gt;Xinyu Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1"&gt;Xiaolong Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vowel-based Meeteilon dialect identification using a Random Forest classifier. (arXiv:2107.13419v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.13419</id>
        <link href="http://arxiv.org/abs/2107.13419"/>
        <updated>2021-07-29T02:00:09.466Z</updated>
        <summary type="html"><![CDATA[This paper presents a vowel-based dialect identification system for
Meeteilon. For this work, a vowel dataset is created by using Meeteilon Speech
Corpora available at Linguistic Data Consortium for Indian Languages (LDC-IL).
Spectral features such as formant frequencies (F1, F1 and F3) and prosodic
features such as pitch (F0), energy, intensity and segment duration values are
extracted from monophthong vowel sounds. Random forest classifier, a decision
tree-based ensemble algorithm is used for classification of three major
dialects of Meeteilon namely, Imphal, Kakching and Sekmai. Model has shown an
average dialect identification performance in terms of accuracy of around
61.57%. The role of spectral and prosodic features are found to be significant
in Meeteilon dialect classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Devi_T/0/1/0/all/0/1"&gt;Thangjam Clarinda Devi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thaoroijam_K/0/1/0/all/0/1"&gt;Kabita Thaoroijam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[XFL: eXtreme Function Labeling. (arXiv:2107.13404v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.13404</id>
        <link href="http://arxiv.org/abs/2107.13404"/>
        <updated>2021-07-29T02:00:09.448Z</updated>
        <summary type="html"><![CDATA[Reverse engineers would benefit from identifiers like function names, but
these are usually unavailable in binaries. Training a machine learning model to
predict function names automatically is promising but fundamentally hard due to
the enormous number of classes. In this paper, we introduce eXtreme Function
Labeling (XFL), an extreme multi-label learning approach to selecting
appropriate labels for binary functions. XFL splits function names into tokens,
treating each as an informative label akin to the problem of tagging texts in
natural language. To capture the semantics of binary code, we introduce DEXTER,
a novel function embedding that combines static analysis-based features with
local context from the call graph and global context from the entire binary. We
demonstrate that XFL outperforms state-of-the-art approaches to function
labeling on a dataset of over 10,000 binaries from the Debian project,
achieving a precision of 82.5%. We also study combinations of XFL with
different published embeddings for binary functions and show that DEXTER
consistently improves over the state of the art in information gain. As a
result, we are able to show that binary function labeling is best phrased in
terms of multi-label learning, and that binary function embeddings benefit from
moving beyond just learning from syntax.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patrick_Evans_J/0/1/0/all/0/1"&gt;James Patrick-Evans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dannehl_M/0/1/0/all/0/1"&gt;Moritz Dannehl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kinder_J/0/1/0/all/0/1"&gt;Johannes Kinder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Snippet Policy Network for Multi-class Varied-length ECG Early Classification. (arXiv:2107.13361v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13361</id>
        <link href="http://arxiv.org/abs/2107.13361"/>
        <updated>2021-07-29T02:00:09.439Z</updated>
        <summary type="html"><![CDATA[Arrhythmia detection from ECG is an important research subject in the
prevention and diagnosis of cardiovascular diseases. The prevailing studies
formulate arrhythmia detection from ECG as a time series classification
problem. Meanwhile, early detection of arrhythmia presents a real-world demand
for early prevention and diagnosis. In this paper, we address a problem of
cardiovascular disease early classification, which is a varied-length and
long-length time series early classification problem as well. For solving this
problem, we propose a deep reinforcement learning-based framework, namely
Snippet Policy Network (SPN), consisting of four modules, snippet generator,
backbone network, controlling agent, and discriminator. Comparing to the
existing approaches, the proposed framework features flexible input length,
solves the dual-optimization solution of the earliness and accuracy goals.
Experimental results demonstrate that SPN achieves an excellent performance of
over 80\% in terms of accuracy. Compared to the state-of-the-art methods, at
least 7% improvement on different metrics, including the precision, recall,
F1-score, and harmonic mean, is delivered by the proposed SPN. To the best of
our knowledge, this is the first work focusing on solving the cardiovascular
early classification problem based on varied-length ECG data. Based on these
excellent features from SPN, it offers a good exemplification for addressing
all kinds of varied-length time series early classification problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yen_G/0/1/0/all/0/1"&gt;Gary G. Yen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tseng_V/0/1/0/all/0/1"&gt;Vincent S. Tseng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonlinear State Space Modeling and Control of the Impact of Patients' Modifiable Lifestyle Behaviors on the Emergence of Multiple Chronic Conditions. (arXiv:2107.13394v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2107.13394</id>
        <link href="http://arxiv.org/abs/2107.13394"/>
        <updated>2021-07-29T02:00:09.432Z</updated>
        <summary type="html"><![CDATA[The emergence and progression of multiple chronic conditions (MCC) over time
often form a dynamic network that depends on patient's modifiable risk factors
and their interaction with non-modifiable risk factors and existing conditions.
Continuous time Bayesian networks (CTBNs) are effective methods for modeling
the complex network of MCC relationships over time. However, CTBNs are not able
to effectively formulate the dynamic impact of patient's modifiable risk
factors on the emergence and progression of MCC. Considering a functional CTBN
(FCTBN) to represent the underlying structure of the MCC relationships with
respect to individuals' risk factors and existing conditions, we propose a
nonlinear state-space model based on Extended Kalman filter (EKF) to capture
the dynamics of the patients' modifiable risk factors and existing conditions
on the MCC evolution over time. We also develop a tensor control chart to
dynamically monitor the effect of changes in the modifiable risk factors of
individual patients on the risk of new chronic conditions emergence. We
validate the proposed approach based on a combination of simulation and real
data from a dataset of 385 patients from Cameron County Hispanic Cohort (CCHC)
over multiple years. The dataset examines the emergence of 5 chronic conditions
(Diabetes, Obesity, Cognitive Impairment, Hyperlipidemia, and Hypertension)
based on 4 modifiable risk factors representing lifestyle behaviors (Diet,
Exercise, Smoking Habit, and Drinking Habit) and 3 non-modifiable risk factors,
including demographic information (Age, Gender, Education). The results
demonstrate the effectiveness of the proposed methodology for dynamic
prediction and monitoring of the risk of MCC emergence in individual patients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Faruqui_S/0/1/0/all/0/1"&gt;Syed Hasib Akhter Faruqui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Alaeddini_A/0/1/0/all/0/1"&gt;Adel Alaeddini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Fisher_Hoch_S/0/1/0/all/0/1"&gt;Susan P Fisher-Hoch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mccormic_J/0/1/0/all/0/1"&gt;Joseph B Mccormic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Unstructured Handwashing Recognition using Smartwatch to Reduce Contact Transmission of Pathogens. (arXiv:2107.13405v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13405</id>
        <link href="http://arxiv.org/abs/2107.13405"/>
        <updated>2021-07-29T02:00:09.424Z</updated>
        <summary type="html"><![CDATA[Current guidelines from the World Health Organization indicate that the
SARSCoV-2 coronavirus, which results in the novel coronavirus disease
(COVID-19), is transmitted through respiratory droplets or by contact. Contact
transmission occurs when contaminated hands touch the mucous membrane of the
mouth, nose, or eyes. Moreover, pathogens can also be transferred from one
surface to another by contaminated hands, which facilitates transmission by
indirect contact. Consequently, hands hygiene is extremely important to prevent
the spread of the SARSCoV-2 virus. Additionally, hand washing and/or hand
rubbing disrupts also the transmission of other viruses and bacteria that cause
common colds, flu and pneumonia, thereby reducing the overall disease burden.
The vast proliferation of wearable devices, such as smartwatches, containing
acceleration, rotation, magnetic field sensors, etc., together with the modern
technologies of artificial intelligence, such as machine learning and more
recently deep-learning, allow the development of accurate applications for
recognition and classification of human activities such as: walking, climbing
stairs, running, clapping, sitting, sleeping, etc. In this work we evaluate the
feasibility of an automatic system, based on current smartwatches, which is
able to recognize when a subject is washing or rubbing its hands, in order to
monitor parameters such as frequency and duration, and to evaluate the
effectiveness of the gesture. Our preliminary results show a classification
accuracy of about 95% and of about 94% for respectively deep and standard
learning techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lattanzi_E/0/1/0/all/0/1"&gt;Emanuele Lattanzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calisti_L/0/1/0/all/0/1"&gt;Lorenzo Calisti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freschi_V/0/1/0/all/0/1"&gt;Valerio Freschi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Neural Schema Alignment for OpenStreetMap and Knowledge Graphs. (arXiv:2107.13257v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13257</id>
        <link href="http://arxiv.org/abs/2107.13257"/>
        <updated>2021-07-29T02:00:09.404Z</updated>
        <summary type="html"><![CDATA[OpenStreetMap (OSM) is one of the richest openly available sources of
volunteered geographic information. Although OSM includes various geographical
entities, their descriptions are highly heterogeneous, incomplete, and do not
follow any well-defined ontology. Knowledge graphs can potentially provide
valuable semantic information to enrich OSM entities. However, interlinking OSM
entities with knowledge graphs is inherently difficult due to the large,
heterogeneous, ambiguous, and flat OSM schema and the annotation sparsity. This
paper tackles the alignment of OSM tags with the corresponding knowledge graph
classes holistically by jointly considering the schema and instance layers. We
propose a novel neural architecture that capitalizes upon a shared latent space
for tag-to-class alignment created using linked entities in OSM and knowledge
graphs. Our experiments performed to align OSM datasets for several countries
with two of the most prominent openly available knowledge graphs, namely,
Wikidata and DBpedia, demonstrate that the proposed approach outperforms the
state-of-the-art schema alignment baselines by up to 53 percentage points in
terms of F1-score. The resulting alignment facilitates new semantic annotations
for over 10 million OSM entities worldwide, which is more than a 400% increase
compared to the existing semantic annotations in OSM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dsouza_A/0/1/0/all/0/1"&gt;Alishiba Dsouza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tempelmeier_N/0/1/0/all/0/1"&gt;Nicolas Tempelmeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demidova_E/0/1/0/all/0/1"&gt;Elena Demidova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SONG: Self-Organizing Neural Graphs. (arXiv:2107.13214v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13214</id>
        <link href="http://arxiv.org/abs/2107.13214"/>
        <updated>2021-07-29T02:00:09.397Z</updated>
        <summary type="html"><![CDATA[Recent years have seen a surge in research on deep interpretable neural
networks with decision trees as one of the most commonly incorporated tools.
There are at least three advantages of using decision trees over logistic
regression classification models: they are easy to interpret since they are
based on binary decisions, they can make decisions faster, and they provide a
hierarchy of classes. However, one of the well-known drawbacks of decision
trees, as compared to decision graphs, is that decision trees cannot reuse the
decision nodes. Nevertheless, decision graphs were not commonly used in deep
learning due to the lack of efficient gradient-based training techniques. In
this paper, we fill this gap and provide a general paradigm based on Markov
processes, which allows for efficient training of the special type of decision
graphs, which we call Self-Organizing Neural Graphs (SONG). We provide an
extensive theoretical study of SONG, complemented by experiments conducted on
Letter, Connect4, MNIST, CIFAR, and TinyImageNet datasets, showing that our
method performs on par or better than existing decision models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Struski_L/0/1/0/all/0/1"&gt;&amp;#x141;ukasz Struski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Danel_T/0/1/0/all/0/1"&gt;Tomasz Danel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smieja_M/0/1/0/all/0/1"&gt;Marek &amp;#x15a;mieja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1"&gt;Jacek Tabor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zielinski_B/0/1/0/all/0/1"&gt;Bartosz Zieli&amp;#x144;ski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Autoencoders for Drift Detection in Industrial Environments. (arXiv:2107.13249v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13249</id>
        <link href="http://arxiv.org/abs/2107.13249"/>
        <updated>2021-07-29T02:00:09.388Z</updated>
        <summary type="html"><![CDATA[Autoencoders are unsupervised models which have been used for detecting
anomalies in multi-sensor environments. A typical use includes training a
predictive model with data from sensors operating under normal conditions and
using the model to detect anomalies. Anomalies can come either from real
changes in the environment (real drift) or from faulty sensory devices (virtual
drift); however, the use of Autoencoders to distinguish between different
anomalies has not yet been considered. To this end, we first propose the
development of Bayesian Autoencoders to quantify epistemic and aleatoric
uncertainties. We then test the Bayesian Autoencoder using a real-world
industrial dataset for hydraulic condition monitoring. The system is injected
with noise and drifts, and we have found the epistemic uncertainty to be less
sensitive to sensor perturbations as compared to the reconstruction loss. By
observing the reconstructed signals with the uncertainties, we gain
interpretable insights, and these uncertainties offer a potential avenue for
distinguishing real and virtual drifts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yong_B/0/1/0/all/0/1"&gt;Bang Xiang Yong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fathy_Y/0/1/0/all/0/1"&gt;Yasmin Fathy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brintrup_A/0/1/0/all/0/1"&gt;Alexandra Brintrup&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning with Multiclass AUC: Theory and Algorithms. (arXiv:2107.13171v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13171</id>
        <link href="http://arxiv.org/abs/2107.13171"/>
        <updated>2021-07-29T02:00:09.381Z</updated>
        <summary type="html"><![CDATA[The Area under the ROC curve (AUC) is a well-known ranking metric for
problems such as imbalanced learning and recommender systems. The vast majority
of existing AUC-optimization-based machine learning methods only focus on
binary-class cases, while leaving the multiclass cases unconsidered. In this
paper, we start an early trial to consider the problem of learning multiclass
scoring functions via optimizing multiclass AUC metrics. Our foundation is
based on the M metric, which is a well-known multiclass extension of AUC. We
first pay a revisit to this metric, showing that it could eliminate the
imbalance issue from the minority class pairs. Motivated by this, we propose an
empirical surrogate risk minimization framework to approximately optimize the M
metric. Theoretically, we show that: (i) optimizing most of the popular
differentiable surrogate losses suffices to reach the Bayes optimal scoring
function asymptotically; (ii) the training framework enjoys an imbalance-aware
generalization error bound, which pays more attention to the bottleneck samples
of minority classes compared with the traditional $O(\sqrt{1/N})$ result.
Practically, to deal with the low scalability of the computational operations,
we propose acceleration methods for three popular surrogate loss functions,
including the exponential loss, squared loss, and hinge loss, to speed up loss
and gradient evaluations. Finally, experimental results on 11 real-world
datasets demonstrate the effectiveness of our proposed framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhiyong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1"&gt;Qianqian Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1"&gt;Shilong Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1"&gt;Xiaochun Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qingming Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepTeeth: A Teeth-photo Based Human Authentication System for Mobile and Hand-held Devices. (arXiv:2107.13217v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13217</id>
        <link href="http://arxiv.org/abs/2107.13217"/>
        <updated>2021-07-29T02:00:09.373Z</updated>
        <summary type="html"><![CDATA[This paper proposes teeth-photo, a new biometric modality for human
authentication on mobile and hand held devices. Biometrics samples are acquired
using the camera mounted on mobile device with the help of a mobile application
having specific markers to register the teeth area. Region of interest (RoI) is
then extracted using the markers and the obtained sample is enhanced using
contrast limited adaptive histogram equalization (CLAHE) for better visual
clarity. We propose a deep learning architecture and novel regularization
scheme to obtain highly discriminative embedding for small size RoI. Proposed
custom loss function was able to achieve perfect classification for the tiny
RoI of $75\times 75$ size. The model is end-to-end and few-shot and therefore
is very efficient in terms of time and energy requirements. The system can be
used in many ways including device unlocking and secure authentication. To the
best of our understanding, this is the first work on teeth-photo based
authentication for mobile device. Experiments have been conducted on an
in-house teeth-photo database collected using our application. The database is
made publicly available. Results have shown that the proposed system has
perfect accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arora_G/0/1/0/all/0/1"&gt;Geetika Arora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bharadwaj_R/0/1/0/all/0/1"&gt;Rohit K Bharadwaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tiwari_K/0/1/0/all/0/1"&gt;Kamlesh Tiwari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoML Meets Time Series Regression Design and Analysis of the AutoSeries Challenge. (arXiv:2107.13186v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13186</id>
        <link href="http://arxiv.org/abs/2107.13186"/>
        <updated>2021-07-29T02:00:09.353Z</updated>
        <summary type="html"><![CDATA[Analyzing better time series with limited human effort is of interest to
academia and industry. Driven by business scenarios, we organized the first
Automated Time Series Regression challenge (AutoSeries) for the WSDM Cup 2020.
We present its design, analysis, and post-hoc experiments. The code submission
requirement precluded participants from any manual intervention, testing
automated machine learning capabilities of solutions, across many datasets,
under hardware and time limitations. We prepared 10 datasets from diverse
application domains (sales, power consumption, air quality, traffic, and
parking), featuring missing data, mixed continuous and categorical variables,
and various sampling rates. Each dataset was split into a training and a test
sequence (which was streamed, allowing models to continuously adapt). The
setting of time series regression, differs from classical forecasting in that
covariates at the present time are known. Great strides were made by
participants to tackle this AutoSeries problem, as demonstrated by the jump in
performance from the sample submission, and post-hoc comparisons with
AutoGluon. Simple yet effective methods were used, based on feature
engineering, LightGBM, and random search hyper-parameter tuning, addressing all
aspects of the challenge. Our post-hoc analyses revealed that providing
additional time did not yield significant improvements. The winners' code was
open-sourced https://www.4paradigm.com/competition/autoseries2020.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_W/0/1/0/all/0/1"&gt;Wei-Wei Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guyon_I/0/1/0/all/0/1"&gt;Isabelle Guyon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Graph Convolutional-Recurrent Neural Network (MGC-RNN) for Short-Term Forecasting of Transit Passenger Flow. (arXiv:2107.13226v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13226</id>
        <link href="http://arxiv.org/abs/2107.13226"/>
        <updated>2021-07-29T02:00:09.345Z</updated>
        <summary type="html"><![CDATA[Short-term forecasting of passenger flow is critical for transit management
and crowd regulation. Spatial dependencies, temporal dependencies,
inter-station correlations driven by other latent factors, and exogenous
factors bring challenges to the short-term forecasts of passenger flow of urban
rail transit networks. An innovative deep learning approach, Multi-Graph
Convolutional-Recurrent Neural Network (MGC-RNN) is proposed to forecast
passenger flow in urban rail transit systems to incorporate these complex
factors. We propose to use multiple graphs to encode the spatial and other
heterogenous inter-station correlations. The temporal dynamics of the
inter-station correlations are also modeled via the proposed multi-graph
convolutional-recurrent neural network structure. Inflow and outflow of all
stations can be collectively predicted with multiple time steps ahead via a
sequence to sequence(seq2seq) architecture. The proposed method is applied to
the short-term forecasts of passenger flow in Shenzhen Metro, China. The
experimental results show that MGC-RNN outperforms the benchmark algorithms in
terms of forecasting accuracy. Besides, it is found that the inter-station
driven by network distance, network structure, and recent flow patterns are
significant factors for passenger flow forecasting. Moreover, the architecture
of LSTM-encoder-decoder can capture the temporal dependencies well. In general,
the proposed framework could provide multiple views of passenger flow dynamics
for fine prediction and exhibit a possibility for multi-source heterogeneous
data fusion in the spatiotemporal forecast tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yuxin He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lishuai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xinting Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsui_K/0/1/0/all/0/1"&gt;Kwok Leung Tsui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Hybrid Inference in State-Space Models. (arXiv:2107.13349v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13349</id>
        <link href="http://arxiv.org/abs/2107.13349"/>
        <updated>2021-07-29T02:00:09.338Z</updated>
        <summary type="html"><![CDATA[We perform approximate inference in state-space models that allow for
nonlinear higher-order Markov chains in latent space. The conditional
independencies of the generative model enable us to parameterize only an
inference model, which learns to estimate clean states in a self-supervised
manner using maximum likelihood. First, we propose a recurrent method that is
trained directly on noisy observations. Afterward, we cast the model such that
the optimization problem leads to an update scheme that backpropagates through
a recursion similar to the classical Kalman filter and smoother. In scientific
applications, domain knowledge can give a linear approximation of the latent
transition maps. We can easily incorporate this knowledge into our model,
leading to a hybrid inference approach. In contrast to other methods,
experiments show that the hybrid method makes the inferred latent states
physically more interpretable and accurate, especially in low-data regimes.
Furthermore, we do not rely on an additional parameterization of the generative
model or supervision via uncorrupted observations or ground truth latent
states. Despite our model's simplicity, we obtain competitive results on the
chaotic Lorenz system compared to a fully supervised approach and outperform a
method based on variational inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruhe_D/0/1/0/all/0/1"&gt;David Ruhe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forre_P/0/1/0/all/0/1"&gt;Patrick Forr&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward AI Assistants That Let Designers Design. (arXiv:2107.13074v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2107.13074</id>
        <link href="http://arxiv.org/abs/2107.13074"/>
        <updated>2021-07-29T02:00:09.331Z</updated>
        <summary type="html"><![CDATA[AI for supporting designers needs to be rethought. It should aim to
cooperate, not automate, by supporting and leveraging the creativity and
problem-solving of designers. The challenge for such AI is how to infer
designers' goals and then help them without being needlessly disruptive. We
present AI-assisted design: a framework for creating such AI, built around
generative user models which enable reasoning about designers' goals,
reasoning, and capabilities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peuter_S/0/1/0/all/0/1"&gt;Sebastiaan De Peuter&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Oulasvirta_A/0/1/0/all/0/1"&gt;Antti Oulasvirta&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Kaski_S/0/1/0/all/0/1"&gt;Samuel Kaski&lt;/a&gt; (1 and 3) ((1) Department of Computer Science, Aalto University, Finland, (2) Department of Communications and Networking, Aalto University, Finland, (3) Department of Computer Science, University of Manchester, UK)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Learning of Neurosymbolic Encoders. (arXiv:2107.13132v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13132</id>
        <link href="http://arxiv.org/abs/2107.13132"/>
        <updated>2021-07-29T02:00:09.324Z</updated>
        <summary type="html"><![CDATA[We present a framework for the unsupervised learning of neurosymbolic
encoders, i.e., encoders obtained by composing neural networks with symbolic
programs from a domain-specific language. Such a framework can naturally
incorporate symbolic expert knowledge into the learning process and lead to
more interpretable and factorized latent representations than fully neural
encoders. Also, models learned this way can have downstream impact, as many
analysis workflows can benefit from having clean programmatic descriptions. We
ground our learning algorithm in the variational autoencoding (VAE) framework,
where we aim to learn a neurosymbolic encoder in conjunction with a standard
decoder. Our algorithm integrates standard VAE-style training with modern
program synthesis techniques. We evaluate our method on learning latent
representations for real-world trajectory data from animal biology and sports
analytics. We show that our approach offers significantly better separation
than standard VAEs and leads to practical gains on downstream tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_E/0/1/0/all/0/1"&gt;Eric Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jennifer J. Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kennedy_A/0/1/0/all/0/1"&gt;Ann Kennedy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1"&gt;Yisong Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1"&gt;Swarat Chaudhuri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi Agent System for Machine Learning Under Uncertainty in Cyber Physical Manufacturing System. (arXiv:2107.13252v1 [cs.MA])]]></title>
        <id>http://arxiv.org/abs/2107.13252</id>
        <link href="http://arxiv.org/abs/2107.13252"/>
        <updated>2021-07-29T02:00:09.318Z</updated>
        <summary type="html"><![CDATA[Recent advancements in predictive machine learning has led to its application
in various use cases in manufacturing. Most research focused on maximising
predictive accuracy without addressing the uncertainty associated with it.
While accuracy is important, focusing primarily on it poses an overfitting
danger, exposing manufacturers to risk, ultimately hindering the adoption of
these techniques. In this paper, we determine the sources of uncertainty in
machine learning and establish the success criteria of a machine learning
system to function well under uncertainty in a cyber-physical manufacturing
system (CPMS) scenario. Then, we propose a multi-agent system architecture
which leverages probabilistic machine learning as a means of achieving such
criteria. We propose possible scenarios for which our proposed architecture is
useful and discuss future work. Experimentally, we implement Bayesian Neural
Networks for multi-tasks classification on a public dataset for the real-time
condition monitoring of a hydraulic system and demonstrate the usefulness of
the system by evaluating the probability of a prediction being accurate given
its uncertainty. We deploy these models using our proposed agent-based
framework and integrate web visualisation to demonstrate its real-time
feasibility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yong_B/0/1/0/all/0/1"&gt;Bang Xiang Yong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brintrup_A/0/1/0/all/0/1"&gt;Alexandra Brintrup&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chance constrained conic-segmentation support vector machine with uncertain data. (arXiv:2107.13319v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13319</id>
        <link href="http://arxiv.org/abs/2107.13319"/>
        <updated>2021-07-29T02:00:09.297Z</updated>
        <summary type="html"><![CDATA[Support vector machines (SVM) is one of the well known supervised classes of
learning algorithms. Furthermore, the conic-segmentation SVM (CS-SVM) is a
natural multiclass analogue of the standard binary SVM, as CS-SVM models are
dealing with the situation where the exact values of the data points are known.
This paper studies CS-SVM when the data points are uncertain or mislabelled.
With some properties known for the distributions, a chance-constrained CS-SVM
approach is used to ensure the small probability of misclassification for the
uncertain data. The geometric interpretation is presented to show how CS-SVM
works. Finally, we present experimental results to investigate the chance
constrained CS-SVM's performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1"&gt;Shen Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Canessa_G/0/1/0/all/0/1"&gt;Gianpiero Canessa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meaning Versus Information, Prediction Versus Memory, and Question Versus Answer. (arXiv:2107.13393v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2107.13393</id>
        <link href="http://arxiv.org/abs/2107.13393"/>
        <updated>2021-07-29T02:00:09.291Z</updated>
        <summary type="html"><![CDATA[Brain science and artificial intelligence have made great progress toward the
understanding and engineering of the human mind. The progress has accelerated
significantly since the turn of the century thanks to new methods for probing
the brain (both structure and function), and rapid development in deep learning
research. However, despite these new developments, there are still many open
questions, such as how to understand the brain at the system level, and various
robustness issues and limitations of deep learning. In this informal essay, I
will talk about some of the concepts that are central to brain science and
artificial intelligence, such as information and memory, and discuss how a
different view on these concepts can help us move forward, beyond current
limits of our understanding in these fields.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Choe_Y/0/1/0/all/0/1"&gt;Yoonsuck Choe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interactive Storytelling for Children: A Case-study of Design and Development Considerations for Ethical Conversational AI. (arXiv:2107.13076v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2107.13076</id>
        <link href="http://arxiv.org/abs/2107.13076"/>
        <updated>2021-07-29T02:00:09.284Z</updated>
        <summary type="html"><![CDATA[Conversational Artificial Intelligence (CAI) systems and Intelligent Personal
Assistants (IPA), such as Alexa, Cortana, Google Home and Siri are becoming
ubiquitous in our lives, including those of children, the implications of which
is receiving increased attention, specifically with respect to the effects of
these systems on children's cognitive, social and linguistic development.
Recent advances address the implications of CAI with respect to privacy,
safety, security, and access. However, there is a need to connect and embed the
ethical and technical aspects in the design. Using a case-study of a research
and development project focused on the use of CAI in storytelling for children,
this paper reflects on the social context within a specific case of technology
development, as substantiated and supported by argumentation from within the
literature. It describes the decision making process behind the recommendations
made on this case for their adoption in the creative industries. Further
research that engages with developers and stakeholders in the ethics of
storytelling through CAI is highlighted as a matter of urgency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chubba_e/0/1/0/all/0/1"&gt;ennifer Chubba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Missaouib_S/0/1/0/all/0/1"&gt;Sondess Missaouib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Concannonc_S/0/1/0/all/0/1"&gt;Shauna Concannonc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maloneyb_L/0/1/0/all/0/1"&gt;Liam Maloneyb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walker_J/0/1/0/all/0/1"&gt;James Alfred Walker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pixyz: a library for developing deep generative models. (arXiv:2107.13109v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13109</id>
        <link href="http://arxiv.org/abs/2107.13109"/>
        <updated>2021-07-29T02:00:09.276Z</updated>
        <summary type="html"><![CDATA[With the recent rapid progress in the study of deep generative models (DGMs),
there is a need for a framework that can implement them in a simple and generic
way. In this research, we focus on two features of the latest DGMs: (1) deep
neural networks are encapsulated by probability distributions and (2) models
are designed and learned based on an objective function. Taking these features
into account, we propose a new DGM library called Pixyz. We experimentally show
that our library is faster than existing probabilistic modeling languages in
learning simple DGMs and we show that our library can be used to implement
complex DGMs in a simple and concise manner, which is difficult to do with
existing libraries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suzuki_M/0/1/0/all/0/1"&gt;Masahiro Suzuki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaneko_T/0/1/0/all/0/1"&gt;Takaaki Kaneko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsuo_Y/0/1/0/all/0/1"&gt;Yutaka Matsuo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Doing Great at Estimating CATE? On the Neglected Assumptions in Benchmark Comparisons of Treatment Effect Estimators. (arXiv:2107.13346v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13346</id>
        <link href="http://arxiv.org/abs/2107.13346"/>
        <updated>2021-07-29T02:00:09.269Z</updated>
        <summary type="html"><![CDATA[The machine learning toolbox for estimation of heterogeneous treatment
effects from observational data is expanding rapidly, yet many of its
algorithms have been evaluated only on a very limited set of semi-synthetic
benchmark datasets. In this paper, we show that even in arguably the simplest
setting -- estimation under ignorability assumptions -- the results of such
empirical evaluations can be misleading if (i) the assumptions underlying the
data-generating mechanisms in benchmark datasets and (ii) their interplay with
baseline algorithms are inadequately discussed. We consider two popular machine
learning benchmark datasets for evaluation of heterogeneous treatment effect
estimators -- the IHDP and ACIC2016 datasets -- in detail. We identify problems
with their current use and highlight that the inherent characteristics of the
benchmark datasets favor some algorithms over others -- a fact that is rarely
acknowledged but of immense relevance for interpretation of empirical results.
We close by discussing implications and possible next steps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Curth_A/0/1/0/all/0/1"&gt;Alicia Curth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1"&gt;Mihaela van der Schaar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Unsupervised Domain Adaptation with Conditional and Label Shift: Infer, Align and Iterate. (arXiv:2107.13469v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13469</id>
        <link href="http://arxiv.org/abs/2107.13469"/>
        <updated>2021-07-29T02:00:09.251Z</updated>
        <summary type="html"><![CDATA[In this work, we propose an adversarial unsupervised domain adaptation (UDA)
approach with the inherent conditional and label shifts, in which we aim to
align the distributions w.r.t. both $p(x|y)$ and $p(y)$. Since the label is
inaccessible in the target domain, the conventional adversarial UDA assumes
$p(y)$ is invariant across domains, and relies on aligning $p(x)$ as an
alternative to the $p(x|y)$ alignment. To address this, we provide a thorough
theoretical and empirical analysis of the conventional adversarial UDA methods
under both conditional and label shifts, and propose a novel and practical
alternative optimization scheme for adversarial UDA. Specifically, we infer the
marginal $p(y)$ and align $p(x|y)$ iteratively in the training, and precisely
align the posterior $p(y|x)$ in testing. Our experimental results demonstrate
its effectiveness on both classification and segmentation UDA, and partial UDA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1"&gt;Zhenhua Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Site Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1"&gt;Fangxu Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1"&gt;Jane You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1"&gt;C.-C. Jay Kuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1"&gt;Georges El Fakhri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1"&gt;Jonghye Woo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Policy Gradient Methods Find the Nash Equilibrium in N-player General-sum Linear-quadratic Games. (arXiv:2107.13090v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2107.13090</id>
        <link href="http://arxiv.org/abs/2107.13090"/>
        <updated>2021-07-29T02:00:09.243Z</updated>
        <summary type="html"><![CDATA[We consider a general-sum N-player linear-quadratic game with stochastic
dynamics over a finite horizon and prove the global convergence of the natural
policy gradient method to the Nash equilibrium. In order to prove the
convergence of the method, we require a certain amount of noise in the system.
We give a condition, essentially a lower bound on the covariance of the noise
in terms of the model parameters, in order to guarantee convergence. We
illustrate our results with numerical experiments to show that even in
situations where the policy gradient method may not converge in the
deterministic setting, the addition of noise leads to convergence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Hambly_B/0/1/0/all/0/1"&gt;Ben Hambly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Xu_R/0/1/0/all/0/1"&gt;Renyuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yang_H/0/1/0/all/0/1"&gt;Huining Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Removing Operational Friction Using Process Mining: Challenges Provided by the Internet of Production (IoP). (arXiv:2107.13066v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2107.13066</id>
        <link href="http://arxiv.org/abs/2107.13066"/>
        <updated>2021-07-29T02:00:09.236Z</updated>
        <summary type="html"><![CDATA[Operational processes in production, logistics, material handling,
maintenance, etc., are supported by cyber-physical systems combining hardware
and software components. As a result, the digital and the physical world are
closely aligned, and it is possible to track operational processes in detail
(e.g., using sensors). The abundance of event data generated by today's
operational processes provides opportunities and challenges for process mining
techniques supporting process discovery, performance analysis, and conformance
checking. Using existing process mining tools, it is already possible to
automatically discover process models and uncover performance and compliance
problems. In the DFG-funded Cluster of Excellence "Internet of Production"
(IoP), process mining is used to create "digital shadows" to improve a wide
variety of operational processes. However, operational processes are dynamic,
distributed, and complex. Driven by the challenges identified in the IoP
cluster, we work on novel techniques for comparative process mining (comparing
process variants for different products at different locations at different
times), object-centric process mining (to handle processes involving different
types of objects that interact), and forward-looking process mining (to explore
"What if?" questions). By addressing these challenges, we aim to develop
valuable "digital shadows" that can be used to remove operational friction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aalst_W/0/1/0/all/0/1"&gt;Wil van der Aalst&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brockhoff_T/0/1/0/all/0/1"&gt;Tobias Brockhoff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghahfarokhi_A/0/1/0/all/0/1"&gt;Anahita Farhang Ghahfarokhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pourbafrani_M/0/1/0/all/0/1"&gt;Mahsa Pourbafrani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uysal_M/0/1/0/all/0/1"&gt;Merih Seran Uysal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zelst_S/0/1/0/all/0/1"&gt;Sebastiaan van Zelst&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Homogeneous Architecture Augmentation for Neural Predictor. (arXiv:2107.13153v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13153</id>
        <link href="http://arxiv.org/abs/2107.13153"/>
        <updated>2021-07-29T02:00:09.228Z</updated>
        <summary type="html"><![CDATA[Neural Architecture Search (NAS) can automatically design well-performed
architectures of Deep Neural Networks (DNNs) for the tasks at hand. However,
one bottleneck of NAS is the prohibitively computational cost largely due to
the expensive performance evaluation. The neural predictors can directly
estimate the performance without any training of the DNNs to be evaluated, thus
have drawn increasing attention from researchers. Despite their popularity,
they also suffer a severe limitation: the shortage of annotated DNN
architectures for effectively training the neural predictors. In this paper, we
proposed Homogeneous Architecture Augmentation for Neural Predictor (HAAP) of
DNN architectures to address the issue aforementioned. Specifically, a
homogeneous architecture augmentation algorithm is proposed in HAAP to generate
sufficient training data taking the use of homogeneous representation.
Furthermore, the one-hot encoding strategy is introduced into HAAP to make the
representation of DNN architectures more effective. The experiments have been
conducted on both NAS-Benchmark-101 and NAS-Bench-201 dataset. The experimental
results demonstrate that the proposed HAAP algorithm outperforms the state of
the arts compared, yet with much less training data. In addition, the ablation
studies on both benchmark datasets have also shown the universality of the
homogeneous architecture augmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuqiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yehui Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yanan Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Q-Learning for Conflict Resolution in B5G Network Automation. (arXiv:2107.13268v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2107.13268</id>
        <link href="http://arxiv.org/abs/2107.13268"/>
        <updated>2021-07-29T02:00:09.218Z</updated>
        <summary type="html"><![CDATA[Network automation is gaining significant attention in the development of B5G
networks, primarily for reducing operational complexity, expenditures and
improving network efficiency. Concurrently operating closed loops aiming for
individual optimization targets may cause conflicts which, left unresolved,
would lead to significant degradation in network Key Performance Indicators
(KPIs), thereby resulting in sub-optimal network performance. Centralized
coordination, albeit optimal, is impractical in large scale networks and for
time-critical applications. Decentralized approaches are therefore envisaged in
the evolution to B5G and subsequently, 6G networks. This work explores
pervasive intelligence for conflict resolution in network automation, as an
alternative to centralized orchestration. A Q-Learning decentralized approach
to network automation is proposed, and an application to network slice
auto-scaling is designed and evaluated. Preliminary results highlight the
potential of the proposed scheme and justify further research work in this
direction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1"&gt;Sayantini Majumdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trivisonno_R/0/1/0/all/0/1"&gt;Riccardo Trivisonno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carle_G/0/1/0/all/0/1"&gt;Georg Carle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reenvisioning Collaborative Filtering vs Matrix Factorization. (arXiv:2107.13472v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.13472</id>
        <link href="http://arxiv.org/abs/2107.13472"/>
        <updated>2021-07-29T02:00:09.198Z</updated>
        <summary type="html"><![CDATA[Collaborative filtering models based on matrix factorization and learned
similarities using Artificial Neural Networks (ANNs) have gained significant
attention in recent years. This is, in part, because ANNs have demonstrated
good results in a wide variety of recommendation tasks. The introduction of
ANNs within the recommendation ecosystem has been recently questioned, raising
several comparisons in terms of efficiency and effectiveness. One aspect most
of these comparisons have in common is their focus on accuracy, neglecting
other evaluation dimensions important for the recommendation, such as novelty,
diversity, or accounting for biases. We replicate experiments from three papers
that compare Neural Collaborative Filtering (NCF) and Matrix Factorization
(MF), to extend the analysis to other evaluation dimensions. Our contribution
shows that the experiments are entirely reproducible, and we extend the study
including other accuracy metrics and two statistical hypothesis tests. We
investigated the Diversity and Novelty of the recommendations, showing that MF
provides a better accuracy also on the long tail, although NCF provides a
better item coverage and more diversified recommendations. We discuss the bias
effect generated by the tested methods. They show a relatively small bias, but
other recommendation baselines, with competitive accuracy performance,
consistently show to be less affected by this issue. This is the first work, to
the best of our knowledge, where several evaluation dimensions have been
explored for an array of SOTA algorithms covering recent adaptations of ANNs
and MF. Hence, we show the potential these techniques may have on
beyond-accuracy evaluation while analyzing the effect on reproducibility these
complementary dimensions may spark. Available at
github.com/sisinflab/Reenvisioning-the-comparison-between-Neural-Collaborative-Filtering-and-Matrix-Factorization]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anelli_V/0/1/0/all/0/1"&gt;Vito Walter Anelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bellogin_A/0/1/0/all/0/1"&gt;Alejandro Bellog&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noia_T/0/1/0/all/0/1"&gt;Tommaso Di Noia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pomo_C/0/1/0/all/0/1"&gt;Claudio Pomo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recursively Conditional Gaussian for Ordinal Unsupervised Domain Adaptation. (arXiv:2107.13467v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13467</id>
        <link href="http://arxiv.org/abs/2107.13467"/>
        <updated>2021-07-29T02:00:09.190Z</updated>
        <summary type="html"><![CDATA[There has been a growing interest in unsupervised domain adaptation (UDA) to
alleviate the data scalability issue, while the existing works usually focus on
classifying independently discrete labels. However, in many tasks (e.g.,
medical diagnosis), the labels are discrete and successively distributed. The
UDA for ordinal classification requires inducing non-trivial ordinal
distribution prior to the latent space. Target for this, the partially ordered
set (poset) is defined for constraining the latent vector. Instead of the
typically i.i.d. Gaussian latent prior, in this work, a recursively conditional
Gaussian (RCG) set is proposed for ordered constraint modeling, which admits a
tractable joint distribution prior. Furthermore, we are able to control the
density of content vectors that violate the poset constraint by a simple
"three-sigma rule". We explicitly disentangle the cross-domain images into a
shared ordinal prior induced ordinal content space and two separate
source/target ordinal-unrelated spaces, and the self-training is worked on the
shared space exclusively for ordinal-aware domain alignment. Extensive
experiments on UDA medical diagnoses and facial age estimation demonstrate its
effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Site Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1"&gt;Yubin Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1"&gt;Pengyi Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1"&gt;Jane You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jun Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explicit Pairwise Factorized Graph Neural Network for Semi-Supervised Node Classification. (arXiv:2107.13059v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13059</id>
        <link href="http://arxiv.org/abs/2107.13059"/>
        <updated>2021-07-29T02:00:09.179Z</updated>
        <summary type="html"><![CDATA[Node features and structural information of a graph are both crucial for
semi-supervised node classification problems. A variety of graph neural network
(GNN) based approaches have been proposed to tackle these problems, which
typically determine output labels through feature aggregation. This can be
problematic, as it implies conditional independence of output nodes given
hidden representations, despite their direct connections in the graph. To learn
the direct influence among output nodes in a graph, we propose the Explicit
Pairwise Factorized Graph Neural Network (EPFGNN), which models the whole graph
as a partially observed Markov Random Field. It contains explicit pairwise
factors to model output-output relations and uses a GNN backbone to model
input-output relations. To balance model complexity and expressivity, the
pairwise factors have a shared component and a separate scaling coefficient for
each edge. We apply the EM algorithm to train our model, and utilize a
star-shaped piecewise likelihood for the tractable surrogate objective. We
conduct experiments on various datasets, which shows that our model can
effectively improve the performance for semi-supervised node classification on
graphs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yuesong Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1"&gt;Daniel Cremers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Functorial String Diagrams for Reverse-Mode Automatic Differentiation. (arXiv:2107.13433v1 [cs.PL])]]></title>
        <id>http://arxiv.org/abs/2107.13433</id>
        <link href="http://arxiv.org/abs/2107.13433"/>
        <updated>2021-07-29T02:00:09.160Z</updated>
        <summary type="html"><![CDATA[We enhance the calculus of string diagrams for monoidal categories with
hierarchical features in order to capture closed monoidal (and cartesian
closed) structure. Using this new syntax we formulate an automatic
differentiation algorithm for (applied) simply typed lambda calculus in the
style of [Pearlmutter and Siskind 2008] and we prove for the first time its
soundness. To give an efficient yet principled implementation of the AD
algorithm we define a sound and complete representation of hierarchical string
diagrams as a class of hierarchical hypergraphs we call hypernets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alvarez_Picallo_M/0/1/0/all/0/1"&gt;Mario Alvarez-Picallo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghica_D/0/1/0/all/0/1"&gt;Dan R. Ghica&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sprunger_D/0/1/0/all/0/1"&gt;David Sprunger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zanasi_F/0/1/0/all/0/1"&gt;Fabio Zanasi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Surrogate Model-Based Explainability Methods for Point Cloud NNs. (arXiv:2107.13459v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13459</id>
        <link href="http://arxiv.org/abs/2107.13459"/>
        <updated>2021-07-29T02:00:09.104Z</updated>
        <summary type="html"><![CDATA[In the field of autonomous driving and robotics, point clouds are showing
their excellent real-time performance as raw data from most of the mainstream
3D sensors. Therefore, point cloud neural networks have become a popular
research direction in recent years. So far, however, there has been little
discussion about the explainability of deep neural networks for point clouds.
In this paper, we propose new explainability approaches for point cloud deep
neural networks based on local surrogate model-based methods to show which
components make the main contribution to the classification. Moreover, we
propose a quantitative validation method for explainability methods of point
clouds which enhances the persuasive power of explainability by dropping the
most positive or negative contributing features and monitoring how the
classification scores of specific categories change. To enable an intuitive
explanation of misclassified instances, we display features with confounding
contributions. Our new explainability approach provides a fairly accurate, more
intuitive and widely applicable explanation for point cloud classification
tasks. Our code is available at https://github.com/Explain3D/Explainable3D]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1"&gt;Hanxiao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kotthaus_H/0/1/0/all/0/1"&gt;Helena Kotthaus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Signal Detection Scheme Based on Deep Learning in OFDM Systems. (arXiv:2107.13423v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2107.13423</id>
        <link href="http://arxiv.org/abs/2107.13423"/>
        <updated>2021-07-29T02:00:09.096Z</updated>
        <summary type="html"><![CDATA[Channel estimation and signal detection are essential steps to ensure the
quality of end-to-end communication in orthogonal frequency-division
multiplexing (OFDM) systems. In this paper, we develop a DDLSD approach, i.e.,
Data-driven Deep Learning for Signal Detection in OFDM systems. First, the OFDM
system model is established. Then, the long short-term memory (LSTM) is
introduced into the OFDM system model. Wireless channel data is generated
through simulation, the preprocessed time series feature information is input
into the LSTM to complete the offline training. Finally, the trained model is
used for online recovery of transmitted signal. The difference between this
scheme and existing OFDM receiver is that explicit estimated channel state
information (CSI) is transformed into invisible estimated CSI, and the transmit
symbol is directly restored. Simulation results show that the DDLSD scheme
outperforms the existing traditional methods in terms of improving channel
estimation and signal detection performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_G/0/1/0/all/0/1"&gt;Guangliang Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zitong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Minglei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learned Optimizers for Analytic Continuation. (arXiv:2107.13265v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13265</id>
        <link href="http://arxiv.org/abs/2107.13265"/>
        <updated>2021-07-29T02:00:09.087Z</updated>
        <summary type="html"><![CDATA[Traditional maximum entropy and sparsity-based algorithms for analytic
continuation often suffer from the ill-posed kernel matrix or demand tremendous
computation time for parameter tuning. Here we propose a neural network method
by convex optimization and replace the ill-posed inverse problem by a sequence
of well-conditioned surrogate problems. After training, the learned optimizers
are able to give a solution of high quality with low time cost and achieve
higher parameter efficiency than heuristic full-connected networks. The output
can also be used as a neural default model to improve the maximum entropy for
better performance. Our methods may be easily extended to other
high-dimensional inverse problems via large-scale pretraining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1"&gt;Dongchen Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi-feng Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-learning Emulators and Eigenvector Continuation. (arXiv:2107.13449v1 [nucl-th])]]></title>
        <id>http://arxiv.org/abs/2107.13449</id>
        <link href="http://arxiv.org/abs/2107.13449"/>
        <updated>2021-07-29T02:00:09.080Z</updated>
        <summary type="html"><![CDATA[Emulators that can bypass computationally expensive scientific calculations
with high accuracy and speed can enable new studies of fundamental science as
well as more potential applications. In this work we focus on solving a system
of constraint equations efficiently using a new machine learning approach that
we call self-learning emulation. A self-learning emulator is an active learning
protocol that can rapidly solve a system of equations over some range of
control parameters. The key ingredient is a fast estimate of the emulator error
that becomes progressively more accurate as the emulator improves. This
acceleration is possible because the emulator itself is used to estimate the
error, and we illustrate with two examples. The first uses cubic spline
interpolation to find the roots of a polynomial with variable coefficients. The
second example uses eigenvector continuation to find the eigenvectors and
eigenvalues of a large Hamiltonian matrix that depends on several control
parameters. We envision future applications of self-learning emulators for
solving systems of algebraic equations, linear and nonlinear differential
equations, and linear and nonlinear eigenvalue problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/nucl-th/1/au:+Sarkar_A/0/1/0/all/0/1"&gt;Avik Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/nucl-th/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dean Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combining Machine Learning Classifiers for Stock Trading with Effective Feature Extraction. (arXiv:2107.13148v1 [q-fin.TR])]]></title>
        <id>http://arxiv.org/abs/2107.13148</id>
        <link href="http://arxiv.org/abs/2107.13148"/>
        <updated>2021-07-29T02:00:09.073Z</updated>
        <summary type="html"><![CDATA[The unpredictability and volatility of the stock market render it challenging
to make a substantial profit using any generalized scheme. This paper intends
to discuss our machine learning model, which can make a significant amount of
profit in the US stock market by performing live trading in the Quantopian
platform while using resources free of cost. Our top approach was to use
ensemble learning with four classifiers: Gaussian Naive Bayes, Decision Tree,
Logistic Regression with L1 regularization and Stochastic Gradient Descent, to
decide whether to go long or short on a particular stock. Our best model
performed daily trade between July 2011 and January 2019, generating 54.35%
profit. Finally, our work showcased that mixtures of weighted classifiers
perform better than any individual predictor about making trading decisions in
the stock market.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Ullah_A/0/1/0/all/0/1"&gt;A. K. M. Amanat Ullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Imtiaz_F/0/1/0/all/0/1"&gt;Fahim Imtiaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Ihsan_M/0/1/0/all/0/1"&gt;Miftah Uddin Md Ihsan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Alam_M/0/1/0/all/0/1"&gt;Md. Golam Rabiul Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Majumdar_M/0/1/0/all/0/1"&gt;Mahbub Majumdar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New Metrics to Evaluate the Performance and Fairness of Personalized Federated Learning. (arXiv:2107.13173v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13173</id>
        <link href="http://arxiv.org/abs/2107.13173"/>
        <updated>2021-07-29T02:00:09.053Z</updated>
        <summary type="html"><![CDATA[In Federated Learning (FL), the clients learn a single global model (FedAvg)
through a central aggregator. In this setting, the non-IID distribution of the
data across clients restricts the global FL model from delivering good
performance on the local data of each client. Personalized FL aims to address
this problem by finding a personalized model for each client. Recent works
widely report the average personalized model accuracy on a particular data
split of a dataset to evaluate the effectiveness of their methods. However,
considering the multitude of personalization approaches proposed, it is
critical to study the per-user personalized accuracy and the accuracy
improvements among users with an equitable notion of fairness. To address these
issues, we present a set of performance and fairness metrics intending to
assess the quality of personalized FL methods. We apply these metrics to four
recently proposed personalized FL methods, PersFL, FedPer, pFedMe, and
Per-FedAvg, on three different data splits of the CIFAR-10 dataset. Our
evaluations show that the personalized model with the highest average accuracy
across users may not necessarily be the fairest. Our code is available at
https://tinyurl.com/1hp9ywfa for public use.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Divi_S/0/1/0/all/0/1"&gt;Siddharth Divi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yi-Shan Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farrukh_H/0/1/0/all/0/1"&gt;Habiba Farrukh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Celik_Z/0/1/0/all/0/1"&gt;Z. Berkay Celik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Autoencoders: Analysing and Fixing the Bernoulli likelihood for Out-of-Distribution Detection. (arXiv:2107.13304v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13304</id>
        <link href="http://arxiv.org/abs/2107.13304"/>
        <updated>2021-07-29T02:00:09.045Z</updated>
        <summary type="html"><![CDATA[After an autoencoder (AE) has learnt to reconstruct one dataset, it might be
expected that the likelihood on an out-of-distribution (OOD) input would be
low. This has been studied as an approach to detect OOD inputs. Recent work
showed this intuitive approach can fail for the dataset pairs FashionMNIST vs
MNIST. This paper suggests this is due to the use of Bernoulli likelihood and
analyses why this is the case, proposing two fixes: 1) Compute the uncertainty
of likelihood estimate by using a Bayesian version of the AE. 2) Use
alternative distributions to model the likelihood.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yong_B/0/1/0/all/0/1"&gt;Bang Xiang Yong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pearce_T/0/1/0/all/0/1"&gt;Tim Pearce&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brintrup_A/0/1/0/all/0/1"&gt;Alexandra Brintrup&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel Density Estimation by Stagewise Algorithm with a Simple Dictionary. (arXiv:2107.13430v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.13430</id>
        <link href="http://arxiv.org/abs/2107.13430"/>
        <updated>2021-07-29T02:00:09.036Z</updated>
        <summary type="html"><![CDATA[This paper studies kernel density estimation by stagewise minimization
algorithm with a simple dictionary on U-divergence. We randomly split an i.i.d.
sample into the two disjoint sets, one to be used for constructing the kernels
in the dictionary and the other for evaluating the estimator, and implement the
algorithm. The resulting estimator brings us data-adaptive weighting parameters
and bandwidth matrices, and realizes a sparse representation of kernel density
estimation. We present the non-asymptotic error bounds of our estimator and
confirm its performance by simulations compared with the direct plug-in
bandwidth matrices and the reduced set density estimator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Nishida_K/0/1/0/all/0/1"&gt;Kiheiji Nishida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Naito_K/0/1/0/all/0/1"&gt;Kanta Naito&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Insights from Generative Modeling for Neural Video Compression. (arXiv:2107.13136v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.13136</id>
        <link href="http://arxiv.org/abs/2107.13136"/>
        <updated>2021-07-29T02:00:09.028Z</updated>
        <summary type="html"><![CDATA[While recent machine learning research has revealed connections between deep
generative models such as VAEs and rate-distortion losses used in learned
compression, most of this work has focused on images. In a similar spirit, we
view recently proposed neural video coding algorithms through the lens of deep
autoregressive and latent variable modeling. We present recent neural video
codecs as instances of a generalized stochastic temporal autoregressive
transform, and propose new avenues for further improvements inspired by
normalizing flows and structured priors. We propose several architectures that
yield state-of-the-art video compression performance on full-resolution video
and discuss their tradeoffs and ablations. In particular, we propose (i)
improved temporal autoregressive transforms, (ii) improved entropy models with
structured and temporal dependencies, and (iii) variable bitrate versions of
our algorithms. Since our improvements are compatible with a large class of
existing models, we provide further evidence that the generative modeling
viewpoint can advance the neural video coding field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1"&gt;Ruihan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yibo Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Marino_J/0/1/0/all/0/1"&gt;Joseph Marino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mandt_S/0/1/0/all/0/1"&gt;Stephan Mandt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistically Meaningful Approximation: a Case Study on Approximating Turing Machines with Transformers. (arXiv:2107.13163v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13163</id>
        <link href="http://arxiv.org/abs/2107.13163"/>
        <updated>2021-07-29T02:00:09.019Z</updated>
        <summary type="html"><![CDATA[A common lens to theoretically study neural net architectures is to analyze
the functions they can approximate. However, the constructions from
approximation theory often have unrealistic aspects, for example, reliance on
infinite precision to memorize target function values, which make these results
potentially less meaningful. To address these issues, this work proposes a
formal definition of statistically meaningful approximation which requires the
approximating network to exhibit good statistical learnability. We present case
studies on statistically meaningful approximation for two classes of functions:
boolean circuits and Turing machines. We show that overparameterized
feedforward neural nets can statistically meaningfully approximate boolean
circuits with sample complexity depending only polynomially on the circuit
size, not the size of the approximating network. In addition, we show that
transformers can statistically meaningfully approximate Turing machines with
computation time bounded by $T$, requiring sample complexity polynomial in the
alphabet size, state space size, and $\log (T)$. Our analysis introduces new
tools for generalization bounds that provide much tighter sample complexity
guarantees than the typical VC-dimension or norm-based bounds, which may be of
independent interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1"&gt;Colin Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yining Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengyu Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An explainable two-dimensional single model deep learning approach for Alzheimer's disease diagnosis and brain atrophy localization. (arXiv:2107.13200v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.13200</id>
        <link href="http://arxiv.org/abs/2107.13200"/>
        <updated>2021-07-29T02:00:08.998Z</updated>
        <summary type="html"><![CDATA[Early and accurate diagnosis of Alzheimer's disease (AD) and its prodromal
period mild cognitive impairment (MCI) is essential for the delayed disease
progression and the improved quality of patients'life. The emerging
computer-aided diagnostic methods that combine deep learning with structural
magnetic resonance imaging (sMRI) have achieved encouraging results, but some
of them are limit of issues such as data leakage and unexplainable diagnosis.
In this research, we propose a novel end-to-end deep learning approach for
automated diagnosis of AD and localization of important brain regions related
to the disease from sMRI data. This approach is based on a 2D single model
strategy and has the following differences from the current approaches: 1)
Convolutional Neural Network (CNN) models of different structures and
capacities are evaluated systemically and the most suitable model is adopted
for AD diagnosis; 2) a data augmentation strategy named Two-stage Random
RandAugment (TRRA) is proposed to alleviate the overfitting issue caused by
limited training data and to improve the classification performance in AD
diagnosis; 3) an explainable method of Grad-CAM++ is introduced to generate the
visually explainable heatmaps that localize and highlight the brain regions
that our model focuses on and to make our model more transparent. Our approach
has been evaluated on two publicly accessible datasets for two classification
tasks of AD vs. cognitively normal (CN) and progressive MCI (pMCI) vs. stable
MCI (sMCI). The experimental results indicate that our approach outperforms the
state-of-the-art approaches, including those using multi-model and 3D CNN
methods. The resultant localization heatmaps from our approach also highlight
the lateral ventricle and some disease-relevant regions of cortex, coincident
with the commonly affected regions during the development of AD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pan_B/0/1/0/all/0/1"&gt;Bo Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shao_P/0/1/0/all/0/1"&gt;Pengfei Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1"&gt;Peng Liu&lt;/a&gt; (Alzheimer&amp;#x27;s Disease Neuroimaging Initiative, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing), &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_S/0/1/0/all/0/1"&gt;Shuwei Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yao_P/0/1/0/all/0/1"&gt;Peng Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_R/0/1/0/all/0/1"&gt;Ronald X. Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Network Approximation of Refinable Functions. (arXiv:2107.13191v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13191</id>
        <link href="http://arxiv.org/abs/2107.13191"/>
        <updated>2021-07-29T02:00:08.989Z</updated>
        <summary type="html"><![CDATA[In the desire to quantify the success of neural networks in deep learning and
other applications, there is a great interest in understanding which functions
are efficiently approximated by the outputs of neural networks. By now, there
exists a variety of results which show that a wide range of functions can be
approximated with sometimes surprising accuracy by these outputs. For example,
it is known that the set of functions that can be approximated with exponential
accuracy (in terms of the number of parameters used) includes, on one hand,
very smooth functions such as polynomials and analytic functions (see e.g.
\cite{E,S,Y}) and, on the other hand, very rough functions such as the
Weierstrass function (see e.g. \cite{EPGB,DDFHP}), which is nowhere
differentiable. In this paper, we add to the latter class of rough functions by
showing that it also includes refinable functions. Namely, we show that
refinable functions are approximated by the outputs of deep ReLU networks with
a fixed width and increasing depth with accuracy exponential in terms of their
number of parameters. Our results apply to functions used in the standard
construction of wavelets as well as to functions constructed via subdivision
algorithms in Computer Aided Geometric Design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Daubechies_I/0/1/0/all/0/1"&gt;Ingrid Daubechies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DeVore_R/0/1/0/all/0/1"&gt;Ronald DeVore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dym_N/0/1/0/all/0/1"&gt;Nadav Dym&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faigenbaum_Golovin_S/0/1/0/all/0/1"&gt;Shira Faigenbaum-Golovin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kovalsky_S/0/1/0/all/0/1"&gt;Shahar Z. Kovalsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1"&gt;Kung-Ching Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Josiah Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petrova_G/0/1/0/all/0/1"&gt;Guergana Petrova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sober_B/0/1/0/all/0/1"&gt;Barak Sober&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Balancing for Causal Continuous Treatment-Effect Estimation. (arXiv:2107.13068v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13068</id>
        <link href="http://arxiv.org/abs/2107.13068"/>
        <updated>2021-07-29T02:00:08.982Z</updated>
        <summary type="html"><![CDATA[We study the problem of observational causal inference with continuous
treatment. We focus on the challenge of estimating the causal response curve
for infrequently-observed treatment values. We design a new algorithm based on
the framework of entropy balancing which learns weights that directly maximize
causal inference accuracy using end-to-end optimization. Our weights can be
customized for different datasets and causal inference algorithms. We propose a
new theory for consistency of entropy balancing for continuous treatments.
Using synthetic and real-world data, we show that our proposed algorithm
outperforms the entropy balancing in terms of causal inference accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bahadori_M/0/1/0/all/0/1"&gt;Mohammad Taha Bahadori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tchetgen_E/0/1/0/all/0/1"&gt;Eric Tchetgen Tchetgen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heckerman_D/0/1/0/all/0/1"&gt;David E. Heckerman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exceeding the Limits of Visual-Linguistic Multi-Task Learning. (arXiv:2107.13054v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.13054</id>
        <link href="http://arxiv.org/abs/2107.13054"/>
        <updated>2021-07-29T02:00:08.975Z</updated>
        <summary type="html"><![CDATA[By leveraging large amounts of product data collected across hundreds of live
e-commerce websites, we construct 1000 unique classification tasks that share
similarly-structured input data, comprised of both text and images. These
classification tasks focus on learning the product hierarchy of different
e-commerce websites, causing many of them to be correlated. Adopting a
multi-modal transformer model, we solve these tasks in unison using multi-task
learning (MTL). Extensive experiments are presented over an initial 100-task
dataset to reveal best practices for "large-scale MTL" (i.e., MTL with more
than 100 tasks). From these experiments, a final, unified methodology is
derived, which is composed of both best practices and new proposals such as
DyPa, a simple heuristic for automatically allocating task-specific parameters
to tasks that could benefit from extra capacity. Using our large-scale MTL
methodology, we successfully train a single model across all 1000 tasks in our
dataset while using minimal task specific parameters, thereby showing that it
is possible to extend several orders of magnitude beyond current efforts in
MTL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1"&gt;Cameron R. Wolfe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lundgaard_K/0/1/0/all/0/1"&gt;Keld T. Lundgaard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust and Active Learning for Deep Neural Network Regression. (arXiv:2107.13124v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13124</id>
        <link href="http://arxiv.org/abs/2107.13124"/>
        <updated>2021-07-29T02:00:08.954Z</updated>
        <summary type="html"><![CDATA[We describe a gradient-based method to discover local error maximizers of a
deep neural network (DNN) used for regression, assuming the availability of an
"oracle" capable of providing real-valued supervision (a regression target) for
samples. For example, the oracle could be a numerical solver which,
operationally, is much slower than the DNN. Given a discovered set of local
error maximizers, the DNN is either fine-tuned or retrained in the manner of
active learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kesidis_G/0/1/0/all/0/1"&gt;George Kesidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miller_D/0/1/0/all/0/1"&gt;David J. Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bergeron_M/0/1/0/all/0/1"&gt;Maxime Bergeron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferguson_R/0/1/0/all/0/1"&gt;Ryan Ferguson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lucic_V/0/1/0/all/0/1"&gt;Vladimir Lucic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Learning Algorithm for Piecewise Linear Interface Construction (PLIC). (arXiv:2107.13067v1 [physics.flu-dyn])]]></title>
        <id>http://arxiv.org/abs/2107.13067</id>
        <link href="http://arxiv.org/abs/2107.13067"/>
        <updated>2021-07-29T02:00:08.947Z</updated>
        <summary type="html"><![CDATA[Piecewise Linear Interface Construction (PLIC) is frequently used to
geometrically reconstruct fluid interfaces in Computational Fluid Dynamics
(CFD) modeling of two-phase flows. PLIC reconstructs interfaces from a scalar
field that represents the volume fraction of each phase in each computational
cell. Given the volume fraction and interface normal, the location of a linear
interface is uniquely defined. For a cubic computational cell (3D), the
position of the planar interface is determined by intersecting the cube with a
plane, such that the volume of the resulting truncated polyhedron cell is equal
to the volume fraction. Yet it is geometrically complex to find the exact
position of the plane, and it involves calculations that can be a computational
bottleneck of many CFD models. However, while the forward problem of 3D PLIC is
challenging, the inverse problem, of finding the volume of the truncated
polyhedron cell given a defined plane, is simple. In this work, we propose a
deep learning model for the solution to the forward problem of PLIC by only
making use of its inverse problem. The proposed model is up to several orders
of magnitude faster than traditional schemes, which significantly reduces the
computational bottleneck of PLIC in CFD simulations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Ataei_M/0/1/0/all/0/1"&gt;Mohammadmehdi Ataei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Pirmorad_E/0/1/0/all/0/1"&gt;Erfan Pirmorad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Costa_F/0/1/0/all/0/1"&gt;Franco Costa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Han_S/0/1/0/all/0/1"&gt;Sejin Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Park_C/0/1/0/all/0/1"&gt;Chul B Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Bussmann_M/0/1/0/all/0/1"&gt;Markus Bussmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Case Study on Sampling Strategies for Evaluating Neural Sequential Item Recommendation Models. (arXiv:2107.13045v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.13045</id>
        <link href="http://arxiv.org/abs/2107.13045"/>
        <updated>2021-07-29T02:00:08.940Z</updated>
        <summary type="html"><![CDATA[At the present time, sequential item recommendation models are compared by
calculating metrics on a small item subset (target set) to speed up
computation. The target set contains the relevant item and a set of negative
items that are sampled from the full item set. Two well-known strategies to
sample negative items are uniform random sampling and sampling by popularity to
better approximate the item frequency distribution in the dataset. Most
recently published papers on sequential item recommendation rely on sampling by
popularity to compare the evaluated models. However, recent work has already
shown that an evaluation with uniform random sampling may not be consistent
with the full ranking, that is, the model ranking obtained by evaluating a
metric using the full item set as target set, which raises the question whether
the ranking obtained by sampling by popularity is equal to the full ranking. In
this work, we re-evaluate current state-of-the-art sequential recommender
models from the point of view, whether these sampling strategies have an impact
on the final ranking of the models. We therefore train four recently proposed
sequential recommendation models on five widely known datasets. For each
dataset and model, we employ three evaluation strategies. First, we compute the
full model ranking. Then we evaluate all models on a target set sampled by the
two different sampling strategies, uniform random sampling and sampling by
popularity with the commonly used target set size of 100, compute the model
ranking for each strategy and compare them with each other. Additionally, we
vary the size of the sampled target set. Overall, we find that both sampling
strategies can produce inconsistent rankings compared with the full ranking of
the models. Furthermore, both sampling by popularity and uniform random
sampling do not consistently produce the same ranking ...]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dallmann_A/0/1/0/all/0/1"&gt;Alexander Dallmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zoller_D/0/1/0/all/0/1"&gt;Daniel Zoller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hotho_A/0/1/0/all/0/1"&gt;Andreas Hotho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Human Cell Classification in Sparse Datasets using Few-Shot Learning. (arXiv:2107.13093v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13093</id>
        <link href="http://arxiv.org/abs/2107.13093"/>
        <updated>2021-07-29T02:00:08.932Z</updated>
        <summary type="html"><![CDATA[Classifying and analyzing human cells is a lengthy procedure, often involving
a trained professional. In an attempt to expedite this process, an active area
of research involves automating cell classification through use of deep
learning-based techniques. In practice, a large amount of data is required to
accurately train these deep learning models. However, due to the sparse human
cell datasets currently available, the performance of these models is typically
low. This study investigates the feasibility of using few-shot learning-based
techniques to mitigate the data requirements for accurate training. The study
is comprised of three parts: First, current state-of-the-art few-shot learning
techniques are evaluated on human cell classification. The selected techniques
are trained on a non-medical dataset and then tested on two out-of-domain,
human cell datasets. The results indicate that, overall, the test accuracy of
state-of-the-art techniques decreased by at least 30% when transitioning from a
non-medical dataset to a medical dataset. Second, this study evaluates the
potential benefits, if any, to varying the backbone architecture and training
schemes in current state-of-the-art few-shot learning techniques when used in
human cell classification. Even with these variations, the overall test
accuracy decreased from 88.66% on non-medical datasets to 44.13% at best on the
medical datasets. Third, this study presents future directions for using
few-shot learning in human cell classification. In general, few-shot learning
in its current state performs poorly on human cell classification. The study
proves that attempts to modify existing network architectures are not effective
and concludes that future research effort should be focused on improving
robustness towards out-of-domain testing using optimization-based or
self-supervised few-shot learning techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Walsh_R/0/1/0/all/0/1"&gt;Reece Walsh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdelpakey_M/0/1/0/all/0/1"&gt;Mohamed H. Abdelpakey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shehata_M/0/1/0/all/0/1"&gt;Mohamed S. Shehata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_M/0/1/0/all/0/1"&gt;Mostafa M.Mohamed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fully Homomorphically Encrypted Deep Learning as a Service. (arXiv:2107.12997v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12997</id>
        <link href="http://arxiv.org/abs/2107.12997"/>
        <updated>2021-07-29T02:00:08.920Z</updated>
        <summary type="html"><![CDATA[Fully Homomorphic Encryption (FHE) is a relatively recent advancement in the
field of privacy-preserving technologies. FHE allows for the arbitrary depth
computation of both addition and multiplication, and thus the application of
abelian/polynomial equations, like those found in deep learning algorithms.
This project investigates, derives, and proves how FHE with deep learning can
be used at scale, with relatively low time complexity, the problems that such a
system incurs, and mitigations/solutions for such problems. In addition, we
discuss how this could have an impact on the future of data privacy and how it
can enable data sharing across various actors in the agri-food supply chain,
hence allowing the development of machine learning-based systems. Finally, we
find that although FHE incurs a high spatial complexity cost, the time
complexity is within expected reasonable bounds, while allowing for absolutely
private predictions to be made, in our case for milk yield prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Onoufriou_G/0/1/0/all/0/1"&gt;George Onoufriou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mayfield_P/0/1/0/all/0/1"&gt;Paul Mayfield&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leontidis_G/0/1/0/all/0/1"&gt;Georgios Leontidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Event Camera Calibration. (arXiv:2107.06749v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06749</id>
        <link href="http://arxiv.org/abs/2107.06749"/>
        <updated>2021-07-29T02:00:08.913Z</updated>
        <summary type="html"><![CDATA[Camera calibration is an important prerequisite towards the solution of 3D
computer vision problems. Traditional methods rely on static images of a
calibration pattern. This raises interesting challenges towards the practical
usage of event cameras, which notably require image change to produce
sufficient measurements. The current standard for event camera calibration
therefore consists of using flashing patterns. They have the advantage of
simultaneously triggering events in all reprojected pattern feature locations,
but it is difficult to construct or use such patterns in the field. We present
the first dynamic event camera calibration algorithm. It calibrates directly
from events captured during relative motion between camera and calibration
pattern. The method is propelled by a novel feature extraction mechanism for
calibration patterns, and leverages existing calibration tools before
optimizing all parameters through a multi-segment continuous-time formulation.
As demonstrated through our results on real data, the obtained calibration
method is highly convenient and reliably calibrates from data sequences
spanning less than 10 seconds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yifu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kneip_L/0/1/0/all/0/1"&gt;Laurent Kneip&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dataset Distillation with Infinitely Wide Convolutional Networks. (arXiv:2107.13034v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13034</id>
        <link href="http://arxiv.org/abs/2107.13034"/>
        <updated>2021-07-29T02:00:08.867Z</updated>
        <summary type="html"><![CDATA[The effectiveness of machine learning algorithms arises from being able to
extract useful features from large amounts of data. As model and dataset sizes
increase, dataset distillation methods that compress large datasets into
significantly smaller yet highly performant ones will become valuable in terms
of training efficiency and useful feature extraction. To that end, we apply a
novel distributed kernel based meta-learning framework to achieve
state-of-the-art results for dataset distillation using infinitely wide
convolutional neural networks. For instance, using only 10 datapoints (0.02% of
original dataset), we obtain over 64% test accuracy on CIFAR-10 image
classification task, a dramatic improvement over the previous best test
accuracy of 40%. Our state-of-the-art results extend across many other settings
for MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, and SVHN. Furthermore, we
perform some preliminary analyses of our distilled datasets to shed light on
how they differ from naturally occurring data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Timothy Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Novak_R/0/1/0/all/0/1"&gt;Roman Novak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1"&gt;Lechao Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jaehoon Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flood Segmentation on Sentinel-1 SAR Imagery with Semi-Supervised Learning. (arXiv:2107.08369v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08369</id>
        <link href="http://arxiv.org/abs/2107.08369"/>
        <updated>2021-07-29T02:00:08.850Z</updated>
        <summary type="html"><![CDATA[Floods wreak havoc throughout the world, causing billions of dollars in
damages, and uprooting communities, ecosystems and economies. Accurate and
robust flood detection including delineating open water flood areas and
identifying flood levels can aid in disaster response and mitigation. However,
estimating flood levels remotely is of essence as physical access to flooded
areas is limited and the ability to deploy instruments in potential flood zones
can be dangerous. Aligning flood extent mapping with local topography can
provide a plan-of-action that the disaster response team can consider. Thus,
remote flood level estimation via satellites like Sentinel-1 can prove to be
remedial. The Emerging Techniques in Computational Intelligence (ETCI)
competition on Flood Detection tasked participants with predicting flooded
pixels after training with synthetic aperture radar (SAR) images in a
supervised setting. We use a cyclical approach involving two stages (1)
training an ensemble model of multiple UNet architectures with available high
and low confidence labeled data and, (2) generating pseudo labels or low
confidence labels on the unlabeled test dataset, and then, combining the
generated labels with the previously available high confidence labeled dataset.
This assimilated dataset is used for the next round of training ensemble
models. This cyclical process is repeated until the performance improvement
plateaus. Additionally, we post process our results with Conditional Random
Fields. Our approach sets a high score on the public leaderboard for the ETCI
competition with 0.7654 IoU. Our method, which we release with all the code
including trained models, can also be used as an open science benchmark for the
Sentinel-1 released dataset on GitHub. To the best of our knowledge we believe
this the first works to try out semi-supervised learning to improve flood
segmentation models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1"&gt;Sayak Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1"&gt;Siddha Ganju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Long-Short Transformer: Efficient Transformers for Language and Vision. (arXiv:2107.02192v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02192</id>
        <link href="http://arxiv.org/abs/2107.02192"/>
        <updated>2021-07-29T02:00:08.842Z</updated>
        <summary type="html"><![CDATA[Transformers have achieved success in both language and vision domains.
However, it is prohibitively expensive to scale them to long sequences such as
long documents or high-resolution images, because self-attention mechanism has
quadratic time and memory complexities with respect to the input sequence
length. In this paper, we propose Long-Short Transformer (Transformer-LS), an
efficient self-attention mechanism for modeling long sequences with linear
complexity for both language and vision tasks. It aggregates a novel long-range
attention with dynamic projection to model distant correlations and a
short-term attention to capture fine-grained local correlations. We propose a
dual normalization strategy to account for the scale mismatch between the two
attention mechanisms. Transformer-LS can be applied to both autoregressive and
bidirectional models without additional complexity. Our method outperforms the
state-of-the-art models on multiple tasks in language and vision domains,
including the Long Range Arena benchmark, autoregressive language modeling, and
ImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on
enwik8 using half the number of parameters than previous method, while being
faster and is able to handle 3x as long sequences compared to its
full-attention version on the same hardware. On ImageNet, it can obtain the
state-of-the-art results (e.g., a moderate size of 55.8M model solely trained
on 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more
scalable on high-resolution images. The source code and models are released at
https://github.com/NVIDIA/transformer-ls .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1"&gt;Chen Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1"&gt;Wei Ping&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Chaowei Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1"&gt;Mohammad Shoeybi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1"&gt;Tom Goldstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1"&gt;Bryan Catanzaro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings. (arXiv:2106.03143v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03143</id>
        <link href="http://arxiv.org/abs/2106.03143"/>
        <updated>2021-07-29T02:00:08.818Z</updated>
        <summary type="html"><![CDATA[Without positional information, attention-based transformer neural networks
are permutation-invariant. Absolute or relative positional embeddings are the
most popular ways to feed transformer models positional information. Absolute
positional embeddings are simple to implement, but suffer from generalization
issues when evaluating on sequences of different length than those seen at
training time. Relative positions are more robust to length change, but are
more complex to implement and yield inferior model throughput. In this paper,
we propose an augmentation-based approach (CAPE) for absolute positional
embeddings, which keeps the advantages of both absolute (simplicity and speed)
and relative position embeddings (better generalization). In addition, our
empirical evaluation on state-of-the-art models in machine translation, image
and speech recognition demonstrates that CAPE leads to better generalization
performance as well as increased stability with respect to training
hyper-parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1"&gt;Tatiana Likhomanenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1"&gt;Qiantong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1"&gt;Ronan Collobert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1"&gt;Gabriel Synnaeve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rogozhnikov_A/0/1/0/all/0/1"&gt;Alex Rogozhnikov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CarveNet: Carving Point-Block for Complex 3D Shape Completion. (arXiv:2107.13452v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13452</id>
        <link href="http://arxiv.org/abs/2107.13452"/>
        <updated>2021-07-29T02:00:08.799Z</updated>
        <summary type="html"><![CDATA[3D point cloud completion is very challenging because it heavily relies on
the accurate understanding of the complex 3D shapes (e.g., high-curvature,
concave/convex, and hollowed-out 3D shapes) and the unknown & diverse patterns
of the partially available point clouds. In this paper, we propose a novel
solution,i.e., Point-block Carving (PC), for completing the complex 3D point
cloud completion. Given the partial point cloud as the guidance, we carve a3D
block that contains the uniformly distributed 3D points, yielding the entire
point cloud. To achieve PC, we propose a new network architecture, i.e.,
CarveNet. This network conducts the exclusive convolution on each point of the
block, where the convolutional kernels are trained on the 3D shape data.
CarveNet determines which point should be carved, for effectively recovering
the details of the complete shapes. Furthermore, we propose a sensor-aware
method for data augmentation,i.e., SensorAug, for training CarveNet on richer
patterns of partial point clouds, thus enhancing the completion power of the
network. The extensive evaluations on the ShapeNet and KITTI datasets
demonstrate the generality of our approach on the partial point clouds with
diverse patterns. On these datasets, CarveNet successfully outperforms the
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1"&gt;Qing Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhijie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1"&gt;Felix Juefei-Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1"&gt;Di Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lei Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1"&gt;Wei Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepSMILE: Self-supervised heterogeneity-aware multiple instance learning for DNA damage response defect classification directly from H&E whole-slide images. (arXiv:2107.09405v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.09405</id>
        <link href="http://arxiv.org/abs/2107.09405"/>
        <updated>2021-07-29T02:00:08.791Z</updated>
        <summary type="html"><![CDATA[We propose a Deep learning-based weak label learning method for analysing
whole slide images (WSIs) of Hematoxylin and Eosin (H&E) stained tumorcells not
requiring pixel-level or tile-level annotations using Self-supervised
pre-training and heterogeneity-aware deep Multiple Instance LEarning
(DeepSMILE). We apply DeepSMILE to the task of Homologous recombination
deficiency (HRD) and microsatellite instability (MSI) prediction. We utilize
contrastive self-supervised learning to pre-train a feature extractor on
histopathology tiles of cancer tissue. Additionally, we use variability-aware
deep multiple instance learning to learn the tile feature aggregation function
while modeling tumor heterogeneity. Compared to state-of-the-art genomic label
classification methods, DeepSMILE improves classification performance for HRD
from $70.43\pm4.10\%$ to $83.79\pm1.25\%$ AUC and MSI from $78.56\pm6.24\%$ to
$90.32\pm3.58\%$ AUC in a multi-center breast and colorectal cancer dataset,
respectively. These improvements suggest we can improve genomic label
classification performance without collecting larger datasets. In the future,
this may reduce the need for expensive genome sequencing techniques, provide
personalized therapy recommendations based on widely available WSIs of cancer
tissue, and improve patient care with quicker treatment decisions - also in
medical centers without access to genome sequencing resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Schirris_Y/0/1/0/all/0/1"&gt;Yoni Schirris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gavves_E/0/1/0/all/0/1"&gt;Efstratios Gavves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nederlof_I/0/1/0/all/0/1"&gt;Iris Nederlof&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Horlings_H/0/1/0/all/0/1"&gt;Hugo Mark Horlings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Teuwen_J/0/1/0/all/0/1"&gt;Jonas Teuwen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Know Thyself: Transferable Visuomotor Control Through Robot-Awareness. (arXiv:2107.09047v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.09047</id>
        <link href="http://arxiv.org/abs/2107.09047"/>
        <updated>2021-07-29T02:00:08.783Z</updated>
        <summary type="html"><![CDATA[Training visuomotor robot controllers from scratch on a new robot typically
requires generating large amounts of robot-specific data. Could we leverage
data previously collected on another robot to reduce or even completely remove
this need for robot-specific data? We propose a "robot-aware" solution paradigm
that exploits readily available robot "self-knowledge" such as proprioception,
kinematics, and camera calibration to achieve this. First, we learn modular
dynamics models that pair a transferable, robot-agnostic world dynamics module
with a robot-specific, analytical robot dynamics module. Next, we set up visual
planning costs that draw a distinction between the robot self and the world.
Our experiments on tabletop manipulation tasks in simulation and on real robots
demonstrate that these plug-in improvements dramatically boost the
transferability of visuomotor controllers, even permitting zero-shot transfer
onto new robots for the very first time. Project website:
https://hueds.github.io/rac/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1"&gt;Edward S. Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rybkin_O/0/1/0/all/0/1"&gt;Oleh Rybkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jayaraman_D/0/1/0/all/0/1"&gt;Dinesh Jayaraman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Boosting for Domain Adaptation: Towards Robust Predictions in Scene Segmentation. (arXiv:2103.15685v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15685</id>
        <link href="http://arxiv.org/abs/2103.15685"/>
        <updated>2021-07-29T02:00:08.776Z</updated>
        <summary type="html"><![CDATA[Domain adaptation is to transfer the shared knowledge learned from the source
domain to a new environment, i.e., target domain. One common practice is to
train the model on both labeled source-domain data and unlabeled target-domain
data. Yet the learned models are usually biased due to the strong supervision
of the source domain. Most researchers adopt the early-stopping strategy to
prevent over-fitting, but when to stop training remains a challenging problem
since the lack of the target-domain validation set. In this paper, we propose
one efficient bootstrapping method, called Adaboost Student, explicitly
learning complementary models during training and liberating users from
empirical early stopping. Adaboost Student combines the deep model learning
with the conventional training strategy, i.e., adaptive boosting, and enables
interactions between learned models and the data sampler. We adopt one adaptive
data sampler to progressively facilitate learning on hard samples and aggregate
"weak" models to prevent over-fitting. Extensive experiments show that (1)
Without the need to worry about the stopping time, AdaBoost Student provides
one robust solution by efficient complementary model learning during training.
(2) AdaBoost Student is orthogonal to most domain adaptation methods, which can
be combined with existing approaches to further improve the state-of-the-art
performance. We have achieved competitive results on three widely-used scene
segmentation domain adaptation benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhedong Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatio-Temporal SAR-Optical Data Fusion for Cloud Removal via a Deep Hierarchical Model. (arXiv:2106.12226v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12226</id>
        <link href="http://arxiv.org/abs/2106.12226"/>
        <updated>2021-07-29T02:00:08.768Z</updated>
        <summary type="html"><![CDATA[The abundance of clouds, located both spatially and temporally, often makes
remote sensing (RS) applications with optical images difficult or even
impossible to perform. Traditional cloud removing techniques have been studied
for years, and recently, Machine Learning (ML)-based approaches have also been
considered. In this manuscript, a novel method for the restoration of
clouds-corrupted optical images is presented, able to generate the whole
optical scene of interest, not only the cloudy pixels, and based on a Joint
Data Fusion paradigm, where three deep neural networks are hierarchically
combined. Spatio-temporal features are separately extracted by a conditional
Generative Adversarial Network (cGAN) and by a Convolutional Long Short-Term
Memory (ConvLSTM), from Synthetic Aperture Radar (SAR) data and optical
time-series of data respectively, and then combined with a U-shaped network.
The use of time-series of data has been rarely explored in the state of the art
for this peculiar objective, and moreover existing models do not combine both
spatio-temporal domains and SAR-optical imagery. Quantitative and qualitative
results have shown a good ability of the proposed method in producing
cloud-free images, by also preserving the details and outperforming the cGAN
and the ConvLSTM when individually used. Both the code and the dataset have
been implemented from scratch and made available to interested researchers for
further analysis and investigation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sebastianelli_A/0/1/0/all/0/1"&gt;Alessandro Sebastianelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nowakowski_A/0/1/0/all/0/1"&gt;Artur Nowakowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Puglisi_E/0/1/0/all/0/1"&gt;Erika Puglisi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosso_M/0/1/0/all/0/1"&gt;Maria Pia Del Rosso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mifdal_J/0/1/0/all/0/1"&gt;Jamila Mifdal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pirri_F/0/1/0/all/0/1"&gt;Fiora Pirri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathieu_P/0/1/0/all/0/1"&gt;Pierre Philippe Mathieu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullo_S/0/1/0/all/0/1"&gt;Silvia Liberata Ullo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking ResNets: Improved Stacking Strategies With High Order Schemes. (arXiv:2103.15244v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15244</id>
        <link href="http://arxiv.org/abs/2103.15244"/>
        <updated>2021-07-29T02:00:08.760Z</updated>
        <summary type="html"><![CDATA[Various deep neural network architectures (DNNs) maintain massive vital
records in computer vision. While drawing attention worldwide, the design of
the overall structure lacks general guidance. Based on the relationship between
DNN design and numerical differential equations, we performed a fair comparison
of the residual design with higher-order perspectives. We show that the widely
used DNN design strategy, constantly stacking a small design (usually 2-3
layers), could be easily improved, supported by solid theoretical knowledge and
with no extra parameters needed. We reorganise the residual design in
higher-order ways, which is inspired by the observation that many effective
networks can be interpreted as different numerical discretisations of
differential equations. The design of ResNet follows a relatively simple
scheme, which is Euler forward; however, the situation becomes complicated
rapidly while stacking. We suppose that stacked ResNet is somehow equalled to a
higher-order scheme; then, the current method of forwarding propagation might
be relatively weak compared with a typical high-order method such as
Runge-Kutta. We propose HO-ResNet to verify the hypothesis of widely used CV
benchmarks with sufficient experiments. Stable and noticeable increases in
performance are observed, and convergence and robustness are also improved. Our
stacking strategy improved ResNet-30 by 2.15 per cent and ResNet-58 by 2.35 per
cent on CIFAR-10, with the same settings and parameters. The proposed strategy
is fundamental and theoretical and can therefore be applied to any network as a
general guideline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1"&gt;Zhengbo Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zitang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Weilian Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zizhang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamata_S/0/1/0/all/0/1"&gt;Sei-ichiro Kamata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistically Significant Stopping of Neural Network Training. (arXiv:2103.01205v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01205</id>
        <link href="http://arxiv.org/abs/2103.01205"/>
        <updated>2021-07-29T02:00:08.752Z</updated>
        <summary type="html"><![CDATA[The general approach taken when training deep learning classifiers is to save
the parameters after every few iterations, train until either a human observer
or a simple metric-based heuristic decides the network isn't learning anymore,
and then backtrack and pick the saved parameters with the best validation
accuracy. Simple methods are used to determine if a neural network isn't
learning anymore because, as long as it's well after the optimal values are
found, the condition doesn't impact the final accuracy of the model. However
from a runtime perspective, this is of great significance to the many cases
where numerous neural networks are trained simultaneously (e.g. hyper-parameter
tuning). Motivated by this, we introduce a statistical significance test to
determine if a neural network has stopped learning. This stopping criterion
appears to represent a happy medium compared to other popular stopping
criterions, achieving comparable accuracy to the criterions that achieve the
highest final accuracies in 77% or fewer epochs, while the criterions which
stop sooner do so with an appreciable loss to final accuracy. Additionally, we
use this as the basis of a new learning rate scheduler, removing the need to
manually choose learning rate schedules and acting as a quasi-line search,
achieving superior or comparable empirical performance to existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Terry_J/0/1/0/all/0/1"&gt;J. K. Terry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jayakumar_M/0/1/0/all/0/1"&gt;Mario Jayakumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alwis_K/0/1/0/all/0/1"&gt;Kusal De Alwis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoLA: Weakly-Supervised Temporal Action Localization with Snippet Contrastive Learning. (arXiv:2103.16392v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16392</id>
        <link href="http://arxiv.org/abs/2103.16392"/>
        <updated>2021-07-29T02:00:08.728Z</updated>
        <summary type="html"><![CDATA[Weakly-supervised temporal action localization (WS-TAL) aims to localize
actions in untrimmed videos with only video-level labels. Most existing models
follow the "localization by classification" procedure: locate temporal regions
contributing most to the video-level classification. Generally, they process
each snippet (or frame) individually and thus overlook the fruitful temporal
context relation. Here arises the single snippet cheating issue: "hard"
snippets are too vague to be classified. In this paper, we argue that learning
by comparing helps identify these hard snippets and we propose to utilize
snippet Contrastive learning to Localize Actions, CoLA for short. Specifically,
we propose a Snippet Contrast (SniCo) Loss to refine the hard snippet
representation in feature space, which guides the network to perceive precise
temporal boundaries and avoid the temporal interval interruption. Besides,
since it is infeasible to access frame-level annotations, we introduce a Hard
Snippet Mining algorithm to locate the potential hard snippets. Substantial
analyses verify that this mining strategy efficaciously captures the hard
snippets and SniCo Loss leads to more informative feature representation.
Extensive experiments show that CoLA achieves state-of-the-art results on
THUMOS'14 and ActivityNet v1.2 datasets. CoLA code is publicly available at
https://github.com/zhang-can/CoLA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Can Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1"&gt;Meng Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1"&gt;Dongming Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Playing to distraction: towards a robust training of CNN classifiers through visual explanation techniques. (arXiv:2012.14173v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14173</id>
        <link href="http://arxiv.org/abs/2012.14173"/>
        <updated>2021-07-29T02:00:08.714Z</updated>
        <summary type="html"><![CDATA[The field of deep learning is evolving in different directions, with still
the need for more efficient training strategies. In this work, we present a
novel and robust training scheme that integrates visual explanation techniques
in the learning process. Unlike the attention mechanisms that focus on the
relevant parts of images, we aim to improve the robustness of the model by
making it pay attention to other regions as well. Broadly speaking, the idea is
to distract the classifier in the learning process to force it to focus not
only on relevant regions but also on those that, a priori, are not so
informative for the discrimination of the class. We tested the proposed
approach by embedding it into the learning process of a convolutional neural
network for the analysis and classification of two well-known datasets, namely
Stanford cars and FGVC-Aircraft. Furthermore, we evaluated our model on a
real-case scenario for the classification of egocentric images, allowing us to
obtain relevant information about peoples' lifestyles. In particular, we work
on the challenging EgoFoodPlaces dataset, achieving state-of-the-art results
with a lower level of complexity. The obtained results indicate the suitability
of our proposed training scheme for image classification, improving the
robustness of the final model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Morales_D/0/1/0/all/0/1"&gt;David Morales&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talavera_E/0/1/0/all/0/1"&gt;Estefania Talavera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Remeseiro_B/0/1/0/all/0/1"&gt;Beatriz Remeseiro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Adversarial Patch Analysis and Certified Defense against Crowd Counting. (arXiv:2104.10868v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10868</id>
        <link href="http://arxiv.org/abs/2104.10868"/>
        <updated>2021-07-29T02:00:08.692Z</updated>
        <summary type="html"><![CDATA[Crowd counting has drawn much attention due to its importance in
safety-critical surveillance systems. Especially, deep neural network (DNN)
methods have significantly reduced estimation errors for crowd counting
missions. Recent studies have demonstrated that DNNs are vulnerable to
adversarial attacks, i.e., normal images with human-imperceptible perturbations
could mislead DNNs to make false predictions. In this work, we propose a robust
attack strategy called Adversarial Patch Attack with Momentum (APAM) to
systematically evaluate the robustness of crowd counting models, where the
attacker's goal is to create an adversarial perturbation that severely degrades
their performances, thus leading to public safety accidents (e.g., stampede
accidents). Especially, the proposed attack leverages the extreme-density
background information of input images to generate robust adversarial patches
via a series of transformations (e.g., interpolation, rotation, etc.). We
observe that by perturbing less than 6\% of image pixels, our attacks severely
degrade the performance of crowd counting systems, both digitally and
physically. To better enhance the adversarial robustness of crowd counting
models, we propose the first regression model-based Randomized Ablation (RA),
which is more sufficient than Adversarial Training (ADT) (Mean Absolute Error
of RA is 5 lower than ADT on clean samples and 30 lower than ADT on adversarial
examples). Extensive experiments on five crowd counting models demonstrate the
effectiveness and generality of the proposed method. The supplementary
materials and certificate retrained models are available at
\url{https://www.dropbox.com/s/hc4fdx133vht0qb/ACM_MM2021_Supp.pdf?dl=0}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qiming Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1"&gt;Zhikang Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1"&gt;Pan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xiaoqing Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Binghui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1"&gt;Ang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HDR Environment Map Estimation for Real-Time Augmented Reality. (arXiv:2011.10687v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10687</id>
        <link href="http://arxiv.org/abs/2011.10687"/>
        <updated>2021-07-29T02:00:08.676Z</updated>
        <summary type="html"><![CDATA[We present a method to estimate an HDR environment map from a narrow
field-of-view LDR camera image in real-time. This enables perceptually
appealing reflections and shading on virtual objects of any material finish,
from mirror to diffuse, rendered into a real physical environment using
augmented reality. Our method is based on our efficient convolutional neural
network architecture, EnvMapNet, trained end-to-end with two novel losses,
ProjectionLoss for the generated image, and ClusterLoss for adversarial
training. Through qualitative and quantitative comparison to state-of-the-art
methods, we demonstrate that our algorithm reduces the directional error of
estimated light sources by more than 50%, and achieves 3.7 times lower Frechet
Inception Distance (FID). We further showcase a mobile application that is able
to run our neural network model in under 9 ms on an iPhone XS, and render in
real-time, visually coherent virtual objects in previously unseen real-world
environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Somanath_G/0/1/0/all/0/1"&gt;Gowri Somanath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurz_D/0/1/0/all/0/1"&gt;Daniel Kurz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Efficient Performance Estimators of Neural Architectures. (arXiv:2008.03064v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.03064</id>
        <link href="http://arxiv.org/abs/2008.03064"/>
        <updated>2021-07-29T02:00:08.658Z</updated>
        <summary type="html"><![CDATA[Conducting efficient performance estimations of neural architectures is a
major challenge in neural architecture search (NAS). To reduce the architecture
training costs in NAS, one-shot estimators (OSEs) amortize the architecture
training costs by sharing the parameters of one supernet between all
architectures. Recently, zero-shot estimators (ZSEs) that involve no training
are proposed to further reduce the architecture evaluation cost. Despite the
high efficiency of these estimators, the quality of such estimations has not
been thoroughly studied. In this paper, we conduct an extensive and organized
assessment of OSEs and ZSEs on three NAS benchmarks: NAS-Bench-101/201/301.
Specifically, we employ a set of NAS-oriented criteria to study the behavior of
OSEs and ZSEs and reveal that they have certain biases and variances. After
analyzing how and why the OSE estimations are unsatisfying, we explore how to
mitigate the correlation gap of OSEs from several perspectives. For ZSEs, we
find that current ZSEs are not satisfying enough in these benchmark search
spaces, and analyze their biases. Through our analysis, we give out suggestions
for future application and development of efficient architecture performance
estimators. Furthermore, the analysis framework proposed in our work could be
utilized in future research to give a more comprehensive understanding of newly
designed architecture performance estimators. All codes and analysis scripts
are available at https://github.com/walkerning/aw_nas.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1"&gt;Xuefei Ning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1"&gt;Changcheng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenshuo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zixuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1"&gt;Shuang Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Huazhong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Unifying Intrinsic Calibration for Spinning and Solid-State LiDARs. (arXiv:2012.03321v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03321</id>
        <link href="http://arxiv.org/abs/2012.03321"/>
        <updated>2021-07-29T02:00:08.644Z</updated>
        <summary type="html"><![CDATA[Sensor calibration, which can be intrinsic or extrinsic, is an essential step
to achieve the measurement accuracy required for modern perception and
navigation systems deployed on autonomous robots. To date, intrinsic
calibration models for spinning LiDARs have been based on hypothesized based on
their physical mechanisms, resulting in anywhere from three to ten parameters
to be estimated from data, while no phenomenological models have yet been
proposed for solid-state LiDARs. Instead of going down that road, we propose to
abstract away from the physics of a LiDAR type (spinning vs solid-state, for
example), and focus on the spatial geometry of the point cloud generated by the
sensor. By modeling the calibration parameters as an element of a special
matrix Lie Group, we achieve a unifying view of calibration for different types
of LiDARs. We further prove mathematically that the proposed model is
well-constrained (has a unique answer) given four appropriately orientated
targets. The proof provides a guideline for target positioning in the form of a
tetrahedron. Moreover, an existing Semidefinite programming global solver for
SE(3) can be modified to compute efficiently the optimal calibration
parameters. For solid state LiDARs, we illustrate how the method works in
simulation. For spinning LiDARs, we show with experimental data that the
proposed matrix Lie Group model performs equally well as physics-based models
in terms of reducing the P2P distance, while being more robust to noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiunn-Kai Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1"&gt;Chenxi Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Achar_M/0/1/0/all/0/1"&gt;Madhav Achar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghaffari_M/0/1/0/all/0/1"&gt;Maani Ghaffari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grizzle_J/0/1/0/all/0/1"&gt;Jessy W. Grizzle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TEDS-Net: Enforcing Diffeomorphisms in Spatial Transformers to Guarantee Topology Preservation in Segmentations. (arXiv:2107.13542v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.13542</id>
        <link href="http://arxiv.org/abs/2107.13542"/>
        <updated>2021-07-29T02:00:08.636Z</updated>
        <summary type="html"><![CDATA[Accurate topology is key when performing meaningful anatomical segmentations,
however, it is often overlooked in traditional deep learning methods. In this
work we propose TEDS-Net: a novel segmentation method that guarantees accurate
topology. Our method is built upon a continuous diffeomorphic framework, which
enforces topology preservation. However, in practice, diffeomorphic fields are
represented using a finite number of parameters and sampled using methods such
as linear interpolation, violating the theoretical guarantees. We therefore
introduce additional modifications to more strictly enforce it. Our network
learns how to warp a binary prior, with the desired topological
characteristics, to complete the segmentation task. We tested our method on
myocardium segmentation from an open-source 2D heart dataset. TEDS-Net
preserved topology in 100% of the cases, compared to 90% from the U-Net,
without sacrificing on Hausdorff Distance or Dice performance. Code will be
made available at: www.github.com/mwyburd/TEDS-Net]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wyburd_M/0/1/0/all/0/1"&gt;Madeleine K. Wyburd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dinsdale_N/0/1/0/all/0/1"&gt;Nicola K. Dinsdale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Namburete_A/0/1/0/all/0/1"&gt;Ana I.L. Namburete&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jenkinson_M/0/1/0/all/0/1"&gt;Mark Jenkinson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When and how do CNNs generalize to out-of-distribution category-viewpoint combinations?. (arXiv:2007.08032v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.08032</id>
        <link href="http://arxiv.org/abs/2007.08032"/>
        <updated>2021-07-29T02:00:08.629Z</updated>
        <summary type="html"><![CDATA[Object recognition and viewpoint estimation lie at the heart of visual
understanding. Recent works suggest that convolutional neural networks (CNNs)
fail to generalize to out-of-distribution (OOD) category-viewpoint
combinations, ie. combinations not seen during training. In this paper, we
investigate when and how such OOD generalization may be possible by evaluating
CNNs trained to classify both object category and 3D viewpoint on OOD
combinations, and identifying the neural mechanisms that facilitate such OOD
generalization. We show that increasing the number of in-distribution
combinations (ie. data diversity) substantially improves generalization to OOD
combinations, even with the same amount of training data. We compare learning
category and viewpoint in separate and shared network architectures, and
observe starkly different trends on in-distribution and OOD combinations, ie.
while shared networks are helpful in-distribution, separate networks
significantly outperform shared ones at OOD combinations. Finally, we
demonstrate that such OOD generalization is facilitated by the neural mechanism
of specialization, ie. the emergence of two types of neurons -- neurons
selective to category and invariant to viewpoint, and vice versa.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Madan_S/0/1/0/all/0/1"&gt;Spandan Madan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henry_T/0/1/0/all/0/1"&gt;Timothy Henry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dozier_J/0/1/0/all/0/1"&gt;Jamell Dozier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_H/0/1/0/all/0/1"&gt;Helen Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhandari_N/0/1/0/all/0/1"&gt;Nishchal Bhandari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1"&gt;Tomotake Sasaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1"&gt;Fr&amp;#xe9;do Durand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1"&gt;Hanspeter Pfister&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1"&gt;Xavier Boix&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recursively Conditional Gaussian for Ordinal Unsupervised Domain Adaptation. (arXiv:2107.13467v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13467</id>
        <link href="http://arxiv.org/abs/2107.13467"/>
        <updated>2021-07-29T02:00:08.588Z</updated>
        <summary type="html"><![CDATA[There has been a growing interest in unsupervised domain adaptation (UDA) to
alleviate the data scalability issue, while the existing works usually focus on
classifying independently discrete labels. However, in many tasks (e.g.,
medical diagnosis), the labels are discrete and successively distributed. The
UDA for ordinal classification requires inducing non-trivial ordinal
distribution prior to the latent space. Target for this, the partially ordered
set (poset) is defined for constraining the latent vector. Instead of the
typically i.i.d. Gaussian latent prior, in this work, a recursively conditional
Gaussian (RCG) set is proposed for ordered constraint modeling, which admits a
tractable joint distribution prior. Furthermore, we are able to control the
density of content vectors that violate the poset constraint by a simple
"three-sigma rule". We explicitly disentangle the cross-domain images into a
shared ordinal prior induced ordinal content space and two separate
source/target ordinal-unrelated spaces, and the self-training is worked on the
shared space exclusively for ordinal-aware domain alignment. Extensive
experiments on UDA medical diagnoses and facial age estimation demonstrate its
effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Site Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1"&gt;Yubin Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1"&gt;Pengyi Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1"&gt;Jane You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jun Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantically Controllable Scene Generation with Guidance of Explicit Knowledge. (arXiv:2106.04066v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04066</id>
        <link href="http://arxiv.org/abs/2106.04066"/>
        <updated>2021-07-29T02:00:08.567Z</updated>
        <summary type="html"><![CDATA[Deep Generative Models (DGMs) are known for their superior capability in
generating realistic data. Extending purely data-driven approaches, recent
specialized DGMs may satisfy additional controllable requirements such as
embedding a traffic sign in a driving scene, by manipulating patterns
\textit{implicitly} in the neuron or feature level. In this paper, we introduce
a novel method to incorporate domain knowledge \textit{explicitly} in the
generation process to achieve semantically controllable scene generation. We
categorize our knowledge into two types to be consistent with the composition
of natural scenes, where the first type represents the property of objects and
the second type represents the relationship among objects. We then propose a
tree-structured generative model to learn complex scene representation, whose
nodes and edges are naturally corresponding to the two types of knowledge
respectively. Knowledge can be explicitly integrated to enable semantically
controllable scene generation by imposing semantic rules on properties of nodes
and edges in the tree structure. We construct a synthetic example to illustrate
the controllability and explainability of our method in a clean setting. We
further extend the synthetic example to realistic autonomous vehicle driving
environments and conduct extensive experiments to show that our method
efficiently identifies adversarial traffic scenes against different
state-of-the-art 3D point cloud segmentation models satisfying the traffic
rules specified as the explicit knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1"&gt;Wenhao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eun_K/0/1/0/all/0/1"&gt;Kim Ji Eun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Ding Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-speed object detection with a single-photon time-of-flight image sensor. (arXiv:2107.13407v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.13407</id>
        <link href="http://arxiv.org/abs/2107.13407"/>
        <updated>2021-07-29T02:00:08.547Z</updated>
        <summary type="html"><![CDATA[3D time-of-flight (ToF) imaging is used in a variety of applications such as
augmented reality (AR), computer interfaces, robotics and autonomous systems.
Single-photon avalanche diodes (SPADs) are one of the enabling technologies
providing accurate depth data even over long ranges. By developing SPADs in
array format with integrated processing combined with pulsed, flood-type
illumination, high-speed 3D capture is possible. However, array sizes tend to
be relatively small, limiting the lateral resolution of the resulting depth
maps, and, consequently, the information that can be extracted from the image
for applications such as object detection. In this paper, we demonstrate that
these limitations can be overcome through the use of convolutional neural
networks (CNNs) for high-performance object detection. We present outdoor
results from a portable SPAD camera system that outputs 16-bin photon timing
histograms with 64x32 spatial resolution. The results, obtained with exposure
times down to 2 ms (equivalent to 500 FPS) and in signal-to-background (SBR)
ratios as low as 0.05, point to the advantages of providing the CNN with full
histogram data rather than point clouds alone. Alternatively, a combination of
point cloud and active intensity data may be used as input, for a similar level
of performance. In either case, the GPU-accelerated processing time is less
than 1 ms per frame, leading to an overall latency (image acquisition plus
processing) in the millisecond range, making the results relevant for
safety-critical computer vision applications which would benefit from faster
than human reaction times.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mora_Martin_G/0/1/0/all/0/1"&gt;Germ&amp;#xe1;n Mora-Mart&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Turpin_A/0/1/0/all/0/1"&gt;Alex Turpin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ruget_A/0/1/0/all/0/1"&gt;Alice Ruget&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Halimi_A/0/1/0/all/0/1"&gt;Abderrahim Halimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Henderson_R/0/1/0/all/0/1"&gt;Robert Henderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Leach_J/0/1/0/all/0/1"&gt;Jonathan Leach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gyongy_I/0/1/0/all/0/1"&gt;Istvan Gyongy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Shared Knowledge from Non-COVID Lesions for Annotation-Efficient COVID-19 CT Lung Infection Segmentation. (arXiv:2012.15564v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15564</id>
        <link href="http://arxiv.org/abs/2012.15564"/>
        <updated>2021-07-29T02:00:08.536Z</updated>
        <summary type="html"><![CDATA[The novel Coronavirus disease (COVID-19) is a highly contagious virus and has
spread all over the world, posing an extremely serious threat to all countries.
Automatic lung infection segmentation from computed tomography (CT) plays an
important role in the quantitative analysis of COVID-19. However, the major
challenge lies in the inadequacy of annotated COVID-19 datasets. Currently,
there are several public non-COVID lung lesion segmentation datasets, providing
the potential for generalizing useful information to the related COVID-19
segmentation task. In this paper, we propose a novel relation-driven
collaborative learning model to exploit shared knowledge from non-COVID lesions
for annotation-efficient COVID-19 CT lung infection segmentation. The model
consists of a general encoder to capture general lung lesion features based on
multiple non-COVID lesions, and a target encoder to focus on task-specific
features based on COVID-19 infections. Features extracted from the two parallel
encoders are concatenated for the subsequent decoder part. We develop a
collaborative learning scheme to regularize feature-level relation consistency
of given input and encourage the model to learn more general and discriminative
representation of COVID-19 infections. Extensive experiments demonstrate that
trained with limited COVID-19 data, exploiting shared knowledge from non-COVID
lesions can further improve state-of-the-art performance with up to 3.0% in
dice similarity coefficient and 4.2% in normalized surface dice. Our proposed
method promotes new insights into annotation-efficient deep learning for
COVID-19 infection segmentation and illustrates strong potential for real-world
applications in the global fight against COVID-19 in the absence of sufficient
high-quality annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yichi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liao_Q/0/1/0/all/0/1"&gt;Qingcheng Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Lin Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_H/0/1/0/all/0/1"&gt;He Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xing_J/0/1/0/all/0/1"&gt;Jiezhen Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jicong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Densely Guided Knowledge Distillation using Multiple Teacher Assistants. (arXiv:2009.08825v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08825</id>
        <link href="http://arxiv.org/abs/2009.08825"/>
        <updated>2021-07-29T02:00:08.529Z</updated>
        <summary type="html"><![CDATA[With the success of deep neural networks, knowledge distillation which guides
the learning of a small student network from a large teacher network is being
actively studied for model compression and transfer learning. However, few
studies have been performed to resolve the poor learning issue of the student
network when the student and teacher model sizes significantly differ. In this
paper, we propose a densely guided knowledge distillation using multiple
teacher assistants that gradually decreases the model size to efficiently
bridge the large gap between the teacher and student networks. To stimulate
more efficient learning of the student network, we guide each teacher assistant
to every other smaller teacher assistants iteratively. Specifically, when
teaching a smaller teacher assistant at the next step, the existing larger
teacher assistants from the previous step are used as well as the teacher
network. Moreover, we design stochastic teaching where, for each mini-batch, a
teacher or teacher assistants are randomly dropped. This acts as a regularizer
to improve the efficiency of teaching of the student network. Thus, the student
can always learn salient distilled knowledge from the multiple sources. We
verified the effectiveness of the proposed method for a classification task
using CIFAR-10, CIFAR-100, and ImageNet. We also achieved significant
performance improvements with various backbone architectures such as ResNet,
WideResNet, and VGG.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Son_W/0/1/0/all/0/1"&gt;Wonchul Son&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Na_J/0/1/0/all/0/1"&gt;Jaemin Na&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Junyong Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1"&gt;Wonjun Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning the shape of female breasts: an open-access 3D statistical shape model of the female breast built from 110 breast scans. (arXiv:2107.13463v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13463</id>
        <link href="http://arxiv.org/abs/2107.13463"/>
        <updated>2021-07-29T02:00:08.502Z</updated>
        <summary type="html"><![CDATA[We present the Regensburg Breast Shape Model (RBSM) - a 3D statistical shape
model of the female breast built from 110 breast scans, and the first ever
publicly available. Together with the model, a fully automated, pairwise
surface registration pipeline used to establish correspondence among 3D breast
scans is introduced. Our method is computationally efficient and requires only
four landmarks to guide the registration process. In order to weaken the strong
coupling between breast and thorax, we propose to minimize the variance outside
the breast region as much as possible. To achieve this goal, a novel concept
called breast probability masks (BPMs) is introduced. A BPM assigns
probabilities to each point of a 3D breast scan, telling how likely it is that
a particular point belongs to the breast area. During registration, we use BPMs
to align the template to the target as accurately as possible inside the breast
region and only roughly outside. This simple yet effective strategy
significantly reduces the unwanted variance outside the breast region, leading
to better statistical shape models in which breast shapes are quite well
decoupled from the thorax. The RBSM is thus able to produce a variety of
different breast shapes as independently as possible from the shape of the
thorax. Our systematic experimental evaluation reveals a generalization ability
of 0.17 mm and a specificity of 2.8 mm for the RBSM. Ultimately, our model is
seen as a first step towards combining physically motivated deformable models
of the breast and statistical approaches in order to enable more realistic
surgical outcome simulation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weiherer_M/0/1/0/all/0/1"&gt;Maximilian Weiherer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eigenberger_A/0/1/0/all/0/1"&gt;Andreas Eigenberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brebant_V/0/1/0/all/0/1"&gt;Vanessa Br&amp;#xe9;bant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prantl_L/0/1/0/all/0/1"&gt;Lukas Prantl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palm_C/0/1/0/all/0/1"&gt;Christoph Palm&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI assisted method for efficiently generating breast ultrasound screening reports. (arXiv:2107.13431v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.13431</id>
        <link href="http://arxiv.org/abs/2107.13431"/>
        <updated>2021-07-29T02:00:08.487Z</updated>
        <summary type="html"><![CDATA[Ultrasound is the preferred choice for early screening of dense breast
cancer. Clinically, doctors have to manually write the screening report which
is time-consuming and laborious, and it is easy to miss and miswrite.
Therefore, this paper proposes a method for efficiently generating personalized
breast ultrasound screening preliminary reports by AI, especially for benign
and normal cases which account for the majority. Doctors then make simple
adjustments or corrections to quickly generate final reports. The proposed
approach has been tested using a database of 1133 breast tumor instances.
Experimental results indicate this pipeline improves doctors' work efficiency
by up to 90%, which greatly reduces repetitive work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ge_S/0/1/0/all/0/1"&gt;Shuang Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qiongyu Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xie_W/0/1/0/all/0/1"&gt;Wenquan Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_D/0/1/0/all/0/1"&gt;Desheng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huabin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xiaobo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yuan_K/0/1/0/all/0/1"&gt;Kehong Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi Point-Voxel Convolution (MPVConv) for Deep Learning on Point Clouds. (arXiv:2107.13152v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13152</id>
        <link href="http://arxiv.org/abs/2107.13152"/>
        <updated>2021-07-29T02:00:08.460Z</updated>
        <summary type="html"><![CDATA[The existing 3D deep learning methods adopt either individual point-based
features or local-neighboring voxel-based features, and demonstrate great
potential for processing 3D data. However, the point based models are
inefficient due to the unordered nature of point clouds and the voxel-based
models suffer from large information loss. Motivated by the success of recent
point-voxel representation, such as PVCNN, we propose a new convolutional
neural network, called Multi Point-Voxel Convolution (MPVConv), for deep
learning on point clouds. Integrating both the advantages of voxel and
point-based methods, MPVConv can effectively increase the neighboring
collection between point-based features and also promote independence among
voxel-based features. Moreover, most of the existing approaches aim at solving
one specific task, and only a few of them can handle a variety of tasks. Simply
replacing the corresponding convolution module with MPVConv, we show that
MPVConv can fit in different backbones to solve a wide range of 3D tasks.
Extensive experiments on benchmark datasets such as ShapeNet Part, S3DIS and
KITTI for various tasks show that MPVConv improves the accuracy of the backbone
(PointNet) by up to \textbf{36\%}, and achieves higher accuracy than the
voxel-based model with up to \textbf{34}$\times$ speedups. In addition, MPVConv
outperforms the state-of-the-art point-based models with up to
\textbf{8}$\times$ speedups. Notably, our MPVConv achieves better accuracy than
the newest point-voxel-based model PVCNN (a model more efficient than PointNet)
with lower latency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1"&gt;Xin Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaodan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_X/0/1/0/all/0/1"&gt;Xingxing Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dekui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Ying He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Task-Specific Normalization for Continual Learning of Blind Image Quality Models. (arXiv:2107.13429v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13429</id>
        <link href="http://arxiv.org/abs/2107.13429"/>
        <updated>2021-07-29T02:00:08.441Z</updated>
        <summary type="html"><![CDATA[The computational vision community has recently paid attention to continual
learning for blind image quality assessment (BIQA). The primary challenge is to
combat catastrophic forgetting of previously-seen IQA datasets (i.e., tasks).
In this paper, we present a simple yet effective continual learning method for
BIQA with improved quality prediction accuracy, plasticity-stability trade-off,
and task-order/length robustness. The key step in our approach is to freeze all
convolution filters of a pre-trained deep neural network (DNN) for an explicit
promise of stability, and learn task-specific normalization parameters for
plasticity. We assign each new task a prediction head, and load the
corresponding normalization parameters to produce a quality score. The final
quality estimate is computed by feature fusion and adaptive weighting using
hierarchical representations, without leveraging the test-time oracle.
Extensive experiments on six IQA datasets demonstrate the advantages of the
proposed method in comparison to previous training techniques for BIQA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weixia Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1"&gt;Kede Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1"&gt;Guangtao Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaokang Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A method to integrate and classify normal distributions. (arXiv:2012.14331v7 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14331</id>
        <link href="http://arxiv.org/abs/2012.14331"/>
        <updated>2021-07-29T02:00:08.434Z</updated>
        <summary type="html"><![CDATA[Univariate and multivariate normal probability distributions are widely used
when modeling decisions under uncertainty. Computing the performance of such
models requires integrating these distributions over specific domains, which
can vary widely across models. Besides some special cases where these integrals
are easy to calculate, there exist no general analytical expressions, standard
numerical methods or software for these integrals. Here we present mathematical
results and open-source software that provide (i) the probability in any domain
of a normal in any dimensions with any parameters, (ii) the probability
density, cumulative distribution, and inverse cumulative distribution of any
function of a normal vector, (iii) the classification errors among any number
of normal distributions, the Bayes-optimal discriminability index and relation
to the operating characteristic, (iv) dimension reduction and visualizations
for such problems, and (v) tests for how reliably these methods may be used on
given data. We demonstrate these tools with vision research applications of
detecting occluding objects in natural scenes, and detecting camouflage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Das_A/0/1/0/all/0/1"&gt;Abhranil Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Geisler_W/0/1/0/all/0/1"&gt;Wilson S Geisler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Indoor Future Person Localization from an Egocentric Wearable Camera. (arXiv:2103.04019v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04019</id>
        <link href="http://arxiv.org/abs/2103.04019"/>
        <updated>2021-07-29T02:00:08.406Z</updated>
        <summary type="html"><![CDATA[Accurate prediction of future person location and movement trajectory from an
egocentric wearable camera can benefit a wide range of applications, such as
assisting visually impaired people in navigation, and the development of
mobility assistance for people with disability. In this work, a new egocentric
dataset was constructed using a wearable camera, with 8,250 short clips of a
targeted person either walking 1) toward, 2) away, or 3) across the camera
wearer in indoor environments, or 4) staying still in the scene, and 13,817
person bounding boxes were manually labelled. Apart from the bounding boxes,
the dataset also contains the estimated pose of the targeted person as well as
the IMU signal of the wearable camera at each time point. An LSTM-based
encoder-decoder framework was designed to predict the future location and
movement trajectory of the targeted person in this egocentric setting.
Extensive experiments have been conducted on the new dataset, and have shown
that the proposed method is able to reliably and better predict future person
location and trajectory in egocentric videos captured by the wearable camera
compared to three baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1"&gt;Jianing Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lo_F/0/1/0/all/0/1"&gt;Frank P.-W. Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1"&gt;Xiao Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yingnan Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Shuo Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lo_B/0/1/0/all/0/1"&gt;Benny Lo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Realistic River Image Synthesis using Deep Generative Adversarial Networks. (arXiv:2003.00826v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.00826</id>
        <link href="http://arxiv.org/abs/2003.00826"/>
        <updated>2021-07-29T02:00:08.397Z</updated>
        <summary type="html"><![CDATA[In this paper, we demonstrated a practical application of realistic river
image generation using deep learning. Specifically, we explored a generative
adversarial network (GAN) model capable of generating high-resolution and
realistic river images that can be used to support modeling and analysis in
surface water estimation, river meandering, wetland loss, and other
hydrological research studies. First, we have created an extensive repository
of overhead river images to be used in training. Second, we incorporated the
Progressive Growing GAN (PGGAN), a network architecture that iteratively trains
smaller-resolution GANs to gradually build up to a very high resolution to
generate high quality (i.e., 1024x1024) synthetic river imagery. With simpler
GAN architectures, difficulties arose in terms of exponential increase of
training time and vanishing/exploding gradient issues, which the PGGAN
implementation seemed to significantly reduce. The results presented in this
study show great promise in generating high-quality images and capturing the
details of river structure and flow to support hydrological research, which
often requires extensive imagery for model performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gautam_A/0/1/0/all/0/1"&gt;Akshat Gautam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sit_M/0/1/0/all/0/1"&gt;Muhammed Sit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demir_I/0/1/0/all/0/1"&gt;Ibrahim Demir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Surrogate Model-Based Explainability Methods for Point Cloud NNs. (arXiv:2107.13459v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13459</id>
        <link href="http://arxiv.org/abs/2107.13459"/>
        <updated>2021-07-29T02:00:08.384Z</updated>
        <summary type="html"><![CDATA[In the field of autonomous driving and robotics, point clouds are showing
their excellent real-time performance as raw data from most of the mainstream
3D sensors. Therefore, point cloud neural networks have become a popular
research direction in recent years. So far, however, there has been little
discussion about the explainability of deep neural networks for point clouds.
In this paper, we propose new explainability approaches for point cloud deep
neural networks based on local surrogate model-based methods to show which
components make the main contribution to the classification. Moreover, we
propose a quantitative validation method for explainability methods of point
clouds which enhances the persuasive power of explainability by dropping the
most positive or negative contributing features and monitoring how the
classification scores of specific categories change. To enable an intuitive
explanation of misclassified instances, we display features with confounding
contributions. Our new explainability approach provides a fairly accurate, more
intuitive and widely applicable explanation for point cloud classification
tasks. Our code is available at https://github.com/Explain3D/Explainable3D]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1"&gt;Hanxiao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kotthaus_H/0/1/0/all/0/1"&gt;Helena Kotthaus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pseudo-LiDAR Based Road Detection. (arXiv:2107.13279v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13279</id>
        <link href="http://arxiv.org/abs/2107.13279"/>
        <updated>2021-07-29T02:00:08.377Z</updated>
        <summary type="html"><![CDATA[Road detection is a critically important task for self-driving cars. By
employing LiDAR data, recent works have significantly improved the accuracy of
road detection. Relying on LiDAR sensors limits the wide application of those
methods when only cameras are available. In this paper, we propose a novel road
detection approach with RGB being the only input during inference.
Specifically, we exploit pseudo-LiDAR using depth estimation, and propose a
feature fusion network where RGB and learned depth information are fused for
improved road detection. To further optimize the network structure and improve
the efficiency of the network. we search for the network structure of the
feature fusion module using NAS techniques. Finally, be aware of that
generating pseudo-LiDAR from RGB via depth estimation introduces extra
computational costs and relies on depth estimation networks, we design a
modality distillation strategy and leverage it to further free our network from
these extra computational cost and dependencies during inference. The proposed
method achieves state-of-the-art performance on two challenging benchmarks,
KITTI and R2D.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Libo Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haokui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1"&gt;Wei Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An explainable two-dimensional single model deep learning approach for Alzheimer's disease diagnosis and brain atrophy localization. (arXiv:2107.13200v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.13200</id>
        <link href="http://arxiv.org/abs/2107.13200"/>
        <updated>2021-07-29T02:00:08.356Z</updated>
        <summary type="html"><![CDATA[Early and accurate diagnosis of Alzheimer's disease (AD) and its prodromal
period mild cognitive impairment (MCI) is essential for the delayed disease
progression and the improved quality of patients'life. The emerging
computer-aided diagnostic methods that combine deep learning with structural
magnetic resonance imaging (sMRI) have achieved encouraging results, but some
of them are limit of issues such as data leakage and unexplainable diagnosis.
In this research, we propose a novel end-to-end deep learning approach for
automated diagnosis of AD and localization of important brain regions related
to the disease from sMRI data. This approach is based on a 2D single model
strategy and has the following differences from the current approaches: 1)
Convolutional Neural Network (CNN) models of different structures and
capacities are evaluated systemically and the most suitable model is adopted
for AD diagnosis; 2) a data augmentation strategy named Two-stage Random
RandAugment (TRRA) is proposed to alleviate the overfitting issue caused by
limited training data and to improve the classification performance in AD
diagnosis; 3) an explainable method of Grad-CAM++ is introduced to generate the
visually explainable heatmaps that localize and highlight the brain regions
that our model focuses on and to make our model more transparent. Our approach
has been evaluated on two publicly accessible datasets for two classification
tasks of AD vs. cognitively normal (CN) and progressive MCI (pMCI) vs. stable
MCI (sMCI). The experimental results indicate that our approach outperforms the
state-of-the-art approaches, including those using multi-model and 3D CNN
methods. The resultant localization heatmaps from our approach also highlight
the lateral ventricle and some disease-relevant regions of cortex, coincident
with the commonly affected regions during the development of AD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pan_B/0/1/0/all/0/1"&gt;Bo Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shao_P/0/1/0/all/0/1"&gt;Pengfei Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1"&gt;Peng Liu&lt;/a&gt; (Alzheimer&amp;#x27;s Disease Neuroimaging Initiative, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing), &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_S/0/1/0/all/0/1"&gt;Shuwei Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yao_P/0/1/0/all/0/1"&gt;Peng Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_R/0/1/0/all/0/1"&gt;Ronald X. Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search. (arXiv:1907.01845v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1907.01845</id>
        <link href="http://arxiv.org/abs/1907.01845"/>
        <updated>2021-07-29T02:00:08.349Z</updated>
        <summary type="html"><![CDATA[One of the most critical problems in weight-sharing neural architecture
search is the evaluation of candidate models within a predefined search space.
In practice, a one-shot supernet is trained to serve as an evaluator. A
faithful ranking certainly leads to more accurate searching results. However,
current methods are prone to making misjudgments. In this paper, we prove that
their biased evaluation is due to inherent unfairness in the supernet training.
In view of this, we propose two levels of constraints: expectation fairness and
strict fairness. Particularly, strict fairness ensures equal optimization
opportunities for all choice blocks throughout the training, which neither
overestimates nor underestimates their capacity. We demonstrate that this is
crucial for improving the confidence of models' ranking. Incorporating the
one-shot supernet trained under the proposed fairness constraints with a
multi-objective evolutionary search algorithm, we obtain various
state-of-the-art models, e.g., FairNAS-A attains 77.5% top-1 validation
accuracy on ImageNet. The models and their evaluation codes are made publicly
available online this http URL .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1"&gt;Xiangxiang Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Ruijun Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New Dataset, Poisson GAN and AquaNet for Underwater Object Grabbing. (arXiv:2003.01446v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.01446</id>
        <link href="http://arxiv.org/abs/2003.01446"/>
        <updated>2021-07-29T02:00:08.335Z</updated>
        <summary type="html"><![CDATA[To boost the object grabbing capability of underwater robots for open-sea
farming, we propose a new dataset (UDD) consisting of three categories
(seacucumber, seaurchin, and scallop) with 2,227 images. To the best of our
knowledge, it is the first 4K HD dataset collected in a real open-sea farm. We
also propose a novel Poisson-blending Generative Adversarial Network (Poisson
GAN) and an efficient object detection network (AquaNet) to address two common
issues within related datasets: the class-imbalance problem and the problem of
mass small object, respectively. Specifically, Poisson GAN combines Poisson
blending into its generator and employs a new loss called Dual Restriction loss
(DR loss), which supervises both implicit space features and image-level
features during training to generate more realistic images. By utilizing
Poisson GAN, objects of minority class like seacucumber or scallop could be
added into an image naturally and annotated automatically, which could increase
the loss of minority classes during training detectors to eliminate the
class-imbalance problem; AquaNet is a high-efficiency detector to address the
problem of detecting mass small objects from cloudy underwater pictures. Within
it, we design two efficient components: a depth-wise-convolution-based
Multi-scale Contextual Features Fusion (MFF) block and a Multi-scale
Blursampling (MBP) module to reduce the parameters of the network to 1.3
million. Both two components could provide multi-scale features of small
objects under a short backbone configuration without any loss of accuracy. In
addition, we construct a large-scale augmented dataset (AUDD) and a
pre-training dataset via Poisson GAN from UDD. Extensive experiments show the
effectiveness of the proposed Poisson GAN, AquaNet, UDD, AUDD, and pre-training
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chongwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhihui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shijie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1"&gt;Tao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1"&gt;Yulong Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Caifei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haojie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1"&gt;Xin Fan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning-Based Depth and Pose Estimation for Monocular Endoscope with Loss Generalization. (arXiv:2107.13263v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13263</id>
        <link href="http://arxiv.org/abs/2107.13263"/>
        <updated>2021-07-29T02:00:08.327Z</updated>
        <summary type="html"><![CDATA[Gastroendoscopy has been a clinical standard for diagnosing and treating
conditions that affect a part of a patient's digestive system, such as the
stomach. Despite the fact that gastroendoscopy has a lot of advantages for
patients, there exist some challenges for practitioners, such as the lack of 3D
perception, including the depth and the endoscope pose information. Such
challenges make navigating the endoscope and localizing any found lesion in a
digestive tract difficult. To tackle these problems, deep learning-based
approaches have been proposed to provide monocular gastroendoscopy with
additional yet important depth and pose information. In this paper, we propose
a novel supervised approach to train depth and pose estimation networks using
consecutive endoscopy images to assist the endoscope navigation in the stomach.
We firstly generate real depth and pose training data using our previously
proposed whole stomach 3D reconstruction pipeline to avoid poor generalization
ability between computer-generated (CG) models and real data for the stomach.
In addition, we propose a novel generalized photometric loss function to avoid
the complicated process of finding proper weights for balancing the depth and
the pose loss terms, which is required for existing direct depth and pose
supervision approaches. We then experimentally show that our proposed
generalized loss performs better than existing direct supervision losses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Widya_A/0/1/0/all/0/1"&gt;Aji Resindra Widya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Monno_Y/0/1/0/all/0/1"&gt;Yusuke Monno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Okutomi_M/0/1/0/all/0/1"&gt;Masatoshi Okutomi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suzuki_S/0/1/0/all/0/1"&gt;Sho Suzuki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gotoda_T/0/1/0/all/0/1"&gt;Takuji Gotoda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miki_K/0/1/0/all/0/1"&gt;Kenji Miki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SimROD: A Simple Adaptation Method for Robust Object Detection. (arXiv:2107.13389v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13389</id>
        <link href="http://arxiv.org/abs/2107.13389"/>
        <updated>2021-07-29T02:00:08.319Z</updated>
        <summary type="html"><![CDATA[This paper presents a Simple and effective unsupervised adaptation method for
Robust Object Detection (SimROD). To overcome the challenging issues of domain
shift and pseudo-label noise, our method integrates a novel domain-centric
augmentation method, a gradual self-labeling adaptation procedure, and a
teacher-guided fine-tuning mechanism. Using our method, target domain samples
can be leveraged to adapt object detection models without changing the model
architecture or generating synthetic data. When applied to image corruptions
and high-level cross-domain adaptation benchmarks, our method outperforms prior
baselines on multiple domain adaptation benchmarks. SimROD achieves new
state-of-the-art on standard real-to-synthetic and cross-camera setup
benchmarks. On the image corruption benchmark, models adapted with our method
achieved a relative robustness improvement of 15-25% AP50 on Pascal-C and 5-6%
AP on COCO-C and Cityscapes-C. On the cross-domain benchmark, our method
outperformed the best baseline performance by up to 8% AP50 on Comic dataset
and up to 4% on Watercolor dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ramamonjison_R/0/1/0/all/0/1"&gt;Rindra Ramamonjison&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banitalebi_Dehkordi_A/0/1/0/all/0/1"&gt;Amin Banitalebi-Dehkordi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1"&gt;Xinyu Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1"&gt;Xiaolong Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CRD-CGAN: Category-Consistent and Relativistic Constraints for Diverse Text-to-Image Generation. (arXiv:2107.13516v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13516</id>
        <link href="http://arxiv.org/abs/2107.13516"/>
        <updated>2021-07-29T02:00:08.298Z</updated>
        <summary type="html"><![CDATA[Generating photo-realistic images from a text description is a challenging
problem in computer vision. Previous works have shown promising performance to
generate synthetic images conditional on text by Generative Adversarial
Networks (GANs). In this paper, we focus on the category-consistent and
relativistic diverse constraints to optimize the diversity of synthetic images.
Based on those constraints, a category-consistent and relativistic diverse
conditional GAN (CRD-CGAN) is proposed to synthesize $K$ photo-realistic images
simultaneously. We use the attention loss and diversity loss to improve the
sensitivity of the GAN to word attention and noises. Then, we employ the
relativistic conditional loss to estimate the probability of relatively real or
fake for synthetic images, which can improve the performance of basic
conditional loss. Finally, we introduce a category-consistent loss to alleviate
the over-category issues between K synthetic images. We evaluate our approach
using the Birds-200-2011, Oxford-102 flower and MSCOCO 2014 datasets, and the
extensive experiments demonstrate superiority of the proposed method in
comparison with state-of-the-art methods in terms of photorealistic and
diversity of the generated synthetic images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1"&gt;Tao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1"&gt;Chengjiang Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Chunxia Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Computer Vision-Based Approach for Driver Distraction Recognition using Deep Learning and Genetic Algorithm Based Ensemble. (arXiv:2107.13355v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13355</id>
        <link href="http://arxiv.org/abs/2107.13355"/>
        <updated>2021-07-29T02:00:08.291Z</updated>
        <summary type="html"><![CDATA[As the proportion of road accidents increases each year, driver distraction
continues to be an important risk component in road traffic injuries and
deaths. The distractions caused by the increasing use of mobile phones and
other wireless devices pose a potential risk to road safety. Our current study
aims to aid the already existing techniques in driver posture recognition by
improving the performance in the driver distraction classification problem. We
present an approach using a genetic algorithm-based ensemble of six independent
deep neural architectures, namely, AlexNet, VGG-16, EfficientNet B0, Vanilla
CNN, Modified DenseNet, and InceptionV3 + BiLSTM. We test it on two
comprehensive datasets, the AUC Distracted Driver Dataset, on which our
technique achieves an accuracy of 96.37%, surpassing the previously obtained
95.98%, and on the State Farm Driver Distraction Dataset, on which we attain an
accuracy of 99.75%. The 6-Model Ensemble gave an inference time of 0.024
seconds as measured on our machine with Ubuntu 20.04(64-bit) and GPU as GeForce
GTX 1080.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Ashlesha Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sangwan_K/0/1/0/all/0/1"&gt;Kuldip Singh Sangwan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhiraj/0/1/0/all/0/1"&gt;Dhiraj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TransAction: ICL-SJTU Submission to EPIC-Kitchens Action Anticipation Challenge 2021. (arXiv:2107.13259v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13259</id>
        <link href="http://arxiv.org/abs/2107.13259"/>
        <updated>2021-07-29T02:00:08.283Z</updated>
        <summary type="html"><![CDATA[In this report, the technical details of our submission to the EPIC-Kitchens
Action Anticipation Challenge 2021 are given. We developed a hierarchical
attention model for action anticipation, which leverages Transformer-based
attention mechanism to aggregate features across temporal dimension,
modalities, symbiotic branches respectively. In terms of Mean Top-5 Recall of
action, our submission with team name ICL-SJTU achieved 13.39% for overall
testing set, 10.05% for unseen subsets and 11.88% for tailed subsets.
Additionally, it is noteworthy that our submission ranked 1st in terms of verb
class in all three (sub)sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1"&gt;Xiao Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1"&gt;Jianing Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lo_B/0/1/0/all/0/1"&gt;Benny Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Guang-Zhong Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Aggregation then Local Distribution for Scene Parsing. (arXiv:2107.13154v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13154</id>
        <link href="http://arxiv.org/abs/2107.13154"/>
        <updated>2021-07-29T02:00:08.276Z</updated>
        <summary type="html"><![CDATA[Modelling long-range contextual relationships is critical for pixel-wise
prediction tasks such as semantic segmentation. However, convolutional neural
networks (CNNs) are inherently limited to model such dependencies due to the
naive structure in its building modules (\eg, local convolution kernel). While
recent global aggregation methods are beneficial for long-range structure
information modelling, they would oversmooth and bring noise to the regions
containing fine details (\eg,~boundaries and small objects), which are very
much cared for the semantic segmentation task. To alleviate this problem, we
propose to explore the local context for making the aggregated long-range
relationship being distributed more accurately in local regions. In particular,
we design a novel local distribution module which models the affinity map
between global and local relationship for each pixel adaptively. Integrating
existing global aggregation modules, we show that our approach can be
modularized as an end-to-end trainable block and easily plugged into existing
semantic segmentation networks, giving rise to the \emph{GALD} networks.
Despite its simplicity and versatility, our approach allows us to build new
state of the art on major semantic segmentation benchmarks including
Cityscapes, ADE20K, Pascal Context, Camvid and COCO-stuff. Code and trained
models are released at \url{https://github.com/lxtGH/GALD-DGCNet} to foster
further research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiangtai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Li Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1"&gt;Guangliang Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kuiyuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1"&gt;Yunhai Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiatian Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1"&gt;Tao Xiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Normalization Matters in Weakly Supervised Object Localization. (arXiv:2107.13221v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13221</id>
        <link href="http://arxiv.org/abs/2107.13221"/>
        <updated>2021-07-29T02:00:08.264Z</updated>
        <summary type="html"><![CDATA[Weakly-supervised object localization (WSOL) enables finding an object using
a dataset without any localization information. By simply training a
classification model using only image-level annotations, the feature map of the
model can be utilized as a score map for localization. In spite of many WSOL
methods proposing novel strategies, there has not been any de facto standard
about how to normalize the class activation map (CAM). Consequently, many WSOL
methods have failed to fully exploit their own capacity because of the misuse
of a normalization method. In this paper, we review many existing normalization
methods and point out that they should be used according to the property of the
given dataset. Additionally, we propose a new normalization method which
substantially enhances the performance of any CAM-based WSOL methods. Using the
proposed normalization method, we provide a comprehensive evaluation over three
datasets (CUB, ImageNet and OpenImages) on three different architectures and
observe significant performance gains over the conventional min-max
normalization method in all the evaluated cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jeesoo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1"&gt;Junsuk Choe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1"&gt;Sangdoo Yun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1"&gt;Nojun Kwak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Multi-View Stereo via Super-Resolution. (arXiv:2107.13261v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13261</id>
        <link href="http://arxiv.org/abs/2107.13261"/>
        <updated>2021-07-29T02:00:08.240Z</updated>
        <summary type="html"><![CDATA[Today, Multi-View Stereo techniques are able to reconstruct robust and
detailed 3D models, especially when starting from high-resolution images.
However, there are cases in which the resolution of input images is relatively
low, for instance, when dealing with old photos, or when hardware constrains
the amount of data that can be acquired. In this paper, we investigate if, how,
and how much increasing the resolution of such input images through
Super-Resolution techniques reflects in quality improvements of the
reconstructed 3D models, despite the artifacts that sometimes this may
generate. We show that applying a Super-Resolution step before recovering the
depth maps in most cases leads to a better 3D model both in the case of
PatchMatch-based and deep-learning-based algorithms. The use of
Super-Resolution improves especially the completeness of reconstructed models
and turns out to be particularly effective in the case of textured scenes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lomurno_E/0/1/0/all/0/1"&gt;Eugenio Lomurno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romanoni_A/0/1/0/all/0/1"&gt;Andrea Romanoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matteucci_M/0/1/0/all/0/1"&gt;Matteo Matteucci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Proof-of-Concept Study of Artificial Intelligence Assisted Contour Revision. (arXiv:2107.13465v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13465</id>
        <link href="http://arxiv.org/abs/2107.13465"/>
        <updated>2021-07-29T02:00:08.231Z</updated>
        <summary type="html"><![CDATA[Automatic segmentation of anatomical structures is critical for many medical
applications. However, the results are not always clinically acceptable and
require tedious manual revision. Here, we present a novel concept called
artificial intelligence assisted contour revision (AIACR) and demonstrate its
feasibility. The proposed clinical workflow of AIACR is as follows given an
initial contour that requires a clinicians revision, the clinician indicates
where a large revision is needed, and a trained deep learning (DL) model takes
this input to update the contour. This process repeats until a clinically
acceptable contour is achieved. The DL model is designed to minimize the
clinicians input at each iteration and to minimize the number of iterations
needed to reach acceptance. In this proof-of-concept study, we demonstrated the
concept on 2D axial images of three head-and-neck cancer datasets, with the
clinicians input at each iteration being one mouse click on the desired
location of the contour segment. The performance of the model is quantified
with Dice Similarity Coefficient (DSC) and 95th percentile of Hausdorff
Distance (HD95). The average DSC/HD95 (mm) of the auto-generated initial
contours were 0.82/4.3, 0.73/5.6 and 0.67/11.4 for three datasets, which were
improved to 0.91/2.1, 0.86/2.4 and 0.86/4.7 with three mouse clicks,
respectively. Each DL-based contour update requires around 20 ms. We proposed a
novel AIACR concept that uses DL models to assist clinicians in revising
contours in an efficient and effective way, and we demonstrated its feasibility
by using 2D axial CT images from three head-and-neck cancer datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1"&gt;Ti Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balagopal_A/0/1/0/all/0/1"&gt;Anjali Balagopal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dohopolski_M/0/1/0/all/0/1"&gt;Michael Dohopolski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morgan_H/0/1/0/all/0/1"&gt;Howard E. Morgan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McBeth_R/0/1/0/all/0/1"&gt;Rafe McBeth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1"&gt;Jun Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Mu-Han Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sher_D/0/1/0/all/0/1"&gt;David J. Sher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Dan Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Steve Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Squeeze-Excitation Convolutional Recurrent Neural Networks for Audio-Visual Scene Classification. (arXiv:2107.13180v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.13180</id>
        <link href="http://arxiv.org/abs/2107.13180"/>
        <updated>2021-07-29T02:00:08.204Z</updated>
        <summary type="html"><![CDATA[The use of multiple and semantically correlated sources can provide
complementary information to each other that may not be evident when working
with individual modalities on their own. In this context, multi-modal models
can help producing more accurate and robust predictions in machine learning
tasks where audio-visual data is available. This paper presents a multi-modal
model for automatic scene classification that exploits simultaneously auditory
and visual information. The proposed approach makes use of two separate
networks which are respectively trained in isolation on audio and visual data,
so that each network specializes in a given modality. The visual subnetwork is
a pre-trained VGG16 model followed by a bidiretional recurrent layer, while the
residual audio subnetwork is based on stacked squeeze-excitation convolutional
blocks trained from scratch. After training each subnetwork, the fusion of
information from the audio and visual streams is performed at two different
stages. The early fusion stage combines features resulting from the last
convolutional block of the respective subnetworks at different time steps to
feed a bidirectional recurrent structure. The late fusion stage combines the
output of the early fusion stage with the independent predictions provided by
the two subnetworks, resulting in the final prediction. We evaluate the method
using the recently published TAU Audio-Visual Urban Scenes 2021, which contains
synchronized audio and video recordings from 12 European cities in 10 different
scene classes. The proposed model has been shown to provide an excellent
trade-off between prediction performance (86.5%) and system complexity (15M
parameters) in the evaluation results of the DCASE 2021 Challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naranjo_Alcazar_J/0/1/0/all/0/1"&gt;Javier Naranjo-Alcazar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Castanos_S/0/1/0/all/0/1"&gt;Sergi Perez-Castanos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lopez_Garcia_A/0/1/0/all/0/1"&gt;Aaron Lopez-Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuccarello_P/0/1/0/all/0/1"&gt;Pedro Zuccarello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cobos_M/0/1/0/all/0/1"&gt;Maximo Cobos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferri_F/0/1/0/all/0/1"&gt;Francesc J. Ferri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rank-based verification for long-term face tracking in crowded scenes. (arXiv:2107.13273v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13273</id>
        <link href="http://arxiv.org/abs/2107.13273"/>
        <updated>2021-07-29T02:00:08.192Z</updated>
        <summary type="html"><![CDATA[Most current multi-object trackers focus on short-term tracking, and are
based on deep and complex systems that often cannot operate in real-time,
making them impractical for video-surveillance. In this paper we present a
long-term, multi-face tracking architecture conceived for working in crowded
contexts where faces are often the only visible part of a person. Our system
benefits from advances in the fields of face detection and face recognition to
achieve long-term tracking, and is particularly unconstrained to the motion and
occlusions of people. It follows a tracking-by-detection approach, combining a
fast short-term visual tracker with a novel online tracklet reconnection
strategy grounded on rank-based face verification. The proposed rank-based
constraint favours higher inter-class distance among tracklets, and reduces the
propagation of errors due to wrong reconnections. Additionally, a correction
module is included to correct past assignments with no extra computational
cost. We present a series of experiments introducing novel specialized metrics
for the evaluation of long-term tracking capabilities, and publicly release a
video dataset with 10 manually annotated videos and a total length of 8' 54".
Our findings validate the robustness of each of the proposed modules, and
demonstrate that, in these challenging contexts, our approach yields up to 50%
longer tracks than state-of-the-art deep learning trackers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barquero_G/0/1/0/all/0/1"&gt;Germ&amp;#xe1;n Barquero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hupont_I/0/1/0/all/0/1"&gt;Isabelle Hupont&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandez_C/0/1/0/all/0/1"&gt;Carles Fern&amp;#xe1;ndez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting the Future from First Person (Egocentric) Vision: A Survey. (arXiv:2107.13411v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13411</id>
        <link href="http://arxiv.org/abs/2107.13411"/>
        <updated>2021-07-29T02:00:08.160Z</updated>
        <summary type="html"><![CDATA[Egocentric videos can bring a lot of information about how humans perceive
the world and interact with the environment, which can be beneficial for the
analysis of human behaviour. The research in egocentric video analysis is
developing rapidly thanks to the increasing availability of wearable devices
and the opportunities offered by new large-scale egocentric datasets. As
computer vision techniques continue to develop at an increasing pace, the tasks
related to the prediction of future are starting to evolve from the need of
understanding the present. Predicting future human activities, trajectories and
interactions with objects is crucial in applications such as human-robot
interaction, assistive wearable technologies for both industrial and daily
living scenarios, entertainment and virtual or augmented reality. This survey
summarises the evolution of studies in the context of future prediction from
egocentric vision making an overview of applications, devices, existing
problems, commonly used datasets, models and input modalities. Our analysis
highlights that methods for future prediction from egocentric vision can have a
significant impact in a range of applications and that further research efforts
should be devoted to the standardisation of tasks and the proposal of datasets
considering real-world scenarios such as the ones with an industrial vocation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rodin_I/0/1/0/all/0/1"&gt;Ivan Rodin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Furnari_A/0/1/0/all/0/1"&gt;Antonino Furnari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mavroedis_D/0/1/0/all/0/1"&gt;Dimitrios Mavroedis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farinella_G/0/1/0/all/0/1"&gt;Giovanni Maria Farinella&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Content-aware Directed Propagation Network with Pixel Adaptive Kernel Attention. (arXiv:2107.13144v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13144</id>
        <link href="http://arxiv.org/abs/2107.13144"/>
        <updated>2021-07-29T02:00:08.140Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) have been not only widespread but also
achieved noticeable results on numerous applications including image
classification, restoration, and generation. Although the weight-sharing
property of convolutions makes them widely adopted in various tasks, its
content-agnostic characteristic can also be considered a major drawback. To
solve this problem, in this paper, we propose a novel operation, called pixel
adaptive kernel attention (PAKA). PAKA provides directivity to the filter
weights by multiplying spatially varying attention from learnable features. The
proposed method infers pixel-adaptive attention maps along the channel and
spatial directions separately to address the decomposed model with fewer
parameters. Our method is trainable in an end-to-end manner and applicable to
any CNN-based models. In addition, we propose an improved information
aggregation module with PAKA, called the hierarchical PAKA module (HPM). We
demonstrate the superiority of our HPM by presenting state-of-the-art
performance on semantic segmentation compared to the conventional information
aggregation modules. We validate the proposed method through additional
ablation studies and visualizing the effect of PAKA providing directivity to
the weights of convolutions. We also show the generalizability of the proposed
method by applying it to multi-modal tasks especially color-guided depth map
super-resolution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sagong_M/0/1/0/all/0/1"&gt;Min-Cheol Sagong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeo_Y/0/1/0/all/0/1"&gt;Yoon-Jae Yeo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1"&gt;Seung-Won Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_S/0/1/0/all/0/1"&gt;Sung-Jea Ko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Segmentation for Terracotta Warrior with Seed-Region-Growing CNN(SRG-Net). (arXiv:2107.13167v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13167</id>
        <link href="http://arxiv.org/abs/2107.13167"/>
        <updated>2021-07-29T02:00:08.132Z</updated>
        <summary type="html"><![CDATA[The repairing work of terracotta warriors in Emperor Qinshihuang Mausoleum
Site Museum is handcrafted by experts, and the increasing amounts of unearthed
pieces of terracotta warriors make the archaeologists too challenging to
conduct the restoration of terracotta warriors efficiently. We hope to segment
the 3D point cloud data of the terracotta warriors automatically and store the
fragment data in the database to assist the archaeologists in matching the
actual fragments with the ones in the database, which could result in higher
repairing efficiency of terracotta warriors. Moreover, the existing 3D neural
network research is mainly focusing on supervised classification, clustering,
unsupervised representation, and reconstruction. There are few pieces of
researches concentrating on unsupervised point cloud part segmentation. In this
paper, we present SRG-Net for 3D point clouds of terracotta warriors to address
these problems. Firstly, we adopt a customized seed-region-growing algorithm to
segment the point cloud coarsely. Then we present a supervised segmentation and
unsupervised reconstruction networks to learn the characteristics of 3D point
clouds. Finally, we combine the SRG algorithm with our improved CNN using a
refinement method. This pipeline is called SRG-Net, which aims at conducting
segmentation tasks on the terracotta warriors. Our proposed SRG-Net is
evaluated on the terracotta warriors data and ShapeNet dataset by measuring the
accuracy and the latency. The experimental results show that our SRG-Net
outperforms the state-of-the-art methods. Our code is shown in Code File
1~\cite{Srgnet_2021}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_G/0/1/0/all/0/1"&gt;Guohua Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Kang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_X/0/1/0/all/0/1"&gt;Xingxing Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1"&gt;Xin Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Constrained Data Representation Learning for Human Motion Segmentation. (arXiv:2107.13362v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13362</id>
        <link href="http://arxiv.org/abs/2107.13362"/>
        <updated>2021-07-29T02:00:08.124Z</updated>
        <summary type="html"><![CDATA[Recently, transfer subspace learning based approaches have shown to be a
valid alternative to unsupervised subspace clustering and temporal data
clustering for human motion segmentation (HMS). These approaches leverage prior
knowledge from a source domain to improve clustering performance on a target
domain, and currently they represent the state of the art in HMS. Bucking this
trend, in this paper, we propose a novel unsupervised model that learns a
representation of the data and digs clustering information from the data
itself. Our model is reminiscent of temporal subspace clustering, but presents
two critical differences. First, we learn an auxiliary data matrix that can
deviate from the initial data, hence confer more degrees of freedom to the
coding matrix. Second, we introduce a regularization term for this auxiliary
data matrix that preserves the local geometrical structure present in the
high-dimensional space. The proposed model is efficiently optimized by using an
original Alternating Direction Method of Multipliers (ADMM) formulation
allowing to learn jointly the auxiliary data representation, a nonnegative
dictionary and a coding matrix. Experimental results on four benchmark datasets
for HMS demonstrate that our approach achieves significantly better clustering
performance then state-of-the-art methods, including both unsupervised and more
recent semi-supervised transfer learning approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dimiccoli_M/0/1/0/all/0/1"&gt;Mariella Dimiccoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garrido_L/0/1/0/all/0/1"&gt;Llu&amp;#xed;s Garrido&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_Corominas_G/0/1/0/all/0/1"&gt;Guillem Rodriguez-Corominas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wendt_H/0/1/0/all/0/1"&gt;Herwig Wendt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Visual Domain Transfer Learning Approach for Heartbeat Sound Classification. (arXiv:2107.13237v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.13237</id>
        <link href="http://arxiv.org/abs/2107.13237"/>
        <updated>2021-07-29T02:00:08.116Z</updated>
        <summary type="html"><![CDATA[Heart disease is the most common reason for human mortality that causes
almost one-third of deaths throughout the world. Detecting the disease early
increases the chances of survival of the patient and there are several ways a
sign of heart disease can be detected early. This research proposes to convert
cleansed and normalized heart sound into visual mel scale spectrograms and then
using visual domain transfer learning approaches to automatically extract
features and categorize between heart sounds. Some of the previous studies
found that the spectrogram of various types of heart sounds is visually
distinguishable to human eyes, which motivated this study to experiment on
visual domain classification approaches for automated heart sound
classification. It will use convolution neural network-based architectures i.e.
ResNet, MobileNetV2, etc as the automated feature extractors from spectrograms.
These well-accepted models in the image domain showed to learn generalized
feature representations of cardiac sounds collected from different environments
with varying amplitude and noise levels. Model evaluation criteria used were
categorical accuracy, precision, recall, and AUROC as the chosen dataset is
unbalanced. The proposed approach has been implemented on datasets A and B of
the PASCAL heart sound collection and resulted in ~ 90% categorical accuracy
and AUROC of ~0.97 for both sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mukherjee_U/0/1/0/all/0/1"&gt;Uddipan Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pancholi_S/0/1/0/all/0/1"&gt;Sidharth Pancholi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepTeeth: A Teeth-photo Based Human Authentication System for Mobile and Hand-held Devices. (arXiv:2107.13217v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13217</id>
        <link href="http://arxiv.org/abs/2107.13217"/>
        <updated>2021-07-29T02:00:08.109Z</updated>
        <summary type="html"><![CDATA[This paper proposes teeth-photo, a new biometric modality for human
authentication on mobile and hand held devices. Biometrics samples are acquired
using the camera mounted on mobile device with the help of a mobile application
having specific markers to register the teeth area. Region of interest (RoI) is
then extracted using the markers and the obtained sample is enhanced using
contrast limited adaptive histogram equalization (CLAHE) for better visual
clarity. We propose a deep learning architecture and novel regularization
scheme to obtain highly discriminative embedding for small size RoI. Proposed
custom loss function was able to achieve perfect classification for the tiny
RoI of $75\times 75$ size. The model is end-to-end and few-shot and therefore
is very efficient in terms of time and energy requirements. The system can be
used in many ways including device unlocking and secure authentication. To the
best of our understanding, this is the first work on teeth-photo based
authentication for mobile device. Experiments have been conducted on an
in-house teeth-photo database collected using our application. The database is
made publicly available. Results have shown that the proposed system has
perfect accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arora_G/0/1/0/all/0/1"&gt;Geetika Arora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bharadwaj_R/0/1/0/all/0/1"&gt;Rohit K Bharadwaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tiwari_K/0/1/0/all/0/1"&gt;Kamlesh Tiwari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Video Instance Segmentation via Temporal Pyramid Routing. (arXiv:2107.13155v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13155</id>
        <link href="http://arxiv.org/abs/2107.13155"/>
        <updated>2021-07-29T02:00:08.090Z</updated>
        <summary type="html"><![CDATA[Video Instance Segmentation (VIS) is a new and inherently multi-task problem,
which aims to detect, segment and track each instance in a video sequence.
Existing approaches are mainly based on single-frame features or single-scale
features of multiple frames, where temporal information or multi-scale
information is ignored. To incorporate both temporal and scale information, we
propose a Temporal Pyramid Routing (TPR) strategy to conditionally align and
conduct pixel-level aggregation from a feature pyramid pair of two adjacent
frames. Specifically, TPR contains two novel components, including Dynamic
Aligned Cell Routing (DACR) and Cross Pyramid Routing (CPR), where DACR is
designed for aligning and gating pyramid features across temporal dimension,
while CPR transfers temporally aggregated features across scale dimension.
Moreover, our approach is a plug-and-play module and can be easily applied to
existing instance segmentation methods. Extensive experiments on YouTube-VIS
dataset demonstrate the effectiveness and efficiency of the proposed approach
on several state-of-the-art instance segmentation methods. Codes and trained
models will be publicly available to facilitate future
research.(\url{https://github.com/lxtGH/TemporalPyramidRouting}).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiangtai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Hao He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1"&gt;Henghui Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kuiyuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1"&gt;Guangliang Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jianping Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1"&gt;Yunhai Tong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Rays for Occlusion-aware Image-based Rendering. (arXiv:2107.13421v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13421</id>
        <link href="http://arxiv.org/abs/2107.13421"/>
        <updated>2021-07-29T02:00:08.082Z</updated>
        <summary type="html"><![CDATA[We present a new neural representation, called Neural Ray (NeuRay), for the
novel view synthesis (NVS) task with multi-view images as input. Existing
neural scene representations for solving the NVS problem, such as NeRF, cannot
generalize to new scenes and take an excessively long time on training on each
new scene from scratch. The other subsequent neural rendering methods based on
stereo matching, such as PixelNeRF, SRF and IBRNet are designed to generalize
to unseen scenes but suffer from view inconsistency in complex scenes with
self-occlusions. To address these issues, our NeuRay method represents every
scene by encoding the visibility of rays associated with the input views. This
neural representation can efficiently be initialized from depths estimated by
external MVS methods, which is able to generalize to new scenes and achieves
satisfactory rendering images without any training on the scene. Then, the
initialized NeuRay can be further optimized on every scene with little training
timing to enforce spatial coherence to ensure view consistency in the presence
of severe self-occlusion. Experiments demonstrate that NeuRay can quickly
generate high-quality novel view images of unseen scenes with little finetuning
and can handle complex scenes with severe self-occlusions which previous
methods struggle with.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1"&gt;Sida Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingjie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qianqian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Peng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xiaowei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenping Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[C^3Net: End-to-End deep learning for efficient real-time visual active camera control. (arXiv:2107.13233v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13233</id>
        <link href="http://arxiv.org/abs/2107.13233"/>
        <updated>2021-07-29T02:00:08.071Z</updated>
        <summary type="html"><![CDATA[The need for automated real-time visual systems in applications such as smart
camera surveillance, smart environments, and drones necessitates the
improvement of methods for visual active monitoring and control. Traditionally,
the active monitoring task has been handled through a pipeline of modules such
as detection, filtering, and control. However, such methods are difficult to
jointly optimize and tune their various parameters for real-time processing in
resource constraint systems. In this paper a deep Convolutional Camera
Controller Neural Network is proposed to go directly from visual information to
camera movement to provide an efficient solution to the active vision problem.
It is trained end-to-end without bounding box annotations to control a camera
and follow multiple targets from raw pixel values. Evaluation through both a
simulation framework and real experimental setup, indicate that the proposed
solution is robust to varying conditions and able to achieve better monitoring
performance than traditional approaches both in terms of number of targets
monitored as well as in effective monitoring time. The advantage of the
proposed approach is that it is computationally less demanding and can run at
over 10 FPS (~4x speedup) on an embedded smart camera providing a practical and
affordable solution to real-time active monitoring.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kyrkou_C/0/1/0/all/0/1"&gt;Christos Kyrkou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessment of Deep Learning-based Heart Rate Estimation using Remote Photoplethysmography under Different Illuminations. (arXiv:2107.13193v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13193</id>
        <link href="http://arxiv.org/abs/2107.13193"/>
        <updated>2021-07-29T02:00:08.022Z</updated>
        <summary type="html"><![CDATA[Remote photoplethysmography (rPPG) monitors heart rate without requiring
physical contact, which allows for a wide variety of applications. Deep
learning-based rPPG have demonstrated superior performance over the traditional
approaches in controlled context. However, the lighting situation in indoor
space is typically complex, with uneven light distribution and frequent
variations in illumination. It lacks a fair comparison of different methods
under different illuminations using the same dataset. In this paper, we present
a public dataset, namely the BH-rPPG dataset, which contains data from twelve
subjects under three illuminations: low, medium, and high illumination. We also
provide the ground truth heart rate measured by an oximeter. We evaluate the
performance of three deep learning-based methods to that of four traditional
methods using two public datasets: the UBFC-rPPG dataset and the BH-rPPG
dataset. The experimental results demonstrate that traditional methods are
generally more resistant to fluctuating illuminations. We found that the
rPPGNet achieves lowest MAE among deep learning-based method under medium
illumination, whereas the CHROM achieves 1.5 beats per minute (BPM),
outperforming the rPPGNet by 60%. These findings suggest that while developing
deep learning-based heart rate estimation algorithms, illumination variation
should be taken into account. This work serves as a benchmark for rPPG
performance evaluation and it opens a pathway for future investigation into
deep learning-based rPPG under illumination variations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Ze Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haofei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1"&gt;Feng Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Whole Slide Images are 2D Point Clouds: Context-Aware Survival Prediction using Patch-based Graph Convolutional Networks. (arXiv:2107.13048v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.13048</id>
        <link href="http://arxiv.org/abs/2107.13048"/>
        <updated>2021-07-29T02:00:07.969Z</updated>
        <summary type="html"><![CDATA[Cancer prognostication is a challenging task in computational pathology that
requires context-aware representations of histology features to adequately
infer patient survival. Despite the advancements made in weakly-supervised deep
learning, many approaches are not context-aware and are unable to model
important morphological feature interactions between cell identities and tissue
types that are prognostic for patient survival. In this work, we present
Patch-GCN, a context-aware, spatially-resolved patch-based graph convolutional
network that hierarchically aggregates instance-level histology features to
model local- and global-level topological structures in the tumor
microenvironment. We validate Patch-GCN with 4,370 gigapixel WSIs across five
different cancer types from the Cancer Genome Atlas (TCGA), and demonstrate
that Patch-GCN outperforms all prior weakly-supervised approaches by
3.58-9.46%. Our code and corresponding models are publicly available at
https://github.com/mahmoodlab/Patch-GCN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_R/0/1/0/all/0/1"&gt;Richard J. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_M/0/1/0/all/0/1"&gt;Ming Y. Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shaban_M/0/1/0/all/0/1"&gt;Muhammad Shaban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chengkuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tiffany Y. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Williamson_D/0/1/0/all/0/1"&gt;Drew F. K. Williamson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mahmood_F/0/1/0/all/0/1"&gt;Faisal Mahmood&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aug3D-RPN: Improving Monocular 3D Object Detection by Synthetic Images with Virtual Depth. (arXiv:2107.13269v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13269</id>
        <link href="http://arxiv.org/abs/2107.13269"/>
        <updated>2021-07-29T02:00:07.928Z</updated>
        <summary type="html"><![CDATA[Current geometry-based monocular 3D object detection models can efficiently
detect objects by leveraging perspective geometry, but their performance is
limited due to the absence of accurate depth information. Though this issue can
be alleviated in a depth-based model where a depth estimation module is plugged
to predict depth information before 3D box reasoning, the introduction of such
module dramatically reduces the detection speed. Instead of training a costly
depth estimator, we propose a rendering module to augment the training data by
synthesizing images with virtual-depths. The rendering module takes as input
the RGB image and its corresponding sparse depth image, outputs a variety of
photo-realistic synthetic images, from which the detection model can learn more
discriminative features to adapt to the depth changes of the objects. Besides,
we introduce an auxiliary module to improve the detection model by jointly
optimizing it through a depth estimation task. Both modules are working in the
training time and no extra computation will be introduced to the detection
model. Experiments show that by working with our proposed modules, a
geometry-based model can represent the leading accuracy on the KITTI 3D
detection benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1"&gt;Chenhang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jianqiang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1"&gt;Xian-Sheng Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Retinal Microvasculature as Biomarker for Diabetes and Cardiovascular Diseases. (arXiv:2107.13157v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.13157</id>
        <link href="http://arxiv.org/abs/2107.13157"/>
        <updated>2021-07-29T02:00:07.920Z</updated>
        <summary type="html"><![CDATA[Purpose: To demonstrate that retinal microvasculature per se is a reliable
biomarker for Diabetic Retinopathy (DR) and, by extension, cardiovascular
diseases. Methods: Deep Learning Convolutional Neural Networks (CNN) applied to
color fundus images for semantic segmentation of the blood vessels and severity
classification on both vascular and full images. Vessel reconstruction through
harmonic descriptors is also used as a smoothing and de-noising tool. The
mathematical background of the theory is also outlined. Results: For diabetic
patients, at least 93.8% of DR No-Refer vs. Refer classification can be related
to vasculature defects. As for the Non-Sight Threatening vs. Sight Threatening
case, the ratio is as high as 96.7%. Conclusion: In the case of DR, most of the
disease biomarkers are related topologically to the vasculature. Translational
Relevance: Experiments conducted on eye blood vasculature reconstruction as a
biomarker shows a strong correlation between vasculature shape and later stages
of DR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Trivedi_A/0/1/0/all/0/1"&gt;Anusua Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Desbiens_J/0/1/0/all/0/1"&gt;Jocelyn Desbiens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gross_R/0/1/0/all/0/1"&gt;Ron Gross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Sunil Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dodhia_R/0/1/0/all/0/1"&gt;Rahul Dodhia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ferres_J/0/1/0/all/0/1"&gt;Juan Lavista Ferres&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exceeding the Limits of Visual-Linguistic Multi-Task Learning. (arXiv:2107.13054v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.13054</id>
        <link href="http://arxiv.org/abs/2107.13054"/>
        <updated>2021-07-29T02:00:07.913Z</updated>
        <summary type="html"><![CDATA[By leveraging large amounts of product data collected across hundreds of live
e-commerce websites, we construct 1000 unique classification tasks that share
similarly-structured input data, comprised of both text and images. These
classification tasks focus on learning the product hierarchy of different
e-commerce websites, causing many of them to be correlated. Adopting a
multi-modal transformer model, we solve these tasks in unison using multi-task
learning (MTL). Extensive experiments are presented over an initial 100-task
dataset to reveal best practices for "large-scale MTL" (i.e., MTL with more
than 100 tasks). From these experiments, a final, unified methodology is
derived, which is composed of both best practices and new proposals such as
DyPa, a simple heuristic for automatically allocating task-specific parameters
to tasks that could benefit from extra capacity. Using our large-scale MTL
methodology, we successfully train a single model across all 1000 tasks in our
dataset while using minimal task specific parameters, thereby showing that it
is possible to extend several orders of magnitude beyond current efforts in
MTL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1"&gt;Cameron R. Wolfe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lundgaard_K/0/1/0/all/0/1"&gt;Keld T. Lundgaard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MixFaceNets: Extremely Efficient Face Recognition Networks. (arXiv:2107.13046v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13046</id>
        <link href="http://arxiv.org/abs/2107.13046"/>
        <updated>2021-07-29T02:00:07.905Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a set of extremely efficient and high throughput
models for accurate face verification, MixFaceNets which are inspired by Mixed
Depthwise Convolutional Kernels. Extensive experiment evaluations on Label Face
in the Wild (LFW), Age-DB, MegaFace, and IARPA Janus Benchmarks IJB-B and IJB-C
datasets have shown the effectiveness of our MixFaceNets for applications
requiring extremely low computational complexity. Under the same level of
computation complexity (< 500M FLOPs), our MixFaceNets outperform
MobileFaceNets on all the evaluated datasets, achieving 99.60% accuracy on LFW,
97.05% accuracy on AgeDB-30, 93.60 TAR (at FAR1e-6) on MegaFace, 90.94 TAR (at
FAR1e-4) on IJB-B and 93.08 TAR (at FAR1e-4) on IJB-C. With computational
complexity between 500M and 1G FLOPs, our MixFaceNets achieved results
comparable to the top-ranked models, while using significantly fewer FLOPs and
less computation overhead, which proves the practical value of our proposed
MixFaceNets. All training codes, pre-trained models, and training logs have
been made available https://github.com/fdbtrs/mixfacenets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1"&gt;Fadi Boutros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1"&gt;Naser Damer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1"&gt;Meiling Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1"&gt;Florian Kirchbuchner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1"&gt;Arjan Kuijper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accurate Grid Keypoint Learning for Efficient Video Prediction. (arXiv:2107.13170v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13170</id>
        <link href="http://arxiv.org/abs/2107.13170"/>
        <updated>2021-07-29T02:00:07.898Z</updated>
        <summary type="html"><![CDATA[Video prediction methods generally consume substantial computing resources in
training and deployment, among which keypoint-based approaches show promising
improvement in efficiency by simplifying dense image prediction to light
keypoint prediction. However, keypoint locations are often modeled only as
continuous coordinates, so noise from semantically insignificant deviations in
videos easily disrupt learning stability, leading to inaccurate keypoint
modeling. In this paper, we design a new grid keypoint learning framework,
aiming at a robust and explainable intermediate keypoint representation for
long-term efficient video prediction. We have two major technical
contributions. First, we detect keypoints by jumping among candidate locations
in our raised grid space and formulate a condensation loss to encourage
meaningful keypoints with strong representative capability. Second, we
introduce a 2D binary map to represent the detected grid keypoints and then
suggest propagating keypoint locations with stochasticity by selecting entries
in the discrete grid space, thus preserving the spatial structure of keypoints
in the longterm horizon for better future frame generation. Extensive
experiments verify that our method outperforms the state-ofthe-art stochastic
video prediction methods while saves more than 98% of computing resources. We
also demonstrate our method on a robotic-assisted surgery dataset with
promising results. Our code is available at
https://github.com/xjgaocs/Grid-Keypoint-Learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xiaojie Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1"&gt;Yueming Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1"&gt;Qi Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Chi-Wing Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1"&gt;Pheng-Ann Heng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Thorough Review on Recent Deep Learning Methodologies for Image Captioning. (arXiv:2107.13114v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13114</id>
        <link href="http://arxiv.org/abs/2107.13114"/>
        <updated>2021-07-29T02:00:07.877Z</updated>
        <summary type="html"><![CDATA[Image Captioning is a task that combines computer vision and natural language
processing, where it aims to generate descriptive legends for images. It is a
two-fold process relying on accurate image understanding and correct language
understanding both syntactically and semantically. It is becoming increasingly
difficult to keep up with the latest research and findings in the field of
image captioning due to the growing amount of knowledge available on the topic.
There is not, however, enough coverage of those findings in the available
review papers. We perform in this paper a run-through of the current
techniques, datasets, benchmarks and evaluation metrics used in image
captioning. The current research on the field is mostly focused on deep
learning-based methods, where attention mechanisms along with deep
reinforcement and adversarial learning appear to be in the forefront of this
research topic. In this paper, we review recent methodologies such as UpDown,
OSCAR, VIVO, Meta Learning and a model that uses conditional generative
adversarial nets. Although the GAN-based model achieves the highest score,
UpDown represents an important basis for image captioning and OSCAR and VIVO
are more useful as they use novel object captioning. This review paper serves
as a roadmap for researchers to keep up to date with the latest contributions
made in the field of image caption generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elhagry_A/0/1/0/all/0/1"&gt;Ahmed Elhagry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadaoui_K/0/1/0/all/0/1"&gt;Karima Kadaoui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Monocular Depth Estimation in Highly Complex Environments. (arXiv:2107.13137v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13137</id>
        <link href="http://arxiv.org/abs/2107.13137"/>
        <updated>2021-07-29T02:00:07.868Z</updated>
        <summary type="html"><![CDATA[Previous unsupervised monocular depth estimation methods mainly focus on the
day-time scenario, and their frameworks are driven by warped photometric
consistency. While in some challenging environments, like night, rainy night or
snowy winter, the photometry of the same pixel on different frames is
inconsistent because of the complex lighting and reflection, so that the
day-time unsupervised frameworks cannot be directly applied to these complex
scenarios. In this paper, we investigate the problem of unsupervised monocular
depth estimation in certain highly complex scenarios. We address this
challenging problem by using domain adaptation, and a unified image
transfer-based adaptation framework is proposed based on monocular videos in
this paper. The depth model trained on day-time scenarios is adapted to
different complex scenarios. Instead of adapting the whole depth network, we
just consider the encoder network for lower computational complexity. The depth
models adapted by the proposed framework to different scenarios share the same
decoder, which is practical. Constraints on both feature space and output space
promote the framework to learn the key features for depth decoding, and the
smoothness loss is introduced into the adaptation framework for better depth
estimation performance. Extensive experiments show the effectiveness of the
proposed unsupervised framework in estimating the dense depth map from the
night-time, rainy night-time and snowy winter images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1"&gt;Chaoqiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1"&gt;Qiyu Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Experimenting with Self-Supervision using Rotation Prediction for Image Captioning. (arXiv:2107.13111v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13111</id>
        <link href="http://arxiv.org/abs/2107.13111"/>
        <updated>2021-07-29T02:00:07.855Z</updated>
        <summary type="html"><![CDATA[Image captioning is a task in the field of Artificial Intelligence that
merges between computer vision and natural language processing. It is
responsible for generating legends that describe images, and has various
applications like descriptions used by assistive technology or indexing images
(for search engines for instance). This makes it a crucial topic in AI that is
undergoing a lot of research. This task however, like many others, is trained
on large images labeled via human annotation, which can be very cumbersome: it
needs manual effort, both financial and temporal costs, it is error-prone and
potentially difficult to execute in some cases (e.g. medical images). To
mitigate the need for labels, we attempt to use self-supervised learning, a
type of learning where models use the data contained within the images
themselves as labels. It is challenging to accomplish though, since the task is
two-fold: the images and captions come from two different modalities and
usually handled by different types of networks. It is thus not obvious what a
completely self-supervised solution would look like. How it would achieve
captioning in a comparable way to how self-supervision is applied today on
image recognition tasks is still an ongoing research topic. In this project, we
are using an encoder-decoder architecture where the encoder is a convolutional
neural network (CNN) trained on OpenImages dataset and learns image features in
a self-supervised fashion using the rotation pretext task. The decoder is a
Long Short-Term Memory (LSTM), and it is trained, along within the image
captioning model, on MS COCO dataset and is responsible of generating captions.
Our GitHub repository can be found:
https://github.com/elhagry1/SSL_ImageCaptioning_RotationPrediction]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elhagry_A/0/1/0/all/0/1"&gt;Ahmed Elhagry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadaoui_K/0/1/0/all/0/1"&gt;Karima Kadaoui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Tale Of Two Long Tails. (arXiv:2107.13098v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13098</id>
        <link href="http://arxiv.org/abs/2107.13098"/>
        <updated>2021-07-29T02:00:07.847Z</updated>
        <summary type="html"><![CDATA[As machine learning models are increasingly employed to assist human
decision-makers, it becomes critical to communicate the uncertainty associated
with these model predictions. However, the majority of work on uncertainty has
focused on traditional probabilistic or ranking approaches - where the model
assigns low probabilities or scores to uncertain examples. While this captures
what examples are challenging for the model, it does not capture the underlying
source of the uncertainty. In this work, we seek to identify examples the model
is uncertain about and characterize the source of said uncertainty. We explore
the benefits of designing a targeted intervention - targeted data augmentation
of the examples where the model is uncertain over the course of training. We
investigate whether the rate of learning in the presence of additional
information differs between atypical and noisy examples? Our results show that
this is indeed the case, suggesting that well-designed interventions over the
course of training can be an effective way to characterize and distinguish
between different sources of uncertainty.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dsouza_D/0/1/0/all/0/1"&gt;Daniel D&amp;#x27;souza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nussbaum_Z/0/1/0/all/0/1"&gt;Zach Nussbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1"&gt;Chirag Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1"&gt;Sara Hooker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Divide-and-Assemble: Learning Block-wise Memory for Unsupervised Anomaly Detection. (arXiv:2107.13118v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13118</id>
        <link href="http://arxiv.org/abs/2107.13118"/>
        <updated>2021-07-29T02:00:07.831Z</updated>
        <summary type="html"><![CDATA[Reconstruction-based methods play an important role in unsupervised anomaly
detection in images. Ideally, we expect a perfect reconstruction for normal
samples and poor reconstruction for abnormal samples. Since the
generalizability of deep neural networks is difficult to control, existing
models such as autoencoder do not work well. In this work, we interpret the
reconstruction of an image as a divide-and-assemble procedure. Surprisingly, by
varying the granularity of division on feature maps, we are able to modulate
the reconstruction capability of the model for both normal and abnormal
samples. That is, finer granularity leads to better reconstruction, while
coarser granularity leads to poorer reconstruction. With proper granularity,
the gap between the reconstruction error of normal and abnormal samples can be
maximized. The divide-and-assemble framework is implemented by embedding a
novel multi-scale block-wise memory module into an autoencoder network.
Besides, we introduce adversarial learning and explore the semantic latent
representation of the discriminator, which improves the detection of subtle
anomaly. We achieve state-of-the-art performance on the challenging MVTec AD
dataset. Remarkably, we improve the vanilla autoencoder model by 10.1% in terms
of the AUROC score.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1"&gt;Jinlei Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yingying Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1"&gt;Qiaoyong Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1"&gt;Di Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1"&gt;Shiliang Pu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hong Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Human Cell Classification in Sparse Datasets using Few-Shot Learning. (arXiv:2107.13093v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13093</id>
        <link href="http://arxiv.org/abs/2107.13093"/>
        <updated>2021-07-29T02:00:07.805Z</updated>
        <summary type="html"><![CDATA[Classifying and analyzing human cells is a lengthy procedure, often involving
a trained professional. In an attempt to expedite this process, an active area
of research involves automating cell classification through use of deep
learning-based techniques. In practice, a large amount of data is required to
accurately train these deep learning models. However, due to the sparse human
cell datasets currently available, the performance of these models is typically
low. This study investigates the feasibility of using few-shot learning-based
techniques to mitigate the data requirements for accurate training. The study
is comprised of three parts: First, current state-of-the-art few-shot learning
techniques are evaluated on human cell classification. The selected techniques
are trained on a non-medical dataset and then tested on two out-of-domain,
human cell datasets. The results indicate that, overall, the test accuracy of
state-of-the-art techniques decreased by at least 30% when transitioning from a
non-medical dataset to a medical dataset. Second, this study evaluates the
potential benefits, if any, to varying the backbone architecture and training
schemes in current state-of-the-art few-shot learning techniques when used in
human cell classification. Even with these variations, the overall test
accuracy decreased from 88.66% on non-medical datasets to 44.13% at best on the
medical datasets. Third, this study presents future directions for using
few-shot learning in human cell classification. In general, few-shot learning
in its current state performs poorly on human cell classification. The study
proves that attempts to modify existing network architectures are not effective
and concludes that future research effort should be focused on improving
robustness towards out-of-domain testing using optimization-based or
self-supervised few-shot learning techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Walsh_R/0/1/0/all/0/1"&gt;Reece Walsh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdelpakey_M/0/1/0/all/0/1"&gt;Mohamed H. Abdelpakey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shehata_M/0/1/0/all/0/1"&gt;Mohamed S. Shehata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_M/0/1/0/all/0/1"&gt;Mostafa M.Mohamed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Insights from Generative Modeling for Neural Video Compression. (arXiv:2107.13136v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.13136</id>
        <link href="http://arxiv.org/abs/2107.13136"/>
        <updated>2021-07-29T02:00:07.797Z</updated>
        <summary type="html"><![CDATA[While recent machine learning research has revealed connections between deep
generative models such as VAEs and rate-distortion losses used in learned
compression, most of this work has focused on images. In a similar spirit, we
view recently proposed neural video coding algorithms through the lens of deep
autoregressive and latent variable modeling. We present recent neural video
codecs as instances of a generalized stochastic temporal autoregressive
transform, and propose new avenues for further improvements inspired by
normalizing flows and structured priors. We propose several architectures that
yield state-of-the-art video compression performance on full-resolution video
and discuss their tradeoffs and ablations. In particular, we propose (i)
improved temporal autoregressive transforms, (ii) improved entropy models with
structured and temporal dependencies, and (iii) variable bitrate versions of
our algorithms. Since our improvements are compatible with a large class of
existing models, we provide further evidence that the generative modeling
viewpoint can advance the neural video coding field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1"&gt;Ruihan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yibo Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Marino_J/0/1/0/all/0/1"&gt;Joseph Marino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mandt_S/0/1/0/all/0/1"&gt;Stephan Mandt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image color correction, enhancement, and editing. (arXiv:2107.13117v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13117</id>
        <link href="http://arxiv.org/abs/2107.13117"/>
        <updated>2021-07-29T02:00:07.789Z</updated>
        <summary type="html"><![CDATA[This thesis presents methods and approaches to image color correction, color
enhancement, and color editing. To begin, we study the color correction problem
from the standpoint of the camera's image signal processor (ISP). A camera's
ISP is hardware that applies a series of in-camera image processing and color
manipulation steps, many of which are nonlinear in nature, to render the
initial sensor image to its final photo-finished representation saved in the
8-bit standard RGB (sRGB) color space. As white balance (WB) is one of the
major procedures applied by the ISP for color correction, this thesis presents
two different methods for ISP white balancing. Afterward, we discuss another
scenario of correcting and editing image colors, where we present a set of
methods to correct and edit WB settings for images that have been improperly
white-balanced by the ISP. Then, we explore another factor that has a
significant impact on the quality of camera-rendered colors, in which we
outline two different methods to correct exposure errors in camera-rendered
images. Lastly, we discuss post-capture auto color editing and manipulation. In
particular, we propose auto image recoloring methods to generate different
realistic versions of the same camera-rendered image with new colors. Through
extensive evaluations, we demonstrate that our methods provide superior
solutions compared to existing alternatives targeting color correction, color
enhancement, and color editing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1"&gt;Mahmoud Afifi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PlaneTR: Structure-Guided Transformers for 3D Plane Recovery. (arXiv:2107.13108v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13108</id>
        <link href="http://arxiv.org/abs/2107.13108"/>
        <updated>2021-07-29T02:00:07.753Z</updated>
        <summary type="html"><![CDATA[This paper presents a neural network built upon Transformers, namely PlaneTR,
to simultaneously detect and reconstruct planes from a single image. Different
from previous methods, PlaneTR jointly leverages the context information and
the geometric structures in a sequence-to-sequence way to holistically detect
plane instances in one forward pass. Specifically, we represent the geometric
structures as line segments and conduct the network with three main components:
(i) context and line segments encoders, (ii) a structure-guided plane decoder,
(iii) a pixel-wise plane embedding decoder. Given an image and its detected
line segments, PlaneTR generates the context and line segment sequences via two
specially designed encoders and then feeds them into a Transformers-based
decoder to directly predict a sequence of plane instances by simultaneously
considering the context and global structure cues. Finally, the pixel-wise
embeddings are computed to assign each pixel to one predicted plane instance
which is nearest to it in embedding space. Comprehensive experiments
demonstrate that PlaneTR achieves a state-of-the-art performance on the ScanNet
and NYUv2 datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1"&gt;Bin Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_N/0/1/0/all/0/1"&gt;Nan Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1"&gt;Song Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tianfu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1"&gt;Gui-Song Xia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DCL: Differential Contrastive Learning for Geometry-Aware Depth Synthesis. (arXiv:2107.13087v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13087</id>
        <link href="http://arxiv.org/abs/2107.13087"/>
        <updated>2021-07-29T02:00:07.746Z</updated>
        <summary type="html"><![CDATA[We describe a method for realistic depth synthesis that learns diverse
variations from the real depth scans and ensures geometric consistency for
effective synthetic-to-real transfer. Unlike general image synthesis pipelines,
where geometries are mostly ignored, we treat geometries carried by the depth
based on their own existence. We propose differential contrastive learning that
explicitly enforces the underlying geometric properties to be invariant
regarding the real variations been learned. The resulting depth synthesis
method is task-agnostic and can be used for training any task-specific networks
with synthetic labels. We demonstrate the effectiveness of the proposed method
by extensive evaluations on downstream real-world geometric reasoning tasks. We
show our method achieves better synthetic-to-real transfer performance than the
other state-of-the-art. When fine-tuned on a small number of real-world
annotations, our method can even surpass the fully supervised baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yanchao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yuefan Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Youyi Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;C. Karen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1"&gt;Leonidas Guibas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is Object Detection Necessary for Human-Object Interaction Recognition?. (arXiv:2107.13083v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13083</id>
        <link href="http://arxiv.org/abs/2107.13083"/>
        <updated>2021-07-29T02:00:07.738Z</updated>
        <summary type="html"><![CDATA[This paper revisits human-object interaction (HOI) recognition at image level
without using supervisions of object location and human pose. We name it
detection-free HOI recognition, in contrast to the existing
detection-supervised approaches which rely on object and keypoint detections to
achieve state of the art. With our method, not only the detection supervision
is evitable, but superior performance can be achieved by properly using
image-text pre-training (such as CLIP) and the proposed Log-Sum-Exp Sign
(LSE-Sign) loss function. Specifically, using text embeddings of class labels
to initialize the linear classifier is essential for leveraging the CLIP
pre-trained image encoder. In addition, LSE-Sign loss facilitates learning from
multiple labels on an imbalanced dataset by normalizing gradients over all
classes in a softmax format. Surprisingly, our detection-free solution achieves
60.5 mAP on the HICO dataset, outperforming the detection-supervised state of
the art by 13.4 mAP]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1"&gt;Ying Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yinpeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lijuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianfeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Pei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zicheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1"&gt;Jenq-Neng Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New Split for Evaluating True Zero-Shot Action Recognition. (arXiv:2107.13029v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13029</id>
        <link href="http://arxiv.org/abs/2107.13029"/>
        <updated>2021-07-29T02:00:07.688Z</updated>
        <summary type="html"><![CDATA[Zero-shot action recognition is the task of classifying action categories
that are not available in the training set. In this setting, the standard
evaluation protocol is to use existing action recognition datasets (e.g.
UCF101) and randomly split the classes into seen and unseen. However, most
recent work builds on representations pre-trained on the Kinetics dataset,
where classes largely overlap with classes in the zero-shot evaluation
datasets. As a result, classes which are supposed to be unseen, are present
during supervised pre-training, invalidating the condition of the zero-shot
setting. A similar concern was previously noted several years ago for image
based zero-shot recognition, but has not been considered by the zero-shot
action recognition community. In this paper, we propose a new split for true
zero-shot action recognition with no overlap between unseen test classes and
training or pre-training classes. We benchmark several recent approaches on the
proposed True Zero-Shot (TruZe) Split for UCF101 and HMDB51, with zero-shot and
generalized zero-shot evaluation. In our extensive analysis we find that our
TruZe splits are significantly harder than comparable random splits as nothing
is leaking from pre-training, i.e. unseen performance is consistently lower, up
to 9.4% for zero-shot action recognition. In an additional evaluation we also
find that similar issues exist in the splits used in few-shot action
recognition, here we see differences of up to 14.1%. We publish our splits and
hope that our benchmark analysis will change how the field is evaluating zero-
and few-shot action recognition moving forward.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gowda_S/0/1/0/all/0/1"&gt;Shreyank N Gowda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sevilla_Lara_L/0/1/0/all/0/1"&gt;Laura Sevilla-Lara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kiyoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1"&gt;Frank Keller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rohrbach_M/0/1/0/all/0/1"&gt;Marcus Rohrbach&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Long-Short Transformer: Efficient Transformers for Language and Vision. (arXiv:2107.02192v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02192</id>
        <link href="http://arxiv.org/abs/2107.02192"/>
        <updated>2021-07-29T02:00:07.674Z</updated>
        <summary type="html"><![CDATA[Transformers have achieved success in both language and vision domains.
However, it is prohibitively expensive to scale them to long sequences such as
long documents or high-resolution images, because self-attention mechanism has
quadratic time and memory complexities with respect to the input sequence
length. In this paper, we propose Long-Short Transformer (Transformer-LS), an
efficient self-attention mechanism for modeling long sequences with linear
complexity for both language and vision tasks. It aggregates a novel long-range
attention with dynamic projection to model distant correlations and a
short-term attention to capture fine-grained local correlations. We propose a
dual normalization strategy to account for the scale mismatch between the two
attention mechanisms. Transformer-LS can be applied to both autoregressive and
bidirectional models without additional complexity. Our method outperforms the
state-of-the-art models on multiple tasks in language and vision domains,
including the Long Range Arena benchmark, autoregressive language modeling, and
ImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on
enwik8 using half the number of parameters than previous method, while being
faster and is able to handle 3x as long sequences compared to its
full-attention version on the same hardware. On ImageNet, it can obtain the
state-of-the-art results (e.g., a moderate size of 55.8M model solely trained
on 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more
scalable on high-resolution images. The source code and models are released at
https://github.com/NVIDIA/transformer-ls .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1"&gt;Chen Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1"&gt;Wei Ping&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Chaowei Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1"&gt;Mohammad Shoeybi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1"&gt;Tom Goldstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1"&gt;Bryan Catanzaro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exceeding the Limits of Visual-Linguistic Multi-Task Learning. (arXiv:2107.13054v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.13054</id>
        <link href="http://arxiv.org/abs/2107.13054"/>
        <updated>2021-07-29T02:00:07.655Z</updated>
        <summary type="html"><![CDATA[By leveraging large amounts of product data collected across hundreds of live
e-commerce websites, we construct 1000 unique classification tasks that share
similarly-structured input data, comprised of both text and images. These
classification tasks focus on learning the product hierarchy of different
e-commerce websites, causing many of them to be correlated. Adopting a
multi-modal transformer model, we solve these tasks in unison using multi-task
learning (MTL). Extensive experiments are presented over an initial 100-task
dataset to reveal best practices for "large-scale MTL" (i.e., MTL with more
than 100 tasks). From these experiments, a final, unified methodology is
derived, which is composed of both best practices and new proposals such as
DyPa, a simple heuristic for automatically allocating task-specific parameters
to tasks that could benefit from extra capacity. Using our large-scale MTL
methodology, we successfully train a single model across all 1000 tasks in our
dataset while using minimal task specific parameters, thereby showing that it
is possible to extend several orders of magnitude beyond current efforts in
MTL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1"&gt;Cameron R. Wolfe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lundgaard_K/0/1/0/all/0/1"&gt;Keld T. Lundgaard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Plug-and-Blend: A Framework for Controllable Story Generation with Blended Control Codes. (arXiv:2104.04039v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04039</id>
        <link href="http://arxiv.org/abs/2104.04039"/>
        <updated>2021-07-29T02:00:07.636Z</updated>
        <summary type="html"><![CDATA[Large pre-trained neural language models (LM) have very powerful text
generation capabilities. However, in practice, they are hard to control for
creative purposes. We describe a Plug-and-Play controllable language generation
framework, Plug-and-Blend, that allows a human user to input multiple control
codes (topics). In the context of automated story generation, this allows a
human user loose or fine-grained control of the topics and transitions between
them that will appear in the generated story, and can even allow for
overlapping, blended topics. Automated evaluations show our framework, working
with different generative LMs, controls the generation towards given
continuous-weighted control codes while keeping the generated sentences fluent,
demonstrating strong blending capability. A human participant evaluation shows
that the generated stories are observably transitioning between two topics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhiyu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1"&gt;Mark Riedl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local and non-local dependency learning and emergence of rule-like representations in speech data by Deep Convolutional Generative Adversarial Networks. (arXiv:2009.12711v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.12711</id>
        <link href="http://arxiv.org/abs/2009.12711"/>
        <updated>2021-07-29T02:00:07.628Z</updated>
        <summary type="html"><![CDATA[This paper argues that training GANs on local and non-local dependencies in
speech data offers insights into how deep neural networks discretize continuous
data and how symbolic-like rule-based morphophonological processes emerge in a
deep convolutional architecture. Acquisition of speech has recently been
modeled as a dependency between latent space and data generated by GANs in
Begu\v{s} (2020b; arXiv:2006.03965), who models learning of a simple local
allophonic distribution. We extend this approach to test learning of local and
non-local phonological processes that include approximations of morphological
processes. We further parallel outputs of the model to results of a behavioral
experiment where human subjects are trained on the data used for training the
GAN network. Four main conclusions emerge: (i) the networks provide useful
information for computational models of speech acquisition even if trained on a
comparatively small dataset of an artificial grammar learning experiment; (ii)
local processes are easier to learn than non-local processes, which matches
both behavioral data in human subjects and typology in the world's languages.
This paper also proposes (iii) how we can actively observe the network's
progress in learning and explore the effect of training steps on learning
representations by keeping latent space constant across different training
steps. Finally, this paper shows that (iv) the network learns to encode the
presence of a prefix with a single latent variable; by interpolating this
variable, we can actively observe the operation of a non-local phonological
process. The proposed technique for retrieving learning representations has
general implications for our understanding of how GANs discretize continuous
speech data and suggests that rule-like generalizations in the training data
are represented as an interaction between variables in the network's latent
space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1"&gt;Ga&amp;#x161;per Begu&amp;#x161;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classifying Textual Data with Pre-trained Vision Models through Transfer Learning and Data Transformations. (arXiv:2106.12479v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12479</id>
        <link href="http://arxiv.org/abs/2106.12479"/>
        <updated>2021-07-29T02:00:07.616Z</updated>
        <summary type="html"><![CDATA[Knowledge is acquired by humans through experience, and no boundary is set
between the kinds of knowledge or skill levels we can achieve on different
tasks at the same time. When it comes to Neural Networks, that is not the case,
the major breakthroughs in the field are extremely task and domain specific.
Vision and language are dealt with in separate manners, using separate methods
and different datasets. In this work, we propose to use knowledge acquired by
benchmark Vision Models which are trained on ImageNet to help a much smaller
architecture learn to classify text. After transforming the textual data
contained in the IMDB dataset to gray scale images. An analysis of different
domains and the Transfer Learning method is carried out. Despite the challenge
posed by the very different datasets, promising results are achieved. The main
contribution of this work is a novel approach which links large pretrained
models on both language and vision to achieve state-of-the-art results in
different sub-fields from the original task. Without needing high compute
capacity resources. Specifically, Sentiment Analysis is achieved after
transferring knowledge between vision and language models. BERT embeddings are
transformed into grayscale images, these images are then used as training
examples for pre-trained vision models such as VGG16 and ResNet

Index Terms: BERT, Convolutional Neural Networks, Domain Adaptation, image
classification, Natural Language Processing, t-SNE, text classification,
Transfer Learning]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Benarab_C/0/1/0/all/0/1"&gt;Charaf Eddine Benarab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[POS tagging, lemmatization and dependency parsing of West Frisian. (arXiv:2107.07974v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.07974</id>
        <link href="http://arxiv.org/abs/2107.07974"/>
        <updated>2021-07-29T02:00:07.608Z</updated>
        <summary type="html"><![CDATA[We present a lemmatizer/POS-tagger/dependency parser for West Frisian using a
corpus of 44,714 words in 3,126 sentences that were annotated according to the
guidelines of Universal Dependency version 2. POS tags were assigned to words
by using a Dutch POS tagger that was applied to a literal word-by-word
translation, or to sentences of a Dutch parallel text. Best results were
obtained when using literal translations that were created by using the Frisian
translation program Oersetter. Morphologic and syntactic annotations were
generated on the basis of a literal Dutch translation as well. The performance
of the lemmatizer/tagger/annotator when it was trained using default parameters
was compared to the performance that was obtained when using the parameter
values that were used for training the LassySmall UD 2.5 corpus. A significant
improvement was found for `lemma'. The Frisian lemmatizer/PoS tagger/dependency
parser is released as a web app and as a web service.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heeringa_W/0/1/0/all/0/1"&gt;Wilbert Heeringa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouma_G/0/1/0/all/0/1"&gt;Gosse Bouma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofman_M/0/1/0/all/0/1"&gt;Martha Hofman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drenth_E/0/1/0/all/0/1"&gt;Eduard Drenth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wijffels_J/0/1/0/all/0/1"&gt;Jan Wijffels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Velde_H/0/1/0/all/0/1"&gt;Hans Van de Velde&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Listen, Read, and Identify: Multimodal Singing Language Identification of Music. (arXiv:2103.01893v4 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01893</id>
        <link href="http://arxiv.org/abs/2103.01893"/>
        <updated>2021-07-29T02:00:07.589Z</updated>
        <summary type="html"><![CDATA[We propose a multimodal singing language classification model that uses both
audio content and textual metadata. LRID-Net, the proposed model, takes an
audio signal and a language probability vector estimated from the metadata and
outputs the probabilities of the target languages. Optionally, LRID-Net is
facilitated with modality dropouts to handle a missing modality. In the
experiment, we trained several LRID-Nets with varying modality dropout
configuration and tested them with various combinations of input modalities.
The experiment results demonstrate that using multimodal input improves
performance. The results also suggest that adopting modality dropout does not
degrade the performance of the model when there are full modality inputs while
enabling the model to handle missing modality cases to some extent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1"&gt;Keunwoo Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuxuan Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Arabic aspect based sentiment analysis using BERT. (arXiv:2107.13290v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.13290</id>
        <link href="http://arxiv.org/abs/2107.13290"/>
        <updated>2021-07-29T02:00:07.582Z</updated>
        <summary type="html"><![CDATA[Aspect-based sentiment analysis(ABSA) is a textual analysis methodology that
defines the polarity of opinions on certain aspects related to specific
targets. The majority of research on ABSA is in English, with a small amount of
work available in Arabic. Most previous Arabic research has relied on deep
learning models that depend primarily on context-independent word embeddings
(e.g.word2vec), where each word has a fixed representation independent of its
context. This article explores the modeling capabilities of contextual
embeddings from pre-trained language models, such as BERT, and making use of
sentence pair input on Arabic ABSA tasks. In particular, we are building a
simple but effective BERT-based neural baseline to handle this task. Our BERT
architecture with a simple linear classification layer surpassed the
state-of-the-art works, according to the experimental results on the
benchmarked Arabic hotel reviews dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdelgwad_M/0/1/0/all/0/1"&gt;Mohammed M.Abdelgwad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Emotion-Aware Agents For Negotiation Dialogues. (arXiv:2107.13165v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2107.13165</id>
        <link href="http://arxiv.org/abs/2107.13165"/>
        <updated>2021-07-29T02:00:07.574Z</updated>
        <summary type="html"><![CDATA[Negotiation is a complex social interaction that encapsulates emotional
encounters in human decision-making. Virtual agents that can negotiate with
humans are useful in pedagogy and conversational AI. To advance the development
of such agents, we explore the prediction of two important subjective goals in
a negotiation - outcome satisfaction and partner perception. Specifically, we
analyze the extent to which emotion attributes extracted from the negotiation
help in the prediction, above and beyond the individual difference variables.
We focus on a recent dataset in chat-based negotiations, grounded in a
realistic camping scenario. We study three degrees of emotion dimensions -
emoticons, lexical, and contextual by leveraging affective lexicons and a
state-of-the-art deep learning architecture. Our insights will be helpful in
designing adaptive negotiation agents that interact through realistic
communication interfaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chawla_K/0/1/0/all/0/1"&gt;Kushal Chawla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clever_R/0/1/0/all/0/1"&gt;Rene Clever&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramirez_J/0/1/0/all/0/1"&gt;Jaysa Ramirez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lucas_G/0/1/0/all/0/1"&gt;Gale Lucas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gratch_J/0/1/0/all/0/1"&gt;Jonathan Gratch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings. (arXiv:2106.03143v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03143</id>
        <link href="http://arxiv.org/abs/2106.03143"/>
        <updated>2021-07-29T02:00:07.459Z</updated>
        <summary type="html"><![CDATA[Without positional information, attention-based transformer neural networks
are permutation-invariant. Absolute or relative positional embeddings are the
most popular ways to feed transformer models positional information. Absolute
positional embeddings are simple to implement, but suffer from generalization
issues when evaluating on sequences of different length than those seen at
training time. Relative positions are more robust to length change, but are
more complex to implement and yield inferior model throughput. In this paper,
we propose an augmentation-based approach (CAPE) for absolute positional
embeddings, which keeps the advantages of both absolute (simplicity and speed)
and relative position embeddings (better generalization). In addition, our
empirical evaluation on state-of-the-art models in machine translation, image
and speech recognition demonstrates that CAPE leads to better generalization
performance as well as increased stability with respect to training
hyper-parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1"&gt;Tatiana Likhomanenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1"&gt;Qiantong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1"&gt;Ronan Collobert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1"&gt;Gabriel Synnaeve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rogozhnikov_A/0/1/0/all/0/1"&gt;Alex Rogozhnikov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Robustness Against Natural Language Word Substitutions. (arXiv:2107.13541v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.13541</id>
        <link href="http://arxiv.org/abs/2107.13541"/>
        <updated>2021-07-29T02:00:07.429Z</updated>
        <summary type="html"><![CDATA[Robustness against word substitutions has a well-defined and widely
acceptable form, i.e., using semantically similar words as substitutions, and
thus it is considered as a fundamental stepping-stone towards broader
robustness in natural language processing. Previous defense methods capture
word substitutions in vector space by using either $l_2$-ball or
hyper-rectangle, which results in perturbation sets that are not inclusive
enough or unnecessarily large, and thus impedes mimicry of worst cases for
robust training. In this paper, we introduce a novel \textit{Adversarial Sparse
Convex Combination} (ASCC) method. We model the word substitution attack space
as a convex hull and leverages a regularization term to enforce perturbation
towards an actual substitution, thus aligning our modeling better with the
discrete textual space. Based on the ASCC method, we further propose
ASCC-defense, which leverages ASCC to generate worst-case perturbations and
incorporates adversarial training towards robustness. Experiments show that
ASCC-defense outperforms the current state-of-the-arts in terms of robustness
on two prevailing NLP tasks, \emph{i.e.}, sentiment analysis and natural
language inference, concerning several attacks across multiple model
architectures. Besides, we also envision a new class of defense towards
robustness in NLP, where our robustly trained word vectors can be plugged into
a normally trained model and enforce its robustness without applying any other
defense techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xinshuai Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1"&gt;Anh Tuan Luu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text-guided Legal Knowledge Graph Reasoning. (arXiv:2104.02284v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02284</id>
        <link href="http://arxiv.org/abs/2104.02284"/>
        <updated>2021-07-29T02:00:07.417Z</updated>
        <summary type="html"><![CDATA[Recent years have witnessed the prosperity of legal artificial intelligence
with the development of technologies. In this paper, we propose a novel legal
application of legal provision prediction (LPP), which aims to predict the
related legal provisions of affairs. We formulate this task as a challenging
knowledge graph completion problem, which requires not only text understanding
but also graph reasoning. To this end, we propose a novel text-guided graph
reasoning approach. We collect amounts of real-world legal provision data from
the Guangdong government service website and construct a legal dataset called
LegalLPP. Extensive experimental results on the dataset show that our approach
achieves better performance compared with baselines. The code and dataset are
available in \url{https://github.com/zjunlp/LegalPP} for reproducibility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Luoqiu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1"&gt;Zhen Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Hongbin Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1"&gt;Shumin Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tou_H/0/1/0/all/0/1"&gt;Huaixiao Tou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual-wav2vec2: an Application of Continual Learning for Self-Supervised Automatic Speech Recognition. (arXiv:2107.13530v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.13530</id>
        <link href="http://arxiv.org/abs/2107.13530"/>
        <updated>2021-07-29T02:00:07.406Z</updated>
        <summary type="html"><![CDATA[We present a method for continual learning of speech representations for
multiple languages using self-supervised learning (SSL) and applying these for
automatic speech recognition. There is an abundance of unannotated speech, so
creating self-supervised representations from raw audio and finetuning on a
small annotated datasets is a promising direction to build speech recognition
systems. Wav2vec models perform SSL on raw audio in a pretraining phase and
then finetune on a small fraction of annotated data. SSL models have produced
state of the art results for ASR. However, these models are very expensive to
pretrain with self-supervision. We tackle the problem of learning new language
representations continually from audio without forgetting a previous language
representation. We use ideas from continual learning to transfer knowledge from
a previous task to speed up pretraining a new language task. Our
continual-wav2vec2 model can decrease pretraining times by 32% when learning a
new language task, and learn this new audio-language representation without
forgetting previous language representation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kessler_S/0/1/0/all/0/1"&gt;Samuel Kessler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thomas_B/0/1/0/all/0/1"&gt;Bethan Thomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Karout_S/0/1/0/all/0/1"&gt;Salah Karout&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Rule-Execution Tracking Machine For Transformer-Based Text Generation. (arXiv:2107.13077v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.13077</id>
        <link href="http://arxiv.org/abs/2107.13077"/>
        <updated>2021-07-29T02:00:07.398Z</updated>
        <summary type="html"><![CDATA[Sequence-to-Sequence (S2S) neural text generation models, especially the
pre-trained ones (e.g., BART and T5), have exhibited compelling performance on
various natural language generation tasks. However, the black-box nature of
these models limits their application in tasks where specific rules (e.g.,
controllable constraints, prior knowledge) need to be executed. Previous works
either design specific model structure (e.g., Copy Mechanism corresponding to
the rule "the generated output should include certain words in the source
input") or implement specialized inference algorithm (e.g., Constrained Beam
Search) to execute particular rules through the text generation. These methods
require careful design case-by-case and are difficult to support multiple rules
concurrently. In this paper, we propose a novel module named Neural
Rule-Execution Tracking Machine that can be equipped into various
transformer-based generators to leverage multiple rules simultaneously to guide
the neural generation model for superior generation performance in a unified
and scalable way. Extensive experimental results on several benchmarks verify
the effectiveness of our proposed model in both controllable and general text
generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yufei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Can Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Huang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1"&gt;Chongyang Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_S/0/1/0/all/0/1"&gt;Stephen Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dras_M/0/1/0/all/0/1"&gt;Mark Dras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1"&gt;Mark Johnson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1"&gt;Daxin Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Growing knowledge culturally across generations to solve novel, complex tasks. (arXiv:2107.13377v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.13377</id>
        <link href="http://arxiv.org/abs/2107.13377"/>
        <updated>2021-07-29T02:00:07.388Z</updated>
        <summary type="html"><![CDATA[Knowledge built culturally across generations allows humans to learn far more
than an individual could glean from their own experience in a lifetime.
Cultural knowledge in turn rests on language: language is the richest record of
what previous generations believed, valued, and practiced. The power and
mechanisms of language as a means of cultural learning, however, are not well
understood. We take a first step towards reverse-engineering cultural learning
through language. We developed a suite of complex high-stakes tasks in the form
of minimalist-style video games, which we deployed in an iterated learning
paradigm. Game participants were limited to only two attempts (two lives) to
beat each game and were allowed to write a message to a future participant who
read the message before playing. Knowledge accumulated gradually across
generations, allowing later generations to advance further in the games and
perform more efficient actions. Multigenerational learning followed a
strikingly similar trajectory to individuals learning alone with an unlimited
number of lives. These results suggest that language provides a sufficient
medium to express and accumulate the knowledge people acquire in these diverse
tasks: the dynamics of the environment, valuable goals, dangerous risks, and
strategies for success. The video game paradigm we pioneer here is thus a rich
test bed for theories of cultural transmission and learning from language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tessler_M/0/1/0/all/0/1"&gt;Michael Henry Tessler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsividis_P/0/1/0/all/0/1"&gt;Pedro A. Tsividis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madeano_J/0/1/0/all/0/1"&gt;Jason Madeano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harper_B/0/1/0/all/0/1"&gt;Brin Harper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Joshua B. Tenenbaum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pretrained Transformers for Text Ranking: BERT and Beyond. (arXiv:2010.06467v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.06467</id>
        <link href="http://arxiv.org/abs/2010.06467"/>
        <updated>2021-07-29T02:00:07.368Z</updated>
        <summary type="html"><![CDATA[The goal of text ranking is to generate an ordered list of texts retrieved
from a corpus in response to a query. Although the most common formulation of
text ranking is search, instances of the task can also be found in many natural
language processing applications. This survey provides an overview of text
ranking with neural network architectures known as transformers, of which BERT
is the best-known example. The combination of transformers and self-supervised
pretraining has been responsible for a paradigm shift in natural language
processing (NLP), information retrieval (IR), and beyond. In this survey, we
provide a synthesis of existing work as a single point of entry for
practitioners who wish to gain a better understanding of how to apply
transformers to text ranking problems and researchers who wish to pursue work
in this area. We cover a wide range of modern techniques, grouped into two
high-level categories: transformer models that perform reranking in multi-stage
architectures and dense retrieval techniques that perform ranking directly.
There are two themes that pervade our survey: techniques for handling long
documents, beyond typical sentence-by-sentence processing in NLP, and
techniques for addressing the tradeoff between effectiveness (i.e., result
quality) and efficiency (e.g., query latency, model and index size). Although
transformer architectures and pretraining techniques are recent innovations,
many aspects of how they are applied to text ranking are relatively well
understood and represent mature techniques. However, there remain many open
research questions, and thus in addition to laying out the foundations of
pretrained transformers for text ranking, this survey also attempts to
prognosticate where the field is heading.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jimmy Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1"&gt;Rodrigo Nogueira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yates_A/0/1/0/all/0/1"&gt;Andrew Yates&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transforming Multi-Conditioned Generation from Meaning Representation. (arXiv:2101.04257v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.04257</id>
        <link href="http://arxiv.org/abs/2101.04257"/>
        <updated>2021-07-29T02:00:07.354Z</updated>
        <summary type="html"><![CDATA[In task-oriented conversation systems, natural language generation systems
that generate sentences with specific information related to conversation flow
are useful. Our study focuses on language generation by considering various
information representing the meaning of utterances as multiple conditions of
generation. NLG from meaning representations, the conditions for sentence
meaning, generally goes through two steps: sentence planning and surface
realization. However, we propose a simple one-stage framework to generate
utterances directly from MR (Meaning Representation). Our model is based on
GPT2 and generates utterances with flat conditions on slot and value pairs,
which does not need to determine the structure of the sentence. We evaluate
several systems in the E2E dataset with 6 automatic metrics. Our system is a
simple method, but it demonstrates comparable performance to previous systems
in automated metrics. In addition, using only 10\% of the data set without any
other techniques, our model achieves comparable performance, and shows the
possibility of performing zero-shot generation and expanding to other datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Joosung Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Goal-Oriented Script Construction. (arXiv:2107.13189v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.13189</id>
        <link href="http://arxiv.org/abs/2107.13189"/>
        <updated>2021-07-29T02:00:07.330Z</updated>
        <summary type="html"><![CDATA[The knowledge of scripts, common chains of events in stereotypical scenarios,
is a valuable asset for task-oriented natural language understanding systems.
We propose the Goal-Oriented Script Construction task, where a model produces a
sequence of steps to accomplish a given goal. We pilot our task on the first
multilingual script learning dataset supporting 18 languages collected from
wikiHow, a website containing half a million how-to articles. For baselines, we
consider both a generation-based approach using a language model and a
retrieval-based approach by first retrieving the relevant steps from a large
candidate pool and then ordering them. We show that our task is practical,
feasible but challenging for state-of-the-art Transformer models, and that our
methods can be readily deployed for various other datasets and domains with
decent zero-shot performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1"&gt;Qing Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Li Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1"&gt;Chris Callison-Burch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Scale Feature and Metric Learning for Relation Extraction. (arXiv:2107.13425v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.13425</id>
        <link href="http://arxiv.org/abs/2107.13425"/>
        <updated>2021-07-29T02:00:07.322Z</updated>
        <summary type="html"><![CDATA[Existing methods in relation extraction have leveraged the lexical features
in the word sequence and the syntactic features in the parse tree. Though
effective, the lexical features extracted from the successive word sequence may
introduce some noise that has little or no meaningful content. Meanwhile, the
syntactic features are usually encoded via graph convolutional networks which
have restricted receptive field. To address the above limitations, we propose a
multi-scale feature and metric learning framework for relation extraction.
Specifically, we first develop a multi-scale convolutional neural network to
aggregate the non-successive mainstays in the lexical sequence. We also design
a multi-scale graph convolutional network which can increase the receptive
field towards specific syntactic roles. Moreover, we present a multi-scale
metric learning paradigm to exploit both the feature-level relation between
lexical and syntactic features and the sample-level relation between instances
with the same or different classes. We conduct extensive experiments on three
real world datasets for various types of relation extraction tasks. The results
demonstrate that our model significantly outperforms the state-of-the-art
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_T/0/1/0/all/0/1"&gt;Tieyun Qian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Red Dragon AI at TextGraphs 2021 Shared Task: Multi-Hop Inference Explanation Regeneration by Matching Expert Ratings. (arXiv:2107.13031v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.13031</id>
        <link href="http://arxiv.org/abs/2107.13031"/>
        <updated>2021-07-29T02:00:07.305Z</updated>
        <summary type="html"><![CDATA[Creating explanations for answers to science questions is a challenging task
that requires multi-hop inference over a large set of fact sentences. This
year, to refocus the Textgraphs Shared Task on the problem of gathering
relevant statements (rather than solely finding a single 'correct path'), the
WorldTree dataset was augmented with expert ratings of 'relevance' of
statements to each overall explanation. Our system, which achieved second place
on the Shared Task leaderboard, combines initial statement retrieval; language
models trained to predict the relevance scores; and ensembling of a number of
the resulting rankings. Our code implementation is made available at
https://github.com/mdda/worldtree_corpus/tree/textgraphs_2021]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kalyan_V/0/1/0/all/0/1"&gt;Vivek Kalyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Witteveen_S/0/1/0/all/0/1"&gt;Sam Witteveen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andrews_M/0/1/0/all/0/1"&gt;Martin Andrews&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CiwGAN and fiwGAN: Encoding information in acoustic data to model lexical learning with Generative Adversarial Networks. (arXiv:2006.02951v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.02951</id>
        <link href="http://arxiv.org/abs/2006.02951"/>
        <updated>2021-07-29T02:00:07.279Z</updated>
        <summary type="html"><![CDATA[How can deep neural networks encode information that corresponds to words in
human speech into raw acoustic data? This paper proposes two neural network
architectures for modeling unsupervised lexical learning from raw acoustic
inputs, ciwGAN (Categorical InfoWaveGAN) and fiwGAN (Featural InfoWaveGAN),
that combine a Deep Convolutional GAN architecture for audio data (WaveGAN;
arXiv:1705.07904) with an information theoretic extension of GAN -- InfoGAN
(arXiv:1606.03657), and propose a new latent space structure that can model
featural learning simultaneously with a higher level classification and allows
for a very low-dimension vector representation of lexical items. Lexical
learning is modeled as emergent from an architecture that forces a deep neural
network to output data such that unique information is retrievable from its
acoustic outputs. The networks trained on lexical items from TIMIT learn to
encode unique information corresponding to lexical items in the form of
categorical variables in their latent space. By manipulating these variables,
the network outputs specific lexical items. The network occasionally outputs
innovative lexical items that violate training data, but are linguistically
interpretable and highly informative for cognitive modeling and neural network
interpretability. Innovative outputs suggest that phonetic and phonological
representations learned by the network can be productively recombined and
directly paralleled to productivity in human speech: a fiwGAN network trained
on `suit' and `dark' outputs innovative `start', even though it never saw
`start' or even a [st] sequence in the training data. We also argue that
setting latent featural codes to values well beyond training range results in
almost categorical generation of prototypical lexical items and reveals
underlying values of each latent code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1"&gt;Ga&amp;#x161;per Begu&amp;#x161;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Perla: A Conversational Agent for Depression Screening in Digital Ecosystems. Design, Implementation and Validation. (arXiv:2008.12875v2 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.12875</id>
        <link href="http://arxiv.org/abs/2008.12875"/>
        <updated>2021-07-29T02:00:07.256Z</updated>
        <summary type="html"><![CDATA[Most depression assessment tools are based on self-report questionnaires,
such as the Patient Health Questionnaire (PHQ-9). These psychometric
instruments can be easily adapted to an online setting by means of electronic
forms. However, this approach lacks the interacting and engaging features of
modern digital environments. With the aim of making depression screening more
available, attractive and effective, we developed Perla, a conversational agent
able to perform an interview based on the PHQ-9. We also conducted a validation
study in which we compared the results obtained by the traditional self-report
questionnaire with Perla's automated interview. Analyzing the results from this
study we draw two significant conclusions: firstly, Perla is much preferred by
Internet users, achieving more than 2.5 times more reach than a traditional
form-based questionnaire; secondly, her psychometric properties (Cronbach's
alpha of 0.81, sensitivity of 96% and specificity of 90%) are excellent and
comparable to the traditional well-established depression screening
questionnaires.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arrabales_R/0/1/0/all/0/1"&gt;Ra&amp;#xfa;l Arrabales&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Long-Short Transformer: Efficient Transformers for Language and Vision. (arXiv:2107.02192v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02192</id>
        <link href="http://arxiv.org/abs/2107.02192"/>
        <updated>2021-07-29T02:00:07.070Z</updated>
        <summary type="html"><![CDATA[Transformers have achieved success in both language and vision domains.
However, it is prohibitively expensive to scale them to long sequences such as
long documents or high-resolution images, because self-attention mechanism has
quadratic time and memory complexities with respect to the input sequence
length. In this paper, we propose Long-Short Transformer (Transformer-LS), an
efficient self-attention mechanism for modeling long sequences with linear
complexity for both language and vision tasks. It aggregates a novel long-range
attention with dynamic projection to model distant correlations and a
short-term attention to capture fine-grained local correlations. We propose a
dual normalization strategy to account for the scale mismatch between the two
attention mechanisms. Transformer-LS can be applied to both autoregressive and
bidirectional models without additional complexity. Our method outperforms the
state-of-the-art models on multiple tasks in language and vision domains,
including the Long Range Arena benchmark, autoregressive language modeling, and
ImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on
enwik8 using half the number of parameters than previous method, while being
faster and is able to handle 3x as long sequences compared to its
full-attention version on the same hardware. On ImageNet, it can obtain the
state-of-the-art results (e.g., a moderate size of 55.8M model solely trained
on 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more
scalable on high-resolution images. The source code and models are released at
https://github.com/NVIDIA/transformer-ls .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1"&gt;Chen Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1"&gt;Wei Ping&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Chaowei Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1"&gt;Mohammad Shoeybi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1"&gt;Tom Goldstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1"&gt;Bryan Catanzaro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Squeeze-Excitation Convolutional Recurrent Neural Networks for Audio-Visual Scene Classification. (arXiv:2107.13180v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.13180</id>
        <link href="http://arxiv.org/abs/2107.13180"/>
        <updated>2021-07-29T02:00:06.906Z</updated>
        <summary type="html"><![CDATA[The use of multiple and semantically correlated sources can provide
complementary information to each other that may not be evident when working
with individual modalities on their own. In this context, multi-modal models
can help producing more accurate and robust predictions in machine learning
tasks where audio-visual data is available. This paper presents a multi-modal
model for automatic scene classification that exploits simultaneously auditory
and visual information. The proposed approach makes use of two separate
networks which are respectively trained in isolation on audio and visual data,
so that each network specializes in a given modality. The visual subnetwork is
a pre-trained VGG16 model followed by a bidiretional recurrent layer, while the
residual audio subnetwork is based on stacked squeeze-excitation convolutional
blocks trained from scratch. After training each subnetwork, the fusion of
information from the audio and visual streams is performed at two different
stages. The early fusion stage combines features resulting from the last
convolutional block of the respective subnetworks at different time steps to
feed a bidirectional recurrent structure. The late fusion stage combines the
output of the early fusion stage with the independent predictions provided by
the two subnetworks, resulting in the final prediction. We evaluate the method
using the recently published TAU Audio-Visual Urban Scenes 2021, which contains
synchronized audio and video recordings from 12 European cities in 10 different
scene classes. The proposed model has been shown to provide an excellent
trade-off between prediction performance (86.5%) and system complexity (15M
parameters) in the evaluation results of the DCASE 2021 Challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naranjo_Alcazar_J/0/1/0/all/0/1"&gt;Javier Naranjo-Alcazar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Castanos_S/0/1/0/all/0/1"&gt;Sergi Perez-Castanos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lopez_Garcia_A/0/1/0/all/0/1"&gt;Aaron Lopez-Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuccarello_P/0/1/0/all/0/1"&gt;Pedro Zuccarello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cobos_M/0/1/0/all/0/1"&gt;Maximo Cobos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferri_F/0/1/0/all/0/1"&gt;Francesc J. Ferri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Unsupervised Domain Adaptation with Conditional and Label Shift: Infer, Align and Iterate. (arXiv:2107.13469v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.13469</id>
        <link href="http://arxiv.org/abs/2107.13469"/>
        <updated>2021-07-29T02:00:06.786Z</updated>
        <summary type="html"><![CDATA[In this work, we propose an adversarial unsupervised domain adaptation (UDA)
approach with the inherent conditional and label shifts, in which we aim to
align the distributions w.r.t. both $p(x|y)$ and $p(y)$. Since the label is
inaccessible in the target domain, the conventional adversarial UDA assumes
$p(y)$ is invariant across domains, and relies on aligning $p(x)$ as an
alternative to the $p(x|y)$ alignment. To address this, we provide a thorough
theoretical and empirical analysis of the conventional adversarial UDA methods
under both conditional and label shifts, and propose a novel and practical
alternative optimization scheme for adversarial UDA. Specifically, we infer the
marginal $p(y)$ and align $p(x|y)$ iteratively in the training, and precisely
align the posterior $p(y|x)$ in testing. Our experimental results demonstrate
its effectiveness on both classification and segmentation UDA, and partial UDA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1"&gt;Zhenhua Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Site Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1"&gt;Fangxu Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1"&gt;Jane You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1"&gt;C.-C. Jay Kuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1"&gt;Georges El Fakhri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1"&gt;Jonghye Woo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Complete End-To-End Open Source Toolchain for the Versatile Video Coding (VVC) Standard. (arXiv:2107.13385v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.13385</id>
        <link href="http://arxiv.org/abs/2107.13385"/>
        <updated>2021-07-29T02:00:06.776Z</updated>
        <summary type="html"><![CDATA[Versatile Video Coding (VVC) is the most recent international video coding
standard jointly developed by ITU-T and ISO/IEC, which has been finalized in
July 2020. VVC allows for significant bit-rate reductions around 50% for the
same subjective video quality compared to its predecessor, High Efficiency
Video Coding (HEVC). One year after finalization, VVC support in devices and
chipsets is still under development, which is aligned with the typical
development cycles of new video coding standards. This paper presents
open-source software packages that allow building a complete VVC end-to-end
toolchain already one year after its finalization. This includes the Fraunhofer
HHI VVenC library for fast and efficient VVC encoding as well as HHI's VVdeC
library for live decoding. An experimental integration of VVC in the GPAC
software tools and FFmpeg media framework allows packaging VVC bitstreams, e.g.
encoded with VVenC, in MP4 file format and using DASH for content creation and
streaming. The integration of VVdeC allows playback on the receiver. Given
these packages, step-by-step tutorials are provided for two possible
application scenarios: VVC file encoding plus playback and adaptive streaming
with DASH.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wieckowski_A/0/1/0/all/0/1"&gt;Adam Wieckowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lehmann_C/0/1/0/all/0/1"&gt;Christian Lehmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bross_B/0/1/0/all/0/1"&gt;Benjamin Bross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Marpe_D/0/1/0/all/0/1"&gt;Detlev Marpe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Biatek_T/0/1/0/all/0/1"&gt;Thibaud Biatek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Raulet_M/0/1/0/all/0/1"&gt;Mickael Raulet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Feuvre_J/0/1/0/all/0/1"&gt;Jean Le Feuvre&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ranker-agnostic Contextual Position Bias Estimation. (arXiv:2107.13327v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.13327</id>
        <link href="http://arxiv.org/abs/2107.13327"/>
        <updated>2021-07-29T02:00:06.739Z</updated>
        <summary type="html"><![CDATA[Learning-to-rank (LTR) algorithms are ubiquitous and necessary to explore the
extensive catalogs of media providers. To avoid the user examining all the
results, its preferences are used to provide a subset of relatively small size.
The user preferences can be inferred from the interactions with the presented
content if explicit ratings are unavailable. However, directly using implicit
feedback can lead to learning wrong relevance models and is known as biased
LTR. The mismatch between implicit feedback and true relevances is due to
various nuisances, with position bias one of the most relevant. Position bias
models consider that the lack of interaction with a presented item is not only
attributed to the item being irrelevant but because the item was not examined.
This paper introduces a method for modeling the probability of an item being
seen in different contexts, e.g., for different users, with a single estimator.
Our suggested method, denoted as contextual (EM)-based regression, is
ranker-agnostic and able to correctly learn the latent examination
probabilities while only using implicit feedback. Our empirical results
indicate that the method introduced in this paper outperforms other existing
position bias estimators in terms of relative error when the examination
probability varies across queries. Moreover, the estimated values provide a
ranking performance boost when used to debias the implicit ranking data even if
there is no context dependency on the examination probabilities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mayor_O/0/1/0/all/0/1"&gt;Oriol Barbany Mayor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bellini_V/0/1/0/all/0/1"&gt;Vito Bellini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buchholz_A/0/1/0/all/0/1"&gt;Alexander Buchholz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benedetto_G/0/1/0/all/0/1"&gt;Giuseppe Di Benedetto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Granziol_D/0/1/0/all/0/1"&gt;Diego Marco Granziol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruffini_M/0/1/0/all/0/1"&gt;Matteo Ruffini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stein_Y/0/1/0/all/0/1"&gt;Yannik Stein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[JPEG Steganography with Embedding Cost Learning and Side-Information Estimation. (arXiv:2107.13151v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.13151</id>
        <link href="http://arxiv.org/abs/2107.13151"/>
        <updated>2021-07-29T02:00:06.723Z</updated>
        <summary type="html"><![CDATA[A great challenge to steganography has arisen with the wide application of
steganalysis methods based on convolutional neural networks (CNNs). To this
end, embedding cost learning frameworks based on generative adversarial
networks (GANs) have been proposed and achieved success for spatial
steganography. However, the application of GAN to JPEG steganography is still
in the prototype stage; its anti-detectability and training efficiency should
be improved. In conventional steganography, research has shown that the
side-information calculated from the precover can be used to enhance security.
However, it is hard to calculate the side-information without the spatial
domain image. In this work, an embedding cost learning framework for JPEG
Steganography via a Generative Adversarial Network (JS-GAN) has been proposed,
the learned embedding cost can be further adjusted asymmetrically according to
the estimated side-information. Experimental results have demonstrated that the
proposed method can automatically learn a content-adaptive embedding cost
function, and use the estimated side-information properly can effectively
improve the security performance. For example, under the attack of a classic
steganalyzer GFR with quality factor 75 and 0.4 bpnzAC, the proposed JS-GAN can
increase the detection error 2.58% over J-UNIWARD, and the estimated
side-information aided version JS-GAN(ESI) can further increase the security
performance by 11.25% over JS-GAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jianhua Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1"&gt;Yi Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1"&gt;Fei Shang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1"&gt;Xiangui Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yun-Qing Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Payload Optimization Method for Federated Recommender Systems. (arXiv:2107.13078v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.13078</id>
        <link href="http://arxiv.org/abs/2107.13078"/>
        <updated>2021-07-29T02:00:06.672Z</updated>
        <summary type="html"><![CDATA[We introduce the payload optimization method for federated recommender
systems (FRS). In federated learning (FL), the global model payload that is
moved between the server and users depends on the number of items to recommend.
The model payload grows when there is an increasing number of items. This
becomes challenging for an FRS if it is running in production mode. To tackle
the payload challenge, we formulated a multi-arm bandit solution that selected
part of the global model and transmitted it to all users. The selection process
was guided by a novel reward function suitable for FL systems. So far as we are
aware, this is the first optimization method that seeks to address item
dependent payloads. The method was evaluated using three benchmark
recommendation datasets. The empirical validation confirmed that the proposed
method outperforms the simpler methods that do not benefit from the bandits for
the purpose of item selection. In addition, we have demonstrated the usefulness
of our proposed method by rigorously evaluating the effects of a payload
reduction on the recommendation performance degradation. Our method achieved up
to a 90\% reduction in model payload, yielding only a $\sim$4\% - 8\% loss in
the recommendation performance for highly sparse datasets]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1"&gt;Farwa K. Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flanagan_A/0/1/0/all/0/1"&gt;Adrian Flanagan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1"&gt;Kuan E. Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alamgir_Z/0/1/0/all/0/1"&gt;Zareen Alamgir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ammad_Ud_Din_M/0/1/0/all/0/1"&gt;Muhammad Ammad-Ud-Din&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pretrained Transformers for Text Ranking: BERT and Beyond. (arXiv:2010.06467v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.06467</id>
        <link href="http://arxiv.org/abs/2010.06467"/>
        <updated>2021-07-29T02:00:06.660Z</updated>
        <summary type="html"><![CDATA[The goal of text ranking is to generate an ordered list of texts retrieved
from a corpus in response to a query. Although the most common formulation of
text ranking is search, instances of the task can also be found in many natural
language processing applications. This survey provides an overview of text
ranking with neural network architectures known as transformers, of which BERT
is the best-known example. The combination of transformers and self-supervised
pretraining has been responsible for a paradigm shift in natural language
processing (NLP), information retrieval (IR), and beyond. In this survey, we
provide a synthesis of existing work as a single point of entry for
practitioners who wish to gain a better understanding of how to apply
transformers to text ranking problems and researchers who wish to pursue work
in this area. We cover a wide range of modern techniques, grouped into two
high-level categories: transformer models that perform reranking in multi-stage
architectures and dense retrieval techniques that perform ranking directly.
There are two themes that pervade our survey: techniques for handling long
documents, beyond typical sentence-by-sentence processing in NLP, and
techniques for addressing the tradeoff between effectiveness (i.e., result
quality) and efficiency (e.g., query latency, model and index size). Although
transformer architectures and pretraining techniques are recent innovations,
many aspects of how they are applied to text ranking are relatively well
understood and represent mature techniques. However, there remain many open
research questions, and thus in addition to laying out the foundations of
pretrained transformers for text ranking, this survey also attempts to
prognosticate where the field is heading.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jimmy Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1"&gt;Rodrigo Nogueira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yates_A/0/1/0/all/0/1"&gt;Andrew Yates&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reenvisioning Collaborative Filtering vs Matrix Factorization. (arXiv:2107.13472v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.13472</id>
        <link href="http://arxiv.org/abs/2107.13472"/>
        <updated>2021-07-29T02:00:06.641Z</updated>
        <summary type="html"><![CDATA[Collaborative filtering models based on matrix factorization and learned
similarities using Artificial Neural Networks (ANNs) have gained significant
attention in recent years. This is, in part, because ANNs have demonstrated
good results in a wide variety of recommendation tasks. The introduction of
ANNs within the recommendation ecosystem has been recently questioned, raising
several comparisons in terms of efficiency and effectiveness. One aspect most
of these comparisons have in common is their focus on accuracy, neglecting
other evaluation dimensions important for the recommendation, such as novelty,
diversity, or accounting for biases. We replicate experiments from three papers
that compare Neural Collaborative Filtering (NCF) and Matrix Factorization
(MF), to extend the analysis to other evaluation dimensions. Our contribution
shows that the experiments are entirely reproducible, and we extend the study
including other accuracy metrics and two statistical hypothesis tests. We
investigated the Diversity and Novelty of the recommendations, showing that MF
provides a better accuracy also on the long tail, although NCF provides a
better item coverage and more diversified recommendations. We discuss the bias
effect generated by the tested methods. They show a relatively small bias, but
other recommendation baselines, with competitive accuracy performance,
consistently show to be less affected by this issue. This is the first work, to
the best of our knowledge, where several evaluation dimensions have been
explored for an array of SOTA algorithms covering recent adaptations of ANNs
and MF. Hence, we show the potential these techniques may have on
beyond-accuracy evaluation while analyzing the effect on reproducibility these
complementary dimensions may spark. Available at
github.com/sisinflab/Reenvisioning-the-comparison-between-Neural-Collaborative-Filtering-and-Matrix-Factorization]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anelli_V/0/1/0/all/0/1"&gt;Vito Walter Anelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bellogin_A/0/1/0/all/0/1"&gt;Alejandro Bellog&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noia_T/0/1/0/all/0/1"&gt;Tommaso Di Noia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pomo_C/0/1/0/all/0/1"&gt;Claudio Pomo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding and Generalizing Monotonic Proximity Graphs for Approximate Nearest Neighbor Search. (arXiv:2107.13052v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.13052</id>
        <link href="http://arxiv.org/abs/2107.13052"/>
        <updated>2021-07-29T02:00:06.609Z</updated>
        <summary type="html"><![CDATA[Graph-based algorithms have shown great empirical potential for the
approximate nearest neighbor (ANN) search problem. Currently, graph-based ANN
search algorithms are designed mainly using heuristics, whereas theoretical
analysis of such algorithms is quite lacking. In this paper, we study a
fundamental model of proximity graphs used in graph-based ANN search, called
Monotonic Relative Neighborhood Graph (MRNG), from a theoretical perspective.
We use mathematical proofs to explain why proximity graphs that are built based
on MRNG tend to have good searching performance. We also run experiments on
MRNG and graphs generalizing MRNG to obtain a deeper understanding of the
model. Our experiments give guidance on how to approximate and generalize MRNG
to build proximity graphs on a large scale. In addition, we discover and study
a hidden structure of MRNG called conflicting nodes, and we give theoretical
evidence how conflicting nodes could be used to improve ANN search methods that
are based on MRNG.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1"&gt;Dantong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Minjia Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Red Dragon AI at TextGraphs 2021 Shared Task: Multi-Hop Inference Explanation Regeneration by Matching Expert Ratings. (arXiv:2107.13031v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.13031</id>
        <link href="http://arxiv.org/abs/2107.13031"/>
        <updated>2021-07-29T02:00:06.591Z</updated>
        <summary type="html"><![CDATA[Creating explanations for answers to science questions is a challenging task
that requires multi-hop inference over a large set of fact sentences. This
year, to refocus the Textgraphs Shared Task on the problem of gathering
relevant statements (rather than solely finding a single 'correct path'), the
WorldTree dataset was augmented with expert ratings of 'relevance' of
statements to each overall explanation. Our system, which achieved second place
on the Shared Task leaderboard, combines initial statement retrieval; language
models trained to predict the relevance scores; and ensembling of a number of
the resulting rankings. Our code implementation is made available at
https://github.com/mdda/worldtree_corpus/tree/textgraphs_2021]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kalyan_V/0/1/0/all/0/1"&gt;Vivek Kalyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Witteveen_S/0/1/0/all/0/1"&gt;Sam Witteveen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andrews_M/0/1/0/all/0/1"&gt;Martin Andrews&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Case Study on Sampling Strategies for Evaluating Neural Sequential Item Recommendation Models. (arXiv:2107.13045v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.13045</id>
        <link href="http://arxiv.org/abs/2107.13045"/>
        <updated>2021-07-29T02:00:06.544Z</updated>
        <summary type="html"><![CDATA[At the present time, sequential item recommendation models are compared by
calculating metrics on a small item subset (target set) to speed up
computation. The target set contains the relevant item and a set of negative
items that are sampled from the full item set. Two well-known strategies to
sample negative items are uniform random sampling and sampling by popularity to
better approximate the item frequency distribution in the dataset. Most
recently published papers on sequential item recommendation rely on sampling by
popularity to compare the evaluated models. However, recent work has already
shown that an evaluation with uniform random sampling may not be consistent
with the full ranking, that is, the model ranking obtained by evaluating a
metric using the full item set as target set, which raises the question whether
the ranking obtained by sampling by popularity is equal to the full ranking. In
this work, we re-evaluate current state-of-the-art sequential recommender
models from the point of view, whether these sampling strategies have an impact
on the final ranking of the models. We therefore train four recently proposed
sequential recommendation models on five widely known datasets. For each
dataset and model, we employ three evaluation strategies. First, we compute the
full model ranking. Then we evaluate all models on a target set sampled by the
two different sampling strategies, uniform random sampling and sampling by
popularity with the commonly used target set size of 100, compute the model
ranking for each strategy and compare them with each other. Additionally, we
vary the size of the sampled target set. Overall, we find that both sampling
strategies can produce inconsistent rankings compared with the full ranking of
the models. Furthermore, both sampling by popularity and uniform random
sampling do not consistently produce the same ranking ...]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dallmann_A/0/1/0/all/0/1"&gt;Alexander Dallmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zoller_D/0/1/0/all/0/1"&gt;Daniel Zoller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hotho_A/0/1/0/all/0/1"&gt;Andreas Hotho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SaliencyMix: A Saliency Guided Data Augmentation Strategy for Better Regularization. (arXiv:2006.01791v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.01791</id>
        <link href="http://arxiv.org/abs/2006.01791"/>
        <updated>2021-07-28T02:02:34.744Z</updated>
        <summary type="html"><![CDATA[Advanced data augmentation strategies have widely been studied to improve the
generalization ability of deep learning models. Regional dropout is one of the
popular solutions that guides the model to focus on less discriminative parts
by randomly removing image regions, resulting in improved regularization.
However, such information removal is undesirable. On the other hand, recent
strategies suggest to randomly cut and mix patches and their labels among
training images, to enjoy the advantages of regional dropout without having any
pointless pixel in the augmented images. We argue that such random selection
strategies of the patches may not necessarily represent sufficient information
about the corresponding object and thereby mixing the labels according to that
uninformative patch enables the model to learn unexpected feature
representation. Therefore, we propose SaliencyMix that carefully selects a
representative image patch with the help of a saliency map and mixes this
indicative patch with the target image, thus leading the model to learn more
appropriate feature representation. SaliencyMix achieves the best known top-1
error of 21.26% and 20.09% for ResNet-50 and ResNet-101 architectures on
ImageNet classification, respectively, and also improves the model robustness
against adversarial perturbations. Furthermore, models that are trained with
SaliencyMix help to improve the object detection performance. Source code is
available at https://github.com/SaliencyMix/SaliencyMix.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Uddin_A/0/1/0/all/0/1"&gt;A. F. M. Shahab Uddin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Monira_M/0/1/0/all/0/1"&gt;Mst. Sirazam Monira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1"&gt;Wheemyung Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_T/0/1/0/all/0/1"&gt;TaeChoong Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1"&gt;Sung-Ho Bae&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Task-Based Information Compression for Multi-Agent Communication Problems with Channel Rate Constraints. (arXiv:2005.14220v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.14220</id>
        <link href="http://arxiv.org/abs/2005.14220"/>
        <updated>2021-07-28T02:02:34.699Z</updated>
        <summary type="html"><![CDATA[A collaborative task is assigned to a multiagent system (MAS) in which agents
are allowed to communicate. The MAS runs over an underlying Markov decision
process and its task is to maximize the averaged sum of discounted one-stage
rewards. Although knowing the global state of the environment is necessary for
the optimal action selection of the MAS, agents are limited to individual
observations. The inter-agent communication can tackle the issue of local
observability, however, the limited rate of the inter-agent communication
prevents the agent from acquiring the precise global state information. To
overcome this challenge, agents need to communicate their observations in a
compact way such that the MAS compromises the minimum possible sum of rewards.
We show that this problem is equivalent to a form of rate-distortion problem
which we call the task-based information compression. We introduce a scheme for
task-based information compression titled State aggregation for information
compression (SAIC), for which a state aggregation algorithm is analytically
designed. The SAIC is shown to be capable of achieving near-optimal performance
in terms of the achieved sum of discounted rewards. The proposed algorithm is
applied to a rendezvous problem and its performance is compared with several
benchmarks. Numerical experiments confirm the superiority of the proposed
algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mostaani_A/0/1/0/all/0/1"&gt;Arsham Mostaani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1"&gt;Thang X. Vu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatzinotas_S/0/1/0/all/0/1"&gt;Symeon Chatzinotas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ottersten_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn Ottersten&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RGB cameras failures and their effects in autonomous driving applications. (arXiv:2008.05938v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.05938</id>
        <link href="http://arxiv.org/abs/2008.05938"/>
        <updated>2021-07-28T02:02:34.652Z</updated>
        <summary type="html"><![CDATA[RGB cameras are one of the most relevant sensors for autonomous driving
applications. It is undeniable that failures of vehicle cameras may compromise
the autonomous driving task, possibly leading to unsafe behaviors when images
that are subsequently processed by the driving system are altered. To support
the definition of safe and robust vehicle architectures and intelligent
systems, in this paper we define the failure modes of a vehicle camera,
together with an analysis of effects and known mitigations. Further, we build a
software library for the generation of the corresponding failed images and we
feed them to six object detectors for mono and stereo cameras and to the
self-driving agent of an autonomous driving simulator. The resulting
misbehaviors with respect to operating with clean images allow a better
understanding of failures effects and the related safety risks in image-based
applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Secci_F/0/1/0/all/0/1"&gt;Francesco Secci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ceccarelli_A/0/1/0/all/0/1"&gt;Andrea Ceccarelli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-architecture Tuning of Silicon and SiGe-based Quantum Devices Using Machine Learning. (arXiv:2107.12975v1 [cond-mat.mes-hall])]]></title>
        <id>http://arxiv.org/abs/2107.12975</id>
        <link href="http://arxiv.org/abs/2107.12975"/>
        <updated>2021-07-28T02:02:34.625Z</updated>
        <summary type="html"><![CDATA[The potential of Si and SiGe-based devices for the scaling of quantum
circuits is tainted by device variability. Each device needs to be tuned to
operation conditions. We give a key step towards tackling this variability with
an algorithm that, without modification, is capable of tuning a 4-gate Si
FinFET, a 5-gate GeSi nanowire and a 7-gate SiGe heterostructure double quantum
dot device from scratch. We achieve tuning times of 30, 10, and 92 minutes,
respectively. The algorithm also provides insight into the parameter space
landscape for each of these devices. These results show that overarching
solutions for the tuning of quantum devices are enabled by machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Severin_B/0/1/0/all/0/1"&gt;B. Severin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Lennon_D/0/1/0/all/0/1"&gt;D. T. Lennon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Camenzind_L/0/1/0/all/0/1"&gt;L. C. Camenzind&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Vigneau_F/0/1/0/all/0/1"&gt;F. Vigneau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Fedele_F/0/1/0/all/0/1"&gt;F. Fedele&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Jirovec_D/0/1/0/all/0/1"&gt;D. Jirovec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Ballabio_A/0/1/0/all/0/1"&gt;A. Ballabio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Chrastina_D/0/1/0/all/0/1"&gt;D. Chrastina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Isella_G/0/1/0/all/0/1"&gt;G. Isella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Kruijf_M/0/1/0/all/0/1"&gt;M. de Kruijf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Carballido_M/0/1/0/all/0/1"&gt;M. J. Carballido&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Svab_S/0/1/0/all/0/1"&gt;S. Svab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Kuhlmann_A/0/1/0/all/0/1"&gt;A. V. Kuhlmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Braakman_F/0/1/0/all/0/1"&gt;F. R. Braakman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Geyer_S/0/1/0/all/0/1"&gt;S. Geyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Froning_F/0/1/0/all/0/1"&gt;F. N. M. Froning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Moon_H/0/1/0/all/0/1"&gt;H. Moon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Osborne_M/0/1/0/all/0/1"&gt;M. A. Osborne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Sejdinovic_D/0/1/0/all/0/1"&gt;D. Sejdinovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Katsaros_G/0/1/0/all/0/1"&gt;G. Katsaros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Zumbuhl_D/0/1/0/all/0/1"&gt;D. M. Zumb&amp;#xfc;hl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Briggs_G/0/1/0/all/0/1"&gt;G. A. D. Briggs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Ares_N/0/1/0/all/0/1"&gt;N. Ares&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting User Emotional Tone in Mental Disorder Online Communities. (arXiv:2005.07473v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.07473</id>
        <link href="http://arxiv.org/abs/2005.07473"/>
        <updated>2021-07-28T02:02:34.616Z</updated>
        <summary type="html"><![CDATA[In recent years, Online Social Networks have become an important medium for
people who suffer from mental disorders to share moments of hardship, and
receive emotional and informational support. In this work, we analyze how
discussions in Reddit communities related to mental disorders can help improve
the health conditions of their users. Using the emotional tone of users'
writing as a proxy for emotional state, we uncover relationships between user
interactions and state changes. First, we observe that authors of negative
posts often write rosier comments after engaging in discussions, indicating
that users' emotional state can improve due to social support. Second, we build
models based on SOTA text embedding techniques and RNNs to predict shifts in
emotional tone. This differs from most of related work, which focuses primarily
on detecting mental disorders from user activity. We demonstrate the
feasibility of accurately predicting the users' reactions to the interactions
experienced in these platforms, and present some examples which illustrate that
the models are correctly capturing the effects of comments on the author's
emotional tone. Our models hold promising implications for interventions to
provide support for people struggling with mental illnesses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Silveira_B/0/1/0/all/0/1"&gt;B&amp;#xe1;rbara Silveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_H/0/1/0/all/0/1"&gt;Henrique S. Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murai_F/0/1/0/all/0/1"&gt;Fabricio Murai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1"&gt;Ana Paula Couto da Silva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uniformity in Heterogeneity:Diving Deep into Count Interval Partition for Crowd Counting. (arXiv:2107.12619v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12619</id>
        <link href="http://arxiv.org/abs/2107.12619"/>
        <updated>2021-07-28T02:02:34.598Z</updated>
        <summary type="html"><![CDATA[Recently, the problem of inaccurate learning targets in crowd counting draws
increasing attention. Inspired by a few pioneering work, we solve this problem
by trying to predict the indices of pre-defined interval bins of counts instead
of the count values themselves. However, an inappropriate interval setting
might make the count error contributions from different intervals extremely
imbalanced, leading to inferior counting performance. Therefore, we propose a
novel count interval partition criterion called Uniform Error Partition (UEP),
which always keeps the expected counting error contributions equal for all
intervals to minimize the prediction risk. Then to mitigate the inevitably
introduced discretization errors in the count quantization process, we propose
another criterion called Mean Count Proxies (MCP). The MCP criterion selects
the best count proxy for each interval to represent its count value during
inference, making the overall expected discretization error of an image nearly
negligible. As far as we are aware, this work is the first to delve into such a
classification task and ends up with a promising solution for count interval
partition. Following the above two theoretically demonstrated criterions, we
propose a simple yet effective model termed Uniform Error Partition Network
(UEPNet), which achieves state-of-the-art performance on several challenging
datasets. The codes will be available at:
https://github.com/TencentYoutuResearch/CrowdCounting-UEPNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Changan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1"&gt;Qingyu Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Boshen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yabiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1"&gt;Ying Tai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xuyi Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengjie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jilin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jiayi Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yang Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Asynchronous Distributed Reinforcement Learning for LQR Control via Zeroth-Order Block Coordinate Descent. (arXiv:2107.12416v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2107.12416</id>
        <link href="http://arxiv.org/abs/2107.12416"/>
        <updated>2021-07-28T02:02:34.579Z</updated>
        <summary type="html"><![CDATA[Recently introduced distributed zeroth-order optimization (ZOO) algorithms
have shown their utility in distributed reinforcement learning (RL).
Unfortunately, in the gradient estimation process, almost all of them require
random samples with the same dimension as the global variable and/or require
evaluation of the global cost function, which may induce high estimation
variance for large-scale networks. In this paper, we propose a novel
distributed zeroth-order algorithm by leveraging the network structure inherent
in the optimization objective, which allows each agent to estimate its local
gradient by local cost evaluation independently, without use of any consensus
protocol. The proposed algorithm exhibits an asynchronous update scheme, and is
designed for stochastic non-convex optimization with a possibly non-convex
feasible domain based on the block coordinate descent method. The algorithm is
later employed as a distributed model-free RL algorithm for distributed linear
quadratic regulator design, where a learning graph is designed to describe the
required interaction relationship among agents in distributed learning. We
provide an empirical validation of the proposed algorithm to benchmark its
performance on convergence rate and variance against a centralized ZOO
algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jing_G/0/1/0/all/0/1"&gt;Gangshan Jing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bai_H/0/1/0/all/0/1"&gt;He Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+George_J/0/1/0/all/0/1"&gt;Jemin George&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chakrabortty_A/0/1/0/all/0/1"&gt;Aranya Chakrabortty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sharma_P/0/1/0/all/0/1"&gt;Piyush K. Sharma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Invariant Representation Learning for Treatment Effect Estimation. (arXiv:2011.12379v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12379</id>
        <link href="http://arxiv.org/abs/2011.12379"/>
        <updated>2021-07-28T02:02:34.572Z</updated>
        <summary type="html"><![CDATA[The defining challenge for causal inference from observational data is the
presence of `confounders', covariates that affect both treatment assignment and
the outcome. To address this challenge, practitioners collect and adjust for
the covariates, hoping that they adequately correct for confounding. However,
including every observed covariate in the adjustment runs the risk of including
`bad controls', variables that induce bias when they are conditioned on. The
problem is that we do not always know which variables in the covariate set are
safe to adjust for and which are not. To address this problem, we develop
Nearly Invariant Causal Estimation (NICE). NICE uses invariant risk
minimization (IRM) [Arj19] to learn a representation of the covariates that,
under some assumptions, strips out bad controls but preserves sufficient
information to adjust for confounding. Adjusting for the learned
representation, rather than the covariates themselves, avoids the induced bias
and provides valid causal inferences. We evaluate NICE on both synthetic and
semi-synthetic data. When the covariates contain unknown collider variables and
other bad controls, NICE performs better than adjusting for all the covariates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1"&gt;Claudia Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veitch_V/0/1/0/all/0/1"&gt;Victor Veitch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blei_D/0/1/0/all/0/1"&gt;David Blei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Symmetry-Aware Reservoir Computing. (arXiv:2102.00310v3 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00310</id>
        <link href="http://arxiv.org/abs/2102.00310"/>
        <updated>2021-07-28T02:02:34.553Z</updated>
        <summary type="html"><![CDATA[We demonstrate that matching the symmetry properties of a reservoir computer
(RC) to the data being processed dramatically increases its processing power.
We apply our method to the parity task, a challenging benchmark problem that
highlights inversion and permutation symmetries, and to a chaotic system
inference task that presents an inversion symmetry rule. For the parity task,
our symmetry-aware RC obtains zero error using an exponentially reduced neural
network and training data, greatly speeding up the time to result and
outperforming hand crafted artificial neural networks. When both symmetries are
respected, we find that the network size $N$ necessary to obtain zero error for
50 different RC instances scales linearly with the parity-order $n$. Moreover,
some symmetry-aware RC instances perform a zero error classification with only
$N=1$ for $n\leq7$. Furthermore, we show that a symmetry-aware RC only needs a
training data set with size on the order of $(n+n/2)$ to obtain such
performance, an exponential reduction in comparison to a regular RC which
requires a training data set with size on the order of $n2^n$ to contain all
$2^n$ possible $n-$bit-long sequences. For the inference task, we show that a
symmetry-aware RC presents a normalized root-mean-square error three
orders-of-magnitude smaller than regular RCs. For both tasks, our RC approach
respects the symmetries by adjusting only the input and the output layers, and
not by problem-based modifications to the neural network. We anticipate that
generalizations of our procedure can be applied in information processing for
problems with known symmetries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barbosa_W/0/1/0/all/0/1"&gt;Wendson A. S. Barbosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Griffith_A/0/1/0/all/0/1"&gt;Aaron Griffith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rowlands_G/0/1/0/all/0/1"&gt;Graham E. Rowlands&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Govia_L/0/1/0/all/0/1"&gt;Luke C. G. Govia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribeill_G/0/1/0/all/0/1"&gt;Guilhem J. Ribeill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1"&gt;Minh-Hai Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ohki_T/0/1/0/all/0/1"&gt;Thomas A. Ohki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gauthier_D/0/1/0/all/0/1"&gt;Daniel J. Gauthier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Angel's Girl for Blind Painters: an Efficient Painting Navigation System Validated by Multimodal Evaluation Approach. (arXiv:2107.12921v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.12921</id>
        <link href="http://arxiv.org/abs/2107.12921"/>
        <updated>2021-07-28T02:02:34.546Z</updated>
        <summary type="html"><![CDATA[For people who ardently love painting but unfortunately have visual
impairments, holding a paintbrush to create a work is a very difficult task.
People in this special group are eager to pick up the paintbrush, like Leonardo
da Vinci, to create and make full use of their own talents. Therefore, to
maximally bridge this gap, we propose a painting navigation system to assist
blind people in painting and artistic creation. The proposed system is composed
of cognitive system and guidance system. The system adopts drawing board
positioning based on QR code, brush navigation based on target detection and
bush real-time positioning. Meanwhile, this paper uses human-computer
interaction on the basis of voice and a simple but efficient position
information coding rule. In addition, we design a criterion to efficiently
judge whether the brush reaches the target or not. According to the
experimental results, the thermal curves extracted from the faces of testers
show that it is relatively well accepted by blindfolded and even blind testers.
With the prompt frequency of 1s, the painting navigation system performs best
with the completion degree of 89% with SD of 8.37% and overflow degree of 347%
with SD of 162.14%. Meanwhile, the excellent and good types of brush tip
trajectory account for 74%, and the relative movement distance is 4.21 with SD
of 2.51. This work demonstrates that it is practicable for the blind people to
feel the world through the brush in their hands. In the future, we plan to
deploy Angle's Eyes on the phone to make it more portable. The demo video of
the proposed painting navigation system is available at:
https://doi.org/10.6084/m9.figshare.9760004.v1.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1"&gt;Menghan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuzhen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qingli Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1"&gt;Guangtao Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Simon X. Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao-Ping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaokang Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Methodology guided by Decision Trees Ensemble and Smart Data for Imbalanced Big Data. (arXiv:2001.05759v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.05759</id>
        <link href="http://arxiv.org/abs/2001.05759"/>
        <updated>2021-07-28T02:02:34.537Z</updated>
        <summary type="html"><![CDATA[Differences in data size per class, also known as imbalanced data
distribution, have become a common problem affecting data quality. Big Data
scenarios pose a new challenge to traditional imbalanced classification
algorithms, since they are not prepared to work with such amount of data. Split
data strategies and lack of data in the minority class due to the use of
MapReduce paradigm have posed new challenges for tackling the imbalance between
classes in Big Data scenarios. Ensembles have shown to be able to successfully
address imbalanced data problems. Smart Data refers to data of enough quality
to achieve high performance models. The combination of ensembles and Smart
Data, achieved through Big Data preprocessing, should be a great synergy. In
this paper, we propose a novel methodology based on Decision Trees Ensemble
with Smart Data for addressing the imbalanced classification problem in Big
Data domains, namely DeTE_SD methodology. This methodology is based on the
learning of different decision trees using distributed quality data for the
ensemble process. This quality data is achieved by fusing Random
Discretization, Principal Components Analysis and clustering-based Random
Oversampling for obtaining different Smart Data versions of the original data.
Experiments carried out in 21 binary adapted datasets have shown that our
methodology outperforms Random Forest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_Gil_D/0/1/0/all/0/1"&gt;Diego Garc&amp;#xed;a-Gil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_S/0/1/0/all/0/1"&gt;Salvador Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_N/0/1/0/all/0/1"&gt;Ning Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herrera_F/0/1/0/all/0/1"&gt;Francisco Herrera&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Fusion Methods for Indexing and Retrieval of Biometric Data: Application to Face Recognition with Privacy Protection. (arXiv:2107.12675v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12675</id>
        <link href="http://arxiv.org/abs/2107.12675"/>
        <updated>2021-07-28T02:02:34.530Z</updated>
        <summary type="html"><![CDATA[Computationally efficient, accurate, and privacy-preserving data storage and
retrieval are among the key challenges faced by practical deployments of
biometric identification systems worldwide. In this work, a method of protected
indexing of biometric data is presented. By utilising feature-level fusion of
intelligently paired templates, a multi-stage search structure is created.
During retrieval, the list of potential candidate identities is successively
pre-filtered, thereby reducing the number of template comparisons necessary for
a biometric identification transaction. Protection of the biometric probe
templates, as well as the stored reference templates and the created index is
carried out using homomorphic encryption. The proposed method is extensively
evaluated in closed-set and open-set identification scenarios on publicly
available databases using two state-of-the-art open-source face recognition
systems. With respect to a typical baseline algorithm utilising an exhaustive
search-based retrieval algorithm, the proposed method enables a reduction of
the computational workload associated with a biometric identification
transaction by 90%, while simultaneously suffering no degradation of the
biometric performance. Furthermore, by facilitating a seamless integration of
template protection with open-source homomorphic encryption libraries, the
proposed method guarantees unlinkability, irreversibility, and renewability of
the protected biometric data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Drozdowski_P/0/1/0/all/0/1"&gt;Pawel Drozdowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stockhardt_F/0/1/0/all/0/1"&gt;Fabian Stockhardt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1"&gt;Christian Rathgeb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osorio_Roig_D/0/1/0/all/0/1"&gt;Dail&amp;#xe9; Osorio-Roig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1"&gt;Christoph Busch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble Learning For Mega Man Level Generation. (arXiv:2107.12524v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12524</id>
        <link href="http://arxiv.org/abs/2107.12524"/>
        <updated>2021-07-28T02:02:34.523Z</updated>
        <summary type="html"><![CDATA[Procedural content generation via machine learning (PCGML) is the process of
procedurally generating game content using models trained on existing game
content. PCGML methods can struggle to capture the true variance present in
underlying data with a single model. In this paper, we investigated the use of
ensembles of Markov chains for procedurally generating \emph{Mega Man} levels.
We conduct an initial investigation of our approach and evaluate it on measures
of playability and stylistic similarity in comparison to a non-ensemble,
existing Markov chain approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bowei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1"&gt;Ruohan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1"&gt;Yuqing Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ricky Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenwen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guzdial_M/0/1/0/all/0/1"&gt;Matthew Guzdial&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13948</id>
        <link href="http://arxiv.org/abs/2106.13948"/>
        <updated>2021-07-28T02:02:34.503Z</updated>
        <summary type="html"><![CDATA[Recent advances in the areas of multimodal machine learning and artificial
intelligence (AI) have led to the development of challenging tasks at the
intersection of Computer Vision, Natural Language Processing, and Embodied AI.
Whereas many approaches and previous survey pursuits have characterised one or
two of these dimensions, there has not been a holistic analysis at the center
of all three. Moreover, even when combinations of these topics are considered,
more focus is placed on describing, e.g., current architectural methods, as
opposed to also illustrating high-level challenges and opportunities for the
field. In this survey paper, we discuss Embodied Vision-Language Planning
(EVLP) tasks, a family of prominent embodied navigation and manipulation
problems that jointly use computer vision and natural language. We propose a
taxonomy to unify these tasks and provide an in-depth analysis and comparison
of the new and current algorithmic approaches, metrics, simulated environments,
as well as the datasets used for EVLP tasks. Finally, we present the core
challenges that we believe new EVLP works should seek to address, and we
advocate for task construction that enables model generalizability and furthers
real-world deployment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1"&gt;Jonathan Francis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1"&gt;Nariaki Kitamura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1"&gt;Felix Labelle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaopeng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1"&gt;Ingrid Navarro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1"&gt;Jean Oh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical Guarantees for Fairness Aware Plug-In Algorithms. (arXiv:2107.12783v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.12783</id>
        <link href="http://arxiv.org/abs/2107.12783"/>
        <updated>2021-07-28T02:02:34.496Z</updated>
        <summary type="html"><![CDATA[A plug-in algorithm to estimate Bayes Optimal Classifiers for fairness-aware
binary classification has been proposed in (Menon & Williamson, 2018). However,
the statistical efficacy of their approach has not been established. We prove
that the plug-in algorithm is statistically consistent. We also derive finite
sample guarantees associated with learning the Bayes Optimal Classifiers via
the plug-in algorithm. Finally, we propose a protocol that modifies the plug-in
approach, so as to simultaneously guarantee fairness and differential privacy
with respect to a binary feature deemed sensitive.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Khurana_D/0/1/0/all/0/1"&gt;Drona Khurana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ravichandran_S/0/1/0/all/0/1"&gt;Srinivasan Ravichandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jain_S/0/1/0/all/0/1"&gt;Sparsh Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Edakunni_N/0/1/0/all/0/1"&gt;Narayanan Unny Edakunni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global optimization using random embeddings. (arXiv:2107.12102v1 [math.OC] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2107.12102</id>
        <link href="http://arxiv.org/abs/2107.12102"/>
        <updated>2021-07-28T02:02:34.490Z</updated>
        <summary type="html"><![CDATA[We propose a random-subspace algorithmic framework for global optimization of
Lipschitz-continuous objectives, and analyse its convergence using novel tools
from conic integral geometry. X-REGO randomly projects, in a sequential or
simultaneous manner, the high-dimensional original problem into low-dimensional
subproblems that can then be solved with any global, or even local,
optimization solver. We estimate the probability that the randomly-embedded
subproblem shares (approximately) the same global optimum as the original
problem. This success probability is then used to show convergence of X-REGO to
an approximate global solution of the original problem, under weak assumptions
on the problem (having a strictly feasible global solution) and on the solver
(guaranteed to find an approximate global solution of the reduced problem with
sufficiently high probability). In the particular case of unconstrained
objectives with low effective dimension, that only vary over a low-dimensional
subspace, we propose an X-REGO variant that explores random subspaces of
increasing dimension until finding the effective dimension of the problem,
leading to X-REGO globally converging after a finite number of embeddings,
proportional to the effective dimension. We show numerically that this variant
efficiently finds both the effective dimension and an approximate global
minimizer of the original problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Cartis_C/0/1/0/all/0/1"&gt;Coralia Cartis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Massart_E/0/1/0/all/0/1"&gt;Estelle Massart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Otemissov_A/0/1/0/all/0/1"&gt;Adilet Otemissov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-speaker Style Transfer with Prosody Bottleneck in Neural Speech Synthesis. (arXiv:2107.12562v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.12562</id>
        <link href="http://arxiv.org/abs/2107.12562"/>
        <updated>2021-07-28T02:02:34.483Z</updated>
        <summary type="html"><![CDATA[Cross-speaker style transfer is crucial to the applications of multi-style
and expressive speech synthesis at scale. It does not require the target
speakers to be experts in expressing all styles and to collect corresponding
recordings for model training. However, the performances of existing style
transfer methods are still far behind real application needs. The root causes
are mainly twofold. Firstly, the style embedding extracted from single
reference speech can hardly provide fine-grained and appropriate prosody
information for arbitrary text to synthesize. Secondly, in these models the
content/text, prosody, and speaker timbre are usually highly entangled, it's
therefore not realistic to expect a satisfied result when freely combining
these components, such as to transfer speaking style between speakers. In this
paper, we propose a cross-speaker style transfer text-to-speech (TTS) model
with explicit prosody bottleneck. The prosody bottleneck builds up the kernels
accounting for speaking style robustly, and disentangles the prosody from
content and speaker timbre, therefore guarantees high quality cross-speaker
style transfer. Evaluation result shows the proposed method even achieves
on-par performance with source speaker's speaker-dependent (SD) model in
objective measurement of prosody, and significantly outperforms the cycle
consistency and GMVAE-based baselines in objective and subjective evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Shifeng Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lei He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Role of Optimization in Double Descent: A Least Squares Study. (arXiv:2107.12685v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12685</id>
        <link href="http://arxiv.org/abs/2107.12685"/>
        <updated>2021-07-28T02:02:34.475Z</updated>
        <summary type="html"><![CDATA[Empirically it has been observed that the performance of deep neural networks
steadily improves as we increase model size, contradicting the classical view
on overfitting and generalization. Recently, the double descent phenomena has
been proposed to reconcile this observation with theory, suggesting that the
test error has a second descent when the model becomes sufficiently
overparameterized, as the model size itself acts as an implicit regularizer. In
this paper we add to the growing body of work in this space, providing a
careful study of learning dynamics as a function of model size for the least
squares scenario. We show an excess risk bound for the gradient descent
solution of the least squares objective. The bound depends on the smallest
non-zero eigenvalue of the covariance matrix of the input features, via a
functional form that has the double descent behavior. This gives a new
perspective on the double descent curves reported in the literature. Our
analysis of the excess risk allows to decouple the effect of optimization and
generalization error. In particular, we find that in case of noiseless
regression, double descent is explained solely by optimization-related
quantities, which was missed in studies focusing on the Moore-Penrose
pseudoinverse solution. We believe that our derivation provides an alternative
view compared to existing work, shedding some light on a possible cause of this
phenomena, at least in the considered least squares setting. We empirically
explore if our predictions hold for neural networks, in particular whether the
covariance of intermediary hidden activations has a similar behavior as the one
predicted by our derivations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuzborskij_I/0/1/0/all/0/1"&gt;Ilja Kuzborskij&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1"&gt;Csaba Szepesv&amp;#xe1;ri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rivasplata_O/0/1/0/all/0/1"&gt;Omar Rivasplata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rannen_Triki_A/0/1/0/all/0/1"&gt;Amal Rannen-Triki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1"&gt;Razvan Pascanu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Formalising the Use of the Activation Function in Neural Inference. (arXiv:2102.04896v2 [q-bio.NC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04896</id>
        <link href="http://arxiv.org/abs/2102.04896"/>
        <updated>2021-07-28T02:02:34.452Z</updated>
        <summary type="html"><![CDATA[We investigate how the activation function can be used to describe neural
firing in an abstract way, and in turn, why it works well in artificial neural
networks. We discuss how a spike in a biological neurone belongs to a
particular universality class of phase transitions in statistical physics. We
then show that the artificial neurone is, mathematically, a mean field model of
biological neural membrane dynamics, which arises from modelling spiking as a
phase transition. This allows us to treat selective neural firing in an
abstract way, and formalise the role of the activation function in perceptron
learning. The resultant statistical physical model allows us to recover the
expressions for some known activation functions as various special cases. Along
with deriving this model and specifying the analogous neural case, we analyse
the phase transition to understand the physics of neural network learning.
Together, it is shown that there is not only a biological meaning, but a
physical justification, for the emergence and performance of typical activation
functions; implications for neural learning and inference are also discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Sakthivadivel_D/0/1/0/all/0/1"&gt;Dalton A R Sakthivadivel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An AI-Assisted Design Method for Topology Optimization Without Pre-Optimized Training Data. (arXiv:2012.06384v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06384</id>
        <link href="http://arxiv.org/abs/2012.06384"/>
        <updated>2021-07-28T02:02:34.444Z</updated>
        <summary type="html"><![CDATA[Topology optimization is widely used by engineers during the initial product
development process to get a first possible geometry design. The
state-of-the-art is the iterative calculation, which requires both time and
computational power. Some newly developed methods use artificial intelligence
to accelerate the topology optimization. These require conventionally
pre-optimized data and therefore are dependent on the quality and number of
available data. This paper proposes an AI-assisted design method for topology
optimization, which does not require pre-optimized data. The designs are
provided by an artificial neural network, the predictor, on the basis of
boundary conditions and degree of filling (the volume percentage filled by
material) as input data. In the training phase, geometries generated on the
basis of random input data are evaluated with respect to given criteria. The
results of those evaluations flow into an objective function which is minimized
by adapting the predictor's parameters. After the training is completed, the
presented AI-assisted design procedure supplies geometries which are similar to
the ones generated by conventional topology optimizers, but requires a small
fraction of the computational effort required by those algorithms. We
anticipate our paper to be a starting point for AI-based methods that requires
data, that is hard to compute or not available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Halle_A/0/1/0/all/0/1"&gt;Alex Halle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Campanile_L/0/1/0/all/0/1"&gt;L. Flavio Campanile&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasse_A/0/1/0/all/0/1"&gt;Alexander Hasse&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Optimization Framework for Training Shallow Neural Networks Using Reachability Method. (arXiv:2107.12801v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12801</id>
        <link href="http://arxiv.org/abs/2107.12801"/>
        <updated>2021-07-28T02:02:34.437Z</updated>
        <summary type="html"><![CDATA[In this paper, a robust optimization framework is developed to train shallow
neural networks based on reachability analysis of neural networks. To
characterize noises of input data, the input training data is disturbed in the
description of interval sets. Interval-based reachability analysis is then
performed for the hidden layer. With the reachability analysis results, a
robust optimization training method is developed in the framework of robust
least-square problems. Then, the developed robust least-square problem is
relaxed to a semidefinite programming problem. It has been shown that the
developed robust learning method can provide better robustness against
perturbations at the price of loss of training accuracy to some extent. At
last, the proposed method is evaluated on a robot arm model learning example.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yejiang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1"&gt;Weiming Xiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning with Formal Performance Metrics for Quadcopter Attitude Control under Non-nominal Contexts. (arXiv:2107.12942v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.12942</id>
        <link href="http://arxiv.org/abs/2107.12942"/>
        <updated>2021-07-28T02:02:34.428Z</updated>
        <summary type="html"><![CDATA[We explore the reinforcement learning approach to designing controllers by
extensively discussing the case of a quadcopter attitude controller. We provide
all details allowing to reproduce our approach, starting with a model of the
dynamics of a crazyflie 2.0 under various nominal and non-nominal conditions,
including partial motor failures and wind gusts. We develop a robust form of a
signal temporal logic to quantitatively evaluate the vehicle's behavior and
measure the performance of controllers. The paper thoroughly describes the
choices in training algorithms, neural net architecture, hyperparameters,
observation space in view of the different performance metrics we have
introduced. We discuss the robustness of the obtained controllers, both to
partial loss of power for one rotor and to wind gusts and finish by drawing
conclusions on practical controller design by reinforcement learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bernini_N/0/1/0/all/0/1"&gt;Nicola Bernini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bessa_M/0/1/0/all/0/1"&gt;Mikhail Bessa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Delmas_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Delmas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gold_A/0/1/0/all/0/1"&gt;Arthur Gold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goubault_E/0/1/0/all/0/1"&gt;Eric Goubault&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pennec_R/0/1/0/all/0/1"&gt;Romain Pennec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Putot_S/0/1/0/all/0/1"&gt;Sylvie Putot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sillion_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Sillion&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parallel Surrogate-assisted Optimization Using Mesh Adaptive Direct Search. (arXiv:2107.12421v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2107.12421</id>
        <link href="http://arxiv.org/abs/2107.12421"/>
        <updated>2021-07-28T02:02:34.421Z</updated>
        <summary type="html"><![CDATA[We consider computationally expensive blackbox optimization problems and
present a method that employs surrogate models and concurrent computing at the
search step of the mesh adaptive direct search (MADS) algorithm. Specifically,
we solve a surrogate optimization problem using locally weighted scatterplot
smoothing (LOWESS) models to find promising candidate points to be evaluated by
the blackboxes. We consider several methods for selecting promising points from
a large number of points. We conduct numerical experiments to assess the
performance of the modified MADS algorithm with respect to available CPU
resources by means of five engineering design problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Talgorn_B/0/1/0/all/0/1"&gt;Bastien Talgorn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Alarie_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phane Alarie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kokkolaras_M/0/1/0/all/0/1"&gt;Michael Kokkolaras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Learning to Classify Macromolecular Structures in situ for Less Supervision in Cryo-Electron Tomography. (arXiv:2102.12040v2 [q-bio.QM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12040</id>
        <link href="http://arxiv.org/abs/2102.12040"/>
        <updated>2021-07-28T02:02:34.402Z</updated>
        <summary type="html"><![CDATA[Motivation: Cryo-Electron Tomography (cryo-ET) is a 3D bioimaging tool that
visualizes the structural and spatial organization of macromolecules at a
near-native state in single cells, which has broad applications in life
science. However, the systematic structural recognition and recovery of
macromolecules captured by cryo-ET are difficult due to high structural
complexity and imaging limits. Deep learning based subtomogram classification
have played critical roles for such tasks. As supervised approaches, however,
their performance relies on sufficient and laborious annotation on a large
training dataset.

Results: To alleviate this major labeling burden, we proposed a Hybrid Active
Learning (HAL) framework for querying subtomograms for labelling from a large
unlabeled subtomogram pool. Firstly, HAL adopts uncertainty sampling to select
the subtomograms that have the most uncertain predictions. Moreover, to
mitigate the sampling bias caused by such strategy, a discriminator is
introduced to judge if a certain subtomogram is labeled or unlabeled and
subsequently the model queries the subtomogram that have higher probabilities
to be unlabeled. Additionally, HAL introduces a subset sampling strategy to
improve the diversity of the query set, so that the information overlap is
decreased between the queried batches and the algorithmic efficiency is
improved. Our experiments on subtomogram classification tasks using both
simulated and real data demonstrate that we can achieve comparable testing
performance (on average only 3% accuracy drop) by using less than 30% of the
labeled subtomograms, which shows a very promising result for subtomogram
classification task with limited labeling resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Du_X/0/1/0/all/0/1"&gt;Xuefeng Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haohan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhenxi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xiangrui Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Chang_Y/0/1/0/all/0/1"&gt;Yi-Wei Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Xu_M/0/1/0/all/0/1"&gt;Min Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Power Constrained Bandits. (arXiv:2004.06230v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.06230</id>
        <link href="http://arxiv.org/abs/2004.06230"/>
        <updated>2021-07-28T02:02:34.395Z</updated>
        <summary type="html"><![CDATA[Contextual bandits often provide simple and effective personalization in
decision making problems, making them popular tools to deliver personalized
interventions in mobile health as well as other health applications. However,
when bandits are deployed in the context of a scientific study -- e.g. a
clinical trial to test if a mobile health intervention is effective -- the aim
is not only to personalize for an individual, but also to determine, with
sufficient statistical power, whether or not the system's intervention is
effective. It is essential to assess the effectiveness of the intervention
before broader deployment for better resource allocation. The two objectives
are often deployed under different model assumptions, making it hard to
determine how achieving the personalization and statistical power affect each
other. In this work, we develop general meta-algorithms to modify existing
algorithms such that sufficient power is guaranteed while still improving each
user's well-being. We also demonstrate that our meta-algorithms are robust to
various model mis-specifications possibly appearing in statistical studies,
thus providing a valuable tool to study designers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1"&gt;Jiayu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brunskill_E/0/1/0/all/0/1"&gt;Emma Brunskill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1"&gt;Weiwei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murphy_S/0/1/0/all/0/1"&gt;Susan Murphy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1"&gt;Finale Doshi-Velez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clickbait Detection in YouTube Videos. (arXiv:2107.12791v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12791</id>
        <link href="http://arxiv.org/abs/2107.12791"/>
        <updated>2021-07-28T02:02:34.388Z</updated>
        <summary type="html"><![CDATA[YouTube videos often include captivating descriptions and intriguing
thumbnails designed to increase the number of views, and thereby increase the
revenue for the person who posted the video. This creates an incentive for
people to post clickbait videos, in which the content might deviate
significantly from the title, description, or thumbnail. In effect, users are
tricked into clicking on clickbait videos. In this research, we consider the
challenging problem of detecting clickbait YouTube videos. We experiment with
multiple state-of-the-art machine learning techniques using a variety of
textual features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gothankar_R/0/1/0/all/0/1"&gt;Ruchira Gothankar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Troia_F/0/1/0/all/0/1"&gt;Fabio Di Troia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1"&gt;Mark Stamp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VIPose: Real-time Visual-Inertial 6D Object Pose Tracking. (arXiv:2107.12617v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.12617</id>
        <link href="http://arxiv.org/abs/2107.12617"/>
        <updated>2021-07-28T02:02:34.381Z</updated>
        <summary type="html"><![CDATA[Estimating the 6D pose of objects is beneficial for robotics tasks such as
transportation, autonomous navigation, manipulation as well as in scenarios
beyond robotics like virtual and augmented reality. With respect to single
image pose estimation, pose tracking takes into account the temporal
information across multiple frames to overcome possible detection
inconsistencies and to improve the pose estimation efficiency. In this work, we
introduce a novel Deep Neural Network (DNN) called VIPose, that combines
inertial and camera data to address the object pose tracking problem in
real-time. The key contribution is the design of a novel DNN architecture which
fuses visual and inertial features to predict the objects' relative 6D pose
between consecutive image frames. The overall 6D pose is then estimated by
consecutively combining relative poses. Our approach shows remarkable pose
estimation results for heavily occluded objects that are well known to be very
challenging to handle by existing state-of-the-art solutions. The effectiveness
of the proposed approach is validated on a new dataset called VIYCB with RGB
image, IMU data, and accurate 6D pose annotations created by employing an
automated labeling technique. The approach presents accuracy performances
comparable to state-of-the-art techniques, but with additional benefit to be
real-time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1"&gt;Rundong Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loianno_G/0/1/0/all/0/1"&gt;Giuseppe Loianno&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedH2L: Federated Learning with Model and Statistical Heterogeneity. (arXiv:2101.11296v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11296</id>
        <link href="http://arxiv.org/abs/2101.11296"/>
        <updated>2021-07-28T02:02:34.373Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) enables distributed participants to collectively
learn a strong global model without sacrificing their individual data privacy.
Mainstream FL approaches require each participant to share a common network
architecture and further assume that data are are sampled IID across
participants. However, in real-world deployments participants may require
heterogeneous network architectures; and the data distribution is almost
certainly non-uniform across participants. To address these issues we introduce
FedH2L, which is agnostic to both the model architecture and robust to
different data distributions across participants. In contrast to approaches
sharing parameters or gradients, FedH2L relies on mutual distillation,
exchanging only posteriors on a shared seed set between participants in a
decentralized manner. This makes it extremely bandwidth efficient, model
agnostic, and crucially produces models capable of performing well on the whole
data distribution when learning from heterogeneous silos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yiying Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huaimin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1"&gt;Haibo Mi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1"&gt;Timothy M. Hospedales&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transfer Learning in Electronic Health Records through Clinical Concept Embedding. (arXiv:2107.12919v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12919</id>
        <link href="http://arxiv.org/abs/2107.12919"/>
        <updated>2021-07-28T02:02:34.352Z</updated>
        <summary type="html"><![CDATA[Deep learning models have shown tremendous potential in learning
representations, which are able to capture some key properties of the data.
This makes them great candidates for transfer learning: Exploiting
commonalities between different learning tasks to transfer knowledge from one
task to another. Electronic health records (EHR) research is one of the domains
that has witnessed a growing number of deep learning techniques employed for
learning clinically-meaningful representations of medical concepts (such as
diseases and medications). Despite this growth, the approaches to benchmark and
assess such learned representations (or, embeddings) is under-investigated;
this can be a big issue when such embeddings are shared to facilitate transfer
learning. In this study, we aim to (1) train some of the most prominent disease
embedding techniques on a comprehensive EHR data from 3.1 million patients, (2)
employ qualitative and quantitative evaluation techniques to assess these
embeddings, and (3) provide pre-trained disease embeddings for transfer
learning. This study can be the first comprehensive approach for clinical
concept embedding evaluation and can be applied to any embedding techniques and
for any EHR concept.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Solares_J/0/1/0/all/0/1"&gt;Jose Roberto Ayala Solares&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yajie Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassaine_A/0/1/0/all/0/1"&gt;Abdelaali Hassaine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_S/0/1/0/all/0/1"&gt;Shishir Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yikuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mamouei_M/0/1/0/all/0/1"&gt;Mohammad Mamouei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Canoy_D/0/1/0/all/0/1"&gt;Dexter Canoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahimi_K/0/1/0/all/0/1"&gt;Kazem Rahimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salimi_Khorshidi_G/0/1/0/all/0/1"&gt;Gholamreza Salimi-Khorshidi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Realistic Ultrasound Image Synthesis for Improved Classification of Liver Disease. (arXiv:2107.12775v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.12775</id>
        <link href="http://arxiv.org/abs/2107.12775"/>
        <updated>2021-07-28T02:02:34.345Z</updated>
        <summary type="html"><![CDATA[With the success of deep learning-based methods applied in medical image
analysis, convolutional neural networks (CNNs) have been investigated for
classifying liver disease from ultrasound (US) data. However, the scarcity of
available large-scale labeled US data has hindered the success of CNNs for
classifying liver disease from US data. In this work, we propose a novel
generative adversarial network (GAN) architecture for realistic diseased and
healthy liver US image synthesis. We adopt the concept of stacking to
synthesize realistic liver US data. Quantitative and qualitative evaluation is
performed on 550 in-vivo B-mode liver US images collected from 55 subjects. We
also show that the synthesized images, together with real in vivo data, can be
used to significantly improve the performance of traditional CNN architectures
for Nonalcoholic fatty liver disease (NAFLD) classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Che_H/0/1/0/all/0/1"&gt;Hui Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ramanathan_S/0/1/0/all/0/1"&gt;Sumana Ramanathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Foran_D/0/1/0/all/0/1"&gt;David Foran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nosher_J/0/1/0/all/0/1"&gt;John L Nosher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Patel_V/0/1/0/all/0/1"&gt;Vishal M Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hacihaliloglu_I/0/1/0/all/0/1"&gt;Ilker Hacihaliloglu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Entropy Maximization and Meta Classification for Out-Of-Distribution Detection in Semantic Segmentation. (arXiv:2012.06575v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06575</id>
        <link href="http://arxiv.org/abs/2012.06575"/>
        <updated>2021-07-28T02:02:34.339Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) for the semantic segmentation of images are
usually trained to operate on a predefined closed set of object classes. This
is in contrast to the "open world" setting where DNNs are envisioned to be
deployed to. From a functional safety point of view, the ability to detect
so-called "out-of-distribution" (OoD) samples, i.e., objects outside of a DNN's
semantic space, is crucial for many applications such as automated driving. A
natural baseline approach to OoD detection is to threshold on the pixel-wise
softmax entropy. We present a two-step procedure that significantly improves
that approach. Firstly, we utilize samples from the COCO dataset as OoD proxy
and introduce a second training objective to maximize the softmax entropy on
these samples. Starting from pretrained semantic segmentation networks we
re-train a number of DNNs on different in-distribution datasets and
consistently observe improved OoD detection performance when evaluating on
completely disjoint OoD datasets. Secondly, we perform a transparent
post-processing step to discard false positive OoD samples by so-called "meta
classification". To this end, we apply linear models to a set of hand-crafted
metrics derived from the DNN's softmax probabilities. In our experiments we
consistently observe a clear additional gain in OoD detection performance,
cutting down the number of detection errors by up to 52% when comparing the
best baseline with our results. We achieve this improvement sacrificing only
marginally in original segmentation performance. Therefore, our method
contributes to safer DNNs with more reliable overall system performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1"&gt;Robin Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rottmann_M/0/1/0/all/0/1"&gt;Matthias Rottmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gottschalk_H/0/1/0/all/0/1"&gt;Hanno Gottschalk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Frozen-to-Paraffin: Categorization of Histological Frozen Sections by the Aid of Paraffin Sections and Generative Adversarial Networks. (arXiv:2012.08158v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08158</id>
        <link href="http://arxiv.org/abs/2012.08158"/>
        <updated>2021-07-28T02:02:34.332Z</updated>
        <summary type="html"><![CDATA[In contrast to paraffin sections, frozen sections can be quickly generated
during surgical interventions. This procedure allows surgeons to wait for
histological findings during the intervention to base intra-operative decisions
on the outcome of the histology. However, compared to paraffin sections, the
quality of frozen sections is typically lower, leading to a higher ratio of
miss-classification. In this work, we investigated the effect of the section
type on automated decision support approaches for classification of thyroid
cancer. This was enabled by a data set consisting of pairs of sections for
individual patients. Moreover, we investigated, whether a frozen-to-paraffin
translation could help to optimize classification scores. Finally, we propose a
specific data augmentation strategy to deal with a small amount of training
data and to increase classification accuracy even further.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gadermayr_M/0/1/0/all/0/1"&gt;Michael Gadermayr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tschuchnig_M/0/1/0/all/0/1"&gt;Maximilian Tschuchnig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Stangassinger_L/0/1/0/all/0/1"&gt;Lea Maria Stangassinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kreutzer_C/0/1/0/all/0/1"&gt;Christina Kreutzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Couillard_Despres_S/0/1/0/all/0/1"&gt;Sebastien Couillard-Despres&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oostingh_G/0/1/0/all/0/1"&gt;Gertie Janneke Oostingh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hittmair_A/0/1/0/all/0/1"&gt;Anton Hittmair&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Greedy Gradient Ensemble for Robust Visual Question Answering. (arXiv:2107.12651v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12651</id>
        <link href="http://arxiv.org/abs/2107.12651"/>
        <updated>2021-07-28T02:02:34.325Z</updated>
        <summary type="html"><![CDATA[Language bias is a critical issue in Visual Question Answering (VQA), where
models often exploit dataset biases for the final decision without considering
the image information. As a result, they suffer from performance drop on
out-of-distribution data and inadequate visual explanation. Based on
experimental analysis for existing robust VQA methods, we stress the language
bias in VQA that comes from two aspects, i.e., distribution bias and shortcut
bias. We further propose a new de-bias framework, Greedy Gradient Ensemble
(GGE), which combines multiple biased models for unbiased base model learning.
With the greedy strategy, GGE forces the biased models to over-fit the biased
data distribution in priority, thus makes the base model pay more attention to
examples that are hard to solve by biased models. The experiments demonstrate
that our method makes better use of visual information and achieves
state-of-the-art performance on diagnosing dataset VQA-CP without using extra
annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xinzhe Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuhui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1"&gt;Chi Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qingming Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A persistent homology-based topological loss for CNN-based multi-class segmentation of CMR. (arXiv:2107.12689v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.12689</id>
        <link href="http://arxiv.org/abs/2107.12689"/>
        <updated>2021-07-28T02:02:34.306Z</updated>
        <summary type="html"><![CDATA[Multi-class segmentation of cardiac magnetic resonance (CMR) images seeks a
separation of data into anatomical components with known structure and
configuration. The most popular CNN-based methods are optimised using pixel
wise loss functions, ignorant of the spatially extended features that
characterise anatomy. Therefore, whilst sharing a high spatial overlap with the
ground truth, inferred CNN-based segmentations can lack coherence, including
spurious connected components, holes and voids. Such results are implausible,
violating anticipated anatomical topology. In response, (single-class)
persistent homology-based loss functions have been proposed to capture global
anatomical features. Our work extends these approaches to the task of
multi-class segmentation. Building an enriched topological description of all
class labels and class label pairs, our loss functions make predictable and
statistically significant improvements in segmentation topology using a
CNN-based post-processing framework. We also present (and make available) a
highly efficient implementation based on cubical complexes and parallel
execution, enabling practical application within high resolution 3D data for
the first time. We demonstrate our approach on 2D short axis and 3D whole heart
CMR segmentation, advancing a detailed and faithful analysis of performance on
two publicly available datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Byrne_N/0/1/0/all/0/1"&gt;Nick Byrne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Clough_J/0/1/0/all/0/1"&gt;James R Clough&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Valverde_I/0/1/0/all/0/1"&gt;Isra Valverde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Montana_G/0/1/0/all/0/1"&gt;Giovanni Montana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+King_A/0/1/0/all/0/1"&gt;Andrew P King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSP: Dual Soft-Paste for Unsupervised Domain Adaptive Semantic Segmentation. (arXiv:2107.09600v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.09600</id>
        <link href="http://arxiv.org/abs/2107.09600"/>
        <updated>2021-07-28T02:02:34.299Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation (UDA) for semantic segmentation aims to adapt
a segmentation model trained on the labeled source domain to the unlabeled
target domain. Existing methods try to learn domain invariant features while
suffering from large domain gaps that make it difficult to correctly align
discrepant features, especially in the initial training phase. To address this
issue, we propose a novel Dual Soft-Paste (DSP) method in this paper.
Specifically, DSP selects some classes from a source domain image using a
long-tail class first sampling strategy and softly pastes the corresponding
image patch on both the source and target training images with a fusion weight.
Technically, we adopt the mean teacher framework for domain adaptation, where
the pasted source and target images go through the student network while the
original target image goes through the teacher network. Output-level alignment
is carried out by aligning the probability maps of the target fused image from
both networks using a weighted cross-entropy loss. In addition, feature-level
alignment is carried out by aligning the feature maps of the source and target
images from student network using a weighted maximum mean discrepancy loss. DSP
facilitates the model learning domain-invariant features from the intermediate
domains, leading to faster convergence and better performance. Experiments on
two challenging benchmarks demonstrate the superiority of DSP over
state-of-the-art methods. Code is available at
\url{https://github.com/GaoLii/DSP}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Li Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lefei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disfluency Detection with Unlabeled Data and Small BERT Models. (arXiv:2104.10769v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10769</id>
        <link href="http://arxiv.org/abs/2104.10769"/>
        <updated>2021-07-28T02:02:34.291Z</updated>
        <summary type="html"><![CDATA[Disfluency detection models now approach high accuracy on English text.
However, little exploration has been done in improving the size and inference
time of the model. At the same time, automatic speech recognition (ASR) models
are moving from server-side inference to local, on-device inference. Supporting
models in the transcription pipeline (like disfluency detection) must follow
suit. In this work we concentrate on the disfluency detection task, focusing on
small, fast, on-device models based on the BERT architecture. We demonstrate it
is possible to train disfluency detection models as small as 1.3 MiB, while
retaining high performance. We build on previous work that showed the benefit
of data augmentation approaches such as self-training. Then, we evaluate the
effect of domain mismatch between conversational and written text on model
performance. We find that domain adaptation and data augmentation strategies
have a more pronounced effect on these smaller models, as compared to
conventional BERT models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rocholl_J/0/1/0/all/0/1"&gt;Johann C. Rocholl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zayats_V/0/1/0/all/0/1"&gt;Vicky Zayats&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walker_D/0/1/0/all/0/1"&gt;Daniel D. Walker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murad_N/0/1/0/all/0/1"&gt;Noah B. Murad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schneider_A/0/1/0/all/0/1"&gt;Aaron Schneider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liebling_D/0/1/0/all/0/1"&gt;Daniel J. Liebling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Greedy Gradient Ensemble for Robust Visual Question Answering. (arXiv:2107.12651v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12651</id>
        <link href="http://arxiv.org/abs/2107.12651"/>
        <updated>2021-07-28T02:02:34.285Z</updated>
        <summary type="html"><![CDATA[Language bias is a critical issue in Visual Question Answering (VQA), where
models often exploit dataset biases for the final decision without considering
the image information. As a result, they suffer from performance drop on
out-of-distribution data and inadequate visual explanation. Based on
experimental analysis for existing robust VQA methods, we stress the language
bias in VQA that comes from two aspects, i.e., distribution bias and shortcut
bias. We further propose a new de-bias framework, Greedy Gradient Ensemble
(GGE), which combines multiple biased models for unbiased base model learning.
With the greedy strategy, GGE forces the biased models to over-fit the biased
data distribution in priority, thus makes the base model pay more attention to
examples that are hard to solve by biased models. The experiments demonstrate
that our method makes better use of visual information and achieves
state-of-the-art performance on diagnosing dataset VQA-CP without using extra
annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xinzhe Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuhui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1"&gt;Chi Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qingming Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Adversarial User Privacy in Lossy Single-Server Information Retrieval. (arXiv:2012.03902v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03902</id>
        <link href="http://arxiv.org/abs/2012.03902"/>
        <updated>2021-07-28T02:02:34.277Z</updated>
        <summary type="html"><![CDATA[We propose to extend the concept of private information retrieval by allowing
for distortion in the retrieval process and relaxing the perfect privacy
requirement at the same time. In particular, we study the tradeoff between
download rate, distortion, and user privacy leakage, and show that in the limit
of large file sizes this trade-off can be captured via a novel
information-theoretical formulation for datasets with a known distribution.
Moreover, for scenarios where the statistics of the dataset is unknown, we
propose a new deep learning framework by leveraging a generative adversarial
network approach, which allows the user to learn efficient schemes from the
data itself, minimizing the download cost. We evaluate the performance of the
scheme on a synthetic Gaussian dataset as well as on both the MNIST and
CIFAR-10 datasets. For the MNIST dataset, the data-driven approach
significantly outperforms a non-learning based scheme which combines source
coding with multiple file download, while the CIFAR-10 performance is notably
better.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weng_C/0/1/0/all/0/1"&gt;Chung-Wei Weng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yakimenka_Y/0/1/0/all/0/1"&gt;Yauhen Yakimenka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Hsuan-Yin Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosnes_E/0/1/0/all/0/1"&gt;Eirik Rosnes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kliewer_J/0/1/0/all/0/1"&gt;Joerg Kliewer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Verifiable Coded Computing: Towards Fast, Secure and Private Distributed Machine Learning. (arXiv:2107.12958v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2107.12958</id>
        <link href="http://arxiv.org/abs/2107.12958"/>
        <updated>2021-07-28T02:02:34.258Z</updated>
        <summary type="html"><![CDATA[Stragglers, Byzantine workers, and data privacy are the main bottlenecks in
distributed cloud computing. Several prior works proposed coded computing
strategies to jointly address all three challenges. They require either a large
number of workers, a significant communication cost or a significant
computational complexity to tolerate malicious workers. Much of the overhead in
prior schemes comes from the fact that they tightly couple coding for all three
problems into a single framework. In this work, we propose Verifiable Coded
Computing (VCC) framework that decouples Byzantine node detection challenge
from the straggler tolerance. VCC leverages coded computing just for handling
stragglers and privacy, and then uses an orthogonal approach of verifiable
computing to tackle Byzantine nodes. Furthermore, VCC dynamically adapts its
coding scheme to tradeoff straggler tolerance with Byzantine protection and
vice-versa. We evaluate VCC on compute intensive distributed logistic
regression application. Our experiments show that VCC speeds up the
conventional uncoded implementation of distributed logistic regression by
$3.2\times-6.9\times$, and also improves the test accuracy by up to $12.6\%$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1"&gt;Tingting Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ali_R/0/1/0/all/0/1"&gt;Ramy E. Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashemi_H/0/1/0/all/0/1"&gt;Hanieh Hashemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gangwani_T/0/1/0/all/0/1"&gt;Tynan Gangwani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1"&gt;Salman Avestimehr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Annavaram_M/0/1/0/all/0/1"&gt;Murali Annavaram&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Restricted Boltzmann Machine and Deep Belief Network: Tutorial and Survey. (arXiv:2107.12521v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12521</id>
        <link href="http://arxiv.org/abs/2107.12521"/>
        <updated>2021-07-28T02:02:34.248Z</updated>
        <summary type="html"><![CDATA[This is a tutorial and survey paper on Boltzmann Machine (BM), Restricted
Boltzmann Machine (RBM), and Deep Belief Network (DBN). We start with the
required background on probabilistic graphical models, Markov random field,
Gibbs sampling, statistical physics, Ising model, and the Hopfield network.
Then, we introduce the structures of BM and RBM. The conditional distributions
of visible and hidden variables, Gibbs sampling in RBM for generating
variables, training BM and RBM by maximum likelihood estimation, and
contrastive divergence are explained. Then, we discuss different possible
discrete and continuous distributions for the variables. We introduce
conditional RBM and how it is trained. Finally, we explain deep belief network
as a stack of RBM models. This paper on Boltzmann machines can be useful in
various fields including data science, statistics, neural computation, and
statistical physics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghojogh_B/0/1/0/all/0/1"&gt;Benyamin Ghojogh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghodsi_A/0/1/0/all/0/1"&gt;Ali Ghodsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karray_F/0/1/0/all/0/1"&gt;Fakhri Karray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crowley_M/0/1/0/all/0/1"&gt;Mark Crowley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction. (arXiv:2107.12512v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12512</id>
        <link href="http://arxiv.org/abs/2107.12512"/>
        <updated>2021-07-28T02:02:34.240Z</updated>
        <summary type="html"><![CDATA[Recent learning approaches that implicitly represent surface geometry using
coordinate-based neural representations have shown impressive results in the
problem of multi-view 3D reconstruction. The effectiveness of these techniques
is, however, subject to the availability of a large number (several tens) of
input views of the scene, and computationally demanding optimizations. In this
paper, we tackle these limitations for the specific problem of few-shot full 3D
head reconstruction, by endowing coordinate-based representations with a
probabilistic shape prior that enables faster convergence and better
generalization when using few input images (down to three). First, we learn a
shape model of 3D heads from thousands of incomplete raw scans using implicit
representations. At test time, we jointly overfit two coordinate-based neural
networks to the scene, one modeling the geometry and another estimating the
surface radiance, using implicit differentiable rendering. We devise a
two-stage optimization strategy in which the learned prior is used to
initialize and constrain the geometry during an initial optimization phase.
Then, the prior is unfrozen and fine-tuned to the scene. By doing this, we
achieve high-fidelity head reconstructions, including hair and shoulders, and
with a high level of detail that consistently outperforms both state-of-the-art
3D Morphable Models methods in the few-shot scenario, and non-parametric
methods when large sets of views are available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ramon_E/0/1/0/all/0/1"&gt;Eduard Ramon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Triginer_G/0/1/0/all/0/1"&gt;Gil Triginer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Escur_J/0/1/0/all/0/1"&gt;Janna Escur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pumarola_A/0/1/0/all/0/1"&gt;Albert Pumarola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_J/0/1/0/all/0/1"&gt;Jaime Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giro_i_Nieto_X/0/1/0/all/0/1"&gt;Xavier Giro-i-Nieto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1"&gt;Francesc Moreno-Noguer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Synergy, Redundancy, and Independence in Global Model Explanations using SHAP Vector Decomposition. (arXiv:2107.12436v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12436</id>
        <link href="http://arxiv.org/abs/2107.12436"/>
        <updated>2021-07-28T02:02:34.232Z</updated>
        <summary type="html"><![CDATA[We offer a new formalism for global explanations of pairwise feature
dependencies and interactions in supervised models. Building upon SHAP values
and SHAP interaction values, our approach decomposes feature contributions into
synergistic, redundant and independent components (S-R-I decomposition of SHAP
vectors). We propose a geometric interpretation of the components and formally
prove its basic properties. Finally, we demonstrate the utility of synergy,
redundancy and independence by applying them to a constructed data set and
model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ittner_J/0/1/0/all/0/1"&gt;Jan Ittner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bolikowski_L/0/1/0/all/0/1"&gt;Lukasz Bolikowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hemker_K/0/1/0/all/0/1"&gt;Konstantin Hemker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kennedy_R/0/1/0/all/0/1"&gt;Ricardo Kennedy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Thompson Sampling strategies for support-aware CVaR bandits. (arXiv:2012.05754v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05754</id>
        <link href="http://arxiv.org/abs/2012.05754"/>
        <updated>2021-07-28T02:02:34.225Z</updated>
        <summary type="html"><![CDATA[In this paper we study a multi-arm bandit problem in which the quality of
each arm is measured by the Conditional Value at Risk (CVaR) at some level
alpha of the reward distribution. While existing works in this setting mainly
focus on Upper Confidence Bound algorithms, we introduce a new Thompson
Sampling approach for CVaR bandits on bounded rewards that is flexible enough
to solve a variety of problems grounded on physical resources. Building on a
recent work by Riou & Honda (2020), we introduce B-CVTS for continuous bounded
rewards and M-CVTS for multinomial distributions. On the theoretical side, we
provide a non-trivial extension of their analysis that enables to theoretically
bound their CVaR regret minimization performance. Strikingly, our results show
that these strategies are the first to provably achieve asymptotic optimality
in CVaR bandits, matching the corresponding asymptotic lower bounds for this
setting. Further, we illustrate empirically the benefit of Thompson Sampling
approaches both in a realistic environment simulating a use-case in agriculture
and on various synthetic examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baudry_D/0/1/0/all/0/1"&gt;Dorian Baudry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gautron_R/0/1/0/all/0/1"&gt;Romain Gautron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaufmann_E/0/1/0/all/0/1"&gt;Emilie Kaufmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maillard_O/0/1/0/all/0/1"&gt;Odalric-Ambryn Maillard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Small-loss bounds for online learning with partial information. (arXiv:1711.03639v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1711.03639</id>
        <link href="http://arxiv.org/abs/1711.03639"/>
        <updated>2021-07-28T02:02:34.208Z</updated>
        <summary type="html"><![CDATA[We consider the problem of adversarial (non-stochastic) online learning with
partial information feedback, where at each round, a decision maker selects an
action from a finite set of alternatives. We develop a black-box approach for
such problems where the learner observes as feedback only losses of a subset of
the actions that includes the selected action. When losses of actions are
non-negative, under the graph-based feedback model introduced by Mannor and
Shamir, we offer algorithms that attain the so called "small-loss" $o(\alpha
L^{\star})$ regret bounds with high probability, where $\alpha$ is the
independence number of the graph, and $L^{\star}$ is the loss of the best
action. Prior to our work, there was no data-dependent guarantee for general
feedback graphs even for pseudo-regret (without dependence on the number of
actions, i.e. utilizing the increased information feedback). Taking advantage
of the black-box nature of our technique, we extend our results to many other
applications such as semi-bandits (including routing in networks), contextual
bandits (even with an infinite comparator class), as well as learning with
slowly changing (shifting) comparators.

In the special case of classical bandit and semi-bandit problems, we provide
optimal small-loss, high-probability guarantees of
$\tilde{O}(\sqrt{dL^{\star}})$ for actual regret, where $d$ is the number of
actions, answering open questions of Neu. Previous bounds for bandits and
semi-bandits were known only for pseudo-regret and only in expectation. We also
offer an optimal $\tilde{O}(\sqrt{\kappa L^{\star}})$ regret guarantee for
fixed feedback graphs with clique-partition number at most $\kappa$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lykouris_T/0/1/0/all/0/1"&gt;Thodoris Lykouris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sridharan_K/0/1/0/all/0/1"&gt;Karthik Sridharan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tardos_E/0/1/0/all/0/1"&gt;Eva Tardos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral Graph Attention Network with Fast Eigen-approximation. (arXiv:2003.07450v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.07450</id>
        <link href="http://arxiv.org/abs/2003.07450"/>
        <updated>2021-07-28T02:02:34.201Z</updated>
        <summary type="html"><![CDATA[Variants of Graph Neural Networks (GNNs) for representation learning have
been proposed recently and achieved fruitful results in various fields. Among
them, Graph Attention Network (GAT) first employs a self-attention strategy to
learn attention weights for each edge in the spatial domain. However, learning
the attentions over edges can only focus on the local information of graphs and
greatly increases the computational costs. In this paper, we first introduce
the attention mechanism in the spectral domain of graphs and present Spectral
Graph Attention Network (SpGAT) that learns representations for different
frequency components regarding weighted filters and graph wavelets bases. In
this way, SpGAT can better capture global patterns of graphs in an efficient
manner with much fewer learned parameters than that of GAT. Further, to reduce
the computational cost of SpGAT brought by the eigen-decomposition, we propose
a fast approximation variant SpGAT-Cheby. We thoroughly evaluate the
performance of SpGAT and SpGAT-Cheby in semi-supervised node classification
tasks and verify the effectiveness of the learned attentions in the spectral
domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Heng Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1"&gt;Yu Rong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1"&gt;Tingyang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wenbing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sojoudi_S/0/1/0/all/0/1"&gt;Somayeh Sojoudi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Junzhou Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1"&gt;Wenwu Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Channel-Wise Early Stopping without a Validation Set via NNK Polytope Interpolation. (arXiv:2107.12972v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12972</id>
        <link href="http://arxiv.org/abs/2107.12972"/>
        <updated>2021-07-28T02:02:34.193Z</updated>
        <summary type="html"><![CDATA[State-of-the-art neural network architectures continue to scale in size and
deliver impressive generalization results, although this comes at the expense
of limited interpretability. In particular, a key challenge is to determine
when to stop training the model, as this has a significant impact on
generalization. Convolutional neural networks (ConvNets) comprise
high-dimensional feature spaces formed by the aggregation of multiple channels,
where analyzing intermediate data representations and the model's evolution can
be challenging owing to the curse of dimensionality. We present channel-wise
DeepNNK (CW-DeepNNK), a novel channel-wise generalization estimate based on
non-negative kernel regression (NNK) graphs with which we perform local
polytope interpolation on low-dimensional channels. This method leads to
instance-based interpretability of both the learned data representations and
the relationship between channels. Motivated by our observations, we use
CW-DeepNNK to propose a novel early stopping criterion that (i) does not
require a validation set, (ii) is based on a task performance metric, and (iii)
allows stopping to be reached at different points for each channel. Our
experiments demonstrate that our proposed method has advantages as compared to
the standard criterion based on validation set performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bonet_D/0/1/0/all/0/1"&gt;David Bonet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ortega_A/0/1/0/all/0/1"&gt;Antonio Ortega&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruiz_Hidalgo_J/0/1/0/all/0/1"&gt;Javier Ruiz-Hidalgo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shekkizhar_S/0/1/0/all/0/1"&gt;Sarath Shekkizhar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Competition in Cross-situational Word Learning: A Computational Study. (arXiv:2012.03370v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03370</id>
        <link href="http://arxiv.org/abs/2012.03370"/>
        <updated>2021-07-28T02:02:34.176Z</updated>
        <summary type="html"><![CDATA[Children learn word meanings by tapping into the commonalities across
different situations in which words are used and overcome the high level of
uncertainty involved in early word learning experiences. We propose a modeling
framework to investigate the role of mutual exclusivity bias - asserting
one-to-one mappings between words and their meanings - in reducing uncertainty
in word learning. In a set of computational studies, we show that to
successfully learn word meanings in the face of uncertainty, a learner needs to
use two types of competition: words competing for association to a referent
when learning from an observation and referents competing for a word when the
word is used. Our work highlights the importance of an algorithmic-level
analysis to shed light on the utility of different mechanisms that can
implement the same computational-level theory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nematzadeh_A/0/1/0/all/0/1"&gt;Aida Nematzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shekarchi_Z/0/1/0/all/0/1"&gt;Zahra Shekarchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1"&gt;Thomas L. Griffiths&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stevenson_S/0/1/0/all/0/1"&gt;Suzanne Stevenson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic sparse adversarial attacks. (arXiv:2011.12423v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12423</id>
        <link href="http://arxiv.org/abs/2011.12423"/>
        <updated>2021-07-28T02:02:34.166Z</updated>
        <summary type="html"><![CDATA[This paper introduces stochastic sparse adversarial attacks (SSAA), simple,
fast and purely noise-based targeted and untargeted $L_0$ attacks of neural
network classifiers (NNC). SSAA are devised by exploiting a simple small-time
expansion idea widely used for Markov processes and offer new examples of $L_0$
attacks whose studies have been limited. They are designed to solve the known
scalability issue of the family of Jacobian-based saliency maps attacks to
large datasets and they succeed in solving it. Experiments on small and large
datasets (CIFAR-10 and ImageNet) illustrate further advantages of SSAA in
comparison with the-state-of-the-art methods. For instance, in the untargeted
case, our method called Voting Folded Gaussian Attack (VFGA) scales efficiently
to ImageNet and achieves a significantly lower $L_0$ score than SparseFool (up
to $\frac{2}{5}$ lower) while being faster. Moreover, VFGA achieves better
$L_0$ scores on ImageNet than Sparse-RS when both attacks are fully successful
on a large number of samples. Codes are publicly available through the link
https://github.com/SSAA3/stochastic-sparse-adv-attacks]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cesaire_M/0/1/0/all/0/1"&gt;Manon C&amp;#xe9;saire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hajri_H/0/1/0/all/0/1"&gt;Hatem Hajri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lamprier_S/0/1/0/all/0/1"&gt;Sylvain Lamprier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1"&gt;Patrick Gallinari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Persistent Reinforcement Learning via Subgoal Curricula. (arXiv:2107.12931v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12931</id>
        <link href="http://arxiv.org/abs/2107.12931"/>
        <updated>2021-07-28T02:02:34.146Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning (RL) promises to enable autonomous acquisition of
complex behaviors for diverse agents. However, the success of current
reinforcement learning algorithms is predicated on an often under-emphasised
requirement -- each trial needs to start from a fixed initial state
distribution. Unfortunately, resetting the environment to its initial state
after each trial requires substantial amount of human supervision and extensive
instrumentation of the environment which defeats the purpose of autonomous
reinforcement learning. In this work, we propose Value-accelerated Persistent
Reinforcement Learning (VaPRL), which generates a curriculum of initial states
such that the agent can bootstrap on the success of easier tasks to efficiently
learn harder tasks. The agent also learns to reach the initial states proposed
by the curriculum, minimizing the reliance on human interventions into the
learning. We observe that VaPRL reduces the interventions required by three
orders of magnitude compared to episodic RL while outperforming prior
state-of-the art methods for reset-free RL both in terms of sample efficiency
and asymptotic performance on a variety of simulated robotics problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Archit Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Abhishek Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1"&gt;Karol Hausman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1"&gt;Chelsea Finn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Simplified Framework for Air Route Clustering Based on ADS-B Data. (arXiv:2107.12869v1 [physics.soc-ph])]]></title>
        <id>http://arxiv.org/abs/2107.12869</id>
        <link href="http://arxiv.org/abs/2107.12869"/>
        <updated>2021-07-28T02:02:34.139Z</updated>
        <summary type="html"><![CDATA[The volume of flight traffic gets increasing over the time, which makes the
strategic traffic flow management become one of the challenging problems since
it requires a lot of computational resources to model entire traffic data. On
the other hand, Automatic Dependent Surveillance - Broadcast (ADS-B) technology
has been considered as a promising data technology to provide both flight crews
and ground control staff the necessary information safely and efficiently about
the position and velocity of the airplanes in a specific area. In the attempt
to tackle this problem, we presented in this paper a simplified framework that
can support to detect the typical air routes between airports based on ADS-B
data. Specifically, the flight traffic will be classified into major groups
based on similarity measures, which helps to reduce the number of flight paths
between airports. As a matter of fact, our framework can be taken into account
to reduce practically the computational cost for air flow optimization and
evaluate the operational performance. Finally, in order to illustrate the
potential applications of our proposed framework, an experiment was performed
using ADS-B traffic flight data of three different pairs of airports. The
detected typical routes between each couple of airports show promising results
by virtue of combining two indices for measuring the clustering performance and
incorporating human judgment into the visual inspection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Duong_Q/0/1/0/all/0/1"&gt;Quan Duong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Tran_T/0/1/0/all/0/1"&gt;Tan Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Pham_D/0/1/0/all/0/1"&gt;Duc-Thinh Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Mai_A/0/1/0/all/0/1"&gt;An Mai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Data-driven feature selection and machine-learning model benchmark for the prediction of longitudinal dispersion coefficient. (arXiv:2107.12970v1 [physics.geo-ph])]]></title>
        <id>http://arxiv.org/abs/2107.12970</id>
        <link href="http://arxiv.org/abs/2107.12970"/>
        <updated>2021-07-28T02:02:34.133Z</updated>
        <summary type="html"><![CDATA[Longitudinal Dispersion(LD) is the dominant process of scalar transport in
natural streams. An accurate prediction on LD coefficient(Dl) can produce a
performance leap in related simulation. The emerging machine learning(ML)
techniques provide a self-adaptive tool for this problem. However, most of the
existing studies utilize an unproved quaternion feature set, obtained through
simple theoretical deduction. Few studies have put attention on its reliability
and rationality. Besides, due to the lack of comparative comparison, the proper
choice of ML models in different scenarios still remains unknown. In this
study, the Feature Gradient selector was first adopted to distill the local
optimal feature sets directly from multivariable data. Then, a global optimal
feature set (the channel width, the flow velocity, the channel slope and the
cross sectional area) was proposed through numerical comparison of the
distilled local optimums in performance with representative ML models. The
channel slope is identified to be the key parameter for the prediction of LDC.
Further, we designed a weighted evaluation metric which enables comprehensive
model comparison. With the simple linear model as the baseline, a benchmark of
single and ensemble learning models was provided. Advantages and disadvantages
of the methods involved were also discussed. Results show that the support
vector machine has significantly better performance than other models. Decision
tree is not suitable for this problem due to poor generalization ability.
Notably, simple models show superiority over complicated model on this
low-dimensional problem, for their better balance between regression and
generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yifeng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Galindo_Torres_S/0/1/0/all/0/1"&gt;S.A. Galindo-Torres&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Li_S/0/1/0/all/0/1"&gt;Stan Z. Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Operating Points for High Performance Lesion Detection and Segmentation Using Lesion Size Reweighting. (arXiv:2107.12978v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.12978</id>
        <link href="http://arxiv.org/abs/2107.12978"/>
        <updated>2021-07-28T02:02:34.125Z</updated>
        <summary type="html"><![CDATA[There are many clinical contexts which require accurate detection and
segmentation of all focal pathologies (e.g. lesions, tumours) in patient
images. In cases where there are a mix of small and large lesions, standard
binary cross entropy loss will result in better segmentation of large lesions
at the expense of missing small ones. Adjusting the operating point to
accurately detect all lesions generally leads to oversegmentation of large
lesions. In this work, we propose a novel reweighing strategy to eliminate this
performance gap, increasing small pathology detection performance while
maintaining segmentation accuracy. We show that our reweighing strategy vastly
outperforms competing strategies based on experiments on a large scale,
multi-scanner, multi-center dataset of Multiple Sclerosis patient images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nichyporuk_B/0/1/0/all/0/1"&gt;Brennan Nichyporuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Szeto_J/0/1/0/all/0/1"&gt;Justin Szeto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arnold_D/0/1/0/all/0/1"&gt;Douglas L. Arnold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arbel_T/0/1/0/all/0/1"&gt;Tal Arbel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Numeric Optimal Differentially Private Truncated Additive Mechanisms. (arXiv:2107.12957v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.12957</id>
        <link href="http://arxiv.org/abs/2107.12957"/>
        <updated>2021-07-28T02:02:34.118Z</updated>
        <summary type="html"><![CDATA[Differentially private (DP) mechanisms face the challenge of providing
accurate results while protecting their inputs: the privacy-utility trade-off.
A simple but powerful technique for DP adds noise to sensitivity-bounded query
outputs to blur the exact query output: additive mechanisms. While a vast body
of work considers infinitely wide noise distributions, some applications (e.g.,
real-time operating systems) require hard bounds on the deviations from the
real query, and only limited work on such mechanisms exist. An additive
mechanism with truncated noise (i.e., with bounded range) can offer such hard
bounds. We introduce a gradient-descent-based tool to learn truncated noise for
additive mechanisms with strong utility bounds while simultaneously optimizing
for differential privacy under sequential composition, i.e., scenarios where
multiple noisy queries on the same data are revealed. Our method can learn
discrete noise patterns and not only hyper-parameters of a predefined
probability distribution. For sensitivity bounded mechanisms, we show that it
is sufficient to consider symmetric and that\new{, for from the mean
monotonically falling noise,} ensuring privacy for a pair of representative
query outputs guarantees privacy for all pairs of inputs (that differ in one
element). We find that the utility-privacy trade-off curves of our generated
noise are remarkably close to truncated Gaussians and even replicate their
shape for $l_2$ utility-loss. For a low number of compositions, we also
improved DP-SGD (sub-sampling). Moreover, we extend Moments Accountant to
truncated distributions, allowing to incorporate mechanism output events with
varying input-dependent zero occurrence probability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sommer_D/0/1/0/all/0/1"&gt;David M. Sommer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abfalterer_L/0/1/0/all/0/1"&gt;Lukas Abfalterer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zingg_S/0/1/0/all/0/1"&gt;Sheila Zingg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohammadi_E/0/1/0/all/0/1"&gt;Esfandiar Mohammadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs. (arXiv:2107.12824v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12824</id>
        <link href="http://arxiv.org/abs/2107.12824"/>
        <updated>2021-07-28T02:02:34.103Z</updated>
        <summary type="html"><![CDATA[Although high-performance deep neural networks are in high demand in edge
environments, computation resources are strictly limited in edge devices, and
light-weight neural network techniques, such as Depthwise Separable Convolution
(DSC), have been developed. ResNet is one of conventional deep neural network
models that stack a lot of layers and parameters for a higher accuracy. To
reduce the parameter size of ResNet, by utilizing a similarity to ODE (Ordinary
Differential Equation), Neural ODE repeatedly uses most of weight parameters
instead of having a lot of different parameters. Thus, Neural ODE becomes
significantly small compared to that of ResNet so that it can be implemented in
resource-limited edge devices. In this paper, a combination of Neural ODE and
DSC, called dsODENet, is designed and implemented for FPGAs (Field-Programmable
Gate Arrays). dsODENet is then applied to edge domain adaptation as a practical
use case and evaluated with image classification datasets. It is implemented on
Xilinx ZCU104 board and evaluated in terms of domain adaptation accuracy,
training speed, FPGA resource utilization, and speedup rate compared to a
software execution. The results demonstrate that dsODENet is comparable to or
slightly better than our baseline Neural ODE implementation in terms of domain
adaptation accuracy, while the total parameter size without pre- and
post-processing layers is reduced by 54.2% to 79.8%. The FPGA implementation
accelerates the prediction tasks by 27.9 times faster than a software
implementation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kawakami_H/0/1/0/all/0/1"&gt;Hiroki Kawakami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watanabe_H/0/1/0/all/0/1"&gt;Hirohisa Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1"&gt;Keisuke Sugiura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsutani_H/0/1/0/all/0/1"&gt;Hiroki Matsutani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linear Prediction Residual for Efficient Diagnosis of Parkinson's Disease from Gait. (arXiv:2107.12878v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.12878</id>
        <link href="http://arxiv.org/abs/2107.12878"/>
        <updated>2021-07-28T02:02:34.096Z</updated>
        <summary type="html"><![CDATA[Parkinson's Disease (PD) is a chronic and progressive neurological disorder
that results in rigidity, tremors and postural instability. There is no
definite medical test to diagnose PD and diagnosis is mostly a clinical
exercise. Although guidelines exist, about 10-30% of the patients are wrongly
diagnosed with PD. Hence, there is a need for an accurate, unbiased and fast
method for diagnosis. In this study, we propose LPGNet, a fast and accurate
method to diagnose PD from gait. LPGNet uses Linear Prediction Residuals (LPR)
to extract discriminating patterns from gait recordings and then uses a 1D
convolution neural network with depth-wise separable convolutions to perform
diagnosis. LPGNet achieves an AUC of 0.91 with a 21 times speedup and about 99%
lesser parameters in the model compared to the state of the art. We also
undertake an analysis of various cross-validation strategies used in literature
in PD diagnosis from gait and find that most methods are affected by some form
of data leakage between various folds which leads to unnecessarily large models
and inflated performance due to overfitting. The analysis clears the path for
future works in correctly evaluating their methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Alle_S/0/1/0/all/0/1"&gt;Shanmukh Alle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Priyakumar_U/0/1/0/all/0/1"&gt;U. Deva Priyakumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse Bayesian Deep Learning for Dynamic System Identification. (arXiv:2107.12910v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2107.12910</id>
        <link href="http://arxiv.org/abs/2107.12910"/>
        <updated>2021-07-28T02:02:34.086Z</updated>
        <summary type="html"><![CDATA[This paper proposes a sparse Bayesian treatment of deep neural networks
(DNNs) for system identification. Although DNNs show impressive approximation
ability in various fields, several challenges still exist for system
identification problems. First, DNNs are known to be too complex that they can
easily overfit the training data. Second, the selection of the input regressors
for system identification is nontrivial. Third, uncertainty quantification of
the model parameters and predictions are necessary. The proposed Bayesian
approach offers a principled way to alleviate the above challenges by marginal
likelihood/model evidence approximation and structured group sparsity-inducing
priors construction. The identification algorithm is derived as an iterative
regularized optimization procedure that can be solved as efficiently as
training typical DNNs. Furthermore, a practical calculation approach based on
the Monte-Carlo integration method is derived to quantify the uncertainty of
the parameters and predictions. The effectiveness of the proposed Bayesian
approach is demonstrated on several linear and nonlinear systems identification
benchmarks with achieving good and competitive simulation accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hongpeng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ibrahim_C/0/1/0/all/0/1"&gt;Chahine Ibrahim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zheng_W/0/1/0/all/0/1"&gt;Wei Xing Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pan_W/0/1/0/all/0/1"&gt;Wei Pan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Physiologically-adapted Gold Standard for Arousal During a Stress Induced Scenario. (arXiv:2107.12964v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12964</id>
        <link href="http://arxiv.org/abs/2107.12964"/>
        <updated>2021-07-28T02:02:34.080Z</updated>
        <summary type="html"><![CDATA[Emotion is an inherently subjective psychophysiological human-state and to
produce an agreed-upon representation (gold standard) for continuous emotion
requires a time-consuming and costly training procedure of multiple human
annotators. There is strong evidence in the literature that physiological
signals are sufficient objective markers for states of emotion, particularly
arousal. In this contribution, we utilise a dataset which includes continuous
emotion and physiological signals - Heartbeats per Minute (BPM), Electrodermal
Activity (EDA), and Respiration-rate - captured during a stress induced
scenario (Trier Social Stress Test). We utilise a Long Short-Term Memory,
Recurrent Neural Network to explore the benefit of fusing these physiological
signals with arousal as the target, learning from various audio, video, and
textual based features. We utilise the state-of-the-art MuSe-Toolbox to
consider both annotation delay and inter-rater agreement weighting when fusing
the target signals. An improvement in Concordance Correlation Coefficient (CCC)
is seen across features sets when fusing EDA with arousal, compared to the
arousal only gold standard results. Additionally, BERT-based textual features'
results improved for arousal plus all physiological signals, obtaining up to
.3344 CCC compared to .2118 CCC for arousal only. Multimodal fusion also
improves overall CCC with audio plus video features obtaining up to .6157 CCC
to recognize arousal plus EDA and BPM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baird_A/0/1/0/all/0/1"&gt;Alice Baird&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stappen_L/0/1/0/all/0/1"&gt;Lukas Stappen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Christ_L/0/1/0/all/0/1"&gt;Lukas Christ&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schumann_L/0/1/0/all/0/1"&gt;Lea Schumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Messner_E/0/1/0/all/0/1"&gt;Eva-Maria Me&amp;#xdf;ner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn W. Schuller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Dimensional Distribution Generation Through Deep Neural Networks. (arXiv:2107.12466v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12466</id>
        <link href="http://arxiv.org/abs/2107.12466"/>
        <updated>2021-07-28T02:02:34.062Z</updated>
        <summary type="html"><![CDATA[We show that every $d$-dimensional probability distribution of bounded
support can be generated through deep ReLU networks out of a $1$-dimensional
uniform input distribution. What is more, this is possible without incurring a
cost - in terms of approximation error measured in Wasserstein-distance -
relative to generating the $d$-dimensional target distribution from $d$
independent random variables. This is enabled by a vast generalization of the
space-filling approach discovered in (Bailey & Telgarsky, 2018). The
construction we propose elicits the importance of network depth in driving the
Wasserstein distance between the target distribution and its neural network
approximation to zero. Finally, we find that, for histogram target
distributions, the number of bits needed to encode the corresponding generative
network equals the fundamental limit for encoding probability distributions as
dictated by quantization theory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perekrestenko_D/0/1/0/all/0/1"&gt;Dmytro Perekrestenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eberhard_L/0/1/0/all/0/1"&gt;L&amp;#xe9;andre Eberhard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bolcskei_H/0/1/0/all/0/1"&gt;Helmut B&amp;#xf6;lcskei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Optimisation for Sequential Experimental Design with Applications in Additive Manufacturing. (arXiv:2107.12809v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12809</id>
        <link href="http://arxiv.org/abs/2107.12809"/>
        <updated>2021-07-28T02:02:34.055Z</updated>
        <summary type="html"><![CDATA[Bayesian optimization (BO) is an approach to globally optimizing black-box
objective functions that are expensive to evaluate. BO-powered experimental
design has found wide application in materials science, chemistry, experimental
physics, drug development, etc. This work aims to bring attention to the
benefits of applying BO in designing experiments and to provide a BO manual,
covering both methodology and software, for the convenience of anyone who wants
to apply or learn BO. In particular, we briefly explain the BO technique,
review all the applications of BO in additive manufacturing, compare and
exemplify the features of different open BO libraries, unlock new potential
applications of BO to other types of data (e.g., preferential output). This
article is aimed at readers with some understanding of Bayesian methods, but
not necessarily with knowledge of additive manufacturing; the software
performance overview and implementation instructions are instrumental for any
experimental-design practitioner. Moreover, our review in the field of additive
manufacturing highlights the current knowledge and technological trends of BO.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mimi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parnell_A/0/1/0/all/0/1"&gt;Andrew Parnell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brabazon_D/0/1/0/all/0/1"&gt;Dermot Brabazon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benavoli_A/0/1/0/all/0/1"&gt;Alessio Benavoli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The social dilemma in AI development and why we have to solve it. (arXiv:2107.12977v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.12977</id>
        <link href="http://arxiv.org/abs/2107.12977"/>
        <updated>2021-07-28T02:02:34.046Z</updated>
        <summary type="html"><![CDATA[While the demand for ethical artificial intelligence (AI) systems increases,
the number of unethical uses of AI accelerates, even though there is no
shortage of ethical guidelines. We argue that a main underlying cause for this
is that AI developers face a social dilemma in AI development ethics,
preventing the widespread adaptation of ethical best practices. We define the
social dilemma for AI development and describe why the current crisis in AI
development ethics cannot be solved without relieving AI developers of their
social dilemma. We argue that AI development must be professionalised to
overcome the social dilemma, and discuss how medicine can be used as a template
in this process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Strumke_I/0/1/0/all/0/1"&gt;Inga Str&amp;#xfc;mke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slavkovik_M/0/1/0/all/0/1"&gt;Marija Slavkovik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madai_V/0/1/0/all/0/1"&gt;Vince Madai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Autoencoders for Embedding Learning in Brain Networks and Major Depressive Disorder Identification. (arXiv:2107.12838v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2107.12838</id>
        <link href="http://arxiv.org/abs/2107.12838"/>
        <updated>2021-07-28T02:02:34.021Z</updated>
        <summary type="html"><![CDATA[Brain functional connectivity (FC) reveals biomarkers for identification of
various neuropsychiatric disorders. Recent application of deep neural networks
(DNNs) to connectome-based classification mostly relies on traditional
convolutional neural networks using input connectivity matrices on a regular
Euclidean grid. We propose a graph deep learning framework to incorporate the
non-Euclidean information about graph structure for classifying functional
magnetic resonance imaging (fMRI)- derived brain networks in major depressive
disorder (MDD). We design a novel graph autoencoder (GAE) architecture based on
the graph convolutional networks (GCNs) to embed the topological structure and
node content of large-sized fMRI networks into low-dimensional latent
representations. In network construction, we employ the Ledoit-Wolf (LDW)
shrinkage method to estimate the high-dimensional FC metrics efficiently from
fMRI data. We consider both supervised and unsupervised approaches for the
graph embedded learning. The learned embeddings are then used as feature inputs
for a deep fully-connected neural network (FCNN) to discriminate MDD from
healthy controls. Evaluated on a resting-state fMRI MDD dataset with 43
subjects, results show that the proposed GAE-FCNN model significantly
outperforms several state-of-the-art DNN methods for brain connectome
classification, achieving accuracy of 72.50% using the LDW-FC metrics as node
features. The graph embeddings of fMRI FC networks learned by the GAE also
reveal apparent group differences between MDD and HC. Our new framework
demonstrates feasibility of learning graph embeddings on brain networks to
provide discriminative information for diagnosis of brain disorders.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Noman_F/0/1/0/all/0/1"&gt;Fuad Noman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ting_C/0/1/0/all/0/1"&gt;Chee-Ming Ting&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kang_H/0/1/0/all/0/1"&gt;Hakmook Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Phan_R/0/1/0/all/0/1"&gt;Raphael C.-W. Phan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Boyd_B/0/1/0/all/0/1"&gt;Brian D. Boyd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Taylor_W/0/1/0/all/0/1"&gt;Warren D. Taylor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ombao_H/0/1/0/all/0/1"&gt;Hernando Ombao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved-Mask R-CNN: Towards an Accurate Generic MSK MRI instance segmentation platform (Data from the Osteoarthritis Initiative). (arXiv:2107.12889v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.12889</id>
        <link href="http://arxiv.org/abs/2107.12889"/>
        <updated>2021-07-28T02:02:34.013Z</updated>
        <summary type="html"><![CDATA[Objective assessment of Magnetic Resonance Imaging (MRI) scans of
osteoarthritis (OA) can address the limitation of the current OA assessment.
Segmentation of bone, cartilage, and joint fluid is necessary for the OA
objective assessment. Most of the proposed segmentation methods are not
performing instance segmentation and suffer from class imbalance problems. This
study deployed Mask R-CNN instance segmentation and improved it (improved-Mask
R-CNN (iMaskRCNN)) to obtain a more accurate generalized segmentation for
OA-associated tissues. Training and validation of the method were performed
using 500 MRI knees from the Osteoarthritis Initiative (OAI) dataset and 97 MRI
scans of patients with symptomatic hip OA. Three modifications to Mask R-CNN
yielded the iMaskRCNN: adding a 2nd ROIAligned block, adding an extra decoder
layer to the mask-header, and connecting them by a skip connection. The results
were assessed using Hausdorff distance, dice score, and coefficients of
variation (CoV). The iMaskRCNN led to improved bone and cartilage segmentation
compared to Mask RCNN as indicated with the increase in dice score from 95% to
98% for the femur, 95% to 97% for tibia, 71% to 80% for femoral cartilage, and
81% to 82% for tibial cartilage. For the effusion detection, dice improved with
iMaskRCNN 72% versus MaskRCNN 71%. The CoV values for effusion detection
between Reader1 and Mask R-CNN (0.33), Reader1 and iMaskRCNN (0.34), Reader2
and Mask R-CNN (0.22), Reader2 and iMaskRCNN (0.29) are close to CoV between
two readers (0.21), indicating a high agreement between the human readers and
both Mask R-CNN and iMaskRCNN. Mask R-CNN and iMaskRCNN can reliably and
simultaneously extract different scale articular tissues involved in OA,
forming the foundation for automated assessment of OA. The iMaskRCNN results
show that the modification improved the network performance around the edges.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Felfeliyan_B/0/1/0/all/0/1"&gt;Banafshe Felfeliyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hareendranathan_A/0/1/0/all/0/1"&gt;Abhilash Hareendranathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kuntze_G/0/1/0/all/0/1"&gt;Gregor Kuntze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jaremko_J/0/1/0/all/0/1"&gt;Jacob L. Jaremko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ronsky_J/0/1/0/all/0/1"&gt;Janet L. Ronsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Statistical Analysis of Summarization Evaluation Metrics using Resampling Methods. (arXiv:2104.00054v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00054</id>
        <link href="http://arxiv.org/abs/2104.00054"/>
        <updated>2021-07-28T02:02:33.975Z</updated>
        <summary type="html"><![CDATA[The quality of a summarization evaluation metric is quantified by calculating
the correlation between its scores and human annotations across a large number
of summaries. Currently, it is unclear how precise these correlation estimates
are, nor whether differences between two metrics' correlations reflect a true
difference or if it is due to mere chance. In this work, we address these two
problems by proposing methods for calculating confidence intervals and running
hypothesis tests for correlations using two resampling methods, bootstrapping
and permutation. After evaluating which of the proposed methods is most
appropriate for summarization through two simulation experiments, we analyze
the results of applying these methods to several different automatic evaluation
metrics across three sets of human annotations. We find that the confidence
intervals are rather wide, demonstrating high uncertainty in the reliability of
automatic metrics. Further, although many metrics fail to show statistical
improvements over ROUGE, two recent works, QAEval and BERTScore, do in some
evaluation settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deutsch_D/0/1/0/all/0/1"&gt;Daniel Deutsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dror_R/0/1/0/all/0/1"&gt;Rotem Dror&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1"&gt;Dan Roth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Network Branch-and-Bound for Neural Network Verification. (arXiv:2107.12855v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12855</id>
        <link href="http://arxiv.org/abs/2107.12855"/>
        <updated>2021-07-28T02:02:33.965Z</updated>
        <summary type="html"><![CDATA[Many available formal verification methods have been shown to be instances of
a unified Branch-and-Bound (BaB) formulation. We propose a novel machine
learning framework that can be used for designing an effective branching
strategy as well as for computing better lower bounds. Specifically, we learn
two graph neural networks (GNN) that both directly treat the network we want to
verify as a graph input and perform forward-backward passes through the GNN
layers. We use one GNN to simulate the strong branching heuristic behaviour and
another to compute a feasible dual solution of the convex relaxation, thereby
providing a valid lower bound.

We provide a new verification dataset that is more challenging than those
used in the literature, thereby providing an effective alternative for testing
algorithmic improvements for verification. Whilst using just one of the GNNs
leads to a reduction in verification time, we get optimal performance when
combining the two GNN approaches. Our combined framework achieves a 50\%
reduction in both the number of branches and the time required for verification
on various convolutional networks when compared to several state-of-the-art
verification methods. In addition, we show that our GNN models generalize well
to harder properties on larger unseen networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jaeckle_F/0/1/0/all/0/1"&gt;Florian Jaeckle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jingyue Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1"&gt;M. Pawan Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coarse to Fine: Domain Adaptive Crowd Counting via Adversarial Scoring Network. (arXiv:2107.12858v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12858</id>
        <link href="http://arxiv.org/abs/2107.12858"/>
        <updated>2021-07-28T02:02:33.959Z</updated>
        <summary type="html"><![CDATA[Recent deep networks have convincingly demonstrated high capability in crowd
counting, which is a critical task attracting widespread attention due to its
various industrial applications. Despite such progress, trained data-dependent
models usually can not generalize well to unseen scenarios because of the
inherent domain shift. To facilitate this issue, this paper proposes a novel
adversarial scoring network (ASNet) to gradually bridge the gap across domains
from coarse to fine granularity. In specific, at the coarse-grained stage, we
design a dual-discriminator strategy to adapt source domain to be close to the
targets from the perspectives of both global and local feature space via
adversarial learning. The distributions between two domains can thus be aligned
roughly. At the fine-grained stage, we explore the transferability of source
characteristics by scoring how similar the source samples are to target ones
from multiple levels based on generative probability derived from coarse stage.
Guided by these hierarchical scores, the transferable source features are
properly selected to enhance the knowledge transfer during the adaptation
process. With the coarse-to-fine design, the generalization bottleneck induced
from the domain discrepancy can be effectively alleviated. Three sets of
migration experiments show that the proposed methods achieve state-of-the-art
counting performance compared with major unsupervised methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1"&gt;Zhikang Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1"&gt;Xiaoye Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1"&gt;Pan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shuangjie Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xiaoqing Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wenhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jin Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Stacked Auto-Encoders for Fair Representation Learning. (arXiv:2107.12826v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12826</id>
        <link href="http://arxiv.org/abs/2107.12826"/>
        <updated>2021-07-28T02:02:33.952Z</updated>
        <summary type="html"><![CDATA[Training machine learning models with the only accuracy as a final goal may
promote prejudices and discriminatory behaviors embedded in the data. One
solution is to learn latent representations that fulfill specific fairness
metrics. Different types of learning methods are employed to map data into the
fair representational space. The main purpose is to learn a latent
representation of data that scores well on a fairness metric while maintaining
the usability for the downstream task. In this paper, we propose a new fair
representation learning approach that leverages different levels of
representation of data to tighten the fairness bounds of the learned
representation. Our results show that stacking different auto-encoders and
enforcing fairness at different latent spaces result in an improvement of
fairness compared to other existing approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kenfack_P/0/1/0/all/0/1"&gt;Patrik Joslin Kenfack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1"&gt;Adil Mehmood Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hussain_R/0/1/0/all/0/1"&gt;Rasheed Hussain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kazmi_S/0/1/0/all/0/1"&gt;S.M. Ahsan Kazmi&lt;/a&gt;,</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wasserstein-Splitting Gaussian Process Regression for Heterogeneous Online Bayesian Inference. (arXiv:2107.12797v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.12797</id>
        <link href="http://arxiv.org/abs/2107.12797"/>
        <updated>2021-07-28T02:02:33.936Z</updated>
        <summary type="html"><![CDATA[Gaussian processes (GPs) are a well-known nonparametric Bayesian inference
technique, but they suffer from scalability problems for large sample sizes,
and their performance can degrade for non-stationary or spatially heterogeneous
data. In this work, we seek to overcome these issues through (i) employing
variational free energy approximations of GPs operating in tandem with online
expectation propagation steps; and (ii) introducing a local splitting step
which instantiates a new GP whenever the posterior distribution changes
significantly as quantified by the Wasserstein metric over posterior
distributions. Over time, then, this yields an ensemble of sparse GPs which may
be updated incrementally, and adapts to locality, heterogeneity, and
non-stationarity in training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kepler_M/0/1/0/all/0/1"&gt;Michael E. Kepler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Koppel_A/0/1/0/all/0/1"&gt;Alec Koppel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bedi_A/0/1/0/all/0/1"&gt;Amrit Singh Bedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Stilwell_D/0/1/0/all/0/1"&gt;Daniel J. Stilwell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Deep Anomaly Detection for Multi-Sensor Time-Series Signals. (arXiv:2107.12626v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.12626</id>
        <link href="http://arxiv.org/abs/2107.12626"/>
        <updated>2021-07-28T02:02:33.901Z</updated>
        <summary type="html"><![CDATA[Nowadays, multi-sensor technologies are applied in many fields, e.g., Health
Care (HC), Human Activity Recognition (HAR), and Industrial Control System
(ICS). These sensors can generate a substantial amount of multivariate
time-series data. Unsupervised anomaly detection on multi-sensor time-series
data has been proven critical in machine learning researches. The key challenge
is to discover generalized normal patterns by capturing spatial-temporal
correlation in multi-sensor data. Beyond this challenge, the noisy data is
often intertwined with the training data, which is likely to mislead the model
by making it hard to distinguish between the normal, abnormal, and noisy data.
Few of previous researches can jointly address these two challenges. In this
paper, we propose a novel deep learning-based anomaly detection algorithm
called Deep Convolutional Autoencoding Memory network (CAE-M). We first build a
Deep Convolutional Autoencoder to characterize spatial dependence of
multi-sensor data with a Maximum Mean Discrepancy (MMD) to better distinguish
between the noisy, normal, and abnormal data. Then, we construct a Memory
Network consisting of linear (Autoregressive Model) and non-linear predictions
(Bidirectional LSTM with Attention) to capture temporal dependence from
time-series data. Finally, CAE-M jointly optimizes these two subnetworks. We
empirically compare the proposed approach with several state-of-the-art anomaly
detection methods on HAR and HC datasets. Experimental results demonstrate that
our proposed model outperforms these existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuxin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiqiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jindong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1"&gt;Zhiwen Pan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Individual Survival Curves with Conditional Normalizing Flows. (arXiv:2107.12825v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12825</id>
        <link href="http://arxiv.org/abs/2107.12825"/>
        <updated>2021-07-28T02:02:33.893Z</updated>
        <summary type="html"><![CDATA[Survival analysis, or time-to-event modelling, is a classical statistical
problem that has garnered a lot of interest for its practical use in
epidemiology, demographics or actuarial sciences. Recent advances on the
subject from the point of view of machine learning have been concerned with
precise per-individual predictions instead of population studies, driven by the
rise of individualized medicine. We introduce here a conditional normalizing
flow based estimate of the time-to-event density as a way to model highly
flexible and individualized conditional survival distributions. We use a novel
hierarchical formulation of normalizing flows to enable efficient fitting of
flexible conditional distributions without overfitting and show how the
normalizing flow formulation can be efficiently adapted to the censored
setting. We experimentally validate the proposed approach on a synthetic
dataset as well as four open medical datasets and an example of a common
financial problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ausset_G/0/1/0/all/0/1"&gt;Guillaume Ausset&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciffreo_T/0/1/0/all/0/1"&gt;Tom Ciffreo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Portier_F/0/1/0/all/0/1"&gt;Francois Portier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clemencon_S/0/1/0/all/0/1"&gt;Stephan Cl&amp;#xe9;men&amp;#xe7;on&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papin_T/0/1/0/all/0/1"&gt;Timoth&amp;#xe9;e Papin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving ClusterGAN Using Self-AugmentedInformation Maximization of Disentangling LatentSpaces. (arXiv:2107.12706v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12706</id>
        <link href="http://arxiv.org/abs/2107.12706"/>
        <updated>2021-07-28T02:02:33.823Z</updated>
        <summary type="html"><![CDATA[The Latent Space Clustering in Generative adversarial networks (ClusterGAN)
method has been successful with high-dimensional data. However, the method
assumes uniformlydistributed priors during the generation of modes, which isa
restrictive assumption in real-world data and cause loss ofdiversity in the
generated modes. In this paper, we proposeself-augmentation information
maximization improved Clus-terGAN (SIMI-ClusterGAN) to learn the distinctive
priorsfrom the data. The proposed SIMI-ClusterGAN consists offour deep neural
networks: self-augmentation prior network,generator, discriminator and
clustering inference autoencoder.The proposed method has been validated using
seven bench-mark data sets and has shown improved performance overstate-of-the
art methods. To demonstrate the superiority ofSIMI-ClusterGAN performance on
imbalanced dataset, wehave discussed two imbalanced conditions on MNIST
datasetswith one-class imbalance and three classes imbalanced cases.The results
highlight the advantages of SIMI-ClusterGAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dam_T/0/1/0/all/0/1"&gt;Tanmoy Dam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anavatti_S/0/1/0/all/0/1"&gt;Sreenatha G. Anavatti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbass_H/0/1/0/all/0/1"&gt;Hussein A. Abbass&lt;/a&gt; (Fellow, IEEESchool of Engineering and Information Technology, University of New South Wales Canberra, Australia)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convergence of Deep ReLU Networks. (arXiv:2107.12530v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12530</id>
        <link href="http://arxiv.org/abs/2107.12530"/>
        <updated>2021-07-28T02:02:33.802Z</updated>
        <summary type="html"><![CDATA[We explore convergence of deep neural networks with the popular ReLU
activation function, as the depth of the networks tends to infinity. To this
end, we introduce the notion of activation domains and activation matrices of a
ReLU network. By replacing applications of the ReLU activation function by
multiplications with activation matrices on activation domains, we obtain an
explicit expression of the ReLU network. We then identify the convergence of
the ReLU networks as convergence of a class of infinite products of matrices.
Sufficient and necessary conditions for convergence of these infinite products
of matrices are studied. As a result, we establish necessary conditions for
ReLU networks to converge that the sequence of weight matrices converges to the
identity matrix and the sequence of the bias vectors converges to zero as the
depth of ReLU networks increases to infinity. Moreover, we obtain sufficient
conditions in terms of the weight matrices and bias vectors at hidden layers
for pointwise convergence of deep ReLU networks. These results provide
mathematical insights to the design strategy of the well-known deep residual
networks in image classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yuesheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haizhang Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Low-Latency Energy-Efficient Deep SNNs via Attention-Guided Compression. (arXiv:2107.12445v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2107.12445</id>
        <link href="http://arxiv.org/abs/2107.12445"/>
        <updated>2021-07-28T02:02:33.598Z</updated>
        <summary type="html"><![CDATA[Deep spiking neural networks (SNNs) have emerged as a potential alternative
to traditional deep learning frameworks, due to their promise to provide
increased compute efficiency on event-driven neuromorphic hardware. However, to
perform well on complex vision applications, most SNN training frameworks yield
large inference latency which translates to increased spike activity and
reduced energy efficiency. Hence,minimizing average spike activity while
preserving accuracy indeep SNNs remains a significant challenge and
opportunity.This paper presents a non-iterative SNN training technique
thatachieves ultra-high compression with reduced spiking activitywhile
maintaining high inference accuracy. In particular, our framework first uses
the attention-maps of an un compressed meta-model to yield compressed ANNs.
This step can be tuned to support both irregular and structured channel pruning
to leverage computational benefits over a broad range of platforms. The
framework then performs sparse-learning-based supervised SNN training using
direct inputs. During the training, it jointly optimizes the SNN weight,
threshold, and leak parameters to drastically minimize the number of time steps
required while retaining compression. To evaluate the merits of our approach,
we performed experiments with variants of VGG and ResNet, on both CIFAR-10 and
CIFAR-100, and VGG16 on Tiny-ImageNet.The SNN models generated through the
proposed technique yield SOTA compression ratios of up to 33.4x with no
significant drops in accuracy compared to baseline unpruned counterparts.
Compared to existing SNN pruning methods, we achieve up to 8.3x higher
compression with improved accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1"&gt;Souvik Kundu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Datta_G/0/1/0/all/0/1"&gt;Gourav Datta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedram_M/0/1/0/all/0/1"&gt;Massoud Pedram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beerel_P/0/1/0/all/0/1"&gt;Peter A. Beerel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Sequence Feature Alignment for Domain Adaptive Detection Transformers. (arXiv:2107.12636v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12636</id>
        <link href="http://arxiv.org/abs/2107.12636"/>
        <updated>2021-07-28T02:02:33.591Z</updated>
        <summary type="html"><![CDATA[Detection transformers have recently shown promising object detection results
and attracted increasing attention. However, how to develop effective domain
adaptation techniques to improve its cross-domain performance remains
unexplored and unclear. In this paper, we delve into this topic and empirically
find that direct feature distribution alignment on the CNN backbone only brings
limited improvements, as it does not guarantee domain-invariant sequence
features in the transformer for prediction. To address this issue, we propose a
novel Sequence Feature Alignment (SFA) method that is specially designed for
the adaptation of detection transformers. Technically, SFA consists of a domain
query-based feature alignment (DQFA) module and a token-wise feature alignment
(TDA) module. In DQFA, a novel domain query is used to aggregate and align
global context from the token sequence of both domains. DQFA reduces the domain
discrepancy in global feature representations and object relations when
deploying in the transformer encoder and decoder, respectively. Meanwhile, TDA
aligns token features in the sequence from both domains, which reduces the
domain gaps in local and instance-level feature representations in the
transformer encoder and decoder, respectively. Besides, a novel bipartite
matching consistency loss is proposed to enhance the feature discriminability
for robust object detection. Experiments on three challenging benchmarks show
that SFA outperforms state-of-the-art domain adaptive object detection methods.
Code has been made available at: https://github.com/encounter1997/SFA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1"&gt;Fengxiang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zheng-Jun Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1"&gt;Yonggang Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finding Failures in High-Fidelity Simulation using Adaptive Stress Testing and the Backward Algorithm. (arXiv:2107.12940v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12940</id>
        <link href="http://arxiv.org/abs/2107.12940"/>
        <updated>2021-07-28T02:02:33.548Z</updated>
        <summary type="html"><![CDATA[Validating the safety of autonomous systems generally requires the use of
high-fidelity simulators that adequately capture the variability of real-world
scenarios. However, it is generally not feasible to exhaustively search the
space of simulation scenarios for failures. Adaptive stress testing (AST) is a
method that uses reinforcement learning to find the most likely failure of a
system. AST with a deep reinforcement learning solver has been shown to be
effective in finding failures across a range of different systems. This
approach generally involves running many simulations, which can be very
expensive when using a high-fidelity simulator. To improve efficiency, we
present a method that first finds failures in a low-fidelity simulator. It then
uses the backward algorithm, which trains a deep neural network policy using a
single expert demonstration, to adapt the low-fidelity failures to
high-fidelity. We have created a series of autonomous vehicle validation case
studies that represent some of the ways low-fidelity and high-fidelity
simulators can differ, such as time discretization. We demonstrate in a variety
of case studies that this new AST approach is able to find failures with
significantly fewer high-fidelity simulation steps than are needed when just
running AST directly in high-fidelity. As a proof of concept, we also
demonstrate AST on NVIDIA's DriveSim simulator, an industry state-of-the-art
high-fidelity simulator for finding failures in autonomous vehicles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koren_M/0/1/0/all/0/1"&gt;Mark Koren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nassar_A/0/1/0/all/0/1"&gt;Ahmed Nassar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1"&gt;Mykel J. Kochenderfer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DV-Det: Efficient 3D Point Cloud Object Detection with Dynamic Voxelization. (arXiv:2107.12707v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12707</id>
        <link href="http://arxiv.org/abs/2107.12707"/>
        <updated>2021-07-28T02:02:33.541Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a novel two-stage framework for the efficient 3D
point cloud object detection. Instead of transforming point clouds into 2D bird
eye view projections, we parse the raw point cloud data directly in the 3D
space yet achieve impressive efficiency and accuracy. To achieve this goal, we
propose dynamic voxelization, a method that voxellizes points at local scale
on-the-fly. By doing so, we preserve the point cloud geometry with 3D voxels,
and therefore waive the dependence on expensive MLPs to learn from point
coordinates. On the other hand, we inherently still follow the same processing
pattern as point-wise methods (e.g., PointNet) and no longer suffer from the
quantization issue like conventional convolutions. For further speed
optimization, we propose the grid-based downsampling and voxelization method,
and provide different CUDA implementations to accommodate to the discrepant
requirements during training and inference phases. We highlight our efficiency
on KITTI 3D object detection dataset with 75 FPS and on Waymo Open dataset with
25 FPS inference speed with satisfactory accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1"&gt;Zhaoyu Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1"&gt;Pin Siang Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu-Hsing Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probing neural networks with t-SNE, class-specific projections and a guided tour. (arXiv:2107.12547v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12547</id>
        <link href="http://arxiv.org/abs/2107.12547"/>
        <updated>2021-07-28T02:02:33.516Z</updated>
        <summary type="html"><![CDATA[We use graphical methods to probe neural nets that classify images. Plots of
t-SNE outputs at successive layers in a network reveal increasingly organized
arrangement of the data points. They can also reveal how a network can diminish
or even forget about within-class structure as the data proceeds through
layers. We use class-specific analogues of principal components to visualize
how succeeding layers separate the classes. These allow us to sort images from
a given class from most typical to least typical (in the data) and they also
serve as very useful projection coordinates for data visualization. We find
them especially useful when defining versions guided tours for animated data
visualization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hoyt_C/0/1/0/all/0/1"&gt;Christopher R. Hoyt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Owen_A/0/1/0/all/0/1"&gt;Art B. Owen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Energy-Based Open-World Uncertainty Modeling for Confidence Calibration. (arXiv:2107.12628v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12628</id>
        <link href="http://arxiv.org/abs/2107.12628"/>
        <updated>2021-07-28T02:02:33.502Z</updated>
        <summary type="html"><![CDATA[Confidence calibration is of great importance to the reliability of decisions
made by machine learning systems. However, discriminative classifiers based on
deep neural networks are often criticized for producing overconfident
predictions that fail to reflect the true correctness likelihood of
classification accuracy. We argue that such an inability to model uncertainty
is mainly caused by the closed-world nature in softmax: a model trained by the
cross-entropy loss will be forced to classify input into one of $K$ pre-defined
categories with high probability. To address this problem, we for the first
time propose a novel $K$+1-way softmax formulation, which incorporates the
modeling of open-world uncertainty as the extra dimension. To unify the
learning of the original $K$-way classification task and the extra dimension
that models uncertainty, we propose a novel energy-based objective function,
and moreover, theoretically prove that optimizing such an objective essentially
forces the extra dimension to capture the marginal data distribution. Extensive
experiments show that our approach, Energy-based Open-World Softmax
(EOW-Softmax), is superior to existing state-of-the-art methods in improving
confidence calibration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yezhen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Che_T/0/1/0/all/0/1"&gt;Tong Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1"&gt;Kaiyang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dongsheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziwei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uniformity in Heterogeneity:Diving Deep into Count Interval Partition for Crowd Counting. (arXiv:2107.12619v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12619</id>
        <link href="http://arxiv.org/abs/2107.12619"/>
        <updated>2021-07-28T02:02:33.499Z</updated>
        <summary type="html"><![CDATA[Recently, the problem of inaccurate learning targets in crowd counting draws
increasing attention. Inspired by a few pioneering work, we solve this problem
by trying to predict the indices of pre-defined interval bins of counts instead
of the count values themselves. However, an inappropriate interval setting
might make the count error contributions from different intervals extremely
imbalanced, leading to inferior counting performance. Therefore, we propose a
novel count interval partition criterion called Uniform Error Partition (UEP),
which always keeps the expected counting error contributions equal for all
intervals to minimize the prediction risk. Then to mitigate the inevitably
introduced discretization errors in the count quantization process, we propose
another criterion called Mean Count Proxies (MCP). The MCP criterion selects
the best count proxy for each interval to represent its count value during
inference, making the overall expected discretization error of an image nearly
negligible. As far as we are aware, this work is the first to delve into such a
classification task and ends up with a promising solution for count interval
partition. Following the above two theoretically demonstrated criterions, we
propose a simple yet effective model termed Uniform Error Partition Network
(UEPNet), which achieves state-of-the-art performance on several challenging
datasets. The codes will be available at:
https://github.com/TencentYoutuResearch/CrowdCounting-UEPNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Changan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1"&gt;Qingyu Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Boshen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yabiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1"&gt;Ying Tai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xuyi Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengjie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jilin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jiayi Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yang Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparing Prophet and Deep Learning to ARIMA in Forecasting Wholesale Food Prices. (arXiv:2107.12770v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12770</id>
        <link href="http://arxiv.org/abs/2107.12770"/>
        <updated>2021-07-28T02:02:33.496Z</updated>
        <summary type="html"><![CDATA[Setting sale prices correctly is of great importance for firms, and the study
and forecast of prices time series is therefore a relevant topic not only from
a data science perspective but also from an economic and applicative one. In
this paper we exhamine different techniques to forecast the sale prices of
three food products applied by an Italian food wholesaler, as a step towards
the automation of pricing tasks usually taken care by human workforce. We
consider ARIMA models and compare them to Prophet, a scalable forecasting tool
developed by Facebook and based on a generalized additive model, and to deep
learning models based on Long Short--Term Memory (LSTM) and Convolutional
Neural Networks (CNNs). ARIMA models are frequently used in econometric
analyses, providing a good bechmark for the problem under study. Our results
indicate that ARIMA performs similarly to LSTM neural networks for the problem
under study, while the combination of CNNs and LSTMs attains the best overall
accuracy, but requires more time to be tuned. On the contrary, Prophet is very
fast to use, but less accurate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Menculini_L/0/1/0/all/0/1"&gt;Lorenzo Menculini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marini_A/0/1/0/all/0/1"&gt;Andrea Marini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Proietti_M/0/1/0/all/0/1"&gt;Massimiliano Proietti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garinei_A/0/1/0/all/0/1"&gt;Alberto Garinei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bozza_A/0/1/0/all/0/1"&gt;Alessio Bozza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moretti_C/0/1/0/all/0/1"&gt;Cecilia Moretti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marconi_M/0/1/0/all/0/1"&gt;Marcello Marconi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-Enforced Modeling for Insertion Loss of Transmission Lines by Deep Neural Networks. (arXiv:2107.12527v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12527</id>
        <link href="http://arxiv.org/abs/2107.12527"/>
        <updated>2021-07-28T02:02:33.494Z</updated>
        <summary type="html"><![CDATA[In this paper, we investigate data-driven parameterized modeling of insertion
loss for transmission lines with respect to design parameters. We first show
that direct application of neural networks can lead to non-physics models with
negative insertion loss. To mitigate this problem, we propose two deep learning
solutions. One solution is to add a regulation term, which represents the
passive condition, to the final loss function to enforce the negative quantity
of insertion loss. In the second method, a third-order polynomial expression is
defined first, which ensures positiveness, to approximate the insertion loss,
then DeepONet neural network structure, which was proposed recently for
function and system modeling, was employed to model the coefficients of
polynomials. The resulting neural network is applied to predict the
coefficients of the polynomial expression. The experimental results on an
open-sourced SI/PI database of a PCB design show that both methods can ensure
the positiveness for the insertion loss. Furthermore, both methods can achieve
similar prediction results, while the polynomial-based DeepONet method is
faster than DeepONet based method in training time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1"&gt;Lesley Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pointer Value Retrieval: A new benchmark for understanding the limits of neural network generalization. (arXiv:2107.12580v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12580</id>
        <link href="http://arxiv.org/abs/2107.12580"/>
        <updated>2021-07-28T02:02:33.449Z</updated>
        <summary type="html"><![CDATA[The successes of deep learning critically rely on the ability of neural
networks to output meaningful predictions on unseen data -- generalization. Yet
despite its criticality, there remain fundamental open questions on how neural
networks generalize. How much do neural networks rely on memorization -- seeing
highly similar training examples -- and how much are they capable of
human-intelligence styled reasoning -- identifying abstract rules underlying
the data? In this paper we introduce a novel benchmark, Pointer Value Retrieval
(PVR) tasks, that explore the limits of neural network generalization. While
PVR tasks can consist of visual as well as symbolic inputs, each with varying
levels of difficulty, they all have a simple underlying rule. One part of the
PVR task input acts as a pointer, giving the location of a different part of
the input, which forms the value (and output). We demonstrate that this task
structure provides a rich testbed for understanding generalization, with our
empirical study showing large variations in neural network performance based on
dataset size, task complexity and model architecture. The interaction of
position, values and the pointer rule also allow the development of nuanced
tests of generalization, by introducing distribution shift and increasing
functional complexity. These reveal both subtle failures and surprising
successes, suggesting many promising directions of exploration on this
benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chiyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raghu_M/0/1/0/all/0/1"&gt;Maithra Raghu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kleinberg_J/0/1/0/all/0/1"&gt;Jon Kleinberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_S/0/1/0/all/0/1"&gt;Samy Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ENHANCE (ENriching Health data by ANnotations of Crowd and Experts): A case study for skin lesion classification. (arXiv:2107.12734v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12734</id>
        <link href="http://arxiv.org/abs/2107.12734"/>
        <updated>2021-07-28T02:02:33.448Z</updated>
        <summary type="html"><![CDATA[We present ENHANCE, an open dataset with multiple annotations to complement
the existing ISIC and PH2 skin lesion classification datasets. This dataset
contains annotations of visual ABC (asymmetry, border, colour) features from
non-expert annotation sources: undergraduate students, crowd workers from
Amazon MTurk and classic image processing algorithms. In this paper we first
analyse the correlations between the annotations and the diagnostic label of
the lesion, as well as study the agreement between different annotation
sources. Overall we find weak correlations of non-expert annotations with the
diagnostic label, and low agreement between different annotation sources. We
then study multi-task learning (MTL) with the annotations as additional labels,
and show that non-expert annotations can improve (ensembles of)
state-of-the-art convolutional neural networks via MTL. We hope that our
dataset can be used in further research into multiple annotations and/or MTL.
All data and models are available on Github:
https://github.com/raumannsr/ENHANCE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raumanns_R/0/1/0/all/0/1"&gt;Ralf Raumanns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schouten_G/0/1/0/all/0/1"&gt;Gerard Schouten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joosten_M/0/1/0/all/0/1"&gt;Max Joosten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pluim_J/0/1/0/all/0/1"&gt;Josien P. W. Pluim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheplygina_V/0/1/0/all/0/1"&gt;Veronika Cheplygina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Self-Attentive Gated RNNs for Real-Time Speaker Separation. (arXiv:2106.13493v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13493</id>
        <link href="http://arxiv.org/abs/2106.13493"/>
        <updated>2021-07-28T02:02:33.412Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have recently shown great success in the task of blind
source separation, both under monaural and binaural settings. Although these
methods were shown to produce high-quality separations, they were mainly
applied under offline settings, in which the model has access to the full input
signal while separating the signal. In this study, we convert a non-causal
state-of-the-art separation model into a causal and real-time model and
evaluate its performance under both online and offline settings. We compare the
performance of the proposed model to several baseline methods under anechoic,
noisy, and noisy-reverberant recording conditions while exploring both monaural
and binaural inputs and outputs. Our findings shed light on the relative
difference between causal and non-causal models when performing separation. Our
stateful implementation for online separation leads to a minor drop in
performance compared to the offline model; 0.8dB for monaural inputs and 0.3dB
for binaural inputs while reaching a real-time factor of 0.65. Samples can be
found under the following link:
https://kwanum.github.io/sagrnnc-stream-results/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kabeli_O/0/1/0/all/0/1"&gt;Ori Kabeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Adi_Y/0/1/0/all/0/1"&gt;Yossi Adi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zhenyu Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_B/0/1/0/all/0/1"&gt;Buye Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Anurag Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning with Neural Tangent Kernels in Near Input Sparsity Time. (arXiv:2104.00415v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00415</id>
        <link href="http://arxiv.org/abs/2104.00415"/>
        <updated>2021-07-28T02:02:33.403Z</updated>
        <summary type="html"><![CDATA[The Neural Tangent Kernel (NTK) characterizes the behavior of infinitely wide
neural nets trained under least squares loss by gradient descent. However,
despite its importance, the super-quadratic runtime of kernel methods limits
the use of NTK in large-scale learning tasks. To accelerate kernel machines
with NTK, we propose a near input sparsity time algorithm that maps the input
data to a randomized low-dimensional feature space so that the inner product of
the transformed data approximates their NTK evaluation. Our transformation
works by sketching the polynomial expansions of arc-cosine kernels.
Furthermore, we propose a feature map for approximating the convolutional
counterpart of the NTK, which can transform any image using a runtime that is
only linear in the number of pixels. We show that in standard large-scale
regression and classification tasks a linear regressor trained on our features
outperforms trained Neural Nets and Nystrom approximation of NTK kernel.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zandieh_A/0/1/0/all/0/1"&gt;Amir Zandieh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PointBA: Towards Backdoor Attacks in 3D Point Cloud. (arXiv:2103.16074v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16074</id>
        <link href="http://arxiv.org/abs/2103.16074"/>
        <updated>2021-07-28T02:02:33.391Z</updated>
        <summary type="html"><![CDATA[3D deep learning has been increasingly more popular for a variety of tasks
including many safety-critical applications. However, recently several works
raise the security issues of 3D deep nets. Although most of these works
consider adversarial attacks, we identify that backdoor attack is indeed a more
serious threat to 3D deep learning systems but remains unexplored. We present
the backdoor attacks in 3D with a unified framework that exploits the unique
properties of 3D data and networks. In particular, we design two attack
approaches: the poison-label attack and the clean-label attack. The first one
is straightforward and effective in practice, while the second one is more
sophisticated assuming there are certain data inspections. The attack
algorithms are mainly motivated and developed by 1) the recent discovery of 3D
adversarial samples which demonstrate the vulnerability of 3D deep nets under
spatial transformations; 2) the proposed feature disentanglement technique that
manipulates the feature of the data through optimization methods and its
potential to embed a new task. Extensive experiments show the efficacy of the
poison-label attack with over 95% success rate across several 3D datasets and
models, and the ability of clean-label attack against data filtering with
around 50% success rate. Our proposed backdoor attack in 3D point cloud is
expected to perform as a baseline for improving the robustness of 3D deep
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xinke Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhirui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yue Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1"&gt;Zekun Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yabang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_A/0/1/0/all/0/1"&gt;Andrew Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Joey Tianyi Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constant Random Perturbations Provide Adversarial Robustness with Minimal Effect on Accuracy. (arXiv:2103.08265v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08265</id>
        <link href="http://arxiv.org/abs/2103.08265"/>
        <updated>2021-07-28T02:02:33.371Z</updated>
        <summary type="html"><![CDATA[This paper proposes an attack-independent (non-adversarial training)
technique for improving adversarial robustness of neural network models, with
minimal loss of standard accuracy. We suggest creating a neighborhood around
each training example, such that the label is kept constant for all inputs
within that neighborhood. Unlike previous work that follows a similar
principle, we apply this idea by extending the training set with multiple
perturbations for each training example, drawn from within the neighborhood.
These perturbations are model independent, and remain constant throughout the
entire training process. We analyzed our method empirically on MNIST, SVHN, and
CIFAR-10, under different attacks and conditions. Results suggest that the
proposed approach improves standard accuracy over other defenses while having
increased robustness compared to vanilla adversarial training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chernyak_B/0/1/0/all/0/1"&gt;Bronya Roni Chernyak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1"&gt;Bhiksha Raj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hazan_T/0/1/0/all/0/1"&gt;Tamir Hazan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keshet_J/0/1/0/all/0/1"&gt;Joseph Keshet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Co-Transport for Class-Incremental Learning. (arXiv:2107.12654v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12654</id>
        <link href="http://arxiv.org/abs/2107.12654"/>
        <updated>2021-07-28T02:02:33.338Z</updated>
        <summary type="html"><![CDATA[Traditional learning systems are trained in closed-world for a fixed number
of classes, and need pre-collected datasets in advance. However, new classes
often emerge in real-world applications and should be learned incrementally.
For example, in electronic commerce, new types of products appear daily, and in
a social media community, new topics emerge frequently. Under such
circumstances, incremental models should learn several new classes at a time
without forgetting. We find a strong correlation between old and new classes in
incremental learning, which can be applied to relate and facilitate different
learning stages mutually. As a result, we propose CO-transport for class
Incremental Learning (COIL), which learns to relate across incremental tasks
with the class-wise semantic relationship. In detail, co-transport has two
aspects: prospective transport tries to augment the old classifier with optimal
transported knowledge as fast model adaptation. Retrospective transport aims to
transport new class classifiers backward as old ones to overcome forgetting.
With these transports, COIL efficiently adapts to new tasks, and stably resists
forgetting. Experiments on benchmark and real-world multimedia datasets
validate the effectiveness of our proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Da-Wei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Han-Jia Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1"&gt;De-Chuan Zhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving ClusterGAN Using Self-AugmentedInformation Maximization of Disentangling LatentSpaces. (arXiv:2107.12706v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12706</id>
        <link href="http://arxiv.org/abs/2107.12706"/>
        <updated>2021-07-28T02:02:33.331Z</updated>
        <summary type="html"><![CDATA[The Latent Space Clustering in Generative adversarial networks (ClusterGAN)
method has been successful with high-dimensional data. However, the method
assumes uniformlydistributed priors during the generation of modes, which isa
restrictive assumption in real-world data and cause loss ofdiversity in the
generated modes. In this paper, we proposeself-augmentation information
maximization improved Clus-terGAN (SIMI-ClusterGAN) to learn the distinctive
priorsfrom the data. The proposed SIMI-ClusterGAN consists offour deep neural
networks: self-augmentation prior network,generator, discriminator and
clustering inference autoencoder.The proposed method has been validated using
seven bench-mark data sets and has shown improved performance overstate-of-the
art methods. To demonstrate the superiority ofSIMI-ClusterGAN performance on
imbalanced dataset, wehave discussed two imbalanced conditions on MNIST
datasetswith one-class imbalance and three classes imbalanced cases.The results
highlight the advantages of SIMI-ClusterGAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dam_T/0/1/0/all/0/1"&gt;Tanmoy Dam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anavatti_S/0/1/0/all/0/1"&gt;Sreenatha G. Anavatti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbass_H/0/1/0/all/0/1"&gt;Hussein A. Abbass&lt;/a&gt; (Fellow, IEEESchool of Engineering and Information Technology, University of New South Wales Canberra, Australia)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vision-Guided Forecasting -- Visual Context for Multi-Horizon Time Series Forecasting. (arXiv:2107.12674v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12674</id>
        <link href="http://arxiv.org/abs/2107.12674"/>
        <updated>2021-07-28T02:02:33.323Z</updated>
        <summary type="html"><![CDATA[Autonomous driving gained huge traction in recent years, due to its potential
to change the way we commute. Much effort has been put into trying to estimate
the state of a vehicle. Meanwhile, learning to forecast the state of a vehicle
ahead introduces new capabilities, such as predicting dangerous situations.
Moreover, forecasting brings new supervision opportunities by learning to
predict richer a context, expressed by multiple horizons. Intuitively, a video
stream originated from a front-facing camera is necessary because it encodes
information about the upcoming road. Besides, historical traces of the
vehicle's states give more context. In this paper, we tackle multi-horizon
forecasting of vehicle states by fusing the two modalities. We design and
experiment with 3 end-to-end architectures that exploit 3D convolutions for
visual features extraction and 1D convolutions for features extraction from
speed and steering angle traces. To demonstrate the effectiveness of our
method, we perform extensive experiments on two publicly available real-world
datasets, Comma2k19 and the Udacity challenge. We show that we are able to
forecast a vehicle's state to various horizons, while outperforming the current
state-of-the-art results on the related task of driving state estimation. We
examine the contribution of vision features, and find that a model fed with
vision features achieves an error that is 56.6% and 66.9% of the error of a
model that doesn't use those features, on the Udacity and Comma2k19 datasets
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kosman_E/0/1/0/all/0/1"&gt;Eitan Kosman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1"&gt;Dotan Di Castro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MU-MIMO Grouping For Real-time Applications. (arXiv:2106.15262v2 [cs.NI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15262</id>
        <link href="http://arxiv.org/abs/2106.15262"/>
        <updated>2021-07-28T02:02:33.313Z</updated>
        <summary type="html"><![CDATA[Over the last decade, the bandwidth expansion and MU-MIMO spectral efficiency
have promised to increase data throughput by allowing concurrent communication
between one Access Point and multiple users. However, we are still a long way
from enjoying such MU-MIMO MAC protocol improvements for bandwidth hungry
applications such as video streaming in practical WiFi network settings due to
heterogeneous channel conditions and devices, unreliable transmissions, and
lack of useful feedback exchange among the lower and upper layers'
requirements. This paper introduces MuViS, a novel dual-phase optimization
framework that proposes a Quality of Experience (QoE) aware MU-MIMO
optimization for multi-user video streaming over IEEE 802.11ac. MuViS first
employs reinforcement learning to optimize the MU-MIMO user group and mode
selection for users based on their PHY/MAC layer characteristics. The video
bitrate is then optimized based on the user's mode (Multi-User (MU) or
Single-User (SU)). We present our design and its evaluation on smartphones and
laptops using 802.11ac WiFi. Our experimental results in various indoor
environments and configurations show a scalable framework that can support a
large number of users with streaming at high video rates and satisfying QoE
requirements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pasandi_H/0/1/0/all/0/1"&gt;Hannaneh Barahouei Pasandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nadeem_T/0/1/0/all/0/1"&gt;Tamer Nadeem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amirpour_H/0/1/0/all/0/1"&gt;Hadi Amirpour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Learning with Neuron Activation Importance. (arXiv:2107.12657v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12657</id>
        <link href="http://arxiv.org/abs/2107.12657"/>
        <updated>2021-07-28T02:02:33.294Z</updated>
        <summary type="html"><![CDATA[Continual learning is a concept of online learning with multiple sequential
tasks. One of the critical barriers of continual learning is that a network
should learn a new task keeping the knowledge of old tasks without access to
any data of the old tasks. In this paper, we propose a neuron activation
importance-based regularization method for stable continual learning regardless
of the order of tasks. We conduct comprehensive experiments on existing
benchmark data sets to evaluate not just the stability and plasticity of our
method with improved classification accuracy also the robustness of the
performance along the changes of task order.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sohee Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seungkyu Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Transfer Learning based COVID-19 Detection in Cough, Breath and Speech using Bottleneck Features. (arXiv:2104.02477v3 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02477</id>
        <link href="http://arxiv.org/abs/2104.02477"/>
        <updated>2021-07-28T02:02:33.288Z</updated>
        <summary type="html"><![CDATA[We present an experimental investigation into the automatic detection of
COVID-19 from coughs, breaths and speech as this type of screening is
non-contact, does not require specialist medical expertise or laboratory
facilities and can easily be deployed on inexpensive consumer hardware.

Smartphone recordings of cough, breath and speech from subjects around the
globe are used for classification by seven standard machine learning
classifiers using leave-$p$-out cross-validation to provide a promising
baseline performance.

Then, a diverse dataset of 10.29 hours of cough, sneeze, speech and noise
audio recordings are used to pre-train a CNN, LSTM and Resnet50 classifier and
fine tuned the model to enhance the performance even further.

We have also extracted the bottleneck features from these pre-trained models
by removing the final-two layers and used them as an input to the LR, SVM, MLP
and KNN classifiers to detect COVID-19 signature.

The highest AUC of 0.98 was achieved using a transfer learning based Resnet50
architecture on coughs from Coswara dataset.

The highest AUC of 0.94 and 0.92 was achieved from an SVM run on the
bottleneck features extracted from the breaths from Coswara dataset and speech
recordings from ComParE dataset.

We conclude that among all vocal audio, coughs carry the strongest COVID-19
signature followed by breath and speech and using transfer learning improves
the classifier performance with higher AUC and lower variance across the
cross-validation folds.

Although these signatures are not perceivable by human ear, machine learning
based COVID-19 detection is possible from vocal audio recorded via smartphone.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pahar_M/0/1/0/all/0/1"&gt;Madhurananda Pahar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niesler_T/0/1/0/all/0/1"&gt;Thomas Niesler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Estimate RIS-Aided mmWave Channels. (arXiv:2107.12631v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.12631</id>
        <link href="http://arxiv.org/abs/2107.12631"/>
        <updated>2021-07-28T02:02:33.281Z</updated>
        <summary type="html"><![CDATA[Inspired by the remarkable learning and prediction performance of deep neural
networks (DNNs), we apply one special type of DNN framework, known as
model-driven deep unfolding neural network, to reconfigurable intelligent
surface (RIS)-aided millimeter wave (mmWave) single-input multiple-output
(SIMO) systems. We focus on uplink cascaded channel estimation, where known and
fixed base station combining and RIS phase control matrices are considered for
collecting observations. To boost the estimation performance and reduce the
training overhead, the inherent channel sparsity of mmWave channels is
leveraged in the deep unfolding method. It is verified that the proposed deep
unfolding network architecture can outperform the least squares (LS) method
with a relatively smaller training overhead and online computational
complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+He_J/0/1/0/all/0/1"&gt;Jiguang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wymeersch_H/0/1/0/all/0/1"&gt;Henk Wymeersch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Renzo_M/0/1/0/all/0/1"&gt;Marco Di Renzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Juntti_M/0/1/0/all/0/1"&gt;Markku Juntti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diffusion Earth Mover's Distance and Distribution Embeddings. (arXiv:2102.12833v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12833</id>
        <link href="http://arxiv.org/abs/2102.12833"/>
        <updated>2021-07-28T02:02:33.274Z</updated>
        <summary type="html"><![CDATA[We propose a new fast method of measuring distances between large numbers of
related high dimensional datasets called the Diffusion Earth Mover's Distance
(EMD). We model the datasets as distributions supported on common data graph
that is derived from the affinity matrix computed on the combined data. In such
cases where the graph is a discretization of an underlying Riemannian closed
manifold, we prove that Diffusion EMD is topologically equivalent to the
standard EMD with a geodesic ground distance. Diffusion EMD can be computed in
$\tilde{O}(n)$ time and is more accurate than similarly fast algorithms such as
tree-based EMDs. We also show Diffusion EMD is fully differentiable, making it
amenable to future uses in gradient-descent frameworks such as deep neural
networks. Finally, we demonstrate an application of Diffusion EMD to single
cell data collected from 210 COVID-19 patient samples at Yale New Haven
Hospital. Here, Diffusion EMD can derive distances between patients on the
manifold of cells at least two orders of magnitude faster than equally accurate
methods. This distance matrix between patients can be embedded into a higher
level patient manifold which uncovers structure and heterogeneity in patients.
More generally, Diffusion EMD is applicable to all datasets that are massively
collected in parallel in many medical and biological systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tong_A/0/1/0/all/0/1"&gt;Alexander Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huguet_G/0/1/0/all/0/1"&gt;Guillaume Huguet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Natik_A/0/1/0/all/0/1"&gt;Amine Natik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+MacDonald_K/0/1/0/all/0/1"&gt;Kincaid MacDonald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuchroo_M/0/1/0/all/0/1"&gt;Manik Kuchroo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coifman_R/0/1/0/all/0/1"&gt;Ronald Coifman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_G/0/1/0/all/0/1"&gt;Guy Wolf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnaswamy_S/0/1/0/all/0/1"&gt;Smita Krishnaswamy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speech Resynthesis from Discrete Disentangled Self-Supervised Representations. (arXiv:2104.00355v3 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00355</id>
        <link href="http://arxiv.org/abs/2104.00355"/>
        <updated>2021-07-28T02:02:33.266Z</updated>
        <summary type="html"><![CDATA[We propose using self-supervised discrete representations for the task of
speech resynthesis. To generate disentangled representation, we separately
extract low-bitrate representations for speech content, prosodic information,
and speaker identity. This allows to synthesize speech in a controllable
manner. We analyze various state-of-the-art, self-supervised representation
learning methods and shed light on the advantages of each method while
considering reconstruction quality and disentanglement properties.
Specifically, we evaluate the F0 reconstruction, speaker identification
performance (for both resynthesis and voice conversion), recordings'
intelligibility, and overall quality using subjective human evaluation. Lastly,
we demonstrate how these representations can be used for an ultra-lightweight
speech codec. Using the obtained representations, we can get to a rate of 365
bits per second while providing better speech quality than the baseline
methods. Audio samples can be found under the following link:
speechbot.github.io/resynthesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Polyak_A/0/1/0/all/0/1"&gt;Adam Polyak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1"&gt;Yossi Adi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1"&gt;Jade Copet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1"&gt;Eugene Kharitonov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1"&gt;Kushal Lakhotia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Wei-Ning Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1"&gt;Abdelrahman Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1"&gt;Emmanuel Dupoux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Learning Rate and Momentum for Training Deep Neural Networks. (arXiv:2106.11548v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11548</id>
        <link href="http://arxiv.org/abs/2106.11548"/>
        <updated>2021-07-28T02:02:33.247Z</updated>
        <summary type="html"><![CDATA[Recent progress on deep learning relies heavily on the quality and efficiency
of training algorithms. In this paper, we develop a fast training method
motivated by the nonlinear Conjugate Gradient (CG) framework. We propose the
Conjugate Gradient with Quadratic line-search (CGQ) method. On the one hand, a
quadratic line-search determines the step size according to current loss
landscape. On the other hand, the momentum factor is dynamically updated in
computing the conjugate gradient parameter (like Polak-Ribiere). Theoretical
results to ensure the convergence of our method in strong convex settings is
developed. And experiments in image classification datasets show that our
method yields faster convergence than other local solvers and has better
generalization capability (test set accuracy). One major advantage of the paper
method is that tedious hand tuning of hyperparameters like the learning rate
and momentum is avoided.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1"&gt;Zhiyong Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yixuan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Huihua Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiang_H/0/1/0/all/0/1"&gt;Hsiao-Dong Chiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Waveshaping Synthesis. (arXiv:2107.05050v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05050</id>
        <link href="http://arxiv.org/abs/2107.05050"/>
        <updated>2021-07-28T02:02:33.240Z</updated>
        <summary type="html"><![CDATA[We present the Neural Waveshaping Unit (NEWT): a novel, lightweight, fully
causal approach to neural audio synthesis which operates directly in the
waveform domain, with an accompanying optimisation (FastNEWT) for efficient CPU
inference. The NEWT uses time-distributed multilayer perceptrons with periodic
activations to implicitly learn nonlinear transfer functions that encode the
characteristics of a target timbre. Once trained, a NEWT can produce complex
timbral evolutions by simple affine transformations of its input and output
signals. We paired the NEWT with a differentiable noise synthesiser and reverb
and found it capable of generating realistic musical instrument performances
with only 260k total model parameters, conditioned on F0 and loudness features.
We compared our method to state-of-the-art benchmarks with a multi-stimulus
listening test and the Fr\'echet Audio Distance and found it performed
competitively across the tested timbral domains. Our method significantly
outperformed the benchmarks in terms of generation speed, and achieved
real-time performance on a consumer CPU, both with and without FastNEWT,
suggesting it is a viable basis for future creative sound design tools.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hayes_B/0/1/0/all/0/1"&gt;Ben Hayes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saitis_C/0/1/0/all/0/1"&gt;Charalampos Saitis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1"&gt;Gy&amp;#xf6;rgy Fazekas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combining Probabilistic Logic and Deep Learning for Self-Supervised Learning. (arXiv:2107.12591v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12591</id>
        <link href="http://arxiv.org/abs/2107.12591"/>
        <updated>2021-07-28T02:02:33.232Z</updated>
        <summary type="html"><![CDATA[Deep learning has proven effective for various application tasks, but its
applicability is limited by the reliance on annotated examples. Self-supervised
learning has emerged as a promising direction to alleviate the supervision
bottleneck, but existing work focuses on leveraging co-occurrences in unlabeled
data for task-agnostic representation learning, as exemplified by masked
language model pretraining. In this chapter, we explore task-specific
self-supervision, which leverages domain knowledge to automatically annotate
noisy training examples for end applications, either by introducing labeling
functions for annotating individual instances, or by imposing constraints over
interdependent label decisions. We first present deep probabilistic logic(DPL),
which offers a unifying framework for task-specific self-supervision by
composing probabilistic logic with deep learning. DPL represents unknown labels
as latent variables and incorporates diverse self-supervision using
probabilistic logic to train a deep neural network end-to-end using variational
EM. Next, we present self-supervised self-supervision(S4), which adds to DPL
the capability to learn new self-supervision automatically. Starting from an
initial seed self-supervision, S4 iteratively uses the deep neural network to
propose new self supervision. These are either added directly (a form of
structured self-training) or verified by a human expert (as in feature-based
active learning). Experiments on real-world applications such as biomedical
machine reading and various text classification tasks show that task-specific
self-supervision can effectively leverage domain expertise and often match the
accuracy of supervised methods with a tiny fraction of human effort.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1"&gt;Hoifung Poon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lang_H/0/1/0/all/0/1"&gt;Hunter Lang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quaternion Generative Adversarial Networks. (arXiv:2104.09630v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09630</id>
        <link href="http://arxiv.org/abs/2104.09630"/>
        <updated>2021-07-28T02:02:33.225Z</updated>
        <summary type="html"><![CDATA[Latest Generative Adversarial Networks (GANs) are gathering outstanding
results through a large-scale training, thus employing models composed of
millions of parameters requiring extensive computational capabilities. Building
such huge models undermines their replicability and increases the training
instability. Moreover, multi-channel data, such as images or audio, are usually
processed by realvalued convolutional networks that flatten and concatenate the
input, often losing intra-channel spatial relations. To address these issues
related to complexity and information loss, we propose a family of
quaternion-valued generative adversarial networks (QGANs). QGANs exploit the
properties of quaternion algebra, e.g., the Hamilton product, that allows to
process channels as a single entity and capture internal latent relations,
while reducing by a factor of 4 the overall number of parameters. We show how
to design QGANs and to extend the proposed approach even to advanced models.We
compare the proposed QGANs with real-valued counterparts on several image
generation benchmarks. Results show that QGANs are able to obtain better FID
scores than real-valued GANs and to generate visually pleasing images.
Furthermore, QGANs save up to 75% of the training parameters. We believe these
results may pave the way to novel, more accessible, GANs capable of improving
performance and saving computational resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grassucci_E/0/1/0/all/0/1"&gt;Eleonora Grassucci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cicero_E/0/1/0/all/0/1"&gt;Edoardo Cicero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Comminiello_D/0/1/0/all/0/1"&gt;Danilo Comminiello&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Lode Runner Levels by Learning Player Paths with LSTMs. (arXiv:2107.12532v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12532</id>
        <link href="http://arxiv.org/abs/2107.12532"/>
        <updated>2021-07-28T02:02:33.218Z</updated>
        <summary type="html"><![CDATA[Machine learning has been a popular tool in many different fields, including
procedural content generation. However, procedural content generation via
machine learning (PCGML) approaches can struggle with controllability and
coherence. In this paper, we attempt to address these problems by learning to
generate human-like paths, and then generating levels based on these paths. We
extract player path data from gameplay video, train an LSTM to generate new
paths based on this data, and then generate game levels based on this path
data. We demonstrate that our approach leads to more coherent levels for the
game Lode Runner in comparison to an existing PCGML approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sorochan_K/0/1/0/all/0/1"&gt;Kynan Sorochan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jerry Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yakun Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guzdial_M/0/1/0/all/0/1"&gt;Matthew Guzdial&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical Measures For Defining Curriculum Scoring Function. (arXiv:2103.00147v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00147</id>
        <link href="http://arxiv.org/abs/2103.00147"/>
        <updated>2021-07-28T02:02:33.211Z</updated>
        <summary type="html"><![CDATA[Curriculum learning is a training strategy that sorts the training examples
by some measure of their difficulty and gradually exposes them to the learner
to improve the network performance. Motivated by our insights from implicit
curriculum ordering, we first introduce a simple curriculum learning strategy
that uses statistical measures such as standard deviation and entropy values to
score the difficulty of data points for real image classification tasks. We
empirically show its improvements in performance with convolutional and
fully-connected neural networks on multiple real image datasets. We also
propose and study the performance of a dynamic curriculum learning algorithm.
Our dynamic curriculum algorithm tries to reduce the distance between the
network weight and an optimal weight at any training step by greedily sampling
examples with gradients that are directed towards the optimal weight. Further,
we use our algorithms to discuss why curriculum learning is helpful.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sadasivan_V/0/1/0/all/0/1"&gt;Vinu Sankar Sadasivan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dasgupta_A/0/1/0/all/0/1"&gt;Anirban Dasgupta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MFAGAN: A Compression Framework for Memory-Efficient On-Device Super-Resolution GAN. (arXiv:2107.12679v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2107.12679</id>
        <link href="http://arxiv.org/abs/2107.12679"/>
        <updated>2021-07-28T02:02:33.193Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) have promoted remarkable advances in
single-image super-resolution (SR) by recovering photo-realistic images.
However, high memory consumption of GAN-based SR (usually generators) causes
performance degradation and more energy consumption, hindering the deployment
of GAN-based SR into resource-constricted mobile devices. In this paper, we
propose a novel compression framework \textbf{M}ulti-scale \textbf{F}eature
\textbf{A}ggregation Net based \textbf{GAN} (MFAGAN) for reducing the memory
access cost of the generator. First, to overcome the memory explosion of dense
connections, we utilize a memory-efficient multi-scale feature aggregation net
as the generator. Second, for faster and more stable training, our method
introduces the PatchGAN discriminator. Third, to balance the student
discriminator and the compressed generator, we distill both the generator and
the discriminator. Finally, we perform a hardware-aware neural architecture
search (NAS) to find a specialized SubGenerator for the target mobile phone.
Benefiting from these improvements, the proposed MFAGAN achieves up to
\textbf{8.3}$\times$ memory saving and \textbf{42.9}$\times$ computation
reduction, with only minor visual quality degradation, compared with ESRGAN.
Empirical studies also show $\sim$\textbf{70} milliseconds latency on Qualcomm
Snapdragon 865 chipset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1"&gt;Wenlong Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1"&gt;Mingbo Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1"&gt;Zhiling Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1"&gt;Shuhang Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning-based decentralized offloading decision making in an adversarial environment. (arXiv:2104.12827v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12827</id>
        <link href="http://arxiv.org/abs/2104.12827"/>
        <updated>2021-07-28T02:02:33.186Z</updated>
        <summary type="html"><![CDATA[Vehicular fog computing (VFC) pushes the cloud computing capability to the
distributed fog nodes at the edge of the Internet, enabling compute-intensive
and latency-sensitive computing services for vehicles through task offloading.
However, a heterogeneous mobility environment introduces uncertainties in terms
of resource supply and demand, which are inevitable bottlenecks for the optimal
offloading decision. Also, these uncertainties bring extra challenges to task
offloading under the oblivious adversary attack and data privacy risks. In this
article, we develop a new adversarial online learning algorithm with bandit
feedback based on the adversarial multi-armed bandit theory, to enable scalable
and low-complexity offloading decision making. Specifically, we focus on
optimizing fog node selection with the aim of minimizing the offloading service
costs in terms of delay and energy. The key is to implicitly tune the
exploration bonus in the selection process and the assessment rules of the
designed algorithm, taking into account volatile resource supply and demand. We
theoretically prove that the input-size dependent selection rule allows to
choose a suitable fog node without exploring the sub-optimal actions, and also
an appropriate score patching rule allows to quickly adapt to evolving
circumstances, which reduce variance and bias simultaneously, thereby achieving
a better exploitation-exploration balance. Simulation results verify the
effectiveness and robustness of the proposed algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cho_B/0/1/0/all/0/1"&gt;Byungjin Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1"&gt;Yu Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Voice Identity Conversion: Manipulating Voice Attributes by Adversarial Learning of Structured Disentangled Representations. (arXiv:2107.12346v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12346</id>
        <link href="http://arxiv.org/abs/2107.12346"/>
        <updated>2021-07-28T02:02:33.178Z</updated>
        <summary type="html"><![CDATA[Voice conversion (VC) consists of digitally altering the voice of an
individual to manipulate part of its content, primarily its identity, while
maintaining the rest unchanged. Research in neural VC has accomplished
considerable breakthroughs with the capacity to falsify a voice identity using
a small amount of data with a highly realistic rendering. This paper goes
beyond voice identity and presents a neural architecture that allows the
manipulation of voice attributes (e.g., gender and age). Leveraging the latest
advances on adversarial learning of structured speech representation, a novel
structured neural network is proposed in which multiple auto-encoders are used
to encode speech as a set of idealistically independent linguistic and
extra-linguistic representations, which are learned adversariarly and can be
manipulated during VC. Moreover, the proposed architecture is time-synchronized
so that the original voice timing is preserved during conversion which allows
lip-sync applications. Applied to voice gender conversion on the real-world
VCTK dataset, our proposed architecture can learn successfully
gender-independent representation and convert the voice gender with a very high
efficiency and naturalness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Benaroya_L/0/1/0/all/0/1"&gt;Laurent Benaroya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Obin_N/0/1/0/all/0/1"&gt;Nicolas Obin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roebel_A/0/1/0/all/0/1"&gt;Axel Roebel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Open-Ended Learning Leads to Generally Capable Agents. (arXiv:2107.12808v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12808</id>
        <link href="http://arxiv.org/abs/2107.12808"/>
        <updated>2021-07-28T02:02:33.166Z</updated>
        <summary type="html"><![CDATA[In this work we create agents that can perform well beyond a single,
individual task, that exhibit much wider generalisation of behaviour to a
massive, rich space of challenges. We define a universe of tasks within an
environment domain and demonstrate the ability to train agents that are
generally capable across this vast space and beyond. The environment is
natively multi-agent, spanning the continuum of competitive, cooperative, and
independent games, which are situated within procedurally generated physical 3D
worlds. The resulting space is exceptionally diverse in terms of the challenges
posed to agents, and as such, even measuring the learning progress of an agent
is an open research problem. We propose an iterative notion of improvement
between successive generations of agents, rather than seeking to maximise a
singular objective, allowing us to quantify progress despite tasks being
incomparable in terms of achievable rewards. We show that through constructing
an open-ended learning process, which dynamically changes the training task
distributions and training objectives such that the agent never stops learning,
we achieve consistent learning of new behaviours. The resulting agent is able
to score reward in every one of our humanly solvable evaluation levels, with
behaviour generalising to many held-out points in the universe of tasks.
Examples of this zero-shot generalisation include good performance on Hide and
Seek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks
we characterise the behaviour of our agent, and find interesting emergent
heuristic behaviours such as trial-and-error experimentation, simple tool use,
option switching, and cooperation. Finally, we demonstrate that the general
capabilities of this agent could unlock larger scale transfer of behaviour
through cheap finetuning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Team_Open_Ended_Learning/0/1/0/all/0/1"&gt;Open-Ended Learning Team&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stooke_A/0/1/0/all/0/1"&gt;Adam Stooke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahajan_A/0/1/0/all/0/1"&gt;Anuj Mahajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barros_C/0/1/0/all/0/1"&gt;Catarina Barros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deck_C/0/1/0/all/0/1"&gt;Charlie Deck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauer_J/0/1/0/all/0/1"&gt;Jakob Bauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sygnowski_J/0/1/0/all/0/1"&gt;Jakub Sygnowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trebacz_M/0/1/0/all/0/1"&gt;Maja Trebacz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaderberg_M/0/1/0/all/0/1"&gt;Max Jaderberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathieu_M/0/1/0/all/0/1"&gt;Michael Mathieu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McAleese_N/0/1/0/all/0/1"&gt;Nat McAleese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bradley_Schmieg_N/0/1/0/all/0/1"&gt;Nathalie Bradley-Schmieg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_N/0/1/0/all/0/1"&gt;Nathaniel Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Porcel_N/0/1/0/all/0/1"&gt;Nicolas Porcel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raileanu_R/0/1/0/all/0/1"&gt;Roberta Raileanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hughes_Fitt_S/0/1/0/all/0/1"&gt;Steph Hughes-Fitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dalibard_V/0/1/0/all/0/1"&gt;Valentin Dalibard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Czarnecki_W/0/1/0/all/0/1"&gt;Wojciech Marian Czarnecki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CFLOW-AD: Real-Time Unsupervised Anomaly Detection with Localization via Conditional Normalizing Flows. (arXiv:2107.12571v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12571</id>
        <link href="http://arxiv.org/abs/2107.12571"/>
        <updated>2021-07-28T02:02:33.147Z</updated>
        <summary type="html"><![CDATA[Unsupervised anomaly detection with localization has many practical
applications when labeling is infeasible and, moreover, when anomaly examples
are completely missing in the train data. While recently proposed models for
such data setup achieve high accuracy metrics, their complexity is a limiting
factor for real-time processing. In this paper, we propose a real-time model
and analytically derive its relationship to prior methods. Our CFLOW-AD model
is based on a conditional normalizing flow framework adopted for anomaly
detection with localization. In particular, CFLOW-AD consists of a
discriminatively pretrained encoder followed by a multi-scale generative
decoders where the latter explicitly estimate likelihood of the encoded
features. Our approach results in a computationally and memory-efficient model:
CFLOW-AD is faster and smaller by a factor of 10x than prior state-of-the-art
with the same input setting. Our experiments on the MVTec dataset show that
CFLOW-AD outperforms previous methods by 0.36% AUROC in detection task, by
1.12% AUROC and 2.5% AUPRO in localization task, respectively. We open-source
our code with fully reproducible experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gudovskiy_D/0/1/0/all/0/1"&gt;Denis Gudovskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ishizaka_S/0/1/0/all/0/1"&gt;Shun Ishizaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kozuka_K/0/1/0/all/0/1"&gt;Kazuki Kozuka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward Co-creative Dungeon Generation via Transfer Learning. (arXiv:2107.12533v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12533</id>
        <link href="http://arxiv.org/abs/2107.12533"/>
        <updated>2021-07-28T02:02:33.138Z</updated>
        <summary type="html"><![CDATA[Co-creative Procedural Content Generation via Machine Learning (PCGML) refers
to systems where a PCGML agent and a human work together to produce output
content. One of the limitations of co-creative PCGML is that it requires
co-creative training data for a PCGML agent to learn to interact with humans.
However, acquiring this data is a difficult and time-consuming process. In this
work, we propose approximating human-AI interaction data and employing transfer
learning to adapt learned co-creative knowledge from one game to a different
game. We explore this approach for co-creative Zelda dungeon room generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zisen Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guzdial_M/0/1/0/all/0/1"&gt;Matthew Guzdial&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-constrained Deep Learning for Robust Inverse ECG Modeling. (arXiv:2107.12780v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12780</id>
        <link href="http://arxiv.org/abs/2107.12780"/>
        <updated>2021-07-28T02:02:33.131Z</updated>
        <summary type="html"><![CDATA[The rapid developments in advanced sensing and imaging bring about a
data-rich environment, facilitating the effective modeling, monitoring, and
control of complex systems. For example, the body-sensor network captures
multi-channel information pertinent to the electrical activity of the heart
(i.e., electrocardiograms (ECG)), which enables medical scientists to monitor
and detect abnormal cardiac conditions. However, the high-dimensional sensing
data are generally complexly structured and realizing the full data potential
depends to a great extent on advanced analytical and predictive methods. This
paper presents a physics-constrained deep learning (P-DL) framework for
high-dimensional inverse ECG modeling. This method integrates the physical laws
of the complex system with the advanced deep learning infrastructure for
effective prediction of the system dynamics. The proposed P-DL approach is
implemented to solve the inverse ECG model and predict the time-varying
distribution of electric potentials in the heart from the ECG data measured by
the body-surface sensor network. Experimental results show that the proposed
P-DL method significantly outperforms existing methods that are commonly used
in current practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jianxin Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1"&gt;Bing Yao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relational Boosted Regression Trees. (arXiv:2107.12373v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2107.12373</id>
        <link href="http://arxiv.org/abs/2107.12373"/>
        <updated>2021-07-28T02:02:33.124Z</updated>
        <summary type="html"><![CDATA[Many tasks use data housed in relational databases to train boosted
regression tree models. In this paper, we give a relational adaptation of the
greedy algorithm for training boosted regression trees. For the subproblem of
calculating the sum of squared residuals of the dataset, which dominates the
runtime of the boosting algorithm, we provide a $(1 + \epsilon)$-approximation
using the tensor sketch technique. Employing this approximation within the
relational boosted regression trees algorithm leads to learning similar model
parameters, but with asymptotically better runtime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cromp_S/0/1/0/all/0/1"&gt;Sonia Cromp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samadian_A/0/1/0/all/0/1"&gt;Alireza Samadian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pruhs_K/0/1/0/all/0/1"&gt;Kirk Pruhs&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Sample Selection for Robust Learning under Label Noise. (arXiv:2106.15292v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15292</id>
        <link href="http://arxiv.org/abs/2106.15292"/>
        <updated>2021-07-28T02:02:33.117Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) have been shown to be susceptible to memorization
or overfitting in the presence of noisily labelled data. For the problem of
robust learning under such noisy data, several algorithms have been proposed. A
prominent class of algorithms rely on sample selection strategies, motivated by
curriculum learning. For example, many algorithms use the `small loss trick'
wherein a fraction of samples with loss values below a certain threshold are
selected for training. These algorithms are sensitive to such thresholds, and
it is difficult to fix or learn these thresholds. Often, these algorithms also
require information such as label noise rates which are typically unavailable
in practice. In this paper, we propose a data-dependent, adaptive sample
selection strategy that relies only on batch statistics of a given mini-batch
to provide robustness against label noise. The algorithm does not have any
additional hyperparameters for sample selection, does not need any information
on noise rates, and does not need access to separate data with clean labels. We
empirically demonstrate the effectiveness of our algorithm on benchmark
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1"&gt;Deep Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sastry_P/0/1/0/all/0/1"&gt;P.S. Sastry&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TaikoNation: Patterning-focused Chart Generation for Rhythm Action Games. (arXiv:2107.12506v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12506</id>
        <link href="http://arxiv.org/abs/2107.12506"/>
        <updated>2021-07-28T02:02:33.105Z</updated>
        <summary type="html"><![CDATA[Generating rhythm game charts from songs via machine learning has been a
problem of increasing interest in recent years. However, all existing systems
struggle to replicate human-like patterning: the placement of game objects in
relation to each other to form congruent patterns based on events in the song.
Patterning is a key identifier of high quality rhythm game content, seen as a
necessary component in human rankings. We establish a new approach for chart
generation that produces charts with more congruent, human-like patterning than
seen in prior work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Halina_E/0/1/0/all/0/1"&gt;Emily Halina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guzdial_M/0/1/0/all/0/1"&gt;Matthew Guzdial&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combining Reward and Rank Signals for Slate Recommendation. (arXiv:2107.12455v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12455</id>
        <link href="http://arxiv.org/abs/2107.12455"/>
        <updated>2021-07-28T02:02:33.086Z</updated>
        <summary type="html"><![CDATA[We consider the problem of slate recommendation, where the recommender system
presents a user with a collection or slate composed of K recommended items at
once. If the user finds the recommended items appealing then the user may click
and the recommender system receives some feedback. Two pieces of information
are available to the recommender system: was the slate clicked? (the reward),
and if the slate was clicked, which item was clicked? (rank). In this paper, we
formulate several Bayesian models that incorporate the reward signal (Reward
model), the rank signal (Rank model), or both (Full model), for
non-personalized slate recommendation. In our experiments, we analyze
performance gains of the Full model and show that it achieves significantly
lower error as the number of products in the catalog grows or as the slate size
increases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aouali_I/0/1/0/all/0/1"&gt;Imad Aouali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ivanov_S/0/1/0/all/0/1"&gt;Sergey Ivanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gartrell_M/0/1/0/all/0/1"&gt;Mike Gartrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rohde_D/0/1/0/all/0/1"&gt;David Rohde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasile_F/0/1/0/all/0/1"&gt;Flavian Vasile&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaytsev_V/0/1/0/all/0/1"&gt;Victor Zaytsev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Legrand_D/0/1/0/all/0/1"&gt;Diego Legrand&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Deep Model of Learning from both Data and Queries for Cardinality Estimation. (arXiv:2107.12295v1 [cs.DB] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2107.12295</id>
        <link href="http://arxiv.org/abs/2107.12295"/>
        <updated>2021-07-28T02:02:33.078Z</updated>
        <summary type="html"><![CDATA[Cardinality estimation is a fundamental problem in database systems. To
capture the rich joint data distributions of a relational table, most of the
existing work either uses data as unsupervised information or uses query
workload as supervised information. Very little work has been done to use both
types of information, and cannot fully make use of both types of information to
learn the joint data distribution. In this work, we aim to close the gap
between data-driven and query-driven methods by proposing a new unified deep
autoregressive model, UAE, that learns the joint data distribution from both
the data and query workload. First, to enable using the supervised query
information in the deep autoregressive model, we develop differentiable
progressive sampling using the Gumbel-Softmax trick. Second, UAE is able to
utilize both types of information to learn the joint data distribution in a
single model. Comprehensive experimental results demonstrate that UAE achieves
single-digit multiplicative error at tail, better accuracies over
state-of-the-art methods, and is both space and time efficient.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1"&gt;Peizhi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cong_G/0/1/0/all/0/1"&gt;Gao Cong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Graph Neural Networking Challenge: A Worldwide Competition for Education in AI/ML for Networks. (arXiv:2107.12433v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2107.12433</id>
        <link href="http://arxiv.org/abs/2107.12433"/>
        <updated>2021-07-28T02:02:33.071Z</updated>
        <summary type="html"><![CDATA[During the last decade, Machine Learning (ML) has increasingly become a hot
topic in the field of Computer Networks and is expected to be gradually adopted
for a plethora of control, monitoring and management tasks in real-world
deployments. This poses the need to count on new generations of students,
researchers and practitioners with a solid background in ML applied to
networks. During 2020, the International Telecommunication Union (ITU) has
organized the "ITU AI/ML in 5G challenge'', an open global competition that has
introduced to a broad audience some of the current main challenges in ML for
networks. This large-scale initiative has gathered 23 different challenges
proposed by network operators, equipment manufacturers and academia, and has
attracted a total of 1300+ participants from 60+ countries. This paper narrates
our experience organizing one of the proposed challenges: the "Graph Neural
Networking Challenge 2020''. We describe the problem presented to participants,
the tools and resources provided, some organization aspects and participation
statistics, an outline of the top-3 awarded solutions, and a summary with some
lessons learned during all this journey. As a result, this challenge leaves a
curated set of educational resources openly available to anyone interested in
the topic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suarez_Varela_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Su&amp;#xe1;rez-Varela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferriol_Galmes_M/0/1/0/all/0/1"&gt;Miquel Ferriol-Galm&amp;#xe9;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lopez_A/0/1/0/all/0/1"&gt;Albert L&amp;#xf3;pez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Almasan_P/0/1/0/all/0/1"&gt;Paul Almasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bernardez_G/0/1/0/all/0/1"&gt;Guillermo Bern&amp;#xe1;rdez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pujol_Perich_D/0/1/0/all/0/1"&gt;David Pujol-Perich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rusek_K/0/1/0/all/0/1"&gt;Krzysztof Rusek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bonniot_L/0/1/0/all/0/1"&gt;Lo&amp;#xef;ck Bonniot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neumann_C/0/1/0/all/0/1"&gt;Christoph Neumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schnitzler_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Schnitzler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taiani_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Ta&amp;#xef;ani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Happ_M/0/1/0/all/0/1"&gt;Martin Happ&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maier_C/0/1/0/all/0/1"&gt;Christian Maier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1"&gt;Jia Lei Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herlich_M/0/1/0/all/0/1"&gt;Matthias Herlich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dorfinger_P/0/1/0/all/0/1"&gt;Peter Dorfinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hainke_N/0/1/0/all/0/1"&gt;Nick Vincent Hainke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venz_S/0/1/0/all/0/1"&gt;Stefan Venz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wegener_J/0/1/0/all/0/1"&gt;Johannes Wegener&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wissing_H/0/1/0/all/0/1"&gt;Henrike Wissing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1"&gt;Bo Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1"&gt;Shihan Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barlet_Ros_P/0/1/0/all/0/1"&gt;Pere Barlet-Ros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cabellos_Aparicio_A/0/1/0/all/0/1"&gt;Albert Cabellos-Aparicio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LETI: Latency Estimation Tool and Investigation of Neural Networks inference on Mobile GPU. (arXiv:2010.02871v2 [cs.PF] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.02871</id>
        <link href="http://arxiv.org/abs/2010.02871"/>
        <updated>2021-07-28T02:02:33.062Z</updated>
        <summary type="html"><![CDATA[A lot of deep learning applications are desired to be run on mobile devices.
Both accuracy and inference time are meaningful for a lot of them. While the
number of FLOPs is usually used as a proxy for neural network latency, it may
be not the best choice. In order to obtain a better approximation of latency,
research community uses look-up tables of all possible layers for latency
calculation for the final prediction of the inference on mobile CPU. It
requires only a small number of experiments. Unfortunately, on mobile GPU this
method is not applicable in a straight-forward way and shows low precision. In
this work, we consider latency approximation on mobile GPU as a data and
hardware-specific problem. Our main goal is to construct a convenient latency
estimation tool for investigation(LETI) of neural network inference and
building robust and accurate latency prediction models for each specific task.
To achieve this goal, we build open-source tools which provide a convenient way
to conduct massive experiments on different target devices focusing on mobile
GPU. After evaluation of the dataset, we learn the regression model on
experimental data and use it for future latency prediction and analysis. We
experimentally demonstrate the applicability of such an approach on a subset of
popular NAS-Benchmark 101 dataset and also evaluate the most popular neural
network architectures for two mobile GPUs. As a result, we construct latency
prediction model with good precision on the target evaluation subset. We
consider LETI as a useful tool for neural architecture search or massive
latency evaluation. The project is available at https://github.com/leti-ai]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ponomarev_E/0/1/0/all/0/1"&gt;Evgeny Ponomarev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matveev_S/0/1/0/all/0/1"&gt;Sergey Matveev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oseledets_I/0/1/0/all/0/1"&gt;Ivan Oseledets&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Operating Points for High Performance Lesion Detection and Segmentation Using Lesion Size Reweighting. (arXiv:2107.12978v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.12978</id>
        <link href="http://arxiv.org/abs/2107.12978"/>
        <updated>2021-07-28T02:02:33.039Z</updated>
        <summary type="html"><![CDATA[There are many clinical contexts which require accurate detection and
segmentation of all focal pathologies (e.g. lesions, tumours) in patient
images. In cases where there are a mix of small and large lesions, standard
binary cross entropy loss will result in better segmentation of large lesions
at the expense of missing small ones. Adjusting the operating point to
accurately detect all lesions generally leads to oversegmentation of large
lesions. In this work, we propose a novel reweighing strategy to eliminate this
performance gap, increasing small pathology detection performance while
maintaining segmentation accuracy. We show that our reweighing strategy vastly
outperforms competing strategies based on experiments on a large scale,
multi-scanner, multi-center dataset of Multiple Sclerosis patient images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nichyporuk_B/0/1/0/all/0/1"&gt;Brennan Nichyporuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Szeto_J/0/1/0/all/0/1"&gt;Justin Szeto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arnold_D/0/1/0/all/0/1"&gt;Douglas L. Arnold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arbel_T/0/1/0/all/0/1"&gt;Tal Arbel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local Temperature Scaling for Probability Calibration. (arXiv:2008.05105v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.05105</id>
        <link href="http://arxiv.org/abs/2008.05105"/>
        <updated>2021-07-28T02:02:33.038Z</updated>
        <summary type="html"><![CDATA[For semantic segmentation, label probabilities are often uncalibrated as they
are typically only the by-product of a segmentation task. Intersection over
Union (IoU) and Dice score are often used as criteria for segmentation success,
while metrics related to label probabilities are not often explored. However,
probability calibration approaches have been studied, which match probability
outputs with experimentally observed errors. These approaches mainly focus on
classification tasks, but not on semantic segmentation. Thus, we propose a
learning-based calibration method that focuses on multi-label semantic
segmentation. Specifically, we adopt a convolutional neural network to predict
local temperature values for probability calibration. One advantage of our
approach is that it does not change prediction accuracy, hence allowing for
calibration as a post-processing step. Experiments on the COCO, CamVid, and
LPBA40 datasets demonstrate improved calibration performance for a range of
different metrics. We also demonstrate the good performance of our method for
multi-atlas brain segmentation from magnetic resonance images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1"&gt;Zhipeng Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Peirong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niethammer_M/0/1/0/all/0/1"&gt;Marc Niethammer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Reinforcement Learning for L3 Slice Localization in Sarcopenia Assessment. (arXiv:2107.12800v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12800</id>
        <link href="http://arxiv.org/abs/2107.12800"/>
        <updated>2021-07-28T02:02:33.031Z</updated>
        <summary type="html"><![CDATA[Sarcopenia is a medical condition characterized by a reduction in muscle mass
and function. A quantitative diagnosis technique consists of localizing the CT
slice passing through the middle of the third lumbar area (L3) and segmenting
muscles at this level. In this paper, we propose a deep reinforcement learning
method for accurate localization of the L3 CT slice. Our method trains a
reinforcement learning agent by incentivizing it to discover the right
position. Specifically, a Deep Q-Network is trained to find the best policy to
follow for this problem. Visualizing the training process shows that the agent
mimics the scrolling of an experienced radiologist. Extensive experiments
against other state-of-the-art deep learning based methods for L3 localization
prove the superiority of our technique which performs well even with limited
amount of data and annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Laousy_O/0/1/0/all/0/1"&gt;Othmane Laousy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chassagnon_G/0/1/0/all/0/1"&gt;Guillaume Chassagnon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oyallon_E/0/1/0/all/0/1"&gt;Edouard Oyallon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paragios_N/0/1/0/all/0/1"&gt;Nikos Paragios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Revel_M/0/1/0/all/0/1"&gt;Marie-Pierre Revel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1"&gt;Maria Vakalopoulou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Learning to Classify Macromolecular Structures in situ for Less Supervision in Cryo-Electron Tomography. (arXiv:2102.12040v2 [q-bio.QM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12040</id>
        <link href="http://arxiv.org/abs/2102.12040"/>
        <updated>2021-07-28T02:02:33.023Z</updated>
        <summary type="html"><![CDATA[Motivation: Cryo-Electron Tomography (cryo-ET) is a 3D bioimaging tool that
visualizes the structural and spatial organization of macromolecules at a
near-native state in single cells, which has broad applications in life
science. However, the systematic structural recognition and recovery of
macromolecules captured by cryo-ET are difficult due to high structural
complexity and imaging limits. Deep learning based subtomogram classification
have played critical roles for such tasks. As supervised approaches, however,
their performance relies on sufficient and laborious annotation on a large
training dataset.

Results: To alleviate this major labeling burden, we proposed a Hybrid Active
Learning (HAL) framework for querying subtomograms for labelling from a large
unlabeled subtomogram pool. Firstly, HAL adopts uncertainty sampling to select
the subtomograms that have the most uncertain predictions. Moreover, to
mitigate the sampling bias caused by such strategy, a discriminator is
introduced to judge if a certain subtomogram is labeled or unlabeled and
subsequently the model queries the subtomogram that have higher probabilities
to be unlabeled. Additionally, HAL introduces a subset sampling strategy to
improve the diversity of the query set, so that the information overlap is
decreased between the queried batches and the algorithmic efficiency is
improved. Our experiments on subtomogram classification tasks using both
simulated and real data demonstrate that we can achieve comparable testing
performance (on average only 3% accuracy drop) by using less than 30% of the
labeled subtomograms, which shows a very promising result for subtomogram
classification task with limited labeling resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Du_X/0/1/0/all/0/1"&gt;Xuefeng Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haohan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhenxi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xiangrui Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Chang_Y/0/1/0/all/0/1"&gt;Yi-Wei Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Xu_M/0/1/0/all/0/1"&gt;Min Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Reputation Mechanism Is All You Need: Collaborative Fairness and Adversarial Robustness in Federated Learning. (arXiv:2011.10464v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10464</id>
        <link href="http://arxiv.org/abs/2011.10464"/>
        <updated>2021-07-28T02:02:33.007Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) is an emerging practical framework for effective and
scalable machine learning among multiple participants, such as end users,
organizations and companies. However, most existing FL or distributed learning
frameworks have not well addressed two important issues together: collaborative
fairness and adversarial robustness (e.g. free-riders and malicious
participants). In conventional FL, all participants receive the global model
(equal rewards), which might be unfair to the high-contributing participants.
Furthermore, due to the lack of a safeguard mechanism, free-riders or malicious
adversaries could game the system to access the global model for free or to
sabotage it. In this paper, we propose a novel Robust and Fair Federated
Learning (RFFL) framework to achieve collaborative fairness and adversarial
robustness simultaneously via a reputation mechanism. RFFL maintains a
reputation for each participant by examining their contributions via their
uploaded gradients (using vector similarity) and thus identifies
non-contributing or malicious participants to be removed. Our approach
differentiates itself by not requiring any auxiliary/validation dataset.
Extensive experiments on benchmark datasets show that RFFL can achieve high
fairness and is very robust to different types of adversaries while achieving
competitive predictive accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xinyi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1"&gt;Lingjuan Lyu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Circular-Symmetric Correlation Layer based on FFT. (arXiv:2107.12480v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12480</id>
        <link href="http://arxiv.org/abs/2107.12480"/>
        <updated>2021-07-28T02:02:33.000Z</updated>
        <summary type="html"><![CDATA[Despite the vast success of standard planar convolutional neural networks,
they are not the most efficient choice for analyzing signals that lie on an
arbitrarily curved manifold, such as a cylinder. The problem arises when one
performs a planar projection of these signals and inevitably causes them to be
distorted or broken where there is valuable information. We propose a
Circular-symmetric Correlation Layer (CCL) based on the formalism of
roto-translation equivariant correlation on the continuous group $S^1 \times
\mathbb{R}$, and implement it efficiently using the well-known Fast Fourier
Transform (FFT) algorithm. We showcase the performance analysis of a general
network equipped with CCL on various recognition and classification tasks and
datasets. The PyTorch package implementation of CCL is provided online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Azari_B/0/1/0/all/0/1"&gt;Bahar Azari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdogmus_D/0/1/0/all/0/1"&gt;Deniz Erdogmus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proof: Accelerating Approximate Aggregation Queries with Expensive Predicates. (arXiv:2107.12525v1 [math.ST])]]></title>
        <id>http://arxiv.org/abs/2107.12525</id>
        <link href="http://arxiv.org/abs/2107.12525"/>
        <updated>2021-07-28T02:02:32.982Z</updated>
        <summary type="html"><![CDATA[Given a dataset $\mathcal{D}$, we are interested in computing the mean of a
subset of $\mathcal{D}$ which matches a predicate. \algname leverages
stratified sampling and proxy models to efficiently compute this statistic
given a sampling budget $N$. In this document, we theoretically analyze
\algname and show that the MSE of the estimate decays at rate $O(N_1^{-1} +
N_2^{-1} + N_1^{1/2}N_2^{-3/2})$, where $N=K \cdot N_1+N_2$ for some integer
constant $K$ and $K \cdot N_1$ and $N_2$ represent the number of samples used
in Stage 1 and Stage 2 of \algname respectively. Hence, if a constant fraction
of the total sample budget $N$ is allocated to each stage, we will achieve a
mean squared error of $O(N^{-1})$ which matches the rate of mean squared error
of the optimal stratified sampling algorithm given a priori knowledge of the
predicate positive rate and standard deviation per stratum.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Kang_D/0/1/0/all/0/1"&gt;Daniel Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Guibas_J/0/1/0/all/0/1"&gt;John Guibas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Bailis_P/0/1/0/all/0/1"&gt;Peter Bailis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hashimoto_T/0/1/0/all/0/1"&gt;Tatsunori Hashimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yi Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zaharia_M/0/1/0/all/0/1"&gt;Matei Zaharia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CKNet: A Convolutional Neural Network Based on Koopman Operator for Modeling Latent Dynamics from Pixels. (arXiv:2102.10205v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10205</id>
        <link href="http://arxiv.org/abs/2102.10205"/>
        <updated>2021-07-28T02:02:32.974Z</updated>
        <summary type="html"><![CDATA[With the development of end-to-end control based on deep learning, it is
important to study new system modeling techniques to realize dynamics modeling
with high-dimensional inputs. In this paper, a novel Koopman-based deep
convolutional network, called CKNet, is proposed to identify latent dynamics
from raw pixels. CKNet learns an encoder and decoder to play the role of the
Koopman eigenfunctions and modes, respectively. The Koopman eigenvalues can be
approximated by eigenvalues of the learned state transition matrix. The
deterministic convolutional Koopman network (DCKNet) and the variational
convolutional Koopman network (VCKNet) are proposed to span some subspace for
approximating the Koopman operator respectively. Because CKNet is trained under
the constraints of the Koopman theory, the identified latent dynamics is in a
linear form and has good interpretability. Besides, the state transition and
control matrices are trained as trainable tensors so that the identified
dynamics is also time-invariant. We also design an auxiliary weight term for
reducing multi-step linearity and prediction losses. Experiments were conducted
on two offline trained and four online trained nonlinear forced dynamical
systems with continuous action spaces in Gym and Mujoco environment
respectively, and the results show that identified dynamics are adequate for
approximating the latent dynamics and generating clear images. Especially for
offline trained cases, this work confirms CKNet from a novel perspective that
we visualize the evolutionary processes of the latent states and the Koopman
eigenfunctions with DCKNet and VCKNet separately to each task based on the same
episode and results demonstrate that different approaches learn similar
features in shapes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xiao_Y/0/1/0/all/0/1"&gt;Yongqian Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lin_Q/0/1/0/all/0/1"&gt;QianLi Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimating Parkinsonism Severity in Natural Gait Videos of Older Adults with Dementia. (arXiv:2105.03464v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03464</id>
        <link href="http://arxiv.org/abs/2105.03464"/>
        <updated>2021-07-28T02:02:32.967Z</updated>
        <summary type="html"><![CDATA[Drug-induced parkinsonism affects many older adults with dementia, often
causing gait disturbances. New advances in vision-based human pose-estimation
have opened possibilities for frequent and unobtrusive analysis of gait in
residential settings. This work proposes novel spatial-temporal graph
convolutional network (ST-GCN) architectures and training procedures to predict
clinical scores of parkinsonism in gait from video of individuals with
dementia. We propose a two-stage training approach consisting of a
self-supervised pretraining stage that encourages the ST-GCN model to learn
about gait patterns before predicting clinical scores in the finetuning stage.
The proposed ST-GCN models are evaluated on joint trajectories extracted from
video and are compared against traditional (ordinal, linear, random forest)
regression models and temporal convolutional network baselines. Three 2D human
pose-estimation libraries (OpenPose, Detectron, AlphaPose) and the Microsoft
Kinect (2D and 3D) are used to extract joint trajectories of 4787 natural
walking bouts from 53 older adults with dementia. A subset of 399 walks from 14
participants is annotated with scores of parkinsonism severity on the gait
criteria of the Unified Parkinson's Disease Rating Scale (UPDRS) and the
Simpson-Angus Scale (SAS). Our results demonstrate that ST-GCN models operating
on 3D joint trajectories extracted from the Kinect consistently outperform all
other models and feature sets. Prediction of parkinsonism scores in natural
walking bouts of unseen participants remains a challenging task, with the best
models achieving macro-averaged F1-scores of 0.53 +/- 0.03 and 0.40 +/- 0.02
for UPDRS-gait and SAS-gait, respectively. Pre-trained model and demo code for
this work is available:
https://github.com/TaatiTeam/stgcn_parkinsonism_prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sabo_A/0/1/0/all/0/1"&gt;Andrea Sabo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehdizadeh_S/0/1/0/all/0/1"&gt;Sina Mehdizadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iaboni_A/0/1/0/all/0/1"&gt;Andrea Iaboni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taati_B/0/1/0/all/0/1"&gt;Babak Taati&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effects of Image Size on Deep Learning. (arXiv:2101.11508v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11508</id>
        <link href="http://arxiv.org/abs/2101.11508"/>
        <updated>2021-07-28T02:02:32.957Z</updated>
        <summary type="html"><![CDATA[This paper presents the evaluation of effects of image size on deep learning
performance via semantic segmentation of magnetic resonance heart images with
U-net for fully automated quantification of myocardial infarction. Both
non-extra pixel and extra pixel interpolation algorithms are used to change the
size of images in datasets of interest. Extra class labels, in interpolated
ground truth segmentation images, are removed using thresholding, median
filtering, and subtraction strategies. Common class metrics are used to
evaluate the quality of semantic segmentation with U-net against the ground
truth segmentation while arbitrary threshold, comparison of the sums, and sums
of differences between medical experts and fully automated results are options
used to estimate the relationship between medical experts-based quantification
and fully automated quantification results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rukundo_O/0/1/0/all/0/1"&gt;Olivier Rukundo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Debiasing In-Sample Policy Performance for Small-Data, Large-Scale Optimization. (arXiv:2107.12438v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2107.12438</id>
        <link href="http://arxiv.org/abs/2107.12438"/>
        <updated>2021-07-28T02:02:32.948Z</updated>
        <summary type="html"><![CDATA[Motivated by the poor performance of cross-validation in settings where data
are scarce, we propose a novel estimator of the out-of-sample performance of a
policy in data-driven optimization.Our approach exploits the optimization
problem's sensitivity analysis to estimate the gradient of the optimal
objective value with respect to the amount of noise in the data and uses the
estimated gradient to debias the policy's in-sample performance. Unlike
cross-validation techniques, our approach avoids sacrificing data for a test
set, utilizes all data when training and, hence, is well-suited to settings
where data are scarce. We prove bounds on the bias and variance of our
estimator for optimization problems with uncertain linear objectives but known,
potentially non-convex, feasible regions. For more specialized optimization
problems where the feasible region is ``weakly-coupled" in a certain sense, we
prove stronger results. Specifically, we provide explicit high-probability
bounds on the error of our estimator that hold uniformly over a policy class
and depends on the problem's dimension and policy class's complexity. Our
bounds show that under mild conditions, the error of our estimator vanishes as
the dimension of the optimization problem grows, even if the amount of
available data remains small and constant. Said differently, we prove our
estimator performs well in the small-data, large-scale regime. Finally, we
numerically compare our proposed method to state-of-the-art approaches through
a case-study on dispatching emergency medical response services using real
data. Our method provides more accurate estimates of out-of-sample performance
and learns better-performing policies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Gupta_V/0/1/0/all/0/1"&gt;Vishal Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Huang_M/0/1/0/all/0/1"&gt;Michael Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Rusmevichientong_P/0/1/0/all/0/1"&gt;Paat Rusmevichientong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerated Gradient Descent Learning over Multiple Access Fading Channels. (arXiv:2107.12452v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12452</id>
        <link href="http://arxiv.org/abs/2107.12452"/>
        <updated>2021-07-28T02:02:32.929Z</updated>
        <summary type="html"><![CDATA[We consider a distributed learning problem in a wireless network, consisting
of N distributed edge devices and a parameter server (PS). The objective
function is a sum of the edge devices' local loss functions, who aim to train a
shared model by communicating with the PS over multiple access channels (MAC).
This problem has attracted a growing interest in distributed sensing systems,
and more recently in federated learning, known as over-the-air computation. In
this paper, we develop a novel Accelerated Gradient-descent Multiple Access
(AGMA) algorithm that uses momentum-based gradient signals over noisy fading
MAC to improve the convergence rate as compared to existing methods.
Furthermore, AGMA does not require power control or beamforming to cancel the
fading effect, which simplifies the implementation complexity. We analyze AGMA
theoretically, and establish a finite-sample bound of the error for both convex
and strongly convex loss functions with Lipschitz gradient. For the strongly
convex case, we show that AGMA approaches the best-known linear convergence
rate as the network increases. For the convex case, we show that AGMA
significantly improves the sub-linear convergence rate as compared to existing
methods. Finally, we present simulation results using real datasets that
demonstrate better performance by AGMA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_R/0/1/0/all/0/1"&gt;Raz Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Friedman_Y/0/1/0/all/0/1"&gt;Yuval Friedman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_K/0/1/0/all/0/1"&gt;Kobi Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constraining dark matter annihilation with cosmic ray antiprotons using neural networks. (arXiv:2107.12395v1 [astro-ph.HE])]]></title>
        <id>http://arxiv.org/abs/2107.12395</id>
        <link href="http://arxiv.org/abs/2107.12395"/>
        <updated>2021-07-28T02:02:32.922Z</updated>
        <summary type="html"><![CDATA[The interpretation of data from indirect detection experiments searching for
dark matter annihilations requires computationally expensive simulations of
cosmic-ray propagation. In this work we present a new method based on Recurrent
Neural Networks that significantly accelerates simulations of secondary and
dark matter Galactic cosmic ray antiprotons while achieving excellent accuracy.
This approach allows for an efficient profiling or marginalisation over the
nuisance parameters of a cosmic ray propagation model in order to perform
parameter scans for a wide range of dark matter models. We identify importance
sampling as particularly suitable for ensuring that the network is only
evaluated in well-trained parameter regions. We present resulting constraints
using the most recent AMS-02 antiproton data on several models of Weakly
Interacting Massive Particles. The fully trained networks are released as
DarkRayNet together with this work and achieve a speed-up of the runtime by at
least two orders of magnitude compared to conventional approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Kahlhoefer_F/0/1/0/all/0/1"&gt;Felix Kahlhoefer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Korsmeier_M/0/1/0/all/0/1"&gt;Michael Korsmeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Kramer_M/0/1/0/all/0/1"&gt;Michael Kr&amp;#xe4;mer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Manconi_S/0/1/0/all/0/1"&gt;Silvia Manconi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Nippel_K/0/1/0/all/0/1"&gt;Kathrin Nippel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Source-Agnostic Gravitational-Wave Detection with Recurrent Autoencoders. (arXiv:2107.12698v1 [gr-qc])]]></title>
        <id>http://arxiv.org/abs/2107.12698</id>
        <link href="http://arxiv.org/abs/2107.12698"/>
        <updated>2021-07-28T02:02:32.904Z</updated>
        <summary type="html"><![CDATA[We present an application of anomaly detection techniques based on deep
recurrent autoencoders to the problem of detecting gravitational wave signals
in laser interferometers. Trained on noise data, this class of algorithms could
detect signals using an unsupervised strategy, i.e., without targeting a
specific kind of source. We develop a custom architecture to analyze the data
from two interferometers. We compare the obtained performance to that obtained
with other autoencoder architectures and with a convolutional classifier. The
unsupervised nature of the proposed strategy comes with a cost in terms of
accuracy, when compared to more traditional supervised techniques. On the other
hand, there is a qualitative gain in generalizing the experimental sensitivity
beyond the ensemble of pre-computed signal templates. The recurrent autoencoder
outperforms other autoencoders based on different architectures. The class of
recurrent autoencoders presented in this paper could complement the search
strategy employed for gravitational wave detection and extend the reach of the
ongoing detection campaigns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/gr-qc/1/au:+Moreno_E/0/1/0/all/0/1"&gt;Eric A. Moreno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/gr-qc/1/au:+Vlimant_J/0/1/0/all/0/1"&gt;Jean-Roch Vlimant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/gr-qc/1/au:+Spiropulu_M/0/1/0/all/0/1"&gt;Maria Spiropulu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/gr-qc/1/au:+Borzyszkowski_B/0/1/0/all/0/1"&gt;Bartlomiej Borzyszkowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/gr-qc/1/au:+Pierini_M/0/1/0/all/0/1"&gt;Maurizio Pierini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometric Deep Learning on Molecular Representations. (arXiv:2107.12375v1 [physics.chem-ph])]]></title>
        <id>http://arxiv.org/abs/2107.12375</id>
        <link href="http://arxiv.org/abs/2107.12375"/>
        <updated>2021-07-28T02:02:32.889Z</updated>
        <summary type="html"><![CDATA[Geometric deep learning (GDL), which is based on neural network architectures
that incorporate and process symmetry information, has emerged as a recent
paradigm in artificial intelligence. GDL bears particular promise in molecular
modeling applications, in which various molecular representations with
different symmetry properties and levels of abstraction exist. This review
provides a structured and harmonized overview of molecular GDL, highlighting
its applications in drug discovery, chemical synthesis prediction, and quantum
chemistry. Emphasis is placed on the relevance of the learned molecular
features and their complementarity to well-established molecular descriptors.
This review provides an overview of current challenges and opportunities, and
presents a forecast of the future of GDL for molecular sciences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Atz_K/0/1/0/all/0/1"&gt;Kenneth Atz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Grisoni_F/0/1/0/all/0/1"&gt;Francesca Grisoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Schneider_G/0/1/0/all/0/1"&gt;Gisbert Schneider&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quaternion Generative Adversarial Networks. (arXiv:2104.09630v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09630</id>
        <link href="http://arxiv.org/abs/2104.09630"/>
        <updated>2021-07-28T02:02:32.872Z</updated>
        <summary type="html"><![CDATA[Latest Generative Adversarial Networks (GANs) are gathering outstanding
results through a large-scale training, thus employing models composed of
millions of parameters requiring extensive computational capabilities. Building
such huge models undermines their replicability and increases the training
instability. Moreover, multi-channel data, such as images or audio, are usually
processed by realvalued convolutional networks that flatten and concatenate the
input, often losing intra-channel spatial relations. To address these issues
related to complexity and information loss, we propose a family of
quaternion-valued generative adversarial networks (QGANs). QGANs exploit the
properties of quaternion algebra, e.g., the Hamilton product, that allows to
process channels as a single entity and capture internal latent relations,
while reducing by a factor of 4 the overall number of parameters. We show how
to design QGANs and to extend the proposed approach even to advanced models.We
compare the proposed QGANs with real-valued counterparts on several image
generation benchmarks. Results show that QGANs are able to obtain better FID
scores than real-valued GANs and to generate visually pleasing images.
Furthermore, QGANs save up to 75% of the training parameters. We believe these
results may pave the way to novel, more accessible, GANs capable of improving
performance and saving computational resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grassucci_E/0/1/0/all/0/1"&gt;Eleonora Grassucci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cicero_E/0/1/0/all/0/1"&gt;Edoardo Cicero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Comminiello_D/0/1/0/all/0/1"&gt;Danilo Comminiello&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Initial Foundation for Predicting Individual Earthquake's Location and Magnitude by Using Glass-Box Physics Rule Learner. (arXiv:2107.12915v1 [physics.geo-ph])]]></title>
        <id>http://arxiv.org/abs/2107.12915</id>
        <link href="http://arxiv.org/abs/2107.12915"/>
        <updated>2021-07-28T02:02:32.864Z</updated>
        <summary type="html"><![CDATA[Although researchers accumulated knowledge about seismogenesis and
decades-long earthquake data, predicting imminent individual earthquakes at a
specific time and location remains a long-standing enigma. This study
hypothesizes that the observed data conceal the hidden rules which may be
unraveled by a novel glass-box (as opposed to black-box) physics rule learner
(GPRL) framework. Without any predefined earthquake-related mechanisms or
statistical laws, GPRL's two essentials, convolved information index and
transparent link function, seek generic expressions of rules directly from
data. GPRL's training with 10-years data appears to identify plausible rules,
suggesting a combination of the pseudo power and the pseudo vorticity of
released energy in the lithosphere. Independent feasibility test supports the
promising role of the unraveled rules in predicting earthquakes' magnitudes and
their specific locations. The identified rules and GPRL are in their infancy
requiring substantial improvement. Still, this study hints at the existence of
the data-guided hidden pathway to imminent individual earthquake prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Cho_I/0/1/0/all/0/1"&gt;In Ho Cho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stability & Generalisation of Gradient Descent for Shallow Neural Networks without the Neural Tangent Kernel. (arXiv:2107.12723v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.12723</id>
        <link href="http://arxiv.org/abs/2107.12723"/>
        <updated>2021-07-28T02:02:32.857Z</updated>
        <summary type="html"><![CDATA[We revisit on-average algorithmic stability of Gradient Descent (GD) for
training overparameterised shallow neural networks and prove new generalisation
and excess risk bounds without the Neural Tangent Kernel (NTK) or
Polyak-{\L}ojasiewicz (PL) assumptions. In particular, we show oracle type
bounds which reveal that the generalisation and excess risk of GD is controlled
by an interpolating network with the shortest GD path from initialisation (in a
sense, an interpolating network with the smallest relative norm). While this
was known for kernelised interpolants, our proof applies directly to networks
trained by GD without intermediate kernelisation. At the same time, by relaxing
oracle inequalities developed here we recover existing NTK-based risk bounds in
a straightforward way, which demonstrates that our analysis is tighter.
Finally, unlike most of the NTK-based analyses we focus on regression with
label noise and show that GD with early stopping is consistent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Richards_D/0/1/0/all/0/1"&gt;Dominic Richards&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kuzborskij_I/0/1/0/all/0/1"&gt;Ilja Kuzborskij&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Self-Consistency for Deepfake Detection. (arXiv:2012.09311v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09311</id>
        <link href="http://arxiv.org/abs/2012.09311"/>
        <updated>2021-07-28T02:02:32.807Z</updated>
        <summary type="html"><![CDATA[We propose a new method to detect deepfake images using the cue of the source
feature inconsistency within the forged images. It is based on the hypothesis
that images' distinct source features can be preserved and extracted after
going through state-of-the-art deepfake generation processes. We introduce a
novel representation learning approach, called pair-wise self-consistency
learning (PCL), for training ConvNets to extract these source features and
detect deepfake images. It is accompanied by a new image synthesis approach,
called inconsistency image generator (I2G), to provide richly annotated
training data for PCL. Experimental results on seven popular datasets show that
our models improve averaged AUC over the state of the art from 96.45% to 98.05%
in the in-dataset evaluation and from 86.03% to 92.18% in the cross-dataset
evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tianchen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mingze Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1"&gt;Hui Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1"&gt;Yuanjun Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1"&gt;Wei Xia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Variational Models for Collaborative Filtering-based Recommender Systems. (arXiv:2107.12677v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.12677</id>
        <link href="http://arxiv.org/abs/2107.12677"/>
        <updated>2021-07-28T02:02:32.794Z</updated>
        <summary type="html"><![CDATA[Deep learning provides accurate collaborative filtering models to improve
recommender system results. Deep matrix factorization and their related
collaborative neural networks are the state-of-art in the field; nevertheless,
both models lack the necessary stochasticity to create the robust, continuous,
and structured latent spaces that variational autoencoders exhibit. On the
other hand, data augmentation through variational autoencoder does not provide
accurate results in the collaborative filtering field due to the high sparsity
of recommender systems. Our proposed models apply the variational concept to
inject stochasticity in the latent space of the deep architecture, introducing
the variational technique in the neural collaborative filtering field. This
method does not depend on the particular model used to generate the latent
representation. In this way, this approach can be applied as a plugin to any
current and future specific models. The proposed models have been tested using
four representative open datasets, three different quality measures, and
state-of-art baselines. The results show the superiority of the proposed
approach in scenarios where the variational enrichment exceeds the injected
noise effect. Additionally, a framework is provided to enable the
reproducibility of the conducted experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bobadilla_J/0/1/0/all/0/1"&gt;Jes&amp;#xfa;s Bobadilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ortega_F/0/1/0/all/0/1"&gt;Fernando Ortega&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gutierrez_A/0/1/0/all/0/1"&gt;Abraham Guti&amp;#xe9;rrez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Prieto_A/0/1/0/all/0/1"&gt;&amp;#xc1;ngel Gonz&amp;#xe1;lez-Prieto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EdgeNets:Edge Varying Graph Neural Networks. (arXiv:2001.07620v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.07620</id>
        <link href="http://arxiv.org/abs/2001.07620"/>
        <updated>2021-07-28T02:02:32.769Z</updated>
        <summary type="html"><![CDATA[Driven by the outstanding performance of neural networks in the structured
Euclidean domain, recent years have seen a surge of interest in developing
neural networks for graphs and data supported on graphs. The graph is leveraged
at each layer of the neural network as a parameterization to capture detail at
the node level with a reduced number of parameters and computational
complexity. Following this rationale, this paper puts forth a general framework
that unifies state-of-the-art graph neural networks (GNNs) through the concept
of EdgeNet. An EdgeNet is a GNN architecture that allows different nodes to use
different parameters to weigh the information of different neighbors. By
extrapolating this strategy to more iterations between neighboring nodes, the
EdgeNet learns edge- and neighbor-dependent weights to capture local detail.
This is a general linear and local operation that a node can perform and
encompasses under one formulation all existing graph convolutional neural
networks (GCNNs) as well as graph attention networks (GATs). In writing
different GNN architectures with a common language, EdgeNets highlight specific
architecture advantages and limitations, while providing guidelines to improve
their capacity without compromising their local implementation. An interesting
conclusion is the unification of GCNNs and GATs -- approaches that have been so
far perceived as separate. In particular, we show that GATs are GCNNs on a
graph that is learned from the features. This particularization opens the doors
to develop alternative attention mechanisms for improving discriminatory power.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Isufi_E/0/1/0/all/0/1"&gt;Elvin Isufi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gama_F/0/1/0/all/0/1"&gt;Fernando Gama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1"&gt;Alejandro Ribeiro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Estimate Hidden Motions with Global Motion Aggregation. (arXiv:2104.02409v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02409</id>
        <link href="http://arxiv.org/abs/2104.02409"/>
        <updated>2021-07-28T02:02:32.750Z</updated>
        <summary type="html"><![CDATA[Occlusions pose a significant challenge to optical flow algorithms that rely
on local evidences. We consider an occluded point to be one that is imaged in
the first frame but not in the next, a slight overloading of the standard
definition since it also includes points that move out-of-frame. Estimating the
motion of these points is extremely difficult, particularly in the two-frame
setting. Previous work relies on CNNs to learn occlusions, without much
success, or requires multiple frames to reason about occlusions using temporal
smoothness. In this paper, we argue that the occlusion problem can be better
solved in the two-frame case by modelling image self-similarities. We introduce
a global motion aggregation module, a transformer-based approach to find
long-range dependencies between pixels in the first image, and perform global
aggregation on the corresponding motion features. We demonstrate that the
optical flow estimates in the occluded regions can be significantly improved
without damaging the performance in non-occluded regions. This approach obtains
new state-of-the-art results on the challenging Sintel dataset, improving the
average end-point error by 13.6% on Sintel Final and 13.7% on Sintel Clean. At
the time of submission, our method ranks first on these benchmarks among all
published and unpublished approaches. Code is available at
https://github.com/zacjiang/GMA]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Shihao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Campbell_D/0/1/0/all/0/1"&gt;Dylan Campbell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongdong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1"&gt;Richard Hartley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Grounding with 3D Objects. (arXiv:2107.12514v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.12514</id>
        <link href="http://arxiv.org/abs/2107.12514"/>
        <updated>2021-07-28T02:02:32.743Z</updated>
        <summary type="html"><![CDATA[Seemingly simple natural language requests to a robot are generally
underspecified, for example "Can you bring me the wireless mouse?" When viewing
mice on the shelf, the number of buttons or presence of a wire may not be
visible from certain angles or positions. Flat images of candidate mice may not
provide the discriminative information needed for "wireless". The world, and
objects in it, are not flat images but complex 3D shapes. If a human requests
an object based on any of its basic properties, such as color, shape, or
texture, robots should perform the necessary exploration to accomplish the
task. In particular, while substantial effort and progress has been made on
understanding explicitly visual attributes like color and category,
comparatively little progress has been made on understanding language about
shapes and contours. In this work, we introduce a novel reasoning task that
targets both visual and non-visual language about 3D objects. Our new
benchmark, ShapeNet Annotated with Referring Expressions (SNARE), requires a
model to choose which of two objects is being referenced by a natural language
description. We introduce several CLIP-based models for distinguishing objects
and demonstrate that while recent advances in jointly modeling vision and
language are useful for robotic language understanding, it is still the case
that these models are weaker at understanding the 3D nature of objects --
properties which play a key role in manipulation. In particular, we find that
adding view estimation to language grounding models improves accuracy on both
SNARE and when identifying objects referred to in language on a robot platform.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1"&gt;Jesse Thomason&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shridhar_M/0/1/0/all/0/1"&gt;Mohit Shridhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1"&gt;Yonatan Bisk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1"&gt;Chris Paxton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1"&gt;Luke Zettlemoyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alleviate Representation Overlapping in Class Incremental Learning by Contrastive Class Concentration. (arXiv:2107.12308v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12308</id>
        <link href="http://arxiv.org/abs/2107.12308"/>
        <updated>2021-07-28T02:02:32.735Z</updated>
        <summary type="html"><![CDATA[The challenge of the Class Incremental Learning (CIL) lies in difficulty for
a learner to discern the old classes' data from the new while no previous data
is preserved. Namely, the representation distribution of different phases
overlaps with each other. In this paper, to alleviate the phenomenon of
representation overlapping for both memory-based and memory-free methods, we
propose a new CIL framework, Contrastive Class Concentration for CIL (C4IL).
Our framework leverages the class concentration effect of contrastive
representation learning, therefore yielding a representation distribution with
better intra-class compactibility and inter-class separability. Quantitative
experiments showcase our framework that is effective in both memory-based and
memory-free cases: it outperforms the baseline methods of both cases by 5% in
terms of the average and top-1 accuracy in 10-phase and 20-phase CIL.
Qualitative results also demonstrate that our method generates a more compact
representation distribution that alleviates the overlapping problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ni_Z/0/1/0/all/0/1"&gt;Zixuan Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1"&gt;Haizhou Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1"&gt;Siliang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1"&gt;Yueting Zhuang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Experiments on Properties of Hidden Structures of Sparse Neural Networks. (arXiv:2107.12917v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12917</id>
        <link href="http://arxiv.org/abs/2107.12917"/>
        <updated>2021-07-28T02:02:32.718Z</updated>
        <summary type="html"><![CDATA[Sparsity in the structure of Neural Networks can lead to less energy
consumption, less memory usage, faster computation times on convenient
hardware, and automated machine learning. If sparsity gives rise to certain
kinds of structure, it can explain automatically obtained features during
learning.

We provide insights into experiments in which we show how sparsity can be
achieved through prior initialization, pruning, and during learning, and answer
questions on the relationship between the structure of Neural Networks and
their performance. This includes the first work of inducing priors from network
theory into Recurrent Neural Networks and an architectural performance
prediction during a Neural Architecture Search. Within our experiments, we show
how magnitude class blinded pruning achieves 97.5% on MNIST with 80%
compression and re-training, which is 0.5 points more than without compression,
that magnitude class uniform pruning is significantly inferior to it and how a
genetic search enhanced with performance prediction achieves 82.4% on CIFAR10.
Further, performance prediction for Recurrent Networks learning the Reber
grammar shows an $R^2$ of up to 0.81 given only structural information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stier_J/0/1/0/all/0/1"&gt;Julian Stier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darji_H/0/1/0/all/0/1"&gt;Harshil Darji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Granitzer_M/0/1/0/all/0/1"&gt;Michael Granitzer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information criteria for non-normalized models. (arXiv:1905.05976v5 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.05976</id>
        <link href="http://arxiv.org/abs/1905.05976"/>
        <updated>2021-07-28T02:02:32.710Z</updated>
        <summary type="html"><![CDATA[Many statistical models are given in the form of non-normalized densities
with an intractable normalization constant. Since maximum likelihood estimation
is computationally intensive for these models, several estimation methods have
been developed which do not require explicit computation of the normalization
constant, such as noise contrastive estimation (NCE) and score matching.
However, model selection methods for general non-normalized models have not
been proposed so far. In this study, we develop information criteria for
non-normalized models estimated by NCE or score matching. They are
approximately unbiased estimators of discrepancy measures for non-normalized
models. Simulation results and applications to real data demonstrate that the
proposed criteria enable selection of the appropriate non-normalized model in a
data-driven manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Matsuda_T/0/1/0/all/0/1"&gt;Takeru Matsuda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Uehara_M/0/1/0/all/0/1"&gt;Masatoshi Uehara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hyvarinen_A/0/1/0/all/0/1"&gt;Aapo Hyvarinen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Short-Term Electricity Price Forecasting based on Graph Convolution Network and Attention Mechanism. (arXiv:2107.12794v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12794</id>
        <link href="http://arxiv.org/abs/2107.12794"/>
        <updated>2021-07-28T02:02:32.702Z</updated>
        <summary type="html"><![CDATA[In electricity markets, locational marginal price (LMP) forecasting is
particularly important for market participants in making reasonable bidding
strategies, managing potential trading risks, and supporting efficient system
planning and operation. Unlike existing methods that only consider LMPs'
temporal features, this paper tailors a spectral graph convolutional network
(GCN) to greatly improve the accuracy of short-term LMP forecasting. A
three-branch network structure is then designed to match the structure of LMPs'
compositions. Such kind of network can extract the spatial-temporal features of
LMPs, and provide fast and high-quality predictions for all nodes
simultaneously. The attention mechanism is also implemented to assign varying
importance weights between different nodes and time slots. Case studies based
on the IEEE-118 test system and real-world data from the PJM validate that the
proposed model outperforms existing forecasting models in accuracy, and
maintains a robust performance by avoiding extreme errors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yuyun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1"&gt;Zhenfei Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haitao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruan_G/0/1/0/all/0/1"&gt;Guangchun Ruan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1"&gt;Haiwang Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Local Recurrent Models for Human Mesh Recovery. (arXiv:2107.12847v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12847</id>
        <link href="http://arxiv.org/abs/2107.12847"/>
        <updated>2021-07-28T02:02:32.695Z</updated>
        <summary type="html"><![CDATA[We consider the problem of estimating frame-level full human body meshes
given a video of a person with natural motion dynamics. While much progress in
this field has been in single image-based mesh estimation, there has been a
recent uptick in efforts to infer mesh dynamics from video given its role in
alleviating issues such as depth ambiguity and occlusions. However, a key
limitation of existing work is the assumption that all the observed motion
dynamics can be modeled using one dynamical/recurrent model. While this may
work well in cases with relatively simplistic dynamics, inference with
in-the-wild videos presents many challenges. In particular, it is typically the
case that different body parts of a person undergo different dynamics in the
video, e.g., legs may move in a way that may be dynamically different from
hands (e.g., a person dancing). To address these issues, we present a new
method for video mesh recovery that divides the human mesh into several local
parts following the standard skeletal model. We then model the dynamics of each
local part with separate recurrent models, with each model conditioned
appropriately based on the known kinematic structure of the human body. This
results in a structure-informed local recurrent learning architecture that can
be trained in an end-to-end fashion with available annotations. We conduct a
variety of experiments on standard video mesh recovery benchmark datasets such
as Human3.6M, MPI-INF-3DHP, and 3DPW, demonstrating the efficacy of our design
of modeling local dynamics as well as establishing state-of-the-art results
based on standard evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Runze Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karanam_S/0/1/0/all/0/1"&gt;Srikrishna Karanam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ren Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Terrence Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhanu_B/0/1/0/all/0/1"&gt;Bir Bhanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Ziyan Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information fusion between knowledge and data in Bayesian network structure learning. (arXiv:2102.00473v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00473</id>
        <link href="http://arxiv.org/abs/2102.00473"/>
        <updated>2021-07-28T02:02:32.688Z</updated>
        <summary type="html"><![CDATA[Bayesian Networks (BNs) have become a powerful technology for reasoning under
uncertainty, particularly in areas that require causal assumptions that enable
us to simulate the effect of intervention. The graphical structure of these
models can be determined by causal knowledge, learnt from data, or a
combination of both. While it seems plausible that the best approach in
constructing a causal graph involves combining knowledge with machine learning,
this approach remains underused in practice. We implement and evaluate 10
knowledge approaches with application to different case studies and BN
structure learning algorithms available in the open-source Bayesys structure
learning system. The approaches enable us to specify pre-existing knowledge
that can be obtained from heterogeneous sources, to constrain or guide
structure learning. Each approach is assessed in terms of structure learning
effectiveness and efficiency, including graphical accuracy, model fitting,
complexity, and runtime; making this the first paper that provides a
comparative evaluation of a wide range of knowledge approaches for BN structure
learning. Because the value of knowledge depends on what data are available, we
illustrate the results both with limited and big data. While the overall
results show that knowledge becomes less important with big data due to higher
learning accuracy rendering knowledge less important, some of the knowledge
approaches are actually found to be more important with big data. Amongst the
main conclusions is the observation that reduced search space obtained from
knowledge does not always imply reduced computational complexity, perhaps
because the relationships implied by the data and knowledge are in tension.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Constantinou_A/0/1/0/all/0/1"&gt;Anthony C. Constantinou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1"&gt;Zhigao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kitson_N/0/1/0/all/0/1"&gt;Neville K. Kitson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model Free Barrier Functions via Implicit Evading Maneuvers. (arXiv:2107.12871v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12871</id>
        <link href="http://arxiv.org/abs/2107.12871"/>
        <updated>2021-07-28T02:02:32.679Z</updated>
        <summary type="html"><![CDATA[This paper demonstrates that in some cases the safety override arising from
the use of a barrier function can be needlessly restrictive. In particular, we
examine the case of fixed wing collision avoidance and show that when using a
barrier function, there are cases where two fixed wing aircraft can come closer
to colliding than if there were no barrier function at all. In addition, we
construct cases where the barrier function labels the system as unsafe even
when the vehicles start arbitrarily far apart. In other words, the barrier
function ensures safety but with unnecessary costs to performance. We therefore
introduce model free barrier functions which take a data driven approach to
creating a barrier function. We demonstrate the effectiveness of model free
barrier functions in a collision avoidance simulation of two fixed-wing
aircraft.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Squires_E/0/1/0/all/0/1"&gt;Eric Squires&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konda_R/0/1/0/all/0/1"&gt;Rohit Konda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coogan_S/0/1/0/all/0/1"&gt;Samuel Coogan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Egerstedt_M/0/1/0/all/0/1"&gt;Magnus Egerstedt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COPS: Controlled Pruning Before Training Starts. (arXiv:2107.12673v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12673</id>
        <link href="http://arxiv.org/abs/2107.12673"/>
        <updated>2021-07-28T02:02:32.672Z</updated>
        <summary type="html"><![CDATA[State-of-the-art deep neural network (DNN) pruning techniques, applied
one-shot before training starts, evaluate sparse architectures with the help of
a single criterion -- called pruning score. Pruning weights based on a solitary
score works well for some architectures and pruning rates but may also fail for
other ones. As a common baseline for pruning scores, we introduce the notion of
a generalized synaptic score (GSS). In this work we do not concentrate on a
single pruning criterion, but provide a framework for combining arbitrary GSSs
to create more powerful pruning strategies. These COmbined Pruning Scores
(COPS) are obtained by solving a constrained optimization problem. Optimizing
for more than one score prevents the sparse network to overly specialize on an
individual task, thus COntrols Pruning before training Starts. The
combinatorial optimization problem given by COPS is relaxed on a linear program
(LP). This LP is solved analytically and determines a solution for COPS.
Furthermore, an algorithm to compute it for two scores numerically is proposed
and evaluated. Solving COPS in such a way has lower complexity than the best
general LP solver. In our experiments we compared pruning with COPS against
state-of-the-art methods for different network architectures and image
classification tasks and obtained improved results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wimmer_P/0/1/0/all/0/1"&gt;Paul Wimmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehnert_J/0/1/0/all/0/1"&gt;Jens Mehnert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Condurache_A/0/1/0/all/0/1"&gt;Alexandru Condurache&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-Constrained Deep Learning Approaches for Inverse Problems. (arXiv:2105.12033v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12033</id>
        <link href="http://arxiv.org/abs/2105.12033"/>
        <updated>2021-07-28T02:02:32.631Z</updated>
        <summary type="html"><![CDATA[Deep Learning (DL), in particular deep neural networks (DNN), by design is
purely data-driven and in general does not require physics. This is the
strength of DL but also one of its key limitations when applied to science and
engineering problems in which underlying physical properties (such as
stability, conservation, and positivity) and desired accuracy need to be
achieved. DL methods in their original forms are not capable of respecting the
underlying mathematical models or achieving desired accuracy even in big-data
regimes. On the other hand, many data-driven science and engineering problems,
such as inverse problems, typically have limited experimental or observational
data, and DL would overfit the data in this case. Leveraging information
encoded in the underlying mathematical models, we argue, not only compensates
missing information in low data regimes but also provides opportunities to
equip DL methods with the underlying physics and hence obtaining higher
accuracy. This short communication introduces several model-constrained DL
approaches (including both feed-forward DNN and autoencoders) that are capable
of learning not only information hidden in the training data but also in the
underlying mathematical models to solve inverse problems. We present and
provide intuitions for our formulations for general nonlinear problems. For
linear inverse problems and linear networks, the first order optimality
conditions show that our model-constrained DL approaches can learn information
encoded in the underlying mathematical models, and thus can produce consistent
or equivalent inverse solutions, while naive purely data-based counterparts
cannot.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Hai V. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bui_Thanh_T/0/1/0/all/0/1"&gt;Tan Bui-Thanh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Entropy Maximization and Meta Classification for Out-Of-Distribution Detection in Semantic Segmentation. (arXiv:2012.06575v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06575</id>
        <link href="http://arxiv.org/abs/2012.06575"/>
        <updated>2021-07-28T02:02:32.609Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) for the semantic segmentation of images are
usually trained to operate on a predefined closed set of object classes. This
is in contrast to the "open world" setting where DNNs are envisioned to be
deployed to. From a functional safety point of view, the ability to detect
so-called "out-of-distribution" (OoD) samples, i.e., objects outside of a DNN's
semantic space, is crucial for many applications such as automated driving. A
natural baseline approach to OoD detection is to threshold on the pixel-wise
softmax entropy. We present a two-step procedure that significantly improves
that approach. Firstly, we utilize samples from the COCO dataset as OoD proxy
and introduce a second training objective to maximize the softmax entropy on
these samples. Starting from pretrained semantic segmentation networks we
re-train a number of DNNs on different in-distribution datasets and
consistently observe improved OoD detection performance when evaluating on
completely disjoint OoD datasets. Secondly, we perform a transparent
post-processing step to discard false positive OoD samples by so-called "meta
classification". To this end, we apply linear models to a set of hand-crafted
metrics derived from the DNN's softmax probabilities. In our experiments we
consistently observe a clear additional gain in OoD detection performance,
cutting down the number of detection errors by up to 52% when comparing the
best baseline with our results. We achieve this improvement sacrificing only
marginally in original segmentation performance. Therefore, our method
contributes to safer DNNs with more reliable overall system performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1"&gt;Robin Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rottmann_M/0/1/0/all/0/1"&gt;Matthias Rottmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gottschalk_H/0/1/0/all/0/1"&gt;Hanno Gottschalk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual-Path Convolutional Image-Text Embeddings with Instance Loss. (arXiv:1711.05535v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1711.05535</id>
        <link href="http://arxiv.org/abs/1711.05535"/>
        <updated>2021-07-28T02:02:32.601Z</updated>
        <summary type="html"><![CDATA[Matching images and sentences demands a fine understanding of both
modalities. In this paper, we propose a new system to discriminatively embed
the image and text to a shared visual-textual space. In this field, most
existing works apply the ranking loss to pull the positive image / text pairs
close and push the negative pairs apart from each other. However, directly
deploying the ranking loss is hard for network learning, since it starts from
the two heterogeneous features to build inter-modal relationship. To address
this problem, we propose the instance loss which explicitly considers the
intra-modal data distribution. It is based on an unsupervised assumption that
each image / text group can be viewed as a class. So the network can learn the
fine granularity from every image/text group. The experiment shows that the
instance loss offers better weight initialization for the ranking loss, so that
more discriminative embeddings can be learned. Besides, existing works usually
apply the off-the-shelf features, i.e., word2vec and fixed visual feature. So
in a minor contribution, this paper constructs an end-to-end dual-path
convolutional network to learn the image and text representations. End-to-end
learning allows the system to directly learn from the data and fully utilize
the supervision. On two generic retrieval datasets (Flickr30k and MSCOCO),
experiments demonstrate that our method yields competitive accuracy compared to
state-of-the-art methods. Moreover, in language based person retrieval, we
improve the state of the art by a large margin. The code has been made publicly
available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhedong Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Liang Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garrett_M/0/1/0/all/0/1"&gt;Michael Garrett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mingliang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yi-Dong Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangle Your Dense Object Detector. (arXiv:2107.02963v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02963</id>
        <link href="http://arxiv.org/abs/2107.02963"/>
        <updated>2021-07-28T02:02:32.594Z</updated>
        <summary type="html"><![CDATA[Deep learning-based dense object detectors have achieved great success in the
past few years and have been applied to numerous multimedia applications such
as video understanding. However, the current training pipeline for dense
detectors is compromised to lots of conjunctions that may not hold. In this
paper, we investigate three such important conjunctions: 1) only samples
assigned as positive in classification head are used to train the regression
head; 2) classification and regression share the same input feature and
computational fields defined by the parallel head architecture; and 3) samples
distributed in different feature pyramid layers are treated equally when
computing the loss. We first carry out a series of pilot experiments to show
disentangling such conjunctions can lead to persistent performance improvement.
Then, based on these findings, we propose Disentangled Dense Object Detector
(DDOD), in which simple and effective disentanglement mechanisms are designed
and integrated into the current state-of-the-art dense object detectors.
Extensive experiments on MS COCO benchmark show that our approach can lead to
2.0 mAP, 2.4 mAP and 2.2 mAP absolute improvements on RetinaNet, FCOS, and ATSS
baselines with negligible extra overhead. Notably, our best model reaches 55.0
mAP on the COCO test-dev set and 93.5 AP on the hard subset of WIDER FACE,
achieving new state-of-the-art performance on these two competitive benchmarks.
Code is available at https://github.com/zehuichen123/DDOD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zehui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chenhongyi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qiaofei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1"&gt;Feng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zheng-Jun Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Feng Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weight Reparametrization for Budget-Aware Network Pruning. (arXiv:2107.03909v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03909</id>
        <link href="http://arxiv.org/abs/2107.03909"/>
        <updated>2021-07-28T02:02:32.587Z</updated>
        <summary type="html"><![CDATA[Pruning seeks to design lightweight architectures by removing redundant
weights in overparameterized networks. Most of the existing techniques first
remove structured sub-networks (filters, channels,...) and then fine-tune the
resulting networks to maintain a high accuracy. However, removing a whole
structure is a strong topological prior and recovering the accuracy, with
fine-tuning, is highly cumbersome. In this paper, we introduce an "end-to-end"
lightweight network design that achieves training and pruning simultaneously
without fine-tuning. The design principle of our method relies on
reparametrization that learns not only the weights but also the topological
structure of the lightweight sub-network. This reparametrization acts as a
prior (or regularizer) that defines pruning masks implicitly from the weights
of the underlying network, without increasing the number of training
parameters. Sparsity is induced with a budget loss that provides an accurate
pruning. Extensive experiments conducted on the CIFAR10 and the TinyImageNet
datasets, using standard architectures (namely Conv4, VGG19 and ResNet18), show
compelling results without fine-tuning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dupont_R/0/1/0/all/0/1"&gt;Robin Dupont&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahbi_H/0/1/0/all/0/1"&gt;Hichem Sahbi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michel_G/0/1/0/all/0/1"&gt;Guillaume Michel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sharp U-Net: Depthwise Convolutional Network for Biomedical Image Segmentation. (arXiv:2107.12461v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.12461</id>
        <link href="http://arxiv.org/abs/2107.12461"/>
        <updated>2021-07-28T02:02:32.568Z</updated>
        <summary type="html"><![CDATA[The U-Net architecture, built upon the fully convolutional network, has
proven to be effective in biomedical image segmentation. However, U-Net applies
skip connections to merge semantically different low- and high-level
convolutional features, resulting in not only blurred feature maps, but also
over- and under-segmented target regions. To address these limitations, we
propose a simple, yet effective end-to-end depthwise encoder-decoder fully
convolutional network architecture, called Sharp U-Net, for binary and
multi-class biomedical image segmentation. The key rationale of Sharp U-Net is
that instead of applying a plain skip connection, a depthwise convolution of
the encoder feature map with a sharpening kernel filter is employed prior to
merging the encoder and decoder features, thereby producing a sharpened
intermediate feature map of the same size as the encoder map. Using this
sharpening filter layer, we are able to not only fuse semantically less
dissimilar features, but also to smooth out artifacts throughout the network
layers during the early stages of training. Our extensive experiments on six
datasets show that the proposed Sharp U-Net model consistently outperforms or
matches the recent state-of-the-art baselines in both binary and multi-class
segmentation tasks, while adding no extra learnable parameters. Furthermore,
Sharp U-Net outperforms baselines that have more than three times the number of
learnable parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zunair_H/0/1/0/all/0/1"&gt;Hasib Zunair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hamza_A/0/1/0/all/0/1"&gt;A. Ben Hamza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved-Mask R-CNN: Towards an Accurate Generic MSK MRI instance segmentation platform (Data from the Osteoarthritis Initiative). (arXiv:2107.12889v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.12889</id>
        <link href="http://arxiv.org/abs/2107.12889"/>
        <updated>2021-07-28T02:02:32.560Z</updated>
        <summary type="html"><![CDATA[Objective assessment of Magnetic Resonance Imaging (MRI) scans of
osteoarthritis (OA) can address the limitation of the current OA assessment.
Segmentation of bone, cartilage, and joint fluid is necessary for the OA
objective assessment. Most of the proposed segmentation methods are not
performing instance segmentation and suffer from class imbalance problems. This
study deployed Mask R-CNN instance segmentation and improved it (improved-Mask
R-CNN (iMaskRCNN)) to obtain a more accurate generalized segmentation for
OA-associated tissues. Training and validation of the method were performed
using 500 MRI knees from the Osteoarthritis Initiative (OAI) dataset and 97 MRI
scans of patients with symptomatic hip OA. Three modifications to Mask R-CNN
yielded the iMaskRCNN: adding a 2nd ROIAligned block, adding an extra decoder
layer to the mask-header, and connecting them by a skip connection. The results
were assessed using Hausdorff distance, dice score, and coefficients of
variation (CoV). The iMaskRCNN led to improved bone and cartilage segmentation
compared to Mask RCNN as indicated with the increase in dice score from 95% to
98% for the femur, 95% to 97% for tibia, 71% to 80% for femoral cartilage, and
81% to 82% for tibial cartilage. For the effusion detection, dice improved with
iMaskRCNN 72% versus MaskRCNN 71%. The CoV values for effusion detection
between Reader1 and Mask R-CNN (0.33), Reader1 and iMaskRCNN (0.34), Reader2
and Mask R-CNN (0.22), Reader2 and iMaskRCNN (0.29) are close to CoV between
two readers (0.21), indicating a high agreement between the human readers and
both Mask R-CNN and iMaskRCNN. Mask R-CNN and iMaskRCNN can reliably and
simultaneously extract different scale articular tissues involved in OA,
forming the foundation for automated assessment of OA. The iMaskRCNN results
show that the modification improved the network performance around the edges.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Felfeliyan_B/0/1/0/all/0/1"&gt;Banafshe Felfeliyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hareendranathan_A/0/1/0/all/0/1"&gt;Abhilash Hareendranathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kuntze_G/0/1/0/all/0/1"&gt;Gregor Kuntze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jaremko_J/0/1/0/all/0/1"&gt;Jacob L. Jaremko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ronsky_J/0/1/0/all/0/1"&gt;Janet L. Ronsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comprehensive Validation of Automated Whole Body Skeletal Muscle, Adipose Tissue, and Bone Segmentation from 3D CT images for Body Composition Analysis: Towards Extended Body Composition. (arXiv:2106.00652v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00652</id>
        <link href="http://arxiv.org/abs/2106.00652"/>
        <updated>2021-07-28T02:02:32.553Z</updated>
        <summary type="html"><![CDATA[The latest advances in computer-assisted precision medicine are making it
feasible to move from population-wide models that are useful to discover
aggregate patterns that hold for group-based analysis to patient-specific
models that can drive patient-specific decisions with regard to treatment
choices, and predictions of outcomes of treatment. Body Composition is
recognized as an important driver and risk factor for a wide variety of
diseases, as well as a predictor of individual patient-specific clinical
outcomes to treatment choices or surgical interventions. 3D CT images are
routinely acquired in the oncological worklows and deliver accurate rendering
of internal anatomy and therefore can be used opportunistically to assess the
amount of skeletal muscle and adipose tissue compartments. Powerful tools of
artificial intelligence such as deep learning are making it feasible now to
segment the entire 3D image and generate accurate measurements of all internal
anatomy. These will enable the overcoming of the severe bottleneck that existed
previously, namely, the need for manual segmentation, which was prohibitive to
scale to the hundreds of 2D axial slices that made up a 3D volumetric image.
Automated tools such as presented here will now enable harvesting whole-body
measurements from 3D CT or MRI images, leading to a new era of discovery of the
drivers of various diseases based on individual tissue, organ volume, shape,
and functional status. These measurements were hitherto unavailable thereby
limiting the field to a very small and limited subset. These discoveries and
the potential to perform individual image segmentation with high speed and
accuracy are likely to lead to the incorporation of these 3D measures into
individual specific treatment planning models related to nutrition, aging,
chemotoxicity, surgery and survival after the onset of a major disease such as
cancer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1"&gt;Da Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chow_V/0/1/0/all/0/1"&gt;Vincent Chow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Popuri_K/0/1/0/all/0/1"&gt;Karteek Popuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beg_M/0/1/0/all/0/1"&gt;Mirza Faisal Beg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Path-Restore: Learning Network Path Selection for Image Restoration. (arXiv:1904.10343v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.10343</id>
        <link href="http://arxiv.org/abs/1904.10343"/>
        <updated>2021-07-28T02:02:32.545Z</updated>
        <summary type="html"><![CDATA[Very deep Convolutional Neural Networks (CNNs) have greatly improved the
performance on various image restoration tasks. However, this comes at a price
of increasing computational burden, hence limiting their practical usages. We
observe that some corrupted image regions are inherently easier to restore than
others since the distortion and content vary within an image. To leverage this,
we propose Path-Restore, a multi-path CNN with a pathfinder that can
dynamically select an appropriate route for each image region. We train the
pathfinder using reinforcement learning with a difficulty-regulated reward.
This reward is related to the performance, complexity and "the difficulty of
restoring a region". A policy mask is further investigated to jointly process
all the image regions. We conduct experiments on denoising and mixed
restoration tasks. The results show that our method achieves comparable or
superior performance to existing approaches with less computational cost. In
particular, Path-Restore is effective for real-world denoising, where the noise
distribution varies across different regions on a single image. Compared to the
state-of-the-art RIDNet, our method achieves comparable performance and runs
2.7x faster on the realistic Darmstadt Noise Dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1"&gt;Ke Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xintao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1"&gt;Chao Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xiaoou Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1"&gt;Chen Change Loy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ENHANCE (ENriching Health data by ANnotations of Crowd and Experts): A case study for skin lesion classification. (arXiv:2107.12734v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12734</id>
        <link href="http://arxiv.org/abs/2107.12734"/>
        <updated>2021-07-28T02:02:32.525Z</updated>
        <summary type="html"><![CDATA[We present ENHANCE, an open dataset with multiple annotations to complement
the existing ISIC and PH2 skin lesion classification datasets. This dataset
contains annotations of visual ABC (asymmetry, border, colour) features from
non-expert annotation sources: undergraduate students, crowd workers from
Amazon MTurk and classic image processing algorithms. In this paper we first
analyse the correlations between the annotations and the diagnostic label of
the lesion, as well as study the agreement between different annotation
sources. Overall we find weak correlations of non-expert annotations with the
diagnostic label, and low agreement between different annotation sources. We
then study multi-task learning (MTL) with the annotations as additional labels,
and show that non-expert annotations can improve (ensembles of)
state-of-the-art convolutional neural networks via MTL. We hope that our
dataset can be used in further research into multiple annotations and/or MTL.
All data and models are available on Github:
https://github.com/raumannsr/ENHANCE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raumanns_R/0/1/0/all/0/1"&gt;Ralf Raumanns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schouten_G/0/1/0/all/0/1"&gt;Gerard Schouten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joosten_M/0/1/0/all/0/1"&gt;Max Joosten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pluim_J/0/1/0/all/0/1"&gt;Josien P. W. Pluim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheplygina_V/0/1/0/all/0/1"&gt;Veronika Cheplygina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COPS: Controlled Pruning Before Training Starts. (arXiv:2107.12673v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12673</id>
        <link href="http://arxiv.org/abs/2107.12673"/>
        <updated>2021-07-28T02:02:32.519Z</updated>
        <summary type="html"><![CDATA[State-of-the-art deep neural network (DNN) pruning techniques, applied
one-shot before training starts, evaluate sparse architectures with the help of
a single criterion -- called pruning score. Pruning weights based on a solitary
score works well for some architectures and pruning rates but may also fail for
other ones. As a common baseline for pruning scores, we introduce the notion of
a generalized synaptic score (GSS). In this work we do not concentrate on a
single pruning criterion, but provide a framework for combining arbitrary GSSs
to create more powerful pruning strategies. These COmbined Pruning Scores
(COPS) are obtained by solving a constrained optimization problem. Optimizing
for more than one score prevents the sparse network to overly specialize on an
individual task, thus COntrols Pruning before training Starts. The
combinatorial optimization problem given by COPS is relaxed on a linear program
(LP). This LP is solved analytically and determines a solution for COPS.
Furthermore, an algorithm to compute it for two scores numerically is proposed
and evaluated. Solving COPS in such a way has lower complexity than the best
general LP solver. In our experiments we compared pruning with COPS against
state-of-the-art methods for different network architectures and image
classification tasks and obtained improved results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wimmer_P/0/1/0/all/0/1"&gt;Paul Wimmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehnert_J/0/1/0/all/0/1"&gt;Jens Mehnert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Condurache_A/0/1/0/all/0/1"&gt;Alexandru Condurache&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LEGATO: A LayerwisE Gradient AggregaTiOn Algorithm for Mitigating Byzantine Attacks in Federated Learning. (arXiv:2107.12490v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12490</id>
        <link href="http://arxiv.org/abs/2107.12490"/>
        <updated>2021-07-28T02:02:32.510Z</updated>
        <summary type="html"><![CDATA[Federated learning has arisen as a mechanism to allow multiple participants
to collaboratively train a model without sharing their data. In these settings,
participants (workers) may not trust each other fully; for instance, a set of
competitors may collaboratively train a machine learning model to detect fraud.
The workers provide local gradients that a central server uses to update a
global model. This global model can be corrupted when Byzantine workers send
malicious gradients, which necessitates robust methods for aggregating
gradients that mitigate the adverse effects of Byzantine inputs. Existing
robust aggregation algorithms are often computationally expensive and only
effective under strict assumptions. In this paper, we introduce LayerwisE
Gradient AggregatTiOn (LEGATO), an aggregation algorithm that is, by contrast,
scalable and generalizable. Informed by a study of layer-specific responses of
gradients to Byzantine attacks, LEGATO employs a dynamic gradient reweighing
scheme that is novel in its treatment of gradients based on layer-specific
robustness. We show that LEGATO is more computationally efficient than multiple
state-of-the-art techniques and more generally robust across a variety of
attack settings in practice. We also demonstrate LEGATO's benefits for gradient
descent convergence in the absence of an attack.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Varma_K/0/1/0/all/0/1"&gt;Kamala Varma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baracaldo_N/0/1/0/all/0/1"&gt;Nathalie Baracaldo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anwar_A/0/1/0/all/0/1"&gt;Ali Anwar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual-stream Network for Visual Recognition. (arXiv:2105.14734v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14734</id>
        <link href="http://arxiv.org/abs/2105.14734"/>
        <updated>2021-07-28T02:02:32.504Z</updated>
        <summary type="html"><![CDATA[Transformers with remarkable global representation capacities achieve
competitive results for visual tasks, but fail to consider high-level local
pattern information in input images. In this paper, we present a generic
Dual-stream Network (DS-Net) to fully explore the representation capacity of
local and global pattern features for image classification. Our DS-Net can
simultaneously calculate fine-grained and integrated features and efficiently
fuse them. Specifically, we propose an Intra-scale Propagation module to
process two different resolutions in each block and an Inter-Scale Alignment
module to perform information interaction across features at dual scales.
Besides, we also design a Dual-stream FPN (DS-FPN) to further enhance
contextual information for downstream dense predictions. Without bells and
whistles, the propsed DS-Net outperforms Deit-Small by 2.4% in terms of top-1
accuracy on ImageNet-1k and achieves state-of-the-art performance over other
Vision Transformers and ResNets. For object detection and instance
segmentation, DS-Net-Small respectively outperforms ResNet-50 by 6.4% and 5.5 %
in terms of mAP on MSCOCO 2017, and surpasses the previous state-of-the-art
scheme, which significantly demonstrates its potential to be a general backbone
in vision tasks. The code will be released soon.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mao_M/0/1/0/all/0/1"&gt;Mingyuan Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Renrui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Honghui Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1"&gt;Peng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Teli Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1"&gt;Yan Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1"&gt;Errui Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Baochang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Shumin Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vision-Guided Forecasting -- Visual Context for Multi-Horizon Time Series Forecasting. (arXiv:2107.12674v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12674</id>
        <link href="http://arxiv.org/abs/2107.12674"/>
        <updated>2021-07-28T02:02:32.496Z</updated>
        <summary type="html"><![CDATA[Autonomous driving gained huge traction in recent years, due to its potential
to change the way we commute. Much effort has been put into trying to estimate
the state of a vehicle. Meanwhile, learning to forecast the state of a vehicle
ahead introduces new capabilities, such as predicting dangerous situations.
Moreover, forecasting brings new supervision opportunities by learning to
predict richer a context, expressed by multiple horizons. Intuitively, a video
stream originated from a front-facing camera is necessary because it encodes
information about the upcoming road. Besides, historical traces of the
vehicle's states give more context. In this paper, we tackle multi-horizon
forecasting of vehicle states by fusing the two modalities. We design and
experiment with 3 end-to-end architectures that exploit 3D convolutions for
visual features extraction and 1D convolutions for features extraction from
speed and steering angle traces. To demonstrate the effectiveness of our
method, we perform extensive experiments on two publicly available real-world
datasets, Comma2k19 and the Udacity challenge. We show that we are able to
forecast a vehicle's state to various horizons, while outperforming the current
state-of-the-art results on the related task of driving state estimation. We
examine the contribution of vision features, and find that a model fed with
vision features achieves an error that is 56.6% and 66.9% of the error of a
model that doesn't use those features, on the Udacity and Comma2k19 datasets
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kosman_E/0/1/0/all/0/1"&gt;Eitan Kosman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1"&gt;Dotan Di Castro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clickbait Detection in YouTube Videos. (arXiv:2107.12791v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12791</id>
        <link href="http://arxiv.org/abs/2107.12791"/>
        <updated>2021-07-28T02:02:32.478Z</updated>
        <summary type="html"><![CDATA[YouTube videos often include captivating descriptions and intriguing
thumbnails designed to increase the number of views, and thereby increase the
revenue for the person who posted the video. This creates an incentive for
people to post clickbait videos, in which the content might deviate
significantly from the title, description, or thumbnail. In effect, users are
tricked into clicking on clickbait videos. In this research, we consider the
challenging problem of detecting clickbait YouTube videos. We experiment with
multiple state-of-the-art machine learning techniques using a variety of
textual features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gothankar_R/0/1/0/all/0/1"&gt;Ruchira Gothankar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Troia_F/0/1/0/all/0/1"&gt;Fabio Di Troia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1"&gt;Mark Stamp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Denoising via GainTuning. (arXiv:2107.12815v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12815</id>
        <link href="http://arxiv.org/abs/2107.12815"/>
        <updated>2021-07-28T02:02:32.472Z</updated>
        <summary type="html"><![CDATA[Deep convolutional neural networks (CNNs) for image denoising are usually
trained on large datasets. These models achieve the current state of the art,
but they have difficulties generalizing when applied to data that deviate from
the training distribution. Recent work has shown that it is possible to train
denoisers on a single noisy image. These models adapt to the features of the
test image, but their performance is limited by the small amount of information
used to train them. Here we propose "GainTuning", in which CNN models
pre-trained on large datasets are adaptively and selectively adjusted for
individual test images. To avoid overfitting, GainTuning optimizes a single
multiplicative scaling parameter (the "Gain") of each channel in the
convolutional layers of the CNN. We show that GainTuning improves
state-of-the-art CNNs on standard image-denoising benchmarks, boosting their
denoising performance on nearly every image in a held-out test set. These
adaptive improvements are even more substantial for test images differing
systematically from the training data, either in noise level or image type. We
illustrate the potential of adaptive denoising in a scientific application, in
which a CNN is trained on synthetic data, and tested on real
transmission-electron-microscope images. In contrast to the existing
methodology, GainTuning is able to faithfully reconstruct the structure of
catalytic nanoparticles from these data at extremely low signal-to-noise
ratios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohan_S/0/1/0/all/0/1"&gt;Sreyas Mohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vincent_J/0/1/0/all/0/1"&gt;Joshua L. Vincent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manzorro_R/0/1/0/all/0/1"&gt;Ramon Manzorro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crozier_P/0/1/0/all/0/1"&gt;Peter A. Crozier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simoncelli_E/0/1/0/all/0/1"&gt;Eero P. Simoncelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1"&gt;Carlos Fernandez-Granda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-Time Activity Recognition and Intention Recognition Using a Vision-based Embedded System. (arXiv:2107.12744v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12744</id>
        <link href="http://arxiv.org/abs/2107.12744"/>
        <updated>2021-07-28T02:02:32.464Z</updated>
        <summary type="html"><![CDATA[With the rapid increase in digital technologies, most fields of study include
recognition of human activity and intention recognition, which are important in
smart environments. In this research, we introduce a real-time activity
recognition to recognize people's intentions to pass or not pass a door. This
system, if applied in elevators and automatic doors will save energy and
increase efficiency. For this study, data preparation is applied to combine the
spatial and temporal features with the help of digital image processing
principles. Nevertheless, unlike previous studies, only one AlexNet neural
network is used instead of two-stream convolutional neural networks. Our
embedded system was implemented with an accuracy of 98.78% on our Intention
Recognition dataset. We also examined our data representation approach on other
datasets, including HMDB-51, KTH, and Weizmann, and obtained accuracy of
78.48%, 97.95%, and 100%, respectively. The image recognition and neural
network models were simulated and implemented using Xilinx simulators for
ZCU102 board. The operating frequency of this embedded system is 333 MHz, and
it works in real-time with 120 frames per second (fps).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Darafsh_S/0/1/0/all/0/1"&gt;Sahar Darafsh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghidary_S/0/1/0/all/0/1"&gt;Saeed Shiry Ghidary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamani_M/0/1/0/all/0/1"&gt;Morteza Saheb Zamani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Physiologically-adapted Gold Standard for Arousal During a Stress Induced Scenario. (arXiv:2107.12964v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12964</id>
        <link href="http://arxiv.org/abs/2107.12964"/>
        <updated>2021-07-28T02:02:32.456Z</updated>
        <summary type="html"><![CDATA[Emotion is an inherently subjective psychophysiological human-state and to
produce an agreed-upon representation (gold standard) for continuous emotion
requires a time-consuming and costly training procedure of multiple human
annotators. There is strong evidence in the literature that physiological
signals are sufficient objective markers for states of emotion, particularly
arousal. In this contribution, we utilise a dataset which includes continuous
emotion and physiological signals - Heartbeats per Minute (BPM), Electrodermal
Activity (EDA), and Respiration-rate - captured during a stress induced
scenario (Trier Social Stress Test). We utilise a Long Short-Term Memory,
Recurrent Neural Network to explore the benefit of fusing these physiological
signals with arousal as the target, learning from various audio, video, and
textual based features. We utilise the state-of-the-art MuSe-Toolbox to
consider both annotation delay and inter-rater agreement weighting when fusing
the target signals. An improvement in Concordance Correlation Coefficient (CCC)
is seen across features sets when fusing EDA with arousal, compared to the
arousal only gold standard results. Additionally, BERT-based textual features'
results improved for arousal plus all physiological signals, obtaining up to
.3344 CCC compared to .2118 CCC for arousal only. Multimodal fusion also
improves overall CCC with audio plus video features obtaining up to .6157 CCC
to recognize arousal plus EDA and BPM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baird_A/0/1/0/all/0/1"&gt;Alice Baird&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stappen_L/0/1/0/all/0/1"&gt;Lukas Stappen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Christ_L/0/1/0/all/0/1"&gt;Lukas Christ&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schumann_L/0/1/0/all/0/1"&gt;Lea Schumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Messner_E/0/1/0/all/0/1"&gt;Eva-Maria Me&amp;#xdf;ner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn W. Schuller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discriminative-Generative Representation Learning for One-Class Anomaly Detection. (arXiv:2107.12753v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12753</id>
        <link href="http://arxiv.org/abs/2107.12753"/>
        <updated>2021-07-28T02:02:32.449Z</updated>
        <summary type="html"><![CDATA[As a kind of generative self-supervised learning methods, generative
adversarial nets have been widely studied in the field of anomaly detection.
However, the representation learning ability of the generator is limited since
it pays too much attention to pixel-level details, and generator is difficult
to learn abstract semantic representations from label prediction pretext tasks
as effective as discriminator. In order to improve the representation learning
ability of generator, we propose a self-supervised learning framework combining
generative methods and discriminative methods. The generator no longer learns
representation by reconstruction error, but the guidance of discriminator, and
could benefit from pretext tasks designed for discriminative methods. Our
discriminative-generative representation learning method has performance close
to discriminative methods and has a great advantage in speed. Our method used
in one-class anomaly detection task significantly outperforms several
state-of-the-arts on multiple benchmark data sets, increases the performance of
the top-performing GAN-based baseline by 6% on CIFAR-10 and 2% on MVTAD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1"&gt;Xuan Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xizhou Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xing He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingfei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1"&gt;Ning Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lin Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RGL-NET: A Recurrent Graph Learning framework for Progressive Part Assembly. (arXiv:2107.12859v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12859</id>
        <link href="http://arxiv.org/abs/2107.12859"/>
        <updated>2021-07-28T02:02:32.441Z</updated>
        <summary type="html"><![CDATA[Autonomous assembly of objects is an essential task in robotics and 3D
computer vision. It has been studied extensively in robotics as a problem of
motion planning, actuator control and obstacle avoidance. However, the task of
developing a generalized framework for assembly robust to structural variants
remains relatively unexplored. In this work, we tackle this problem using a
recurrent graph learning framework considering inter-part relations and the
progressive update of the part pose. Our network can learn more plausible
predictions of shape structure by accounting for priorly assembled parts.
Compared to the current state-of-the-art, our network yields up to 10%
improvement in part accuracy and up to 15% improvement in connectivity accuracy
on the PartNet dataset. Moreover, our resulting latent space facilitates
exciting applications such as shape recovery from the point-cloud components.
We conduct extensive experiments to justify our design choices and demonstrate
the effectiveness of the proposed framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Harish_A/0/1/0/all/0/1"&gt;Abhinav Narayan Harish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagar_R/0/1/0/all/0/1"&gt;Rajendra Nagar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raman_S/0/1/0/all/0/1"&gt;Shanmuganathan Raman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PiSLTRc: Position-informed Sign Language Transformer with Content-aware Convolution. (arXiv:2107.12600v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12600</id>
        <link href="http://arxiv.org/abs/2107.12600"/>
        <updated>2021-07-28T02:02:32.420Z</updated>
        <summary type="html"><![CDATA[Since the superiority of Transformer in learning long-term dependency, the
sign language Transformer model achieves remarkable progress in Sign Language
Recognition (SLR) and Translation (SLT). However, there are several issues with
the Transformer that prevent it from better sign language understanding. The
first issue is that the self-attention mechanism learns sign video
representation in a frame-wise manner, neglecting the temporal semantic
structure of sign gestures. Secondly, the attention mechanism with absolute
position encoding is direction and distance unaware, thus limiting its ability.
To address these issues, we propose a new model architecture, namely PiSLTRc,
with two distinctive characteristics: (i) content-aware and position-aware
convolution layers. Specifically, we explicitly select relevant features using
a novel content-aware neighborhood gathering method. Then we aggregate these
features with position-informed temporal convolution layers, thus generating
robust neighborhood-enhanced sign representation. (ii) injecting the relative
position information to the attention mechanism in the encoder, decoder, and
even encoder-decoder cross attention. Compared with the vanilla Transformer
model, our model performs consistently better on three large-scale sign
language benchmarks: PHOENIX-2014, PHOENIX-2014-T and CSL. Furthermore,
extensive experiments demonstrate that the proposed method achieves
state-of-the-art performance on translation quality with $+1.6$ BLEU
improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1"&gt;Pan Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1"&gt;Mengyi Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xiaohui Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning-based Single Image Face Depth Data Enhancement. (arXiv:2006.11091v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.11091</id>
        <link href="http://arxiv.org/abs/2006.11091"/>
        <updated>2021-07-28T02:02:32.413Z</updated>
        <summary type="html"><![CDATA[Face recognition can benefit from the utilization of depth data captured
using low-cost cameras, in particular for presentation attack detection
purposes. Depth video output from these capture devices can however contain
defects such as holes or general depth inaccuracies. This work proposes a deep
learning face depth enhancement method in this context of facial biometrics,
which adds a security aspect to the topic. U-Net-like architectures are
utilized, and the networks are compared against hand-crafted enhancer types, as
well as a similar depth enhancer network from related work trained for an
adjacent application scenario. All tested enhancer types exclusively use depth
data as input, which differs from methods that enhance depth based on
additional input data such as visible light color images. Synthetic face depth
ground truth images and degraded forms thereof are created with help of PRNet,
to train multiple deep learning enhancer models with different network sizes
and training configurations. Evaluations are carried out on the synthetic data,
on Kinect v1 images from the KinectFaceDB, and on in-house RealSense D435
images. These evaluations include an assessment of the falsification for
occluded face depth input, which is relevant to biometric security. The
proposed deep learning enhancers yield noticeably better results than the
tested preexisting enhancers, without overly falsifying depth data when
non-face input is provided, and are shown to reduce the error of a simple
landmark-based PAD method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schlett_T/0/1/0/all/0/1"&gt;Torsten Schlett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1"&gt;Christian Rathgeb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1"&gt;Christoph Busch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Random Forest Classifier for Automated Game Design. (arXiv:2107.12501v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12501</id>
        <link href="http://arxiv.org/abs/2107.12501"/>
        <updated>2021-07-28T02:02:32.394Z</updated>
        <summary type="html"><![CDATA[Autonomous game design, generating games algorithmically, has been a longtime
goal within the technical games research field. However, existing autonomous
game design systems have relied in large part on human-authoring for game
design knowledge, such as fitness functions in search-based methods. In this
paper, we describe an experiment to attempt to learn a human-like fitness
function for autonomous game design in an adversarial manner. While our
experimental work did not meet our expectations, we present an analysis of our
system and results that we hope will be informative to future autonomous game
design research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maurer_T/0/1/0/all/0/1"&gt;Thomas Maurer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guzdial_M/0/1/0/all/0/1"&gt;Matthew Guzdial&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StarEnhancer: Learning Real-Time and Style-Aware Image Enhancement. (arXiv:2107.12898v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12898</id>
        <link href="http://arxiv.org/abs/2107.12898"/>
        <updated>2021-07-28T02:02:32.384Z</updated>
        <summary type="html"><![CDATA[Image enhancement is a subjective process whose targets vary with user
preferences. In this paper, we propose a deep learning-based image enhancement
method covering multiple tonal styles using only a single model dubbed
StarEnhancer. It can transform an image from one tonal style to another, even
if that style is unseen. With a simple one-time setting, users can customize
the model to make the enhanced images more in line with their aesthetics. To
make the method more practical, we propose a well-designed enhancer that can
process a 4K-resolution image over 200 FPS but surpasses the contemporaneous
single style image enhancement methods in terms of PSNR, SSIM, and LPIPS.
Finally, our proposed enhancement method has good interactability, which allows
the user to fine-tune the enhanced image using intuitive options.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yuda Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1"&gt;Hui Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1"&gt;Xin Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Don't Sweep your Learning Rate under the Rug: A Closer Look at Cross-modal Transfer of Pretrained Transformers. (arXiv:2107.12460v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12460</id>
        <link href="http://arxiv.org/abs/2107.12460"/>
        <updated>2021-07-28T02:02:32.353Z</updated>
        <summary type="html"><![CDATA[Self-supervised pre-training of large-scale transformer models on text
corpora followed by finetuning has achieved state-of-the-art on a number of
natural language processing tasks. Recently, Lu et al. (2021, arXiv:2103.05247)
claimed that frozen pretrained transformers (FPTs) match or outperform training
from scratch as well as unfrozen (fine-tuned) pretrained transformers in a set
of transfer tasks to other modalities. In our work, we find that this result
is, in fact, an artifact of not tuning the learning rates. After carefully
redesigning the empirical setup, we find that when tuning learning rates
properly, pretrained transformers do outperform or match training from scratch
in all of our tasks, but only as long as the entire model is finetuned. Thus,
while transfer from pretrained language models to other modalities does indeed
provide gains and hints at exciting possibilities for future work, properly
tuning hyperparameters is important for arriving at robust findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rothermel_D/0/1/0/all/0/1"&gt;Danielle Rothermel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Margaret Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1"&gt;Tim Rockt&amp;#xe4;schel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1"&gt;Jakob Foerster&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Take-over Time for Autonomous Driving with Real-World Data: Robust Data Augmentation, Models, and Evaluation. (arXiv:2107.12932v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12932</id>
        <link href="http://arxiv.org/abs/2107.12932"/>
        <updated>2021-07-28T02:02:32.345Z</updated>
        <summary type="html"><![CDATA[Understanding occupant-vehicle interactions by modeling control transitions
is important to ensure safe approaches to passenger vehicle automation. Models
which contain contextual, semantically meaningful representations of driver
states can be used to determine the appropriate timing and conditions for
transfer of control between driver and vehicle. However, such models rely on
real-world control take-over data from drivers engaged in distracting
activities, which is costly to collect. Here, we introduce a scheme for data
augmentation for such a dataset. Using the augmented dataset, we develop and
train take-over time (TOT) models that operate sequentially on mid and
high-level features produced by computer vision algorithms operating on
different driver-facing camera views, showing models trained on the augmented
dataset to outperform the initial dataset. The demonstrated model features
encode different aspects of the driver state, pertaining to the face, hands,
foot and upper body of the driver. We perform ablative experiments on feature
combinations as well as model architectures, showing that a TOT model supported
by augmented data can be used to produce continuous estimates of take-over
times without delay, suitable for complex real-world scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rangesh_A/0/1/0/all/0/1"&gt;Akshay Rangesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deo_N/0/1/0/all/0/1"&gt;Nachiket Deo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greer_R/0/1/0/all/0/1"&gt;Ross Greer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunaratne_P/0/1/0/all/0/1"&gt;Pujitha Gunaratne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_M/0/1/0/all/0/1"&gt;Mohan M. Trivedi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effects of Image Size on Deep Learning. (arXiv:2101.11508v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11508</id>
        <link href="http://arxiv.org/abs/2101.11508"/>
        <updated>2021-07-28T02:02:32.326Z</updated>
        <summary type="html"><![CDATA[This paper presents the evaluation of effects of image size on deep learning
performance via semantic segmentation of magnetic resonance heart images with
U-net for fully automated quantification of myocardial infarction. Both
non-extra pixel and extra pixel interpolation algorithms are used to change the
size of images in datasets of interest. Extra class labels, in interpolated
ground truth segmentation images, are removed using thresholding, median
filtering, and subtraction strategies. Common class metrics are used to
evaluate the quality of semantic segmentation with U-net against the ground
truth segmentation while arbitrary threshold, comparison of the sums, and sums
of differences between medical experts and fully automated results are options
used to estimate the relationship between medical experts-based quantification
and fully automated quantification results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rukundo_O/0/1/0/all/0/1"&gt;Olivier Rukundo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Local Recurrent Models for Human Mesh Recovery. (arXiv:2107.12847v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12847</id>
        <link href="http://arxiv.org/abs/2107.12847"/>
        <updated>2021-07-28T02:02:32.316Z</updated>
        <summary type="html"><![CDATA[We consider the problem of estimating frame-level full human body meshes
given a video of a person with natural motion dynamics. While much progress in
this field has been in single image-based mesh estimation, there has been a
recent uptick in efforts to infer mesh dynamics from video given its role in
alleviating issues such as depth ambiguity and occlusions. However, a key
limitation of existing work is the assumption that all the observed motion
dynamics can be modeled using one dynamical/recurrent model. While this may
work well in cases with relatively simplistic dynamics, inference with
in-the-wild videos presents many challenges. In particular, it is typically the
case that different body parts of a person undergo different dynamics in the
video, e.g., legs may move in a way that may be dynamically different from
hands (e.g., a person dancing). To address these issues, we present a new
method for video mesh recovery that divides the human mesh into several local
parts following the standard skeletal model. We then model the dynamics of each
local part with separate recurrent models, with each model conditioned
appropriately based on the known kinematic structure of the human body. This
results in a structure-informed local recurrent learning architecture that can
be trained in an end-to-end fashion with available annotations. We conduct a
variety of experiments on standard video mesh recovery benchmark datasets such
as Human3.6M, MPI-INF-3DHP, and 3DPW, demonstrating the efficacy of our design
of modeling local dynamics as well as establishing state-of-the-art results
based on standard evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Runze Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karanam_S/0/1/0/all/0/1"&gt;Srikrishna Karanam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ren Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Terrence Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhanu_B/0/1/0/all/0/1"&gt;Bir Bhanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Ziyan Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Reinforcement Learning for L3 Slice Localization in Sarcopenia Assessment. (arXiv:2107.12800v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12800</id>
        <link href="http://arxiv.org/abs/2107.12800"/>
        <updated>2021-07-28T02:02:32.301Z</updated>
        <summary type="html"><![CDATA[Sarcopenia is a medical condition characterized by a reduction in muscle mass
and function. A quantitative diagnosis technique consists of localizing the CT
slice passing through the middle of the third lumbar area (L3) and segmenting
muscles at this level. In this paper, we propose a deep reinforcement learning
method for accurate localization of the L3 CT slice. Our method trains a
reinforcement learning agent by incentivizing it to discover the right
position. Specifically, a Deep Q-Network is trained to find the best policy to
follow for this problem. Visualizing the training process shows that the agent
mimics the scrolling of an experienced radiologist. Extensive experiments
against other state-of-the-art deep learning based methods for L3 localization
prove the superiority of our technique which performs well even with limited
amount of data and annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Laousy_O/0/1/0/all/0/1"&gt;Othmane Laousy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chassagnon_G/0/1/0/all/0/1"&gt;Guillaume Chassagnon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oyallon_E/0/1/0/all/0/1"&gt;Edouard Oyallon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paragios_N/0/1/0/all/0/1"&gt;Nikos Paragios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Revel_M/0/1/0/all/0/1"&gt;Marie-Pierre Revel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1"&gt;Maria Vakalopoulou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic and Static Object Detection Considering Fusion Regions and Point-wise Features. (arXiv:2107.12692v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12692</id>
        <link href="http://arxiv.org/abs/2107.12692"/>
        <updated>2021-07-28T02:02:31.716Z</updated>
        <summary type="html"><![CDATA[Object detection is a critical problem for the safe interaction between
autonomous vehicles and road users. Deep-learning methodologies allowed the
development of object detection approaches with better performance. However,
there is still the challenge to obtain more characteristics from the objects
detected in real-time. The main reason is that more information from the
environment's objects can improve the autonomous vehicle capacity to face
different urban situations. This paper proposes a new approach to detect static
and dynamic objects in front of an autonomous vehicle. Our approach can also
get other characteristics from the objects detected, like their position,
velocity, and heading. We develop our proposal fusing results of the
environment's interpretations achieved of YoloV3 and a Bayesian filter. To
demonstrate our proposal's performance, we asses it through a benchmark dataset
and real-world data obtained from an autonomous platform. We compared the
results achieved with another approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9;s G&amp;#xf3;mez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Genevois_T/0/1/0/all/0/1"&gt;Thomas Genevois&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lussereau_J/0/1/0/all/0/1"&gt;Jerome Lussereau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laugier_C/0/1/0/all/0/1"&gt;Christian Laugier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic sparse adversarial attacks. (arXiv:2011.12423v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12423</id>
        <link href="http://arxiv.org/abs/2011.12423"/>
        <updated>2021-07-28T02:02:31.697Z</updated>
        <summary type="html"><![CDATA[This paper introduces stochastic sparse adversarial attacks (SSAA), simple,
fast and purely noise-based targeted and untargeted $L_0$ attacks of neural
network classifiers (NNC). SSAA are devised by exploiting a simple small-time
expansion idea widely used for Markov processes and offer new examples of $L_0$
attacks whose studies have been limited. They are designed to solve the known
scalability issue of the family of Jacobian-based saliency maps attacks to
large datasets and they succeed in solving it. Experiments on small and large
datasets (CIFAR-10 and ImageNet) illustrate further advantages of SSAA in
comparison with the-state-of-the-art methods. For instance, in the untargeted
case, our method called Voting Folded Gaussian Attack (VFGA) scales efficiently
to ImageNet and achieves a significantly lower $L_0$ score than SparseFool (up
to $\frac{2}{5}$ lower) while being faster. Moreover, VFGA achieves better
$L_0$ scores on ImageNet than Sparse-RS when both attacks are fully successful
on a large number of samples. Codes are publicly available through the link
https://github.com/SSAA3/stochastic-sparse-adv-attacks]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cesaire_M/0/1/0/all/0/1"&gt;Manon C&amp;#xe9;saire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hajri_H/0/1/0/all/0/1"&gt;Hatem Hajri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lamprier_S/0/1/0/all/0/1"&gt;Sylvain Lamprier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1"&gt;Patrick Gallinari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extraction and Analysis of Fictional Character Networks: A Survey. (arXiv:1907.02704v4 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1907.02704</id>
        <link href="http://arxiv.org/abs/1907.02704"/>
        <updated>2021-07-28T02:02:31.671Z</updated>
        <summary type="html"><![CDATA[A character network is a graph extracted from a narrative, in which vertices
represent characters and edges correspond to interactions between them. A
number of narrative-related problems can be addressed automatically through the
analysis of character networks, such as summarization, classification, or role
detection. Character networks are particularly relevant when considering works
of fictions (e.g. novels, plays, movies, TV series), as their exploitation
allows developing information retrieval and recommendation systems. However,
works of fiction possess specific properties making these tasks harder. This
survey aims at presenting and organizing the scientific literature related to
the extraction of character networks from works of fiction, as well as their
analysis. We first describe the extraction process in a generic way, and
explain how its constituting steps are implemented in practice, depending on
the medium of the narrative, the goal of the network analysis, and other
factors. We then review the descriptive tools used to characterize character
networks, with a focus on the way they are interpreted in this context. We
illustrate the relevance of character networks by also providing a review of
applications derived from their analysis. Finally, we identify the limitations
of the existing approaches, and the most promising perspectives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Labatut_V/0/1/0/all/0/1"&gt;Vincent Labatut&lt;/a&gt; (LIA), &lt;a href="http://arxiv.org/find/cs/1/au:+Bost_X/0/1/0/all/0/1"&gt;Xavier Bost&lt;/a&gt; (LIA)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Scale Local-Temporal Similarity Fusion for Continuous Sign Language Recognition. (arXiv:2107.12762v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12762</id>
        <link href="http://arxiv.org/abs/2107.12762"/>
        <updated>2021-07-28T02:02:31.663Z</updated>
        <summary type="html"><![CDATA[Continuous sign language recognition (cSLR) is a public significant task that
transcribes a sign language video into an ordered gloss sequence. It is
important to capture the fine-grained gloss-level details, since there is no
explicit alignment between sign video frames and the corresponding glosses.
Among the past works, one promising way is to adopt a one-dimensional
convolutional network (1D-CNN) to temporally fuse the sequential frames.
However, CNNs are agnostic to similarity or dissimilarity, and thus are unable
to capture local consistent semantics within temporally neighboring frames. To
address the issue, we propose to adaptively fuse local features via temporal
similarity for this task. Specifically, we devise a Multi-scale Local-Temporal
Similarity Fusion Network (mLTSF-Net) as follows: 1) In terms of a specific
video frame, we firstly select its similar neighbours with multi-scale
receptive regions to accommodate different lengths of glosses. 2) To ensure
temporal consistency, we then use position-aware convolution to temporally
convolve each scale of selected frames. 3) To obtain a local-temporally
enhanced frame-wise representation, we finally fuse the results of different
scales using a content-dependent aggregator. We train our model in an
end-to-end fashion, and the experimental results on RWTH-PHOENIX-Weather 2014
datasets (RWTH) demonstrate that our model achieves competitive performance
compared with several state-of-the-art models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1"&gt;Pan Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1"&gt;Zhi Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yao Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1"&gt;Mengyi Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1"&gt;Jianwei Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xiaohui Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PointBA: Towards Backdoor Attacks in 3D Point Cloud. (arXiv:2103.16074v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16074</id>
        <link href="http://arxiv.org/abs/2103.16074"/>
        <updated>2021-07-28T02:02:31.649Z</updated>
        <summary type="html"><![CDATA[3D deep learning has been increasingly more popular for a variety of tasks
including many safety-critical applications. However, recently several works
raise the security issues of 3D deep nets. Although most of these works
consider adversarial attacks, we identify that backdoor attack is indeed a more
serious threat to 3D deep learning systems but remains unexplored. We present
the backdoor attacks in 3D with a unified framework that exploits the unique
properties of 3D data and networks. In particular, we design two attack
approaches: the poison-label attack and the clean-label attack. The first one
is straightforward and effective in practice, while the second one is more
sophisticated assuming there are certain data inspections. The attack
algorithms are mainly motivated and developed by 1) the recent discovery of 3D
adversarial samples which demonstrate the vulnerability of 3D deep nets under
spatial transformations; 2) the proposed feature disentanglement technique that
manipulates the feature of the data through optimization methods and its
potential to embed a new task. Extensive experiments show the efficacy of the
poison-label attack with over 95% success rate across several 3D datasets and
models, and the ability of clean-label attack against data filtering with
around 50% success rate. Our proposed backdoor attack in 3D point cloud is
expected to perform as a baseline for improving the robustness of 3D deep
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xinke Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhirui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yue Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1"&gt;Zekun Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yabang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_A/0/1/0/all/0/1"&gt;Andrew Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Joey Tianyi Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Sample Selection for Robust Learning under Label Noise. (arXiv:2106.15292v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15292</id>
        <link href="http://arxiv.org/abs/2106.15292"/>
        <updated>2021-07-28T02:02:31.629Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) have been shown to be susceptible to memorization
or overfitting in the presence of noisily labelled data. For the problem of
robust learning under such noisy data, several algorithms have been proposed. A
prominent class of algorithms rely on sample selection strategies, motivated by
curriculum learning. For example, many algorithms use the `small loss trick'
wherein a fraction of samples with loss values below a certain threshold are
selected for training. These algorithms are sensitive to such thresholds, and
it is difficult to fix or learn these thresholds. Often, these algorithms also
require information such as label noise rates which are typically unavailable
in practice. In this paper, we propose a data-dependent, adaptive sample
selection strategy that relies only on batch statistics of a given mini-batch
to provide robustness against label noise. The algorithm does not have any
additional hyperparameters for sample selection, does not need any information
on noise rates, and does not need access to separate data with clean labels. We
empirically demonstrate the effectiveness of our algorithm on benchmark
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1"&gt;Deep Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sastry_P/0/1/0/all/0/1"&gt;P.S. Sastry&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Co-Transport for Class-Incremental Learning. (arXiv:2107.12654v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12654</id>
        <link href="http://arxiv.org/abs/2107.12654"/>
        <updated>2021-07-28T02:02:31.622Z</updated>
        <summary type="html"><![CDATA[Traditional learning systems are trained in closed-world for a fixed number
of classes, and need pre-collected datasets in advance. However, new classes
often emerge in real-world applications and should be learned incrementally.
For example, in electronic commerce, new types of products appear daily, and in
a social media community, new topics emerge frequently. Under such
circumstances, incremental models should learn several new classes at a time
without forgetting. We find a strong correlation between old and new classes in
incremental learning, which can be applied to relate and facilitate different
learning stages mutually. As a result, we propose CO-transport for class
Incremental Learning (COIL), which learns to relate across incremental tasks
with the class-wise semantic relationship. In detail, co-transport has two
aspects: prospective transport tries to augment the old classifier with optimal
transported knowledge as fast model adaptation. Retrospective transport aims to
transport new class classifiers backward as old ones to overcome forgetting.
With these transports, COIL efficiently adapts to new tasks, and stably resists
forgetting. Experiments on benchmark and real-world multimedia datasets
validate the effectiveness of our proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Da-Wei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Han-Jia Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1"&gt;De-Chuan Zhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LETI: Latency Estimation Tool and Investigation of Neural Networks inference on Mobile GPU. (arXiv:2010.02871v2 [cs.PF] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.02871</id>
        <link href="http://arxiv.org/abs/2010.02871"/>
        <updated>2021-07-28T02:02:31.615Z</updated>
        <summary type="html"><![CDATA[A lot of deep learning applications are desired to be run on mobile devices.
Both accuracy and inference time are meaningful for a lot of them. While the
number of FLOPs is usually used as a proxy for neural network latency, it may
be not the best choice. In order to obtain a better approximation of latency,
research community uses look-up tables of all possible layers for latency
calculation for the final prediction of the inference on mobile CPU. It
requires only a small number of experiments. Unfortunately, on mobile GPU this
method is not applicable in a straight-forward way and shows low precision. In
this work, we consider latency approximation on mobile GPU as a data and
hardware-specific problem. Our main goal is to construct a convenient latency
estimation tool for investigation(LETI) of neural network inference and
building robust and accurate latency prediction models for each specific task.
To achieve this goal, we build open-source tools which provide a convenient way
to conduct massive experiments on different target devices focusing on mobile
GPU. After evaluation of the dataset, we learn the regression model on
experimental data and use it for future latency prediction and analysis. We
experimentally demonstrate the applicability of such an approach on a subset of
popular NAS-Benchmark 101 dataset and also evaluate the most popular neural
network architectures for two mobile GPUs. As a result, we construct latency
prediction model with good precision on the target evaluation subset. We
consider LETI as a useful tool for neural architecture search or massive
latency evaluation. The project is available at https://github.com/leti-ai]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ponomarev_E/0/1/0/all/0/1"&gt;Evgeny Ponomarev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matveev_S/0/1/0/all/0/1"&gt;Sergey Matveev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oseledets_I/0/1/0/all/0/1"&gt;Ivan Oseledets&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analyzing vehicle pedestrian interactions combining data cube structure and predictive collision risk estimation model. (arXiv:2107.12507v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12507</id>
        <link href="http://arxiv.org/abs/2107.12507"/>
        <updated>2021-07-28T02:02:31.608Z</updated>
        <summary type="html"><![CDATA[Traffic accidents are a threat to human lives, particularly pedestrians
causing premature deaths. Therefore, it is necessary to devise systems to
prevent accidents in advance and respond proactively, using potential risky
situations as one of the surrogate safety measurements. This study introduces a
new concept of a pedestrian safety system that combines the field and the
centralized processes. The system can warn of upcoming risks immediately in the
field and improve the safety of risk frequent areas by assessing the safety
levels of roads without actual collisions. In particular, this study focuses on
the latter by introducing a new analytical framework for a crosswalk safety
assessment with behaviors of vehicle/pedestrian and environmental features. We
obtain these behavioral features from actual traffic video footage in the city
with complete automatic processing. The proposed framework mainly analyzes
these behaviors in multidimensional perspectives by constructing a data cube
structure, which combines the LSTM based predictive collision risk estimation
model and the on line analytical processing operations. From the PCR estimation
model, we categorize the severity of risks as four levels and apply the
proposed framework to assess the crosswalk safety with behavioral features. Our
analytic experiments are based on two scenarios, and the various descriptive
results are harvested the movement patterns of vehicles and pedestrians by road
environment and the relationships between risk levels and car speeds. Thus, the
proposed framework can support decision makers by providing valuable
information to improve pedestrian safety for future accidents, and it can help
us better understand their behaviors near crosswalks proactively. In order to
confirm the feasibility and applicability of the proposed framework, we
implement and apply it to actual operating CCTVs in Osan City, Korea.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Noh_B/0/1/0/all/0/1"&gt;Byeongjoon Noh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1"&gt;Hansaem Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeo_H/0/1/0/all/0/1"&gt;Hwasoo Yeo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Super-Human Performance in Online Low-latency Recognition of Conversational Speech. (arXiv:2010.03449v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03449</id>
        <link href="http://arxiv.org/abs/2010.03449"/>
        <updated>2021-07-28T02:02:31.599Z</updated>
        <summary type="html"><![CDATA[Achieving super-human performance in recognizing human speech has been a goal
for several decades, as researchers have worked on increasingly challenging
tasks. In the 1990's it was discovered, that conversational speech between two
humans turns out to be considerably more difficult than read speech as
hesitations, disfluencies, false starts and sloppy articulation complicate
acoustic processing and require robust handling of acoustic, lexical and
language context, jointly. Early attempts with statistical models could only
reach error rates over 50% and far from human performance (WER of around 5.5%).
Neural hybrid models and recent attention-based encoder-decoder models have
considerably improved performance as such contexts can now be learned in an
integral fashion. However, processing such contexts requires an entire
utterance presentation and thus introduces unwanted delays before a recognition
result can be output. In this paper, we address performance as well as latency.
We present results for a system that can achieve super-human performance (at a
WER of 5.0%, over the Switchboard conversational benchmark) at a word based
latency of only 1 second behind a speaker's speech. The system uses multiple
attention-based encoder-decoder networks integrated within a novel low latency
incremental inference approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thai-Son Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stueker_S/0/1/0/all/0/1"&gt;Sebastian Stueker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1"&gt;Alex Waibel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RGB cameras failures and their effects in autonomous driving applications. (arXiv:2008.05938v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.05938</id>
        <link href="http://arxiv.org/abs/2008.05938"/>
        <updated>2021-07-28T02:02:31.581Z</updated>
        <summary type="html"><![CDATA[RGB cameras are one of the most relevant sensors for autonomous driving
applications. It is undeniable that failures of vehicle cameras may compromise
the autonomous driving task, possibly leading to unsafe behaviors when images
that are subsequently processed by the driving system are altered. To support
the definition of safe and robust vehicle architectures and intelligent
systems, in this paper we define the failure modes of a vehicle camera,
together with an analysis of effects and known mitigations. Further, we build a
software library for the generation of the corresponding failed images and we
feed them to six object detectors for mono and stereo cameras and to the
self-driving agent of an autonomous driving simulator. The resulting
misbehaviors with respect to operating with clean images allow a better
understanding of failures effects and the related safety risks in image-based
applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Secci_F/0/1/0/all/0/1"&gt;Francesco Secci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ceccarelli_A/0/1/0/all/0/1"&gt;Andrea Ceccarelli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Frozen-to-Paraffin: Categorization of Histological Frozen Sections by the Aid of Paraffin Sections and Generative Adversarial Networks. (arXiv:2012.08158v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08158</id>
        <link href="http://arxiv.org/abs/2012.08158"/>
        <updated>2021-07-28T02:02:31.574Z</updated>
        <summary type="html"><![CDATA[In contrast to paraffin sections, frozen sections can be quickly generated
during surgical interventions. This procedure allows surgeons to wait for
histological findings during the intervention to base intra-operative decisions
on the outcome of the histology. However, compared to paraffin sections, the
quality of frozen sections is typically lower, leading to a higher ratio of
miss-classification. In this work, we investigated the effect of the section
type on automated decision support approaches for classification of thyroid
cancer. This was enabled by a data set consisting of pairs of sections for
individual patients. Moreover, we investigated, whether a frozen-to-paraffin
translation could help to optimize classification scores. Finally, we propose a
specific data augmentation strategy to deal with a small amount of training
data and to increase classification accuracy even further.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gadermayr_M/0/1/0/all/0/1"&gt;Michael Gadermayr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tschuchnig_M/0/1/0/all/0/1"&gt;Maximilian Tschuchnig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Stangassinger_L/0/1/0/all/0/1"&gt;Lea Maria Stangassinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kreutzer_C/0/1/0/all/0/1"&gt;Christina Kreutzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Couillard_Despres_S/0/1/0/all/0/1"&gt;Sebastien Couillard-Despres&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oostingh_G/0/1/0/all/0/1"&gt;Gertie Janneke Oostingh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hittmair_A/0/1/0/all/0/1"&gt;Anton Hittmair&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Learning with Neuron Activation Importance. (arXiv:2107.12657v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12657</id>
        <link href="http://arxiv.org/abs/2107.12657"/>
        <updated>2021-07-28T02:02:31.567Z</updated>
        <summary type="html"><![CDATA[Continual learning is a concept of online learning with multiple sequential
tasks. One of the critical barriers of continual learning is that a network
should learn a new task keeping the knowledge of old tasks without access to
any data of the old tasks. In this paper, we propose a neuron activation
importance-based regularization method for stable continual learning regardless
of the order of tasks. We conduct comprehensive experiments on existing
benchmark data sets to evaluate not just the stability and plasticity of our
method with improved classification accuracy also the robustness of the
performance along the changes of task order.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sohee Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seungkyu Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time Keypoints Detection for Autonomous Recovery of the Unmanned Ground Vehicle. (arXiv:2107.12852v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12852</id>
        <link href="http://arxiv.org/abs/2107.12852"/>
        <updated>2021-07-28T02:02:31.560Z</updated>
        <summary type="html"><![CDATA[The combination of a small unmanned ground vehicle (UGV) and a large unmanned
carrier vehicle allows more flexibility in real applications such as rescue in
dangerous scenarios. The autonomous recovery system, which is used to guide the
small UGV back to the carrier vehicle, is an essential component to achieve a
seamless combination of the two vehicles. This paper proposes a novel
autonomous recovery framework with a low-cost monocular vision system to
provide accurate positioning and attitude estimation of the UGV during
navigation. First, we introduce a light-weight convolutional neural network
called UGV-KPNet to detect the keypoints of the small UGV from the images
captured by a monocular camera. UGV-KPNet is computationally efficient with a
small number of parameters and provides pixel-level accurate keypoints
detection results in real-time. Then, six degrees of freedom pose is estimated
using the detected keypoints to obtain positioning and attitude information of
the UGV. Besides, we are the first to create a large-scale real-world keypoints
dataset of the UGV. The experimental results demonstrate that the proposed
system achieves state-of-the-art performance in terms of both accuracy and
speed on UGV keypoint detection, and can further boost the 6-DoF pose
estimation for the UGV.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Sheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1"&gt;Kai Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1"&gt;Xia Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1"&gt;Chunxia Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yu Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scale-Localized Abstract Reasoning. (arXiv:2009.09405v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.09405</id>
        <link href="http://arxiv.org/abs/2009.09405"/>
        <updated>2021-07-28T02:02:31.553Z</updated>
        <summary type="html"><![CDATA[We consider the abstract relational reasoning task, which is commonly used as
an intelligence test. Since some patterns have spatial rationales, while others
are only semantic, we propose a multi-scale architecture that processes each
query in multiple resolutions. We show that indeed different rules are solved
by different resolutions and a combined multi-scale approach outperforms the
existing state of the art in this task on all benchmarks by 5-54%. The success
of our method is shown to arise from multiple novelties. First, it searches for
relational patterns in multiple resolutions, which allows it to readily detect
visual relations, such as location, in higher resolution, while allowing the
lower resolution module to focus on semantic relations, such as shape type.
Second, we optimize the reasoning network of each resolution proportionally to
its performance, hereby we motivate each resolution to specialize on the rules
for which it performs better than the others and ignore cases that are already
solved by the other resolutions. Third, we propose a new way to pool
information along the rows and the columns of the illustration-grid of the
query. Our work also analyses the existing benchmarks, demonstrating that the
RAVEN dataset selects the negative examples in a way that is easily exploited.
We, therefore, propose a modified version of the RAVEN dataset, named
RAVEN-FAIR. Our code and pretrained models are available at
https://github.com/yanivbenny/MRNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Benny_Y/0/1/0/all/0/1"&gt;Yaniv Benny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pekar_N/0/1/0/all/0/1"&gt;Niv Pekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1"&gt;Lior Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DV-ConvNet: Fully Convolutional Deep Learning on Point Clouds with Dynamic Voxelization and 3D Group Convolution. (arXiv:2009.02918v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.02918</id>
        <link href="http://arxiv.org/abs/2009.02918"/>
        <updated>2021-07-28T02:02:31.534Z</updated>
        <summary type="html"><![CDATA[3D point cloud interpretation is a challenging task due to the randomness and
sparsity of the component points. Many of the recently proposed methods like
PointNet and PointCNN have been focusing on learning shape descriptions from
point coordinates as point-wise input features, which usually involves
complicated network architectures. In this work, we draw attention back to the
standard 3D convolutions towards an efficient 3D point cloud interpretation.
Instead of converting the entire point cloud into voxel representations like
the other volumetric methods, we voxelize the sub-portions of the point cloud
only at necessary locations within each convolution layer on-the-fly, using our
dynamic voxelization operation with self-adaptive voxelization resolution. In
addition, we incorporate 3D group convolution into our dense convolution kernel
implementation to further exploit the rotation invariant features of point
cloud. Benefiting from its simple fully-convolutional architecture, our network
is able to run and converge at a considerably fast speed, while yields on-par
or even better performance compared with the state-of-the-art methods on
several benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1"&gt;Zhaoyu Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1"&gt;Pin Siang Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chow_J/0/1/0/all/0/1"&gt;Junkang Chow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jimmy Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheong_Y/0/1/0/all/0/1"&gt;Yehur Cheong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu-Hsing Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimating Parkinsonism Severity in Natural Gait Videos of Older Adults with Dementia. (arXiv:2105.03464v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03464</id>
        <link href="http://arxiv.org/abs/2105.03464"/>
        <updated>2021-07-28T02:02:31.527Z</updated>
        <summary type="html"><![CDATA[Drug-induced parkinsonism affects many older adults with dementia, often
causing gait disturbances. New advances in vision-based human pose-estimation
have opened possibilities for frequent and unobtrusive analysis of gait in
residential settings. This work proposes novel spatial-temporal graph
convolutional network (ST-GCN) architectures and training procedures to predict
clinical scores of parkinsonism in gait from video of individuals with
dementia. We propose a two-stage training approach consisting of a
self-supervised pretraining stage that encourages the ST-GCN model to learn
about gait patterns before predicting clinical scores in the finetuning stage.
The proposed ST-GCN models are evaluated on joint trajectories extracted from
video and are compared against traditional (ordinal, linear, random forest)
regression models and temporal convolutional network baselines. Three 2D human
pose-estimation libraries (OpenPose, Detectron, AlphaPose) and the Microsoft
Kinect (2D and 3D) are used to extract joint trajectories of 4787 natural
walking bouts from 53 older adults with dementia. A subset of 399 walks from 14
participants is annotated with scores of parkinsonism severity on the gait
criteria of the Unified Parkinson's Disease Rating Scale (UPDRS) and the
Simpson-Angus Scale (SAS). Our results demonstrate that ST-GCN models operating
on 3D joint trajectories extracted from the Kinect consistently outperform all
other models and feature sets. Prediction of parkinsonism scores in natural
walking bouts of unseen participants remains a challenging task, with the best
models achieving macro-averaged F1-scores of 0.53 +/- 0.03 and 0.40 +/- 0.02
for UPDRS-gait and SAS-gait, respectively. Pre-trained model and demo code for
this work is available:
https://github.com/TaatiTeam/stgcn_parkinsonism_prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sabo_A/0/1/0/all/0/1"&gt;Andrea Sabo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehdizadeh_S/0/1/0/all/0/1"&gt;Sina Mehdizadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iaboni_A/0/1/0/all/0/1"&gt;Andrea Iaboni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taati_B/0/1/0/all/0/1"&gt;Babak Taati&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13948</id>
        <link href="http://arxiv.org/abs/2106.13948"/>
        <updated>2021-07-28T02:02:31.518Z</updated>
        <summary type="html"><![CDATA[Recent advances in the areas of multimodal machine learning and artificial
intelligence (AI) have led to the development of challenging tasks at the
intersection of Computer Vision, Natural Language Processing, and Embodied AI.
Whereas many approaches and previous survey pursuits have characterised one or
two of these dimensions, there has not been a holistic analysis at the center
of all three. Moreover, even when combinations of these topics are considered,
more focus is placed on describing, e.g., current architectural methods, as
opposed to also illustrating high-level challenges and opportunities for the
field. In this survey paper, we discuss Embodied Vision-Language Planning
(EVLP) tasks, a family of prominent embodied navigation and manipulation
problems that jointly use computer vision and natural language. We propose a
taxonomy to unify these tasks and provide an in-depth analysis and comparison
of the new and current algorithmic approaches, metrics, simulated environments,
as well as the datasets used for EVLP tasks. Finally, we present the core
challenges that we believe new EVLP works should seek to address, and we
advocate for task construction that enables model generalizability and furthers
real-world deployment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1"&gt;Jonathan Francis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1"&gt;Nariaki Kitamura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1"&gt;Felix Labelle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaopeng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1"&gt;Ingrid Navarro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1"&gt;Jean Oh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantically Self-Aligned Network for Text-to-Image Part-aware Person Re-identification. (arXiv:2107.12666v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12666</id>
        <link href="http://arxiv.org/abs/2107.12666"/>
        <updated>2021-07-28T02:02:31.511Z</updated>
        <summary type="html"><![CDATA[Text-to-image person re-identification (ReID) aims to search for images
containing a person of interest using textual descriptions. However, due to the
significant modality gap and the large intra-class variance in textual
descriptions, text-to-image ReID remains a challenging problem. Accordingly, in
this paper, we propose a Semantically Self-Aligned Network (SSAN) to handle the
above problems. First, we propose a novel method that automatically extracts
semantically aligned part-level features from the two modalities. Second, we
design a multi-view non-local network that captures the relationships between
body parts, thereby establishing better correspondences between body parts and
noun phrases. Third, we introduce a Compound Ranking (CR) loss that makes use
of textual descriptions for other images of the same identity to provide extra
supervision, thereby effectively reducing the intra-class variance in textual
features. Finally, to expedite future research in text-to-image ReID, we build
a new database named ICFG-PEDES. Extensive experiments demonstrate that SSAN
outperforms state-of-the-art approaches by significant margins. Both the new
ICFG-PEDES database and the SSAN code are available at
https://github.com/zifyloo/SSAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1"&gt;Zefeng Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1"&gt;Changxing Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1"&gt;Zhiyin Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Boundary Proposal Network for Arbitrary Shape Text Detection. (arXiv:2107.12664v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12664</id>
        <link href="http://arxiv.org/abs/2107.12664"/>
        <updated>2021-07-28T02:02:31.489Z</updated>
        <summary type="html"><![CDATA[Arbitrary shape text detection is a challenging task due to the high
complexity and variety of scene texts. In this work, we propose a novel
adaptive boundary proposal network for arbitrary shape text detection, which
can learn to directly produce accurate boundary for arbitrary shape text
without any post-processing. Our method mainly consists of a boundary proposal
model and an innovative adaptive boundary deformation model. The boundary
proposal model constructed by multi-layer dilated convolutions is adopted to
produce prior information (including classification map, distance field, and
direction field) and coarse boundary proposals. The adaptive boundary
deformation model is an encoder-decoder network, in which the encoder mainly
consists of a Graph Convolutional Network (GCN) and a Recurrent Neural Network
(RNN). It aims to perform boundary deformation in an iterative way for
obtaining text instance shape guided by prior information from the boundary
proposal model.In this way, our method can directly and efficiently generate
accurate text boundaries without complex post-processing. Extensive experiments
on publicly available datasets demonstrate the state-of-the-art performance of
our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shi-Xue Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaobin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongfa Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1"&gt;Xu-Cheng Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Counting and Localization in Crowds:A Purely Point-Based Framework. (arXiv:2107.12746v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12746</id>
        <link href="http://arxiv.org/abs/2107.12746"/>
        <updated>2021-07-28T02:02:31.462Z</updated>
        <summary type="html"><![CDATA[Localizing individuals in crowds is more in accordance with the practical
demands of subsequent high-level crowd analysis tasks than simply counting.
However, existing localization based methods relying on intermediate
representations (\textit{i.e.}, density maps or pseudo boxes) serving as
learning targets are counter-intuitive and error-prone. In this paper, we
propose a purely point-based framework for joint crowd counting and individual
localization. For this framework, instead of merely reporting the absolute
counting error at image level, we propose a new metric, called density
Normalized Average Precision (nAP), to provide more comprehensive and more
precise performance evaluation. Moreover, we design an intuitive solution under
this framework, which is called Point to Point Network (P2PNet). P2PNet
discards superfluous steps and directly predicts a set of point proposals to
represent heads in an image, being consistent with the human annotation
results. By thorough analysis, we reveal the key step towards implementing such
a novel idea is to assign optimal learning targets for these proposals.
Therefore, we propose to conduct this crucial association in an one-to-one
matching manner using the Hungarian algorithm. The P2PNet not only
significantly surpasses state-of-the-art methods on popular counting
benchmarks, but also achieves promising localization accuracy. The codes will
be available at: https://github.com/TencentYoutuResearch/CrowdCounting-P2PNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1"&gt;Qingyu Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Changan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhengkai Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yabiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1"&gt;Ying Tai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengjie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jilin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feiyue Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yang Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identify Apple Leaf Diseases Using Deep Learning Algorithm. (arXiv:2107.12598v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12598</id>
        <link href="http://arxiv.org/abs/2107.12598"/>
        <updated>2021-07-28T02:02:31.454Z</updated>
        <summary type="html"><![CDATA[Agriculture is an essential industry in the both society and economy of a
country. However, the pests and diseases cause a great amount of reduction in
agricultural production while there is not sufficient guidance for farmers to
avoid this disaster. To address this problem, we apply CNNs to plant disease
recognition by building a classification model. Within the dataset of 3,642
images of apple leaves, We use a pre-trained image classification model
Restnet34 based on a Convolutional neural network (CNN) with the Fastai
framework in order to save the training time. Overall, the accuracy of
classification is 93.765%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Daping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongyu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jiayu Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CFLOW-AD: Real-Time Unsupervised Anomaly Detection with Localization via Conditional Normalizing Flows. (arXiv:2107.12571v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12571</id>
        <link href="http://arxiv.org/abs/2107.12571"/>
        <updated>2021-07-28T02:02:31.447Z</updated>
        <summary type="html"><![CDATA[Unsupervised anomaly detection with localization has many practical
applications when labeling is infeasible and, moreover, when anomaly examples
are completely missing in the train data. While recently proposed models for
such data setup achieve high accuracy metrics, their complexity is a limiting
factor for real-time processing. In this paper, we propose a real-time model
and analytically derive its relationship to prior methods. Our CFLOW-AD model
is based on a conditional normalizing flow framework adopted for anomaly
detection with localization. In particular, CFLOW-AD consists of a
discriminatively pretrained encoder followed by a multi-scale generative
decoders where the latter explicitly estimate likelihood of the encoded
features. Our approach results in a computationally and memory-efficient model:
CFLOW-AD is faster and smaller by a factor of 10x than prior state-of-the-art
with the same input setting. Our experiments on the MVTec dataset show that
CFLOW-AD outperforms previous methods by 0.36% AUROC in detection task, by
1.12% AUROC and 2.5% AUPRO in localization task, respectively. We open-source
our code with fully reproducible experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gudovskiy_D/0/1/0/all/0/1"&gt;Denis Gudovskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ishizaka_S/0/1/0/all/0/1"&gt;Shun Ishizaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kozuka_K/0/1/0/all/0/1"&gt;Kazuki Kozuka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enriching Local and Global Contexts for Temporal Action Localization. (arXiv:2107.12960v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12960</id>
        <link href="http://arxiv.org/abs/2107.12960"/>
        <updated>2021-07-28T02:02:31.433Z</updated>
        <summary type="html"><![CDATA[Effectively tackling the problem of temporal action localization (TAL)
necessitates a visual representation that jointly pursues two confounding
goals, i.e., fine-grained discrimination for temporal localization and
sufficient visual invariance for action classification. We address this
challenge by enriching both the local and global contexts in the popular
two-stage temporal localization framework, where action proposals are first
generated followed by action classification and temporal boundary regression.
Our proposed model, dubbed ContextLoc, can be divided into three sub-networks:
L-Net, G-Net and P-Net. L-Net enriches the local context via fine-grained
modeling of snippet-level features, which is formulated as a
query-and-retrieval process. G-Net enriches the global context via higher-level
modeling of the video-level representation. In addition, we introduce a novel
context adaptation module to adapt the global context to different proposals.
P-Net further models the context-aware inter-proposal relations. We explore two
existing models to be the P-Net in our experiments. The efficacy of our
proposed method is validated by experimental results on the THUMOS14 (54.3\% at
IoU@0.5) and ActivityNet v1.3 (51.24\% at IoU@0.5) datasets, which outperforms
recent states of the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zixin Zhu&lt;/a&gt; (Xi&amp;#x27;an jiaotong University), &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1"&gt;Wei Tang&lt;/a&gt; (University of Illinois at Chicago), &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Le Wang&lt;/a&gt; (Xi&amp;#x27;an Jiaotong University), &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1"&gt;Nanning Zheng&lt;/a&gt; (Xi&amp;#x27;an Jiaotong University), &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1"&gt;Gang Hua&lt;/a&gt; (Wormpex AI Research)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Technical Report: Quality Assessment Tool for Machine Learning with Clinical CT. (arXiv:2107.12842v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12842</id>
        <link href="http://arxiv.org/abs/2107.12842"/>
        <updated>2021-07-28T02:02:31.423Z</updated>
        <summary type="html"><![CDATA[Image Quality Assessment (IQA) is important for scientific inquiry,
especially in medical imaging and machine learning. Potential data quality
issues can be exacerbated when human-based workflows use limited views of the
data that may obscure digital artifacts. In practice, multiple factors such as
network issues, accelerated acquisitions, motion artifacts, and imaging
protocol design can impede the interpretation of image collections. The medical
image processing community has developed a wide variety of tools for the
inspection and validation of imaging data. Yet, IQA of computed tomography (CT)
remains an under-recognized challenge, and no user-friendly tool is commonly
available to address these potential issues. Here, we create and illustrate a
pipeline specifically designed to identify and resolve issues encountered with
large-scale data mining of clinically acquired CT data. Using the widely
studied National Lung Screening Trial (NLST), we have identified approximately
4% of image volumes with quality concerns out of 17,392 scans. To assess
robustness, we applied the proposed pipeline to our internal datasets where we
find our tool is generalizable to clinically acquired medical images. In
conclusion, the tool has been useful and time-saving for research study of
clinical data, and the code and tutorials are publicly available at
https://github.com/MASILab/QA_tool.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1"&gt;Riqiang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mirza S. Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yucheng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kaiwen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deppen_S/0/1/0/all/0/1"&gt;Steve Deppen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1"&gt;Yuankai Huo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sandler_K/0/1/0/all/0/1"&gt;Kim L. Sandler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Massion_P/0/1/0/all/0/1"&gt;Pierre P. Massion&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Landman_B/0/1/0/all/0/1"&gt;Bennett A. Landman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CKConv: Learning Feature Voxelization for Point Cloud Analysis. (arXiv:2107.12655v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12655</id>
        <link href="http://arxiv.org/abs/2107.12655"/>
        <updated>2021-07-28T02:02:31.412Z</updated>
        <summary type="html"><![CDATA[Despite the remarkable success of deep learning, optimal convolution
operation on point cloud remains indefinite due to its irregular data
structure. In this paper, we present Cubic Kernel Convolution (CKConv) that
learns to voxelize the features of local points by exploiting both continuous
and discrete convolutions. Our continuous convolution uniquely employs a 3D
cubic form of kernel weight representation that splits a feature into voxels in
embedding space. By consecutively applying discrete 3D convolutions on the
voxelized features in a spatial manner, preceding continuous convolution is
forced to learn spatial feature mapping, i.e., feature voxelization. In this
way, geometric information can be detailed by encoding with subdivided
features, and our 3D convolutions on these fixed structured data do not suffer
from discretization artifacts thanks to voxelization in embedding space.
Furthermore, we propose a spatial attention module, Local Set Attention (LSA),
to provide comprehensive structure awareness within the local point set and
hence produce representative features. By learning feature voxelization with
LSA, CKConv can extract enriched features for effective point cloud analysis.
We show that CKConv has great applicability to point cloud processing tasks
including object classification, object part segmentation, and scene semantic
segmentation with state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1"&gt;Sungmin Woo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dogyoon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Junhyeop Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Sangwon Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1"&gt;Woojin Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sangyoun Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-modal estimation of the properties of containers and their content: survey and evaluation. (arXiv:2107.12719v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.12719</id>
        <link href="http://arxiv.org/abs/2107.12719"/>
        <updated>2021-07-28T02:02:31.386Z</updated>
        <summary type="html"><![CDATA[Acoustic and visual sensing can support the contactless estimation of the
weight of a container and the amount of its content when the container is
manipulated by a person. However, transparencies (both of the container and of
the content) and the variability of materials, shapes and sizes make this
problem challenging. In this paper, we present an open benchmarking framework
and an in-depth comparative analysis of recent methods that estimate the
capacity of a container, as well as the type, mass, and amount of its content.
These methods use learned and handcrafted features, such as mel-frequency
cepstrum coefficients, zero-crossing rate, spectrograms, with different types
of classifiers to estimate the type and amount of the content with acoustic
data, and geometric approaches with visual data to determine the capacity of
the container. Results on a newly distributed dataset show that audio alone is
a strong modality and methods achieves a weighted average F1-score up to 81%
and 97% for content type and level classification, respectively. Estimating the
container capacity with vision-only approaches and filling mass with
multi-modal, multi-stage algorithms reaches up to 65% weighted average capacity
and mass scores.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xompero_A/0/1/0/all/0/1"&gt;Alessio Xompero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donaher_S/0/1/0/all/0/1"&gt;Santiago Donaher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iashin_V/0/1/0/all/0/1"&gt;Vladimir Iashin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palermo_F/0/1/0/all/0/1"&gt;Francesca Palermo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Solak_G/0/1/0/all/0/1"&gt;G&amp;#xf6;khan Solak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coppola_C/0/1/0/all/0/1"&gt;Claudio Coppola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ishikawa_R/0/1/0/all/0/1"&gt;Reina Ishikawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagao_Y/0/1/0/all/0/1"&gt;Yuichi Nagao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hachiuma_R/0/1/0/all/0/1"&gt;Ryo Hachiuma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1"&gt;Fan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1"&gt;Chuanlin Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1"&gt;Rosa H. M. Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Christmann_G/0/1/0/all/0/1"&gt;Guilherme Christmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jyun-Ting Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neeharika_G/0/1/0/all/0/1"&gt;Gonuguntla Neeharika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1"&gt;Chinnakotla Krishna Teja Reddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_D/0/1/0/all/0/1"&gt;Dinesh Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rehman_B/0/1/0/all/0/1"&gt;Bakhtawar Ur Rehman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cavallaro_A/0/1/0/all/0/1"&gt;Andrea Cavallaro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Remember What You have drawn: Semantic Image Manipulation with Memory. (arXiv:2107.12579v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12579</id>
        <link href="http://arxiv.org/abs/2107.12579"/>
        <updated>2021-07-28T02:02:31.377Z</updated>
        <summary type="html"><![CDATA[Image manipulation with natural language, which aims to manipulate images
with the guidance of language descriptions, has been a challenging problem in
the fields of computer vision and natural language processing (NLP). Currently,
a number of efforts have been made for this task, but their performances are
still distant away from generating realistic and text-conformed manipulated
images. Therefore, in this paper, we propose a memory-based Image Manipulation
Network (MIM-Net), where a set of memories learned from images is introduced to
synthesize the texture information with the guidance of the textual
description. We propose a two-stage network with an additional reconstruction
stage to learn the latent memories efficiently. To avoid the unnecessary
background changes, we propose a Target Localization Unit (TLU) to focus on the
manipulation of the region mentioned by the text. Moreover, to learn a robust
memory, we further propose a novel randomized memory training loss. Experiments
on the four popular datasets show the better performance of our method compared
to the existing ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1"&gt;Xiangxi Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhonghua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1"&gt;Guosheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jianfei Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1"&gt;Shafiq Joty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Energy-Based Open-World Uncertainty Modeling for Confidence Calibration. (arXiv:2107.12628v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.12628</id>
        <link href="http://arxiv.org/abs/2107.12628"/>
        <updated>2021-07-28T02:02:31.369Z</updated>
        <summary type="html"><![CDATA[Confidence calibration is of great importance to the reliability of decisions
made by machine learning systems. However, discriminative classifiers based on
deep neural networks are often criticized for producing overconfident
predictions that fail to reflect the true correctness likelihood of
classification accuracy. We argue that such an inability to model uncertainty
is mainly caused by the closed-world nature in softmax: a model trained by the
cross-entropy loss will be forced to classify input into one of $K$ pre-defined
categories with high probability. To address this problem, we for the first
time propose a novel $K$+1-way softmax formulation, which incorporates the
modeling of open-world uncertainty as the extra dimension. To unify the
learning of the original $K$-way classification task and the extra dimension
that models uncertainty, we propose a novel energy-based objective function,
and moreover, theoretically prove that optimizing such an objective essentially
forces the extra dimension to capture the marginal data distribution. Extensive
experiments show that our approach, Energy-based Open-World Softmax
(EOW-Softmax), is superior to existing state-of-the-art methods in improving
confidence calibration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yezhen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Che_T/0/1/0/all/0/1"&gt;Tong Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1"&gt;Kaiyang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dongsheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziwei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Outlier Detection using Memory and Contrastive Learning. (arXiv:2107.12642v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12642</id>
        <link href="http://arxiv.org/abs/2107.12642"/>
        <updated>2021-07-28T02:02:31.350Z</updated>
        <summary type="html"><![CDATA[Outlier detection is one of the most important processes taken to create
good, reliable data in machine learning. The most methods of outlier detection
leverage an auxiliary reconstruction task by assuming that outliers are more
difficult to be recovered than normal samples (inliers). However, it is not
always true, especially for auto-encoder (AE) based models. They may recover
certain outliers even outliers are not in the training data, because they do
not constrain the feature learning. Instead, we think outlier detection can be
done in the feature space by measuring the feature distance between outliers
and inliers. We then propose a framework, MCOD, using a memory module and a
contrastive learning module. The memory module constrains the consistency of
features, which represent the normal data. The contrastive learning module
learns more discriminating features, which boosts the distinction between
outliers and inliers. Extensive experiments on four benchmark datasets show
that our proposed MCOD achieves a considerable performance and outperforms nine
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huyan_N/0/1/0/all/0/1"&gt;Ning Huyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quan_D/0/1/0/all/0/1"&gt;Dou Quan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiangrong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xuefeng Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1"&gt;Jocelyn Chanussot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1"&gt;Licheng Jiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Sequence Feature Alignment for Domain Adaptive Detection Transformers. (arXiv:2107.12636v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12636</id>
        <link href="http://arxiv.org/abs/2107.12636"/>
        <updated>2021-07-28T02:02:31.341Z</updated>
        <summary type="html"><![CDATA[Detection transformers have recently shown promising object detection results
and attracted increasing attention. However, how to develop effective domain
adaptation techniques to improve its cross-domain performance remains
unexplored and unclear. In this paper, we delve into this topic and empirically
find that direct feature distribution alignment on the CNN backbone only brings
limited improvements, as it does not guarantee domain-invariant sequence
features in the transformer for prediction. To address this issue, we propose a
novel Sequence Feature Alignment (SFA) method that is specially designed for
the adaptation of detection transformers. Technically, SFA consists of a domain
query-based feature alignment (DQFA) module and a token-wise feature alignment
(TDA) module. In DQFA, a novel domain query is used to aggregate and align
global context from the token sequence of both domains. DQFA reduces the domain
discrepancy in global feature representations and object relations when
deploying in the transformer encoder and decoder, respectively. Meanwhile, TDA
aligns token features in the sequence from both domains, which reduces the
domain gaps in local and instance-level feature representations in the
transformer encoder and decoder, respectively. Besides, a novel bipartite
matching consistency loss is proposed to enhance the feature discriminability
for robust object detection. Experiments on three challenging benchmarks show
that SFA outperforms state-of-the-art domain adaptive object detection methods.
Code has been made available at: https://github.com/encounter1997/SFA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1"&gt;Fengxiang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zheng-Jun Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1"&gt;Yonggang Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computer Vision-Based Guidance Assistance Concept for Plowing Using RGB-D Camera. (arXiv:2107.12646v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12646</id>
        <link href="http://arxiv.org/abs/2107.12646"/>
        <updated>2021-07-28T02:02:31.318Z</updated>
        <summary type="html"><![CDATA[This paper proposes a concept of computer vision-based guidance assistance
for agricultural vehicles to increase the accuracy in plowing and reduce
driver's cognitive burden in long-lasting tillage operations. Plowing is a
common agricultural practice to prepare the soil for planting in many countries
and it can take place both in the spring and the fall. Since plowing operation
requires high traction forces, it causes increased energy consumption.
Moreover, longer operation time due to unnecessary maneuvers leads to higher
fuel consumption. To provide necessary information for the driver and the
control unit of the tractor, a first concept of furrow detection system based
on an RGB-D camera was developed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Turkoz_E/0/1/0/all/0/1"&gt;Erkin T&amp;#xfc;rk&amp;#xf6;z&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olcay_E/0/1/0/all/0/1"&gt;Ertug Olcay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oksanen_T/0/1/0/all/0/1"&gt;Timo Oksanen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parallel Detection for Efficient Video Analytics at the Edge. (arXiv:2107.12563v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2107.12563</id>
        <link href="http://arxiv.org/abs/2107.12563"/>
        <updated>2021-07-28T02:02:31.304Z</updated>
        <summary type="html"><![CDATA[Deep Neural Network (DNN) trained object detectors are widely deployed in
many mission-critical systems for real time video analytics at the edge, such
as autonomous driving and video surveillance. A common performance requirement
in these mission-critical edge services is the near real-time latency of online
object detection on edge devices. However, even with well-trained DNN object
detectors, the online detection quality at edge may deteriorate for a number of
reasons, such as limited capacity to run DNN object detection models on
heterogeneous edge devices, and detection quality degradation due to random
frame dropping when the detection processing rate is significantly slower than
the incoming video frame rate. This paper addresses these problems by
exploiting multi-model multi-device detection parallelism for fast object
detection in edge systems with heterogeneous edge devices. First, we analyze
the performance bottleneck of running a well-trained DNN model at edge for real
time online object detection. We use the offline detection as a reference
model, and examine the root cause by analyzing the mismatch among the incoming
video streaming rate, video processing rate for object detection, and output
rate for real time detection visualization of video streaming. Second, we study
performance optimizations by exploiting multi-model detection parallelism. We
show that the model-parallel detection approach can effectively speed up the
FPS detection processing rate, minimizing the FPS disparity with the incoming
video frame rate on heterogeneous edge devices. We evaluate the proposed
approach using SSD300 and YOLOv3 on benchmark videos of different video stream
rates. The results show that exploiting multi-model detection parallelism can
speed up the online object detection processing rate and deliver near real-time
object detection performance for efficient video analytics at edge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yanzhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Ling Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kompella_R/0/1/0/all/0/1"&gt;Ramana Kompella&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BridgeNet: A Joint Learning Network of Depth Map Super-Resolution and Monocular Depth Estimation. (arXiv:2107.12541v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12541</id>
        <link href="http://arxiv.org/abs/2107.12541"/>
        <updated>2021-07-28T02:02:31.295Z</updated>
        <summary type="html"><![CDATA[Depth map super-resolution is a task with high practical application
requirements in the industry. Existing color-guided depth map super-resolution
methods usually necessitate an extra branch to extract high-frequency detail
information from RGB image to guide the low-resolution depth map
reconstruction. However, because there are still some differences between the
two modalities, direct information transmission in the feature dimension or
edge map dimension cannot achieve satisfactory result, and may even trigger
texture copying in areas where the structures of the RGB-D pair are
inconsistent. Inspired by the multi-task learning, we propose a joint learning
network of depth map super-resolution (DSR) and monocular depth estimation
(MDE) without introducing additional supervision labels. For the interaction of
two subnetworks, we adopt a differentiated guidance strategy and design two
bridges correspondingly. One is the high-frequency attention bridge (HABdg)
designed for the feature encoding process, which learns the high-frequency
information of the MDE task to guide the DSR task. The other is the content
guidance bridge (CGBdg) designed for the depth map reconstruction process,
which provides the content guidance learned from DSR task for MDE task. The
entire network architecture is highly portable and can provide a paradigm for
associating the DSR and MDE tasks. Extensive experiments on benchmark datasets
demonstrate that our method achieves competitive performance. Our code and
models are available at https://rmcong.github.io/proj_BridgeNet.html.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1"&gt;Qi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cong_R/0/1/0/all/0/1"&gt;Runmin Cong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_R/0/1/0/all/0/1"&gt;Ronghui Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lingzhi He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yao Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1"&gt;Sam Kwong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nearest Neighborhood-Based Deep Clustering for Source Data-absent Unsupervised Domain Adaptation. (arXiv:2107.12585v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12585</id>
        <link href="http://arxiv.org/abs/2107.12585"/>
        <updated>2021-07-28T02:02:31.288Z</updated>
        <summary type="html"><![CDATA[In the classic setting of unsupervised domain adaptation (UDA), the labeled
source data are available in the training phase. However, in many real-world
scenarios, owing to some reasons such as privacy protection and information
security, the source data is inaccessible, and only a model trained on the
source domain is available. This paper proposes a novel deep clustering method
for this challenging task. Aiming at the dynamical clustering at feature-level,
we introduce extra constraints hidden in the geometric structure between data
to assist the process. Concretely, we propose a geometry-based constraint,
named semantic consistency on the nearest neighborhood (SCNNH), and use it to
encourage robust clustering. To reach this goal, we construct the nearest
neighborhood for every target data and take it as the fundamental clustering
unit by building our objective on the geometry. Also, we develop a more
SCNNH-compliant structure with an additional semantic credibility constraint,
named semantic hyper-nearest neighborhood (SHNNH). After that, we extend our
method to this new geometry. Extensive experiments on three challenging UDA
datasets indicate that our method achieves state-of-the-art results. The
proposed method has significant improvement on all datasets (as we adopt SHNNH,
the average accuracy increases by over 3.0\% on the large-scaled dataset). Code
is available at https://github.com/tntek/N2DCX.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1"&gt;Song Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhiyuan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hendrich_N/0/1/0/all/0/1"&gt;Norman Hendrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_F/0/1/0/all/0/1"&gt;Fanyu Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1"&gt;Shuzhi Sam Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Changshui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianwei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Perception-and-Regulation Network for Salient Object Detection. (arXiv:2107.12560v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12560</id>
        <link href="http://arxiv.org/abs/2107.12560"/>
        <updated>2021-07-28T02:02:31.238Z</updated>
        <summary type="html"><![CDATA[Effective fusion of different types of features is the key to salient object
detection. The majority of existing network structure design is based on the
subjective experience of scholars and the process of feature fusion does not
consider the relationship between the fused features and highest-level
features. In this paper, we focus on the feature relationship and propose a
novel global attention unit, which we term the "perception- and-regulation"
(PR) block, that adaptively regulates the feature fusion process by explicitly
modeling interdependencies between features. The perception part uses the
structure of fully-connected layers in classification networks to learn the
size and shape of objects. The regulation part selectively strengthens and
weakens the features to be fused. An imitating eye observation module (IEO) is
further employed for improving the global perception ability of the network.
The imitation of foveal vision and peripheral vision enables IEO to scrutinize
highly detailed objects and to organize the broad spatial scene to better
segment objects. Sufficient experiments conducted on SOD datasets demonstrate
that the proposed method performs favorably against 22 state-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jinchao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaoyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1"&gt;Xian Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Junnan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Attacks with Time-Scale Representations. (arXiv:2107.12473v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12473</id>
        <link href="http://arxiv.org/abs/2107.12473"/>
        <updated>2021-07-28T02:02:31.230Z</updated>
        <summary type="html"><![CDATA[We propose a novel framework for real-time black-box universal attacks which
disrupts activations of early convolutional layers in deep learning models. Our
hypothesis is that perturbations produced in the wavelet space disrupt early
convolutional layers more effectively than perturbations performed in the time
domain. The main challenge in adversarial attacks is to preserve low frequency
image content while minimally changing the most meaningful high frequency
content. To address this, we formulate an optimization problem using time-scale
(wavelet) representations as a dual space in three steps. First, we project
original images into orthonormal sub-spaces for low and high scales via wavelet
coefficients. Second, we perturb wavelet coefficients for high scale projection
using a generator network. Third, we generate new adversarial images by
projecting back the original coefficients from the low scale and the perturbed
coefficients from the high scale sub-space. We provide a theoretical framework
that guarantees a dual mapping from time and time-scale domain representations.
We compare our results with state-of-the-art black-box attacks from
generative-based and gradient-based models. We also verify efficacy against
multiple defense methods such as JPEG compression, Guided Denoiser and
Comdefend. Our results show that wavelet-based perturbations consistently
outperform time-based attacks thus providing new insights into vulnerabilities
of deep learning models and could potentially lead to robust architectures or
new defense and attack mechanisms by leveraging time-scale representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Santamaria_Pang_A/0/1/0/all/0/1"&gt;Alberto Santamaria-Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1"&gt;Jianwei Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1"&gt;Aritra Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kubricht_J/0/1/0/all/0/1"&gt;James Kubricht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_P/0/1/0/all/0/1"&gt;Peter Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naresh_I/0/1/0/all/0/1"&gt;Iyer Naresh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Virani_N/0/1/0/all/0/1"&gt;Nurali Virani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MonoIndoor: Towards Good Practice of Self-Supervised Monocular Depth Estimation for Indoor Environments. (arXiv:2107.12429v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12429</id>
        <link href="http://arxiv.org/abs/2107.12429"/>
        <updated>2021-07-28T02:02:31.221Z</updated>
        <summary type="html"><![CDATA[Self-supervised depth estimation for indoor environments is more challenging
than its outdoor counterpart in at least the following two aspects: (i) the
depth range of indoor sequences varies a lot across different frames, making it
difficult for the depth network to induce consistent depth cues, whereas the
maximum distance in outdoor scenes mostly stays the same as the camera usually
sees the sky; (ii) the indoor sequences contain much more rotational motions,
which cause difficulties for the pose network, while the motions of outdoor
sequences are pre-dominantly translational, especially for driving datasets
such as KITTI. In this paper, special considerations are given to those
challenges and a set of good practices are consolidated for improving the
performance of self-supervised monocular depth estimation in indoor
environments. The proposed method mainly consists of two novel modules, \ie, a
depth factorization module and a residual pose estimation module, each of which
is designed to respectively tackle the aforementioned challenges. The
effectiveness of each module is shown through a carefully conducted ablation
study and the demonstration of the state-of-the-art performance on two indoor
datasets, \ie, EuRoC and NYUv2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_P/0/1/0/all/0/1"&gt;Pan Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Runze Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhanu_B/0/1/0/all/0/1"&gt;Bir Bhanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yi Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Video Object Segmentation by Motion-Aware Mask Propagation. (arXiv:2107.12569v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12569</id>
        <link href="http://arxiv.org/abs/2107.12569"/>
        <updated>2021-07-28T02:02:31.204Z</updated>
        <summary type="html"><![CDATA[We propose a self-supervised spatio-temporal matching method coined
Motion-Aware Mask Propagation (MAMP) for semi-supervised video object
segmentation. During training, MAMP leverages the frame reconstruction task to
train the model without the need for annotations. During inference, MAMP
extracts high-resolution features from each frame to build a memory bank from
the features as well as the predicted masks of selected past frames. MAMP then
propagates the masks from the memory bank to subsequent frames according to our
motion-aware spatio-temporal matching module, also proposed in this paper.
Evaluation on DAVIS-2017 and YouTube-VOS datasets show that MAMP achieves
state-of-the-art performance with stronger generalization ability compared to
existing self-supervised methods, i.e. 4.9\% higher mean
$\mathcal{J}\&\mathcal{F}$ on DAVIS-2017 and 4.85\% higher mean
$\mathcal{J}\&\mathcal{F}$ on the unseen categories of YouTube-VOS than the
nearest competitor. Moreover, MAMP performs on par with many supervised video
object segmentation methods. Our code is available at:
\url{https://github.com/bo-miao/MAMP}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miao_B/0/1/0/all/0/1"&gt;Bo Miao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1"&gt;Mohammed Bennamoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yongsheng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1"&gt;Ajmal Mian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Circular-Symmetric Correlation Layer based on FFT. (arXiv:2107.12480v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12480</id>
        <link href="http://arxiv.org/abs/2107.12480"/>
        <updated>2021-07-28T02:02:30.923Z</updated>
        <summary type="html"><![CDATA[Despite the vast success of standard planar convolutional neural networks,
they are not the most efficient choice for analyzing signals that lie on an
arbitrarily curved manifold, such as a cylinder. The problem arises when one
performs a planar projection of these signals and inevitably causes them to be
distorted or broken where there is valuable information. We propose a
Circular-symmetric Correlation Layer (CCL) based on the formalism of
roto-translation equivariant correlation on the continuous group $S^1 \times
\mathbb{R}$, and implement it efficiently using the well-known Fast Fourier
Transform (FFT) algorithm. We showcase the performance analysis of a general
network equipped with CCL on various recognition and classification tasks and
datasets. The PyTorch package implementation of CCL is provided online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Azari_B/0/1/0/all/0/1"&gt;Bahar Azari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdogmus_D/0/1/0/all/0/1"&gt;Deniz Erdogmus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Grounding with 3D Objects. (arXiv:2107.12514v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.12514</id>
        <link href="http://arxiv.org/abs/2107.12514"/>
        <updated>2021-07-28T02:02:30.915Z</updated>
        <summary type="html"><![CDATA[Seemingly simple natural language requests to a robot are generally
underspecified, for example "Can you bring me the wireless mouse?" When viewing
mice on the shelf, the number of buttons or presence of a wire may not be
visible from certain angles or positions. Flat images of candidate mice may not
provide the discriminative information needed for "wireless". The world, and
objects in it, are not flat images but complex 3D shapes. If a human requests
an object based on any of its basic properties, such as color, shape, or
texture, robots should perform the necessary exploration to accomplish the
task. In particular, while substantial effort and progress has been made on
understanding explicitly visual attributes like color and category,
comparatively little progress has been made on understanding language about
shapes and contours. In this work, we introduce a novel reasoning task that
targets both visual and non-visual language about 3D objects. Our new
benchmark, ShapeNet Annotated with Referring Expressions (SNARE), requires a
model to choose which of two objects is being referenced by a natural language
description. We introduce several CLIP-based models for distinguishing objects
and demonstrate that while recent advances in jointly modeling vision and
language are useful for robotic language understanding, it is still the case
that these models are weaker at understanding the 3D nature of objects --
properties which play a key role in manipulation. In particular, we find that
adding view estimation to language grounding models improves accuracy on both
SNARE and when identifying objects referred to in language on a robot platform.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1"&gt;Jesse Thomason&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shridhar_M/0/1/0/all/0/1"&gt;Mohit Shridhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1"&gt;Yonatan Bisk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1"&gt;Chris Paxton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1"&gt;Luke Zettlemoyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13948</id>
        <link href="http://arxiv.org/abs/2106.13948"/>
        <updated>2021-07-28T02:02:30.851Z</updated>
        <summary type="html"><![CDATA[Recent advances in the areas of multimodal machine learning and artificial
intelligence (AI) have led to the development of challenging tasks at the
intersection of Computer Vision, Natural Language Processing, and Embodied AI.
Whereas many approaches and previous survey pursuits have characterised one or
two of these dimensions, there has not been a holistic analysis at the center
of all three. Moreover, even when combinations of these topics are considered,
more focus is placed on describing, e.g., current architectural methods, as
opposed to also illustrating high-level challenges and opportunities for the
field. In this survey paper, we discuss Embodied Vision-Language Planning
(EVLP) tasks, a family of prominent embodied navigation and manipulation
problems that jointly use computer vision and natural language. We propose a
taxonomy to unify these tasks and provide an in-depth analysis and comparison
of the new and current algorithmic approaches, metrics, simulated environments,
as well as the datasets used for EVLP tasks. Finally, we present the core
challenges that we believe new EVLP works should seek to address, and we
advocate for task construction that enables model generalizability and furthers
real-world deployment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1"&gt;Jonathan Francis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1"&gt;Nariaki Kitamura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1"&gt;Felix Labelle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaopeng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1"&gt;Ingrid Navarro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1"&gt;Jean Oh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Formal Language Theory Meets Modern NLP. (arXiv:2102.10094v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10094</id>
        <link href="http://arxiv.org/abs/2102.10094"/>
        <updated>2021-07-28T02:02:30.830Z</updated>
        <summary type="html"><![CDATA[NLP is deeply intertwined with the formal study of language, both
conceptually and historically. Arguably, this connection goes all the way back
to Chomsky's Syntactic Structures in 1957. It also still holds true today, with
a strand of recent works building formal analysis of modern neural networks
methods in terms of formal languages. In this document, I aim to explain
background about formal languages as they relate to this recent work. I will by
necessity ignore large parts of the rich history of this field, instead
focusing on concepts connecting to modern deep learning-based NLP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1"&gt;William Merrill&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transferable Knowledge-Based Multi-Granularity Aggregation Network for Temporal Action Localization: Submission to ActivityNet Challenge 2021. (arXiv:2107.12618v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12618</id>
        <link href="http://arxiv.org/abs/2107.12618"/>
        <updated>2021-07-28T02:02:30.805Z</updated>
        <summary type="html"><![CDATA[This technical report presents an overview of our solution used in the
submission to 2021 HACS Temporal Action Localization Challenge on both
Supervised Learning Track and Weakly-Supervised Learning Track. Temporal Action
Localization (TAL) requires to not only precisely locate the temporal
boundaries of action instances, but also accurately classify the untrimmed
videos into specific categories. However, Weakly-Supervised TAL indicates
locating the action instances using only video-level class labels. In this
paper, to train a supervised temporal action localizer, we adopt Temporal
Context Aggregation Network (TCANet) to generate high-quality action proposals
through ``local and global" temporal context aggregation and complementary as
well as progressive boundary refinement. As for the WSTAL, a novel framework is
proposed to handle the poor quality of CAS generated by simple classification
network, which can only focus on local discriminative parts, rather than locate
the entire interval of target actions. Further inspired by the transfer
learning method, we also adopt an additional module to transfer the knowledge
from trimmed videos (HACS Clips dataset) to untrimmed videos (HACS Segments
dataset), aiming at promoting the classification performance on untrimmed
videos. Finally, we employ a boundary regression module embedded with
Outer-Inner-Contrastive (OIC) loss to automatically predict the boundaries
based on the enhanced CAS. Our proposed scheme achieves 39.91 and 29.78 average
mAP on the challenge testing set of supervised and weakly-supervised temporal
action localization track respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Haisheng Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_P/0/1/0/all/0/1"&gt;Peiqin Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yukun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dongliang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1"&gt;Weihao Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangled Implicit Shape and Pose Learning for Scalable 6D Pose Estimation. (arXiv:2107.12549v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12549</id>
        <link href="http://arxiv.org/abs/2107.12549"/>
        <updated>2021-07-28T02:02:30.796Z</updated>
        <summary type="html"><![CDATA[6D pose estimation of rigid objects from a single RGB image has seen
tremendous improvements recently by using deep learning to combat complex
real-world variations, but a majority of methods build models on the per-object
level, failing to scale to multiple objects simultaneously. In this paper, we
present a novel approach for scalable 6D pose estimation, by self-supervised
learning on synthetic data of multiple objects using a single autoencoder. To
handle multiple objects and generalize to unseen objects, we disentangle the
latent object shape and pose representations, so that the latent shape space
models shape similarities, and the latent pose code is used for rotation
retrieval by comparison with canonical rotations. To encourage shape space
construction, we apply contrastive metric learning and enable the processing of
unseen objects by referring to similar training objects. The different
symmetries across objects induce inconsistent latent pose spaces, which we
capture with a conditioned block producing shape-dependent pose codebooks by
re-entangling shape and pose representations. We test our method on two
multi-object benchmarks with real data, T-LESS and NOCS REAL275, and show it
outperforms existing RGB-based methods in terms of pose estimation accuracy and
generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1"&gt;Yilin Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiangyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1"&gt;Hao Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1"&gt;Taku Komura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenping Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Competition in Cross-situational Word Learning: A Computational Study. (arXiv:2012.03370v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03370</id>
        <link href="http://arxiv.org/abs/2012.03370"/>
        <updated>2021-07-28T02:02:30.787Z</updated>
        <summary type="html"><![CDATA[Children learn word meanings by tapping into the commonalities across
different situations in which words are used and overcome the high level of
uncertainty involved in early word learning experiences. We propose a modeling
framework to investigate the role of mutual exclusivity bias - asserting
one-to-one mappings between words and their meanings - in reducing uncertainty
in word learning. In a set of computational studies, we show that to
successfully learn word meanings in the face of uncertainty, a learner needs to
use two types of competition: words competing for association to a referent
when learning from an observation and referents competing for a word when the
word is used. Our work highlights the importance of an algorithmic-level
analysis to shed light on the utility of different mechanisms that can
implement the same computational-level theory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nematzadeh_A/0/1/0/all/0/1"&gt;Aida Nematzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shekarchi_Z/0/1/0/all/0/1"&gt;Zahra Shekarchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1"&gt;Thomas L. Griffiths&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stevenson_S/0/1/0/all/0/1"&gt;Suzanne Stevenson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Question-Answering as an Automatic Metric for Evaluating the Content Quality of a Summary. (arXiv:2010.00490v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.00490</id>
        <link href="http://arxiv.org/abs/2010.00490"/>
        <updated>2021-07-28T02:02:30.780Z</updated>
        <summary type="html"><![CDATA[A desirable property of a reference-based evaluation metric that measures the
content quality of a summary is that it should estimate how much information
that summary has in common with a reference. Traditional text overlap based
metrics such as ROUGE fail to achieve this because they are limited to matching
tokens, either lexically or via embeddings. In this work, we propose a metric
to evaluate the content quality of a summary using question-answering (QA).
QA-based methods directly measure a summary's information overlap with a
reference, making them fundamentally different than text overlap metrics. We
demonstrate the experimental benefits of QA-based metrics through an analysis
of our proposed metric, QAEval. QAEval out-performs current state-of-the-art
metrics on most evaluations using benchmark datasets, while being competitive
on others due to limitations of state-of-the-art models. Through a careful
analysis of each component of QAEval, we identify its performance bottlenecks
and estimate that its potential upper-bound performance surpasses all other
automatic metrics, approaching that of the gold-standard Pyramid Method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deutsch_D/0/1/0/all/0/1"&gt;Daniel Deutsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bedrax_Weiss_T/0/1/0/all/0/1"&gt;Tania Bedrax-Weiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1"&gt;Dan Roth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Efficient Tensor Decomposition-Based DNN Model Compression with Optimization Framework. (arXiv:2107.12422v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12422</id>
        <link href="http://arxiv.org/abs/2107.12422"/>
        <updated>2021-07-28T02:02:30.772Z</updated>
        <summary type="html"><![CDATA[Advanced tensor decomposition, such as Tensor train (TT) and Tensor ring
(TR), has been widely studied for deep neural network (DNN) model compression,
especially for recurrent neural networks (RNNs). However, compressing
convolutional neural networks (CNNs) using TT/TR always suffers significant
accuracy loss. In this paper, we propose a systematic framework for tensor
decomposition-based model compression using Alternating Direction Method of
Multipliers (ADMM). By formulating TT decomposition-based model compression to
an optimization problem with constraints on tensor ranks, we leverage ADMM
technique to systemically solve this optimization problem in an iterative way.
During this procedure, the entire DNN model is trained in the original
structure instead of TT format, but gradually enjoys the desired low tensor
rank characteristics. We then decompose this uncompressed model to TT format
and fine-tune it to finally obtain a high-accuracy TT-format DNN model. Our
framework is very general, and it works for both CNNs and RNNs, and can be
easily modified to fit other tensor decomposition approaches. We evaluate our
proposed framework on different DNN models for image classification and video
recognition tasks. Experimental results show that our ADMM-based TT-format
models demonstrate very high compression performance with high accuracy.
Notably, on CIFAR-100, with 2.3X and 2.4X compression ratios, our models have
1.96% and 2.21% higher top-1 accuracy than the original ResNet-20 and
ResNet-32, respectively. For compressing ResNet-18 on ImageNet, our model
achieves 2.47X FLOPs reduction without accuracy loss.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1"&gt;Miao Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1"&gt;Yang Sui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1"&gt;Siyu Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1"&gt;Bo Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Segmentation in Style: Unsupervised Semantic Image Segmentation with Stylegan and CLIP. (arXiv:2107.12518v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12518</id>
        <link href="http://arxiv.org/abs/2107.12518"/>
        <updated>2021-07-28T02:02:30.749Z</updated>
        <summary type="html"><![CDATA[We introduce a method that allows to automatically segment images into
semantically meaningful regions without human supervision. Derived regions are
consistent across different images and coincide with human-defined semantic
classes on some datasets. In cases where semantic regions might be hard for
human to define and consistently label, our method is still able to find
meaningful and consistent semantic classes. In our work, we use pretrained
StyleGAN2~\cite{karras2020analyzing} generative model: clustering in the
feature space of the generative model allows to discover semantic classes. Once
classes are discovered, a synthetic dataset with generated images and
corresponding segmentation masks can be created. After that a segmentation
model is trained on the synthetic dataset and is able to generalize to real
images. Additionally, by using CLIP~\cite{radford2021learning} we are able to
use prompts defined in a natural language to discover some desired semantic
classes. We test our method on publicly available datasets and show
state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pakhomov_D/0/1/0/all/0/1"&gt;Daniil Pakhomov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hira_S/0/1/0/all/0/1"&gt;Sanchit Hira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wagle_N/0/1/0/all/0/1"&gt;Narayani Wagle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Green_K/0/1/0/all/0/1"&gt;Kemar E. Green&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1"&gt;Nassir Navab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noisy Self-Knowledge Distillation for Text Summarization. (arXiv:2009.07032v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.07032</id>
        <link href="http://arxiv.org/abs/2009.07032"/>
        <updated>2021-07-28T02:02:30.742Z</updated>
        <summary type="html"><![CDATA[In this paper we apply self-knowledge distillation to text summarization
which we argue can alleviate problems with maximum-likelihood training on
single reference and noisy datasets. Instead of relying on one-hot annotation
labels, our student summarization model is trained with guidance from a teacher
which generates smoothed labels to help regularize training. Furthermore, to
better model uncertainty during training, we introduce multiple noise signals
for both teacher and student models. We demonstrate experimentally on three
benchmarks that our framework boosts the performance of both pretrained and
non-pretrained summarizers achieving state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1"&gt;Sheng Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1"&gt;Mirella Lapata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emotion Stimulus Detection in German News Headlines. (arXiv:2107.12920v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.12920</id>
        <link href="http://arxiv.org/abs/2107.12920"/>
        <updated>2021-07-28T02:02:30.731Z</updated>
        <summary type="html"><![CDATA[Emotion stimulus extraction is a fine-grained subtask of emotion analysis
that focuses on identifying the description of the cause behind an emotion
expression from a text passage (e.g., in the sentence "I am happy that I passed
my exam" the phrase "passed my exam" corresponds to the stimulus.). Previous
work mainly focused on Mandarin and English, with no resources or models for
German. We fill this research gap by developing a corpus of 2006 German news
headlines annotated with emotions and 811 instances with annotations of
stimulus phrases. Given that such corpus creation efforts are time-consuming
and expensive, we additionally work on an approach for projecting the existing
English GoodNewsEveryone (GNE) corpus to a machine-translated German version.
We compare the performance of a conditional random field (CRF) model (trained
monolingually on German and cross-lingually via projection) with a multilingual
XLM-RoBERTa (XLM-R) model. Our results show that training with the German
corpus achieves higher F1 scores than projection. Experiments with XLM-R
outperform their respective CRF counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dang%7D_%7B/0/1/0/all/0/1"&gt;{Bao Minh} {Doan Dang}&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oberlander_L/0/1/0/all/0/1"&gt;Laura Oberl&amp;#xe4;nder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1"&gt;Roman Klinger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-modal Consensus Network for Weakly Supervised Temporal Action Localization. (arXiv:2107.12589v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12589</id>
        <link href="http://arxiv.org/abs/2107.12589"/>
        <updated>2021-07-28T02:02:30.724Z</updated>
        <summary type="html"><![CDATA[Weakly supervised temporal action localization (WS-TAL) is a challenging task
that aims to localize action instances in the given video with video-level
categorical supervision. Both appearance and motion features are used in
previous works, while they do not utilize them in a proper way but apply simple
concatenation or score-level fusion. In this work, we argue that the features
extracted from the pretrained extractor, e.g., I3D, are not the
WS-TALtask-specific features, thus the feature re-calibration is needed for
reducing the task-irrelevant information redundancy. Therefore, we propose a
cross-modal consensus network (CO2-Net) to tackle this problem. In CO2-Net, we
mainly introduce two identical proposed cross-modal consensus modules (CCM)
that design a cross-modal attention mechanism to filter out the task-irrelevant
information redundancy using the global information from the main modality and
the cross-modal local information of the auxiliary modality. Moreover, we treat
the attention weights derived from each CCMas the pseudo targets of the
attention weights derived from another CCM to maintain the consistency between
the predictions derived from two CCMs, forming a mutual learning manner.
Finally, we conduct extensive experiments on two common used temporal action
localization datasets, THUMOS14 and ActivityNet1.2, to verify our method and
achieve the state-of-the-art results. The experimental results show that our
proposed cross-modal consensus module can produce more representative features
for temporal action localization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_F/0/1/0/all/0/1"&gt;Fa-Ting Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jia-Chang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Dan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1"&gt;Ying Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1"&gt;Wei-Shi Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation. (arXiv:2101.00390v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00390</id>
        <link href="http://arxiv.org/abs/2101.00390"/>
        <updated>2021-07-28T02:02:30.716Z</updated>
        <summary type="html"><![CDATA[We introduce VoxPopuli, a large-scale multilingual corpus providing 100K
hours of unlabelled speech data in 23 languages. It is the largest open data to
date for unsupervised representation learning as well as semi-supervised
learning. VoxPopuli also contains 1.8K hours of transcribed speeches in 16
languages and their aligned oral interpretations into 5 other languages
totaling 5.1K hours. We provide speech recognition baselines and validate the
versatility of VoxPopuli unlabelled data in semi-supervised learning under
challenging out-of-domain settings. We will release the corpus at
https://github.com/facebookresearch/voxpopuli under an open license.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Changhan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riviere_M/0/1/0/all/0/1"&gt;Morgane Rivi&amp;#xe8;re&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1"&gt;Ann Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1"&gt;Anne Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talnikar_C/0/1/0/all/0/1"&gt;Chaitanya Talnikar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haziza_D/0/1/0/all/0/1"&gt;Daniel Haziza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williamson_M/0/1/0/all/0/1"&gt;Mary Williamson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1"&gt;Juan Pino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1"&gt;Emmanuel Dupoux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extraction and Analysis of Fictional Character Networks: A Survey. (arXiv:1907.02704v4 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1907.02704</id>
        <link href="http://arxiv.org/abs/1907.02704"/>
        <updated>2021-07-28T02:02:30.707Z</updated>
        <summary type="html"><![CDATA[A character network is a graph extracted from a narrative, in which vertices
represent characters and edges correspond to interactions between them. A
number of narrative-related problems can be addressed automatically through the
analysis of character networks, such as summarization, classification, or role
detection. Character networks are particularly relevant when considering works
of fictions (e.g. novels, plays, movies, TV series), as their exploitation
allows developing information retrieval and recommendation systems. However,
works of fiction possess specific properties making these tasks harder. This
survey aims at presenting and organizing the scientific literature related to
the extraction of character networks from works of fiction, as well as their
analysis. We first describe the extraction process in a generic way, and
explain how its constituting steps are implemented in practice, depending on
the medium of the narrative, the goal of the network analysis, and other
factors. We then review the descriptive tools used to characterize character
networks, with a focus on the way they are interpreted in this context. We
illustrate the relevance of character networks by also providing a review of
applications derived from their analysis. Finally, we identify the limitations
of the existing approaches, and the most promising perspectives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Labatut_V/0/1/0/all/0/1"&gt;Vincent Labatut&lt;/a&gt; (LIA), &lt;a href="http://arxiv.org/find/cs/1/au:+Bost_X/0/1/0/all/0/1"&gt;Xavier Bost&lt;/a&gt; (LIA)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Scene Graph Generation (SGG) Benchmark. (arXiv:2107.12604v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12604</id>
        <link href="http://arxiv.org/abs/2107.12604"/>
        <updated>2021-07-28T02:02:30.684Z</updated>
        <summary type="html"><![CDATA[There is a surge of interest in image scene graph generation (object,
attribute and relationship detection) due to the need of building fine-grained
image understanding models that go beyond object detection. Due to the lack of
a good benchmark, the reported results of different scene graph generation
models are not directly comparable, impeding the research progress. We have
developed a much-needed scene graph generation benchmark based on the
maskrcnn-benchmark and several popular models. This paper presents main
features of our benchmark and a comprehensive ablation study of scene graph
generation models using the Visual Genome and OpenImages Visual relationship
detection datasets. Our codebase is made publicly available at
https://github.com/microsoft/scene_graph_benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xiaotian Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jianwei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Houdong Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pengchuan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comprehensive Study on Colorectal Polyp Segmentation with ResUNet++, Conditional Random Field and Test-Time Augmentation. (arXiv:2107.12435v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12435</id>
        <link href="http://arxiv.org/abs/2107.12435"/>
        <updated>2021-07-28T02:02:30.676Z</updated>
        <summary type="html"><![CDATA[Colonoscopy is considered the gold standard for detection of colorectal
cancer and its precursors. Existing examination methods are, however, hampered
by high overall miss-rate, and many abnormalities are left undetected.
Computer-Aided Diagnosis systems based on advanced machine learning algorithms
are touted as a game-changer that can identify regions in the colon overlooked
by the physicians during endoscopic examinations, and help detect and
characterize lesions. In previous work, we have proposed the ResUNet++
architecture and demonstrated that it produces more efficient results compared
with its counterparts U-Net and ResUNet. In this paper, we demonstrate that
further improvements to the overall prediction performance of the ResUNet++
architecture can be achieved by using conditional random field and test-time
augmentation. We have performed extensive evaluations and validated the
improvements using six publicly available datasets: Kvasir-SEG, CVC-ClinicDB,
CVC-ColonDB, ETIS-Larib Polyp DB, ASU-Mayo Clinic Colonoscopy Video Database,
and CVC-VideoClinicDB. Moreover, we compare our proposed architecture and
resulting model with other State-of-the-art methods. To explore the
generalization capability of ResUNet++ on different publicly available polyp
datasets, so that it could be used in a real-world setting, we performed an
extensive cross-dataset evaluation. The experimental results show that applying
CRF and TTA improves the performance on various polyp segmentation datasets
both on the same dataset and cross-dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jha_D/0/1/0/all/0/1"&gt;Debesh Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smedsrud_P/0/1/0/all/0/1"&gt;Pia H. Smedsrud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johansen_D/0/1/0/all/0/1"&gt;Dag Johansen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lange_T/0/1/0/all/0/1"&gt;Thomas de Lange&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johansen_H/0/1/0/all/0/1"&gt;H&amp;#xe5;vard D. Johansen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Halvorsen_P/0/1/0/all/0/1"&gt;P&amp;#xe5;l Halvorsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riegler_M/0/1/0/all/0/1"&gt;Michael A. Riegler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CalCROP21: A Georeferenced multi-spectral dataset of Satellite Imagery and Crop Labels. (arXiv:2107.12499v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12499</id>
        <link href="http://arxiv.org/abs/2107.12499"/>
        <updated>2021-07-28T02:02:30.668Z</updated>
        <summary type="html"><![CDATA[Mapping and monitoring crops is a key step towards sustainable
intensification of agriculture and addressing global food security. A dataset
like ImageNet that revolutionized computer vision applications can accelerate
development of novel crop mapping techniques. Currently, the United States
Department of Agriculture (USDA) annually releases the Cropland Data Layer
(CDL) which contains crop labels at 30m resolution for the entire United States
of America. While CDL is state of the art and is widely used for a number of
agricultural applications, it has a number of limitations (e.g., pixelated
errors, labels carried over from previous errors and absence of input imagery
along with class labels). In this work, we create a new semantic segmentation
benchmark dataset, which we call CalCROP21, for the diverse crops in the
Central Valley region of California at 10m spatial resolution using a Google
Earth Engine based robust image processing pipeline and a novel attention based
spatio-temporal semantic segmentation algorithm STATT. STATT uses re-sampled
(interpolated) CDL labels for training, but is able to generate a better
prediction than CDL by leveraging spatial and temporal patterns in Sentinel2
multi-spectral image series to effectively capture phenologic differences
amongst crops and uses attention to reduce the impact of clouds and other
atmospheric disturbances. We also present a comprehensive evaluation to show
that STATT has significantly better results when compared to the resampled CDL
labels. We have released the dataset and the processing pipeline code for
generating the benchmark dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1"&gt;Rahul Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravirathinam_P/0/1/0/all/0/1"&gt;Praveen Ravirathinam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xiaowei Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1"&gt;Ankush Khandelwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mulla_D/0/1/0/all/0/1"&gt;David Mulla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vipin Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SaRNet: A Dataset for Deep Learning Assisted Search and Rescue with Satellite Imagery. (arXiv:2107.12469v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.12469</id>
        <link href="http://arxiv.org/abs/2107.12469"/>
        <updated>2021-07-28T02:02:30.657Z</updated>
        <summary type="html"><![CDATA[Access to high resolution satellite imagery has dramatically increased in
recent years as several new constellations have entered service. High revisit
frequencies as well as improved resolution has widened the use cases of
satellite imagery to areas such as humanitarian relief and even Search and
Rescue (SaR). We propose a novel remote sensing object detection dataset for
deep learning assisted SaR. This dataset contains only small objects that have
been identified as potential targets as part of a live SaR response. We
evaluate the application of popular object detection models to this dataset as
a baseline to inform further research. We also propose a novel object detection
metric, specifically designed to be used in a deep learning assisted SaR
setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Thoreau_M/0/1/0/all/0/1"&gt;Michael Thoreau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wilson_F/0/1/0/all/0/1"&gt;Frazer Wilson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting User Emotional Tone in Mental Disorder Online Communities. (arXiv:2005.07473v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.07473</id>
        <link href="http://arxiv.org/abs/2005.07473"/>
        <updated>2021-07-28T02:02:30.649Z</updated>
        <summary type="html"><![CDATA[In recent years, Online Social Networks have become an important medium for
people who suffer from mental disorders to share moments of hardship, and
receive emotional and informational support. In this work, we analyze how
discussions in Reddit communities related to mental disorders can help improve
the health conditions of their users. Using the emotional tone of users'
writing as a proxy for emotional state, we uncover relationships between user
interactions and state changes. First, we observe that authors of negative
posts often write rosier comments after engaging in discussions, indicating
that users' emotional state can improve due to social support. Second, we build
models based on SOTA text embedding techniques and RNNs to predict shifts in
emotional tone. This differs from most of related work, which focuses primarily
on detecting mental disorders from user activity. We demonstrate the
feasibility of accurately predicting the users' reactions to the interactions
experienced in these platforms, and present some examples which illustrate that
the models are correctly capturing the effects of comments on the author's
emotional tone. Our models hold promising implications for interventions to
provide support for people struggling with mental illnesses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Silveira_B/0/1/0/all/0/1"&gt;B&amp;#xe1;rbara Silveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_H/0/1/0/all/0/1"&gt;Henrique S. Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murai_F/0/1/0/all/0/1"&gt;Fabricio Murai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1"&gt;Ana Paula Couto da Silva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[gaBERT -- an Irish Language Model. (arXiv:2107.12930v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.12930</id>
        <link href="http://arxiv.org/abs/2107.12930"/>
        <updated>2021-07-28T02:02:30.630Z</updated>
        <summary type="html"><![CDATA[The BERT family of neural language models have become highly popular due to
their ability to provide sequences of text with rich context-sensitive token
encodings which are able to generalise well to many Natural Language Processing
tasks. Over 120 monolingual BERT models covering over 50 languages have been
released, as well as a multilingual model trained on 104 languages. We
introduce, gaBERT, a monolingual BERT model for the Irish language. We compare
our gaBERT model to multilingual BERT and show that gaBERT provides better
representations for a downstream parsing task. We also show how different
filtering criteria, vocabulary size and the choice of subword tokenisation
model affect downstream performance. We release gaBERT and related code to the
community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barry_J/0/1/0/all/0/1"&gt;James Barry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wagner_J/0/1/0/all/0/1"&gt;Joachim Wagner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cassidy_L/0/1/0/all/0/1"&gt;Lauren Cassidy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cowap_A/0/1/0/all/0/1"&gt;Alan Cowap&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lynn_T/0/1/0/all/0/1"&gt;Teresa Lynn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walsh_A/0/1/0/all/0/1"&gt;Abigail Walsh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meachair_M/0/1/0/all/0/1"&gt;M&amp;#xed;che&amp;#xe1;l J. &amp;#xd3; Meachair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1"&gt;Jennifer Foster&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-modal estimation of the properties of containers and their content: survey and evaluation. (arXiv:2107.12719v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.12719</id>
        <link href="http://arxiv.org/abs/2107.12719"/>
        <updated>2021-07-28T02:02:30.621Z</updated>
        <summary type="html"><![CDATA[Acoustic and visual sensing can support the contactless estimation of the
weight of a container and the amount of its content when the container is
manipulated by a person. However, transparencies (both of the container and of
the content) and the variability of materials, shapes and sizes make this
problem challenging. In this paper, we present an open benchmarking framework
and an in-depth comparative analysis of recent methods that estimate the
capacity of a container, as well as the type, mass, and amount of its content.
These methods use learned and handcrafted features, such as mel-frequency
cepstrum coefficients, zero-crossing rate, spectrograms, with different types
of classifiers to estimate the type and amount of the content with acoustic
data, and geometric approaches with visual data to determine the capacity of
the container. Results on a newly distributed dataset show that audio alone is
a strong modality and methods achieves a weighted average F1-score up to 81%
and 97% for content type and level classification, respectively. Estimating the
container capacity with vision-only approaches and filling mass with
multi-modal, multi-stage algorithms reaches up to 65% weighted average capacity
and mass scores.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xompero_A/0/1/0/all/0/1"&gt;Alessio Xompero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donaher_S/0/1/0/all/0/1"&gt;Santiago Donaher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iashin_V/0/1/0/all/0/1"&gt;Vladimir Iashin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palermo_F/0/1/0/all/0/1"&gt;Francesca Palermo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Solak_G/0/1/0/all/0/1"&gt;G&amp;#xf6;khan Solak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coppola_C/0/1/0/all/0/1"&gt;Claudio Coppola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ishikawa_R/0/1/0/all/0/1"&gt;Reina Ishikawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagao_Y/0/1/0/all/0/1"&gt;Yuichi Nagao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hachiuma_R/0/1/0/all/0/1"&gt;Ryo Hachiuma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1"&gt;Fan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1"&gt;Chuanlin Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1"&gt;Rosa H. M. Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Christmann_G/0/1/0/all/0/1"&gt;Guilherme Christmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jyun-Ting Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neeharika_G/0/1/0/all/0/1"&gt;Gonuguntla Neeharika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1"&gt;Chinnakotla Krishna Teja Reddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_D/0/1/0/all/0/1"&gt;Dinesh Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rehman_B/0/1/0/all/0/1"&gt;Bakhtawar Ur Rehman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cavallaro_A/0/1/0/all/0/1"&gt;Andrea Cavallaro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extraction and Analysis of Fictional Character Networks: A Survey. (arXiv:1907.02704v4 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1907.02704</id>
        <link href="http://arxiv.org/abs/1907.02704"/>
        <updated>2021-07-28T02:02:30.601Z</updated>
        <summary type="html"><![CDATA[A character network is a graph extracted from a narrative, in which vertices
represent characters and edges correspond to interactions between them. A
number of narrative-related problems can be addressed automatically through the
analysis of character networks, such as summarization, classification, or role
detection. Character networks are particularly relevant when considering works
of fictions (e.g. novels, plays, movies, TV series), as their exploitation
allows developing information retrieval and recommendation systems. However,
works of fiction possess specific properties making these tasks harder. This
survey aims at presenting and organizing the scientific literature related to
the extraction of character networks from works of fiction, as well as their
analysis. We first describe the extraction process in a generic way, and
explain how its constituting steps are implemented in practice, depending on
the medium of the narrative, the goal of the network analysis, and other
factors. We then review the descriptive tools used to characterize character
networks, with a focus on the way they are interpreted in this context. We
illustrate the relevance of character networks by also providing a review of
applications derived from their analysis. Finally, we identify the limitations
of the existing approaches, and the most promising perspectives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Labatut_V/0/1/0/all/0/1"&gt;Vincent Labatut&lt;/a&gt; (LIA), &lt;a href="http://arxiv.org/find/cs/1/au:+Bost_X/0/1/0/all/0/1"&gt;Xavier Bost&lt;/a&gt; (LIA)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emotion Recognition under Consideration of the Emotion Component Process Model. (arXiv:2107.12895v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.12895</id>
        <link href="http://arxiv.org/abs/2107.12895"/>
        <updated>2021-07-28T02:02:30.591Z</updated>
        <summary type="html"><![CDATA[Emotion classification in text is typically performed with neural network
models which learn to associate linguistic units with emotions. While this
often leads to good predictive performance, it does only help to a limited
degree to understand how emotions are communicated in various domains. The
emotion component process model (CPM) by Scherer (2005) is an interesting
approach to explain emotion communication. It states that emotions are a
coordinated process of various subcomponents, in reaction to an event, namely
the subjective feeling, the cognitive appraisal, the expression, a
physiological bodily reaction, and a motivational action tendency. We
hypothesize that these components are associated with linguistic realizations:
an emotion can be expressed by describing a physiological bodily reaction ("he
was trembling"), or the expression ("she smiled"), etc. We annotate existing
literature and Twitter emotion corpora with emotion component classes and find
that emotions on Twitter are predominantly expressed by event descriptions or
subjective reports of the feeling, while in literature, authors prefer to
describe what characters do, and leave the interpretation to the reader. We
further include the CPM in a multitask learning model and find that this
supports the emotion categorization. The annotated corpora are available at
https://www.ims.uni-stuttgart.de/data/emotion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Casel_F/0/1/0/all/0/1"&gt;Felix Casel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heindl_A/0/1/0/all/0/1"&gt;Amelie Heindl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1"&gt;Roman Klinger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Biomedically oriented automatically annotated Twitter COVID-19 Dataset. (arXiv:2107.12565v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.12565</id>
        <link href="http://arxiv.org/abs/2107.12565"/>
        <updated>2021-07-28T02:02:30.579Z</updated>
        <summary type="html"><![CDATA[The use of social media data, like Twitter, for biomedical research has been
gradually increasing over the years. With the COVID-19 pandemic, researchers
have turned to more nontraditional sources of clinical data to characterize the
disease in near real-time, study the societal implications of interventions, as
well as the sequelae that recovered COVID-19 cases present (Long-COVID).
However, manually curated social media datasets are difficult to come by due to
the expensive costs of manual annotation and the efforts needed to identify the
correct texts. When datasets are available, they are usually very small and
their annotations do not generalize well over time or to larger sets of
documents. As part of the 2021 Biomedical Linked Annotation Hackathon, we
release our dataset of over 120 million automatically annotated tweets for
biomedical research purposes. Incorporating best practices, we identify tweets
with potentially high clinical relevance. We evaluated our work by comparing
several SpaCy-based annotation frameworks against a manually annotated
gold-standard dataset. Selecting the best method to use for automatic
annotation, we then annotated 120 million tweets and released them publicly for
future downstream usage within the biomedical domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hernandez_L/0/1/0/all/0/1"&gt;Luis Alberto Robles Hernandez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Callahan_T/0/1/0/all/0/1"&gt;Tiffany J. Callahan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banda_J/0/1/0/all/0/1"&gt;Juan M. Banda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Graph Neural Networks for Sequential Recommendation. (arXiv:2104.07368v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07368</id>
        <link href="http://arxiv.org/abs/2104.07368"/>
        <updated>2021-07-28T02:02:30.556Z</updated>
        <summary type="html"><![CDATA[Modeling user preference from his historical sequences is one of the core
problems of sequential recommendation. Existing methods in this field are
widely distributed from conventional methods to deep learning methods. However,
most of them only model users' interests within their own sequences and ignore
the dynamic collaborative signals among different user sequences, making it
insufficient to explore users' preferences. We take inspiration from dynamic
graph neural networks to cope with this challenge, modeling the user sequence
and dynamic collaborative signals into one framework. We propose a new method
named Dynamic Graph Neural Network for Sequential Recommendation (DGSR), which
connects different user sequences through a dynamic graph structure, exploring
the interactive behavior of users and items with time and order information.
Furthermore, we design a Dynamic Graph Recommendation Network to extract user's
preferences from the dynamic graph. Consequently, the next-item prediction task
in sequential recommendation is converted into a link prediction between the
user node and the item node in a dynamic graph. Extensive experiments on three
public benchmarks show that DGSR outperforms several state-of-the-art methods.
Further studies demonstrate the rationality and effectiveness of modeling user
sequences through a dynamic graph.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mengqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xueli Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Audio-to-Score Alignment Using Deep Automatic Music Transcription. (arXiv:2107.12854v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.12854</id>
        <link href="http://arxiv.org/abs/2107.12854"/>
        <updated>2021-07-28T02:02:30.546Z</updated>
        <summary type="html"><![CDATA[Audio-to-score alignment (A2SA) is a multimodal task consisting in the
alignment of audio signals to music scores. Recent literature confirms the
benefits of Automatic Music Transcription (AMT) for A2SA at the frame-level. In
this work, we aim to elaborate on the exploitation of AMT Deep Learning (DL)
models for achieving alignment at the note-level. We propose a method which
benefits from HMM-based score-to-score alignment and AMT, showing a remarkable
advancement beyond the state-of-the-art. We design a systematic procedure to
take advantage of large datasets which do not offer an aligned score. Finally,
we perform a thorough comparison and extensive tests on multiple datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Simonetta_F/0/1/0/all/0/1"&gt;Federico Simonetta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ntalampiras_S/0/1/0/all/0/1"&gt;Stavros Ntalampiras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avanzini_F/0/1/0/all/0/1"&gt;Federico Avanzini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Variational Models for Collaborative Filtering-based Recommender Systems. (arXiv:2107.12677v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.12677</id>
        <link href="http://arxiv.org/abs/2107.12677"/>
        <updated>2021-07-28T02:02:30.538Z</updated>
        <summary type="html"><![CDATA[Deep learning provides accurate collaborative filtering models to improve
recommender system results. Deep matrix factorization and their related
collaborative neural networks are the state-of-art in the field; nevertheless,
both models lack the necessary stochasticity to create the robust, continuous,
and structured latent spaces that variational autoencoders exhibit. On the
other hand, data augmentation through variational autoencoder does not provide
accurate results in the collaborative filtering field due to the high sparsity
of recommender systems. Our proposed models apply the variational concept to
inject stochasticity in the latent space of the deep architecture, introducing
the variational technique in the neural collaborative filtering field. This
method does not depend on the particular model used to generate the latent
representation. In this way, this approach can be applied as a plugin to any
current and future specific models. The proposed models have been tested using
four representative open datasets, three different quality measures, and
state-of-art baselines. The results show the superiority of the proposed
approach in scenarios where the variational enrichment exceeds the injected
noise effect. Additionally, a framework is provided to enable the
reproducibility of the conducted experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bobadilla_J/0/1/0/all/0/1"&gt;Jes&amp;#xfa;s Bobadilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ortega_F/0/1/0/all/0/1"&gt;Fernando Ortega&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gutierrez_A/0/1/0/all/0/1"&gt;Abraham Guti&amp;#xe9;rrez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Prieto_A/0/1/0/all/0/1"&gt;&amp;#xc1;ngel Gonz&amp;#xe1;lez-Prieto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grover's Algorithm for Question Answering. (arXiv:2106.05299v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05299</id>
        <link href="http://arxiv.org/abs/2106.05299"/>
        <updated>2021-07-28T02:02:30.531Z</updated>
        <summary type="html"><![CDATA[Grover's algorithm, a well-know quantum search algorithm, allows one to find
the correct item in a database, with quadratic speedup. In this paper we adapt
Grover's algorithm to the problem of finding a correct answer to a natural
language question in English, thus contributing to the growing field of Quantum
Natural Language Processing. Using a grammar that can be interpreted as tensor
contractions, each word is represented as a quantum state that serves as input
to the quantum circuit. We here introduce a quantum measurement to contract the
representations of words, resulting in the representation of larger text
fragments. Using this framework, a representation for the question is found
that contains all the possible answers in equal quantum superposition, and
allows for the building of an oracle that can detect a correct answer, being
agnostic to the specific question. Furthermore, we show that our construction
can deal with certain types of ambiguous phrases by keeping the various
different meanings in quantum superposition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Correia_A/0/1/0/all/0/1"&gt;A. D. Correia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Moortgat_M/0/1/0/all/0/1"&gt;M. Moortgat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Stoof_H/0/1/0/all/0/1"&gt;H. T. C. Stoof&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anchor-based Bilingual Word Embeddings for Low-Resource Languages. (arXiv:2010.12627v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12627</id>
        <link href="http://arxiv.org/abs/2010.12627"/>
        <updated>2021-07-28T02:02:30.522Z</updated>
        <summary type="html"><![CDATA[Good quality monolingual word embeddings (MWEs) can be built for languages
which have large amounts of unlabeled text. MWEs can be aligned to bilingual
spaces using only a few thousand word translation pairs. For low resource
languages training MWEs monolingually results in MWEs of poor quality, and thus
poor bilingual word embeddings (BWEs) as well. This paper proposes a new
approach for building BWEs in which the vector space of the high resource
source language is used as a starting point for training an embedding space for
the low resource target language. By using the source vectors as anchors the
vector spaces are automatically aligned during training. We experiment on
English-German, English-Hiligaynon and English-Macedonian. We show that our
approach results not only in improved BWEs and bilingual lexicon induction
performance, but also in improved target language MWE quality as measured using
monolingual word similarity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eder_T/0/1/0/all/0/1"&gt;Tobias Eder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hangya_V/0/1/0/all/0/1"&gt;Viktor Hangya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fraser_A/0/1/0/all/0/1"&gt;Alexander Fraser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extraction and Analysis of Fictional Character Networks: A Survey. (arXiv:1907.02704v4 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1907.02704</id>
        <link href="http://arxiv.org/abs/1907.02704"/>
        <updated>2021-07-28T02:02:30.498Z</updated>
        <summary type="html"><![CDATA[A character network is a graph extracted from a narrative, in which vertices
represent characters and edges correspond to interactions between them. A
number of narrative-related problems can be addressed automatically through the
analysis of character networks, such as summarization, classification, or role
detection. Character networks are particularly relevant when considering works
of fictions (e.g. novels, plays, movies, TV series), as their exploitation
allows developing information retrieval and recommendation systems. However,
works of fiction possess specific properties making these tasks harder. This
survey aims at presenting and organizing the scientific literature related to
the extraction of character networks from works of fiction, as well as their
analysis. We first describe the extraction process in a generic way, and
explain how its constituting steps are implemented in practice, depending on
the medium of the narrative, the goal of the network analysis, and other
factors. We then review the descriptive tools used to characterize character
networks, with a focus on the way they are interpreted in this context. We
illustrate the relevance of character networks by also providing a review of
applications derived from their analysis. Finally, we identify the limitations
of the existing approaches, and the most promising perspectives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Labatut_V/0/1/0/all/0/1"&gt;Vincent Labatut&lt;/a&gt; (LIA), &lt;a href="http://arxiv.org/find/cs/1/au:+Bost_X/0/1/0/all/0/1"&gt;Xavier Bost&lt;/a&gt; (LIA)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Angel's Girl for Blind Painters: an Efficient Painting Navigation System Validated by Multimodal Evaluation Approach. (arXiv:2107.12921v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.12921</id>
        <link href="http://arxiv.org/abs/2107.12921"/>
        <updated>2021-07-28T02:02:30.489Z</updated>
        <summary type="html"><![CDATA[For people who ardently love painting but unfortunately have visual
impairments, holding a paintbrush to create a work is a very difficult task.
People in this special group are eager to pick up the paintbrush, like Leonardo
da Vinci, to create and make full use of their own talents. Therefore, to
maximally bridge this gap, we propose a painting navigation system to assist
blind people in painting and artistic creation. The proposed system is composed
of cognitive system and guidance system. The system adopts drawing board
positioning based on QR code, brush navigation based on target detection and
bush real-time positioning. Meanwhile, this paper uses human-computer
interaction on the basis of voice and a simple but efficient position
information coding rule. In addition, we design a criterion to efficiently
judge whether the brush reaches the target or not. According to the
experimental results, the thermal curves extracted from the faces of testers
show that it is relatively well accepted by blindfolded and even blind testers.
With the prompt frequency of 1s, the painting navigation system performs best
with the completion degree of 89% with SD of 8.37% and overflow degree of 347%
with SD of 162.14%. Meanwhile, the excellent and good types of brush tip
trajectory account for 74%, and the relative movement distance is 4.21 with SD
of 2.51. This work demonstrates that it is practicable for the blind people to
feel the world through the brush in their hands. In the future, we plan to
deploy Angle's Eyes on the phone to make it more portable. The demo video of
the proposed painting navigation system is available at:
https://doi.org/10.6084/m9.figshare.9760004.v1.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1"&gt;Menghan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuzhen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qingli Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1"&gt;Guangtao Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Simon X. Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao-Ping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaokang Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning Meets Natural Language Processing: A Survey. (arXiv:2107.12603v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.12603</id>
        <link href="http://arxiv.org/abs/2107.12603"/>
        <updated>2021-07-28T02:02:30.479Z</updated>
        <summary type="html"><![CDATA[Federated Learning aims to learn machine learning models from multiple
decentralized edge devices (e.g. mobiles) or servers without sacrificing local
data privacy. Recent Natural Language Processing techniques rely on deep
learning and large pre-trained language models. However, both big deep neural
and language models are trained with huge amounts of data which often lies on
the server side. Since text data is widely originated from end users, in this
work, we look into recent NLP models and techniques which use federated
learning as the learning framework. Our survey discusses major challenges in
federated natural language processing, including the algorithm challenges,
system challenges as well as the privacy issues. We also provide a critical
review of the existing Federated NLP evaluation methods and tools. Finally, we
highlight the current research gaps and future directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Ming Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_S/0/1/0/all/0/1"&gt;Stella Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mengqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Longxiang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1"&gt;Yuan Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;He Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Generation of H.264 Parameter Sets to Recover Video File Fragments. (arXiv:2104.14522v2 [cs.MM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14522</id>
        <link href="http://arxiv.org/abs/2104.14522"/>
        <updated>2021-07-28T02:02:30.469Z</updated>
        <summary type="html"><![CDATA[We address the problem of decoding video file fragments when the necessary
encoding parameters are missing. With this objective, we propose a method that
automatically generates H.264 video headers containing these parameters and
extracts coded pictures in the partially available compressed video data. To
accomplish this, we examined a very large corpus of videos to learn patterns of
encoding settings commonly used by encoders and created a parameter dictionary.
Further, to facilitate a more efficient search our method identifies
characteristics of a coded bitstream to discriminate the entropy coding mode.
It also utilizes the application logs created by the decoder to identify
correct parameter values. Evaluation of the effectiveness of the proposed
method on more than 55K videos with diverse provenance shows that it can
generate valid headers on average in 11.3 decoding trials per video. This
result represents an improvement by more than a factor of 10 over the
conventional approach of video header stitching to recover video file
fragments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Altinisik_E/0/1/0/all/0/1"&gt;Enes Altinisik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sencar_H/0/1/0/all/0/1"&gt;H&amp;#xfc;srev Taha Sencar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual-Path Convolutional Image-Text Embeddings with Instance Loss. (arXiv:1711.05535v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1711.05535</id>
        <link href="http://arxiv.org/abs/1711.05535"/>
        <updated>2021-07-28T02:02:30.455Z</updated>
        <summary type="html"><![CDATA[Matching images and sentences demands a fine understanding of both
modalities. In this paper, we propose a new system to discriminatively embed
the image and text to a shared visual-textual space. In this field, most
existing works apply the ranking loss to pull the positive image / text pairs
close and push the negative pairs apart from each other. However, directly
deploying the ranking loss is hard for network learning, since it starts from
the two heterogeneous features to build inter-modal relationship. To address
this problem, we propose the instance loss which explicitly considers the
intra-modal data distribution. It is based on an unsupervised assumption that
each image / text group can be viewed as a class. So the network can learn the
fine granularity from every image/text group. The experiment shows that the
instance loss offers better weight initialization for the ranking loss, so that
more discriminative embeddings can be learned. Besides, existing works usually
apply the off-the-shelf features, i.e., word2vec and fixed visual feature. So
in a minor contribution, this paper constructs an end-to-end dual-path
convolutional network to learn the image and text representations. End-to-end
learning allows the system to directly learn from the data and fully utilize
the supervision. On two generic retrieval datasets (Flickr30k and MSCOCO),
experiments demonstrate that our method yields competitive accuracy compared to
state-of-the-art methods. Moreover, in language based person retrieval, we
improve the state of the art by a large margin. The code has been made publicly
available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhedong Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Liang Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garrett_M/0/1/0/all/0/1"&gt;Michael Garrett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mingliang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yi-Dong Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual Slot Selector via Local Reliability Verification for Dialogue State Tracking. (arXiv:2107.12578v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.12578</id>
        <link href="http://arxiv.org/abs/2107.12578"/>
        <updated>2021-07-28T02:02:30.433Z</updated>
        <summary type="html"><![CDATA[The goal of dialogue state tracking (DST) is to predict the current dialogue
state given all previous dialogue contexts. Existing approaches generally
predict the dialogue state at every turn from scratch. However, the
overwhelming majority of the slots in each turn should simply inherit the slot
values from the previous turn. Therefore, the mechanism of treating slots
equally in each turn not only is inefficient but also may lead to additional
errors because of the redundant slot value generation. To address this problem,
we devise the two-stage DSS-DST which consists of the Dual Slot Selector based
on the current turn dialogue, and the Slot Value Generator based on the
dialogue history. The Dual Slot Selector determines each slot whether to update
slot value or to inherit the slot value from the previous turn from two
aspects: (1) if there is a strong relationship between it and the current turn
dialogue utterances; (2) if a slot value with high reliability can be obtained
for it through the current turn dialogue. The slots selected to be updated are
permitted to enter the Slot Value Generator to update values by a hybrid
method, while the other slots directly inherit the values from the previous
turn. Empirical results show that our method achieves 56.93%, 60.73%, and
58.04% joint accuracy on MultiWOZ 2.0, MultiWOZ 2.1, and MultiWOZ 2.2 datasets
respectively and achieves a new state-of-the-art performance with significant
improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jinyu Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shuang_K/0/1/0/all/0/1"&gt;Kai Shuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jijie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zihan Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MU-MIMO Grouping For Real-time Applications. (arXiv:2106.15262v2 [cs.NI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15262</id>
        <link href="http://arxiv.org/abs/2106.15262"/>
        <updated>2021-07-28T02:02:30.421Z</updated>
        <summary type="html"><![CDATA[Over the last decade, the bandwidth expansion and MU-MIMO spectral efficiency
have promised to increase data throughput by allowing concurrent communication
between one Access Point and multiple users. However, we are still a long way
from enjoying such MU-MIMO MAC protocol improvements for bandwidth hungry
applications such as video streaming in practical WiFi network settings due to
heterogeneous channel conditions and devices, unreliable transmissions, and
lack of useful feedback exchange among the lower and upper layers'
requirements. This paper introduces MuViS, a novel dual-phase optimization
framework that proposes a Quality of Experience (QoE) aware MU-MIMO
optimization for multi-user video streaming over IEEE 802.11ac. MuViS first
employs reinforcement learning to optimize the MU-MIMO user group and mode
selection for users based on their PHY/MAC layer characteristics. The video
bitrate is then optimized based on the user's mode (Multi-User (MU) or
Single-User (SU)). We present our design and its evaluation on smartphones and
laptops using 802.11ac WiFi. Our experimental results in various indoor
environments and configurations show a scalable framework that can support a
large number of users with streaming at high video rates and satisfying QoE
requirements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pasandi_H/0/1/0/all/0/1"&gt;Hannaneh Barahouei Pasandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nadeem_T/0/1/0/all/0/1"&gt;Tamer Nadeem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amirpour_H/0/1/0/all/0/1"&gt;Hadi Amirpour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coarse to Fine: Domain Adaptive Crowd Counting via Adversarial Scoring Network. (arXiv:2107.12858v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12858</id>
        <link href="http://arxiv.org/abs/2107.12858"/>
        <updated>2021-07-28T02:02:30.408Z</updated>
        <summary type="html"><![CDATA[Recent deep networks have convincingly demonstrated high capability in crowd
counting, which is a critical task attracting widespread attention due to its
various industrial applications. Despite such progress, trained data-dependent
models usually can not generalize well to unseen scenarios because of the
inherent domain shift. To facilitate this issue, this paper proposes a novel
adversarial scoring network (ASNet) to gradually bridge the gap across domains
from coarse to fine granularity. In specific, at the coarse-grained stage, we
design a dual-discriminator strategy to adapt source domain to be close to the
targets from the perspectives of both global and local feature space via
adversarial learning. The distributions between two domains can thus be aligned
roughly. At the fine-grained stage, we explore the transferability of source
characteristics by scoring how similar the source samples are to target ones
from multiple levels based on generative probability derived from coarse stage.
Guided by these hierarchical scores, the transferable source features are
properly selected to enhance the knowledge transfer during the adaptation
process. With the coarse-to-fine design, the generalization bottleneck induced
from the domain discrepancy can be effectively alleviated. Three sets of
migration experiments show that the proposed methods achieve state-of-the-art
counting performance compared with major unsupervised methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1"&gt;Zhikang Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1"&gt;Xiaoye Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1"&gt;Pan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shuangjie Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xiaoqing Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wenhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jin Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring daily-life fear perception change: a computational study in the context of COVID-19. (arXiv:2107.12606v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.12606</id>
        <link href="http://arxiv.org/abs/2107.12606"/>
        <updated>2021-07-28T02:02:30.394Z</updated>
        <summary type="html"><![CDATA[COVID-19, as a global health crisis, has triggered the fear emotion with
unprecedented intensity. Besides the fear of getting infected, the outbreak of
COVID-19 also created significant disruptions in people's daily life and thus
evoked intensive psychological responses indirect to COVID-19 infections. Here,
we construct an expressed fear database using 16 million social media posts
generated by 536 thousand users between January 1st, 2019 and August 31st, 2020
in China. We employ deep learning techniques to detect the fear emotion within
each post and apply topic models to extract the central fear topics. Based on
this database, we find that sleep disorders ("nightmare" and "insomnia") take
up the largest share of fear-labeled posts in the pre-pandemic period (January
2019-December 2019), and significantly increase during the COVID-19. We
identify health and work-related concerns are the two major sources of fear
induced by the COVID-19. We also detect gender differences, with females
generating more posts containing the daily-life fear sources during the
COVID-19 period. This research adopts a data-driven approach to trace back
public emotion, which can be used to complement traditional surveys to achieve
real-time emotion monitoring to discern societal concerns and support policy
decision-making.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1"&gt;Yuchen Chai&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Palacios_J/0/1/0/all/0/1"&gt;Juan Palacios&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianghao Wang&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1"&gt;Yichun Fan&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Siqi Zheng&lt;/a&gt; (1) ((1) Massachusetts Institute of Technology, (2) Chinese Academy of Science)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Grounding with 3D Objects. (arXiv:2107.12514v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.12514</id>
        <link href="http://arxiv.org/abs/2107.12514"/>
        <updated>2021-07-28T02:02:30.381Z</updated>
        <summary type="html"><![CDATA[Seemingly simple natural language requests to a robot are generally
underspecified, for example "Can you bring me the wireless mouse?" When viewing
mice on the shelf, the number of buttons or presence of a wire may not be
visible from certain angles or positions. Flat images of candidate mice may not
provide the discriminative information needed for "wireless". The world, and
objects in it, are not flat images but complex 3D shapes. If a human requests
an object based on any of its basic properties, such as color, shape, or
texture, robots should perform the necessary exploration to accomplish the
task. In particular, while substantial effort and progress has been made on
understanding explicitly visual attributes like color and category,
comparatively little progress has been made on understanding language about
shapes and contours. In this work, we introduce a novel reasoning task that
targets both visual and non-visual language about 3D objects. Our new
benchmark, ShapeNet Annotated with Referring Expressions (SNARE), requires a
model to choose which of two objects is being referenced by a natural language
description. We introduce several CLIP-based models for distinguishing objects
and demonstrate that while recent advances in jointly modeling vision and
language are useful for robotic language understanding, it is still the case
that these models are weaker at understanding the 3D nature of objects --
properties which play a key role in manipulation. In particular, we find that
adding view estimation to language grounding models improves accuracy on both
SNARE and when identifying objects referred to in language on a robot platform.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1"&gt;Jesse Thomason&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shridhar_M/0/1/0/all/0/1"&gt;Mohit Shridhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1"&gt;Yonatan Bisk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1"&gt;Chris Paxton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1"&gt;Luke Zettlemoyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension. (arXiv:2107.12708v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.12708</id>
        <link href="http://arxiv.org/abs/2107.12708"/>
        <updated>2021-07-28T02:02:30.344Z</updated>
        <summary type="html"><![CDATA[Alongside huge volumes of research on deep learning models in NLP in the
recent years, there has been also much work on benchmark datasets needed to
track modeling progress. Question answering and reading comprehension have been
particularly prolific in this regard, with over 80 new datasets appearing in
the past two years. This study is the largest survey of the field to date. We
provide an overview of the various formats and domains of the current
resources, highlighting the current lacunae for future work. We further discuss
the current classifications of ``reasoning types" in question answering and
propose a new taxonomy. We also discuss the implications of over-focusing on
English, and survey the current monolingual resources for other languages and
multilingual resources. The study is aimed at both practitioners looking for
pointers to the wealth of existing data, and at researchers working on new
resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rogers_A/0/1/0/all/0/1"&gt;Anna Rogers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1"&gt;Matt Gardner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1"&gt;Isabelle Augenstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Energy-based Unknown Intent Detection with Data Manipulation. (arXiv:2107.12542v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.12542</id>
        <link href="http://arxiv.org/abs/2107.12542"/>
        <updated>2021-07-28T02:02:30.332Z</updated>
        <summary type="html"><![CDATA[Unknown intent detection aims to identify the out-of-distribution (OOD)
utterance whose intent has never appeared in the training set. In this paper,
we propose using energy scores for this task as the energy score is
theoretically aligned with the density of the input and can be derived from any
classifier. However, high-quality OOD utterances are required during the
training stage in order to shape the energy gap between OOD and in-distribution
(IND), and these utterances are difficult to collect in practice. To tackle
this problem, we propose a data manipulation framework to Generate high-quality
OOD utterances with importance weighTs (GOT). Experimental results show that
the energy-based detector fine-tuned by GOT can achieve state-of-the-art
results on two benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_Y/0/1/0/all/0/1"&gt;Yawen Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jiasheng Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1"&gt;Xinyu Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shujian Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiajun Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is all this new MeSH about? Exploring the semantic provenance of new descriptors in the MeSH thesaurus. (arXiv:2101.08293v3 [cs.DL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08293</id>
        <link href="http://arxiv.org/abs/2101.08293"/>
        <updated>2021-07-28T02:02:30.319Z</updated>
        <summary type="html"><![CDATA[The Medical Subject Headings (MeSH) thesaurus is a controlled vocabulary
widely used in biomedical knowledge systems, particularly for semantic indexing
of scientific literature. As the MeSH hierarchy evolves through annual version
updates, some new descriptors are introduced that were not previously
available. This paper explores the conceptual provenance of these new
descriptors. In particular, we investigate whether such new descriptors have
been previously covered by older descriptors and what is their current relation
to them. To this end, we propose a framework to categorize new descriptors
based on their current relation to older descriptors. Based on the proposed
classification scheme, we quantify, analyse and present the different types of
new descriptors introduced in MeSH during the last fifteen years. The results
show that only about 25% of new MeSH descriptors correspond to new emerging
concepts, whereas the rest were previously covered by one or more existing
descriptors, either implicitly or explicitly. Most of them were covered by a
single existing descriptor and they usually end up as descendants of it in the
current hierarchy, gradually leading towards a more fine-grained MeSH
vocabulary. These insights about the dynamics of the thesaurus are useful for
the retrospective study of scientific articles annotated with MeSH, but could
also be used to inform the policy of updating the thesaurus in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nentidis_A/0/1/0/all/0/1"&gt;Anastasios Nentidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krithara_A/0/1/0/all/0/1"&gt;Anastasia Krithara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1"&gt;Grigorios Tsoumakas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paliouras_G/0/1/0/all/0/1"&gt;Georgios Paliouras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-lingual Transferring of Pre-trained Contextualized Language Models. (arXiv:2107.12627v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.12627</id>
        <link href="http://arxiv.org/abs/2107.12627"/>
        <updated>2021-07-28T02:02:30.304Z</updated>
        <summary type="html"><![CDATA[Though the pre-trained contextualized language model (PrLM) has made a
significant impact on NLP, training PrLMs in languages other than English can
be impractical for two reasons: other languages often lack corpora sufficient
for training powerful PrLMs, and because of the commonalities among human
languages, computationally expensive PrLM training for different languages is
somewhat redundant. In this work, building upon the recent works connecting
cross-lingual model transferring and neural machine translation, we thus
propose a novel cross-lingual model transferring framework for PrLMs: TreLM. To
handle the symbol order and sequence length differences between languages, we
propose an intermediate ``TRILayer" structure that learns from these
differences and creates a better transfer in our primary translation direction,
as well as a new cross-lingual language modeling objective for transfer
training. Additionally, we showcase an embedding aligning that adversarially
adapts a PrLM's non-contextualized embedding space and the TRILayer structure
to learn a text transformation network across languages, which addresses the
vocabulary difference between languages. Experiments on both language
understanding and structure parsing tasks show the proposed framework
significantly outperforms language models trained from scratch with limited
data in both performance and efficiency. Moreover, despite an insignificant
performance loss compared to pre-training from scratch in resource-rich
scenarios, our cross-lingual model transferring framework is significantly more
economical.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zuchao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parnow_K/0/1/0/all/0/1"&gt;Kevin Parnow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hai Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhuosheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Utiyama_M/0/1/0/all/0/1"&gt;Masao Utiyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sumita_E/0/1/0/all/0/1"&gt;Eiichiro Sumita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Greedy Gradient Ensemble for Robust Visual Question Answering. (arXiv:2107.12651v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.12651</id>
        <link href="http://arxiv.org/abs/2107.12651"/>
        <updated>2021-07-28T02:02:30.284Z</updated>
        <summary type="html"><![CDATA[Language bias is a critical issue in Visual Question Answering (VQA), where
models often exploit dataset biases for the final decision without considering
the image information. As a result, they suffer from performance drop on
out-of-distribution data and inadequate visual explanation. Based on
experimental analysis for existing robust VQA methods, we stress the language
bias in VQA that comes from two aspects, i.e., distribution bias and shortcut
bias. We further propose a new de-bias framework, Greedy Gradient Ensemble
(GGE), which combines multiple biased models for unbiased base model learning.
With the greedy strategy, GGE forces the biased models to over-fit the biased
data distribution in priority, thus makes the base model pay more attention to
examples that are hard to solve by biased models. The experiments demonstrate
that our method makes better use of visual information and achieves
state-of-the-art performance on diagnosing dataset VQA-CP without using extra
annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xinzhe Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuhui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1"&gt;Chi Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qingming Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Domain Adaptation for Hate Speech Detection Using a Data Augmentation Approach. (arXiv:2107.12866v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.12866</id>
        <link href="http://arxiv.org/abs/2107.12866"/>
        <updated>2021-07-28T02:02:30.266Z</updated>
        <summary type="html"><![CDATA[Online harassment in the form of hate speech has been on the rise in recent
years. Addressing the issue requires a combination of content moderation by
people, aided by automatic detection methods. As content moderation is itself
harmful to the people doing it, we desire to reduce the burden by improving the
automatic detection of hate speech. Hate speech presents a challenge as it is
directed at different target groups using a completely different vocabulary.
Further the authors of the hate speech are incentivized to disguise their
behavior to avoid being removed from a platform. This makes it difficult to
develop a comprehensive data set for training and evaluating hate speech
detection models because the examples that represent one hate speech domain do
not typically represent others, even within the same language or culture. We
propose an unsupervised domain adaptation approach to augment labeled data for
hate speech detection. We evaluate the approach with three different models
(character CNNs, BiLSTMs and BERT) on three different collections. We show our
approach improves Area under the Precision/Recall curve by as much as 42% and
recall by as much as 278%, with no loss (and in some cases a significant gain)
in precision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sarwar_S/0/1/0/all/0/1"&gt;Sheikh Muhammad Sarwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murdock_V/0/1/0/all/0/1"&gt;Vanessa Murdock&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Word Recognition in Speech Transcriptions by Decision-level Fusion of Stemming and Two-way Phoneme Pruning. (arXiv:2107.12428v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.12428</id>
        <link href="http://arxiv.org/abs/2107.12428"/>
        <updated>2021-07-28T02:02:30.231Z</updated>
        <summary type="html"><![CDATA[We introduce an unsupervised approach for correcting highly imperfect speech
transcriptions based on a decision-level fusion of stemming and two-way phoneme
pruning. Transcripts are acquired from videos by extracting audio using Ffmpeg
framework and further converting audio to text transcript using Google API. In
the benchmark LRW dataset, there are 500 word categories, and 50 videos per
class in mp4 format. All videos consist of 29 frames (each 1.16 s long) and the
word appears in the middle of the video. In our approach we tried to improve
the baseline accuracy from 9.34% by using stemming, phoneme extraction,
filtering and pruning. After applying the stemming algorithm to the text
transcript and evaluating the results, we achieved 23.34% accuracy in word
recognition. To convert words to phonemes we used the Carnegie Mellon
University (CMU) pronouncing dictionary that provides a phonetic mapping of
English words to their pronunciations. A two-way phoneme pruning is proposed
that comprises of the two non-sequential steps: 1) filtering and pruning the
phonemes containing vowels and plosives 2) filtering and pruning the phonemes
containing vowels and fricatives. After obtaining results of stemming and
two-way phoneme pruning, we applied decision-level fusion and that led to an
improvement of word recognition rate upto 32.96%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mehra_S/0/1/0/all/0/1"&gt;Sunakshi Mehra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Susan_S/0/1/0/all/0/1"&gt;Seba Susan&lt;/a&gt;</name>
        </author>
    </entry>
</feed>