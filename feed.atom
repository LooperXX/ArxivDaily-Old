<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://looperxx.github.io/ArxivDaily/index.html</id>
    <title>ArxivDaily</title>
    <updated>2021-07-14T00:44:08.930Z</updated>
    <generator>osmosfeed 1.11.0</generator>
    <link rel="alternate" href="https://looperxx.github.io/ArxivDaily/index.html"/>
    <link rel="self" href="https://looperxx.github.io/ArxivDaily/feed.atom"/>
    <entry>
        <title type="html"><![CDATA[A unified framework based on graph consensus term for multi-view learning. (arXiv:2105.11781v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11781</id>
        <link href="http://arxiv.org/abs/2105.11781"/>
        <updated>2021-07-13T01:59:37.467Z</updated>
        <summary type="html"><![CDATA[In recent years, multi-view learning technologies for various applications
have attracted a surge of interest. Due to more compatible and complementary
information from multiple views, existing multi-view methods could achieve more
promising performance than conventional single-view methods in most situations.
However, there are still no sufficient researches on the unified framework in
existing multi-view works. Meanwhile, how to efficiently integrate multi-view
information is still full of challenges. In this paper, we propose a novel
multi-view learning framework, which aims to leverage most existing graph
embedding works into a unified formula via introducing the graph consensus
term. In particular, our method explores the graph structure in each view
independently to preserve the diversity property of graph embedding methods.
Meanwhile, we choose heterogeneous graphs to construct the graph consensus term
to explore the correlations among multiple views jointly. To this end, the
diversity and complementary information among different views could be
simultaneously considered. Furthermore, the proposed framework is utilized to
implement the multi-view extension of Locality Linear Embedding, named
Multi-view Locality Linear Embedding (MvLLE), which could be efficiently solved
by applying the alternating optimization strategy. Empirical validations
conducted on six benchmark datasets can show the effectiveness of our proposed
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1"&gt;Xiangzhu Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1"&gt;Lin Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1"&gt;Chonghui Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coupled VAE: Improved Accuracy and Robustness of a Variational Autoencoder. (arXiv:1906.00536v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.00536</id>
        <link href="http://arxiv.org/abs/1906.00536"/>
        <updated>2021-07-13T01:59:37.461Z</updated>
        <summary type="html"><![CDATA[We present a coupled Variational Auto-Encoder (VAE) method that improves the
accuracy and robustness of the probabilistic inferences on represented data.
The new method models the dependency between input feature vectors (images) and
weighs the outliers with a higher penalty by generalizing the original loss
function to the coupled entropy function, using the principles of nonlinear
statistical coupling. We evaluate the performance of the coupled VAE model
using the MNIST dataset. Compared with the traditional VAE algorithm, the
output images generated by the coupled VAE method are clearer and less blurry.
The visualization of the input images embedded in 2D latent variable space
provides a deeper insight into the structure of new model with coupled loss
function: the latent variable has a smaller deviation and a more compact latent
space generates the output values. We analyze the histogram of the likelihoods
of the input images using the generalized mean, which measures the model's
accuracy as a function of the relative risk. The neutral accuracy, which is the
geometric mean and is consistent with a measure of the Shannon cross-entropy,
is improved. The robust accuracy, measured by the -2/3 generalized mean, is
also improved.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1"&gt;Shichen Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jingjing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nelson_K/0/1/0/all/0/1"&gt;Kenric P. Nelson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kon_M/0/1/0/all/0/1"&gt;Mark A. Kon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StructFormer: Joint Unsupervised Induction of Dependency and Constituency Structure from Masked Language Modeling. (arXiv:2012.00857v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00857</id>
        <link href="http://arxiv.org/abs/2012.00857"/>
        <updated>2021-07-13T01:59:37.448Z</updated>
        <summary type="html"><![CDATA[There are two major classes of natural language grammar -- the dependency
grammar that models one-to-one correspondences between words and the
constituency grammar that models the assembly of one or several corresponded
words. While previous unsupervised parsing methods mostly focus on only
inducing one class of grammars, we introduce a novel model, StructFormer, that
can simultaneously induce dependency and constituency structure. To achieve
this, we propose a new parsing framework that can jointly generate a
constituency tree and dependency graph. Then we integrate the induced
dependency relations into the transformer, in a differentiable manner, through
a novel dependency-constrained self-attention mechanism. Experimental results
show that our model can achieve strong results on unsupervised constituency
parsing, unsupervised dependency parsing, and masked language modeling at the
same time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yikang Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1"&gt;Yi Tay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Che Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1"&gt;Dara Bahri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1"&gt;Donald Metzler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1"&gt;Aaron Courville&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tractable structured natural gradient descent using local parameterizations. (arXiv:2102.07405v6 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07405</id>
        <link href="http://arxiv.org/abs/2102.07405"/>
        <updated>2021-07-13T01:59:37.442Z</updated>
        <summary type="html"><![CDATA[Natural-gradient descent (NGD) on structured parameter spaces (e.g., low-rank
covariances) is computationally challenging due to difficult Fisher-matrix
computations. We address this issue by using \emph{local-parameter coordinates}
to obtain a flexible and efficient NGD method that works well for a
wide-variety of structured parameterizations. We show four applications where
our method (1) generalizes the exponential natural evolutionary strategy, (2)
recovers existing Newton-like algorithms, (3) yields new structured
second-order algorithms, and (4) gives new algorithms to learn covariances of
Gaussian and Wishart-based distributions. We show results on a range of
problems from deep learning, variational inference, and evolution strategies.
Our work opens a new direction for scalable structured geometric methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nielsen_F/0/1/0/all/0/1"&gt;Frank Nielsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mohammad Emtiyaz Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schmidt_M/0/1/0/all/0/1"&gt;Mark Schmidt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Learning with Graph Neural Networks: Methods and Applications. (arXiv:2103.00137v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00137</id>
        <link href="http://arxiv.org/abs/2103.00137"/>
        <updated>2021-07-13T01:59:37.436Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs), a generalization of deep neural networks on
graph data have been widely used in various domains, ranging from drug
discovery to recommender systems. However, GNNs on such applications are
limited when there are few available samples. Meta-learning has been an
important framework to address the lack of samples in machine learning, and in
recent years, researchers have started to apply meta-learning to GNNs. In this
work, we provide a comprehensive survey of different meta-learning approaches
involving GNNs on various graph problems showing the power of using these two
approaches together. We categorize the literature based on proposed
architectures, shared representations, and applications. Finally, we discuss
several exciting future research directions and open problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mandal_D/0/1/0/all/0/1"&gt;Debmalya Mandal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Medya_S/0/1/0/all/0/1"&gt;Sourav Medya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uzzi_B/0/1/0/all/0/1"&gt;Brian Uzzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1"&gt;Charu Aggarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sliding Spectrum Decomposition for Diversified Recommendation. (arXiv:2107.05204v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.05204</id>
        <link href="http://arxiv.org/abs/2107.05204"/>
        <updated>2021-07-13T01:59:37.430Z</updated>
        <summary type="html"><![CDATA[Content feed, a type of product that recommends a sequence of items for users
to browse and engage with, has gained tremendous popularity among social media
platforms. In this paper, we propose to study the diversity problem in such a
scenario from an item sequence perspective using time series analysis
techniques. We derive a method called sliding spectrum decomposition (SSD) that
captures users' perception of diversity in browsing a long item sequence. We
also share our experiences in designing and implementing a suitable item
embedding method for accurate similarity measurement under long tail effect.
Combined together, they are now fully implemented and deployed in Xiaohongshu
App's production recommender system that serves the main Explore Feed product
for tens of millions of users every day. We demonstrate the effectiveness and
efficiency of the method through theoretical analysis, offline experiments and
online A/B tests.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yanhua Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weikun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Ruiwen Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prb-GAN: A Probabilistic Framework for GAN Modelling. (arXiv:2107.05241v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05241</id>
        <link href="http://arxiv.org/abs/2107.05241"/>
        <updated>2021-07-13T01:59:37.424Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) are very popular to generate realistic
images, but they often suffer from the training instability issues and the
phenomenon of mode loss. In order to attain greater diversity in GAN
synthesized data, it is critical to solving the problem of mode loss. Our work
explores probabilistic approaches to GAN modelling that could allow us to
tackle these issues. We present Prb-GANs, a new variation that uses dropout to
create a distribution over the network parameters with the posterior learnt
using variational inference. We describe theoretically and validate
experimentally using simple and complex datasets the benefits of such an
approach. We look into further improvements using the concept of uncertainty
measures. Through a set of further modifications to the loss functions for each
network of the GAN, we are able to get results that show the improvement of GAN
performance. Our methods are extremely simple and require very little
modification to existing GAN architecture.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+George_B/0/1/0/all/0/1"&gt;Blessen George&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurmi_V/0/1/0/all/0/1"&gt;Vinod K. Kurmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1"&gt;Vinay P. Namboodiri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Efficiency of Various Deep Transfer Learning Models in Glitch Waveform Detection in Gravitational-Wave Data. (arXiv:2107.01863v2 [gr-qc] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01863</id>
        <link href="http://arxiv.org/abs/2107.01863"/>
        <updated>2021-07-13T01:59:37.417Z</updated>
        <summary type="html"><![CDATA[LIGO is considered the most sensitive and complicated gravitational
experiment ever built. Its main objective is to detect the gravitational wave
from the strongest events in the universe by observing if the length of its
4-kilometer arms change by a distance 10,000 times smaller than the diameter of
a proton. Due to its sensitivity, LIGO is prone to the disturbance of external
noises which affects the data being collected to detect the gravitational wave.
These noises are commonly called by the LIGO community as glitches. The
objective of this study is to evaluate the effeciency of various deep trasnfer
learning models namely VGG19, ResNet50V2, VGG16 and ResNet101 to detect glitch
waveform in gravitational wave data. The accuracy achieved by the said models
are 98.98%, 98.35%, 97.56% and 94.73% respectively. Even though the models
achieved fairly high accuracy, it is observed that all of the model suffered
from the lack of data for certain classes which is the main concern found in
the experiment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/gr-qc/1/au:+Mesuga_R/0/1/0/all/0/1"&gt;Reymond Mesuga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/gr-qc/1/au:+Bayanay_B/0/1/0/all/0/1"&gt;Brian James Bayanay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample Complexity of Offline Reinforcement Learning with Deep ReLU Networks. (arXiv:2103.06671v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06671</id>
        <link href="http://arxiv.org/abs/2103.06671"/>
        <updated>2021-07-13T01:59:37.410Z</updated>
        <summary type="html"><![CDATA[We study the statistical theory of offline reinforcement learning (RL) with
deep ReLU network function approximation. We analyze a variant of fitted-Q
iteration (FQI) algorithm under a new dynamic condition that we call Besov
dynamic closure, which encompasses the conditions from prior analyses for deep
neural network function approximation. Under Besov dynamic closure, we prove
that the FQI-type algorithm enjoys the sample complexity of
$\tilde{\mathcal{O}}\left( \kappa^{1 + d/\alpha} \cdot \epsilon^{-2 -
2d/\alpha} \right)$ where $\kappa$ is a distribution shift measure, $d$ is the
dimensionality of the state-action space, $\alpha$ is the (possibly fractional)
smoothness parameter of the underlying MDP, and $\epsilon$ is a user-specified
precision. This is an improvement over the sample complexity of
$\tilde{\mathcal{O}}\left( K \cdot \kappa^{2 + d/\alpha} \cdot \epsilon^{-2 -
d/\alpha} \right)$ in the prior result [Yang et al., 2019] where $K$ is an
algorithmic iteration number which is arbitrarily large in practice.
Importantly, our sample complexity is obtained under the new general dynamic
condition and a data-dependent structure where the latter is either ignored in
prior algorithms or improperly handled by prior analyses. This is the first
comprehensive analysis for offline RL with deep ReLU network function
approximation under a general setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Nguyen_Tang_T/0/1/0/all/0/1"&gt;Thanh Nguyen-Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Sunil Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tran_The_H/0/1/0/all/0/1"&gt;Hung Tran-The&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Venkatesh_S/0/1/0/all/0/1"&gt;Svetha Venkatesh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Attentive Survey of Attention Models. (arXiv:1904.02874v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.02874</id>
        <link href="http://arxiv.org/abs/1904.02874"/>
        <updated>2021-07-13T01:59:37.392Z</updated>
        <summary type="html"><![CDATA[Attention Model has now become an important concept in neural networks that
has been researched within diverse application domains. This survey provides a
structured and comprehensive overview of the developments in modeling
attention. In particular, we propose a taxonomy which groups existing
techniques into coherent categories. We review salient neural architectures in
which attention has been incorporated, and discuss applications in which
modeling attention has shown a significant impact. We also describe how
attention has been used to improve the interpretability of neural networks.
Finally, we discuss some future research directions in attention. We hope this
survey will provide a succinct introduction to attention models and guide
practitioners while developing approaches for their applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhari_S/0/1/0/all/0/1"&gt;Sneha Chaudhari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mithal_V/0/1/0/all/0/1"&gt;Varun Mithal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polatkan_G/0/1/0/all/0/1"&gt;Gungor Polatkan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramanath_R/0/1/0/all/0/1"&gt;Rohan Ramanath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring intermediate representation for monocular vehicle pose estimation. (arXiv:2011.08464v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08464</id>
        <link href="http://arxiv.org/abs/2011.08464"/>
        <updated>2021-07-13T01:59:37.383Z</updated>
        <summary type="html"><![CDATA[We present a new learning-based framework to recover vehicle pose in SO(3)
from a single RGB image. In contrast to previous works that map from local
appearance to observation angles, we explore a progressive approach by
extracting meaningful Intermediate Geometrical Representations (IGRs) to
estimate egocentric vehicle orientation. This approach features a deep model
that transforms perceived intensities to IGRs, which are mapped to a 3D
representation encoding object orientation in the camera coordinate system.
Core problems are what IGRs to use and how to learn them more effectively. We
answer the former question by designing IGRs based on an interpolated cuboid
that derives from primitive 3D annotation readily. The latter question
motivates us to incorporate geometry knowledge with a new loss function based
on a projective invariant. This loss function allows unlabeled data to be used
in the training stage to improve representation learning. Without additional
labels, our system outperforms previous monocular RGB-based methods for joint
vehicle detection and pose estimation on the KITTI benchmark, achieving
performance even comparable to stereo methods. Code and pre-trained models are
available at this https URL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shichao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zengqiang Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1"&gt;Kwang-Ting Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prescient teleoperation of humanoid robots. (arXiv:2107.01281v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01281</id>
        <link href="http://arxiv.org/abs/2107.01281"/>
        <updated>2021-07-13T01:59:37.375Z</updated>
        <summary type="html"><![CDATA[Humanoid robots could be versatile and intuitive human avatars that operate
remotely in inaccessible places: the robot could reproduce in the remote
location the movements of an operator equipped with a wearable motion capture
device while sending visual feedback to the operator. While substantial
progress has been made on transferring ("retargeting") human motions to
humanoid robots, a major problem preventing the deployment of such systems in
real applications is the presence of communication delays between the human
input and the feedback from the robot: even a few hundred milliseconds of delay
can irreversibly disturb the operator, let alone a few seconds. To overcome
these delays, we introduce a system in which a humanoid robot executes commands
before it actually receives them, so that the visual feedback appears to be
synchronized to the operator, whereas the robot executed the commands in the
past. To do so, the robot continuously predicts future commands by querying a
machine learning model that is trained on past trajectories and conditioned on
the last received commands. In our experiments, an operator was able to
successfully control a humanoid robot (32 degrees of freedom) with stochastic
delays up to 2 seconds in several whole-body manipulation tasks, including
reaching different targets, picking up, and placing a box at distinct
locations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Penco_L/0/1/0/all/0/1"&gt;Luigi Penco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mouret_J/0/1/0/all/0/1"&gt;Jean-Baptiste Mouret&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ivaldi_S/0/1/0/all/0/1"&gt;Serena Ivaldi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of Plant Leaf Disease Directly in the JPEG Compressed Domain using Transfer Learning Technique. (arXiv:2107.04813v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04813</id>
        <link href="http://arxiv.org/abs/2107.04813"/>
        <updated>2021-07-13T01:59:37.352Z</updated>
        <summary type="html"><![CDATA[Plant leaf diseases pose a significant danger to food security and they cause
depletion in quality and volume of production. Therefore accurate and timely
detection of leaf disease is very important to check the loss of the crops and
meet the growing food demand of the people. Conventional techniques depend on
lab investigation and human skills which are generally costly and inaccessible.
Recently, Deep Neural Networks have been exceptionally fruitful in image
classification. In this research paper, plant leaf disease detection employing
transfer learning is explored in the JPEG compressed domain. Here, the JPEG
compressed stream consisting of DCT coefficients is, directly fed into the
Neural Network to improve the efficiency of classification. The experimental
results on JPEG compressed leaf dataset demonstrate the efficacy of the
proposed model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Atul Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajesh_B/0/1/0/all/0/1"&gt;Bulla Rajesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javed_M/0/1/0/all/0/1"&gt;Mohammed Javed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bypassing the Monster: A Faster and Simpler Optimal Algorithm for Contextual Bandits under Realizability. (arXiv:2003.12699v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.12699</id>
        <link href="http://arxiv.org/abs/2003.12699"/>
        <updated>2021-07-13T01:59:37.345Z</updated>
        <summary type="html"><![CDATA[We consider the general (stochastic) contextual bandit problem under the
realizability assumption, i.e., the expected reward, as a function of contexts
and actions, belongs to a general function class $\mathcal{F}$. We design a
fast and simple algorithm that achieves the statistically optimal regret with
only ${O}(\log T)$ calls to an offline regression oracle across all $T$ rounds.
The number of oracle calls can be further reduced to $O(\log\log T)$ if $T$ is
known in advance. Our results provide the first universal and optimal reduction
from contextual bandits to offline regression, solving an important open
problem in the contextual bandit literature. A direct consequence of our
results is that any advances in offline regression immediately translate to
contextual bandits, statistically and computationally. This leads to faster
algorithms and improved regret guarantees for broader classes of contextual
bandit problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Simchi_Levi_D/0/1/0/all/0/1"&gt;David Simchi-Levi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yunzong Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Position-enhanced and Time-aware Graph Convolutional Network for Sequential Recommendations. (arXiv:2107.05235v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.05235</id>
        <link href="http://arxiv.org/abs/2107.05235"/>
        <updated>2021-07-13T01:59:37.330Z</updated>
        <summary type="html"><![CDATA[Most of the existing deep learning-based sequential recommendation approaches
utilize the recurrent neural network architecture or self-attention to model
the sequential patterns and temporal influence among a user's historical
behavior and learn the user's preference at a specific time. However, these
methods have two main drawbacks. First, they focus on modeling users' dynamic
states from a user-centric perspective and always neglect the dynamics of items
over time. Second, most of them deal with only the first-order user-item
interactions and do not consider the high-order connectivity between users and
items, which has recently been proved helpful for the sequential
recommendation. To address the above problems, in this article, we attempt to
model user-item interactions by a bipartite graph structure and propose a new
recommendation approach based on a Position-enhanced and Time-aware Graph
Convolutional Network (PTGCN) for the sequential recommendation. PTGCN models
the sequential patterns and temporal dynamics between user-item interactions by
defining a position-enhanced and time-aware graph convolution operation and
learning the dynamic representations of users and items simultaneously on the
bipartite graph with a self-attention aggregator. Also, it realizes the
high-order connectivity between users and items by stacking multi-layer graph
convolutions. To demonstrate the effectiveness of PTGCN, we carried out a
comprehensive evaluation of PTGCN on three real-world datasets of different
sizes compared with a few competitive baselines. Experimental results indicate
that PTGCN outperforms several state-of-the-art models in terms of two
commonly-used evaluation metrics for ranking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Liwei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yutao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yanbo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuliang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Deyi Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Safe Reinforcement Learning with Constraint-mismatched Policies. (arXiv:2006.11645v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.11645</id>
        <link href="http://arxiv.org/abs/2006.11645"/>
        <updated>2021-07-13T01:59:37.315Z</updated>
        <summary type="html"><![CDATA[We consider the problem of reinforcement learning when provided with (1) a
baseline control policy and (2) a set of constraints that the learner must
satisfy. The baseline policy can arise from demonstration data or a teacher
agent and may provide useful cues for learning, but it might also be
sub-optimal for the task at hand, and is not guaranteed to satisfy the
specified constraints, which might encode safety, fairness or other
application-specific requirements. In order to safely learn from baseline
policies, we propose an iterative policy optimization algorithm that alternates
between maximizing expected return on the task, minimizing distance to the
baseline policy, and projecting the policy onto the constraint-satisfying set.
We analyze our algorithm theoretically and provide a finite-time convergence
guarantee. In our experiments on five different control tasks, our algorithm
consistently outperforms several state-of-the-art baselines, achieving 10 times
fewer constraint violations and 40% higher reward on average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1"&gt;Tsung-Yen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosca_J/0/1/0/all/0/1"&gt;Justinian Rosca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1"&gt;Karthik Narasimhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramadge_P/0/1/0/all/0/1"&gt;Peter J. Ramadge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TransClaw U-Net: Claw U-Net with Transformers for Medical Image Segmentation. (arXiv:2107.05188v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05188</id>
        <link href="http://arxiv.org/abs/2107.05188"/>
        <updated>2021-07-13T01:59:37.309Z</updated>
        <summary type="html"><![CDATA[In recent years, computer-aided diagnosis has become an increasingly popular
topic. Methods based on convolutional neural networks have achieved good
performance in medical image segmentation and classification. Due to the
limitations of the convolution operation, the long-term spatial features are
often not accurately obtained. Hence, we propose a TransClaw U-Net network
structure, which combines the convolution operation with the transformer
operation in the encoding part. The convolution part is applied for extracting
the shallow spatial features to facilitate the recovery of the image resolution
after upsampling. The transformer part is used to encode the patches, and the
self-attention mechanism is used to obtain global information between
sequences. The decoding part retains the bottom upsampling structure for better
detail segmentation performance. The experimental results on Synapse
Multi-organ Segmentation Datasets show that the performance of TransClaw U-Net
is better than other network structures. The ablation experiments also prove
the generalization performance of TransClaw U-Net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1"&gt;Yao Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menghan_H/0/1/0/all/0/1"&gt;Hu Menghan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guangtao_Z/0/1/0/all/0/1"&gt;Zhai Guangtao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Ping_Z/0/1/0/all/0/1"&gt;Zhang Xiao-Ping&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[U-Net with Hierarchical Bottleneck Attention for Landmark Detection in Fundus Images of the Degenerated Retina. (arXiv:2107.04721v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04721</id>
        <link href="http://arxiv.org/abs/2107.04721"/>
        <updated>2021-07-13T01:59:37.292Z</updated>
        <summary type="html"><![CDATA[Fundus photography has routinely been used to document the presence and
severity of retinal degenerative diseases such as age-related macular
degeneration (AMD), glaucoma, and diabetic retinopathy (DR) in clinical
practice, for which the fovea and optic disc (OD) are important retinal
landmarks. However, the occurrence of lesions, drusen, and other retinal
abnormalities during retinal degeneration severely complicates automatic
landmark detection and segmentation. Here we propose HBA-U-Net: a U-Net
backbone enriched with hierarchical bottleneck attention. The network consists
of a novel bottleneck attention block that combines and refines self-attention,
channel attention, and relative-position attention to highlight retinal
abnormalities that may be important for fovea and OD segmentation in the
degenerated retina. HBA-U-Net achieved state-of-the-art results on fovea
detection across datasets and eye conditions (ADAM: Euclidean Distance (ED) of
25.4 pixels, REFUGE: 32.5 pixels, IDRiD: 32.1 pixels), on OD segmentation for
AMD (ADAM: Dice Coefficient (DC) of 0.947), and on OD detection for DR (IDRiD:
ED of 20.5 pixels). Our results suggest that HBA-U-Net may be well suited for
landmark detection in the presence of a variety of retinal degenerative
diseases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tang_S/0/1/0/all/0/1"&gt;Shuyun Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qi_Z/0/1/0/all/0/1"&gt;Ziming Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Granley_J/0/1/0/all/0/1"&gt;Jacob Granley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Beyeler_M/0/1/0/all/0/1"&gt;Michael Beyeler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Error analysis for physics informed neural networks (PINNs) approximating Kolmogorov PDEs. (arXiv:2106.14473v2 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14473</id>
        <link href="http://arxiv.org/abs/2106.14473"/>
        <updated>2021-07-13T01:59:37.287Z</updated>
        <summary type="html"><![CDATA[Physics informed neural networks approximate solutions of PDEs by minimizing
pointwise residuals. We derive rigorous bounds on the error, incurred by PINNs
in approximating the solutions of a large class of linear parabolic PDEs,
namely Kolmogorov equations that include the heat equation and Black-Scholes
equation of option pricing, as examples. We construct neural networks, whose
PINN residual (generalization error) can be made as small as desired. We also
prove that the total $L^2$-error can be bounded by the generalization error,
which in turn is bounded in terms of the training error, provided that a
sufficient number of randomly chosen training (collocation) points is used.
Moreover, we prove that the size of the PINNs and the number of training
samples only grow polynomially with the underlying dimension, enabling PINNs to
overcome the curse of dimensionality in this context. These results enable us
to provide a comprehensive error analysis for PINNs in approximating Kolmogorov
PDEs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Ryck_T/0/1/0/all/0/1"&gt;Tim De Ryck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Siddhartha Mishra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Positive-Unlabeled Classification under Class-Prior Shift: A Prior-invariant Approach Based on Density Ratio Estimation. (arXiv:2107.05045v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05045</id>
        <link href="http://arxiv.org/abs/2107.05045"/>
        <updated>2021-07-13T01:59:37.281Z</updated>
        <summary type="html"><![CDATA[Learning from positive and unlabeled (PU) data is an important problem in
various applications. Most of the recent approaches for PU classification
assume that the class-prior (the ratio of positive samples) in the training
unlabeled dataset is identical to that of the test data, which does not hold in
many practical cases. In addition, we usually do not know the class-priors of
the training and test data, thus we have no clue on how to train a classifier
without them. To address these problems, we propose a novel PU classification
method based on density ratio estimation. A notable advantage of our proposed
method is that it does not require the class-priors in the training phase;
class-prior shift is incorporated only in the test phase. We theoretically
justify our proposed method and experimentally demonstrate its effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nakajima_S/0/1/0/all/0/1"&gt;Shota Nakajima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Longitudinal Correlation Analysis for Decoding Multi-Modal Brain Development. (arXiv:2107.04724v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04724</id>
        <link href="http://arxiv.org/abs/2107.04724"/>
        <updated>2021-07-13T01:59:37.276Z</updated>
        <summary type="html"><![CDATA[Starting from childhood, the human brain restructures and rewires throughout
life. Characterizing such complex brain development requires effective analysis
of longitudinal and multi-modal neuroimaging data. Here, we propose such an
analysis approach named Longitudinal Correlation Analysis (LCA). LCA couples
the data of two modalities by first reducing the input from each modality to a
latent representation based on autoencoders. A self-supervised strategy then
relates the two latent spaces by jointly disentangling two directions, one in
each space, such that the longitudinal changes in latent representations along
those directions are maximally correlated between modalities. We applied LCA to
analyze the longitudinal T1-weighted and diffusion-weighted MRIs of 679 youths
from the National Consortium on Alcohol and Neurodevelopment in Adolescence.
Unlike existing approaches that focus on either cross-sectional or single-modal
modeling, LCA successfully unraveled coupled macrostructural and
microstructural brain development from morphological and diffusivity features
extracted from the data. A retesting of LCA on raw 3D image volumes of those
subjects successfully replicated the findings from the feature-based analysis.
Lastly, the developmental effects revealed by LCA were inline with the current
understanding of maturational patterns of the adolescent brain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qingyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1"&gt;Ehsan Adeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1"&gt;Kilian M. Pohl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MidiBERT-Piano: Large-scale Pre-training for Symbolic Music Understanding. (arXiv:2107.05223v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.05223</id>
        <link href="http://arxiv.org/abs/2107.05223"/>
        <updated>2021-07-13T01:59:37.270Z</updated>
        <summary type="html"><![CDATA[This paper presents an attempt to employ the mask language modeling approach
of BERT to pre-train a 12-layer Transformer model over 4,166 pieces of
polyphonic piano MIDI files for tackling a number of symbolic-domain
discriminative music understanding tasks. These include two note-level
classification tasks, i.e., melody extraction and velocity prediction, as well
as two sequence-level classification tasks, i.e., composer classification and
emotion classification. We find that, given a pre-trained Transformer, our
models outperform recurrent neural network based baselines with less than 10
epochs of fine-tuning. Ablation studies show that the pre-training remains
effective even if none of the MIDI data of the downstream tasks are seen at the
pre-training stage, and that freezing the self-attention layers of the
Transformer at the fine-tuning stage slightly degrades performance. All the
five datasets employed in this work are publicly available, as well as
checkpoints of our pre-trained and fine-tuned models. As such, our research can
be taken as a benchmark for symbolic-domain music understanding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1"&gt;Yi-Hui Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_I/0/1/0/all/0/1"&gt;I-Chun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1"&gt;Chin-Jui Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ching_J/0/1/0/all/0/1"&gt;Joann Ching&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi-Hsuan Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cautious Actor-Critic. (arXiv:2107.05217v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05217</id>
        <link href="http://arxiv.org/abs/2107.05217"/>
        <updated>2021-07-13T01:59:37.253Z</updated>
        <summary type="html"><![CDATA[The oscillating performance of off-policy learning and persisting errors in
the actor-critic (AC) setting call for algorithms that can conservatively learn
to suit the stability-critical applications better. In this paper, we propose a
novel off-policy AC algorithm cautious actor-critic (CAC). The name cautious
comes from the doubly conservative nature that we exploit the classic policy
interpolation from conservative policy iteration for the actor and the
entropy-regularization of conservative value iteration for the critic. Our key
observation is the entropy-regularized critic facilitates and simplifies the
unwieldy interpolated actor update while still ensuring robust policy
improvement. We compare CAC to state-of-the-art AC methods on a set of
challenging continuous control problems and demonstrate that CAC achieves
comparable performance while significantly stabilizes learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lingwei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kitamura_T/0/1/0/all/0/1"&gt;Toshinori Kitamura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsubara_T/0/1/0/all/0/1"&gt;Takamitsu Matsubara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Polynomial Time Reinforcement Learning in Correlated FMDPs with Linear Value Functions. (arXiv:2107.05187v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05187</id>
        <link href="http://arxiv.org/abs/2107.05187"/>
        <updated>2021-07-13T01:59:37.247Z</updated>
        <summary type="html"><![CDATA[Many reinforcement learning (RL) environments in practice feature enormous
state spaces that may be described compactly by a "factored" structure, that
may be modeled by Factored Markov Decision Processes (FMDPs). We present the
first polynomial-time algorithm for RL with FMDPs that does not rely on an
oracle planner, and instead of requiring a linear transition model, only
requires a linear value function with a suitable local basis with respect to
the factorization. With this assumption, we can solve FMDPs in polynomial time
by constructing an efficient separation oracle for convex optimization.
Importantly, and in contrast to prior work, we do not assume that the
transitions on various factors are independent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Devic_S/0/1/0/all/0/1"&gt;Siddartha Devic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1"&gt;Zihao Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juba_B/0/1/0/all/0/1"&gt;Brendan Juba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Re-understanding Finite-State Representations of Recurrent Policy Networks. (arXiv:2006.03745v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03745</id>
        <link href="http://arxiv.org/abs/2006.03745"/>
        <updated>2021-07-13T01:59:37.241Z</updated>
        <summary type="html"><![CDATA[We introduce an approach for understanding control policies represented as
recurrent neural networks. Recent work has approached this problem by
transforming such recurrent policy networks into finite-state machines (FSM)
and then analyzing the equivalent minimized FSM. While this led to interesting
insights, the minimization process can obscure a deeper understanding of a
machine's operation by merging states that are semantically distinct. To
address this issue, we introduce an analysis approach that starts with an
unminimized FSM and applies more-interpretable reductions that preserve the key
decision points of the policy. We also contribute an attention tool to attain a
deeper understanding of the role of observations in the decisions. Our case
studies on 7 Atari games and 3 control benchmarks demonstrate that the approach
can reveal insights that have not been previously noticed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Danesh_M/0/1/0/all/0/1"&gt;Mohamad H. Danesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1"&gt;Anurag Koul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fern_A/0/1/0/all/0/1"&gt;Alan Fern&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khorram_S/0/1/0/all/0/1"&gt;Saeed Khorram&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Risk Model: A Deep Learning Solution for Mining Latent Risk Factors to Improve Covariance Matrix Estimation. (arXiv:2107.05201v1 [q-fin.RM])]]></title>
        <id>http://arxiv.org/abs/2107.05201</id>
        <link href="http://arxiv.org/abs/2107.05201"/>
        <updated>2021-07-13T01:59:37.235Z</updated>
        <summary type="html"><![CDATA[Modeling and managing portfolio risk is perhaps the most important step to
achieve growing and preserving investment performance. Within the modern
portfolio construction framework that built on Markowitz's theory, the
covariance matrix of stock returns is required to model the portfolio risk.
Traditional approaches to estimate the covariance matrix are based on human
designed risk factors, which often requires tremendous time and effort to
design better risk factors to improve the covariance estimation. In this work,
we formulate the quest of mining risk factors as a learning problem and propose
a deep learning solution to effectively "design" risk factors with neural
networks. The learning objective is carefully set to ensure the learned risk
factors are effective in explaining stock returns as well as have desired
orthogonality and stability. Our experiments on the stock market data
demonstrate the effectiveness of the proposed method: our method can obtain
$1.9\%$ higher explained variance measured by $R^2$ and also reduce the risk of
a global minimum variance portfolio. Incremental analysis further supports our
design of both the architecture and the learning objective.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Lin_H/0/1/0/all/0/1"&gt;Hengxu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Dong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weiqing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Bian_J/0/1/0/all/0/1"&gt;Jiang Bian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Over-parameterized Models with Non-decomposable Objectives. (arXiv:2107.04641v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04641</id>
        <link href="http://arxiv.org/abs/2107.04641"/>
        <updated>2021-07-13T01:59:37.229Z</updated>
        <summary type="html"><![CDATA[Many modern machine learning applications come with complex and nuanced
design goals such as minimizing the worst-case error, satisfying a given
precision or recall target, or enforcing group-fairness constraints. Popular
techniques for optimizing such non-decomposable objectives reduce the problem
into a sequence of cost-sensitive learning tasks, each of which is then solved
by re-weighting the training loss with example-specific costs. We point out
that the standard approach of re-weighting the loss to incorporate label costs
can produce unsatisfactory results when used to train over-parameterized
models. As a remedy, we propose new cost-sensitive losses that extend the
classical idea of logit adjustment to handle more general cost matrices. Our
losses are calibrated, and can be further improved with distilled labels from a
teacher model. Through experiments on benchmark image datasets, we showcase the
effectiveness of our approach in training ResNet models with common robust and
constrained optimization objectives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Narasimhan_H/0/1/0/all/0/1"&gt;Harikrishna Narasimhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1"&gt;Aditya Krishna Menon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Challenges and Opportunities in the African Agricultural Sector -- A General Perspective. (arXiv:2107.05101v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05101</id>
        <link href="http://arxiv.org/abs/2107.05101"/>
        <updated>2021-07-13T01:59:37.223Z</updated>
        <summary type="html"><![CDATA[The improvement of computers' capacities, advancements in algorithmic
techniques, and the significant increase of available data have enabled the
recent developments of Artificial Intelligence (AI) technology. One of its
branches, called Machine Learning (ML), has shown strong capacities in
mimicking characteristics attributed to human intelligence, such as vision,
speech, and problem-solving. However, as previous technological revolutions
suggest, their most significant impacts could be mostly expected on other
sectors that were not traditional users of that technology. The agricultural
sector is vital for African economies; improving yields, mitigating losses, and
effective management of natural resources are crucial in a climate change era.
Machine Learning is a technology with an added value in making predictions,
hence the potential to reduce uncertainties and risk across sectors, in this
case, the agricultural sector. The purpose of this paper is to contextualize
and discuss barriers to ML-based solutions for African agriculture. In the
second section, we provided an overview of ML technology from a historical and
technical perspective and its main driving force. In the third section, we
provided a brief review of the current use of ML in agriculture. Finally, in
section 4, we discuss ML growing interest in Africa and the potential barriers
to creating and using ML-based solutions in the agricultural sector.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ly_R/0/1/0/all/0/1"&gt;Racine Ly&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Social Media Information Sharing for Natural Disaster Response. (arXiv:2005.07019v5 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.07019</id>
        <link href="http://arxiv.org/abs/2005.07019"/>
        <updated>2021-07-13T01:59:37.204Z</updated>
        <summary type="html"><![CDATA[Social media has become an essential channel for posting disaster-related
information, which provide governments and relief agencies real-time data for
better disaster management. However, research in this field has not received
sufficient attention and extracting useful information is still challenging.
This paper aims to improve disaster relief efficiency via mining and analyzing
social media data like public attitudes towards disaster response and public
demands for targeted relief supplies during different types of disasters. We
focus on different natural disasters based on properties such as types,
durations, and damages, which contains a total of 41,993 tweets. In this paper,
public perception is assessed qualitatively by manually classified tweets,
which contain information like the demand for targeted relief supplies,
satisfactions of disaster response, and public fear. Public attitudes to
natural disasters are studied via a quantitative analysis using eight machine
learning models. To better provide decision-makers with the appropriate model,
the comparison of machine learning models based on computational time and
prediction accuracy is conducted. The change of public opinion during different
natural disasters and the evolution of people's behavior of using social media
for disaster relief in the face of the identical type of natural disasters as
Twitter continues to evolve are studied. The results in this paper demonstrate
the feasibility and validation of the proposed research approach and provide
relief agencies with insights into better disaster management.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1"&gt;Zhijie Sasha Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1"&gt;Lingyu Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Christenson_L/0/1/0/all/0/1"&gt;Lauren Christenson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fulton_L/0/1/0/all/0/1"&gt;Lawrence Fulton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EndoUDA: A modality independent segmentation approach for endoscopy imaging. (arXiv:2107.05342v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.05342</id>
        <link href="http://arxiv.org/abs/2107.05342"/>
        <updated>2021-07-13T01:59:37.198Z</updated>
        <summary type="html"><![CDATA[Gastrointestinal (GI) cancer precursors require frequent monitoring for risk
stratification of patients. Automated segmentation methods can help to assess
risk areas more accurately, and assist in therapeutic procedures or even
removal. In clinical practice, addition to the conventional white-light imaging
(WLI), complimentary modalities such as narrow-band imaging (NBI) and
fluorescence imaging are used. While, today most segmentation approaches are
supervised and only concentrated on a single modality dataset, this work
exploits to use a target-independent unsupervised domain adaptation (UDA)
technique that is capable to generalize to an unseen target modality. In this
context, we propose a novel UDA-based segmentation method that couples the
variational autoencoder and U-Net with a common EfficientNet-B4 backbone, and
uses a joint loss for latent-space optimization for target samples. We show
that our model can generalize to unseen target NBI (target) modality when
trained using only WLI (source) modality. Our experiments on both upper and
lower GI endoscopy data show the effectiveness of our approach compared to
naive supervised approach and state-of-the-art UDA segmentation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Celik_N/0/1/0/all/0/1"&gt;Numan Celik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1"&gt;Sharib Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Soumya Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Braden_B/0/1/0/all/0/1"&gt;Barbara Braden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rittscher_J/0/1/0/all/0/1"&gt;Jens Rittscher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attack Rules: An Adversarial Approach to Generate Attacks for Industrial Control Systems using Machine Learning. (arXiv:2107.05127v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.05127</id>
        <link href="http://arxiv.org/abs/2107.05127"/>
        <updated>2021-07-13T01:59:37.192Z</updated>
        <summary type="html"><![CDATA[Adversarial learning is used to test the robustness of machine learning
algorithms under attack and create attacks that deceive the anomaly detection
methods in Industrial Control System (ICS). Given that security assessment of
an ICS demands that an exhaustive set of possible attack patterns is studied,
in this work, we propose an association rule mining-based attack generation
technique. The technique has been implemented using data from a secure Water
Treatment plant. The proposed technique was able to generate more than 300,000
attack patterns constituting a vast majority of new attack vectors which were
not seen before. Automatically generated attacks improve our understanding of
the potential attacks and enable the design of robust attack detection
techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Umer_M/0/1/0/all/0/1"&gt;Muhammad Azmi Umer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_C/0/1/0/all/0/1"&gt;Chuadhry Mujeeb Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jilani_M/0/1/0/all/0/1"&gt;Muhammad Taha Jilani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathur_A/0/1/0/all/0/1"&gt;Aditya P. Mathur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Preference-based Online Learning with Dueling Bandits: A Survey. (arXiv:1807.11398v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1807.11398</id>
        <link href="http://arxiv.org/abs/1807.11398"/>
        <updated>2021-07-13T01:59:37.185Z</updated>
        <summary type="html"><![CDATA[In machine learning, the notion of multi-armed bandits refers to a class of
online learning problems, in which an agent is supposed to simultaneously
explore and exploit a given set of choice alternatives in the course of a
sequential decision process. In the standard setting, the agent learns from
stochastic feedback in the form of real-valued rewards. In many applications,
however, numerical reward signals are not readily available -- instead, only
weaker information is provided, in particular relative preferences in the form
of qualitative comparisons between pairs of alternatives. This observation has
motivated the study of variants of the multi-armed bandit problem, in which
more general representations are used both for the type of feedback to learn
from and the target of prediction. The aim of this paper is to provide a survey
of the state of the art in this field, referred to as preference-based
multi-armed bandits or dueling bandits. To this end, we provide an overview of
problems that have been considered in the literature as well as methods for
tackling them. Our taxonomy is mainly based on the assumptions made by these
methods about the data-generating process and, related to this, the properties
of the preference-based feedback.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bengs_V/0/1/0/all/0/1"&gt;Viktor Bengs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Busa_Fekete_R/0/1/0/all/0/1"&gt;Robert Busa-Fekete&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mesaoudi_Paul_A/0/1/0/all/0/1"&gt;Adil El Mesaoudi-Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1"&gt;Eyke H&amp;#xfc;llermeier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implicit Langevin Algorithms for Sampling From Log-concave Densities. (arXiv:1903.12322v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1903.12322</id>
        <link href="http://arxiv.org/abs/1903.12322"/>
        <updated>2021-07-13T01:59:37.168Z</updated>
        <summary type="html"><![CDATA[For sampling from a log-concave density, we study implicit integrators
resulting from $\theta$-method discretization of the overdamped Langevin
diffusion stochastic differential equation. Theoretical and algorithmic
properties of the resulting sampling methods for $ \theta \in [0,1] $ and a
range of step sizes are established. Our results generalize and extend prior
works in several directions. In particular, for $\theta\ge1/2$, we prove
geometric ergodicity and stability of the resulting methods for all step sizes.
We show that obtaining subsequent samples amounts to solving a strongly-convex
optimization problem, which is readily achievable using one of numerous
existing methods. Numerical examples supporting our theoretical analysis are
also presented.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hodgkinson_L/0/1/0/all/0/1"&gt;Liam Hodgkinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Salomone_R/0/1/0/all/0/1"&gt;Robert Salomone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Roosta_F/0/1/0/all/0/1"&gt;Fred Roosta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Rich Transcription-Style Automatic Speech Recognition with Semi-Supervised Learning. (arXiv:2107.05382v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05382</id>
        <link href="http://arxiv.org/abs/2107.05382"/>
        <updated>2021-07-13T01:59:37.161Z</updated>
        <summary type="html"><![CDATA[We propose a semi-supervised learning method for building end-to-end rich
transcription-style automatic speech recognition (RT-ASR) systems from
small-scale rich transcription-style and large-scale common transcription-style
datasets. In spontaneous speech tasks, various speech phenomena such as
fillers, word fragments, laughter and coughs, etc. are often included. While
common transcriptions do not give special awareness to these phenomena, rich
transcriptions explicitly convert them into special phenomenon tokens as well
as textual tokens. In previous studies, the textual and phenomenon tokens were
simultaneously estimated in an end-to-end manner. However, it is difficult to
build accurate RT-ASR systems because large-scale rich transcription-style
datasets are often unavailable. To solve this problem, our training method uses
a limited rich transcription-style dataset and common transcription-style
dataset simultaneously. The Key process in our semi-supervised learning is to
convert the common transcription-style dataset into a pseudo-rich
transcription-style dataset. To this end, we introduce style tokens which
control phenomenon tokens are generated or not into transformer-based
autoregressive modeling. We use this modeling for generating the pseudo-rich
transcription-style datasets and for building RT-ASR system from the pseudo and
original datasets. Our experiments on spontaneous ASR tasks showed the
effectiveness of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1"&gt;Tomohiro Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masumura_R/0/1/0/all/0/1"&gt;Ryo Masumura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ihori_M/0/1/0/all/0/1"&gt;Mana Ihori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takashima_A/0/1/0/all/0/1"&gt;Akihiko Takashima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orihashi_S/0/1/0/all/0/1"&gt;Shota Orihashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makishima_N/0/1/0/all/0/1"&gt;Naoki Makishima&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One Map Does Not Fit All: Evaluating Saliency Map Explanation on Multi-Modal Medical Images. (arXiv:2107.05047v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05047</id>
        <link href="http://arxiv.org/abs/2107.05047"/>
        <updated>2021-07-13T01:59:37.155Z</updated>
        <summary type="html"><![CDATA[Being able to explain the prediction to clinical end-users is a necessity to
leverage the power of AI models for clinical decision support. For medical
images, saliency maps are the most common form of explanation. The maps
highlight important features for AI model's prediction. Although many saliency
map methods have been proposed, it is unknown how well they perform on
explaining decisions on multi-modal medical images, where each modality/channel
carries distinct clinical meanings of the same underlying biomedical
phenomenon. Understanding such modality-dependent features is essential for
clinical users' interpretation of AI decisions. To tackle this clinically
important but technically ignored problem, we propose the MSFI
(Modality-Specific Feature Importance) metric to examine whether saliency maps
can highlight modality-specific important features. MSFI encodes the clinical
requirements on modality prioritization and modality-specific feature
localization. Our evaluations on 16 commonly used saliency map methods,
including a clinician user study, show that although most saliency map methods
captured modality importance information in general, most of them failed to
highlight modality-specific important features consistently and precisely. The
evaluation results guide the choices of saliency map methods and provide
insights to propose new ones targeting clinical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1"&gt;Weina Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoxiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamarneh_G/0/1/0/all/0/1"&gt;Ghassan Hamarneh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonlinear Traffic Prediction as a Matrix Completion Problem with Ensemble Learning. (arXiv:2001.02492v4 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.02492</id>
        <link href="http://arxiv.org/abs/2001.02492"/>
        <updated>2021-07-13T01:59:37.149Z</updated>
        <summary type="html"><![CDATA[This paper addresses the problem of short-term traffic prediction for
signalized traffic operations management. Specifically, we focus on predicting
sensor states in high-resolution (second-by-second). This contrasts with
traditional traffic forecasting problems, which have focused on predicting
aggregated traffic variables, typically over intervals that are no shorter than
5 minutes. Our contributions can be summarized as offering three insights:
first, we show how the prediction problem can be modeled as a matrix completion
problem. Second, we employ a block-coordinate descent algorithm and demonstrate
that the algorithm converges in sub-linear time to a block coordinate-wise
optimizer. This allows us to capitalize on the "bigness" of high-resolution
data in a computationally feasible way. Third, we develop an ensemble learning
(or adaptive boosting) approach to reduce the training error to within any
arbitrary error threshold. The latter utilizes past days so that the boosting
can be interpreted as capturing periodic patterns in the data. The performance
of the proposed method is analyzed theoretically and tested empirically using
both simulated data and a real-world high-resolution traffic dataset from Abu
Dhabi, UAE. Our experimental results show that the proposed method outperforms
other state-of-the-art algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenqing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chuhan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jabari_S/0/1/0/all/0/1"&gt;Saif Eddin Jabari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Federated Learning via Maximizing Correlation with Sparse and Hierarchical Extensions. (arXiv:2107.05330v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05330</id>
        <link href="http://arxiv.org/abs/2107.05330"/>
        <updated>2021-07-13T01:59:37.143Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) is a collaborative machine learning technique to
train a global model without obtaining clients' private data. The main
challenges in FL are statistical diversity among clients, limited computing
capability among client equipments and the excessive communication overhead and
long latency between server and clients. To address these problems,

we propose a novel personalized federated learning via maximizing correlation
pFedMac), and further extend it to sparse and hierarchical models. By
minimizing loss functions including the properties of an approximated L1-norm
and the hierarchical correlation, the performance on statistical diversity data
is improved and the communicational and computational loads required in the
network are reduced. Theoretical proofs show that pFedMac performs better than
the L2-norm distance based personalization methods. Experimentally, we
demonstrate the benefits of this sparse hierarchical personalization
architecture compared with the state-of-the-art personalization methods and
their extensions (e.g. pFedMac achieves 99.75% accuracy on MNIST and 87.27%
accuracy on Synthetic under heterogeneous and non-i.i.d data distributions)]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+YinchuanLi/0/1/0/all/0/1"&gt;YinchuanLi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+XiaofengLiu/0/1/0/all/0/1"&gt;XiaofengLiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+XuZhang/0/1/0/all/0/1"&gt;XuZhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+YunfengShao/0/1/0/all/0/1"&gt;YunfengShao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+QingWang/0/1/0/all/0/1"&gt;QingWang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+YanhuiGeng/0/1/0/all/0/1"&gt;YanhuiGeng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PDE-based Group Equivariant Convolutional Neural Networks. (arXiv:2001.09046v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.09046</id>
        <link href="http://arxiv.org/abs/2001.09046"/>
        <updated>2021-07-13T01:59:37.138Z</updated>
        <summary type="html"><![CDATA[We present a PDE-based framework that generalizes Group equivariant
Convolutional Neural Networks (G-CNNs). In this framework, a network layer is
seen as a set of PDE-solvers where geometrically meaningful PDE-coefficients
become the layer's trainable weights. Formulating our PDEs on homogeneous
spaces allows these networks to be designed with built-in symmetries such as
rotation in addition to the standard translation equivariance of CNNs.

Having all the desired symmetries included in the design obviates the need to
include them by means of costly techniques such as data augmentation. We will
discuss our PDE-based G-CNNs (PDE-G-CNNs) in a general homogeneous space
setting while also going into the specifics of our primary case of interest:
roto-translation equivariance.

We solve the PDE of interest by a combination of linear group convolutions
and non-linear morphological group convolutions with analytic kernel
approximations that we underpin with formal theorems. Our kernel approximations
allow for fast GPU-implementation of the PDE-solvers, we release our
implementation with this article. Just like for linear convolution a
morphological convolution is specified by a kernel that we train in our
PDE-G-CNNs. In PDE-G-CNNs we do not use non-linearities such as max/min-pooling
and ReLUs as they are already subsumed by morphological convolutions.

We present a set of experiments to demonstrate the strength of the proposed
PDE-G-CNNs in increasing the performance of deep learning based imaging
applications with far fewer parameters than traditional CNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smets_B/0/1/0/all/0/1"&gt;Bart Smets&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Portegies_J/0/1/0/all/0/1"&gt;Jim Portegies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1"&gt;Erik Bekkers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duits_R/0/1/0/all/0/1"&gt;Remco Duits&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Flexible Multi-Task Model for BERT Serving. (arXiv:2107.05377v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05377</id>
        <link href="http://arxiv.org/abs/2107.05377"/>
        <updated>2021-07-13T01:59:37.103Z</updated>
        <summary type="html"><![CDATA[In this demonstration, we present an efficient BERT-based multi-task (MT)
framework that is particularly suitable for iterative and incremental
development of the tasks. The proposed framework is based on the idea of
partial fine-tuning, i.e. only fine-tune some top layers of BERT while keep the
other layers frozen. For each task, we train independently a single-task (ST)
model using partial fine-tuning. Then we compress the task-specific layers in
each ST model using knowledge distillation. Those compressed ST models are
finally merged into one MT model so that the frozen layers of the former are
shared across the tasks. We exemplify our approach on eight GLUE tasks,
demonstrating that it is able to achieve both strong performance and
efficiency. We have implemented our method in the utterance understanding
system of XiaoAI, a commercial AI assistant developed by Xiaomi. We estimate
that our model reduces the overall serving cost by 86%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1"&gt;Tianwen Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1"&gt;Jianwei Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shenghuan He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonparametric Regression with Shallow Overparameterized Neural Networks Trained by GD with Early Stopping. (arXiv:2107.05341v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05341</id>
        <link href="http://arxiv.org/abs/2107.05341"/>
        <updated>2021-07-13T01:59:37.083Z</updated>
        <summary type="html"><![CDATA[We explore the ability of overparameterized shallow neural networks to learn
Lipschitz regression functions with and without label noise when trained by
Gradient Descent (GD). To avoid the problem that in the presence of noisy
labels, neural networks trained to nearly zero training error are inconsistent
on this class, we propose an early stopping rule that allows us to show optimal
rates. This provides an alternative to the result of Hu et al. (2021) who
studied the performance of $\ell 2$ -regularized GD for training shallow
networks in nonparametric regression which fully relied on the infinite-width
network (Neural Tangent Kernel (NTK)) approximation. Here we present a simpler
analysis which is based on a partitioning argument of the input space (as in
the case of 1-nearest-neighbor rule) coupled with the fact that trained neural
networks are smooth with respect to their inputs when trained by GD. In the
noise-free case the proof does not rely on any kernelization and can be
regarded as a finite-width result. In the case of label noise, by slightly
modifying the proof, the noise is controlled using a technique of Yao, Rosasco,
and Caponnetto (2007).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuzborskij_I/0/1/0/all/0/1"&gt;Ilja Kuzborskij&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1"&gt;Csaba Szepesv&amp;#xe1;ri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DISCO : efficient unsupervised decoding for discrete natural language problems via convex relaxation. (arXiv:2107.05380v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05380</id>
        <link href="http://arxiv.org/abs/2107.05380"/>
        <updated>2021-07-13T01:59:37.077Z</updated>
        <summary type="html"><![CDATA[In this paper we study test time decoding; an ubiquitous step in almost all
sequential text generation task spanning across a wide array of natural
language processing (NLP) problems. Our main contribution is to develop a
continuous relaxation framework for the combinatorial NP-hard decoding problem
and propose Disco - an efficient algorithm based on standard first order
gradient based. We provide tight analysis and show that our proposed algorithm
linearly converges to within $\epsilon$ neighborhood of the optima. Finally, we
perform preliminary experiments on the task of adversarial text generation and
show superior performance of Disco over several popular decoding approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1"&gt;Anish Acharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1"&gt;Rudrajit Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1"&gt;Greg Durrett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1"&gt;Inderjit Dhillon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanghavi_S/0/1/0/all/0/1"&gt;Sujay Sanghavi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards explainable meta-learning. (arXiv:2002.04276v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.04276</id>
        <link href="http://arxiv.org/abs/2002.04276"/>
        <updated>2021-07-13T01:59:37.071Z</updated>
        <summary type="html"><![CDATA[Meta-learning is a field that aims at discovering how different machine
learning algorithms perform on a wide range of predictive tasks. Such knowledge
speeds up the hyperparameter tuning or feature engineering. With the use of
surrogate models various aspects of the predictive task such as meta-features,
landmarker models e.t.c. are used to predict the expected performance. State of
the art approaches are focused on searching for the best meta-model but do not
explain how these different aspects contribute to its performance. However, to
build a new generation of meta-models we need a deeper understanding of the
importance and effect of meta-features on the model tunability. In this paper,
we propose techniques developed for eXplainable Artificial Intelligence (XAI)
to examine and extract knowledge from black-box surrogate models. To our
knowledge, this is the first paper that shows how post-hoc explainability can
be used to improve the meta-learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Woznica_K/0/1/0/all/0/1"&gt;Katarzyna Wo&amp;#x17a;nica&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Biecek_P/0/1/0/all/0/1"&gt;Przemys&amp;#x142;aw Biecek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DaCy: A Unified Framework for Danish NLP. (arXiv:2107.05295v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05295</id>
        <link href="http://arxiv.org/abs/2107.05295"/>
        <updated>2021-07-13T01:59:37.054Z</updated>
        <summary type="html"><![CDATA[Danish natural language processing (NLP) has in recent years obtained
considerable improvements with the addition of multiple new datasets and
models. However, at present, there is no coherent framework for applying
state-of-the-art models for Danish. We present DaCy: a unified framework for
Danish NLP built on SpaCy. DaCy uses efficient multitask models which obtain
state-of-the-art performance on named entity recognition, part-of-speech
tagging, and dependency parsing. DaCy contains tools for easy integration of
existing models such as for polarity, emotion, or subjectivity detection. In
addition, we conduct a series of tests for biases and robustness of Danish NLP
pipelines through augmentation of the test set of DaNE. DaCy large compares
favorably and is especially robust to long input lengths and spelling
variations and errors. All models except DaCy large display significant biases
related to ethnicity while only Polyglot shows a significant gender bias. We
argue that for languages with limited benchmark sets, data augmentation can be
particularly useful for obtaining more realistic and fine-grained performance
estimates. We provide a series of augmenters as a first step towards a more
thorough evaluation of language models for low and medium resource languages
and encourage further development.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Enevoldsen_K/0/1/0/all/0/1"&gt;Kenneth Enevoldsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hansen_L/0/1/0/all/0/1"&gt;Lasse Hansen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nielbo_K/0/1/0/all/0/1"&gt;Kristoffer Nielbo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Metalearning Linear Bandits by Prior Update. (arXiv:2107.05320v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.05320</id>
        <link href="http://arxiv.org/abs/2107.05320"/>
        <updated>2021-07-13T01:59:37.048Z</updated>
        <summary type="html"><![CDATA[Fully Bayesian approaches to sequential decision-making assume that problem
parameters are generated from a known prior, while in practice, such
information is often lacking, and needs to be estimated through learning. This
problem is exacerbated in decision-making setups with partial information,
where using a misspecified prior may lead to poor exploration and inferior
performance. In this work we prove, in the context of stochastic linear bandits
and Gaussian priors, that as long as the prior estimate is sufficiently close
to the true prior, the performance of an algorithm that uses the misspecified
prior is close to that of the algorithm that uses the true prior. Next, we
address the task of learning the prior through metalearning, where a learner
updates its estimate of the prior across multiple task instances in order to
improve performance on future tasks. The estimated prior is then updated within
each task based on incoming observations, while actions are selected in order
to maximize expected reward. In this work we apply this scheme within a linear
bandit setting, and provide algorithms and regret bounds, demonstrating its
effectiveness, as compared to an algorithm that knows the correct prior. Our
results hold for a broad class of algorithms, including, for example, Thompson
Sampling and Information Directed Sampling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Peleg_A/0/1/0/all/0/1"&gt;Amit Peleg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pearl_N/0/1/0/all/0/1"&gt;Naama Pearl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Meir_R/0/1/0/all/0/1"&gt;Ron Meir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counterfactual Mean Embeddings. (arXiv:1805.08845v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1805.08845</id>
        <link href="http://arxiv.org/abs/1805.08845"/>
        <updated>2021-07-13T01:59:37.042Z</updated>
        <summary type="html"><![CDATA[Counterfactual inference has become a ubiquitous tool in online
advertisement, recommendation systems, medical diagnosis, and econometrics.
Accurate modeling of outcome distributions associated with different
interventions -- known as counterfactual distributions -- is crucial for the
success of these applications. In this work, we propose to model counterfactual
distributions using a novel Hilbert space representation called counterfactual
mean embedding (CME). The CME embeds the associated counterfactual distribution
into a reproducing kernel Hilbert space (RKHS) endowed with a positive definite
kernel, which allows us to perform causal inference over the entire landscape
of the counterfactual distribution. Based on this representation, we propose a
distributional treatment effect (DTE) that can quantify the causal effect over
entire outcome distributions. Our approach is nonparametric as the CME can be
estimated under the unconfoundedness assumption from observational data without
requiring any parametric assumption about the underlying distributions. We also
establish a rate of convergence of the proposed estimator which depends on the
smoothness of the conditional mean and the Radon-Nikodym derivative of the
underlying marginal distributions. Furthermore, our framework allows for more
complex outcomes such as images, sequences, and graphs. Our experimental
results on synthetic data and off-policy evaluation tasks demonstrate the
advantages of the proposed estimator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Muandet_K/0/1/0/all/0/1"&gt;Krikamol Muandet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kanagawa_M/0/1/0/all/0/1"&gt;Motonobu Kanagawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Saengkyongam_S/0/1/0/all/0/1"&gt;Sorawit Saengkyongam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Marukatat_S/0/1/0/all/0/1"&gt;Sanparith Marukatat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learned super resolution ultrasound for improved breast lesion characterization. (arXiv:2107.05270v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05270</id>
        <link href="http://arxiv.org/abs/2107.05270"/>
        <updated>2021-07-13T01:59:37.035Z</updated>
        <summary type="html"><![CDATA[Breast cancer is the most common malignancy in women. Mammographic findings
such as microcalcifications and masses, as well as morphologic features of
masses in sonographic scans, are the main diagnostic targets for tumor
detection. However, improved specificity of these imaging modalities is
required. A leading alternative target is neoangiogenesis. When pathological,
it contributes to the development of numerous types of tumors, and the
formation of metastases. Hence, demonstrating neoangiogenesis by visualization
of the microvasculature may be of great importance. Super resolution ultrasound
localization microscopy enables imaging of the microvasculature at the
capillary level. Yet, challenges such as long reconstruction time, dependency
on prior knowledge of the system Point Spread Function (PSF), and separability
of the Ultrasound Contrast Agents (UCAs), need to be addressed for translation
of super-resolution US into the clinic. In this work we use a deep neural
network architecture that makes effective use of signal structure to address
these challenges. We present in vivo human results of three different breast
lesions acquired with a clinical US scanner. By leveraging our trained network,
the microvasculature structure is recovered in a short time, without prior PSF
knowledge, and without requiring separability of the UCAs. Each of the
recoveries exhibits a different structure that corresponds with the known
histological structure. This study demonstrates the feasibility of in vivo
human super resolution, based on a clinical scanner, to increase US specificity
for different breast lesions and promotes the use of US in the diagnosis of
breast pathologies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bar_Shira_O/0/1/0/all/0/1"&gt;Or Bar-Shira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grubstein_A/0/1/0/all/0/1"&gt;Ahuva Grubstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rapson_Y/0/1/0/all/0/1"&gt;Yael Rapson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suhami_D/0/1/0/all/0/1"&gt;Dror Suhami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atar_E/0/1/0/all/0/1"&gt;Eli Atar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peri_Hanania_K/0/1/0/all/0/1"&gt;Keren Peri-Hanania&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosen_R/0/1/0/all/0/1"&gt;Ronnie Rosen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eldar_Y/0/1/0/all/0/1"&gt;Yonina C. Eldar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HEMP: High-order Entropy Minimization for neural network comPression. (arXiv:2107.05298v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05298</id>
        <link href="http://arxiv.org/abs/2107.05298"/>
        <updated>2021-07-13T01:59:37.029Z</updated>
        <summary type="html"><![CDATA[We formulate the entropy of a quantized artificial neural network as a
differentiable function that can be plugged as a regularization term into the
cost function minimized by gradient descent. Our formulation scales efficiently
beyond the first order and is agnostic of the quantization scheme. The network
can then be trained to minimize the entropy of the quantized parameters, so
that they can be optimally compressed via entropy coding. We experiment with
our entropy formulation at quantizing and compressing well-known network
architectures over multiple datasets. Our approach compares favorably over
similar methods, enjoying the benefits of higher order entropy estimate,
showing flexibility towards non-uniform quantization (we use Lloyd-max
quantization), scalability towards any entropy order to be minimized and
efficiency in terms of compression. We show that HEMP is able to work in
synergy with other approaches aiming at pruning or quantizing the model itself,
delivering significant benefits in terms of storage size compressibility
without harming the model's performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tartaglione_E/0/1/0/all/0/1"&gt;Enzo Tartaglione&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phane Lathuili&amp;#xe8;re&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fiandrotti_A/0/1/0/all/0/1"&gt;Attilio Fiandrotti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cagnazzo_M/0/1/0/all/0/1"&gt;Marco Cagnazzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grangetto_M/0/1/0/all/0/1"&gt;Marco Grangetto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Brownian motion in the transformer model. (arXiv:2107.05264v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05264</id>
        <link href="http://arxiv.org/abs/2107.05264"/>
        <updated>2021-07-13T01:59:37.013Z</updated>
        <summary type="html"><![CDATA[Transformer is the state of the art model for many language and visual tasks.
In this paper, we give a deep analysis of its multi-head self-attention (MHSA)
module and find that: 1) Each token is a random variable in high dimensional
feature space. 2) After layer normalization, these variables are mapped to
points on the hyper-sphere. 3) The update of these tokens is a Brownian motion.
The Brownian motion has special properties, its second order item should not be
ignored. So we present a new second-order optimizer(an iterative K-FAC
algorithm) for the MHSA module.

In some short words: All tokens are mapped to high dimension hyper-sphere.
The Scaled Dot-Product Attention
$softmax(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d}})$ is just the Markov
transition matrix for the random walking on the sphere. And the deep learning
process would learn proper kernel function to get proper positions of these
tokens. The training process in the MHSA module corresponds to a Brownian
motion worthy of further study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yingshi Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OmniLytics: A Blockchain-based Secure Data Market for Decentralized Machine Learning. (arXiv:2107.05252v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.05252</id>
        <link href="http://arxiv.org/abs/2107.05252"/>
        <updated>2021-07-13T01:59:37.006Z</updated>
        <summary type="html"><![CDATA[We propose OmniLytics, a blockchain-based secure data trading marketplace for
machine learning applications. Utilizing OmniLytics, many distributed data
owners can contribute their private data to collectively train a ML model
requested by some model owners, and get compensated for data contribution.
OmniLytics enables such model training while simultaneously providing 1) model
security against curious data owners; 2) data security against curious model
and data owners; 3) resilience to malicious data owners who provide faulty
results to poison model training; and 4) resilience to malicious model owner
who intents to evade the payment. OmniLytics is implemented as a smart contract
on the Ethereum blockchain to guarantee the atomicity of payment. In
OmniLytics, a model owner publishes encrypted initial model on the contract,
over which the participating data owners compute gradients using their private
data, and securely aggregate the gradients through the contract. Finally, the
contract reimburses the data owners, and the model owner decrypts the
aggregated model update. We implement a working prototype of OmniLytics on
Ethereum, and perform extensive experiments to measure its gas cost and
execution time under various parameter combinations, demonstrating its high
computation and cost efficiency and strong practicality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jiacheng Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1"&gt;Wensi Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Songze Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Remote Blood Oxygen Estimation From Videos Using Neural Networks. (arXiv:2107.05087v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05087</id>
        <link href="http://arxiv.org/abs/2107.05087"/>
        <updated>2021-07-13T01:59:37.000Z</updated>
        <summary type="html"><![CDATA[Blood oxygen saturation (SpO$_2$) is an essential indicator of respiratory
functionality and is receiving increasing attention during the COVID-19
pandemic. Clinical findings show that it is possible for COVID-19 patients to
have significantly low SpO$_2$ before any obvious symptoms. The prevalence of
cameras has motivated researchers to investigate methods for monitoring SpO$_2$
using videos. Most prior schemes involving smartphones are contact-based: They
require a fingertip to cover the phone's camera and the nearby light source to
capture re-emitted light from the illuminated tissue. In this paper, we propose
the first convolutional neural network based noncontact SpO$_2$ estimation
scheme using smartphone cameras. The scheme analyzes the videos of a
participant's hand for physiological sensing, which is convenient and
comfortable, and can protect their privacy and allow for keeping face masks on.
We design our neural network architectures inspired by the optophysiological
models for SpO$_2$ measurement and demonstrate the explainability by
visualizing the weights for channel combination. Our proposed models outperform
the state-of-the-art model that is designed for contact-based SpO$_2$
measurement, showing the potential of our proposed method to contribute to
public health. We also analyze the impact of skin type and the side of a hand
on SpO$_2$ estimation performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mathew_J/0/1/0/all/0/1"&gt;Joshua Mathew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1"&gt;Xin Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1"&gt;Min Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1"&gt;Chau-Wai Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effect of Input Size on the Classification of Lung Nodules Using Convolutional Neural Networks. (arXiv:2107.05085v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.05085</id>
        <link href="http://arxiv.org/abs/2107.05085"/>
        <updated>2021-07-13T01:59:36.993Z</updated>
        <summary type="html"><![CDATA[Recent studies have shown that lung cancer screening using annual low-dose
computed tomography (CT) reduces lung cancer mortality by 20% compared to
traditional chest radiography. Therefore, CT lung screening has started to be
used widely all across the world. However, analyzing these images is a serious
burden for radiologists. The number of slices in a CT scan can be up to 600.
Therefore, computer-aided-detection (CAD) systems are very important for faster
and more accurate assessment of the data. In this study, we proposed a
framework that analyzes CT lung screenings using convolutional neural networks
(CNNs) to reduce false positives. We trained our model with different volume
sizes and showed that volume size plays a critical role in the performance of
the system. We also used different fusions in order to show their power and
effect on the overall accuracy. 3D CNNs were preferred over 2D CNNs because 2D
convolutional operations applied to 3D data could result in information loss.
The proposed framework has been tested on the dataset provided by the LUNA16
Challenge and resulted in a sensitivity of 0.831 at 1 false positive per scan.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Polat_G/0/1/0/all/0/1"&gt;Gorkem Polat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Serinagaoglu_Y/0/1/0/all/0/1"&gt;Yesim Dogrusoz Serinagaoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Halici_U/0/1/0/all/0/1"&gt;Ugur Halici&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuous Time Bandits With Sampling Costs. (arXiv:2107.05289v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05289</id>
        <link href="http://arxiv.org/abs/2107.05289"/>
        <updated>2021-07-13T01:59:36.982Z</updated>
        <summary type="html"><![CDATA[We consider a continuous-time multi-arm bandit problem (CTMAB), where the
learner can sample arms any number of times in a given interval and obtain a
random reward from each sample, however, increasing the frequency of sampling
incurs an additive penalty/cost. Thus, there is a tradeoff between obtaining
large reward and incurring sampling cost as a function of the sampling
frequency. The goal is to design a learning algorithm that minimizes regret,
that is defined as the difference of the payoff of the oracle policy and that
of the learning algorithm. CTMAB is fundamentally different than the usual
multi-arm bandit problem (MAB), e.g., even the single-arm case is non-trivial
in CTMAB, since the optimal sampling frequency depends on the mean of the arm,
which needs to be estimated. We first establish lower bounds on the regret
achievable with any algorithm and then propose algorithms that achieve the
lower bound up to logarithmic factors. For the single-arm case, we show that
the lower bound on the regret is $\Omega((\log T)^2/\mu)$, where $\mu$ is the
mean of the arm, and $T$ is the time horizon. For the multiple arms case, we
show that the lower bound on the regret is $\Omega((\log T)^2 \mu/\Delta^2)$,
where $\mu$ now represents the mean of the best arm, and $\Delta$ is the
difference of the mean of the best and the second-best arm. We then propose an
algorithm that achieves the bound up to constant terms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vaze_R/0/1/0/all/0/1"&gt;Rahul Vaze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanawal_M/0/1/0/all/0/1"&gt;Manjesh K. Hanawal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stateful Detection of Model Extraction Attacks. (arXiv:2107.05166v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05166</id>
        <link href="http://arxiv.org/abs/2107.05166"/>
        <updated>2021-07-13T01:59:36.966Z</updated>
        <summary type="html"><![CDATA[Machine-Learning-as-a-Service providers expose machine learning (ML) models
through application programming interfaces (APIs) to developers. Recent work
has shown that attackers can exploit these APIs to extract good approximations
of such ML models, by querying them with samples of their choosing. We propose
VarDetect, a stateful monitor that tracks the distribution of queries made by
users of such a service, to detect model extraction attacks. Harnessing the
latent distributions learned by a modified variational autoencoder, VarDetect
robustly separates three types of attacker samples from benign samples, and
successfully raises an alarm for each. Further, with VarDetect deployed as an
automated defense mechanism, the extracted substitute models are found to
exhibit poor performance and transferability, as intended. Finally, we
demonstrate that even adaptive attackers with prior knowledge of the deployment
of VarDetect, are detected by it.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1"&gt;Soham Pal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_Y/0/1/0/all/0/1"&gt;Yash Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanade_A/0/1/0/all/0/1"&gt;Aditya Kanade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shevade_S/0/1/0/all/0/1"&gt;Shirish Shevade&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out-of-Distribution Dynamics Detection: RL-Relevant Benchmarks and Results. (arXiv:2107.04982v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04982</id>
        <link href="http://arxiv.org/abs/2107.04982"/>
        <updated>2021-07-13T01:59:36.960Z</updated>
        <summary type="html"><![CDATA[We study the problem of out-of-distribution dynamics (OODD) detection, which
involves detecting when the dynamics of a temporal process change compared to
the training-distribution dynamics. This is relevant to applications in
control, reinforcement learning (RL), and multi-variate time-series, where
changes to test time dynamics can impact the performance of learning
controllers/predictors in unknown ways. This problem is particularly important
in the context of deep RL, where learned controllers often overfit to the
training environment. Currently, however, there is a lack of established OODD
benchmarks for the types of environments commonly used in RL research. Our
first contribution is to design a set of OODD benchmarks derived from common RL
environments with varying types and intensities of OODD. Our second
contribution is to design a strong OODD baseline approach based on recurrent
implicit quantile networks (RIQNs), which monitors autoregressive prediction
errors for OODD detection. Our final contribution is to evaluate the RIQN
approach on the benchmarks to provide baseline results for future comparison.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Danesh_M/0/1/0/all/0/1"&gt;Mohamad H Danesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fern_A/0/1/0/all/0/1"&gt;Alan Fern&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual Training of Energy-Based Models with Overparametrized Shallow Neural Networks. (arXiv:2107.05134v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05134</id>
        <link href="http://arxiv.org/abs/2107.05134"/>
        <updated>2021-07-13T01:59:36.927Z</updated>
        <summary type="html"><![CDATA[Energy-based models (EBMs) are generative models that are usually trained via
maximum likelihood estimation. This approach becomes challenging in generic
situations where the trained energy is nonconvex, due to the need to sample the
Gibbs distribution associated with this energy. Using general Fenchel duality
results, we derive variational principles dual to maximum likelihood EBMs with
shallow overparametrized neural network energies, both in the active (aka
feature-learning) and lazy regimes. In the active regime, this dual formulation
leads to a training algorithm in which one updates concurrently the particles
in the sample space and the neurons in the parameter space of the energy. We
also consider a variant of this algorithm in which the particles are sometimes
restarted at random samples drawn from the data set, and show that performing
these restarts at every iteration step corresponds to score matching training.
Using intermediate parameter setups in our dual algorithm thereby gives a way
to interpolate between maximum likelihood and score matching training. These
results are illustrated in simple numerical experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Domingo_Enrich_C/0/1/0/all/0/1"&gt;Carles Domingo-Enrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bietti_A/0/1/0/all/0/1"&gt;Alberto Bietti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gabrie_M/0/1/0/all/0/1"&gt;Marylou Gabri&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1"&gt;Joan Bruna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vanden_Eijnden_E/0/1/0/all/0/1"&gt;Eric Vanden-Eijnden&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anatomy of Domain Shift Impact on U-Net Layers in MRI Segmentation. (arXiv:2107.04914v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04914</id>
        <link href="http://arxiv.org/abs/2107.04914"/>
        <updated>2021-07-13T01:59:36.921Z</updated>
        <summary type="html"><![CDATA[Domain Adaptation (DA) methods are widely used in medical image segmentation
tasks to tackle the problem of differently distributed train (source) and test
(target) data. We consider the supervised DA task with a limited number of
annotated samples from the target domain. It corresponds to one of the most
relevant clinical setups: building a sufficiently accurate model on the minimum
possible amount of annotated data. Existing methods mostly fine-tune specific
layers of the pretrained Convolutional Neural Network (CNN). However, there is
no consensus on which layers are better to fine-tune, e.g. the first layers for
images with low-level domain shift or the deeper layers for images with
high-level domain shift. To this end, we propose SpotTUnet - a CNN architecture
that automatically chooses the layers which should be optimally fine-tuned.
More specifically, on the target domain, our method additionally learns the
policy that indicates whether a specific layer should be fine-tuned or reused
from the pretrained network. We show that our method performs at the same level
as the best of the nonflexible fine-tuning methods even under the extreme
scarcity of annotated data. Secondly, we show that SpotTUnet policy provides a
layer-wise visualization of the domain shift impact on the network, which could
be further used to develop robust domain generalization methods. In order to
extensively evaluate SpotTUnet performance, we use a publicly available dataset
of brain MR images (CC359), characterized by explicit domain shift. We release
a reproducible experimental pipeline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zakazov_I/0/1/0/all/0/1"&gt;Ivan Zakazov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shirokikh_B/0/1/0/all/0/1"&gt;Boris Shirokikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chernyavskiy_A/0/1/0/all/0/1"&gt;Alexey Chernyavskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belyaev_M/0/1/0/all/0/1"&gt;Mikhail Belyaev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel Mean Estimation by Marginalized Corrupted Distributions. (arXiv:2107.04855v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04855</id>
        <link href="http://arxiv.org/abs/2107.04855"/>
        <updated>2021-07-13T01:59:36.915Z</updated>
        <summary type="html"><![CDATA[Estimating the kernel mean in a reproducing kernel Hilbert space is a
critical component in many kernel learning algorithms. Given a finite sample,
the standard estimate of the target kernel mean is the empirical average.
Previous works have shown that better estimators can be constructed by
shrinkage methods. In this work, we propose to corrupt data examples with noise
from known distributions and present a new kernel mean estimator, called the
marginalized kernel mean estimator, which estimates kernel mean under the
corrupted distribution. Theoretically, we show that the marginalized kernel
mean estimator introduces implicit regularization in kernel mean estimation.
Empirically, we show on a variety of datasets that the marginalized kernel mean
estimator obtains much lower estimation error than the existing estimators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1"&gt;Xiaobo Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1"&gt;Shuo Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1"&gt;Mingming Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1"&gt;Nannan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1"&gt;Fei Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1"&gt;Haikun Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MugRep: A Multi-Task Hierarchical Graph Representation Learning Framework for Real Estate Appraisal. (arXiv:2107.05180v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05180</id>
        <link href="http://arxiv.org/abs/2107.05180"/>
        <updated>2021-07-13T01:59:36.898Z</updated>
        <summary type="html"><![CDATA[Real estate appraisal refers to the process of developing an unbiased opinion
for real property's market value, which plays a vital role in decision-making
for various players in the marketplace (e.g., real estate agents, appraisers,
lenders, and buyers). However, it is a nontrivial task for accurate real estate
appraisal because of three major challenges: (1) The complicated influencing
factors for property value; (2) The asynchronously spatiotemporal dependencies
among real estate transactions; (3) The diversified correlations between
residential communities. To this end, we propose a Multi-Task Hierarchical
Graph Representation Learning (MugRep) framework for accurate real estate
appraisal. Specifically, by acquiring and integrating multi-source urban data,
we first construct a rich feature set to comprehensively profile the real
estate from multiple perspectives (e.g., geographical distribution, human
mobility distribution, and resident demographics distribution). Then, an
evolving real estate transaction graph and a corresponding event graph
convolution module are proposed to incorporate asynchronously spatiotemporal
dependencies among real estate transactions. Moreover, to further incorporate
valuable knowledge from the view of residential communities, we devise a
hierarchical heterogeneous community graph convolution module to capture
diversified correlations between residential communities. Finally, an urban
district partitioned multi-task learning module is introduced to generate
differently distributed value opinions for real estate. Extensive experiments
on two real-world datasets demonstrate the effectiveness of MugRep and its
components and features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weijia Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_L/0/1/0/all/0/1"&gt;Lijun Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hengshu Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Ji Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1"&gt;Dejing Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Hui Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SE-PSNet: Silhouette-based Enhancement Feature for Panoptic Segmentation Network. (arXiv:2107.05093v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05093</id>
        <link href="http://arxiv.org/abs/2107.05093"/>
        <updated>2021-07-13T01:59:36.891Z</updated>
        <summary type="html"><![CDATA[Recently, there has been a panoptic segmentation task combining semantic and
instance segmentation, in which the goal is to classify each pixel with the
corresponding instance ID. In this work, we propose a solution to tackle the
panoptic segmentation task. The overall structure combines the bottom-up method
and the top-down method. Therefore, not only can there be better performance,
but also the execution speed can be maintained. The network mainly pays
attention to the quality of the mask. In the previous work, we can see that the
uneven contour of the object is more likely to appear, resulting in low-quality
prediction. Accordingly, we propose enhancement features and corresponding loss
functions for the silhouette of objects and backgrounds to improve the mask.
Meanwhile, we use the new proposed confidence score to solve the occlusion
problem and make the network tend to use higher quality masks as prediction
results. To verify our research, we used the COCO dataset and CityScapes
dataset to do experiments and obtained competitive results with fast inference
time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Shuo-En Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi-Cheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1"&gt;En-Ting Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsiao_P/0/1/0/all/0/1"&gt;Pei-Yung Hsiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1"&gt;Li-Chen Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual Optimization for Kolmogorov Model Learning Using Enhanced Gradient Descent. (arXiv:2107.05011v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05011</id>
        <link href="http://arxiv.org/abs/2107.05011"/>
        <updated>2021-07-13T01:59:36.873Z</updated>
        <summary type="html"><![CDATA[Data representation techniques have made a substantial contribution to
advancing data processing and machine learning (ML). Improving predictive power
was the focus of previous representation techniques, which unfortunately
perform rather poorly on the interpretability in terms of extracting underlying
insights of the data. Recently, Kolmogorov model (KM) was studied, which is an
interpretable and predictable representation approach to learning the
underlying probabilistic structure of a set of random variables. The existing
KM learning algorithms using semi-definite relaxation with randomization
(SDRwR) or discrete monotonic optimization (DMO) have, however, limited utility
to big data applications because they do not scale well computationally. In
this paper, we propose a computationally scalable KM learning algorithm, based
on the regularized dual optimization combined with enhanced gradient descent
(GD) method. To make our method more scalable to large-dimensional problems, we
propose two acceleration schemes, namely, eigenvalue decomposition (EVD)
elimination strategy and proximal EVD algorithm. Furthermore, a thresholding
technique by exploiting the approximation error analysis and leveraging the
normalized Minkowski $\ell_1$-norm and its bounds, is provided for the
selection of the number of iterations of the proximal EVD algorithm. When
applied to big data applications, it is demonstrated that the proposed method
can achieve compatible training/prediction performance with significantly
reduced computational complexity; roughly two orders of magnitude improvement
in terms of the time overhead, compared to the existing KM learning algorithms.
Furthermore, it is shown that the accuracy of logical relation mining for
interpretability by using the proposed KM learning algorithm exceeds $80\%$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duan_Q/0/1/0/all/0/1"&gt;Qiyou Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghauch_H/0/1/0/all/0/1"&gt;Hadi Ghauch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1"&gt;Taejoon Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Topological-Framework to Improve Analysis of Machine Learning Model Performance. (arXiv:2107.04714v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04714</id>
        <link href="http://arxiv.org/abs/2107.04714"/>
        <updated>2021-07-13T01:59:36.868Z</updated>
        <summary type="html"><![CDATA[As both machine learning models and the datasets on which they are evaluated
have grown in size and complexity, the practice of using a few summary
statistics to understand model performance has become increasingly problematic.
This is particularly true in real-world scenarios where understanding model
failure on certain subpopulations of the data is of critical importance. In
this paper we propose a topological framework for evaluating machine learning
models in which a dataset is treated as a "space" on which a model operates.
This provides us with a principled way to organize information about model
performance at both the global level (over the entire test set) and also the
local level (on specific subpopulations). Finally, we describe a topological
data structure, presheaves, which offer a convenient way to store and analyze
model performance between different subpopulations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1"&gt;Henry Kvinge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wight_C/0/1/0/all/0/1"&gt;Colby Wight&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akers_S/0/1/0/all/0/1"&gt;Sarah Akers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Howland_S/0/1/0/all/0/1"&gt;Scott Howland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_W/0/1/0/all/0/1"&gt;Woongjo Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xiaolong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gosink_L/0/1/0/all/0/1"&gt;Luke Gosink&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jurrus_E/0/1/0/all/0/1"&gt;Elizabeth Jurrus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kappagantula_K/0/1/0/all/0/1"&gt;Keerti Kappagantula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Emerson_T/0/1/0/all/0/1"&gt;Tegan H. Emerson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MOOCRep: A Unified Pre-trained Embedding of MOOC Entities. (arXiv:2107.05154v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05154</id>
        <link href="http://arxiv.org/abs/2107.05154"/>
        <updated>2021-07-13T01:59:36.861Z</updated>
        <summary type="html"><![CDATA[Many machine learning models have been built to tackle information overload
issues on Massive Open Online Courses (MOOC) platforms. These models rely on
learning powerful representations of MOOC entities. However, they suffer from
the problem of scarce expert label data. To overcome this problem, we propose
to learn pre-trained representations of MOOC entities using abundant unlabeled
data from the structure of MOOCs which can directly be applied to the
downstream tasks. While existing pre-training methods have been successful in
NLP areas as they learn powerful textual representation, their models do not
leverage the richer information about MOOC entities. This richer information
includes the graph relationship between the lectures, concepts, and courses
along with the domain knowledge about the complexity of a concept. We develop
MOOCRep, a novel method based on Transformer language model trained with two
pre-training objectives : 1) graph-based objective to capture the powerful
signal of entities and relations that exist in the graph, and 2)
domain-oriented objective to effectively incorporate the complexity level of
concepts. Our experiments reveal that MOOCRep's embeddings outperform
state-of-the-art representation learning methods on two tasks important for
education community, concept pre-requisite prediction and lecture
recommendation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_S/0/1/0/all/0/1"&gt;Shalini Pandey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_J/0/1/0/all/0/1"&gt;Jaideep Srivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Propagation-aware Social Recommendation by Transfer Learning. (arXiv:2107.04846v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.04846</id>
        <link href="http://arxiv.org/abs/2107.04846"/>
        <updated>2021-07-13T01:59:36.843Z</updated>
        <summary type="html"><![CDATA[Social-aware recommendation approaches have been recognized as an effective
way to solve the data sparsity issue of traditional recommender systems. The
assumption behind is that the knowledge in social user-user connections can be
shared and transferred to the domain of user-item interactions, whereby to help
learn user preferences. However, most existing approaches merely adopt the
first-order connections among users during transfer learning, ignoring those
connections in higher orders. We argue that better recommendation performance
can also benefit from high-order social relations. In this paper, we propose a
novel Propagation-aware Transfer Learning Network (PTLN) based on the
propagation of social relations. We aim to better mine the sharing knowledge
hidden in social networks and thus further improve recommendation performance.
Specifically, we explore social influence in two aspects: (a) higher-order
friends have been taken into consideration by order bias; (b) different friends
in the same order will have distinct importance for recommendation by an
attention mechanism. Besides, we design a novel regularization to bridge the
gap between social relations and user-item interactions. We conduct extensive
experiments on two real-world datasets and beat other counterparts in terms of
ranking accuracy, especially for the cold-start users with few historical
interactions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Haodong Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_Y/0/1/0/all/0/1"&gt;Yabo Chu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating stable molecules using imitation and reinforcement learning. (arXiv:2107.05007v1 [physics.chem-ph])]]></title>
        <id>http://arxiv.org/abs/2107.05007</id>
        <link href="http://arxiv.org/abs/2107.05007"/>
        <updated>2021-07-13T01:59:36.837Z</updated>
        <summary type="html"><![CDATA[Chemical space is routinely explored by machine learning methods to discover
interesting molecules, before time-consuming experimental synthesizing is
attempted. However, these methods often rely on a graph representation,
ignoring 3D information necessary for determining the stability of the
molecules. We propose a reinforcement learning approach for generating
molecules in cartesian coordinates allowing for quantum chemical prediction of
the stability. To improve sample-efficiency we learn basic chemical rules from
imitation learning on the GDB-11 database to create an initial model applicable
for all stoichiometries. We then deploy multiple copies of the model
conditioned on a specific stoichiometry in a reinforcement learning setting.
The models correctly identify low energy molecules in the database and produce
novel isomers not found in the training set. Finally, we apply the model to
larger molecules to show how reinforcement learning further refines the
imitation learning model in domains far from the training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Meldgaard_S/0/1/0/all/0/1"&gt;S&amp;#xf8;ren Ager Meldgaard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kohler_J/0/1/0/all/0/1"&gt;Jonas K&amp;#xf6;hler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Mortensen_H/0/1/0/all/0/1"&gt;Henrik Lund Mortensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Christiansen_M/0/1/0/all/0/1"&gt;Mads-Peter V. Christiansen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Noe_F/0/1/0/all/0/1"&gt;Frank No&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Hammer_B/0/1/0/all/0/1"&gt;Bj&amp;#xf8;rk Hammer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Inductive Link Prediction Using Hyper-Relational Facts. (arXiv:2107.04894v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04894</id>
        <link href="http://arxiv.org/abs/2107.04894"/>
        <updated>2021-07-13T01:59:36.831Z</updated>
        <summary type="html"><![CDATA[For many years, link prediction on knowledge graphs (KGs) has been a purely
transductive task, not allowing for reasoning on unseen entities. Recently,
increasing efforts are put into exploring semi- and fully inductive scenarios,
enabling inference over unseen and emerging entities. Still, all these
approaches only consider triple-based \glspl{kg}, whereas their richer
counterparts, hyper-relational KGs (e.g., Wikidata), have not yet been properly
studied. In this work, we classify different inductive settings and study the
benefits of employing hyper-relational KGs on a wide range of semi- and fully
inductive link prediction tasks powered by recent advancements in graph neural
networks. Our experiments on a novel set of benchmarks show that qualifiers
over typed edges can lead to performance improvements of 6% of absolute gains
(for the Hits@10 metric) compared to triple-only baselines. Our code is
available at \url{https://github.com/mali-git/hyper_relational_ilp}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1"&gt;Mehdi Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berrendorf_M/0/1/0/all/0/1"&gt;Max Berrendorf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galkin_M/0/1/0/all/0/1"&gt;Mikhail Galkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thost_V/0/1/0/all/0/1"&gt;Veronika Thost&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengfei Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1"&gt;Volker Tresp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1"&gt;Jens Lehmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReconVAT: A Semi-Supervised Automatic Music Transcription Framework for Low-Resource Real-World Data. (arXiv:2107.04954v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.04954</id>
        <link href="http://arxiv.org/abs/2107.04954"/>
        <updated>2021-07-13T01:59:36.826Z</updated>
        <summary type="html"><![CDATA[Most of the current supervised automatic music transcription (AMT) models
lack the ability to generalize. This means that they have trouble transcribing
real-world music recordings from diverse musical genres that are not presented
in the labelled training data. In this paper, we propose a semi-supervised
framework, ReconVAT, which solves this issue by leveraging the huge amount of
available unlabelled music recordings. The proposed ReconVAT uses
reconstruction loss and virtual adversarial training. When combined with
existing U-net models for AMT, ReconVAT achieves competitive results on common
benchmark datasets such as MAPS and MusicNet. For example, in the few-shot
setting for the string part version of MusicNet, ReconVAT achieves F1-scores of
61.0% and 41.6% for the note-wise and note-with-offset-wise metrics
respectively, which translates into an improvement of 22.2% and 62.5% compared
to the supervised baseline model. Our proposed framework also demonstrates the
potential of continual learning on new data, which could be useful in
real-world applications whereby new data is constantly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheuk_K/0/1/0/all/0/1"&gt;Kin Wai Cheuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herremans_D/0/1/0/all/0/1"&gt;Dorien Herremans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1"&gt;Li Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformers with multi-modal features and post-fusion context for e-commerce session-based recommendation. (arXiv:2107.05124v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.05124</id>
        <link href="http://arxiv.org/abs/2107.05124"/>
        <updated>2021-07-13T01:59:36.819Z</updated>
        <summary type="html"><![CDATA[Session-based recommendation is an important task for e-commerce services,
where a large number of users browse anonymously or may have very distinct
interests for different sessions. In this paper we present one of the winning
solutions for the Recommendation task of the SIGIR 2021 Workshop on E-commerce
Data Challenge. Our solution was inspired by NLP techniques and consists of an
ensemble of two Transformer architectures - Transformer-XL and XLNet - trained
with autoregressive and autoencoding approaches. To leverage most of the rich
dataset made available for the competition, we describe how we prepared
multi-model features by combining tabular events with textual and image
vectors. We also present a model prediction analysis to better understand the
effectiveness of our architectures for the session-based recommendation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moreira_G/0/1/0/all/0/1"&gt;Gabriel de Souza P. Moreira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabhi_S/0/1/0/all/0/1"&gt;Sara Rabhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ak_R/0/1/0/all/0/1"&gt;Ronay Ak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1"&gt;Md Yasin Kabir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oldridge_E/0/1/0/all/0/1"&gt;Even Oldridge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convergence Analysis of Schr{\"o}dinger-F{\"o}llmer Sampler without Convexity. (arXiv:2107.04766v1 [stat.CO])]]></title>
        <id>http://arxiv.org/abs/2107.04766</id>
        <link href="http://arxiv.org/abs/2107.04766"/>
        <updated>2021-07-13T01:59:36.803Z</updated>
        <summary type="html"><![CDATA[Schr\"{o}dinger-F\"{o}llmer sampler (SFS) is a novel and efficient approach
for sampling from possibly unnormalized distributions without ergodicity. SFS
is based on the Euler-Maruyama discretization of Schr\"{o}dinger-F\"{o}llmer
diffusion process $$\mathrm{d} X_{t}=-\nabla U\left(X_t, t\right) \mathrm{d}
t+\mathrm{d} B_{t}, \quad t \in[0,1],\quad X_0=0$$ on the unit interval, which
transports the degenerate distribution at time zero to the target distribution
at time one. In \cite{sfs21}, the consistency of SFS is established under a
restricted assumption that %the drift term $b(x,t)$ the potential $U(x,t)$ is
uniformly (on $t$) strongly %concave convex (on $x$). In this paper we provide
a nonasymptotic error bound of SFS in Wasserstein distance under some smooth
and bounded conditions on the density ratio of the target distribution over the
standard normal distribution, but without requiring the strongly convexity of
the potential.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Jiao_Y/0/1/0/all/0/1"&gt;Yuling Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kang_L/0/1/0/all/0/1"&gt;Lican Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yanyan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Youzhou Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectro-Temporal RF Identification using Deep Learning. (arXiv:2107.05114v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2107.05114</id>
        <link href="http://arxiv.org/abs/2107.05114"/>
        <updated>2021-07-13T01:59:36.796Z</updated>
        <summary type="html"><![CDATA[RF emissions detection, classification, and spectro-temporal localization are
crucial not only for tasks relating to understanding, managing, and protecting
the RF spectrum, but also for safety and security applications such as
detecting intruding drones or jammers. Achieving this goal for wideband
spectrum and in real-time performance is a challenging problem. We present
WRIST, a Wideband, Real-time RF Identification system with Spectro-Temporal
detection, framework and system. Our resulting deep learning model is capable
to detect, classify, and precisely locate RF emissions in time and frequency
using RF samples of 100 MHz spectrum in real-time (over 6Gbps incoming I&Q
streams). Such capabilities are made feasible by leveraging a deep-learning
based one-stage object detection framework, and transfer learning to a
multi-channel image-based RF signals representation. We also introduce an
iterative training approach which leverages synthesized and augmented RF data
to efficiently build large labelled datasets of RF emissions (SPREAD). WRIST
detector achieves 90 mean Average Precision even in extremely congested
environment in the wild. WRIST model classifies five technologies (Bluetooth,
Lightbridge, Wi-Fi, XPD, and ZigBee) and is easily extendable to others. We are
making our curated and annotated dataset available to the whole community. It
consists of nearly 1 million fully labelled RF emissions collected from various
off-the-shelf wireless radios in a range of environments and spanning the five
classes of emissions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Hai N. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vomvas_M/0/1/0/all/0/1"&gt;Marinos Vomvas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vo_Huu_T/0/1/0/all/0/1"&gt;Triet Vo-Huu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noubir_G/0/1/0/all/0/1"&gt;Guevara Noubir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cluster Regularization via a Hierarchical Feature Regression. (arXiv:2107.04831v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.04831</id>
        <link href="http://arxiv.org/abs/2107.04831"/>
        <updated>2021-07-13T01:59:36.790Z</updated>
        <summary type="html"><![CDATA[Prediction tasks with high-dimensional nonorthogonal predictor sets pose a
challenge for least squares based fitting procedures. A large and productive
literature exists, discussing various regularized approaches to improving the
out-of-sample robustness of parameter estimates. This paper proposes a novel
cluster-based regularization - the hierarchical feature regression (HFR) -,
which mobilizes insights from the domains of machine learning and graph theory
to estimate parameters along a supervised hierarchical representation of the
predictor set, shrinking parameters towards group targets. The method is
innovative in its ability to estimate optimal compositions of predictor groups,
as well as the group targets endogenously. The HFR can be viewed as a
supervised factor regression, with the strength of shrinkage governed by a
penalty on the extent of idiosyncratic variation captured in the fitting
process. The method demonstrates good predictive accuracy and versatility,
outperforming a panel of benchmark regularized estimators across a diverse set
of simulated regression tasks, including dense, sparse and grouped data
generating processes. An application to the prediction of economic growth is
used to illustrate the HFR's effectiveness in an empirical setting, with
favorable comparisons to several frequentist and Bayesian alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Pfitzinger_J/0/1/0/all/0/1"&gt;Johann Pfitzinger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from Crowds with Sparse and Imbalanced Annotations. (arXiv:2107.05039v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05039</id>
        <link href="http://arxiv.org/abs/2107.05039"/>
        <updated>2021-07-13T01:59:36.783Z</updated>
        <summary type="html"><![CDATA[Traditional supervised learning requires ground truth labels for the training
data, whose collection can be difficult in many cases. Recently, crowdsourcing
has established itself as an efficient labeling solution through resorting to
non-expert crowds. To reduce the labeling error effects, one common practice is
to distribute each instance to multiple workers, whereas each worker only
annotates a subset of data, resulting in the {\it sparse annotation}
phenomenon. In this paper, we note that when meeting with class-imbalance,
i.e., when the ground truth labels are {\it class-imbalanced}, the sparse
annotations are prone to be skewly distributed, which thus can severely bias
the learning algorithm. To combat this issue, we propose one self-training
based approach named {\it Self-Crowd} by progressively adding confident
pseudo-annotations and rebalancing the annotation distribution. Specifically,
we propose one distribution aware confidence measure to select confident
pseudo-annotations, which adopts the resampling strategy to oversample the
minority annotations and undersample the majority annotations. On one
real-world crowdsourcing image classification task, we show that the proposed
method yields more balanced annotations throughout training than the
distribution agnostic methods and substantially improves the learning
performance at different annotation sparsity levels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Ye Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shao-Yuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Sheng-Jun Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Waveshaping Synthesis. (arXiv:2107.05050v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.05050</id>
        <link href="http://arxiv.org/abs/2107.05050"/>
        <updated>2021-07-13T01:59:36.777Z</updated>
        <summary type="html"><![CDATA[We present the Neural Waveshaping Unit (NEWT): a novel, lightweight, fully
causal approach to neural audio synthesis which operates directly in the
waveform domain, with an accompanying optimisation (FastNEWT) for efficient CPU
inference. The NEWT uses time-distributed multilayer perceptrons with periodic
activations to implicitly learn nonlinear transfer functions that encode the
characteristics of a target timbre. Once trained, a NEWT can produce complex
timbral evolutions by simple affine transformations of its input and output
signals. We paired the NEWT with a differentiable noise synthesiser and reverb
and found it capable of generating realistic musical instrument performances
with only 260k total model parameters, conditioned on F0 and loudness features.
We compared our method to state-of-the-art benchmarks with a multi-stimulus
listening test and the Fr\'echet Audio Distance and found it performed
competitively across the tested timbral domains. Our method significantly
outperformed the benchmarks in terms of generation speed, and achieved
real-time performance on a consumer CPU, both with and without FastNEWT,
suggesting it is a viable basis for future creative sound design tools.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hayes_B/0/1/0/all/0/1"&gt;Ben Hayes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saitis_C/0/1/0/all/0/1"&gt;Charalampos Saitis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1"&gt;Gy&amp;#xf6;rgy Fazekas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-linear Visual Knowledge Discovery with Elliptic Paired Coordinates. (arXiv:2107.04974v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04974</id>
        <link href="http://arxiv.org/abs/2107.04974"/>
        <updated>2021-07-13T01:59:36.771Z</updated>
        <summary type="html"><![CDATA[It is challenging for humans to enable visual knowledge discovery in data
with more than 2-3 dimensions with a naked eye. This chapter explores the
efficiency of discovering predictive machine learning models interactively
using new Elliptic Paired coordinates (EPC) visualizations. It is shown that
EPC are capable to visualize multidimensional data and support visual machine
learning with preservation of multidimensional information in 2-D. Relative to
parallel and radial coordinates, EPC visualization requires only a half of the
visual elements for each n-D point. An interactive software system EllipseVis,
which is developed in this work, processes high-dimensional datasets, creates
EPC visualizations, and produces predictive classification models by
discovering dominance rules in EPC. By using interactive and automatic
processes it discovers zones in EPC with a high dominance of a single class.
The EPC methodology has been successful in discovering non-linear predictive
models with high coverage and precision in the computational experiments. This
can benefit multiple domains by producing visually appealing dominance rules.
This chapter presents results of successful testing the EPC non-linear
methodology in experiments using real and simulated data, EPC generalized to
the Dynamic Elliptic Paired Coordinates (DEPC), incorporation of the weights of
coordinates to optimize the visual discovery, introduction of an alternative
EPC design and introduction of the concept of incompact machine learning
methodology based on EPC/DEPC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McDonald_R/0/1/0/all/0/1"&gt;Rose McDonald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1"&gt;Boris Kovalerchuk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning based CVD Virtual Metrology in Mass Produced Semiconductor Process. (arXiv:2107.05071v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05071</id>
        <link href="http://arxiv.org/abs/2107.05071"/>
        <updated>2021-07-13T01:59:36.754Z</updated>
        <summary type="html"><![CDATA[A cross-benchmark has been done on three critical aspects, data imputing,
feature selection and regression algorithms, for machine learning based
chemical vapor deposition (CVD) virtual metrology (VM). The result reveals that
linear feature selection regression algorithm would extensively under-fit the
VM data. Data imputing is also necessary to achieve a higher prediction
accuracy as the data availability is only ~70% when optimal accuracy is
obtained. This work suggests a nonlinear feature selection and regression
algorithm combined with nearest data imputing algorithm would provide a
prediction accuracy as high as 0.7. This would lead to 70% reduced CVD
processing variation, which is believed to will lead to reduced frequency of
physical metrology as well as more reliable mass-produced wafer with improved
quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yunsong Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stearrett_R/0/1/0/all/0/1"&gt;Ryan Stearrett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Blending Pruning Criteria for Convolutional Neural Networks. (arXiv:2107.05033v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05033</id>
        <link href="http://arxiv.org/abs/2107.05033"/>
        <updated>2021-07-13T01:59:36.746Z</updated>
        <summary type="html"><![CDATA[The advancement of convolutional neural networks (CNNs) on various vision
applications has attracted lots of attention. Yet the majority of CNNs are
unable to satisfy the strict requirement for real-world deployment. To overcome
this, the recent popular network pruning is an effective method to reduce the
redundancy of the models. However, the ranking of filters according to their
"importance" on different pruning criteria may be inconsistent. One filter
could be important according to a certain criterion, while it is unnecessary
according to another one, which indicates that each criterion is only a partial
view of the comprehensive "importance". From this motivation, we propose a
novel framework to integrate the existing filter pruning criteria by exploring
the criteria diversity. The proposed framework contains two stages: Criteria
Clustering and Filters Importance Calibration. First, we condense the pruning
criteria via layerwise clustering based on the rank of "importance" score.
Second, within each cluster, we propose a calibration factor to adjust their
significance for each selected blending candidates and search for the optimal
blending criterion via Evolutionary Algorithm. Quantitative results on the
CIFAR-100 and ImageNet benchmarks show that our framework outperforms the
state-of-the-art baselines, regrading to the compact model performance after
pruning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1"&gt;Wei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhongzhan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1"&gt;Mingfu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1"&gt;Senwei Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haizhao Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[QoS Prediction for 5G Connected and Automated Driving. (arXiv:2107.05000v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2107.05000</id>
        <link href="http://arxiv.org/abs/2107.05000"/>
        <updated>2021-07-13T01:59:36.707Z</updated>
        <summary type="html"><![CDATA[5G communication system can support the demanding quality-of-service (QoS)
requirements of many advanced vehicle-to-everything (V2X) use cases. However,
the safe and efficient driving, especially of automated vehicles, may be
affected by sudden changes of the provided QoS. For that reason, the prediction
of the QoS changes and the early notification of these predicted changes to the
vehicles have been recently enabled by 5G communication systems. This solution
enables the vehicles to avoid or mitigate the effect of sudden QoS changes at
the application level. This article describes how QoS prediction could be
generated by a 5G communication system and delivered to a V2X application. The
tele-operated driving use case is used as an example to analyze the feasibility
of a QoS prediction scheme. Useful recommendations for the development of a QoS
prediction solution are provided, while open research topics are identified.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kousaridas_A/0/1/0/all/0/1"&gt;Apostolos Kousaridas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manjunath_R/0/1/0/all/0/1"&gt;Ramya Panthangi Manjunath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perdomo_J/0/1/0/all/0/1"&gt;Jose Mauricio Perdomo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zielinski_E/0/1/0/all/0/1"&gt;Ernst Zielinski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmitz_S/0/1/0/all/0/1"&gt;Steffen Schmitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfadler_A/0/1/0/all/0/1"&gt;Andreas Pfadler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SGD: The Role of Implicit Regularization, Batch-size and Multiple-epochs. (arXiv:2107.05074v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05074</id>
        <link href="http://arxiv.org/abs/2107.05074"/>
        <updated>2021-07-13T01:59:36.660Z</updated>
        <summary type="html"><![CDATA[Multi-epoch, small-batch, Stochastic Gradient Descent (SGD) has been the
method of choice for learning with large over-parameterized models. A popular
theory for explaining why SGD works well in practice is that the algorithm has
an implicit regularization that biases its output towards a good solution.
Perhaps the theoretically most well understood learning setting for SGD is that
of Stochastic Convex Optimization (SCO), where it is well known that SGD learns
at a rate of $O(1/\sqrt{n})$, where $n$ is the number of samples. In this
paper, we consider the problem of SCO and explore the role of implicit
regularization, batch size and multiple epochs for SGD. Our main contributions
are threefold:

(a) We show that for any regularizer, there is an SCO problem for which
Regularized Empirical Risk Minimzation fails to learn. This automatically rules
out any implicit regularization based explanation for the success of SGD.

(b) We provide a separation between SGD and learning via Gradient Descent on
empirical loss (GD) in terms of sample complexity. We show that there is an SCO
problem such that GD with any step size and number of iterations can only learn
at a suboptimal rate: at least $\widetilde{\Omega}(1/n^{5/12})$.

(c) We present a multi-epoch variant of SGD commonly used in practice. We
prove that this algorithm is at least as good as single pass SGD in the worst
case. However, for certain SCO problems, taking multiple passes over the
dataset can significantly outperform single pass SGD.

We extend our results to the general learning setting by showing a problem
which is learnable for any data distribution, and for this problem, SGD is
strictly better than RERM for any regularization function. We conclude by
discussing the implications of our results for deep learning, and show a
separation between SGD and ERM for two layer diagonal neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kale_S/0/1/0/all/0/1"&gt;Satyen Kale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sekhari_A/0/1/0/all/0/1"&gt;Ayush Sekhari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sridharan_K/0/1/0/all/0/1"&gt;Karthik Sridharan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is a Single Model Enough? MuCoS: A Multi-Model Ensemble Learning for Semantic Code Search. (arXiv:2107.04773v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2107.04773</id>
        <link href="http://arxiv.org/abs/2107.04773"/>
        <updated>2021-07-13T01:59:36.653Z</updated>
        <summary type="html"><![CDATA[Recently, deep learning methods have become mainstream in code search since
they do better at capturing semantic correlations between code snippets and
search queries and have promising performance. However, code snippets have
diverse information from different dimensions, such as business logic, specific
algorithm, and hardware communication, so it is hard for a single code
representation module to cover all the perspectives. On the other hand, as a
specific query may focus on one or several perspectives, it is difficult for a
single query representation module to represent different user intents. In this
paper, we propose MuCoS, a multi-model ensemble learning architecture for
semantic code search. It combines several individual learners, each of which
emphasizes a specific perspective of code snippets. We train the individual
learners on different datasets which contain different perspectives of code
information, and we use a data augmentation strategy to get these different
datasets. Then we ensemble the learners to capture comprehensive features of
code snippets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1"&gt;Lun Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1"&gt;Xiaozhou Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanlin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_E/0/1/0/all/0/1"&gt;Ensheng Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Shi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongmei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep-Bayesian Framework for Adaptive Speech Duration Modification. (arXiv:2107.04973v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.04973</id>
        <link href="http://arxiv.org/abs/2107.04973"/>
        <updated>2021-07-13T01:59:36.645Z</updated>
        <summary type="html"><![CDATA[We propose the first method to adaptively modify the duration of a given
speech signal. Our approach uses a Bayesian framework to define a latent
attention map that links frames of the input and target utterances. We train a
masked convolutional encoder-decoder network to produce this attention map via
a stochastic version of the mean absolute error loss function; our model also
predicts the length of the target speech signal using the encoder embeddings.
The predicted length determines the number of steps for the decoder operation.
During inference, we generate the attention map as a proxy for the similarity
matrix between the given input speech and an unknown target speech signal.
Using this similarity matrix, we compute a warping path of alignment between
the two signals. Our experiments demonstrate that this adaptive framework
produces similar results to dynamic time warping, which relies on a known
target signal, on both voice conversion and emotion conversion tasks. We also
show that our technique results in a high quality of generated speech that is
on par with state-of-the-art vocoders.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shankar_R/0/1/0/all/0/1"&gt;Ravi Shankar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Venkataraman_A/0/1/0/all/0/1"&gt;Archana Venkataraman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coordinate-wise Control Variates for Deep Policy Gradients. (arXiv:2107.04987v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04987</id>
        <link href="http://arxiv.org/abs/2107.04987"/>
        <updated>2021-07-13T01:59:36.639Z</updated>
        <summary type="html"><![CDATA[The control variates (CV) method is widely used in policy gradient estimation
to reduce the variance of the gradient estimators in practice. A control
variate is applied by subtracting a baseline function from the state-action
value estimates. Then the variance-reduced policy gradient presumably leads to
higher learning efficiency. Recent research on control variates with deep
neural net policies mainly focuses on scalar-valued baseline functions. The
effect of vector-valued baselines is under-explored. This paper investigates
variance reduction with coordinate-wise and layer-wise control variates
constructed from vector-valued baselines for neural net policies. We present
experimental evidence suggesting that lower variance can be obtained with such
baselines than with the conventional scalar-valued baseline. We demonstrate how
to equip the popular Proximal Policy Optimization (PPO) algorithm with these
new control variates. We show that the resulting algorithm with proper
regularization can achieve higher sample efficiency than scalar control
variates in continuous control benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1"&gt;Yuanyi Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jian Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Efficiency and Accuracy of Causal Discovery Using a Hierarchical Wrapper. (arXiv:2107.05001v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.05001</id>
        <link href="http://arxiv.org/abs/2107.05001"/>
        <updated>2021-07-13T01:59:36.633Z</updated>
        <summary type="html"><![CDATA[Causal discovery from observational data is an important tool in many
branches of science. Under certain assumptions it allows scientists to explain
phenomena, predict, and make decisions. In the large sample limit, sound and
complete causal discovery algorithms have been previously introduced, where a
directed acyclic graph (DAG), or its equivalence class, representing causal
relations is searched. However, in real-world cases, only finite training data
is available, which limits the power of statistical tests used by these
algorithms, leading to errors in the inferred causal model. This is commonly
addressed by devising a strategy for using as few as possible statistical
tests. In this paper, we introduce such a strategy in the form of a recursive
wrapper for existing constraint-based causal discovery algorithms, which
preserves soundness and completeness. It recursively clusters the observed
variables using the normalized min-cut criterion from the outset, and uses a
baseline causal discovery algorithm during backtracking for learning local
sub-graphs. It then combines them and ensures completeness. By an ablation
study, using synthetic data, and by common real-world benchmarks, we
demonstrate that our approach requires significantly fewer statistical tests,
learns more accurate graphs, and requires shorter run-times than the baseline
algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Nisimov_S/0/1/0/all/0/1"&gt;Shami Nisimov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gurwicz_Y/0/1/0/all/0/1"&gt;Yaniv Gurwicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rohekar_R/0/1/0/all/0/1"&gt;Raanan Y. Rohekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Novik_G/0/1/0/all/0/1"&gt;Gal Novik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization. (arXiv:2107.04649v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04649</id>
        <link href="http://arxiv.org/abs/2107.04649"/>
        <updated>2021-07-13T01:59:36.627Z</updated>
        <summary type="html"><![CDATA[For machine learning systems to be reliable, we must understand their
performance in unseen, out-of-distribution environments. In this paper, we
empirically show that out-of-distribution performance is strongly correlated
with in-distribution performance for a wide range of models and distribution
shifts. Specifically, we demonstrate strong correlations between
in-distribution and out-of-distribution performance on variants of CIFAR-10 &
ImageNet, a synthetic pose estimation task derived from YCB objects, satellite
imagery classification in FMoW-WILDS, and wildlife classification in
iWildCam-WILDS. The strong correlations hold across model architectures,
hyperparameters, training set size, and training duration, and are more precise
than what is expected from existing domain adaptation theory. To complete the
picture, we also investigate cases where the correlation is weaker, for
instance some synthetic distribution shifts from CIFAR-10-C and the tissue
classification dataset Camelyon17-WILDS. Finally, we provide a candidate theory
based on a Gaussian data model that shows how changes in the data covariance
arising from distribution shift can affect the observed correlations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1"&gt;John Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taori_R/0/1/0/all/0/1"&gt;Rohan Taori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1"&gt;Aditi Raghunathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sagawa_S/0/1/0/all/0/1"&gt;Shiori Sagawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koh_P/0/1/0/all/0/1"&gt;Pang Wei Koh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1"&gt;Vaishaal Shankar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Percy Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carmon_Y/0/1/0/all/0/1"&gt;Yair Carmon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1"&gt;Ludwig Schmidt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction of concept lengths for fast concept learning in description logics. (arXiv:2107.04911v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04911</id>
        <link href="http://arxiv.org/abs/2107.04911"/>
        <updated>2021-07-13T01:59:36.620Z</updated>
        <summary type="html"><![CDATA[Concept learning approaches based on refinement operators explore partially
ordered solution spaces to compute concepts, which are used as binary
classification models for individuals. However, the refinement trees spanned by
these approaches can easily grow to millions of nodes for complex learning
problems. This leads to refinement-based approaches often failing to detect
optimal concepts efficiently. In this paper, we propose a supervised machine
learning approach for learning concept lengths, which allows predicting the
length of the target concept and therefore facilitates the reduction of the
search space during concept learning. To achieve this goal, we compare four
neural architectures and evaluate them on four benchmark knowledge
graphs--Carcinogenesis, Mutagenesis, Semantic Bible, Family Benchmark. Our
evaluation results suggest that recurrent neural network architectures perform
best at concept length prediction with an F-measure of up to 92%. We show that
integrating our concept length predictor into the CELOE (Class Expression
Learner for Ontology Engineering) algorithm improves CELOE's runtime by a
factor of up to 13.4 without any significant changes to the quality of the
results it generates. For reproducibility, we provide our implementation in the
public GitHub repository at
https://github.com/ConceptLengthLearner/ReproducibilityRepo]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kouagou_N/0/1/0/all/0/1"&gt;N&amp;#x27;Dah Jean Kouagou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heindorf_S/0/1/0/all/0/1"&gt;Stefan Heindorf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demir_C/0/1/0/all/0/1"&gt;Caglar Demir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ngomo_A/0/1/0/all/0/1"&gt;Axel-Cyrille Ngonga Ngomo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HOMRS: High Order Metamorphic Relations Selector for Deep Neural Networks. (arXiv:2107.04863v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04863</id>
        <link href="http://arxiv.org/abs/2107.04863"/>
        <updated>2021-07-13T01:59:36.603Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNN) applications are increasingly becoming a part of
our everyday life, from medical applications to autonomous cars. Traditional
validation of DNN relies on accuracy measures, however, the existence of
adversarial examples has highlighted the limitations of these accuracy
measures, raising concerns especially when DNN are integrated into
safety-critical systems. In this paper, we present HOMRS, an approach to boost
metamorphic testing by automatically building a small optimized set of high
order metamorphic relations from an initial set of elementary metamorphic
relations. HOMRS' backbone is a multi-objective search; it exploits ideas drawn
from traditional systems testing such as code coverage, test case, and path
diversity. We applied HOMRS to LeNet5 DNN with MNIST dataset and we report
evidence that it builds a small but effective set of high order transformations
achieving a 95% kill ratio. Five raters manually labeled a pool of images
before and after high order transformation; Fleiss' Kappa and statistical tests
confirmed that they are metamorphic properties. HOMRS built-in relations are
also effective to confront adversarial or out-of-distribution examples; HOMRS
detected 92% of randomly sampled out-of-distribution images. HOMRS
transformations are also suitable for online real-time use.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tambon_F/0/1/0/all/0/1"&gt;Florian Tambon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antoniol_G/0/1/0/all/0/1"&gt;Giulio Antoniol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1"&gt;Foutse Khomh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Low-pass Filtering: Graph Convolutional Networks with Automatic Filtering. (arXiv:2107.04755v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04755</id>
        <link href="http://arxiv.org/abs/2107.04755"/>
        <updated>2021-07-13T01:59:36.593Z</updated>
        <summary type="html"><![CDATA[Graph convolutional networks are becoming indispensable for deep learning
from graph-structured data. Most of the existing graph convolutional networks
share two big shortcomings. First, they are essentially low-pass filters, thus
the potentially useful middle and high frequency band of graph signals are
ignored. Second, the bandwidth of existing graph convolutional filters is
fixed. Parameters of a graph convolutional filter only transform the graph
inputs without changing the curvature of a graph convolutional filter function.
In reality, we are uncertain about whether we should retain or cut off the
frequency at a certain point unless we have expert domain knowledge. In this
paper, we propose Automatic Graph Convolutional Networks (AutoGCN) to capture
the full spectrum of graph signals and automatically update the bandwidth of
graph convolutional filters. While it is based on graph spectral theory, our
AutoGCN is also localized in space and has a spatial form. Experimental results
show that AutoGCN achieves significant improvement over baseline methods which
only work as low-pass filters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zonghan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Shirui Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1"&gt;Guodong Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jing Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chengqi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Agent Imitation Learning with Copulas. (arXiv:2107.04750v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04750</id>
        <link href="http://arxiv.org/abs/2107.04750"/>
        <updated>2021-07-13T01:59:36.585Z</updated>
        <summary type="html"><![CDATA[Multi-agent imitation learning aims to train multiple agents to perform tasks
from demonstrations by learning a mapping between observations and actions,
which is essential for understanding physical, social, and team-play systems.
However, most existing works on modeling multi-agent interactions typically
assume that agents make independent decisions based on their observations,
ignoring the complex dependence among agents. In this paper, we propose to use
copula, a powerful statistical tool for capturing dependence among random
variables, to explicitly model the correlation and coordination in multi-agent
systems. Our proposed model is able to separately learn marginals that capture
the local behavioral patterns of each individual agent, as well as a copula
function that solely and fully captures the dependence structure among agents.
Extensive experiments on synthetic and real-world datasets show that our model
outperforms state-of-the-art baselines across various scenarios in the action
prediction task, and is able to generate new trajectories close to expert
demonstrations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1"&gt;Lantao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1"&gt;Zhangjie Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1"&gt;Stefano Ermon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out of Distribution Detection and Adversarial Attacks on Deep Neural Networks for Robust Medical Image Analysis. (arXiv:2107.04882v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04882</id>
        <link href="http://arxiv.org/abs/2107.04882"/>
        <updated>2021-07-13T01:59:36.576Z</updated>
        <summary type="html"><![CDATA[Deep learning models have become a popular choice for medical image analysis.
However, the poor generalization performance of deep learning models limits
them from being deployed in the real world as robustness is critical for
medical applications. For instance, the state-of-the-art Convolutional Neural
Networks (CNNs) fail to detect adversarial samples or samples drawn
statistically far away from the training distribution. In this work, we
experimentally evaluate the robustness of a Mahalanobis distance-based
confidence score, a simple yet effective method for detecting abnormal input
samples, in classifying malaria parasitized cells and uninfected cells. Results
indicated that the Mahalanobis confidence score detector exhibits improved
performance and robustness of deep learning models, and achieves
stateof-the-art performance on both out-of-distribution (OOD) and adversarial
samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Uwimana1_A/0/1/0/all/0/1"&gt;Anisie Uwimana1&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Senanayake_R/0/1/0/all/0/1"&gt;Ransalu Senanayake&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LS3: Latent Space Safe Sets for Long-Horizon Visuomotor Control of Iterative Tasks. (arXiv:2107.04775v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04775</id>
        <link href="http://arxiv.org/abs/2107.04775"/>
        <updated>2021-07-13T01:59:36.558Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning (RL) algorithms have shown impressive success in
exploring high-dimensional environments to learn complex, long-horizon tasks,
but can often exhibit unsafe behaviors and require extensive environment
interaction when exploration is unconstrained. A promising strategy for safe
learning in dynamically uncertain environments is requiring that the agent can
robustly return to states where task success (and therefore safety) can be
guaranteed. While this approach has been successful in low-dimensions,
enforcing this constraint in environments with high-dimensional state spaces,
such as images, is challenging. We present Latent Space Safe Sets (LS3), which
extends this strategy to iterative, long-horizon tasks with image observations
by using suboptimal demonstrations and a learned dynamics model to restrict
exploration to the neighborhood of a learned Safe Set where task completion is
likely. We evaluate LS3 on 4 domains, including a challenging sequential
pushing task in simulation and a physical cable routing task. We find that LS3
can use prior task successes to restrict exploration and learn more efficiently
than prior algorithms while satisfying constraints. See
https://tinyurl.com/latent-ss for code and supplementary material.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wilcox_A/0/1/0/all/0/1"&gt;Albert Wilcox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balakrishna_A/0/1/0/all/0/1"&gt;Ashwin Balakrishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thananjeyan_B/0/1/0/all/0/1"&gt;Brijen Thananjeyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1"&gt;Joseph E. Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1"&gt;Ken Goldberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lifelong Teacher-Student Network Learning. (arXiv:2107.04689v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04689</id>
        <link href="http://arxiv.org/abs/2107.04689"/>
        <updated>2021-07-13T01:59:36.535Z</updated>
        <summary type="html"><![CDATA[A unique cognitive capability of humans consists in their ability to acquire
new knowledge and skills from a sequence of experiences. Meanwhile, artificial
intelligence systems are good at learning only the last given task without
being able to remember the databases learnt in the past. We propose a novel
lifelong learning methodology by employing a Teacher-Student network framework.
While the Student module is trained with a new given database, the Teacher
module would remind the Student about the information learnt in the past. The
Teacher, implemented by a Generative Adversarial Network (GAN), is trained to
preserve and replay past knowledge corresponding to the probabilistic
representations of previously learn databases. Meanwhile, the Student module is
implemented by a Variational Autoencoder (VAE) which infers its latent variable
representation from both the output of the Teacher module as well as from the
newly available database. Moreover, the Student module is trained to capture
both continuous and discrete underlying data representations across different
domains. The proposed lifelong learning framework is applied in supervised,
semi-supervised and unsupervised training. The code is available~:
\url{https://github.com/dtuzi123/Lifelong-Teacher-Student-Network-Learning}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1"&gt;Fei Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1"&gt;Adrian G. Bors&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learn from Anywhere: Rethinking Generalized Zero-Shot Learning with Limited Supervision. (arXiv:2107.04952v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04952</id>
        <link href="http://arxiv.org/abs/2107.04952"/>
        <updated>2021-07-13T01:59:36.528Z</updated>
        <summary type="html"><![CDATA[A common problem with most zero and few-shot learning approaches is they
suffer from bias towards seen classes resulting in sub-optimal performance.
Existing efforts aim to utilize unlabeled images from unseen classes (i.e
transductive zero-shot) during training to enable generalization. However, this
limits their use in practical scenarios where data from target unseen classes
is unavailable or infeasible to collect. In this work, we present a practical
setting of inductive zero and few-shot learning, where unlabeled images from
other out-of-data classes, that do not belong to seen or unseen categories, can
be used to improve generalization in any-shot learning. We leverage a
formulation based on product-of-experts and introduce a new AUD module that
enables us to use unlabeled samples from out-of-data classes which are usually
easily available and practically entail no annotation cost. In addition, we
also demonstrate the applicability of our model to address a more practical and
challenging, Generalized Zero-shot under a limited supervision setting, where
even base seen classes do not have sufficient annotated samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhatt_G/0/1/0/all/0/1"&gt;Gaurav Bhatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandok_S/0/1/0/all/0/1"&gt;Shivam Chandok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1"&gt;Vineeth N Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Impossibility of What? Formal and Substantive Equality in Algorithmic Fairness. (arXiv:2107.04642v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2107.04642</id>
        <link href="http://arxiv.org/abs/2107.04642"/>
        <updated>2021-07-13T01:59:36.507Z</updated>
        <summary type="html"><![CDATA[In the face of compounding crises of social and economic inequality, many
have turned to algorithmic decision-making to achieve greater fairness in
society. As these efforts intensify, reasoning within the burgeoning field of
"algorithmic fairness" increasingly shapes how fairness manifests in practice.
This paper interrogates whether algorithmic fairness provides the appropriate
conceptual and practical tools for enhancing social equality. I argue that the
dominant, "formal" approach to algorithmic fairness is ill-equipped as a
framework for pursuing equality, as its narrow frame of analysis generates
restrictive approaches to reform. In light of these shortcomings, I propose an
alternative: a "substantive" approach to algorithmic fairness that centers
opposition to social hierarchies and provides a more expansive analysis of how
to address inequality. This substantive approach enables more fruitful
theorizing about the role of algorithms in combatting oppression. The
distinction between formal and substantive algorithmic fairness is exemplified
by each approach's responses to the "impossibility of fairness" (an
incompatibility between mathematical definitions of algorithmic fairness).
While the formal approach requires us to accept the "impossibility of fairness"
as a harsh limit on efforts to enhance equality, the substantive approach
allows us to escape the "impossibility of fairness" by suggesting reforms that
are not subject to this false dilemma and that are better equipped to
ameliorate conditions of social oppression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Green_B/0/1/0/all/0/1"&gt;Ben Green&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Learning with Multi-Head Co-Training. (arXiv:2107.04795v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04795</id>
        <link href="http://arxiv.org/abs/2107.04795"/>
        <updated>2021-07-13T01:59:36.500Z</updated>
        <summary type="html"><![CDATA[Co-training, extended from self-training, is one of the frameworks for
semi-supervised learning. It works at the cost of training extra classifiers,
where the algorithm should be delicately designed to prevent individual
classifiers from collapsing into each other. In this paper, we present a simple
and efficient co-training algorithm, named Multi-Head Co-Training, for
semi-supervised image classification. By integrating base learners into a
multi-head structure, the model is in a minimal amount of extra parameters.
Every classification head in the unified model interacts with its peers through
a "Weak and Strong Augmentation" strategy, achieving single-view co-training
without promoting diversity explicitly. The effectiveness of Multi-Head
Co-Training is demonstrated in an empirical study on standard semi-supervised
learning benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mingcai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yuntao Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1"&gt;Shuwei Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chongjun Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifying Layers Susceptible to Adversarial Attacks. (arXiv:2107.04827v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04827</id>
        <link href="http://arxiv.org/abs/2107.04827"/>
        <updated>2021-07-13T01:59:36.494Z</updated>
        <summary type="html"><![CDATA[Common neural network architectures are susceptible to attack by adversarial
samples. Neural network architectures are commonly thought of as divided into
low-level feature extraction layers and high-level classification layers;
susceptibility of networks to adversarial samples is often thought of as a
problem related to classification rather than feature extraction. We test this
idea by selectively retraining different portions of VGG and ResNet
architectures on CIFAR-10, Imagenette and ImageNet using non-adversarial and
adversarial data. Our experimental results show that susceptibility to
adversarial samples is associated with low-level feature extraction layers.
Therefore, retraining high-level layers is insufficient for achieving
robustness. This phenomenon could have two explanations: either, adversarial
attacks yield outputs from early layers that are indistinguishable from
features found in the attack classes, or adversarial attacks yield outputs from
early layers that differ statistically from features for non-adversarial
samples and do not permit consistent classification by subsequent layers. We
test this question by large-scale non-linear dimensionality reduction and
density modeling on distributions of feature vectors in hidden layers and find
that the feature distributions between non-adversarial and adversarial samples
differ substantially. Our results provide new insights into the statistical
origins of adversarial samples and possible defenses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Siddiqui_S/0/1/0/all/0/1"&gt;Shoaib Ahmed Siddiqui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Breuel_T/0/1/0/all/0/1"&gt;Thomas Breuel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Communication-Computation Efficient Secure Aggregation for Federated Learning. (arXiv:2012.05433v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05433</id>
        <link href="http://arxiv.org/abs/2012.05433"/>
        <updated>2021-07-13T01:59:36.419Z</updated>
        <summary type="html"><![CDATA[Federated learning has been spotlighted as a way to train neural networks
using distributed data with no need for individual nodes to share data.
Unfortunately, it has also been shown that adversaries may be able to extract
local data contents off model parameters transmitted during federated learning.
A recent solution based on the secure aggregation primitive enabled
privacy-preserving federated learning, but at the expense of significant extra
communication/computational resources. In this paper, we propose a
low-complexity scheme that provides data privacy using substantially reduced
communication/computational resources relative to the existing secure solution.
The key idea behind the suggested scheme is to design the topology of
secret-sharing nodes as a sparse random graph instead of the complete graph
corresponding to the existing solution. We first obtain the necessary and
sufficient condition on the graph to guarantee both reliability and privacy. We
then suggest using the Erd\H{o}s-R\'enyi graph in particular and provide
theoretical guarantees on the reliability/privacy of the proposed scheme.
Through extensive real-world experiments, we demonstrate that our scheme, using
only $20 \sim 30\%$ of the resources required in the conventional scheme,
maintains virtually the same levels of reliability and data privacy in
practical federated learning systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_B/0/1/0/all/0/1"&gt;Beongjun Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohn_J/0/1/0/all/0/1"&gt;Jy-yong Sohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1"&gt;Dong-Jun Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1"&gt;Jaekyun Moon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simple and Effective VAE Training with Calibrated Decoders. (arXiv:2006.13202v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.13202</id>
        <link href="http://arxiv.org/abs/2006.13202"/>
        <updated>2021-07-13T01:59:36.413Z</updated>
        <summary type="html"><![CDATA[Variational autoencoders (VAEs) provide an effective and simple method for
modeling complex distributions. However, training VAEs often requires
considerable hyperparameter tuning to determine the optimal amount of
information retained by the latent variable. We study the impact of calibrated
decoders, which learn the uncertainty of the decoding distribution and can
determine this amount of information automatically, on the VAE performance.
While many methods for learning calibrated decoders have been proposed, many of
the recent papers that employ VAEs rely on heuristic hyperparameters and ad-hoc
modifications instead. We perform the first comprehensive comparative analysis
of calibrated decoder and provide recommendations for simple and effective VAE
training. Our analysis covers a range of image and video datasets and several
single-image and sequential VAE models. We further propose a simple but novel
modification to the commonly used Gaussian decoder, which computes the
prediction variance analytically. We observe empirically that using heuristic
modifications is not necessary with our method. Project website is at
https://orybkin.github.io/sigma-vae/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rybkin_O/0/1/0/all/0/1"&gt;Oleh Rybkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1"&gt;Kostas Daniilidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridge Composite and Real: Towards End-to-end Deep Image Matting. (arXiv:2010.16188v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.16188</id>
        <link href="http://arxiv.org/abs/2010.16188"/>
        <updated>2021-07-13T01:59:36.391Z</updated>
        <summary type="html"><![CDATA[Extracting accurate foregrounds from natural images benefits many downstream
applications such as film production and augmented reality. However, the furry
characteristics and various appearance of the foregrounds, e.g., animal and
portrait, challenge existing matting methods, which usually require extra user
inputs such as trimap or scribbles. To resolve these problems, we study the
distinct roles of semantics and details for image matting and decompose the
task into two parallel sub-tasks: high-level semantic segmentation and
low-level details matting. Specifically, we propose a novel Glance and Focus
Matting network (GFM), which employs a shared encoder and two separate decoders
to learn both tasks in a collaborative manner for end-to-end natural image
matting. Besides, due to the limitation of available natural images in the
matting task, previous methods typically adopt composite images for training
and evaluation, which result in limited generalization ability on real-world
images. In this paper, we investigate the domain gap issue between composite
images and real-world images systematically by conducting comprehensive
analyses of various discrepancies between foreground and background images. We
find that a carefully designed composition route RSSN that aims to reduce the
discrepancies can lead to a better model with remarkable generalization
ability. Furthermore, we provide a benchmark containing 2,000 high-resolution
real-world animal images and 10,000 portrait images along with their manually
labeled alpha mattes to serve as a test bed for evaluating matting model's
generalization ability on real-world images. Comprehensive empirical studies
have demonstrated that GFM outperforms state-of-the-art methods and effectively
reduces the generalization error. The code and the dataset will be released.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jizhizi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maybank_S/0/1/0/all/0/1"&gt;Stephen J. Maybank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recovering Joint Probability of Discrete Random Variables from Pairwise Marginals. (arXiv:2006.16912v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.16912</id>
        <link href="http://arxiv.org/abs/2006.16912"/>
        <updated>2021-07-13T01:59:36.384Z</updated>
        <summary type="html"><![CDATA[Learning the joint probability of random variables (RVs) is the cornerstone
of statistical signal processing and machine learning. However, direct
nonparametric estimation for high-dimensional joint probability is in general
impossible, due to the curse of dimensionality. Recent work has proposed to
recover the joint probability mass function (PMF) of an arbitrary number of RVs
from three-dimensional marginals, leveraging the algebraic properties of
low-rank tensor decomposition and the (unknown) dependence among the RVs.
Nonetheless, accurately estimating three-dimensional marginals can still be
costly in terms of sample complexity, affecting the performance of this line of
work in practice in the sample-starved regime. Using three-dimensional
marginals also involves challenging tensor decomposition problems whose
tractability is unclear. This work puts forth a new framework for learning the
joint PMF using only pairwise marginals, which naturally enjoys a lower sample
complexity relative to the third-order ones. A coupled nonnegative matrix
factorization (CNMF) framework is developed, and its joint PMF recovery
guarantees under various conditions are analyzed. Our method also features a
Gram--Schmidt (GS)-like algorithm that exhibits competitive runtime
performance. The algorithm is shown to provably recover the joint PMF up to
bounded error in finite iterations, under reasonable conditions. It is also
shown that a recently proposed economical expectation maximization (EM)
algorithm guarantees to improve upon the GS-like algorithm's output, thereby
further lifting up the accuracy and efficiency. Real-data experiments are
employed to showcase the effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ibrahim_S/0/1/0/all/0/1"&gt;Shahana Ibrahim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Fu_X/0/1/0/all/0/1"&gt;Xiao Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoFB: Automating Fetal Biometry Estimation from Standard Ultrasound Planes. (arXiv:2107.05255v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05255</id>
        <link href="http://arxiv.org/abs/2107.05255"/>
        <updated>2021-07-13T01:59:36.378Z</updated>
        <summary type="html"><![CDATA[During pregnancy, ultrasound examination in the second trimester can assess
fetal size according to standardized charts. To achieve a reproducible and
accurate measurement, a sonographer needs to identify three standard 2D planes
of the fetal anatomy (head, abdomen, femur) and manually mark the key
anatomical landmarks on the image for accurate biometry and fetal weight
estimation. This can be a time-consuming operator-dependent task, especially
for a trainee sonographer. Computer-assisted techniques can help in automating
the fetal biometry computation process. In this paper, we present a unified
automated framework for estimating all measurements needed for the fetal weight
assessment. The proposed framework semantically segments the key fetal
anatomies using state-of-the-art segmentation models, followed by region
fitting and scale recovery for the biometry estimation. We present an ablation
study of segmentation algorithms to show their robustness through 4-fold
cross-validation on a dataset of 349 ultrasound standard plane images from 42
pregnancies. Moreover, we show that the network with the best segmentation
performance tends to be more accurate for biometry estimation. Furthermore, we
demonstrate that the error between clinically measured and predicted fetal
biometry is lower than the permissible error during routine clinical
measurements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bano_S/0/1/0/all/0/1"&gt;Sophia Bano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dromey_B/0/1/0/all/0/1"&gt;Brian Dromey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasconcelos_F/0/1/0/all/0/1"&gt;Francisco Vasconcelos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Napolitano_R/0/1/0/all/0/1"&gt;Raffaele Napolitano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+David_A/0/1/0/all/0/1"&gt;Anna L. David&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peebles_D/0/1/0/all/0/1"&gt;Donald M. Peebles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1"&gt;Danail Stoyanov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data efficiency in graph networks through equivariance. (arXiv:2106.13786v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13786</id>
        <link href="http://arxiv.org/abs/2106.13786"/>
        <updated>2021-07-13T01:59:36.359Z</updated>
        <summary type="html"><![CDATA[We introduce a novel architecture for graph networks which is equivariant to
any transformation in the coordinate embeddings that preserves the distance
between neighbouring nodes. In particular, it is equivariant to the Euclidean
and conformal orthogonal groups in $n$-dimensions. Thanks to its equivariance
properties, the proposed model is extremely more data efficient with respect to
classical graph architectures and also intrinsically equipped with a better
inductive bias. We show that, learning on a minimal amount of data, the
architecture we propose can perfectly generalise to unseen data in a synthetic
problem, while much more training data are required from a standard model to
reach comparable performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farina_F/0/1/0/all/0/1"&gt;Francesco Farina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slade_E/0/1/0/all/0/1"&gt;Emma Slade&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BAGUA: Scaling up Distributed Learning with System Relaxations. (arXiv:2107.01499v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01499</id>
        <link href="http://arxiv.org/abs/2107.01499"/>
        <updated>2021-07-13T01:59:36.353Z</updated>
        <summary type="html"><![CDATA[Recent years have witnessed a growing list of systems for distributed
data-parallel training. Existing systems largely fit into two paradigms, i.e.,
parameter server and MPI-style collective operations. On the algorithmic side,
researchers have proposed a wide range of techniques to lower the communication
via system relaxations: quantization, decentralization, and communication
delay. However, most, if not all, existing systems only rely on standard
synchronous and asynchronous stochastic gradient (SG) based optimization,
therefore, cannot take advantage of all possible optimizations that the machine
learning community has been developing recently. Given this emerging gap
between the current landscapes of systems and theory, we build BAGUA, a
communication framework whose design goal is to provide a system abstraction
that is both flexible and modular to support state-of-the-art system relaxation
techniques of distributed training. Powered by the new system design, BAGUA has
a great ability to implement and extend various state-of-the-art distributed
learning algorithms. In a production cluster with up to 16 machines (128 GPUs),
BAGUA can outperform PyTorch-DDP, Horovod and BytePS in the end-to-end training
time by a significant margin (up to 1.95 times) across a diverse range of
tasks. Moreover, we conduct a rigorous tradeoff exploration showing that
different algorithms and system relaxations achieve the best performance over
different network conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gan_S/0/1/0/all/0/1"&gt;Shaoduo Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lian_X/0/1/0/all/0/1"&gt;Xiangru Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1"&gt;Jianbin Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chengjun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1"&gt;Hongmei Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shengzhuo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xianghong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1"&gt;Tengxu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jiawei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1"&gt;Binhang Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Sen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Ji Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Ce Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simulating the Time Projection Chamber responses at the MPD detector using Generative Adversarial Networks. (arXiv:2012.04595v2 [physics.ins-det] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04595</id>
        <link href="http://arxiv.org/abs/2012.04595"/>
        <updated>2021-07-13T01:59:36.346Z</updated>
        <summary type="html"><![CDATA[High energy physics experiments rely heavily on the detailed detector
simulation models in many tasks. Running these detailed models typically
requires a notable amount of the computing time available to the experiments.
In this work, we demonstrate a new approach to speed up the simulation of the
Time Projection Chamber tracker of the MPD experiment at the NICA accelerator
complex. Our method is based on a Generative Adversarial Network - a deep
learning technique allowing for implicit estimation of the population
distribution for a given set of objects. This approach lets us learn and then
sample from the distribution of raw detector responses, conditioned on the
parameters of the charged particle tracks. To evaluate the quality of the
proposed model, we integrate a prototype into the MPD software stack and
demonstrate that it produces high-quality events similar to the detailed
simulator, with a speed-up of at least an order of magnitude. The prototype is
trained on the responses from the inner part of the detector and, once expanded
to the full detector, should be ready for use in physics tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Maevskiy_A/0/1/0/all/0/1"&gt;A. Maevskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ratnikov_F/0/1/0/all/0/1"&gt;F. Ratnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zinchenko_A/0/1/0/all/0/1"&gt;A. Zinchenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Riabov_V/0/1/0/all/0/1"&gt;V. Riabov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random Features for Kernel Approximation: A Survey on Algorithms, Theory, and Beyond. (arXiv:2004.11154v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.11154</id>
        <link href="http://arxiv.org/abs/2004.11154"/>
        <updated>2021-07-13T01:59:36.340Z</updated>
        <summary type="html"><![CDATA[Random features is one of the most popular techniques to speed up kernel
methods in large-scale problems. Related works have been recognized by the
NeurIPS Test-of-Time award in 2017 and the ICML Best Paper Finalist in 2019.
The body of work on random features has grown rapidly, and hence it is
desirable to have a comprehensive overview on this topic explaining the
connections among various algorithms and theoretical results. In this survey,
we systematically review the work on random features from the past ten years.
First, the motivations, characteristics and contributions of representative
random features based algorithms are summarized according to their sampling
schemes, learning procedures, variance reduction properties and how they
exploit training data. Second, we review theoretical results that center around
the following key question: how many random features are needed to ensure a
high approximation quality or no loss in the empirical/expected risks of the
learned estimator. Third, we provide a comprehensive evaluation of popular
random features based algorithms on several large-scale benchmark datasets and
discuss their approximation quality and prediction performance for
classification. Last, we discuss the relationship between random features and
modern over-parameterized deep neural networks (DNNs), including the use of
high dimensional random features in the analysis of DNNs as well as the gaps
between current theoretical and empirical results. This survey may serve as a
gentle introduction to this topic, and as a users' guide for practitioners
interested in applying the representative algorithms and understanding
theoretical results under various technical assumptions. We hope that this
survey will facilitate discussion on the open problems in this topic, and more
importantly, shed light on future research directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fanghui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaolin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yudong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Suykens_J/0/1/0/all/0/1"&gt;Johan A.K. Suykens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LexSubCon: Integrating Knowledge from Lexical Resources into Contextual Embeddings for Lexical Substitution. (arXiv:2107.05132v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05132</id>
        <link href="http://arxiv.org/abs/2107.05132"/>
        <updated>2021-07-13T01:59:36.333Z</updated>
        <summary type="html"><![CDATA[Lexical substitution is the task of generating meaningful substitutes for a
word in a given textual context. Contextual word embedding models have achieved
state-of-the-art results in the lexical substitution task by relying on
contextual information extracted from the replaced word within the sentence.
However, such models do not take into account structured knowledge that exists
in external lexical databases.

We introduce LexSubCon, an end-to-end lexical substitution framework based on
contextual embedding models that can identify highly accurate substitute
candidates. This is achieved by combining contextual information with knowledge
from structured lexical resources. Our approach involves: (i) introducing a
novel mix-up embedding strategy in the creation of the input embedding of the
target word through linearly interpolating the pair of the target input
embedding and the average embedding of its probable synonyms; (ii) considering
the similarity of the sentence-definition embeddings of the target word and its
proposed candidates; and, (iii) calculating the effect of each substitution in
the semantics of the sentence through a fine-tuned sentence similarity model.
Our experiments show that LexSubCon outperforms previous state-of-the-art
methods on LS07 and CoInCo benchmark datasets that are widely used for lexical
substitution tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Michalopoulos_G/0/1/0/all/0/1"&gt;George Michalopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McKillop_I/0/1/0/all/0/1"&gt;Ian McKillop&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alexander Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Helen Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards a Multimodal System for Precision Agriculture using IoT and Machine Learning. (arXiv:2107.04895v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04895</id>
        <link href="http://arxiv.org/abs/2107.04895"/>
        <updated>2021-07-13T01:59:36.315Z</updated>
        <summary type="html"><![CDATA[Precision agriculture system is an arising idea that refers to overseeing
farms utilizing current information and communication technologies to improve
the quantity and quality of yields while advancing the human work required. The
automation requires the assortment of information given by the sensors such as
soil, water, light, humidity, temperature for additional information to furnish
the operator with exact data to acquire excellent yield to farmers. In this
work, a study is proposed that incorporates all common state-of-the-art
approaches for precision agriculture use. Technologies like the Internet of
Things (IoT) for data collection, machine Learning for crop damage prediction,
and deep learning for crop disease detection is used. The data collection using
IoT is responsible for the measure of moisture levels for smart irrigation, n,
p, k estimations of fertilizers for best yield development. For crop damage
prediction, various algorithms like Random Forest (RF), Light gradient boosting
machine (LGBM), XGBoost (XGB), Decision Tree (DT) and K Nearest Neighbor (KNN)
are used. Subsequently, Pre-Trained Convolutional Neural Network (CNN) models
such as VGG16, Resnet50, and DenseNet121 are also trained to check if the crop
was tainted with some illness or not.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1"&gt;Satvik Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pundir_P/0/1/0/all/0/1"&gt;Pradyumn Pundir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jindal_H/0/1/0/all/0/1"&gt;Himanshu Jindal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saini_H/0/1/0/all/0/1"&gt;Hemraj Saini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1"&gt;Somya Garg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Shot Scene Graph Relation Prediction through Commonsense Knowledge Integration. (arXiv:2107.05080v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05080</id>
        <link href="http://arxiv.org/abs/2107.05080"/>
        <updated>2021-07-13T01:59:36.309Z</updated>
        <summary type="html"><![CDATA[Relation prediction among entities in images is an important step in scene
graph generation (SGG), which further impacts various visual understanding and
reasoning tasks. Existing SGG frameworks, however, require heavy training yet
are incapable of modeling unseen (i.e.,zero-shot) triplets. In this work, we
stress that such incapability is due to the lack of commonsense reasoning,i.e.,
the ability to associate similar entities and infer similar relations based on
general understanding of the world. To fill this gap, we propose
CommOnsense-integrAted sCenegrapHrElation pRediction (COACHER), a framework to
integrate commonsense knowledge for SGG, especially for zero-shot relation
prediction. Specifically, we develop novel graph mining pipelines to model the
neighborhoods and paths around entities in an external commonsense knowledge
graph, and integrate them on top of state-of-the-art SGG frameworks. Extensive
quantitative evaluations and qualitative case studies on both original and
manipulated datasets from Visual Genome demonstrate the effectiveness of our
proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kan_X/0/1/0/all/0/1"&gt;Xuan Kan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1"&gt;Hejie Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Carl Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Details Preserving Deep Collaborative Filtering-Based Method for Image Denoising. (arXiv:2107.05115v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.05115</id>
        <link href="http://arxiv.org/abs/2107.05115"/>
        <updated>2021-07-13T01:59:36.302Z</updated>
        <summary type="html"><![CDATA[In spite of the improvements achieved by the several denoising algorithms
over the years, many of them still fail at preserving the fine details of the
image after denoising. This is as a result of the smooth-out effect they have
on the images. Most neural network-based algorithms have achieved better
quantitative performance than the classical denoising algorithms. However, they
also suffer from qualitative (visual) performance as a result of the smooth-out
effect. In this paper, we propose an algorithm to address this shortcoming. We
propose a deep collaborative filtering-based (Deep-CoFiB) algorithm for image
denoising. This algorithm performs collaborative denoising of image patches in
the sparse domain using a set of optimized neural network models. This results
in a fast algorithm that is able to excellently obtain a trade-off between
noise removal and details preservation. Extensive experiments show that the
DeepCoFiB performed quantitatively (in terms of PSNR and SSIM) and
qualitatively (visually) better than many of the state-of-the-art denoising
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Alawode_B/0/1/0/all/0/1"&gt;Basit O. Alawode&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Masood_M/0/1/0/all/0/1"&gt;Mudassir Masood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ballal_T/0/1/0/all/0/1"&gt;Tarig Ballal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Al_Naffouri_T/0/1/0/all/0/1"&gt;Tareq Al-Naffouri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-service Data Classification Using Interactive Visualization and Interpretable Machine Learning. (arXiv:2107.04971v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04971</id>
        <link href="http://arxiv.org/abs/2107.04971"/>
        <updated>2021-07-13T01:59:36.295Z</updated>
        <summary type="html"><![CDATA[Machine learning algorithms often produce models considered as complex
black-box models by both end users and developers. They fail to explain the
model in terms of the domain they are designed for. The proposed Iterative
Visual Logical Classifier (IVLC) is an interpretable machine learning algorithm
that allows end users to design a model and classify data with more confidence
and without having to compromise on the accuracy. Such technique is especially
helpful when dealing with sensitive and crucial data like cancer data in the
medical domain with high cost of errors. With the help of the proposed
interactive and lossless multidimensional visualization, end users can identify
the pattern in the data based on which they can make explainable decisions.
Such options would not be possible in black box machine learning methodologies.
The interpretable IVLC algorithm is supported by the Interactive Shifted Paired
Coordinates Software System (SPCVis). It is a lossless multidimensional data
visualization system with user interactive features. The interactive approach
provides flexibility to the end user to perform data classification as
self-service without having to rely on a machine learning expert. Interactive
pattern discovery becomes challenging while dealing with large data sets with
hundreds of dimensions/features. To overcome this problem, this chapter
proposes an automated classification approach combined with new Coordinate
Order Optimizer (COO) algorithm and a Genetic algorithm. The COO algorithm
automatically generates the coordinate pair sequences that best represent the
data separation and the genetic algorithm helps optimizing the proposed IVLC
algorithm by automatically generating the areas for data classification. The
feasibility of the approach is shown by experiments on benchmark datasets
covering both interactive and automated processes used for data classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wagle_S/0/1/0/all/0/1"&gt;Sridevi Narayana Wagle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1"&gt;Boris Kovalerchuk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting. (arXiv:2106.13008v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13008</id>
        <link href="http://arxiv.org/abs/2106.13008"/>
        <updated>2021-07-13T01:59:36.289Z</updated>
        <summary type="html"><![CDATA[Extending the forecasting time is a critical demand for real applications,
such as extreme weather early warning and long-term energy consumption
planning. This paper studies the \textit{long-term forecasting} problem of time
series. Prior Transformer-based models adopt various self-attention mechanisms
to discover the long-range dependencies. However, intricate temporal patterns
of the long-term future prohibit the model from finding reliable dependencies.
Also, Transformers have to adopt the sparse versions of point-wise
self-attentions for long series efficiency, resulting in the information
utilization bottleneck. Towards these challenges, we propose Autoformer as a
novel decomposition architecture with an Auto-Correlation mechanism. We go
beyond the pre-processing convention of series decomposition and renovate it as
a basic inner block of deep models. This design empowers Autoformer with
progressive decomposition capacities for complex time series. Further, inspired
by the stochastic process theory, we design the Auto-Correlation mechanism
based on the series periodicity, which conducts the dependencies discovery and
representation aggregation at the sub-series level. Auto-Correlation
outperforms self-attention in both efficiency and accuracy. In long-term
forecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative
improvement on six benchmarks, covering five practical applications: energy,
traffic, economics, weather and disease.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Haixu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jiehui Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianmin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1"&gt;Mingsheng Long&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BrainNNExplainer: An Interpretable Graph Neural Network Framework for Brain Network based Disease Analysis. (arXiv:2107.05097v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05097</id>
        <link href="http://arxiv.org/abs/2107.05097"/>
        <updated>2021-07-13T01:59:36.273Z</updated>
        <summary type="html"><![CDATA[Interpretable brain network models for disease prediction are of great value
for the advancement of neuroscience. GNNs are promising to model complicated
network data, but they are prone to overfitting and suffer from poor
interpretability, which prevents their usage in decision-critical scenarios
like healthcare. To bridge this gap, we propose BrainNNExplainer, an
interpretable GNN framework for brain network analysis. It is mainly composed
of two jointly learned modules: a backbone prediction model that is
specifically designed for brain networks and an explanation generator that
highlights disease-specific prominent brain network connections. Extensive
experimental results with visualizations on two challenging disease prediction
datasets demonstrate the unique interpretability and outstanding performance of
BrainNNExplainer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1"&gt;Hejie Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1"&gt;Wei Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yanqiao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoxiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lifang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Carl Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hate Speech Detection in Clubhouse. (arXiv:2106.13238v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13238</id>
        <link href="http://arxiv.org/abs/2106.13238"/>
        <updated>2021-07-13T01:59:36.267Z</updated>
        <summary type="html"><![CDATA[With the rise of voice chat rooms, a gigantic resource of data can be exposed
to the research community for natural language processing tasks. Moderators in
voice chat rooms actively monitor the discussions and remove the participants
with offensive language. However, it makes the hate speech detection even more
difficult since some participants try to find creative ways to articulate hate
speech. This makes the hate speech detection challenging in new social media
like Clubhouse. To the best of our knowledge all the hate speech datasets have
been collected from text resources like Twitter. In this paper, we take the
first step to collect a significant dataset from Clubhouse as the rising star
in social media industry. We analyze the collected instances from statistical
point of view using the Google Perspective Scores. Our experiments show that,
the Perspective Scores can outperform Bag of Words and Word2Vec as high level
text features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mansourifar_H/0/1/0/all/0/1"&gt;Hadi Mansourifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alsagheer_D/0/1/0/all/0/1"&gt;Dana Alsagheer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fathi_R/0/1/0/all/0/1"&gt;Reza Fathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1"&gt;Weidong Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_L/0/1/0/all/0/1"&gt;Lan Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yan Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Function approximation by deep neural networks with parameters $\{0,\pm \frac{1}{2}, \pm 1, 2\}$. (arXiv:2103.08659v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08659</id>
        <link href="http://arxiv.org/abs/2103.08659"/>
        <updated>2021-07-13T01:59:36.261Z</updated>
        <summary type="html"><![CDATA[In this paper it is shown that $C_\beta$-smooth functions can be approximated
by neural networks with parameters $\{0,\pm \frac{1}{2}, \pm 1, 2\}$. The
depth, width and the number of active parameters of the constructed networks
have, up to a logarithmic factor, the same dependence on the approximation
error as the networks with parameters in $[-1,1]$. In particular, this means
that the nonparametric regression estimation with the constructed networks
attains the same convergence rate as with sparse networks with parameters in
$[-1,1]$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Beknazaryan_A/0/1/0/all/0/1"&gt;Aleksandr Beknazaryan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Matrix Autoencoder Framework to Align the Functional and Structural Connectivity Manifolds as Guided by Behavioral Phenotypes. (arXiv:2105.14409v2 [q-bio.NC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14409</id>
        <link href="http://arxiv.org/abs/2105.14409"/>
        <updated>2021-07-13T01:59:36.256Z</updated>
        <summary type="html"><![CDATA[We propose a novel matrix autoencoder to map functional connectomes from
resting state fMRI (rs-fMRI) to structural connectomes from Diffusion Tensor
Imaging (DTI), as guided by subject-level phenotypic measures. Our specialized
autoencoder infers a low dimensional manifold embedding for the rs-fMRI
correlation matrices that mimics a canonical outer-product decomposition. The
embedding is simultaneously used to reconstruct DTI tractography matrices via a
second manifold alignment decoder and to predict inter-subject phenotypic
variability via an artificial neural network. We validate our framework on a
dataset of 275 healthy individuals from the Human Connectome Project database
and on a second clinical dataset consisting of 57 subjects with Autism Spectrum
Disorder. We demonstrate that the model reliably recovers structural
connectivity patterns across individuals, while robustly extracting predictive
and interpretable brain biomarkers in a cross-validated setting. Finally, our
framework outperforms several baselines at predicting behavioral phenotypes in
both real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+DSouza_N/0/1/0/all/0/1"&gt;Niharika Shimona D&amp;#x27;Souza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Nebel_M/0/1/0/all/0/1"&gt;Mary Beth Nebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Crocetti_D/0/1/0/all/0/1"&gt;Deana Crocetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Wymbs_N/0/1/0/all/0/1"&gt;Nicholas Wymbs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Robinson_J/0/1/0/all/0/1"&gt;Joshua Robinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Mostofsky_S/0/1/0/all/0/1"&gt;Stewart Mostofsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Venkataraman_A/0/1/0/all/0/1"&gt;Archana Venkataraman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Customizable Reference Runtime Monitoring of Neural Networks using Resolution Boxes. (arXiv:2104.14435v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14435</id>
        <link href="http://arxiv.org/abs/2104.14435"/>
        <updated>2021-07-13T01:59:36.249Z</updated>
        <summary type="html"><![CDATA[Classification neural networks fail to detect inputs that do not fall inside
the classes they have been trained for. Runtime monitoring techniques on the
neuron activation pattern can be used to detect such inputs. We present an
approach for monitoring classification systems via data abstraction. Data
abstraction relies on the notion of box with a resolution. Box-based
abstraction consists in representing a set of values by its minimal and maximal
values in each dimension. We augment boxes with a notion of resolution and
define their clustering coverage, which is intuitively a quantitative metric
that indicates the abstraction quality. This allows studying the effect of
different clustering parameters on the constructed boxes and estimating an
interval of sub-optimal parameters. Moreover, we automatically construct
monitors that leverage both the correct and incorrect behaviors of a system.
This allows checking the size of the monitor abstractions and analyzing the
separability of the network. Monitors are obtained by combining the
sub-monitors of each class of the system placed at some selected layers. Our
experiments demonstrate the effectiveness of our clustering coverage estimation
and show how to assess the effectiveness and precision of monitors according to
the selected clustering parameter and monitored layers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Changshun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Falcone_Y/0/1/0/all/0/1"&gt;Yli&amp;#xe8;s Falcone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bensalem_S/0/1/0/all/0/1"&gt;Saddek Bensalem&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Learning with Optimism and Delay. (arXiv:2106.06885v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06885</id>
        <link href="http://arxiv.org/abs/2106.06885"/>
        <updated>2021-07-13T01:59:36.243Z</updated>
        <summary type="html"><![CDATA[Inspired by the demands of real-time climate and weather forecasting, we
develop optimistic online learning algorithms that require no parameter tuning
and have optimal regret guarantees under delayed feedback. Our algorithms --
DORM, DORM+, and AdaHedgeD -- arise from a novel reduction of delayed online
learning to optimistic online learning that reveals how optimistic hints can
mitigate the regret penalty caused by delay. We pair this delay-as-optimism
perspective with a new analysis of optimistic learning that exposes its
robustness to hinting errors and a new meta-algorithm for learning effective
hinting strategies in the presence of delay. We conclude by benchmarking our
algorithms on four subseasonal climate forecasting tasks, demonstrating low
regret relative to state-of-the-art forecasting models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Flaspohler_G/0/1/0/all/0/1"&gt;Genevieve Flaspohler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orabona_F/0/1/0/all/0/1"&gt;Francesco Orabona&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_J/0/1/0/all/0/1"&gt;Judah Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mouatadid_S/0/1/0/all/0/1"&gt;Soukayna Mouatadid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oprescu_M/0/1/0/all/0/1"&gt;Miruna Oprescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orenstein_P/0/1/0/all/0/1"&gt;Paulo Orenstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mackey_L/0/1/0/all/0/1"&gt;Lester Mackey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyperspectral and Multispectral Classification for Coastal Wetland Using Depthwise Feature Interaction Network. (arXiv:2106.06896v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06896</id>
        <link href="http://arxiv.org/abs/2106.06896"/>
        <updated>2021-07-13T01:59:36.227Z</updated>
        <summary type="html"><![CDATA[The monitoring of coastal wetlands is of great importance to the protection
of marine and terrestrial ecosystems. However, due to the complex environment,
severe vegetation mixture, and difficulty of access, it is impossible to
accurately classify coastal wetlands and identify their species with
traditional classifiers. Despite the integration of multisource remote sensing
data for performance enhancement, there are still challenges with acquiring and
exploiting the complementary merits from multisource data. In this paper, the
Deepwise Feature Interaction Network (DFINet) is proposed for wetland
classification. A depthwise cross attention module is designed to extract
self-correlation and cross-correlation from multisource feature pairs. In this
way, meaningful complementary information is emphasized for classification.
DFINet is optimized by coordinating consistency loss, discrimination loss, and
classification loss. Accordingly, DFINet reaches the standard solution-space
under the regularity of loss functions, while the spatial consistency and
feature discrimination are preserved. Comprehensive experimental results on two
hyperspectral and multispectral wetland datasets demonstrate that the proposed
DFINet outperforms other competitive methods in terms of overall accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yunhao Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mengmeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianbu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Weiwei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1"&gt;Ran Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1"&gt;Qian Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The DEformer: An Order-Agnostic Distribution Estimating Transformer. (arXiv:2106.06989v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06989</id>
        <link href="http://arxiv.org/abs/2106.06989"/>
        <updated>2021-07-13T01:59:36.221Z</updated>
        <summary type="html"><![CDATA[Order-agnostic autoregressive distribution (density) estimation (OADE), i.e.,
autoregressive distribution estimation where the features can occur in an
arbitrary order, is a challenging problem in generative machine learning. Prior
work on OADE has encoded feature identity by assigning each feature to a
distinct fixed position in an input vector. As a result, architectures built
for these inputs must strategically mask either the input or model weights to
learn the various conditional distributions necessary for inferring the full
joint distribution of the dataset in an order-agnostic way. In this paper, we
propose an alternative approach for encoding feature identities, where each
feature's identity is included alongside its value in the input. This feature
identity encoding strategy allows neural architectures designed for sequential
data to be applied to the OADE task without modification. As a proof of
concept, we show that a Transformer trained on this input (which we refer to as
"the DEformer", i.e., the distribution estimating Transformer) can effectively
model binarized-MNIST, approaching the performance of fixed-order
autoregressive distribution estimating algorithms while still being entirely
order-agnostic. Additionally, we find that the DEformer surpasses the
performance of recent flow-based architectures when modeling a tabular dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alcorn_M/0/1/0/all/0/1"&gt;Michael A. Alcorn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;Anh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Mandarin Tone Classification with Short Term Context Information. (arXiv:2104.05657v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05657</id>
        <link href="http://arxiv.org/abs/2104.05657"/>
        <updated>2021-07-13T01:59:36.215Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an end-to-end Mandarin tone classification method
from continuous speech utterances utilizing both the spectrogram and the
short-term context information as the input. Both spectrograms and context
segment features are used to train the tone classifier. We first divide the
spectrogram frames into syllable segments using force alignment results
produced by an ASR model. Then we extract the short-term segment features to
capture the context information across multiple syllables. Feeding both the
spectrogram and the short-term context segment features into an end-to-end
model could significantly improve the performance. Experiments are performed on
a large-scale open-source Mandarin speech dataset to evaluate the proposed
method. Results show that this method improves the classification accuracy from
79.5% to 92.6% on the AISHELL3 database.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jiyang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Ming Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cold Posteriors Improve Bayesian Medical Image Post-Processing. (arXiv:2106.07533v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07533</id>
        <link href="http://arxiv.org/abs/2106.07533"/>
        <updated>2021-07-13T01:59:36.209Z</updated>
        <summary type="html"><![CDATA[Cold posteriors have been reported to perform better in practice in the
context of Bayesian deep learning (Wenzel et al., 2020). In variational
inference, it is common to employ only a partially tempered posterior by
scaling the complexity term in the log-evidence lower bound (ELBO). In this
work, we optimize the ELBO for a fully tempered posterior in mean-field
variational inference and use Bayesian optimization to automatically find the
optimal posterior temperature and prior scale. Choosing an appropriate
posterior temperature leads to better predictive performance and improved
uncertainty calibration, which we demonstrate for the task of denoising medical
X-ray images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Laves_M/0/1/0/all/0/1"&gt;Max-Heinrich Laves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tolle_M/0/1/0/all/0/1"&gt;Malte T&amp;#xf6;lle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schlaefer_A/0/1/0/all/0/1"&gt;Alexander Schlaefer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pairing Conceptual Modeling with Machine Learning. (arXiv:2106.14251v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14251</id>
        <link href="http://arxiv.org/abs/2106.14251"/>
        <updated>2021-07-13T01:59:36.203Z</updated>
        <summary type="html"><![CDATA[Both conceptual modeling and machine learning have long been recognized as
important areas of research. With the increasing emphasis on digitizing and
processing large amounts of data for business and other applications, it would
be helpful to consider how these areas of research can complement each other.
To understand how they can be paired, we provide an overview of machine
learning foundations and development cycle. We then examine how conceptual
modeling can be applied to machine learning and propose a framework for
incorporating conceptual modeling into data science projects. The framework is
illustrated by applying it to a healthcare application. For the inverse
pairing, machine learning can impact conceptual modeling through text and rule
mining, as well as knowledge graphs. The pairing of conceptual modeling and
machine learning in this this way should help lay the foundations for future
research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maass_W/0/1/0/all/0/1"&gt;Wolfgang Maass&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Storey_V/0/1/0/all/0/1"&gt;Veda C. Storey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fed-EINI: An Efficient and Interpretable Inference Framework for Decision Tree Ensembles in Federated Learning. (arXiv:2105.09540v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09540</id>
        <link href="http://arxiv.org/abs/2105.09540"/>
        <updated>2021-07-13T01:59:36.188Z</updated>
        <summary type="html"><![CDATA[The increasing concerns about data privacy and security drive an emerging
field of studying privacy-preserving machine learning from isolated data
sources, i.e., federated learning. A class of federated learning, vertical
federated learning, where different parties hold different features for common
users, has a great potential of driving a more variety of business cooperation
among enterprises in many fields. In machine learning, decision tree ensembles
such as gradient boosting decision tree (GBDT) and random forest are widely
applied powerful models with high interpretability and modeling efficiency.
However, the interpretability is compromised in state-of-the-art vertical
federated learning frameworks such as SecureBoost with anonymous features to
avoid possible data breaches. To address this issue in the inference process,
in this paper, we propose Fed-EINI to protect data privacy and allow the
disclosure of feature meaning by concealing decision paths with a
communication-efficient secure computation method for inference outputs. The
advantages of Fed-EINI will be demonstrated through both theoretical analysis
and extensive numerical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaolin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shuai Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kai Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1"&gt;Hao Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zejin Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yongji Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Online Shopping Behaviors as a Proxy for Personal Lifestyle Choices: New Insights into Chronic Disease Prevention Literacy. (arXiv:2104.14281v3 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14281</id>
        <link href="http://arxiv.org/abs/2104.14281"/>
        <updated>2021-07-13T01:59:36.182Z</updated>
        <summary type="html"><![CDATA[Ubiquitous internet access is reshaping the way we live, but it is
accompanied by unprecedented challenges in preventing chronic diseases planted
by long exposure to unhealthy lifestyles. This paper proposes leveraging online
shopping behaviors as a proxy for personal lifestyle choices to improve chronic
disease prevention literacy targeted for times when e-commerce user experience
has been assimilated into most people's daily lives. Here, retrospective
longitudinal query logs and purchase records from millions of online shoppers
were accessed, constructing a broad spectrum of lifestyle features covering
assorted product categories and buyer personas. Using the lifestyle-related
information preceding their first purchases of prescription drugs, we could
determine associations between online shoppers' past lifestyle choices and
whether they suffered from a particular chronic disease or not. Novel lifestyle
risk factors were discovered in two exemplars -- depression and diabetes, most
of which showed cognitive congruence with existing healthcare knowledge.
Further, such empirical findings could be adopted to locate online shoppers at
high risk of these chronic diseases with fair accuracy, closely matching the
performance of screening surveys benchmarked against medical diagnosis.
Unobtrusive chronic disease surveillance via e-commerce sites may soon meet
consenting individuals in the digital space they already inhabit.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yongzhen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaozhong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borner_K/0/1/0/all/0/1"&gt;Katy B&amp;#xf6;rner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jun Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ju_Y/0/1/0/all/0/1"&gt;Yingnan Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changlong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1"&gt;Luo Si&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wasserstein Robust Classification with Fairness Constraints. (arXiv:2103.06828v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06828</id>
        <link href="http://arxiv.org/abs/2103.06828"/>
        <updated>2021-07-13T01:59:36.175Z</updated>
        <summary type="html"><![CDATA[We propose a distributionally robust classification model with a fairness
constraint that encourages the classifier to be fair in view of the equality of
opportunity criterion. We use a type-$\infty$ Wasserstein ambiguity set
centered at the empirical distribution to model distributional uncertainty and
derive a conservative reformulation for the worst-case equal opportunity
unfairness measure. We establish that the model is equivalent to a mixed binary
optimization problem, which can be solved by standard off-the-shelf solvers. To
improve scalability, we further propose a convex, hinge-loss-based model for
large problem instances whose reformulation does not incur any binary
variables. Moreover, we also consider the distributionally robust learning
problem with a generic ground transportation cost to hedge against the
uncertainties in the label and sensitive attribute. Finally, we numerically
demonstrate that our proposed approaches improve fairness with negligible loss
of predictive accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yijie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1"&gt;Viet Anh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanasusanto_G/0/1/0/all/0/1"&gt;Grani A. Hanasusanto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bilinear Classes: A Structural Framework for Provable Generalization in RL. (arXiv:2103.10897v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10897</id>
        <link href="http://arxiv.org/abs/2103.10897"/>
        <updated>2021-07-13T01:59:36.169Z</updated>
        <summary type="html"><![CDATA[This work introduces Bilinear Classes, a new structural framework, which
permit generalization in reinforcement learning in a wide variety of settings
through the use of function approximation. The framework incorporates nearly
all existing models in which a polynomial sample complexity is achievable, and,
notably, also includes new models, such as the Linear $Q^*/V^*$ model in which
both the optimal $Q$-function and the optimal $V$-function are linear in some
known feature space. Our main result provides an RL algorithm which has
polynomial sample complexity for Bilinear Classes; notably, this sample
complexity is stated in terms of a reduction to the generalization error of an
underlying supervised learning sub-problem. These bounds nearly match the best
known sample complexity bounds for existing models. Furthermore, this framework
also extends to the infinite dimensional (RKHS) setting: for the the Linear
$Q^*/V^*$ model, linear MDPs, and linear mixture MDPs, we provide sample
complexities that have no explicit dependence on the explicit feature dimension
(which could be infinite), but instead depends only on information theoretic
quantities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1"&gt;Simon S. Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1"&gt;Sham M. Kakade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jason D. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lovett_S/0/1/0/all/0/1"&gt;Shachar Lovett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahajan_G/0/1/0/all/0/1"&gt;Gaurav Mahajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wen Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruosong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accuracy Improvement for Fully Convolutional Networks via Selective Augmentation with Applications to Electrocardiogram Data. (arXiv:2104.12284v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12284</id>
        <link href="http://arxiv.org/abs/2104.12284"/>
        <updated>2021-07-13T01:59:36.163Z</updated>
        <summary type="html"><![CDATA[Deep learning methods have shown suitability for time series classification
in the health and medical domain, with promising results for electrocardiogram
data classification. Successful identification of myocardial infarction holds
life saving potential and any meaningful improvement upon deep learning models
in this area is of great interest. Conventionally, data augmentation methods
are applied universally to the training set when data are limited in order to
ameliorate data resolution or sample size. In the method proposed in this
study, data augmentation was not applied in the context of data scarcity.
Instead, samples that yield low confidence predictions were selectively
augmented in order to bolster the model's sensitivity to features or patterns
less strongly associated with a given class. This approach was tested for
improving the performance of a Fully Convolutional Network. The proposed
approach achieved 90 percent accuracy for classifying myocardial infarction as
opposed to 82 percent accuracy for the baseline, a marked improvement. Further,
the accuracy of the proposed approach was optimal near a defined upper
threshold for qualifying low confidence samples and decreased as this threshold
was raised to include higher confidence samples. This suggests exclusively
selecting lower confidence samples for data augmentation comes with distinct
benefits for electrocardiogram data classification with Fully Convolutional
Networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jacaruso_L/0/1/0/all/0/1"&gt;Lucas Cassiel Jacaruso&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Node Co-occurrence based Dual Quaternion Graph Neural Networks for Knowledge Graph Link Prediction. (arXiv:2104.07396v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07396</id>
        <link href="http://arxiv.org/abs/2104.07396"/>
        <updated>2021-07-13T01:59:36.146Z</updated>
        <summary type="html"><![CDATA[We introduce a novel embedding model, named NoGE, which aims to integrate
co-occurrence among entities and relations into graph neural networks to
improve knowledge graph completion (i.e., link prediction). Given a knowledge
graph, NoGE constructs a single graph considering entities and relations as
individual nodes. NoGE then computes weights for edges among nodes based on the
co-occurrence of entities and relations. Next, NoGE proposes Dual Quaternion
Graph Neural Networks (Dual-QGNN) and utilizes Dual-QGNN to update vector
representations for entity and relation nodes. NoGE then adopts a score
function to produce the triple scores. Comprehensive experimental results show
that NoGE obtains state-of-the-art results on three new and difficult benchmark
datasets CoDEx for knowledge graph completion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Dai Quoc Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_V/0/1/0/all/0/1"&gt;Vinh Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1"&gt;Dinh Phung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Dat Quoc Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Attentive Ensemble Transformer: Representing Ensemble Interactions in Neural Networks for Earth System Models. (arXiv:2106.13924v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13924</id>
        <link href="http://arxiv.org/abs/2106.13924"/>
        <updated>2021-07-13T01:59:36.130Z</updated>
        <summary type="html"><![CDATA[Ensemble data from Earth system models has to be calibrated and
post-processed. I propose a novel member-by-member post-processing approach
with neural networks. I bridge ideas from ensemble data assimilation with
self-attention, resulting into the self-attentive ensemble transformer. Here,
interactions between ensemble members are represented as additive and dynamic
self-attentive part. As proof-of-concept, I regress global ECMWF ensemble
forecasts to 2-metre-temperature fields from the ERA5 reanalysis. I demonstrate
that the ensemble transformer can calibrate the ensemble spread and extract
additional information from the ensemble. As it is a member-by-member approach,
the ensemble transformer directly outputs multivariate and spatially-coherent
ensemble members. Therefore, self-attention and the transformer technique can
be a missing piece for a non-parametric post-processing of ensemble data with
neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Finn_T/0/1/0/all/0/1"&gt;Tobias Sebastian Finn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memory-based Deep Reinforcement Learning for POMDP. (arXiv:2102.12344v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12344</id>
        <link href="http://arxiv.org/abs/2102.12344"/>
        <updated>2021-07-13T01:59:36.124Z</updated>
        <summary type="html"><![CDATA[A promising characteristic of Deep Reinforcement Learning (DRL) is its
capability to learn optimal policy in an end-to-end manner without relying on
feature engineering. However, most approaches assume a fully observable state
space, i.e. fully observable Markov Decision Process (MDP). In real-world
robotics, this assumption is unpractical, because of the sensor issues such as
sensors' capacity limitation and sensor noise, and the lack of knowledge about
if the observation design is complete or not. These scenarios lead to Partially
Observable MDP (POMDP) and need special treatment. In this paper, we propose
Long-Short-Term-Memory-based Twin Delayed Deep Deterministic Policy Gradient
(LSTM-TD3) by introducing a memory component to TD3, and compare its
performance with other DRL algorithms in both MDPs and POMDPs. Our results
demonstrate the significant advantages of the memory component in addressing
POMDPs, including the ability to handle missing and noisy observation data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1"&gt;Lingheng Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gorbet_R/0/1/0/all/0/1"&gt;Rob Gorbet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulic_D/0/1/0/all/0/1"&gt;Dana Kuli&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosted Embeddings for Time Series Forecasting. (arXiv:2104.04781v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04781</id>
        <link href="http://arxiv.org/abs/2104.04781"/>
        <updated>2021-07-13T01:59:36.084Z</updated>
        <summary type="html"><![CDATA[Time series forecasting is a fundamental task emerging from diverse
data-driven applications. Many advanced autoregressive methods such as ARIMA
were used to develop forecasting models. Recently, deep learning based methods
such as DeepAr, NeuralProphet, Seq2Seq have been explored for time series
forecasting problem. In this paper, we propose a novel time series forecast
model, DeepGB. We formulate and implement a variant of Gradient boosting
wherein the weak learners are DNNs whose weights are incrementally found in a
greedy manner over iterations. In particular, we develop a new embedding
architecture that improves the performance of many deep learning models on time
series using Gradient boosting variant. We demonstrate that our model
outperforms existing comparable state-of-the-art models using real-world sensor
data and public dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karingula_S/0/1/0/all/0/1"&gt;Sankeerth Rao Karingula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramanan_N/0/1/0/all/0/1"&gt;Nandini Ramanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tahmasbi_R/0/1/0/all/0/1"&gt;Rasool Tahmasbi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amjadi_M/0/1/0/all/0/1"&gt;Mehrnaz Amjadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1"&gt;Deokwoo Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_R/0/1/0/all/0/1"&gt;Ricky Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thimmisetty_C/0/1/0/all/0/1"&gt;Charanraj Thimmisetty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cabrera_L/0/1/0/all/0/1"&gt;Luisa Polania Cabrera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sayer_M/0/1/0/all/0/1"&gt;Marjorie Sayer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coelho_C/0/1/0/all/0/1"&gt;Claudionor Nunes Coelho Jr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Task-Optimal Exploration in Linear Dynamical Systems. (arXiv:2102.05214v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05214</id>
        <link href="http://arxiv.org/abs/2102.05214"/>
        <updated>2021-07-13T01:59:36.068Z</updated>
        <summary type="html"><![CDATA[Exploration in unknown environments is a fundamental problem in reinforcement
learning and control. In this work, we study task-guided exploration and
determine what precisely an agent must learn about their environment in order
to complete a particular task. Formally, we study a broad class of
decision-making problems in the setting of linear dynamical systems, a class
that includes the linear quadratic regulator problem. We provide instance- and
task-dependent lower bounds which explicitly quantify the difficulty of
completing a task of interest. Motivated by our lower bound, we propose a
computationally efficient experiment-design based exploration algorithm. We
show that it optimally explores the environment, collecting precisely the
information needed to complete the task, and provide finite-time bounds
guaranteeing that it achieves the instance- and task-optimal sample complexity,
up to constant factors. Through several examples of the LQR problem, we show
that performing task-guided exploration provably improves on exploration
schemes which do not take into account the task of interest. Along the way, we
establish that certainty equivalence decision making is instance- and
task-optimal, and obtain the first algorithm for the linear quadratic regulator
problem which is instance-optimal. We conclude with several experiments
illustrating the effectiveness of our approach in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wagenmaker_A/0/1/0/all/0/1"&gt;Andrew Wagenmaker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1"&gt;Max Simchowitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jamieson_K/0/1/0/all/0/1"&gt;Kevin Jamieson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Discriminative Feature Learning for Deep Multi-view Clustering. (arXiv:2103.15069v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15069</id>
        <link href="http://arxiv.org/abs/2103.15069"/>
        <updated>2021-07-13T01:59:36.061Z</updated>
        <summary type="html"><![CDATA[Multi-view clustering is an important research topic due to its capability to
utilize complementary information from multiple views. However, there are few
methods to consider the negative impact caused by certain views with unclear
clustering structures, resulting in poor multi-view clustering performance. To
address this drawback, we propose self-supervised discriminative feature
learning for deep multi-view clustering (SDMVC). Concretely, deep autoencoders
are applied to learn embedded features for each view independently. To leverage
the multi-view complementary information, we concatenate all views' embedded
features to form the global features, which can overcome the negative impact of
some views' unclear clustering structures. In a self-supervised manner,
pseudo-labels are obtained to build a unified target distribution to perform
multi-view discriminative feature learning. During this process, global
discriminative information can be mined to supervise all views to learn more
discriminative features, which in turn are used to update the target
distribution. Besides, this unified target distribution can make SDMVC learn
consistent cluster assignments, which accomplishes the clustering consistency
of multiple views while preserving their features' diversity. Experiments on
various types of multi-view datasets show that SDMVC achieves state-of-the-art
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jie Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yazhou Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Huayi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhimeng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1"&gt;Lili Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_X/0/1/0/all/0/1"&gt;Xiaorong Pu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges. (arXiv:2103.11251v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11251</id>
        <link href="http://arxiv.org/abs/2103.11251"/>
        <updated>2021-07-13T01:59:36.055Z</updated>
        <summary type="html"><![CDATA[Interpretability in machine learning (ML) is crucial for high stakes
decisions and troubleshooting. In this work, we provide fundamental principles
for interpretable ML, and dispel common misunderstandings that dilute the
importance of this crucial topic. We also identify 10 technical challenge areas
in interpretable machine learning and provide history and background on each
problem. Some of these problems are classically important, and some are recent
problems that have arisen in the last few years. These problems are: (1)
Optimizing sparse logical models such as decision trees; (2) Optimization of
scoring systems; (3) Placing constraints into generalized additive models to
encourage sparsity and better interpretability; (4) Modern case-based
reasoning, including neural networks and matching for causal inference; (5)
Complete supervised disentanglement of neural networks; (6) Complete or even
partial unsupervised disentanglement of neural networks; (7) Dimensionality
reduction for data visualization; (8) Machine learning models that can
incorporate physics and other generative or causal constraints; (9)
Characterization of the "Rashomon set" of good models; and (10) Interpretable
reinforcement learning. This survey is suitable as a starting point for
statisticians and computer scientists interested in working in interpretable
machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rudin_C/0/1/0/all/0/1"&gt;Cynthia Rudin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chaofan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Haiyang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Semenova_L/0/1/0/all/0/1"&gt;Lesia Semenova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1"&gt;Chudi Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifying Helpful Sentences in Product Reviews. (arXiv:2104.09792v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09792</id>
        <link href="http://arxiv.org/abs/2104.09792"/>
        <updated>2021-07-13T01:59:36.046Z</updated>
        <summary type="html"><![CDATA[In recent years online shopping has gained momentum and became an important
venue for customers wishing to save time and simplify their shopping process. A
key advantage of shopping online is the ability to read what other customers
are saying about products of interest. In this work, we aim to maintain this
advantage in situations where extreme brevity is needed, for example, when
shopping by voice. We suggest a novel task of extracting a single
representative helpful sentence from a set of reviews for a given product. The
selected sentence should meet two conditions: first, it should be helpful for a
purchase decision and second, the opinion it expresses should be supported by
multiple reviewers. This task is closely related to the task of Multi Document
Summarization in the product reviews domain but differs in its objective and
its level of conciseness. We collect a dataset in English of sentence
helpfulness scores via crowd-sourcing and demonstrate its reliability despite
the inherent subjectivity involved. Next, we describe a complete model that
extracts representative helpful sentences with positive and negative sentiment
towards the product and demonstrate that it outperforms several baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gamzu_I/0/1/0/all/0/1"&gt;Iftah Gamzu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonen_H/0/1/0/all/0/1"&gt;Hila Gonen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kutiel_G/0/1/0/all/0/1"&gt;Gilad Kutiel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1"&gt;Ran Levy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agichtein_E/0/1/0/all/0/1"&gt;Eugene Agichtein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer. (arXiv:2102.09550v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09550</id>
        <link href="http://arxiv.org/abs/2102.09550"/>
        <updated>2021-07-13T01:59:36.039Z</updated>
        <summary type="html"><![CDATA[We address the challenging problem of Natural Language Comprehension beyond
plain-text documents by introducing the TILT neural network architecture which
simultaneously learns layout information, visual features, and textual
semantics. Contrary to previous approaches, we rely on a decoder capable of
unifying a variety of problems involving natural language. The layout is
represented as an attention bias and complemented with contextualized visual
information, while the core of our model is a pretrained encoder-decoder
Transformer. Our novel approach achieves state-of-the-art results in extracting
information from documents and answering questions which demand layout
understanding (DocVQA, CORD, SROIE). At the same time, we simplify the process
by employing an end-to-end model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Powalski_R/0/1/0/all/0/1"&gt;Rafa&amp;#x142; Powalski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borchmann_L/0/1/0/all/0/1"&gt;&amp;#x141;ukasz Borchmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jurkiewicz_D/0/1/0/all/0/1"&gt;Dawid Jurkiewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dwojak_T/0/1/0/all/0/1"&gt;Tomasz Dwojak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pietruszka_M/0/1/0/all/0/1"&gt;Micha&amp;#x142; Pietruszka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palka_G/0/1/0/all/0/1"&gt;Gabriela Pa&amp;#x142;ka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Online Reward Shaping in Sparse-Reward Environments. (arXiv:2103.04529v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04529</id>
        <link href="http://arxiv.org/abs/2103.04529"/>
        <updated>2021-07-13T01:59:36.032Z</updated>
        <summary type="html"><![CDATA[We propose a novel reinforcement learning framework that performs
self-supervised online reward shaping, yielding faster, sample efficient
performance in sparse-reward environments. The proposed framework alternates
between updating a policy and inferring a reward function. While the policy
update is performed with the inferred, potentially dense reward function, the
original sparse reward is used to provide a self-supervisory signal for the
reward update by serving as an ordering over the observed trajectories. The
proposed framework is based on the theory that altering the reward function
does not affect the optimal policy of the original MDP as long as certain
relations between the altered and the original reward are maintained. We name
the proposed framework ClAssification-based Reward Shaping (CaReS), since the
altered reward is learned in a self-supervised manner using classifier-based
reward inference. Experimental results on several sparse-reward environments
demonstrate that the proposed algorithm is not only significantly more sample
efficient than the state-of-the-art reinforcement learning baseline but also
achieves a similar sample efficiency to a baseline that uses hand-designed
dense reward functions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Memarian_F/0/1/0/all/0/1"&gt;Farzan Memarian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goo_W/0/1/0/all/0/1"&gt;Wonjoon Goo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lioutikov_R/0/1/0/all/0/1"&gt;Rudolf Lioutikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1"&gt;Ufuk Topcu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1"&gt;Scott Niekum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enabling collaborative data science development with the Ballet framework. (arXiv:2012.07816v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07816</id>
        <link href="http://arxiv.org/abs/2012.07816"/>
        <updated>2021-07-13T01:59:36.000Z</updated>
        <summary type="html"><![CDATA[While the open-source software development model has led to successful
large-scale collaborations in building software systems, data science projects
are frequently developed by individuals or small teams. We describe challenges
to scaling data science collaborations and present a conceptual framework and
ML programming model to address them. We instantiate these ideas in Ballet, a
lightweight framework for collaborative, open-source data science through a
focus on feature engineering, and an accompanying cloud-based development
environment. Using our framework, collaborators incrementally propose feature
definitions to a repository which are each subjected to an ML performance
evaluation and can be automatically merged into an executable feature
engineering pipeline. We leverage Ballet to conduct a case study analysis of an
income prediction problem with 27 collaborators, and discuss implications for
future designers of collaborative projects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smith_M/0/1/0/all/0/1"&gt;Micah J. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cito_J/0/1/0/all/0/1"&gt;J&amp;#xfc;rgen Cito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1"&gt;Kelvin Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veeramachaneni_K/0/1/0/all/0/1"&gt;Kalyan Veeramachaneni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixed Membership Graph Clustering via Systematic Edge Query. (arXiv:2011.12988v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12988</id>
        <link href="http://arxiv.org/abs/2011.12988"/>
        <updated>2021-07-13T01:59:35.994Z</updated>
        <summary type="html"><![CDATA[This work considers clustering nodes of a largely incomplete graph. Under the
problem setting, only a small amount of queries about the edges can be made,
but the entire graph is not observable. This problem finds applications in
large-scale data clustering using limited annotations, community detection
under restricted survey resources, and graph topology inference under
hidden/removed node interactions. Prior works tackled this problem from various
perspectives, e.g., convex programming-based low-rank matrix completion and
active query-based clique finding. Nonetheless, many existing methods are
designed for estimating the single-cluster membership of the nodes, but nodes
may often have mixed (i.e., multi-cluster) membership in practice. Some query
and computational paradigms, e.g., the random query patterns and nuclear
norm-based optimization advocated in the convex approaches, may give rise to
scalability and implementation challenges. This work aims at learning mixed
membership of nodes using queried edges. The proposed method is developed
together with a systematic query principle that can be controlled and adjusted
by the system designers to accommodate implementation challenges -- e.g., to
avoid querying edges that are physically hard to acquire. Our framework also
features a lightweight and scalable algorithm with membership learning
guarantees. Real-data experiments on crowdclustering and community detection
are used to showcase the effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ibrahim_S/0/1/0/all/0/1"&gt;Shahana Ibrahim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1"&gt;Xiao Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Attention Network: Accelerate Attention by Searching Where to Plug. (arXiv:2011.14058v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14058</id>
        <link href="http://arxiv.org/abs/2011.14058"/>
        <updated>2021-07-13T01:59:35.987Z</updated>
        <summary type="html"><![CDATA[Recently, many plug-and-play self-attention modules are proposed to enhance
the model generalization by exploiting the internal information of deep
convolutional neural networks (CNNs). Previous works lay an emphasis on the
design of attention module for specific functionality, e.g., light-weighted or
task-oriented attention. However, they ignore the importance of where to plug
in the attention module since they connect the modules individually with each
block of the entire CNN backbone for granted, leading to incremental
computational cost and number of parameters with the growth of network depth.
Thus, we propose a framework called Efficient Attention Network (EAN) to
improve the efficiency for the existing attention modules. In EAN, we leverage
the sharing mechanism (Huang et al. 2020) to share the attention module within
the backbone and search where to connect the shared attention module via
reinforcement learning. Finally, we obtain the attention network with sparse
connections between the backbone and modules, while (1) maintaining accuracy
(2) reducing extra parameter increment and (3) accelerating inference.
Extensive experiments on widely-used benchmarks and popular attention networks
show the effectiveness of EAN. Furthermore, we empirically illustrate that our
EAN has the capacity of transferring to other tasks and capturing the
informative features. The code is available at
https://github.com/gbup-group/EAN-efficient-attention-network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhongzhan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1"&gt;Senwei Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1"&gt;Mingfu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1"&gt;Wei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haizhao Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WheaCha: A Method for Explaining the Predictions of Code Summarization Models. (arXiv:2102.04625v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04625</id>
        <link href="http://arxiv.org/abs/2102.04625"/>
        <updated>2021-07-13T01:59:35.980Z</updated>
        <summary type="html"><![CDATA[The last decade has witnessed a rapid advance in machine learning models.
While the black-box nature of these systems allows powerful predictions, it
cannot be directly explained, posing a threat to the continuing democratization
of machine learning technology.

Tackling the challenge of model explainability, research has made significant
progress in demystifying the image classification models. In the same spirit of
these works, this paper studies code summarization models, particularly, given
an input program for which a model makes a prediction, our goal is to reveal
the key features that the model uses for predicting the label of the program.
We realize our approach in HouYi, which we use to evaluate four prominent code
summarization models: extreme summarizer, code2vec, code2seq, and sequence GNN.
Results show that all models base their predictions on syntactic and lexical
properties with little to none semantic implication. Based on this finding, we
present a novel approach to explaining the predictions of code summarization
models through the lens of training data.

Our work opens up this exciting, new direction of studying what models have
learned from source code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Ke Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Linzhang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Graph Dictionary Learning. (arXiv:2102.06555v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06555</id>
        <link href="http://arxiv.org/abs/2102.06555"/>
        <updated>2021-07-13T01:59:35.966Z</updated>
        <summary type="html"><![CDATA[Dictionary learning is a key tool for representation learning, that explains
the data as linear combination of few basic elements. Yet, this analysis is not
amenable in the context of graph learning, as graphs usually belong to
different metric spaces. We fill this gap by proposing a new online Graph
Dictionary Learning approach, which uses the Gromov Wasserstein divergence for
the data fitting term. In our work, graphs are encoded through their nodes'
pairwise relations and modeled as convex combination of graph atoms, i.e.
dictionary elements, estimated thanks to an online stochastic algorithm, which
operates on a dataset of unregistered graphs with potentially different number
of nodes. Our approach naturally extends to labeled graphs, and is completed by
a novel upper bound that can be used as a fast approximation of Gromov
Wasserstein in the embedding space. We provide numerical evidences showing the
interest of our approach for unsupervised embedding of graph datasets and for
online graph subspace estimation and tracking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vincent_Cuaz_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric Vincent-Cuaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vayer_T/0/1/0/all/0/1"&gt;Titouan Vayer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flamary_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Flamary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Corneli_M/0/1/0/all/0/1"&gt;Marco Corneli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courty_N/0/1/0/all/0/1"&gt;Nicolas Courty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Electromagnetic Source Imaging via a Data-Synthesis-Based Denoising Autoencoder. (arXiv:2010.12876v4 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12876</id>
        <link href="http://arxiv.org/abs/2010.12876"/>
        <updated>2021-07-13T01:59:35.929Z</updated>
        <summary type="html"><![CDATA[Electromagnetic source imaging (ESI) is a highly ill-posed inverse problem.
To find a unique solution, traditional ESI methods impose a variety of priors
that may not reflect the actual source properties. Such limitations of
traditional ESI methods hinder their further applications. Inspired by deep
learning approaches, a novel data-synthesized spatio-temporal denoising
autoencoder method (DST-DAE) method was proposed to solve the ESI inverse
problem. Unlike the traditional methods, we utilize a neural network to
directly seek generalized mapping from the measured E/MEG signals to the
cortical sources. A novel data synthesis strategy is employed by introducing
the prior information of sources to the generated large-scale samples using the
forward model of ESI. All the generated data are used to drive the neural
network to automatically learn inverse mapping. To achieve better estimation
performance, a denoising autoencoder (DAE) architecture with spatio-temporal
feature extraction blocks is designed. Compared with the traditional methods,
we show (1) that the novel deep learning approach provides an effective and
easy-to-apply way to solve the ESI problem, that (2) compared to traditional
methods, DST-DAE with the data synthesis strategy can better consider the
characteristics of real sources than the mathematical formulation of prior
assumptions, and that (3) the specifically designed architecture of DAE can not
only provide a better estimation of source signals but also be robust to noise
pollution. Extensive numerical experiments show that the proposed method is
superior to the traditional knowledge-driven ESI methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Huang_G/0/1/0/all/0/1"&gt;Gexin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhu Liang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_K/0/1/0/all/0/1"&gt;Ke Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gu_Z/0/1/0/all/0/1"&gt;ZhengHui Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qi_F/0/1/0/all/0/1"&gt;Feifei Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;YuanQing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jiawen Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Selection Based on Sparse Neural Network Layer with Normalizing Constraints. (arXiv:2012.06365v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06365</id>
        <link href="http://arxiv.org/abs/2012.06365"/>
        <updated>2021-07-13T01:59:35.923Z</updated>
        <summary type="html"><![CDATA[Feature selection is important step in machine learning since it has shown to
improve prediction accuracy while depressing the curse of dimensionality of
high dimensional data. The neural networks have experienced tremendous success
in solving many nonlinear learning problems. Here, we propose new
neural-network based feature selection approach that introduces two constrains,
the satisfying of which leads to sparse FS layer. We have performed extensive
experiments on synthetic and real world data to evaluate performance of the
proposed FS. In experiments we focus on the high dimension, low sample size
data since those represent the main challenge for feature selection. The
results confirm that proposed Feature Selection Based on Sparse Neural Network
Layer with Normalizing Constraints (SNEL-FS) is able to select the important
features and yields superior performance compared to other conventional FS
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bugata_P/0/1/0/all/0/1"&gt;Peter Bugata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drotar_P/0/1/0/all/0/1"&gt;Peter Drotar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unified lower bounds for interactive high-dimensional estimation under information constraints. (arXiv:2010.06562v4 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.06562</id>
        <link href="http://arxiv.org/abs/2010.06562"/>
        <updated>2021-07-13T01:59:35.917Z</updated>
        <summary type="html"><![CDATA[We consider the task of distributed parameter estimation using interactive
protocols subject to local information constraints such as bandwidth
limitations, local differential privacy, and restricted measurements. We
provide a unified framework enabling us to derive a variety of (tight) minimax
lower bounds for different parametric families of distributions, both
continuous and discrete, under any $\ell_p$ loss. Our lower bound framework is
versatile and yields "plug-and-play" bounds that are widely applicable to a
large range of estimation problems. In particular, our approach recovers bounds
obtained using data processing inequalities and Cram\'er--Rao bounds, two other
alternative approaches for proving lower bounds in our setting of interest.
Further, for the families considered, we complement our lower bounds with
matching upper bounds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Acharya_J/0/1/0/all/0/1"&gt;Jayadev Acharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Canonne_C/0/1/0/all/0/1"&gt;Cl&amp;#xe9;ment L. Canonne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Ziteng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tyagi_H/0/1/0/all/0/1"&gt;Himanshu Tyagi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse sketches with small inversion bias. (arXiv:2011.10695v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10695</id>
        <link href="http://arxiv.org/abs/2011.10695"/>
        <updated>2021-07-13T01:59:35.910Z</updated>
        <summary type="html"><![CDATA[For a tall $n\times d$ matrix $A$ and a random $m\times n$ sketching matrix
$S$, the sketched estimate of the inverse covariance matrix $(A^\top A)^{-1}$
is typically biased: $E[(\tilde A^\top\tilde A)^{-1}]\ne(A^\top A)^{-1}$, where
$\tilde A=SA$. This phenomenon, which we call inversion bias, arises, e.g., in
statistics and distributed optimization, when averaging multiple independently
constructed estimates of quantities that depend on the inverse covariance. We
develop a framework for analyzing inversion bias, based on our proposed concept
of an $(\epsilon,\delta)$-unbiased estimator for random matrices. We show that
when the sketching matrix $S$ is dense and has i.i.d. sub-gaussian entries,
then after simple rescaling, the estimator $(\frac m{m-d}\tilde A^\top\tilde
A)^{-1}$ is $(\epsilon,\delta)$-unbiased for $(A^\top A)^{-1}$ with a sketch of
size $m=O(d+\sqrt d/\epsilon)$. This implies that for $m=O(d)$, the inversion
bias of this estimator is $O(1/\sqrt d)$, which is much smaller than the
$\Theta(1)$ approximation error obtained as a consequence of the subspace
embedding guarantee for sub-gaussian sketches. We then propose a new sketching
technique, called LEverage Score Sparsified (LESS) embeddings, which uses ideas
from both data-oblivious sparse embeddings as well as data-aware leverage-based
row sampling methods, to get $\epsilon$ inversion bias for sketch size
$m=O(d\log d+\sqrt d/\epsilon)$ in time $O(\text{nnz}(A)\log n+md^2)$, where
nnz is the number of non-zeros. The key techniques enabling our analysis
include an extension of a classical inequality of Bai and Silverstein for
random quadratic forms, which we call the Restricted Bai-Silverstein
inequality; and anti-concentration of the Binomial distribution via the
Paley-Zygmund inequality, which we use to prove a lower bound showing that
leverage score sampling sketches generally do not achieve small inversion bias.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Derezinski_M/0/1/0/all/0/1"&gt;Micha&amp;#x142; Derezi&amp;#x144;ski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1"&gt;Zhenyu Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dobriban_E/0/1/0/all/0/1"&gt;Edgar Dobriban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1"&gt;Michael W. Mahoney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy Preserving Domain Adaptation for Semantic Segmentation of Medical Images. (arXiv:2101.00522v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00522</id>
        <link href="http://arxiv.org/abs/2101.00522"/>
        <updated>2021-07-13T01:59:35.904Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) have led to significant improvements in
tasks involving semantic segmentation of images. CNNs are vulnerable in the
area of biomedical image segmentation because of distributional gap between two
source and target domains with different data modalities which leads to domain
shift. Domain shift makes data annotations in new modalities necessary because
models must be retrained from scratch. Unsupervised domain adaptation (UDA) is
proposed to adapt a model to new modalities using solely unlabeled target
domain data. Common UDA algorithms require access to data points in the source
domain which may not be feasible in medical imaging due to privacy concerns. In
this work, we develop an algorithm for UDA in a privacy-constrained setting,
where the source domain data is inaccessible. Our idea is based on encoding the
information from the source samples into a prototypical distribution that is
used as an intermediate distribution for aligning the target domain
distribution with the source domain distribution. We demonstrate the
effectiveness of our algorithm by comparing it to state-of-the-art medical
image semantic segmentation approaches on two medical image semantic
segmentation datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stan_S/0/1/0/all/0/1"&gt;Serban Stan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1"&gt;Mohammad Rostami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Multilingual Neural Machine Translation For Low-Resource Languages: French,English - Vietnamese. (arXiv:2012.08743v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08743</id>
        <link href="http://arxiv.org/abs/2012.08743"/>
        <updated>2021-07-13T01:59:35.898Z</updated>
        <summary type="html"><![CDATA[Prior works have demonstrated that a low-resource language pair can benefit
from multilingual machine translation (MT) systems, which rely on many language
pairs' joint training. This paper proposes two simple strategies to address the
rare word issue in multilingual MT systems for two low-resource language pairs:
French-Vietnamese and English-Vietnamese. The first strategy is about dynamical
learning word similarity of tokens in the shared space among source languages
while another one attempts to augment the translation ability of rare words
through updating their embeddings during the training. Besides, we leverage
monolingual data for multilingual MT systems to increase the amount of
synthetic parallel corpora while dealing with the data sparsity problem. We
have shown significant improvements of up to +1.62 and +2.54 BLEU points over
the bilingual baseline systems for both language pairs and released our
datasets for the research community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_T/0/1/0/all/0/1"&gt;Thi-Vinh Ngo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1"&gt;Phuong-Thai Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ha_T/0/1/0/all/0/1"&gt;Thanh-Le Ha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dinh_K/0/1/0/all/0/1"&gt;Khac-Quy Dinh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1"&gt;Le-Minh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Transformer Growth for Progressive BERT Training. (arXiv:2010.12562v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12562</id>
        <link href="http://arxiv.org/abs/2010.12562"/>
        <updated>2021-07-13T01:59:35.876Z</updated>
        <summary type="html"><![CDATA[Due to the excessive cost of large-scale language model pre-training,
considerable efforts have been made to train BERT progressively -- start from
an inferior but low-cost model and gradually grow the model to increase the
computational complexity. Our objective is to advance the understanding of
Transformer growth and discover principles that guide progressive training.
First, we find that similar to network architecture search, Transformer growth
also favors compound scaling. Specifically, while existing methods only conduct
network growth in a single dimension, we observe that it is beneficial to use
compound growth operators and balance multiple dimensions (e.g., depth, width,
and input length of the model). Moreover, we explore alternative growth
operators in each dimension via controlled comparison to give operator
selection practical guidance. In light of our analyses, the proposed method
speeds up BERT pre-training by 73.6% and 82.2% for the base and large models
respectively, while achieving comparable performances]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1"&gt;Xiaotao Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Liyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hongkun Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiawei Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Long-tail learning via logit adjustment. (arXiv:2007.07314v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.07314</id>
        <link href="http://arxiv.org/abs/2007.07314"/>
        <updated>2021-07-13T01:59:35.867Z</updated>
        <summary type="html"><![CDATA[Real-world classification problems typically exhibit an imbalanced or
long-tailed label distribution, wherein many labels are associated with only a
few samples. This poses a challenge for generalisation on such labels, and also
makes na\"ive learning biased towards dominant labels. In this paper, we
present two simple modifications of standard softmax cross-entropy training to
cope with these challenges. Our techniques revisit the classic idea of logit
adjustment based on the label frequencies, either applied post-hoc to a trained
model, or enforced in the loss during training. Such adjustment encourages a
large relative margin between logits of rare versus dominant labels. These
techniques unify and generalise several recent proposals in the literature,
while possessing firmer statistical grounding and empirical performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1"&gt;Aditya Krishna Menon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jayasumana_S/0/1/0/all/0/1"&gt;Sadeep Jayasumana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rawat_A/0/1/0/all/0/1"&gt;Ankit Singh Rawat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_H/0/1/0/all/0/1"&gt;Himanshu Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1"&gt;Andreas Veit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sanjiv Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Message Passing Adaptive Resonance Theory for Online Active Semi-supervised Learning. (arXiv:2012.01227v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01227</id>
        <link href="http://arxiv.org/abs/2012.01227"/>
        <updated>2021-07-13T01:59:35.830Z</updated>
        <summary type="html"><![CDATA[Active learning is widely used to reduce labeling effort and training time by
repeatedly querying only the most beneficial samples from unlabeled data. In
real-world problems where data cannot be stored indefinitely due to limited
storage or privacy issues, the query selection and the model update should be
performed as soon as a new data sample is observed. Various online active
learning methods have been studied to deal with these challenges; however,
there are difficulties in selecting representative query samples and updating
the model efficiently without forgetting. In this study, we propose Message
Passing Adaptive Resonance Theory (MPART) that learns the distribution and
topology of input data online. Through message passing on the topological
graph, MPART actively queries informative and representative samples, and
continuously improves the classification performance using both labeled and
unlabeled data. We evaluate our model in stream-based selective sampling
scenarios with comparable query selection strategies, showing that MPART
significantly outperforms competitive models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1"&gt;Taehyeong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_I/0/1/0/all/0/1"&gt;Injune Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hyundo Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyunseo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_W/0/1/0/all/0/1"&gt;Won-Seok Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1"&gt;Joseph J. Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Byoung-Tak Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Building population models for large-scale neural recordings: opportunities and pitfalls. (arXiv:2102.01807v4 [q-bio.NC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01807</id>
        <link href="http://arxiv.org/abs/2102.01807"/>
        <updated>2021-07-13T01:59:35.812Z</updated>
        <summary type="html"><![CDATA[Modern recording technologies now enable simultaneous recording from large
numbers of neurons. This has driven the development of new statistical models
for analyzing and interpreting neural population activity. Here we provide a
broad overview of recent developments in this area. We compare and contrast
different approaches, highlight strengths and limitations, and discuss
biological and mechanistic insights that these methods provide.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Hurwitz_C/0/1/0/all/0/1"&gt;Cole Hurwitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kudryashova_N/0/1/0/all/0/1"&gt;Nina Kudryashova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Onken_A/0/1/0/all/0/1"&gt;Arno Onken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hennig_M/0/1/0/all/0/1"&gt;Matthias H. Hennig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Targeted VAE: Variational and Targeted Learning for Causal Inference. (arXiv:2009.13472v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.13472</id>
        <link href="http://arxiv.org/abs/2009.13472"/>
        <updated>2021-07-13T01:59:35.805Z</updated>
        <summary type="html"><![CDATA[Undertaking causal inference with observational data is incredibly useful
across a wide range of tasks including the development of medical treatments,
advertisements and marketing, and policy making. There are two significant
challenges associated with undertaking causal inference using observational
data: treatment assignment heterogeneity (i.e., differences between the treated
and untreated groups), and an absence of counterfactual data (i.e., not knowing
what would have happened if an individual who did get treatment, were instead
to have not been treated). We address these two challenges by combining
structured inference and targeted learning. In terms of structure, we factorize
the joint distribution into risk, confounding, instrumental, and miscellaneous
factors, and in terms of targeted learning, we apply a regularizer derived from
the influence curve in order to reduce residual bias. An ablation study is
undertaken, and an evaluation on benchmark datasets demonstrates that TVAE has
competitive and state of the art performance across.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Vowels_M/0/1/0/all/0/1"&gt;Matthew James Vowels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Camgoz_N/0/1/0/all/0/1"&gt;Necati Cihan Camgoz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bowden_R/0/1/0/all/0/1"&gt;Richard Bowden&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dense-Sparse Deep CNN Training for Image Denoising. (arXiv:2107.04857v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04857</id>
        <link href="http://arxiv.org/abs/2107.04857"/>
        <updated>2021-07-13T01:59:35.799Z</updated>
        <summary type="html"><![CDATA[Recently, deep learning (DL) methods such as convolutional neural networks
(CNNs) have gained prominence in the area of image denoising. This is owing to
their proven ability to surpass state-of-the-art classical image denoising
algorithms such as BM3D. Deep denoising CNNs (DnCNNs) use many feedforward
convolution layers with added regularization methods of batch normalization and
residual learning to improve denoising performance significantly. However, this
comes at the expense of a huge number of trainable parameters. In this paper,
we address this issue by reducing the number of parameters while achieving a
comparable level of performance. We derive motivation from the improved
performance obtained by training networks using the dense-sparse-dense (DSD)
training approach. We extend this training approach to a reduced DnCNN (RDnCNN)
network resulting in a faster denoising network with significantly reduced
parameters and comparable performance to the DnCNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Alawode_B/0/1/0/all/0/1"&gt;Basit O. Alawode&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Masood_M/0/1/0/all/0/1"&gt;Mudassir Masood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ballal_T/0/1/0/all/0/1"&gt;Tarig Ballal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Al_Naffouri_T/0/1/0/all/0/1"&gt;Tareq Al-Naffouri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nondeterminism and Instability in Neural Network Optimization. (arXiv:2103.04514v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04514</id>
        <link href="http://arxiv.org/abs/2103.04514"/>
        <updated>2021-07-13T01:59:35.791Z</updated>
        <summary type="html"><![CDATA[Nondeterminism in neural network optimization produces uncertainty in
performance, making small improvements difficult to discern from run-to-run
variability. While uncertainty can be reduced by training multiple model
copies, doing so is time-consuming, costly, and harms reproducibility. In this
work, we establish an experimental protocol for understanding the effect of
optimization nondeterminism on model diversity, allowing us to isolate the
effects of a variety of sources of nondeterminism. Surprisingly, we find that
all sources of nondeterminism have similar effects on measures of model
diversity. To explain this intriguing fact, we identify the instability of
model training, taken as an end-to-end procedure, as the key determinant. We
show that even one-bit changes in initial parameters result in models
converging to vastly different values. Last, we propose two approaches for
reducing the effects of instability on run-to-run variability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Summers_C/0/1/0/all/0/1"&gt;Cecilia Summers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dinneen_M/0/1/0/all/0/1"&gt;Michael J. Dinneen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[General Invertible Transformations for Flow-based Generative Modeling. (arXiv:2011.15056v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.15056</id>
        <link href="http://arxiv.org/abs/2011.15056"/>
        <updated>2021-07-13T01:59:35.775Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a new class of invertible transformations with an
application to flow-based generative models. We indicate that many well-known
invertible transformations in reversible logic and reversible neural networks
could be derived from our proposition. Next, we propose two new coupling layers
that are important building blocks of flow-based generative models. In the
experiments on digit data, we present how these new coupling layers could be
used in Integer Discrete Flows (IDF), and that they achieve better results than
standard coupling layers used in IDF and RealNVP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tomczak_J/0/1/0/all/0/1"&gt;Jakub M. Tomczak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Neural Models for Symbolic Regression at Scale. (arXiv:2007.10784v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.10784</id>
        <link href="http://arxiv.org/abs/2007.10784"/>
        <updated>2021-07-13T01:59:35.767Z</updated>
        <summary type="html"><![CDATA[Deep learning owes much of its success to the astonishing expressiveness of
neural networks. However, this comes at the cost of complex, black-boxed models
that extrapolate poorly beyond the domain of the training dataset, conflicting
with goals of finding analytic expressions to describe science, engineering and
real world data. Under the hypothesis that the hierarchical modularity of such
laws can be captured by training a neural network, we introduce OccamNet, a
neural network model that finds interpretable, compact, and sparse solutions
for fitting data, \`{a} la Occam's razor. Our model defines a probability
distribution over a non-differentiable function space. We introduce a two-step
optimization method that samples functions and updates the weights with
backpropagation based on cross-entropy matching in an evolutionary strategy: we
train by biasing the probability mass toward better fitting solutions. OccamNet
is able to fit a variety of symbolic laws including simple analytic functions,
recursive programs, implicit functions, simple image classification, and can
outperform noticeably state-of-the-art symbolic regression methods on real
world regression datasets. Our method requires minimal memory footprint, does
not require AI accelerators for efficient training, fits complicated functions
in minutes of training on a single CPU, and demonstrates significant
performance gains when scaled on a GPU. Our implementation, demonstrations and
instructions for reproducing the experiments are available at
https://github.com/druidowm/OccamNet_Public.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Costa_A/0/1/0/all/0/1"&gt;Allan Costa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dangovski_R/0/1/0/all/0/1"&gt;Rumen Dangovski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dugan_O/0/1/0/all/0/1"&gt;Owen Dugan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Samuel Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1"&gt;Pawan Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soljacic_M/0/1/0/all/0/1"&gt;Marin Solja&amp;#x10d;i&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacobson_J/0/1/0/all/0/1"&gt;Joseph Jacobson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STR-GODEs: Spatial-Temporal-Ridership Graph ODEs for Metro Ridership Prediction. (arXiv:2107.04980v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04980</id>
        <link href="http://arxiv.org/abs/2107.04980"/>
        <updated>2021-07-13T01:59:35.760Z</updated>
        <summary type="html"><![CDATA[The metro ridership prediction has always received extensive attention from
governments and researchers. Recent works focus on designing complicated graph
convolutional recurrent network architectures to capture spatial and temporal
patterns. These works extract the information of spatial dimension well, but
the limitation of temporal dimension still exists. We extended Neural ODE
algorithms to the graph network and proposed the STR-GODEs network, which can
effectively learn spatial, temporal, and ridership correlations without the
limitation of dividing data into equal-sized intervals on the timeline. While
learning the spatial relations and the temporal correlations, we modify the
GODE-RNN cell to obtain the ridership feature and hidden states. Ridership
information and its hidden states are added to the GODESolve to reduce the
error accumulation caused by long time series in prediction. Extensive
experiments on two large-scale datasets demonstrate the efficacy and robustness
of our model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chuyu Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RBM-Flow and D-Flow: Invertible Flows with Discrete Energy Base Spaces. (arXiv:2012.13196v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13196</id>
        <link href="http://arxiv.org/abs/2012.13196"/>
        <updated>2021-07-13T01:59:35.753Z</updated>
        <summary type="html"><![CDATA[Efficient sampling of complex data distributions can be achieved using
trained invertible flows (IF), where the model distribution is generated by
pushing a simple base distribution through multiple non-linear bijective
transformations. However, the iterative nature of the transformations in IFs
can limit the approximation to the target distribution. In this paper we seek
to mitigate this by implementing RBM-Flow, an IF model whose base distribution
is a Restricted Boltzmann Machine (RBM) with a continuous smoothing applied. We
show that by using RBM-Flow we are able to improve the quality of samples
generated, quantified by the Inception Scores (IS) and Frechet Inception
Distance (FID), over baseline models with the same IF transformations, but with
less expressive base distributions. Furthermore, we also obtain D-Flow, an IF
model with uncorrelated discrete latent variables. We show that D-Flow achieves
similar likelihoods and FID/IS scores to those of a typical IF with Gaussian
base variables, but with the additional benefit that global features are
meaningfully encoded as discrete labels in the latent space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+OConnor_D/0/1/0/all/0/1"&gt;Daniel O&amp;#x27;Connor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinci_W/0/1/0/all/0/1"&gt;Walter Vinci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Assumptions in Deep Anomaly Detection. (arXiv:2006.00339v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.00339</id>
        <link href="http://arxiv.org/abs/2006.00339"/>
        <updated>2021-07-13T01:59:35.747Z</updated>
        <summary type="html"><![CDATA[Though anomaly detection (AD) can be viewed as a classification problem
(nominal vs. anomalous) it is usually treated in an unsupervised manner since
one typically does not have access to, or it is infeasible to utilize, a
dataset that sufficiently characterizes what it means to be "anomalous." In
this paper we present results demonstrating that this intuition surprisingly
seems not to extend to deep AD on images. For a recent AD benchmark on
ImageNet, classifiers trained to discern between normal samples and just a few
(64) random natural images are able to outperform the current state of the art
in deep AD. Experimentally we discover that the multiscale structure of image
data makes example anomalies exceptionally informative.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruff_L/0/1/0/all/0/1"&gt;Lukas Ruff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vandermeulen_R/0/1/0/all/0/1"&gt;Robert A. Vandermeulen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franks_B/0/1/0/all/0/1"&gt;Billy Joe Franks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1"&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kloft_M/0/1/0/all/0/1"&gt;Marius Kloft&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable artificial intelligence for mechanics: physics-informing neural networks for constitutive models. (arXiv:2104.10683v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10683</id>
        <link href="http://arxiv.org/abs/2104.10683"/>
        <updated>2021-07-13T01:59:35.730Z</updated>
        <summary type="html"><![CDATA[(Artificial) neural networks have become increasingly popular in mechanics to
accelerate computations with model order reduction techniques and as universal
models for a wide variety of materials. However, the major disadvantage of
neural networks remains: their numerous parameters are challenging to interpret
and explain. Thus, neural networks are often labeled as black boxes, and their
results often elude human interpretation. In mechanics, the new and active
field of physics-informed neural networks attempts to mitigate this
disadvantage by designing deep neural networks on the basis of mechanical
knowledge. By using this a priori knowledge, deeper and more complex neural
networks became feasible, since the mechanical assumptions could be explained.
However, the internal reasoning and explanation of neural network parameters
remain mysterious.

Complementary to the physics-informed approach, we propose a first step
towards a physics-informing approach, which explains neural networks trained on
mechanical data a posteriori. This novel explainable artificial intelligence
approach aims at elucidating the black box of neural networks and their
high-dimensional representations. Therein, the principal component analysis
decorrelates the distributed representations in cell states of RNNs and allows
the comparison to known and fundamental functions. The novel approach is
supported by a systematic hyperparameter search strategy that identifies the
best neural network architectures and training parameters. The findings of
three case studies on fundamental constitutive models (hyperelasticity,
elastoplasticity, and viscoelasticity) imply that the proposed strategy can
help identify numerical and analytical closed-form solutions to characterize
new materials.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koeppe_A/0/1/0/all/0/1"&gt;Arnd Koeppe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bamer_F/0/1/0/all/0/1"&gt;Franz Bamer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Selzer_M/0/1/0/all/0/1"&gt;Michael Selzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nestler_B/0/1/0/all/0/1"&gt;Britta Nestler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markert_B/0/1/0/all/0/1"&gt;Bernd Markert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smooth Adversarial Training. (arXiv:2006.14536v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.14536</id>
        <link href="http://arxiv.org/abs/2006.14536"/>
        <updated>2021-07-13T01:59:35.723Z</updated>
        <summary type="html"><![CDATA[It is commonly believed that networks cannot be both accurate and robust,
that gaining robustness means losing accuracy. It is also generally believed
that, unless making networks larger, network architectural elements would
otherwise matter little in improving adversarial robustness. Here we present
evidence to challenge these common beliefs by a careful study about adversarial
training. Our key observation is that the widely-used ReLU activation function
significantly weakens adversarial training due to its non-smooth nature. Hence
we propose smooth adversarial training (SAT), in which we replace ReLU with its
smooth approximations to strengthen adversarial training. The purpose of smooth
activation functions in SAT is to allow it to find harder adversarial examples
and compute better gradient updates during adversarial training.

Compared to standard adversarial training, SAT improves adversarial
robustness for "free", i.e., no drop in accuracy and no increase in
computational cost. For example, without introducing additional computations,
SAT significantly enhances ResNet-50's robustness from 33.0% to 42.3%, while
also improving accuracy by 0.9% on ImageNet. SAT also works well with larger
networks: it helps EfficientNet-L1 to achieve 82.2% accuracy and 58.6%
robustness on ImageNet, outperforming the previous state-of-the-art defense by
9.5% for accuracy and 11.6% for robustness. Models are available at
https://github.com/cihangxie/SmoothAdversarialTraining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1"&gt;Cihang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1"&gt;Mingxing Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1"&gt;Boqing Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1"&gt;Alan Yuille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1"&gt;Quoc V. Le&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Shot Adaptation for mmWave Beam-Tracking on Overhead Messenger Wires through Robust Adversarial Reinforcement Learning. (arXiv:2102.08055v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08055</id>
        <link href="http://arxiv.org/abs/2102.08055"/>
        <updated>2021-07-13T01:59:35.716Z</updated>
        <summary type="html"><![CDATA[Millimeter wave (mmWave) beam-tracking based on machine learning enables the
development of accurate tracking policies while obviating the need to
periodically solve beam-optimization problems. However, its applicability is
still arguable when training-test gaps exist in terms of environmental
parameters that affect the node dynamics. From this skeptical point of view,
the contribution of this study is twofold. First, by considering an example
scenario, we confirm that the training-test gap adversely affects the
beam-tracking performance. More specifically, we consider nodes placed on
overhead messenger wires, where the node dynamics are affected by several
environmental parameters, e.g, the wire mass and tension. Although these are
particular scenarios, they yield insight into the validation of the
training-test gap problems. Second, we demonstrate the feasibility of
\textit{zero-shot adaptation} as a solution, where a learning agent adapts to
environmental parameters unseen during training. This is achieved by leveraging
a robust adversarial reinforcement learning (RARL) technique, where such
training-and-test gaps are regarded as disturbances by adversaries that are
jointly trained with a legitimate beam-tracking agent. Numerical evaluations
demonstrate that the beam-tracking policy learned via RARL can be applied to a
wide range of environmental parameters without severely degrading the received
power.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shinzaki_M/0/1/0/all/0/1"&gt;Masao Shinzaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koda_Y/0/1/0/all/0/1"&gt;Yusuke Koda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamamoto_K/0/1/0/all/0/1"&gt;Koji Yamamoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nishio_T/0/1/0/all/0/1"&gt;Takayuki Nishio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morikura_M/0/1/0/all/0/1"&gt;Masahiro Morikura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shirato_Y/0/1/0/all/0/1"&gt;Yushi Shirato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uchida_D/0/1/0/all/0/1"&gt;Daisei Uchida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kita_N/0/1/0/all/0/1"&gt;Naoki Kita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SleepTransformer: Automatic Sleep Staging with Interpretability and Uncertainty Quantification. (arXiv:2105.11043v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11043</id>
        <link href="http://arxiv.org/abs/2105.11043"/>
        <updated>2021-07-13T01:59:35.706Z</updated>
        <summary type="html"><![CDATA[Black-box skepticism is one of the main hindrances impeding
deep-learning-based automatic sleep scoring from being used in clinical
environments. Towards interpretability, this work proposes a
sequence-to-sequence sleep-staging model, namely SleepTransformer. It is based
on the transformer backbone whose self-attention scores offer interpretability
of the model's decisions at both the epoch and sequence level. At the epoch
level, the attention scores can be encoded as a heat map to highlight
sleep-relevant features captured from the input EEG signal. At the sequence
level, the attention scores are visualized as the influence of different
neighboring epochs in an input sequence (i.e. the context) to recognition of a
target epoch, mimicking the way manual scoring is done by human experts. We
further propose a simple yet efficient method to quantify uncertainty in the
model's decisions. The method, which is based on entropy, can serve as a metric
for deferring low-confidence epochs to a human expert for further inspection.
Additionally, we demonstrate that the proposed SleepTransformer outperforms
existing methods at a lower computational cost and achieves state-of-the-art
performance on two experimental databases of different sizes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Phan_H/0/1/0/all/0/1"&gt;Huy Phan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mikkelsen_K/0/1/0/all/0/1"&gt;Kaare Mikkelsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_O/0/1/0/all/0/1"&gt;Oliver Y. Ch&amp;#xe9;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koch_P/0/1/0/all/0/1"&gt;Philipp Koch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mertins_A/0/1/0/all/0/1"&gt;Alfred Mertins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vos_M/0/1/0/all/0/1"&gt;Maarten De Vos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confidence-Budget Matching for Sequential Budgeted Learning. (arXiv:2102.03400v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03400</id>
        <link href="http://arxiv.org/abs/2102.03400"/>
        <updated>2021-07-13T01:59:35.700Z</updated>
        <summary type="html"><![CDATA[A core element in decision-making under uncertainty is the feedback on the
quality of the performed actions. However, in many applications, such feedback
is restricted. For example, in recommendation systems, repeatedly asking the
user to provide feedback on the quality of recommendations will annoy them. In
this work, we formalize decision-making problems with querying budget, where
there is a (possibly time-dependent) hard limit on the number of reward queries
allowed. Specifically, we consider multi-armed bandits, linear bandits, and
reinforcement learning problems. We start by analyzing the performance of
`greedy' algorithms that query a reward whenever they can. We show that in
fully stochastic settings, doing so performs surprisingly well, but in the
presence of any adversity, this might lead to linear regret. To overcome this
issue, we propose the Confidence-Budget Matching (CBM) principle that queries
rewards when the confidence intervals are wider than the inverse square root of
the available budget. We analyze the performance of CBM based algorithms in
different settings and show that they perform well in the presence of adversity
in the contexts, initial states, and budgets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Efroni_Y/0/1/0/all/0/1"&gt;Yonathan Efroni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merlis_N/0/1/0/all/0/1"&gt;Nadav Merlis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1"&gt;Aadirupa Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1"&gt;Shie Mannor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maximum Mean Discrepancy Test is Aware of Adversarial Attacks. (arXiv:2010.11415v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11415</id>
        <link href="http://arxiv.org/abs/2010.11415"/>
        <updated>2021-07-13T01:59:35.657Z</updated>
        <summary type="html"><![CDATA[The maximum mean discrepancy (MMD) test could in principle detect any
distributional discrepancy between two datasets. However, it has been shown
that the MMD test is unaware of adversarial attacks -- the MMD test failed to
detect the discrepancy between natural and adversarial data. Given this
phenomenon, we raise a question: are natural and adversarial data really from
different distributions? The answer is affirmative -- the previous use of the
MMD test on the purpose missed three key factors, and accordingly, we propose
three components. Firstly, the Gaussian kernel has limited representation
power, and we replace it with an effective deep kernel. Secondly, the test
power of the MMD test was neglected, and we maximize it following asymptotic
statistics. Finally, adversarial data may be non-independent, and we overcome
this issue with the wild bootstrap. By taking care of the three factors, we
verify that the MMD test is aware of adversarial attacks, which lights up a
novel road for adversarial data detection based on two-sample tests.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1"&gt;Ruize Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Feng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingfeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Better SGD using Second-order Momentum. (arXiv:2103.03265v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03265</id>
        <link href="http://arxiv.org/abs/2103.03265"/>
        <updated>2021-07-13T01:59:35.652Z</updated>
        <summary type="html"><![CDATA[We develop a new algorithm for non-convex stochastic optimization that finds
an $\epsilon$-critical point in the optimal $O(\epsilon^{-3})$ stochastic
gradient and Hessian-vector product computations. Our algorithm uses
Hessian-vector products to "correct" a bias term in the momentum of SGD with
momentum. This leads to better gradient estimates in a manner analogous to
variance reduction methods. In contrast to prior work, we do not require
excessively large batch sizes, and are able to provide an adaptive algorithm
whose convergence rate automatically improves with decreasing variance in the
gradient estimates. We validate our results on a variety of large-scale deep
learning architectures and benchmarks tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1"&gt;Hoang Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cutkosky_A/0/1/0/all/0/1"&gt;Ashok Cutkosky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured Directional Pruning via Perturbation Orthogonal Projection. (arXiv:2107.05328v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05328</id>
        <link href="http://arxiv.org/abs/2107.05328"/>
        <updated>2021-07-13T01:59:35.634Z</updated>
        <summary type="html"><![CDATA[Structured pruning is an effective compression technique to reduce the
computation of neural networks, which is usually achieved by adding
perturbations to reduce network parameters at the cost of slightly increasing
training loss. A more reasonable approach is to find a sparse minimizer along
the flat minimum valley found by optimizers, i.e. stochastic gradient descent,
which keeps the training loss constant. To achieve this goal, we propose the
structured directional pruning based on orthogonal projecting the perturbations
onto the flat minimum valley. We also propose a fast solver sDprun and further
prove that it achieves directional pruning asymptotically after sufficient
training. Experiments using VGG-Net and ResNet on CIFAR-10 and CIFAR-100
datasets show that our method obtains the state-of-the-art pruned accuracy
(i.e. 93.97% on VGG16, CIFAR-10 task) without retraining. Experiments using
DNN, VGG-Net and WRN28X10 on MNIST, CIFAR-10 and CIFAR-100 datasets demonstrate
our method performs structured directional pruning, reaching the same minimum
valley as the optimizer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+YinchuanLi/0/1/0/all/0/1"&gt;YinchuanLi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+XiaofengLiu/0/1/0/all/0/1"&gt;XiaofengLiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+YunfengShao/0/1/0/all/0/1"&gt;YunfengShao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+QingWang/0/1/0/all/0/1"&gt;QingWang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+YanhuiGeng/0/1/0/all/0/1"&gt;YanhuiGeng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning interaction rules from multi-animal trajectories via augmented behavioral models. (arXiv:2107.05326v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05326</id>
        <link href="http://arxiv.org/abs/2107.05326"/>
        <updated>2021-07-13T01:59:35.625Z</updated>
        <summary type="html"><![CDATA[Extracting the interaction rules of biological agents from moving sequences
pose challenges in various domains. Granger causality is a practical framework
for analyzing the interactions from observed time-series data; however, this
framework ignores the structures of the generative process in animal behaviors,
which may lead to interpretational problems and sometimes erroneous assessments
of causality. In this paper, we propose a new framework for learning Granger
causality from multi-animal trajectories via augmented theory-based behavioral
models with interpretable data-driven models. We adopt an approach for
augmenting incomplete multi-agent behavioral models described by time-varying
dynamical systems with neural networks. For efficient and interpretable
learning, our model leverages theory-based architectures separating navigation
and motion processes, and the theory-guided regularization for reliable
behavioral modeling. This can provide interpretable signs of Granger-causal
effects over time, i.e., when specific others cause the approach or separation.
In experiments using synthetic datasets, our method achieved better performance
than various baselines. We then analyzed multi-animal datasets of mice, flies,
birds, and bats, which verified our method and obtained novel biological
insights.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fujii_K/0/1/0/all/0/1"&gt;Keisuke Fujii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takeishi_N/0/1/0/all/0/1"&gt;Naoya Takeishi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsutsui_K/0/1/0/all/0/1"&gt;Kazushi Tsutsui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fujioka_E/0/1/0/all/0/1"&gt;Emyo Fujioka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nishiumi_N/0/1/0/all/0/1"&gt;Nozomi Nishiumi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tanaka_R/0/1/0/all/0/1"&gt;Ryooya Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fukushiro_M/0/1/0/all/0/1"&gt;Mika Fukushiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ide_K/0/1/0/all/0/1"&gt;Kaoru Ide&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kohno_H/0/1/0/all/0/1"&gt;Hiroyoshi Kohno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoda_K/0/1/0/all/0/1"&gt;Ken Yoda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takahashi_S/0/1/0/all/0/1"&gt;Susumu Takahashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hiryu_S/0/1/0/all/0/1"&gt;Shizuko Hiryu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawahara_Y/0/1/0/all/0/1"&gt;Yoshinobu Kawahara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastically forced ensemble dynamic mode decomposition for forecasting and analysis of near-periodic systems. (arXiv:2010.04248v2 [physics.soc-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04248</id>
        <link href="http://arxiv.org/abs/2010.04248"/>
        <updated>2021-07-13T01:59:35.554Z</updated>
        <summary type="html"><![CDATA[Time series forecasting remains a central challenge problem in almost all
scientific disciplines. We introduce a novel load forecasting method in which
observed dynamics are modeled as a forced linear system using Dynamic Mode
Decomposition (DMD) in time delay coordinates. Central to this approach is the
insight that grid load, like many observables on complex real-world systems,
has an "almost-periodic" character, i.e., a continuous Fourier spectrum
punctuated by dominant peaks, which capture regular (e.g., daily or weekly)
recurrences in the dynamics. The forecasting method presented takes advantage
of this property by (i) regressing to a deterministic linear model whose
eigenspectrum maps onto those peaks, and (ii) simultaneously learning a
stochastic Gaussian process regression (GPR) process to actuate this system.
Our forecasting algorithm is compared against state-of-the-art forecasting
techniques not using additional explanatory variables and is shown to produce
superior performance. Moreover, its use of linear intrinsic dynamics offers a
number of desirable properties in terms of interpretability and parsimony.
Results are presented for a test case using load data from an electrical grid.
Load forecasting is an essential challenge in power systems engineering, with
major implications for real-time control, pricing, maintenance, and security
decisions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Dylewsky_D/0/1/0/all/0/1"&gt;Daniel Dylewsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Barajas_Solano_D/0/1/0/all/0/1"&gt;David Barajas-Solano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Tartakovsky_A/0/1/0/all/0/1"&gt;Alexandre M. Tartakovsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kutz_J/0/1/0/all/0/1"&gt;J. Nathan Kutz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reliable Post hoc Explanations: Modeling Uncertainty in Explainability. (arXiv:2008.05030v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.05030</id>
        <link href="http://arxiv.org/abs/2008.05030"/>
        <updated>2021-07-13T01:59:35.535Z</updated>
        <summary type="html"><![CDATA[As black box explanations are increasingly being employed to establish model
credibility in high stakes settings, it is important to ensure that these
explanations are accurate and reliable. However, prior work demonstrates that
explanations generated by state-of-the-art techniques are inconsistent,
unstable, and provide very little insight into their correctness and
reliability. In addition, these methods are also computationally inefficient,
and require significant hyper-parameter tuning. In this paper, we address the
aforementioned challenges by developing a novel Bayesian framework for
generating local explanations along with their associated uncertainty. We
instantiate this framework to obtain Bayesian versions of LIME and KernelSHAP
which output credible intervals for the feature importances, capturing the
associated uncertainty. The resulting explanations not only enable us to make
concrete inferences about their quality (e.g., there is a 95\% chance that the
feature importance lies within the given range), but are also highly consistent
and stable. We carry out a detailed theoretical analysis that leverages the
aforementioned uncertainty to estimate how many perturbations to sample, and
how to sample for faster convergence. This work makes the first attempt at
addressing several critical issues with popular explanation methods in one
shot, thereby generating consistent, stable, and reliable explanations with
guarantees in a computationally efficient manner. Experimental evaluation with
multiple real world datasets and user studies demonstrate that the efficacy of
the proposed framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Slack_D/0/1/0/all/0/1"&gt;Dylan Slack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hilgard_S/0/1/0/all/0/1"&gt;Sophie Hilgard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sameer Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1"&gt;Himabindu Lakkaraju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Joint introduction to Gaussian Processes and Relevance Vector Machines with Connections to Kalman filtering and other Kernel Smoothers. (arXiv:2009.09217v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.09217</id>
        <link href="http://arxiv.org/abs/2009.09217"/>
        <updated>2021-07-13T01:59:35.529Z</updated>
        <summary type="html"><![CDATA[The expressive power of Bayesian kernel-based methods has led them to become
an important tool across many different facets of artificial intelligence, and
useful to a plethora of modern application domains, providing both power and
interpretability via uncertainty analysis. This article introduces and
discusses two methods which straddle the areas of probabilistic Bayesian
schemes and kernel methods for regression: Gaussian Processes and Relevance
Vector Machines. Our focus is on developing a common framework with which to
view these methods, via intermediate methods a probabilistic version of the
well-known kernel ridge regression, and drawing connections among them, via
dual formulations, and discussion of their application in the context of major
tasks: regression, smoothing, interpolation, and filtering. Overall, we provide
understanding of the mathematical concepts behind these models, and we
summarize and discuss in depth different interpretations and highlight the
relationship to other methods, such as linear kernel smoothers, Kalman
filtering and Fourier approximations. Throughout, we provide numerous figures
to promote understanding, and we make numerous recommendations to
practitioners. Benefits and drawbacks of the different techniques are
highlighted. To our knowledge, this is the most in-depth study of its kind to
date focused on these two methods, and will be relevant to theoretical
understanding and practitioners throughout the domains of data-science, signal
processing, machine learning, and artificial intelligence in general.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Martino_L/0/1/0/all/0/1"&gt;Luca Martino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Read_J/0/1/0/all/0/1"&gt;Jesse Read&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Graph Learning via Population Based Self-Tuning GCN. (arXiv:2107.04713v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04713</id>
        <link href="http://arxiv.org/abs/2107.04713"/>
        <updated>2021-07-13T01:59:35.523Z</updated>
        <summary type="html"><![CDATA[Owing to the remarkable capability of extracting effective graph embeddings,
graph convolutional network (GCN) and its variants have been successfully
applied to a broad range of tasks, such as node classification, link
prediction, and graph classification. Traditional GCN models suffer from the
issues of overfitting and oversmoothing, while some recent techniques like
DropEdge could alleviate these issues and thus enable the development of deep
GCN. However, training GCN models is non-trivial, as it is sensitive to the
choice of hyperparameters such as dropout rate and learning weight decay,
especially for deep GCN models. In this paper, we aim to automate the training
of GCN models through hyperparameter optimization. To be specific, we propose a
self-tuning GCN approach with an alternate training algorithm, and further
extend our approach by incorporating the population based training scheme.
Experimental results on three benchmark datasets demonstrate the effectiveness
of our approaches on optimizing multi-layer GCN, compared with several
representative baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1"&gt;Ronghang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1"&gt;Zhiqiang Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yaliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Sheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[L2M: Practical posterior Laplace approximation with optimization-driven second moment estimation. (arXiv:2107.04695v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04695</id>
        <link href="http://arxiv.org/abs/2107.04695"/>
        <updated>2021-07-13T01:59:35.517Z</updated>
        <summary type="html"><![CDATA[Uncertainty quantification for deep neural networks has recently evolved
through many techniques. In this work, we revisit Laplace approximation, a
classical approach for posterior approximation that is computationally
attractive. However, instead of computing the curvature matrix, we show that,
under some regularity conditions, the Laplace approximation can be easily
constructed using the gradient second moment. This quantity is already
estimated by many exponential moving average variants of Adagrad such as Adam
and RMSprop, but is traditionally discarded after training. We show that our
method (L2M) does not require changes in models or optimization, can be
implemented in a few lines of code to yield reasonable results, and it does not
require any extra computational steps besides what is already being computed by
optimizers, without introducing any new hyperparameter. We hope our method can
open new research directions on using quantities already computed by optimizers
for uncertainty estimation in deep neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perone_C/0/1/0/all/0/1"&gt;Christian S. Perone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silveira_R/0/1/0/all/0/1"&gt;Roberto Pereira Silveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paula_T/0/1/0/all/0/1"&gt;Thomas Paula&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HRVGAN: High Resolution Video Generation using Spatio-Temporal GAN. (arXiv:2008.09646v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.09646</id>
        <link href="http://arxiv.org/abs/2008.09646"/>
        <updated>2021-07-13T01:59:35.510Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a novel network for high resolution video
generation. Our network uses ideas from Wasserstein GANs by enforcing
k-Lipschitz constraint on the loss term and Conditional GANs using class labels
for training and testing. We present Generator and Discriminator network
layerwise details along with the combined network architecture, optimization
details and algorithm used in this work. Our network uses a combination of two
loss terms: mean square pixel loss and an adversarial loss. The datasets used
for training and testing our network are UCF101, Golf and Aeroplane Datasets.
Using Inception Score and Fr\'echet Inception Distance as the evaluation
metrics, our network outperforms previous state of the art networks on
unsupervised video generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sagar_A/0/1/0/all/0/1"&gt;Abhinav Sagar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Layer-wise Analysis of a Self-supervised Speech Representation Model. (arXiv:2107.04734v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04734</id>
        <link href="http://arxiv.org/abs/2107.04734"/>
        <updated>2021-07-13T01:59:35.504Z</updated>
        <summary type="html"><![CDATA[Recently proposed self-supervised learning approaches have been successful
for pre-training speech representation models. The utility of these learned
representations has been observed empirically, but not much has been studied
about the type or extent of information encoded in the pre-trained
representations themselves. Developing such insights can help understand the
capabilities and limits of these models and enable the research community to
more efficiently develop their usage for downstream applications. In this work,
we begin to fill this gap by examining one recent and successful pre-trained
model (wav2vec 2.0), via its intermediate representation vectors, using a suite
of analysis tools. We use the metrics of canonical correlation, mutual
information, and performance on simple downstream tasks with non-parametric
probes, in order to (i) query for acoustic and linguistic information content,
(ii) characterize the evolution of information across model layers, and (iii)
understand how fine-tuning the model for automatic speech recognition (ASR)
affects these observations. Our findings motivate modifying the fine-tuning
protocol for ASR, which produces improved word error rates in a low-resource
setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pasad_A/0/1/0/all/0/1"&gt;Ankita Pasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chou_J/0/1/0/all/0/1"&gt;Ju-Chieh Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1"&gt;Karen Livescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hack The Box: Fooling Deep Learning Abstraction-Based Monitors. (arXiv:2107.04764v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04764</id>
        <link href="http://arxiv.org/abs/2107.04764"/>
        <updated>2021-07-13T01:59:35.488Z</updated>
        <summary type="html"><![CDATA[Deep learning is a type of machine learning that adapts a deep hierarchy of
concepts. Deep learning classifiers link the most basic version of concepts at
the input layer to the most abstract version of concepts at the output layer,
also known as a class or label. However, once trained over a finite set of
classes, a deep learning model does not have the power to say that a given
input does not belong to any of the classes and simply cannot be linked.
Correctly invalidating the prediction of unrelated classes is a challenging
problem that has been tackled in many ways in the literature. Novelty detection
gives deep learning the ability to output "do not know" for novel/unseen
classes. Still, no attention has been given to the security aspects of novelty
detection. In this paper, we consider the case study of abstraction-based
novelty detection and show that it is not robust against adversarial samples.
Moreover, we show the feasibility of crafting adversarial samples that fool the
deep learning classifier and bypass the novelty detection monitoring at the
same time. In other words, these monitoring boxes are hackable. We demonstrate
that novelty detection itself ends up as an attack surface.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ibrahim_S/0/1/0/all/0/1"&gt;Sara Hajj Ibrahim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nassar_M/0/1/0/all/0/1"&gt;Mohamed Nassar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Semi-Supervised Learning for 2D Medical Image Segmentation. (arXiv:2106.06801v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06801</id>
        <link href="http://arxiv.org/abs/2106.06801"/>
        <updated>2021-07-13T01:59:35.482Z</updated>
        <summary type="html"><![CDATA[Contrastive Learning (CL) is a recent representation learning approach, which
encourages inter-class separability and intra-class compactness in learned
image representations. Since medical images often contain multiple semantic
classes in an image, using CL to learn representations of local features (as
opposed to global) is important. In this work, we present a novel
semi-supervised 2D medical segmentation solution that applies CL on image
patches, instead of full images. These patches are meaningfully constructed
using the semantic information of different classes obtained via pseudo
labeling. We also propose a novel consistency regularization (CR) scheme, which
works in synergy with CL. It addresses the problem of confirmation bias, and
encourages better clustering in the feature space. We evaluate our method on
four public medical segmentation datasets and a novel histopathology dataset
that we introduce. Our method obtains consistent improvements over
state-of-the-art semi-supervised segmentation approaches for all datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_P/0/1/0/all/0/1"&gt;Prashant Pandey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pai_A/0/1/0/all/0/1"&gt;Ajey Pai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhatt_N/0/1/0/all/0/1"&gt;Nisarg Bhatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1"&gt;Prasenjit Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makharia_G/0/1/0/all/0/1"&gt;Govind Makharia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+AP_P/0/1/0/all/0/1"&gt;Prathosh AP&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1"&gt;Mausam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diverse Video Generation using a Gaussian Process Trigger. (arXiv:2107.04619v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04619</id>
        <link href="http://arxiv.org/abs/2107.04619"/>
        <updated>2021-07-13T01:59:35.476Z</updated>
        <summary type="html"><![CDATA[Generating future frames given a few context (or past) frames is a
challenging task. It requires modeling the temporal coherence of videos and
multi-modality in terms of diversity in the potential future states. Current
variational approaches for video generation tend to marginalize over
multi-modal future outcomes. Instead, we propose to explicitly model the
multi-modality in the future outcomes and leverage it to sample diverse
futures. Our approach, Diverse Video Generator, uses a Gaussian Process (GP) to
learn priors on future states given the past and maintains a probability
distribution over possible futures given a particular sample. In addition, we
leverage the changes in this distribution over time to control the sampling of
diverse future states by estimating the end of ongoing sequences. That is, we
use the variance of GP over the output function space to trigger a change in an
action sequence. We achieve state-of-the-art results on diverse future frame
generation in terms of reconstruction quality and diversity of the generated
sequences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_G/0/1/0/all/0/1"&gt;Gaurav Shrivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Abhinav Shrivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FPS-Net: A Convolutional Fusion Network for Large-Scale LiDAR Point Cloud Segmentation. (arXiv:2103.00738v1 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2103.00738</id>
        <link href="http://arxiv.org/abs/2103.00738"/>
        <updated>2021-07-13T01:59:35.470Z</updated>
        <summary type="html"><![CDATA[Scene understanding based on LiDAR point cloud is an essential task for
autonomous cars to drive safely, which often employs spherical projection to
map 3D point cloud into multi-channel 2D images for semantic segmentation. Most
existing methods simply stack different point attributes/modalities (e.g.
coordinates, intensity, depth, etc.) as image channels to increase information
capacity, but ignore distinct characteristics of point attributes in different
image channels. We design FPS-Net, a convolutional fusion network that exploits
the uniqueness and discrepancy among the projected image channels for optimal
point cloud segmentation. FPS-Net adopts an encoder-decoder structure. Instead
of simply stacking multiple channel images as a single input, we group them
into different modalities to first learn modality-specific features separately
and then map the learned features into a common high-dimensional feature space
for pixel-level fusion and learning. Specifically, we design a residual dense
block with multiple receptive fields as a building block in the encoder which
preserves detailed information in each modality and learns hierarchical
modality-specific and fused features effectively. In the FPS-Net decoder, we
use a recurrent convolution block likewise to hierarchically decode fused
features into output space for pixel-level classification. Extensive
experiments conducted on two widely adopted point cloud datasets show that
FPS-Net achieves superior semantic segmentation as compared with
state-of-the-art projection-based methods. In addition, the proposed modality
fusion idea is compatible with typical projection-based methods and can be
incorporated into them with consistent performance improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1"&gt;Aoran Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaofei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shijian Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1"&gt;Dayan Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiaxing Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Curious Case of Convex Neural Networks. (arXiv:2006.05103v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05103</id>
        <link href="http://arxiv.org/abs/2006.05103"/>
        <updated>2021-07-13T01:59:35.464Z</updated>
        <summary type="html"><![CDATA[In this paper, we investigate a constrained formulation of neural networks
where the output is a convex function of the input. We show that the convexity
constraints can be enforced on both fully connected and convolutional layers,
making them applicable to most architectures. The convexity constraints include
restricting the weights (for all but the first layer) to be non-negative and
using a non-decreasing convex activation function. Albeit simple, these
constraints have profound implications on the generalization abilities of the
network. We draw three valuable insights: (a) Input Output Convex Neural
Networks (IOC-NNs) self regularize and reduce the problem of overfitting; (b)
Although heavily constrained, they outperform the base multi layer perceptrons
and achieve similar performance as compared to base convolutional architectures
and (c) IOC-NNs show robustness to noise in train labels. We demonstrate the
efficacy of the proposed idea using thorough experiments and ablation studies
on standard image classification datasets with three different neural network
architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sivaprasad_S/0/1/0/all/0/1"&gt;Sarath Sivaprasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Ankur Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manwani_N/0/1/0/all/0/1"&gt;Naresh Manwani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gandhi_V/0/1/0/all/0/1"&gt;Vineet Gandhi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SITHCon: A neural network robust to variations in input scaling on the time dimension. (arXiv:2107.04616v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04616</id>
        <link href="http://arxiv.org/abs/2107.04616"/>
        <updated>2021-07-13T01:59:35.450Z</updated>
        <summary type="html"><![CDATA[In machine learning, convolutional neural networks (CNNs) have been extremely
influential in both computer vision and in recognizing patterns extended over
time. In computer vision, part of the flexibility arises from the use of
max-pooling operations over the convolutions to attain translation invariance.
In the mammalian brain, neural representations of time use a set of temporal
basis functions. Critically, these basis functions appear to be arranged in a
geometric series such that the basis set is evenly distributed over logarithmic
time. This paper introduces a Scale-Invariant Temporal History Convolution
network (SITHCon) that uses a logarithmically-distributed temporal memory. A
max-pool over a logarithmically-distributed temporal memory results in
scale-invariance in time. We compare performance of SITHCon to a Temporal
Convolution Network (TCN) and demonstrate that, although both networks can
learn classification and regression problems on both univariate and
multivariate time series $f(t)$, only SITHCon has the property that it
generalizes without retraining to rescaled versions of the input $f(at)$. This
property, inspired by findings from neuroscience and psychology, could lead to
large-scale networks with dramatically different capabilities, including faster
training and greater generalizability, even with significantly fewer free
parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jacques_B/0/1/0/all/0/1"&gt;Brandon G. Jacques&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tiganj_Z/0/1/0/all/0/1"&gt;Zoran Tiganj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1"&gt;Aakash Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Howard_M/0/1/0/all/0/1"&gt;Marc W. Howard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sederberg_P/0/1/0/all/0/1"&gt;Per B. Sederberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[H\"older Bounds for Sensitivity Analysis in Causal Reasoning. (arXiv:2107.04661v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04661</id>
        <link href="http://arxiv.org/abs/2107.04661"/>
        <updated>2021-07-13T01:59:35.443Z</updated>
        <summary type="html"><![CDATA[We examine interval estimation of the effect of a treatment T on an outcome Y
given the existence of an unobserved confounder U. Using H\"older's inequality,
we derive a set of bounds on the confounding bias |E[Y|T=t]-E[Y|do(T=t)]| based
on the degree of unmeasured confounding (i.e., the strength of the connection
U->T, and the strength of U->Y). These bounds are tight either when U is
independent of T or when U is independent of Y given T (when there is no
unobserved confounding). We focus on a special case of this bound depending on
the total variation distance between the distributions p(U) and p(U|T=t), as
well as the maximum (over all possible values of U) deviation of the
conditional expected outcome E[Y|U=u,T=t] from the average expected outcome
E[Y|T=t]. We discuss possible calibration strategies for this bound to get
interval estimates for treatment effects, and experimentally validate the bound
using synthetic and semi-synthetic datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Assaad_S/0/1/0/all/0/1"&gt;Serge Assaad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1"&gt;Shuxi Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1"&gt;Henry Pfister&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1"&gt;Fan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1"&gt;Lawrence Carin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Effects of Invertibility on the Representational Complexity of Encoders in Variational Autoencoders. (arXiv:2107.04652v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04652</id>
        <link href="http://arxiv.org/abs/2107.04652"/>
        <updated>2021-07-13T01:59:35.437Z</updated>
        <summary type="html"><![CDATA[Training and using modern neural-network based latent-variable generative
models (like Variational Autoencoders) often require simultaneously training a
generative direction along with an inferential(encoding) direction, which
approximates the posterior distribution over the latent variables. Thus, the
question arises: how complex does the inferential model need to be, in order to
be able to accurately model the posterior distribution of a given generative
model?

In this paper, we identify an important property of the generative map
impacting the required size of the encoder. We show that if the generative map
is "strongly invertible" (in a sense we suitably formalize), the inferential
model need not be much more complex. Conversely, we prove that there exist
non-invertible generative maps, for which the encoding direction needs to be
exponentially larger (under standard assumptions in computational complexity).
Importantly, we do not require the generative model to be layerwise invertible,
which a lot of the related literature assumes and isn't satisfied by many
architectures used in practice (e.g. convolution and pooling based networks).
Thus, we provide theoretical support for the empirical wisdom that learning
deep generative models is harder when data lies on a low-dimensional manifold.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pareek_D/0/1/0/all/0/1"&gt;Divyansh Pareek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Risteski_A/0/1/0/all/0/1"&gt;Andrej Risteski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FibAR: Embedding Optical Fibers in 3D Printed Objects for Active Markers in Dynamic Projection Mapping. (arXiv:2002.02159v1 [cs.GR] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2002.02159</id>
        <link href="http://arxiv.org/abs/2002.02159"/>
        <updated>2021-07-13T01:59:35.429Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel active marker for dynamic projection mapping (PM)
that emits a temporal blinking pattern of infrared (IR) light representing its
ID. We used a multi-material three dimensional (3D) printer to fabricate a
projection object with optical fibers that can guide IR light from LEDs
attached on the bottom of the object. The aperture of an optical fiber is
typically very small; thus, it is unnoticeable to human observers under
projection and can be placed on a strongly curved part of a projection surface.
In addition, the working range of our system can be larger than previous
marker-based methods as the blinking patterns can theoretically be recognized
by a camera placed at a wide range of distances from markers. We propose an
automatic marker placement algorithm to spread multiple active markers over the
surface of a projection object such that its pose can be robustly estimated
using captured images from arbitrary directions. We also propose an
optimization framework for determining the routes of the optical fibers in such
a way that collisions of the fibers can be avoided while minimizing the loss of
light intensity in the fibers. Through experiments conducted using three
fabricated objects containing strongly curved surfaces, we confirmed that the
proposed method can achieve accurate dynamic PMs in a significantly wide
working range.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tone_D/0/1/0/all/0/1"&gt;Daiki Tone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iwai_D/0/1/0/all/0/1"&gt;Daisuke Iwai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hiura_S/0/1/0/all/0/1"&gt;Shinsaku Hiura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sato_K/0/1/0/all/0/1"&gt;Kosuke Sato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Perceptual-based deep-learning denoiser as a defense against adversarial attacks on ASR systems. (arXiv:2107.05222v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.05222</id>
        <link href="http://arxiv.org/abs/2107.05222"/>
        <updated>2021-07-13T01:59:35.422Z</updated>
        <summary type="html"><![CDATA[In this paper we investigate speech denoising as a defense against
adversarial attacks on automatic speech recognition (ASR) systems. Adversarial
attacks attempt to force misclassification by adding small perturbations to the
original speech signal. We propose to counteract this by employing a
neural-network based denoiser as a pre-processor in the ASR pipeline. The
denoiser is independent of the downstream ASR model, and thus can be rapidly
deployed in existing systems. We found that training the denoisier using a
perceptually motivated loss function resulted in increased adversarial
robustness without compromising ASR performance on benign samples. Our defense
was evaluated (as a part of the DARPA GARD program) on the 'Kenansville' attack
strategy across a range of attack strengths and speech samples. An average
improvement in Word Error Rate (WER) of about 7.7% was observed over the
undefended model at 20 dB signal-to-noise-ratio (SNR) attack strength.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sreeram_A/0/1/0/all/0/1"&gt;Anirudh Sreeram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mehlman_N/0/1/0/all/0/1"&gt;Nicholas Mehlman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Peri_R/0/1/0/all/0/1"&gt;Raghuveer Peri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Knox_D/0/1/0/all/0/1"&gt;Dillon Knox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Narayanan_S/0/1/0/all/0/1"&gt;Shrikanth Narayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[P2T: Pyramid Pooling Transformer for Scene Understanding. (arXiv:2106.12011v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12011</id>
        <link href="http://arxiv.org/abs/2106.12011"/>
        <updated>2021-07-13T01:59:35.406Z</updated>
        <summary type="html"><![CDATA[This paper jointly resolves two problems in vision transformer: i) the
computation of Multi-Head Self-Attention (MHSA) has high computational/space
complexity; ii) recent vision transformer networks are overly tuned for image
classification, ignoring the difference between image classification (simple
scenarios, more similar to NLP) and downstream scene understanding tasks
(complicated scenarios, rich structural and contextual information). To this
end, we note that pyramid pooling has been demonstrated to be effective in
various vision tasks owing to its powerful ability in context abstraction, and
its natural property of spatial invariance is also suitable to address the loss
of structural information (problem ii)). Hence, we propose to adapt pyramid
pooling to MHSA for alleviating its high requirement on computational resources
(problem i)). In this way, this pooling-based MHSA can well address the above
two problems and is thus flexible and powerful for downstream scene
understanding tasks. Plugged with our pooling-based MHSA, we build a
downstream-task-oriented transformer network, dubbed Pyramid Pooling
Transformer (P2T). Extensive experiments demonstrate that, when applied P2T as
the backbone network, it shows substantial superiority in various downstream
scene understanding tasks such as semantic segmentation, object detection,
instance segmentation, and visual saliency detection, compared to previous CNN-
and transformer-based networks. The code will be released at
https://github.com/yuhuan-wu/P2T.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yu-Huan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1"&gt;Xin Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1"&gt;Ming-Ming Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[InfoVAEGAN : learning joint interpretable representations by information maximization and maximum likelihood. (arXiv:2107.04705v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04705</id>
        <link href="http://arxiv.org/abs/2107.04705"/>
        <updated>2021-07-13T01:59:35.400Z</updated>
        <summary type="html"><![CDATA[Learning disentangled and interpretable representations is an important step
towards accomplishing comprehensive data representations on the manifold. In
this paper, we propose a novel representation learning algorithm which combines
the inference abilities of Variational Autoencoders (VAE) with the
generalization capability of Generative Adversarial Networks (GAN). The
proposed model, called InfoVAEGAN, consists of three networks~: Encoder,
Generator and Discriminator. InfoVAEGAN aims to jointly learn discrete and
continuous interpretable representations in an unsupervised manner by using two
different data-free log-likelihood functions onto the variables sampled from
the generator's distribution. We propose a two-stage algorithm for optimizing
the inference network separately from the generator training. Moreover, we
enforce the learning of interpretable representations through the maximization
of the mutual information between the existing latent variables and those
created through generative and inference processes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1"&gt;Fei Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1"&gt;Adrian G. Bors&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cognitive Visual Commonsense Reasoning Using Dynamic Working Memory. (arXiv:2107.01671v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01671</id>
        <link href="http://arxiv.org/abs/2107.01671"/>
        <updated>2021-07-13T01:59:35.394Z</updated>
        <summary type="html"><![CDATA[Visual Commonsense Reasoning (VCR) predicts an answer with corresponding
rationale, given a question-image input. VCR is a recently introduced visual
scene understanding task with a wide range of applications, including visual
question answering, automated vehicle systems, and clinical decision support.
Previous approaches to solving the VCR task generally rely on pre-training or
exploiting memory with long dependency relationship encoded models. However,
these approaches suffer from a lack of generalizability and prior knowledge. In
this paper we propose a dynamic working memory based cognitive VCR network,
which stores accumulated commonsense between sentences to provide prior
knowledge for inference. Extensive experiments show that the proposed model
yields significant improvements over existing methods on the benchmark VCR
dataset. Moreover, the proposed model provides intuitive interpretation into
visual commonsense reasoning. A Python implementation of our mechanism is
publicly available at https://github.com/tanjatang/DMVCR]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xuejiao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wenbin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Child_T/0/1/0/all/0/1"&gt;Travers B. Child&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1"&gt;Qiong Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Ji Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Customizable Reference Runtime Monitoring of Neural Networks using Resolution Boxes. (arXiv:2104.14435v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14435</id>
        <link href="http://arxiv.org/abs/2104.14435"/>
        <updated>2021-07-13T01:59:35.388Z</updated>
        <summary type="html"><![CDATA[Classification neural networks fail to detect inputs that do not fall inside
the classes they have been trained for. Runtime monitoring techniques on the
neuron activation pattern can be used to detect such inputs. We present an
approach for monitoring classification systems via data abstraction. Data
abstraction relies on the notion of box with a resolution. Box-based
abstraction consists in representing a set of values by its minimal and maximal
values in each dimension. We augment boxes with a notion of resolution and
define their clustering coverage, which is intuitively a quantitative metric
that indicates the abstraction quality. This allows studying the effect of
different clustering parameters on the constructed boxes and estimating an
interval of sub-optimal parameters. Moreover, we automatically construct
monitors that leverage both the correct and incorrect behaviors of a system.
This allows checking the size of the monitor abstractions and analyzing the
separability of the network. Monitors are obtained by combining the
sub-monitors of each class of the system placed at some selected layers. Our
experiments demonstrate the effectiveness of our clustering coverage estimation
and show how to assess the effectiveness and precision of monitors according to
the selected clustering parameter and monitored layers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Changshun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Falcone_Y/0/1/0/all/0/1"&gt;Yli&amp;#xe8;s Falcone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bensalem_S/0/1/0/all/0/1"&gt;Saddek Bensalem&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Probabilistic Reward Machines from Non-Markovian Stochastic Reward Processes. (arXiv:2107.04633v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04633</id>
        <link href="http://arxiv.org/abs/2107.04633"/>
        <updated>2021-07-13T01:59:35.381Z</updated>
        <summary type="html"><![CDATA[The success of reinforcement learning in typical settings is, in part,
predicated on underlying Markovian assumptions on the reward signal by which an
agent learns optimal policies. In recent years, the use of reward machines has
relaxed this assumption by enabling a structured representation of
non-Markovian rewards. In particular, such representations can be used to
augment the state space of the underlying decision process, thereby
facilitating non-Markovian reinforcement learning. However, these reward
machines cannot capture the semantics of stochastic reward signals. In this
paper, we make progress on this front by introducing probabilistic reward
machines (PRMs) as a representation of non-Markovian stochastic rewards. We
present an algorithm to learn PRMs from the underlying decision process as well
as to learn the PRM representation of a given decision-making policy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Velasquez_A/0/1/0/all/0/1"&gt;Alvaro Velasquez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beckus_A/0/1/0/all/0/1"&gt;Andre Beckus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dohmen_T/0/1/0/all/0/1"&gt;Taylor Dohmen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1"&gt;Ashutosh Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Topper_N/0/1/0/all/0/1"&gt;Noah Topper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atia_G/0/1/0/all/0/1"&gt;George Atia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CBNetV2: A Composite Backbone Network Architecture for Object Detection. (arXiv:2107.00420v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00420</id>
        <link href="http://arxiv.org/abs/2107.00420"/>
        <updated>2021-07-13T01:59:35.364Z</updated>
        <summary type="html"><![CDATA[Modern top-performing object detectors depend heavily on backbone networks,
whose advances bring consistent performance gains through exploring more
effective network structures. In this paper, we propose a novel and flexible
backbone framework, namely CBNetV2, to better train existing open-sourced
pre-trained backbones under the pre-training fine-tuning protocol. In
particular, CBNetV2 architecture groups multiple identical backbones, which are
connected through composite connections. Specifically, it integrates the high-
and low-level features of multiple backbone networks and gradually expands the
receptive field to more efficiently perform object detection. We also propose a
better training strategy with assistant supervision for CBNet-based detectors.
CBNetV2 has strong generalization capabilities for different backbones and head
designs of the detector architecture. Without additional pre-training, CBNetV2
can be adapted to various backbones, including manual-based and NAS-based, as
well as CNN-based and Transformer-based ones. Experiments provide strong
evidence showing that composite backbones are more efficient, effective, and
resource-friendly than wider and deeper networks. CBNetV2 is compatible with
the head designs of most mainstream detectors, including one-stage and
two-stage detectors, as well as anchor-based and anchor-free-based ones, and
significantly improve their performances by more than 3.0% AP over the baseline
on COCO. Particularly, under the single-model and single-scale testing
protocol, our Dual-Swin-L achieves 59.4% box AP and 51.6% mask AP on COCO
test-dev, which is significantly better than the state-of-the-art result (i.e.,
57.7% box AP and 50.2% mask AP). Code is available at
https://github.com/VDIGPKU/CBNetV2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1"&gt;Tingting Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1"&gt;Xiaojie Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yudong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yongtao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zhi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1"&gt;Wei Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jingdong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1"&gt;Haibin Ling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient Forward-Propagation for Large-Scale Temporal Video Modelling. (arXiv:2106.08318v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08318</id>
        <link href="http://arxiv.org/abs/2106.08318"/>
        <updated>2021-07-13T01:59:35.358Z</updated>
        <summary type="html"><![CDATA[How can neural networks be trained on large-volume temporal data efficiently?
To compute the gradients required to update parameters, backpropagation blocks
computations until the forward and backward passes are completed. For temporal
signals, this introduces high latency and hinders real-time learning. It also
creates a coupling between consecutive layers, which limits model parallelism
and increases memory consumption. In this paper, we build upon Sideways, which
avoids blocking by propagating approximate gradients forward in time, and we
propose mechanisms for temporal integration of information based on different
variants of skip connections. We also show how to decouple computation and
delegate individual neural modules to different devices, allowing distributed
and parallel training. The proposed Skip-Sideways achieves low latency
training, model parallelism, and, importantly, is capable of extracting
temporal features, leading to more stable training and improved performance on
real-world action recognition video datasets such as HMDB51, UCF101, and the
large-scale Kinetics-600. Finally, we also show that models trained with
Skip-Sideways generate better future frames than Sideways models, and hence
they can better utilize motion cues.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1"&gt;Mateusz Malinowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vytiniotis_D/0/1/0/all/0/1"&gt;Dimitrios Vytiniotis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swirszcz_G/0/1/0/all/0/1"&gt;Grzegorz Swirszcz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patraucean_V/0/1/0/all/0/1"&gt;Viorica Patraucean&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1"&gt;Joao Carreira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MVT: Mask Vision Transformer for Facial Expression Recognition in the wild. (arXiv:2106.04520v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04520</id>
        <link href="http://arxiv.org/abs/2106.04520"/>
        <updated>2021-07-13T01:59:35.352Z</updated>
        <summary type="html"><![CDATA[Facial Expression Recognition (FER) in the wild is an extremely challenging
task in computer vision due to variant backgrounds, low-quality facial images,
and the subjectiveness of annotators. These uncertainties make it difficult for
neural networks to learn robust features on limited-scale datasets. Moreover,
the networks can be easily distributed by the above factors and perform
incorrect decisions. Recently, vision transformer (ViT) and data-efficient
image transformers (DeiT) present their significant performance in traditional
classification tasks. The self-attention mechanism makes transformers obtain a
global receptive field in the first layer which dramatically enhances the
feature extraction capability. In this work, we first propose a novel pure
transformer-based mask vision transformer (MVT) for FER in the wild, which
consists of two modules: a transformer-based mask generation network (MGN) to
generate a mask that can filter out complex backgrounds and occlusion of face
images, and a dynamic relabeling module to rectify incorrect labels in FER
datasets in the wild. Extensive experimental results demonstrate that our MVT
outperforms state-of-the-art methods on RAF-DB with 88.62%, FERPlus with
89.22%, and AffectNet-7 with 64.57%, respectively, and achieves a comparable
result on AffectNet-8 with 61.40%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hanting Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sui_M/0/1/0/all/0/1"&gt;Mingzhe Sui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1"&gt;Feng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zhengjun Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Feng Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SSC: Semantic Scan Context for Large-Scale Place Recognition. (arXiv:2107.00382v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00382</id>
        <link href="http://arxiv.org/abs/2107.00382"/>
        <updated>2021-07-13T01:59:35.345Z</updated>
        <summary type="html"><![CDATA[Place recognition gives a SLAM system the ability to correct cumulative
errors. Unlike images that contain rich texture features, point clouds are
almost pure geometric information which makes place recognition based on point
clouds challenging. Existing works usually encode low-level features such as
coordinate, normal, reflection intensity, etc., as local or global descriptors
to represent scenes. Besides, they often ignore the translation between point
clouds when matching descriptors. Different from most existing methods, we
explore the use of high-level features, namely semantics, to improve the
descriptor's representation ability. Also, when matching descriptors, we try to
correct the translation between point clouds to improve accuracy. Concretely,
we propose a novel global descriptor, Semantic Scan Context, which explores
semantic information to represent scenes more effectively. We also present a
two-step global semantic ICP to obtain the 3D pose (x, y, yaw) used to align
the point cloud to improve matching performance. Our experiments on the KITTI
dataset show that our approach outperforms the state-of-the-art methods with a
large margin. Our code is available at: https://github.com/lilin-hitcrt/SSC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1"&gt;Xin Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xiangrui Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1"&gt;Tianxin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weather and Light Level Classification for Autonomous Driving: Dataset, Baseline and Active Learning. (arXiv:2104.14042v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14042</id>
        <link href="http://arxiv.org/abs/2104.14042"/>
        <updated>2021-07-13T01:59:35.338Z</updated>
        <summary type="html"><![CDATA[Autonomous driving is rapidly advancing, and Level 2 functions are becoming a
standard feature. One of the foremost outstanding hurdles is to obtain robust
visual perception in harsh weather and low light conditions where accuracy
degradation is severe. It is critical to have a weather classification model to
decrease visual perception confidence during these scenarios. Thus, we have
built a new dataset for weather (fog, rain, and snow) classification and light
level (bright, moderate, and low) classification. Furthermore, we provide
street type (asphalt, grass, and cobblestone) classification, leading to 9
labels. Each image has three labels corresponding to weather, light level, and
street type. We recorded the data utilizing an industrial front camera of RCCC
(red/clear) format with a resolution of $1024\times1084$. We collected 15k
video sequences and sampled 60k images. We implement an active learning
framework to reduce the dataset's redundancy and find the optimal set of frames
for training a model. We distilled the 60k images further to 1.1k images, which
will be shared publicly after privacy anonymization. There is no public dataset
for weather and light level classification focused on autonomous driving to the
best of our knowledge. The baseline ResNet18 network used for weather
classification achieves state-of-the-art results in two non-automotive weather
classification public datasets but significantly lower accuracy on our proposed
dataset, demonstrating it is not saturated and needs further research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dhananjaya_M/0/1/0/all/0/1"&gt;Mahesh M Dhananjaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Varun Ravi Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1"&gt;Senthil Yogamani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Framework and Benchmarking Study for Counterfactual Generating Methods on Tabular Data. (arXiv:2107.04680v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04680</id>
        <link href="http://arxiv.org/abs/2107.04680"/>
        <updated>2021-07-13T01:59:35.321Z</updated>
        <summary type="html"><![CDATA[Counterfactual explanations are viewed as an effective way to explain machine
learning predictions. This interest is reflected by a relatively young
literature with already dozens of algorithms aiming to generate such
explanations. These algorithms are focused on finding how features can be
modified to change the output classification. However, this rather general
objective can be achieved in different ways, which brings about the need for a
methodology to test and benchmark these algorithms. The contributions of this
work are manifold: First, a large benchmarking study of 10 algorithmic
approaches on 22 tabular datasets is performed, using 9 relevant evaluation
metrics. Second, the introduction of a novel, first of its kind, framework to
test counterfactual generation algorithms. Third, a set of objective metrics to
evaluate and compare counterfactual results. And finally, insight from the
benchmarking results that indicate which approaches obtain the best performance
on what type of dataset. This benchmarking study and framework can help
practitioners in determining which technique and building blocks most suit
their context, and can help researchers in the design and evaluation of current
and future counterfactual generation algorithms. Our findings show that,
overall, there's no single best algorithm to generate counterfactual
explanations as the performance highly depends on properties related to the
dataset, model, score and factual point specificities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mazzine_R/0/1/0/all/0/1"&gt;Raphael Mazzine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martens_D/0/1/0/all/0/1"&gt;David Martens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyperspectral and Multispectral Classification for Coastal Wetland Using Depthwise Feature Interaction Network. (arXiv:2106.06896v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06896</id>
        <link href="http://arxiv.org/abs/2106.06896"/>
        <updated>2021-07-13T01:59:35.315Z</updated>
        <summary type="html"><![CDATA[The monitoring of coastal wetlands is of great importance to the protection
of marine and terrestrial ecosystems. However, due to the complex environment,
severe vegetation mixture, and difficulty of access, it is impossible to
accurately classify coastal wetlands and identify their species with
traditional classifiers. Despite the integration of multisource remote sensing
data for performance enhancement, there are still challenges with acquiring and
exploiting the complementary merits from multisource data. In this paper, the
Deepwise Feature Interaction Network (DFINet) is proposed for wetland
classification. A depthwise cross attention module is designed to extract
self-correlation and cross-correlation from multisource feature pairs. In this
way, meaningful complementary information is emphasized for classification.
DFINet is optimized by coordinating consistency loss, discrimination loss, and
classification loss. Accordingly, DFINet reaches the standard solution-space
under the regularity of loss functions, while the spatial consistency and
feature discrimination are preserved. Comprehensive experimental results on two
hyperspectral and multispectral wetland datasets demonstrate that the proposed
DFINet outperforms other competitive methods in terms of overall accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yunhao Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mengmeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianbu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Weiwei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1"&gt;Ran Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1"&gt;Qian Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Space Targeted Attacks by Statistic Alignment. (arXiv:2105.11645v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11645</id>
        <link href="http://arxiv.org/abs/2105.11645"/>
        <updated>2021-07-13T01:59:35.308Z</updated>
        <summary type="html"><![CDATA[By adding human-imperceptible perturbations to images, DNNs can be easily
fooled. As one of the mainstream methods, feature space targeted attacks
perturb images by modulating their intermediate feature maps, for the
discrepancy between the intermediate source and target features is minimized.
However, the current choice of pixel-wise Euclidean Distance to measure the
discrepancy is questionable because it unreasonably imposes a
spatial-consistency constraint on the source and target features. Intuitively,
an image can be categorized as "cat" no matter the cat is on the left or right
of the image. To address this issue, we propose to measure this discrepancy
using statistic alignment. Specifically, we design two novel approaches called
Pair-wise Alignment Attack and Global-wise Alignment Attack, which attempt to
measure similarities between feature maps by high-order statistics with
translation invariance. Furthermore, we systematically analyze the layer-wise
transferability with varied difficulties to obtain highly reliable attacks.
Extensive experiments verify the effectiveness of our proposed method, and it
outperforms the state-of-the-art algorithms by a large margin. Our code is
publicly available at https://github.com/yaya-cheng/PAA-GAA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Lianli Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yaya Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qilong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jingkuan Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cells are Actors: Social Network Analysis with Classical ML for SOTA Histology Image Classification. (arXiv:2106.15299v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15299</id>
        <link href="http://arxiv.org/abs/2106.15299"/>
        <updated>2021-07-13T01:59:35.300Z</updated>
        <summary type="html"><![CDATA[Digitization of histology images and the advent of new computational methods,
like deep learning, have helped the automatic grading of colorectal
adenocarcinoma cancer (CRA). Present automated CRA grading methods, however,
usually use tiny image patches and thus fail to integrate the entire tissue
micro-architecture for grading purposes. To tackle these challenges, we propose
to use a statistical network analysis method to describe the complex structure
of the tissue micro-environment by modelling nuclei and their connections as a
network. We show that by analyzing only the interactions between the cells in a
network, we can extract highly discriminative statistical features for CRA
grading. Unlike other deep learning or convolutional graph-based approaches,
our method is highly scalable (can be used for cell networks consist of
millions of nodes), completely explainable, and computationally inexpensive. We
create cell networks on a broad CRC histology image dataset, experiment with
our method, and report state-of-the-art performance for the prediction of
three-class CRA grading.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zamanitajeddin_N/0/1/0/all/0/1"&gt;Neda Zamanitajeddin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jahanifar_M/0/1/0/all/0/1"&gt;Mostafa Jahanifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajpoot_N/0/1/0/all/0/1"&gt;Nasir Rajpoot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Optimization of Hadamard Sensing and Reconstruction in Compressed Sensing Fluorescence Microscopy. (arXiv:2105.07961v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07961</id>
        <link href="http://arxiv.org/abs/2105.07961"/>
        <updated>2021-07-13T01:59:35.295Z</updated>
        <summary type="html"><![CDATA[Compressed sensing fluorescence microscopy (CS-FM) proposes a scheme whereby
less measurements are collected during sensing and reconstruction is performed
to recover the image. Much work has gone into optimizing the sensing and
reconstruction portions separately. We propose a method of jointly optimizing
both sensing and reconstruction end-to-end under a total measurement
constraint, enabling learning of the optimal sensing scheme concurrently with
the parameters of a neural network-based reconstruction network. We train our
model on a rich dataset of confocal, two-photon, and wide-field microscopy
images comprising of a variety of biological samples. We show that our method
outperforms several baseline sensing schemes and a regularized regression
reconstruction algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_A/0/1/0/all/0/1"&gt;Alan Q. Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+LaViolette_A/0/1/0/all/0/1"&gt;Aaron K. LaViolette&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moon_L/0/1/0/all/0/1"&gt;Leo Moon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chris Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sabuncu_M/0/1/0/all/0/1"&gt;Mert R. Sabuncu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble-based Semi-supervised Learning to Improve Noisy Soiling Annotations in Autonomous Driving. (arXiv:2105.07930v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07930</id>
        <link href="http://arxiv.org/abs/2105.07930"/>
        <updated>2021-07-13T01:59:35.275Z</updated>
        <summary type="html"><![CDATA[Manual annotation of soiling on surround view cameras is a very challenging
and expensive task. The unclear boundary for various soiling categories like
water drops or mud particles usually results in a large variance in the
annotation quality. As a result, the models trained on such poorly annotated
data are far from being optimal. In this paper, we focus on handling such noisy
annotations via pseudo-label driven ensemble model which allow us to quickly
spot problematic annotations and in most cases also sufficiently fixing them.
We train a soiling segmentation model on both noisy and refined labels and
demonstrate significant improvements using the refined annotations. It also
illustrates that it is possible to effectively refine lower cost coarse
annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Uricar_M/0/1/0/all/0/1"&gt;Michal Uricar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1"&gt;Ganesh Sistu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yahiaoui_L/0/1/0/all/0/1"&gt;Lucie Yahiaoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1"&gt;Senthil Yogamani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lifelong Mixture of Variational Autoencoders. (arXiv:2107.04694v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04694</id>
        <link href="http://arxiv.org/abs/2107.04694"/>
        <updated>2021-07-13T01:59:35.269Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an end-to-end lifelong learning mixture of experts.
Each expert is implemented by a Variational Autoencoder (VAE). The experts in
the mixture system are jointly trained by maximizing a mixture of individual
component evidence lower bounds (MELBO) on the log-likelihood of the given
training samples. The mixing coefficients in the mixture, control the
contributions of each expert in the goal representation. These are sampled from
a Dirichlet distribution whose parameters are determined through non-parametric
estimation during lifelong learning. The model can learn new tasks fast when
these are similar to those previously learnt. The proposed Lifelong mixture of
VAE (L-MVAE) expands its architecture with new components when learning a
completely new task. After the training, our model can automatically determine
the relevant expert to be used when fed with new data samples. This mechanism
benefits both the memory efficiency and the required computational cost as only
one expert is used during the inference. The L-MVAE inference model is able to
perform interpolation in the joint latent space across the data domains
associated with different tasks and is shown to be efficient for disentangled
learning representation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1"&gt;Fei Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1"&gt;Adrian G. Bors&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Progressive Joint Low-light Enhancement and Noise Removal for Raw Images. (arXiv:2106.14844v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14844</id>
        <link href="http://arxiv.org/abs/2106.14844"/>
        <updated>2021-07-13T01:59:35.258Z</updated>
        <summary type="html"><![CDATA[Low-light imaging on mobile devices is typically challenging due to
insufficient incident light coming through the relatively small aperture,
resulting in a low signal-to-noise ratio. Most of the previous works on
low-light image processing focus either only on a single task such as
illumination adjustment, color enhancement, or noise removal; or on a joint
illumination adjustment and denoising task that heavily relies on short-long
exposure image pairs collected from specific camera models, and thus these
approaches are less practical and generalizable in real-world settings where
camera-specific joint enhancement and restoration is required. To tackle this
problem, in this paper, we propose a low-light image processing framework that
performs joint illumination adjustment, color enhancement, and denoising.
Considering the difficulty in model-specific data collection and the ultra-high
definition of the captured images, we design two branches: a coefficient
estimation branch as well as a joint enhancement and denoising branch. The
coefficient estimation branch works in a low-resolution space and predicts the
coefficients for enhancement via bilateral learning, whereas the joint
enhancement and denoising branch works in a full-resolution space and
progressively performs joint enhancement and denoising. In contrast to existing
methods, our framework does not need to recollect massive data when being
adapted to another camera model, which significantly reduces the efforts
required to fine-tune our approach for practical usage. Through extensive
experiments, we demonstrate its great potential in real-world low-light imaging
applications when compared with current state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yucheng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jung_S/0/1/0/all/0/1"&gt;Seung-Won Jung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One Map Does Not Fit All: Evaluating Saliency Map Explanation on Multi-Modal Medical Images. (arXiv:2107.05047v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05047</id>
        <link href="http://arxiv.org/abs/2107.05047"/>
        <updated>2021-07-13T01:59:35.239Z</updated>
        <summary type="html"><![CDATA[Being able to explain the prediction to clinical end-users is a necessity to
leverage the power of AI models for clinical decision support. For medical
images, saliency maps are the most common form of explanation. The maps
highlight important features for AI model's prediction. Although many saliency
map methods have been proposed, it is unknown how well they perform on
explaining decisions on multi-modal medical images, where each modality/channel
carries distinct clinical meanings of the same underlying biomedical
phenomenon. Understanding such modality-dependent features is essential for
clinical users' interpretation of AI decisions. To tackle this clinically
important but technically ignored problem, we propose the MSFI
(Modality-Specific Feature Importance) metric to examine whether saliency maps
can highlight modality-specific important features. MSFI encodes the clinical
requirements on modality prioritization and modality-specific feature
localization. Our evaluations on 16 commonly used saliency map methods,
including a clinician user study, show that although most saliency map methods
captured modality importance information in general, most of them failed to
highlight modality-specific important features consistently and precisely. The
evaluation results guide the choices of saliency map methods and provide
insights to propose new ones targeting clinical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1"&gt;Weina Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoxiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamarneh_G/0/1/0/all/0/1"&gt;Ghassan Hamarneh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two-Stream Consensus Network: Submission to HACS Challenge 2021 Weakly-Supervised Learning Track. (arXiv:2106.10829v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10829</id>
        <link href="http://arxiv.org/abs/2106.10829"/>
        <updated>2021-07-13T01:59:35.233Z</updated>
        <summary type="html"><![CDATA[This technical report presents our solution to the HACS Temporal Action
Localization Challenge 2021, Weakly-Supervised Learning Track. The goal of
weakly-supervised temporal action localization is to temporally locate and
classify action of interest in untrimmed videos given only video-level labels.
We adopt the two-stream consensus network (TSCN) as the main framework in this
challenge. The TSCN consists of a two-stream base model training procedure and
a pseudo ground truth learning procedure. The base model training encourages
the model to predict reliable predictions based on single modality (i.e., RGB
or optical flow), based on the fusion of which a pseudo ground truth is
generated and in turn used as supervision to train the base models. On the HACS
v1.1.1 dataset, without fine-tuning the feature-extraction I3D models, our
method achieves 22.20% on the validation set and 21.68% on the testing set in
terms of average mAP. Our solution ranked the 2rd in this challenge, and we
hope our method can serve as a baseline for future academic research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1"&gt;Yuanhao Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Le Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1"&gt;David Doermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Junsong Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles. (arXiv:2106.11810v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11810</id>
        <link href="http://arxiv.org/abs/2106.11810"/>
        <updated>2021-07-13T01:59:35.202Z</updated>
        <summary type="html"><![CDATA[In this work, we propose the world's first closed-loop ML-based planning
benchmark for autonomous driving. While there is a growing body of ML-based
motion planners, the lack of established datasets and metrics has limited the
progress in this area. Existing benchmarks for autonomous vehicle motion
prediction have focused on short-term motion forecasting, rather than long-term
planning. This has led previous works to use open-loop evaluation with L2-based
metrics, which are not suitable for fairly evaluating long-term planning. Our
benchmark overcomes these limitations by introducing a large-scale driving
dataset, lightweight closed-loop simulator, and motion-planning-specific
metrics. We provide a high-quality dataset with 1500h of human driving data
from 4 cities across the US and Asia with widely varying traffic patterns
(Boston, Pittsburgh, Las Vegas and Singapore). We will provide a closed-loop
simulation framework with reactive agents and provide a large set of both
general and scenario-specific planning metrics. We plan to release the dataset
at NeurIPS 2021 and organize benchmark challenges starting in early 2022.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Caesar_H/0/1/0/all/0/1"&gt;Holger Caesar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kabzan_J/0/1/0/all/0/1"&gt;Juraj Kabzan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1"&gt;Kok Seang Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fong_W/0/1/0/all/0/1"&gt;Whye Kit Fong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolff_E/0/1/0/all/0/1"&gt;Eric Wolff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lang_A/0/1/0/all/0/1"&gt;Alex Lang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fletcher_L/0/1/0/all/0/1"&gt;Luke Fletcher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beijbom_O/0/1/0/all/0/1"&gt;Oscar Beijbom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Omari_S/0/1/0/all/0/1"&gt;Sammy Omari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Surface Normal Constraint for Depth Estimation. (arXiv:2103.15483v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15483</id>
        <link href="http://arxiv.org/abs/2103.15483"/>
        <updated>2021-07-13T01:59:35.196Z</updated>
        <summary type="html"><![CDATA[We present a novel method for single image depth estimation using surface
normal constraints. Existing depth estimation methods either suffer from the
lack of geometric constraints, or are limited to the difficulty of reliably
capturing geometric context, which leads to a bottleneck of depth estimation
quality. We therefore introduce a simple yet effective method, named Adaptive
Surface Normal (ASN) constraint, to effectively correlate the depth estimation
with geometric consistency. Our key idea is to adaptively determine the
reliable local geometry from a set of randomly sampled candidates to derive
surface normal constraint, for which we measure the consistency of the
geometric contextual features. As a result, our method can faithfully
reconstruct the 3D geometry and is robust to local shape variations, such as
boundaries, sharp corners and noises. We conduct extensive evaluations and
comparisons using public datasets. The experimental results demonstrate our
method outperforms the state-of-the-art methods and has superior efficiency and
robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1"&gt;Xiaoxiao Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Cheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingjie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1"&gt;Ruigang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenping Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BEVDetNet: Bird's Eye View LiDAR Point Cloud based Real-time 3D Object Detection for Autonomous Driving. (arXiv:2104.10780v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10780</id>
        <link href="http://arxiv.org/abs/2104.10780"/>
        <updated>2021-07-13T01:59:35.184Z</updated>
        <summary type="html"><![CDATA[3D object detection based on LiDAR point clouds is a crucial module in
autonomous driving particularly for long range sensing. Most of the research is
focused on achieving higher accuracy and these models are not optimized for
deployment on embedded systems from the perspective of latency and power
efficiency. For high speed driving scenarios, latency is a crucial parameter as
it provides more time to react to dangerous situations. Typically a voxel or
point-cloud based 3D convolution approach is utilized for this module. Firstly,
they are inefficient on embedded platforms as they are not suitable for
efficient parallelization. Secondly, they have a variable runtime due to level
of sparsity of the scene which is against the determinism needed in a safety
system. In this work, we aim to develop a very low latency algorithm with fixed
runtime. We propose a novel semantic segmentation architecture as a single
unified model for object center detection using key points, box predictions and
orientation prediction using binned classification in a simpler Bird's Eye View
(BEV) 2D representation. The proposed architecture can be trivially extended to
include semantic segmentation classes like road without any additional
computation. The proposed model has a latency of 4 ms on the embedded Nvidia
Xavier platform. The model is 5X faster than other top accuracy models with a
minimal accuracy degradation of 2% in Average Precision at IoU=0.5 on KITTI
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohapatra_S/0/1/0/all/0/1"&gt;Sambit Mohapatra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1"&gt;Senthil Yogamani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gotzig_H/0/1/0/all/0/1"&gt;Heinrich Gotzig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milz_S/0/1/0/all/0/1"&gt;Stefan Milz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mader_P/0/1/0/all/0/1"&gt;Patrick Mader&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Security in Next Generation Mobile Payment Systems: A Comprehensive Survey. (arXiv:2105.12097v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12097</id>
        <link href="http://arxiv.org/abs/2105.12097"/>
        <updated>2021-07-13T01:59:35.166Z</updated>
        <summary type="html"><![CDATA[Cash payment is still king in several markets, accounting for more than 90\
of the payments in almost all the developing countries. The usage of mobile
phones is pretty ordinary in this present era. Mobile phones have become an
inseparable friend for many users, serving much more than just communication
tools. Every subsequent person is heavily relying on them due to multifaceted
usage and affordability. Every person wants to manage his/her daily
transactions and related issues by using his/her mobile phone. With the rise
and advancements of mobile-specific security, threats are evolving as well. In
this paper, we provide a survey of various security models for mobile phones.
We explore multiple proposed models of the mobile payment system (MPS), their
technologies and comparisons, payment methods, different security mechanisms
involved in MPS, and provide analysis of the encryption technologies,
authentication methods, and firewall in MPS. We also present current challenges
and future directions of mobile phone security.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_W/0/1/0/all/0/1"&gt;Waqas Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasool_A/0/1/0/all/0/1"&gt;Amir Rasool&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1"&gt;Neeraj Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+RehmanJaved_A/0/1/0/all/0/1"&gt;Abdul RehmanJaved&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gadekallu_T/0/1/0/all/0/1"&gt;Thippa Reddy Gadekallu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jalil_Z/0/1/0/all/0/1"&gt;Zunera Jalil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kryvinska_N/0/1/0/all/0/1"&gt;Natalia Kryvinska&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy Preserving Domain Adaptation for Semantic Segmentation of Medical Images. (arXiv:2101.00522v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00522</id>
        <link href="http://arxiv.org/abs/2101.00522"/>
        <updated>2021-07-13T01:59:35.157Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) have led to significant improvements in
tasks involving semantic segmentation of images. CNNs are vulnerable in the
area of biomedical image segmentation because of distributional gap between two
source and target domains with different data modalities which leads to domain
shift. Domain shift makes data annotations in new modalities necessary because
models must be retrained from scratch. Unsupervised domain adaptation (UDA) is
proposed to adapt a model to new modalities using solely unlabeled target
domain data. Common UDA algorithms require access to data points in the source
domain which may not be feasible in medical imaging due to privacy concerns. In
this work, we develop an algorithm for UDA in a privacy-constrained setting,
where the source domain data is inaccessible. Our idea is based on encoding the
information from the source samples into a prototypical distribution that is
used as an intermediate distribution for aligning the target domain
distribution with the source domain distribution. We demonstrate the
effectiveness of our algorithm by comparing it to state-of-the-art medical
image semantic segmentation approaches on two medical image semantic
segmentation datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stan_S/0/1/0/all/0/1"&gt;Serban Stan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1"&gt;Mohammad Rostami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles. (arXiv:2104.00946v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00946</id>
        <link href="http://arxiv.org/abs/2104.00946"/>
        <updated>2021-07-13T01:59:35.149Z</updated>
        <summary type="html"><![CDATA[Human behavior understanding with unmanned aerial vehicles (UAVs) is of great
significance for a wide range of applications, which simultaneously brings an
urgent demand of large, challenging, and comprehensive benchmarks for the
development and evaluation of UAV-based models. However, existing benchmarks
have limitations in terms of the amount of captured data, types of data
modalities, categories of provided tasks, and diversities of subjects and
environments. Here we propose a new benchmark - UAVHuman - for human behavior
understanding with UAVs, which contains 67,428 multi-modal video sequences and
119 subjects for action recognition, 22,476 frames for pose estimation, 41,290
frames and 1,144 identities for person re-identification, and 22,263 frames for
attribute recognition. Our dataset was collected by a flying UAV in multiple
urban and rural districts in both daytime and nighttime over three months,
hence covering extensive diversities w.r.t subjects, backgrounds,
illuminations, weathers, occlusions, camera motions, and UAV flying attitudes.
Such a comprehensive and challenging benchmark shall be able to promote the
research of UAV-based human behavior understanding, including action
recognition, pose estimation, re-identification, and attribute recognition.
Furthermore, we propose a fisheye-based action recognition method that
mitigates the distortions in fisheye videos via learning unbounded
transformations guided by flat RGB videos. Experiments show the efficacy of our
method on the UAV-Human dataset. The project page:
https://github.com/SUTDCV/UAV-Human]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianjiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1"&gt;Yun Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenqian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhiheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Remote Sensing Image Change Detection with Transformers. (arXiv:2103.00208v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00208</id>
        <link href="http://arxiv.org/abs/2103.00208"/>
        <updated>2021-07-13T01:59:35.142Z</updated>
        <summary type="html"><![CDATA[Modern change detection (CD) has achieved remarkable success by the powerful
discriminative ability of deep convolutions. However, high-resolution remote
sensing CD remains challenging due to the complexity of objects in the scene.
Objects with the same semantic concept may show distinct spectral
characteristics at different times and spatial locations. Most recent CD
pipelines using pure convolutions are still struggling to relate long-range
concepts in space-time. Non-local self-attention approaches show promising
performance via modeling dense relations among pixels, yet are computationally
inefficient. Here, we propose a bitemporal image transformer (BIT) to
efficiently and effectively model contexts within the spatial-temporal domain.
Our intuition is that the high-level concepts of the change of interest can be
represented by a few visual words, i.e., semantic tokens. To achieve this, we
express the bitemporal image into a few tokens, and use a transformer encoder
to model contexts in the compact token-based space-time. The learned
context-rich tokens are then feedback to the pixel-space for refining the
original features via a transformer decoder. We incorporate BIT in a deep
feature differencing-based CD framework. Extensive experiments on three CD
datasets demonstrate the effectiveness and efficiency of the proposed method.
Notably, our BIT-based model significantly outperforms the purely convolutional
baseline using only 3 times lower computational costs and model parameters.
Based on a naive backbone (ResNet18) without sophisticated structures (e.g.,
FPN, UNet), our model surpasses several state-of-the-art CD methods, including
better than four recent attention-based methods in terms of efficiency and
accuracy. Our code is available at https://github.com/justchenhao/BIT\_CD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1"&gt;Zipeng Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1"&gt;Zhenwei Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Visual Models using a Knowledge Graph as a Trainer. (arXiv:2102.08747v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08747</id>
        <link href="http://arxiv.org/abs/2102.08747"/>
        <updated>2021-07-13T01:59:35.134Z</updated>
        <summary type="html"><![CDATA[Traditional computer vision approaches, based on neural networks (NN), are
typically trained on a large amount of image data. By minimizing the
cross-entropy loss between a prediction and a given class label, the NN and its
visual embedding space are learned to fulfill a given task. However, due to the
sole dependence on the image data distribution of the training domain, these
models tend to fail when applied to a target domain that differs from their
source domain. To learn a more robust NN to domain shifts, we propose the
knowledge graph neural network (KG-NN), a neuro-symbolic approach that
supervises the training using image-data-invariant auxiliary knowledge. The
auxiliary knowledge is first encoded in a knowledge graph with respective
concepts and their relationships, which is then transformed into a dense vector
representation via an embedding method. Using a contrastive loss function,
KG-NN learns to adapt its visual embedding space and thus its weights according
to the image-data invariant knowledge graph embedding space. We evaluate KG-NN
on visual transfer learning tasks for classification using the mini-ImageNet
dataset and its derivatives, as well as road sign recognition datasets from
Germany and China. The results show that a visual model trained with a
knowledge graph as a trainer outperforms a model trained with cross-entropy in
all experiments, in particular when the domain gap increases. Besides better
performance and stronger robustness to domain shifts, these KG-NN adapts to
multiple datasets and classes without suffering heavily from catastrophic
forgetting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Monka_S/0/1/0/all/0/1"&gt;Sebastian Monka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Halilaj_L/0/1/0/all/0/1"&gt;Lavdim Halilaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmid_S/0/1/0/all/0/1"&gt;Stefan Schmid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rettinger_A/0/1/0/all/0/1"&gt;Achim Rettinger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation. (arXiv:2102.08005v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08005</id>
        <link href="http://arxiv.org/abs/2102.08005"/>
        <updated>2021-07-13T01:59:35.127Z</updated>
        <summary type="html"><![CDATA[Medical image segmentation - the prerequisite of numerous clinical needs -
has been significantly prospered by recent advances in convolutional neural
networks (CNNs). However, it exhibits general limitations on modeling explicit
long-range relation, and existing cures, resorting to building deep encoders
along with aggressive downsampling operations, leads to redundant deepened
networks and loss of localized details. Hence, the segmentation task awaits a
better solution to improve the efficiency of modeling global contexts while
maintaining a strong grasp of low-level details. In this paper, we propose a
novel parallel-in-branch architecture, TransFuse, to address this challenge.
TransFuse combines Transformers and CNNs in a parallel style, where both global
dependency and low-level spatial details can be efficiently captured in a much
shallower manner. Besides, a novel fusion technique - BiFusion module is
created to efficiently fuse the multi-level features from both branches.
Extensive experiments demonstrate that TransFuse achieves the newest
state-of-the-art results on both 2D and 3D medical image sets including polyp,
skin lesion, hip, and prostate segmentation, with significant parameter
decrease and inference speed improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yundong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Huiye Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1"&gt;Qiang Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[More Reliable AI Solution: Breast Ultrasound Diagnosis Using Multi-AI Combination. (arXiv:2101.02639v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02639</id>
        <link href="http://arxiv.org/abs/2101.02639"/>
        <updated>2021-07-13T01:59:35.121Z</updated>
        <summary type="html"><![CDATA[Objective: Breast cancer screening is of great significance in contemporary
women's health prevention. The existing machines embedded in the AI system do
not reach the accuracy that clinicians hope. How to make intelligent systems
more reliable is a common problem. Methods: 1) Ultrasound image
super-resolution: the SRGAN super-resolution network reduces the unclearness of
ultrasound images caused by the device itself and improves the accuracy and
generalization of the detection model. 2) In response to the needs of medical
images, we have improved the YOLOv4 and the CenterNet models. 3) Multi-AI
model: based on the respective advantages of different AI models, we employ two
AI models to determine clinical resuls cross validation. And we accept the same
results and refuses others. Results: 1) With the help of the super-resolution
model, the YOLOv4 model and the CenterNet model both increased the mAP score by
9.6% and 13.8%. 2) Two methods for transforming the target model into a
classification model are proposed. And the unified output is in a specified
format to facilitate the call of the molti-AI model. 3) In the classification
evaluation experiment, concatenated by the YOLOv4 model (sensitivity 57.73%,
specificity 90.08%) and the CenterNet model (sensitivity 62.64%, specificity
92.54%), the multi-AI model will refuse to make judgments on 23.55% of the
input data. Correspondingly, the performance has been greatly improved to
95.91% for the sensitivity and 96.02% for the specificity. Conclusion: Our work
makes the AI model more reliable in medical image diagnosis. Significance: 1)
The proposed method makes the target detection model more suitable for
diagnosing breast ultrasound images. 2) It provides a new idea for artificial
intelligence in medical diagnosis, which can more conveniently introduce target
detection models from other fields to serve medical lesion screening.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1"&gt;Jian Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1"&gt;Shuge Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1"&gt;Licong Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xiaona Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huabin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1"&gt;Desheng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1"&gt;Kehong Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trans-SVNet: Accurate Phase Recognition from Surgical Videos via Hybrid Embedding Aggregation Transformer. (arXiv:2103.09712v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09712</id>
        <link href="http://arxiv.org/abs/2103.09712"/>
        <updated>2021-07-13T01:59:35.095Z</updated>
        <summary type="html"><![CDATA[Real-time surgical phase recognition is a fundamental task in modern
operating rooms. Previous works tackle this task relying on architectures
arranged in spatio-temporal order, however, the supportive benefits of
intermediate spatial features are not considered. In this paper, we introduce,
for the first time in surgical workflow analysis, Transformer to reconsider the
ignored complementary effects of spatial and temporal features for accurate
surgical phase recognition. Our hybrid embedding aggregation Transformer fuses
cleverly designed spatial and temporal embeddings by allowing for active
queries based on spatial information from temporal embedding sequences. More
importantly, our framework processes the hybrid embeddings in parallel to
achieve a high inference speed. Our method is thoroughly validated on two large
surgical video datasets, i.e., Cholec80 and M2CAI16 Challenge datasets, and
outperforms the state-of-the-art approaches at a processing speed of 91 fps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xiaojie Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1"&gt;Yueming Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1"&gt;Yonghao Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1"&gt;Qi Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1"&gt;Pheng-Ann Heng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Discriminative Feature Learning for Deep Multi-view Clustering. (arXiv:2103.15069v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15069</id>
        <link href="http://arxiv.org/abs/2103.15069"/>
        <updated>2021-07-13T01:59:35.075Z</updated>
        <summary type="html"><![CDATA[Multi-view clustering is an important research topic due to its capability to
utilize complementary information from multiple views. However, there are few
methods to consider the negative impact caused by certain views with unclear
clustering structures, resulting in poor multi-view clustering performance. To
address this drawback, we propose self-supervised discriminative feature
learning for deep multi-view clustering (SDMVC). Concretely, deep autoencoders
are applied to learn embedded features for each view independently. To leverage
the multi-view complementary information, we concatenate all views' embedded
features to form the global features, which can overcome the negative impact of
some views' unclear clustering structures. In a self-supervised manner,
pseudo-labels are obtained to build a unified target distribution to perform
multi-view discriminative feature learning. During this process, global
discriminative information can be mined to supervise all views to learn more
discriminative features, which in turn are used to update the target
distribution. Besides, this unified target distribution can make SDMVC learn
consistent cluster assignments, which accomplishes the clustering consistency
of multiple views while preserving their features' diversity. Experiments on
various types of multi-view datasets show that SDMVC achieves state-of-the-art
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jie Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yazhou Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Huayi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhimeng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1"&gt;Lili Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_X/0/1/0/all/0/1"&gt;Xiaorong Pu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual-Task Mutual Learning for Semi-Supervised Medical Image Segmentation. (arXiv:2103.04708v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04708</id>
        <link href="http://arxiv.org/abs/2103.04708"/>
        <updated>2021-07-13T01:59:35.060Z</updated>
        <summary type="html"><![CDATA[The success of deep learning methods in medical image segmentation tasks
usually requires a large amount of labeled data. However, obtaining reliable
annotations is expensive and time-consuming. Semi-supervised learning has
attracted much attention in medical image segmentation by taking the advantage
of unlabeled data which is much easier to acquire. In this paper, we propose a
novel dual-task mutual learning framework for semi-supervised medical image
segmentation. Our framework can be formulated as an integration of two
individual segmentation networks based on two tasks: learning region-based
shape constraint and learning boundary-based surface mismatch. Different from
the one-way transfer between teacher and student networks, an ensemble of
dual-task students can learn collaboratively and implicitly explore useful
knowledge from each other during the training process. By jointly learning the
segmentation probability maps and signed distance maps of targets, our
framework can enforce the geometric shape constraint and learn more reliable
information. Experimental results demonstrate that our method achieves
performance gains by leveraging unlabeled data and outperforms the
state-of-the-art semi-supervised segmentation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yichi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jicong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Representation Learning via Maximization of Local Mutual Information. (arXiv:2103.04537v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04537</id>
        <link href="http://arxiv.org/abs/2103.04537"/>
        <updated>2021-07-13T01:59:35.053Z</updated>
        <summary type="html"><![CDATA[We propose and demonstrate a representation learning approach by maximizing
the mutual information between local features of images and text. The goal of
this approach is to learn useful image representations by taking advantage of
the rich information contained in the free text that describes the findings in
the image. Our method trains image and text encoders by encouraging the
resulting representations to exhibit high local mutual information. We make use
of recent advances in mutual information estimation with neural network
discriminators. We argue that the sum of local mutual information is typically
a lower bound on the global mutual information. Our experimental results in the
downstream image classification tasks demonstrate the advantages of using local
features for image-text representation learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1"&gt;Ruizhi Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moyer_D/0/1/0/all/0/1"&gt;Daniel Moyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cha_M/0/1/0/all/0/1"&gt;Miriam Cha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quigley_K/0/1/0/all/0/1"&gt;Keegan Quigley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berkowitz_S/0/1/0/all/0/1"&gt;Seth Berkowitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horng_S/0/1/0/all/0/1"&gt;Steven Horng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golland_P/0/1/0/all/0/1"&gt;Polina Golland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wells_W/0/1/0/all/0/1"&gt;William M. Wells&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Makes for End-to-End Object Detection?. (arXiv:2012.05780v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05780</id>
        <link href="http://arxiv.org/abs/2012.05780"/>
        <updated>2021-07-13T01:59:35.036Z</updated>
        <summary type="html"><![CDATA[Object detection has recently achieved a breakthrough for removing the last
one non-differentiable component in the pipeline, Non-Maximum Suppression
(NMS), and building up an end-to-end system. However, what makes for its
one-to-one prediction has not been well understood. In this paper, we first
point out that one-to-one positive sample assignment is the key factor, while,
one-to-many assignment in previous detectors causes redundant predictions in
inference. Second, we surprisingly find that even training with one-to-one
assignment, previous detectors still produce redundant predictions. We identify
that classification cost in matching cost is the main ingredient: (1) previous
detectors only consider location cost, (2) by additionally introducing
classification cost, previous detectors immediately produce one-to-one
prediction during inference. We introduce the concept of score gap to explore
the effect of matching cost. Classification cost enlarges the score gap by
choosing positive samples as those of highest score in the training iteration
and reducing noisy positive samples brought by only location cost. Finally, we
demonstrate the advantages of end-to-end object detection on crowded scenes.
The code is available at: \url{https://github.com/PeizeSun/OneNet}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1"&gt;Peize Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yi Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1"&gt;Enze Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1"&gt;Wenqi Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1"&gt;Zehuan Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Changhu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1"&gt;Ping Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Semantic Scene Completion: a Survey. (arXiv:2103.07466v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07466</id>
        <link href="http://arxiv.org/abs/2103.07466"/>
        <updated>2021-07-13T01:59:35.030Z</updated>
        <summary type="html"><![CDATA[Semantic Scene Completion (SSC) aims to jointly estimate the complete
geometry and semantics of a scene, assuming partial sparse input. In the last
years following the multiplication of large-scale 3D datasets, SSC has gained
significant momentum in the research community because it holds unresolved
challenges. Specifically, SSC lies in the ambiguous completion of large
unobserved areas and the weak supervision signal of the ground truth. This led
to a substantially increasing number of papers on the matter. This survey aims
to identify, compare and analyze the techniques providing a critical analysis
of the SSC literature on both methods and datasets. Throughout the paper, we
provide an in-depth analysis of the existing works covering all choices made by
the authors while highlighting the remaining avenues of research. SSC
performance of the SoA on the most popular datasets is also evaluated and
analyzed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roldao_L/0/1/0/all/0/1"&gt;Luis Roldao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1"&gt;Raoul de Charette&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verroust_Blondet_A/0/1/0/all/0/1"&gt;Anne Verroust-Blondet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A review of 3D human pose estimation algorithms for markerless motion capture. (arXiv:2010.06449v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.06449</id>
        <link href="http://arxiv.org/abs/2010.06449"/>
        <updated>2021-07-13T01:59:35.023Z</updated>
        <summary type="html"><![CDATA[Human pose estimation is a very active research field, stimulated by its
important applications in robotics, entertainment or health and sports
sciences, among others. Advances in convolutional networks triggered noticeable
improvements in 2D pose estimation, leading modern 3D markerless motion capture
techniques to an average error per joint of 20 mm. However, with the
proliferation of methods, it is becoming increasingly difficult to make an
informed choice. Here, we review the leading human pose estimation methods of
the past five years, focusing on metrics, benchmarks and method structures. We
propose a taxonomy based on accuracy, speed and robustness that we use to
classify de methods and derive directions for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Desmarais_Y/0/1/0/all/0/1"&gt;Yann Desmarais&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mottet_D/0/1/0/all/0/1"&gt;Denis Mottet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slangen_P/0/1/0/all/0/1"&gt;Pierre Slangen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Montesinos_P/0/1/0/all/0/1"&gt;Philippe Montesinos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Multiscale Correlations for Human Motion Prediction. (arXiv:2103.10674v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10674</id>
        <link href="http://arxiv.org/abs/2103.10674"/>
        <updated>2021-07-13T01:59:35.017Z</updated>
        <summary type="html"><![CDATA[In spite of the great progress in human motion prediction, it is still a
challenging task to predict those aperiodic and complicated motions. We believe
that to capture the correlations among human body components is the key to
understand the human motion. In this paper, we propose a novel multiscale graph
convolution network (MGCN) to address this problem. Firstly, we design an
adaptive multiscale interactional encoding module (MIEM) which is composed of
two sub modules: scale transformation module and scale interaction module to
learn the human body correlations. Secondly, we apply a coarse-to-fine decoding
strategy to decode the motions sequentially. We evaluate our approach on two
standard benchmark datasets for human motion prediction: Human3.6M and CMU
motion capture dataset. The experiments show that the proposed approach
achieves the state-of-the-art performance for both short-term and long-term
prediction especially in those complicated action category.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Honghong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1"&gt;Caili Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanjun Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Task-Oriented Low-Dose CT Image Denoising. (arXiv:2103.13557v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13557</id>
        <link href="http://arxiv.org/abs/2103.13557"/>
        <updated>2021-07-13T01:59:35.010Z</updated>
        <summary type="html"><![CDATA[The extensive use of medical CT has raised a public concern over the
radiation dose to the patient. Reducing the radiation dose leads to increased
CT image noise and artifacts, which can adversely affect not only the
radiologists judgement but also the performance of downstream medical image
analysis tasks. Various low-dose CT denoising methods, especially the recent
deep learning based approaches, have produced impressive results. However, the
existing denoising methods are all downstream-task-agnostic and neglect the
diverse needs of the downstream applications. In this paper, we introduce a
novel Task-Oriented Denoising Network (TOD-Net) with a task-oriented loss
leveraging knowledge from the downstream tasks. Comprehensive empirical
analysis shows that the task-oriented loss complements other task agnostic
losses by steering the denoiser to enhance the image quality in the task
related regions of interest. Such enhancement in turn brings general boosts on
the performance of various methods for the downstream task. The presented work
may shed light on the future development of context-aware image denoising
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiajin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chao_H/0/1/0/all/0/1"&gt;Hanqing Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xuanang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Niu_C/0/1/0/all/0/1"&gt;Chuang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1"&gt;Ge Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yan_P/0/1/0/all/0/1"&gt;Pingkun Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AMINN: Autoencoder-based Multiple Instance Neural Network Improves Outcome Prediction of Multifocal Liver Metastases. (arXiv:2012.06875v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06875</id>
        <link href="http://arxiv.org/abs/2012.06875"/>
        <updated>2021-07-13T01:59:34.991Z</updated>
        <summary type="html"><![CDATA[Colorectal cancer is one of the most common and lethal cancers and colorectal
cancer liver metastases (CRLM) is the major cause of death in patients with
colorectal cancer. Multifocality occurs frequently in CRLM, but is relatively
unexplored in CRLM outcome prediction. Most existing clinical and imaging
biomarkers do not take the imaging features of all multifocal lesions into
account. In this paper, we present an end-to-end autoencoder-based multiple
instance neural network (AMINN) for the prediction of survival outcomes in
multifocal CRLM patients using radiomic features extracted from
contrast-enhanced MRIs. Specifically, we jointly train an autoencoder to
reconstruct input features and a multiple instance network to make predictions
by aggregating information from all tumour lesions of a patient. Also, we
incorporate a two-step normalization technique to improve the training of deep
neural networks, built on the observation that the distributions of radiomic
features are almost always severely skewed. Experimental results empirically
validated our hypothesis that incorporating imaging features of all lesions
improves outcome prediction for multifocal cancer. The proposed AMINN framework
achieved an area under the ROC curve (AUC) of 0.70, which is 11.4% higher than
the best baseline method. A risk score based on the outputs of AMINN achieved
superior prediction in our multifocal CRLM cohort. The effectiveness of
incorporating all lesions and applying two-step normalization is demonstrated
by a series of ablation studies. A Keras implementation of AMINN is released.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jianan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheung_H/0/1/0/all/0/1"&gt;Helen M. C. Cheung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milot_L/0/1/0/all/0/1"&gt;Laurent Milot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martel_A/0/1/0/all/0/1"&gt;Anne L. Martel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alpha-Refine: Boosting Tracking Performance by Precise Bounding Box Estimation. (arXiv:2012.06815v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06815</id>
        <link href="http://arxiv.org/abs/2012.06815"/>
        <updated>2021-07-13T01:59:34.985Z</updated>
        <summary type="html"><![CDATA[Visual object tracking aims to precisely estimate the bounding box for the
given target, which is a challenging problem due to factors such as deformation
and occlusion. Many recent trackers adopt the multiple-stage tracking strategy
to improve the quality of bounding box estimation. These methods first coarsely
locate the target and then refine the initial prediction in the following
stages. However, existing approaches still suffer from limited precision, and
the coupling of different stages severely restricts the method's
transferability. This work proposes a novel, flexible, and accurate refinement
module called Alpha-Refine (AR), which can significantly improve the base
trackers' box estimation quality. By exploring a series of design options, we
conclude that the key to successful refinement is extracting and maintaining
detailed spatial information as much as possible. Following this principle,
Alpha-Refine adopts a pixel-wise correlation, a corner prediction head, and an
auxiliary mask head as the core components. Comprehensive experiments on
TrackingNet, LaSOT, GOT-10K, and VOT2020 benchmarks with multiple base trackers
show that our approach significantly improves the base trackers' performance
with little extra latency. The proposed Alpha-Refine method leads to a series
of strengthened trackers, among which the ARSiamRPN (AR strengthened SiamRPNpp)
and the ARDiMP50 (ARstrengthened DiMP50) achieve good efficiency-precision
trade-off, while the ARDiMPsuper (AR strengthened DiMP-super) achieves very
competitive performance at a real-time speed. Code and pretrained models are
available at https://github.com/MasterBin-IIAU/AlphaRefine.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1"&gt;Bin Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xinyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Huchuan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaoyun Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Attention Network: Accelerate Attention by Searching Where to Plug. (arXiv:2011.14058v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14058</id>
        <link href="http://arxiv.org/abs/2011.14058"/>
        <updated>2021-07-13T01:59:34.979Z</updated>
        <summary type="html"><![CDATA[Recently, many plug-and-play self-attention modules are proposed to enhance
the model generalization by exploiting the internal information of deep
convolutional neural networks (CNNs). Previous works lay an emphasis on the
design of attention module for specific functionality, e.g., light-weighted or
task-oriented attention. However, they ignore the importance of where to plug
in the attention module since they connect the modules individually with each
block of the entire CNN backbone for granted, leading to incremental
computational cost and number of parameters with the growth of network depth.
Thus, we propose a framework called Efficient Attention Network (EAN) to
improve the efficiency for the existing attention modules. In EAN, we leverage
the sharing mechanism (Huang et al. 2020) to share the attention module within
the backbone and search where to connect the shared attention module via
reinforcement learning. Finally, we obtain the attention network with sparse
connections between the backbone and modules, while (1) maintaining accuracy
(2) reducing extra parameter increment and (3) accelerating inference.
Extensive experiments on widely-used benchmarks and popular attention networks
show the effectiveness of EAN. Furthermore, we empirically illustrate that our
EAN has the capacity of transferring to other tasks and capturing the
informative features. The code is available at
https://github.com/gbup-group/EAN-efficient-attention-network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhongzhan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1"&gt;Senwei Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1"&gt;Mingfu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1"&gt;Wei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haizhao Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-learning of Pooling Layers for Character Recognition. (arXiv:2103.09528v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09528</id>
        <link href="http://arxiv.org/abs/2103.09528"/>
        <updated>2021-07-13T01:59:34.972Z</updated>
        <summary type="html"><![CDATA[In convolutional neural network-based character recognition, pooling layers
play an important role in dimensionality reduction and deformation
compensation. However, their kernel shapes and pooling operations are
empirically predetermined; typically, a fixed-size square kernel shape and max
pooling operation are used. In this paper, we propose a meta-learning framework
for pooling layers. As part of our framework, a parameterized pooling layer is
proposed in which the kernel shape and pooling operation are trainable using
two parameters, thereby allowing flexible pooling of the input data. We also
propose a meta-learning algorithm for the parameterized pooling layer, which
allows us to acquire a suitable pooling layer across multiple tasks. In the
experiment, we applied the proposed meta-learning framework to character
recognition tasks. The results demonstrate that a pooling layer that is
suitable across character recognition tasks was obtained via meta-learning, and
the obtained pooling layer improved the performance of the model in both
few-shot character recognition and noisy image recognition tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Otsuzuki_T/0/1/0/all/0/1"&gt;Takato Otsuzuki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1"&gt;Heon Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1"&gt;Seiichi Uchida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayashi_H/0/1/0/all/0/1"&gt;Hideaki Hayashi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Radiomics as Prior Knowledge for Thorax Disease Classification and Localization in Chest X-rays. (arXiv:2011.12506v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12506</id>
        <link href="http://arxiv.org/abs/2011.12506"/>
        <updated>2021-07-13T01:59:34.967Z</updated>
        <summary type="html"><![CDATA[Chest X-ray becomes one of the most common medical diagnoses due to its
noninvasiveness. The number of chest X-ray images has skyrocketed, but reading
chest X-rays still have been manually performed by radiologists, which creates
huge burnouts and delays. Traditionally, radiomics, as a subfield of radiology
that can extract a large number of quantitative features from medical images,
demonstrates its potential to facilitate medical imaging diagnosis before the
deep learning era. In this paper, we develop an end-to-end framework,
ChexRadiNet, that can utilize the radiomics features to improve the abnormality
classification performance. Specifically, ChexRadiNet first applies a
light-weight but efficient triplet-attention mechanism to classify the chest
X-rays and highlight the abnormal regions. Then it uses the generated class
activation map to extract radiomic features, which further guides our model to
learn more robust image features. After a number of iterations and with the
help of radiomic features, our framework can converge to more accurate image
regions. We evaluate the ChexRadiNet framework using three public datasets: NIH
ChestX-ray, CheXpert, and MIMIC-CXR. We find that ChexRadiNet outperforms the
state-of-the-art on both disease detection (0.843 in AUC) and localization
(0.679 in T(IoU) = 0.1). We will make the code publicly available at
https://github.com/bionlplab/lung_disease_detection_amia2021, with the hope
that this method can facilitate the development of automatic systems with a
higher-level understanding of the radiological world.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1"&gt;Yan Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chongyan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1"&gt;Liyan Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Mingquan Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaiswal_A/0/1/0/all/0/1"&gt;Ajay Jaiswal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Song Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tewfik_A/0/1/0/all/0/1"&gt;Ahmed Tewfik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shih_G/0/1/0/all/0/1"&gt;George Shih&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Ying Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1"&gt;Yifan Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Selection Based on Sparse Neural Network Layer with Normalizing Constraints. (arXiv:2012.06365v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06365</id>
        <link href="http://arxiv.org/abs/2012.06365"/>
        <updated>2021-07-13T01:59:34.949Z</updated>
        <summary type="html"><![CDATA[Feature selection is important step in machine learning since it has shown to
improve prediction accuracy while depressing the curse of dimensionality of
high dimensional data. The neural networks have experienced tremendous success
in solving many nonlinear learning problems. Here, we propose new
neural-network based feature selection approach that introduces two constrains,
the satisfying of which leads to sparse FS layer. We have performed extensive
experiments on synthetic and real world data to evaluate performance of the
proposed FS. In experiments we focus on the high dimension, low sample size
data since those represent the main challenge for feature selection. The
results confirm that proposed Feature Selection Based on Sparse Neural Network
Layer with Normalizing Constraints (SNEL-FS) is able to select the important
features and yields superior performance compared to other conventional FS
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bugata_P/0/1/0/all/0/1"&gt;Peter Bugata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drotar_P/0/1/0/all/0/1"&gt;Peter Drotar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compressive spectral image classification using 3D coded convolutional neural network. (arXiv:2009.11948v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.11948</id>
        <link href="http://arxiv.org/abs/2009.11948"/>
        <updated>2021-07-13T01:59:34.943Z</updated>
        <summary type="html"><![CDATA[Hyperspectral image classification (HIC) is an active research topic in
remote sensing. Hyperspectral images typically generate large data cubes posing
big challenges in data acquisition, storage, transmission and processing. To
overcome these limitations, this paper develops a novel deep learning HIC
approach based on compressive measurements of coded-aperture snapshot spectral
imagers (CASSI), without reconstructing the complete hyperspectral data cube. A
new kind of deep learning strategy, namely 3D coded convolutional neural
network (3D-CCNN) is proposed to efficiently solve for the classification
problem, where the hardware-based coded aperture is regarded as a pixel-wise
connected network layer. An end-to-end training method is developed to jointly
optimize the network parameters and the coded apertures with periodic
structures. The accuracy of classification is effectively improved by
exploiting the synergy between the deep learning network and coded apertures.
The superiority of the proposed method is assessed over the state-of-the-art
HIC methods on several hyperspectral datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xu Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xianhong Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arce_G/0/1/0/all/0/1"&gt;Gonzalo R. Arce&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from Self-Discrepancy via Multiple Co-teaching for Cross-Domain Person Re-Identification. (arXiv:2104.02265v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02265</id>
        <link href="http://arxiv.org/abs/2104.02265"/>
        <updated>2021-07-13T01:59:34.936Z</updated>
        <summary type="html"><![CDATA[Employing clustering strategy to assign unlabeled target images with pseudo
labels has become a trend for person re-identification (re-ID) algorithms in
domain adaptation. A potential limitation of these clustering-based methods is
that they always tend to introduce noisy labels, which will undoubtedly hamper
the performance of our re-ID system. To handle this limitation, an intuitive
solution is to utilize collaborative training to purify the pseudo label
quality. However, there exists a challenge that the complementarity of two
networks, which inevitably share a high similarity, becomes weakened gradually
as training process goes on; worse still, these approaches typically ignore to
consider the self-discrepancy of intra-class relations. To address this issue,
in this paper, we propose a multiple co-teaching framework for domain adaptive
person re-ID, opening up a promising direction about self-discrepancy problem
under unsupervised condition. On top of that, a mean-teaching mechanism is
leveraged to enlarge the difference and discover more complementary features.
Comprehensive experiments conducted on several large-scale datasets show that
our method achieves competitive performance compared with the
state-of-the-arts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1"&gt;Suncheng Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yuzhuo Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_M/0/1/0/all/0/1"&gt;Mengyuan Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Ting Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks. (arXiv:2011.13118v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.13118</id>
        <link href="http://arxiv.org/abs/2011.13118"/>
        <updated>2021-07-13T01:59:34.929Z</updated>
        <summary type="html"><![CDATA[We present a novel method for multi-view depth estimation from a single
video, which is a critical task in various applications, such as perception,
reconstruction and robot navigation. Although previous learning-based methods
have demonstrated compelling results, most works estimate depth maps of
individual video frames independently, without taking into consideration the
strong geometric and temporal coherence among the frames. Moreover, current
state-of-the-art (SOTA) models mostly adopt a fully 3D convolution network for
cost regularization and therefore require high computational cost, thus
limiting their deployment in real-world applications. Our method achieves
temporally coherent depth estimation results by using a novel Epipolar
Spatio-Temporal (EST) transformer to explicitly associate geometric and
temporal correlation with multiple estimated depth maps. Furthermore, to reduce
the computational cost, inspired by recent Mixture-of-Experts models, we design
a compact hybrid network consisting of a 2D context-aware network and a 3D
matching network which learn 2D context information and 3D disparity cues
separately. Extensive experiments demonstrate that our method achieves higher
accuracy in depth estimation and significant speedup than the SOTA methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1"&gt;Xiaoxiao Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingjie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenping Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simple and Effective VAE Training with Calibrated Decoders. (arXiv:2006.13202v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.13202</id>
        <link href="http://arxiv.org/abs/2006.13202"/>
        <updated>2021-07-13T01:59:34.921Z</updated>
        <summary type="html"><![CDATA[Variational autoencoders (VAEs) provide an effective and simple method for
modeling complex distributions. However, training VAEs often requires
considerable hyperparameter tuning to determine the optimal amount of
information retained by the latent variable. We study the impact of calibrated
decoders, which learn the uncertainty of the decoding distribution and can
determine this amount of information automatically, on the VAE performance.
While many methods for learning calibrated decoders have been proposed, many of
the recent papers that employ VAEs rely on heuristic hyperparameters and ad-hoc
modifications instead. We perform the first comprehensive comparative analysis
of calibrated decoder and provide recommendations for simple and effective VAE
training. Our analysis covers a range of image and video datasets and several
single-image and sequential VAE models. We further propose a simple but novel
modification to the commonly used Gaussian decoder, which computes the
prediction variance analytically. We observe empirically that using heuristic
modifications is not necessary with our method. Project website is at
https://orybkin.github.io/sigma-vae/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rybkin_O/0/1/0/all/0/1"&gt;Oleh Rybkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1"&gt;Kostas Daniilidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Align Deep Features for Oriented Object Detection. (arXiv:2008.09397v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.09397</id>
        <link href="http://arxiv.org/abs/2008.09397"/>
        <updated>2021-07-13T01:59:34.903Z</updated>
        <summary type="html"><![CDATA[The past decade has witnessed significant progress on detecting objects in
aerial images that are often distributed with large scale variations and
arbitrary orientations. However most of existing methods rely on heuristically
defined anchors with different scales, angles and aspect ratios and usually
suffer from severe misalignment between anchor boxes and axis-aligned
convolutional features, which leads to the common inconsistency between the
classification score and localization accuracy. To address this issue, we
propose a Single-shot Alignment Network (S$^2$A-Net) consisting of two modules:
a Feature Alignment Module (FAM) and an Oriented Detection Module (ODM). The
FAM can generate high-quality anchors with an Anchor Refinement Network and
adaptively align the convolutional features according to the anchor boxes with
a novel Alignment Convolution. The ODM first adopts active rotating filters to
encode the orientation information and then produces orientation-sensitive and
orientation-invariant features to alleviate the inconsistency between
classification score and localization accuracy. Besides, we further explore the
approach to detect objects in large-size images, which leads to a better
trade-off between speed and accuracy. Extensive experiments demonstrate that
our method can achieve state-of-the-art performance on two commonly used aerial
objects datasets (i.e., DOTA and HRSC2016) while keeping high efficiency. The
code is available at https://github.com/csuhan/s2anet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiaming Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1"&gt;Jian Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1"&gt;Gui-Song Xia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring intermediate representation for monocular vehicle pose estimation. (arXiv:2011.08464v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08464</id>
        <link href="http://arxiv.org/abs/2011.08464"/>
        <updated>2021-07-13T01:59:34.897Z</updated>
        <summary type="html"><![CDATA[We present a new learning-based framework to recover vehicle pose in SO(3)
from a single RGB image. In contrast to previous works that map from local
appearance to observation angles, we explore a progressive approach by
extracting meaningful Intermediate Geometrical Representations (IGRs) to
estimate egocentric vehicle orientation. This approach features a deep model
that transforms perceived intensities to IGRs, which are mapped to a 3D
representation encoding object orientation in the camera coordinate system.
Core problems are what IGRs to use and how to learn them more effectively. We
answer the former question by designing IGRs based on an interpolated cuboid
that derives from primitive 3D annotation readily. The latter question
motivates us to incorporate geometry knowledge with a new loss function based
on a projective invariant. This loss function allows unlabeled data to be used
in the training stage to improve representation learning. Without additional
labels, our system outperforms previous monocular RGB-based methods for joint
vehicle detection and pose estimation on the KITTI benchmark, achieving
performance even comparable to stereo methods. Code and pre-trained models are
available at this https URL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shichao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zengqiang Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1"&gt;Kwang-Ting Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridge Composite and Real: Towards End-to-end Deep Image Matting. (arXiv:2010.16188v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.16188</id>
        <link href="http://arxiv.org/abs/2010.16188"/>
        <updated>2021-07-13T01:59:34.814Z</updated>
        <summary type="html"><![CDATA[Extracting accurate foregrounds from natural images benefits many downstream
applications such as film production and augmented reality. However, the furry
characteristics and various appearance of the foregrounds, e.g., animal and
portrait, challenge existing matting methods, which usually require extra user
inputs such as trimap or scribbles. To resolve these problems, we study the
distinct roles of semantics and details for image matting and decompose the
task into two parallel sub-tasks: high-level semantic segmentation and
low-level details matting. Specifically, we propose a novel Glance and Focus
Matting network (GFM), which employs a shared encoder and two separate decoders
to learn both tasks in a collaborative manner for end-to-end natural image
matting. Besides, due to the limitation of available natural images in the
matting task, previous methods typically adopt composite images for training
and evaluation, which result in limited generalization ability on real-world
images. In this paper, we investigate the domain gap issue between composite
images and real-world images systematically by conducting comprehensive
analyses of various discrepancies between foreground and background images. We
find that a carefully designed composition route RSSN that aims to reduce the
discrepancies can lead to a better model with remarkable generalization
ability. Furthermore, we provide a benchmark containing 2,000 high-resolution
real-world animal images and 10,000 portrait images along with their manually
labeled alpha mattes to serve as a test bed for evaluating matting model's
generalization ability on real-world images. Comprehensive empirical studies
have demonstrated that GFM outperforms state-of-the-art methods and effectively
reduces the generalization error. The code and the dataset will be released.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jizhizi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maybank_S/0/1/0/all/0/1"&gt;Stephen J. Maybank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Segmenting overlapped cell clusters in biomedical images by concave point detection. (arXiv:2008.00997v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.00997</id>
        <link href="http://arxiv.org/abs/2008.00997"/>
        <updated>2021-07-13T01:59:34.807Z</updated>
        <summary type="html"><![CDATA[In this paper we propose a method to detect concave points as a first step to
segment overlapped objects on images. Given an image of an object cluster we
compute the curvature on each point of its contour. Then, we select regions
with the highest probability to contain an interest point, that is, regions
with higher curvature. Finally we obtain an interest point from each region and
we classify them between convex and concave. In order to evaluate the quality
of the concave point detection algorithm we constructed a synthetic dataset to
simulate overlapping objects, providing the position of the concave points as a
ground truth. As a case study, the performance of a well-known application is
evaluated, such as the splitting of overlapped cells in images of peripheral
blood smears samples of patients with sickle cell anaemia. We used the proposed
method to detect the concave points in clusters of cells and then we separate
this clusters by ellipse fitting. Experimentally we demonstrate that our
proposal has a better performance than the state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miro_Nicolau_M/0/1/0/all/0/1"&gt;Miquel Mir&amp;#xf3;-Nicolau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moya_Alcover_B/0/1/0/all/0/1"&gt;Biel Moy&amp;#xe0;-Alcover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Hidalgo_M/0/1/0/all/0/1"&gt;Manuel Gonz&amp;#xe1;lez-Hidalgo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaume_i_Capo_A/0/1/0/all/0/1"&gt;Antoni Jaume-i-Cap&amp;#xf3;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review and Comparative Study on Probabilistic Object Detection in Autonomous Driving. (arXiv:2011.10671v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10671</id>
        <link href="http://arxiv.org/abs/2011.10671"/>
        <updated>2021-07-13T01:59:34.800Z</updated>
        <summary type="html"><![CDATA[Capturing uncertainty in object detection is indispensable for safe
autonomous driving. In recent years, deep learning has become the de-facto
approach for object detection, and many probabilistic object detectors have
been proposed. However, there is no summary on uncertainty estimation in deep
object detection, and existing methods are not only built with different
network architectures and uncertainty estimation methods, but also evaluated on
different datasets with a wide range of evaluation metrics. As a result, a
comparison among methods remains challenging, as does the selection of a model
that best suits a particular application. This paper aims to alleviate this
problem by providing a review and comparative study on existing probabilistic
object detection methods for autonomous driving applications. First, we provide
an overview of generic uncertainty estimation in deep learning, and then
systematically survey existing methods and evaluation metrics for probabilistic
object detection. Next, we present a strict comparative study for probabilistic
object detection based on an image detector and three public autonomous driving
datasets. Finally, we present a discussion of the remaining challenges and
future works. Code has been made available at
https://github.com/asharakeh/pod_compare.git]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1"&gt;Di Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harakeh_A/0/1/0/all/0/1"&gt;Ali Harakeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waslander_S/0/1/0/all/0/1"&gt;Steven Waslander&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dietmayer_K/0/1/0/all/0/1"&gt;Klaus Dietmayer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Part-Aware Data Augmentation for 3D Object Detection in Point Cloud. (arXiv:2007.13373v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.13373</id>
        <link href="http://arxiv.org/abs/2007.13373"/>
        <updated>2021-07-13T01:59:34.782Z</updated>
        <summary type="html"><![CDATA[Data augmentation has greatly contributed to improving the performance in
image recognition tasks, and a lot of related studies have been conducted.
However, data augmentation on 3D point cloud data has not been much explored.
3D label has more sophisticated and rich structural information than the 2D
label, so it enables more diverse and effective data augmentation. In this
paper, we propose part-aware data augmentation (PA-AUG) that can better utilize
rich information of 3D label to enhance the performance of 3D object detectors.
PA-AUG divides objects into partitions and stochastically applies five
augmentation methods to each local region. It is compatible with existing point
cloud data augmentation methods and can be used universally regardless of the
detector's architecture. PA-AUG has improved the performance of
state-of-the-art 3D object detector for all classes of the KITTI dataset and
has the equivalent effect of increasing the train data by about 2.5$\times$. We
also show that PA-AUG not only increases performance for a given dataset but
also is robust to corrupted data. The code is available at
https://github.com/sky77764/pa-aug.pytorch]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jaeseok Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yeji Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1"&gt;Nojun Kwak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient Forward-Propagation for Large-Scale Temporal Video Modelling. (arXiv:2106.08318v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08318</id>
        <link href="http://arxiv.org/abs/2106.08318"/>
        <updated>2021-07-13T01:59:34.776Z</updated>
        <summary type="html"><![CDATA[How can neural networks be trained on large-volume temporal data efficiently?
To compute the gradients required to update parameters, backpropagation blocks
computations until the forward and backward passes are completed. For temporal
signals, this introduces high latency and hinders real-time learning. It also
creates a coupling between consecutive layers, which limits model parallelism
and increases memory consumption. In this paper, we build upon Sideways, which
avoids blocking by propagating approximate gradients forward in time, and we
propose mechanisms for temporal integration of information based on different
variants of skip connections. We also show how to decouple computation and
delegate individual neural modules to different devices, allowing distributed
and parallel training. The proposed Skip-Sideways achieves low latency
training, model parallelism, and, importantly, is capable of extracting
temporal features, leading to more stable training and improved performance on
real-world action recognition video datasets such as HMDB51, UCF101, and the
large-scale Kinetics-600. Finally, we also show that models trained with
Skip-Sideways generate better future frames than Sideways models, and hence
they can better utilize motion cues.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1"&gt;Mateusz Malinowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vytiniotis_D/0/1/0/all/0/1"&gt;Dimitrios Vytiniotis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swirszcz_G/0/1/0/all/0/1"&gt;Grzegorz Swirszcz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patraucean_V/0/1/0/all/0/1"&gt;Viorica Patraucean&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1"&gt;Joao Carreira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Early Convolutions Help Transformers See Better. (arXiv:2106.14881v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14881</id>
        <link href="http://arxiv.org/abs/2106.14881"/>
        <updated>2021-07-13T01:59:34.770Z</updated>
        <summary type="html"><![CDATA[Vision transformer (ViT) models exhibit substandard optimizability. In
particular, they are sensitive to the choice of optimizer (AdamW vs. SGD),
optimizer hyperparameters, and training schedule length. In comparison, modern
convolutional neural networks are far easier to optimize. Why is this the case?
In this work, we conjecture that the issue lies with the patchify stem of ViT
models, which is implemented by a stride-p pxp convolution (p=16 by default)
applied to the input image. This large-kernel plus large-stride convolution
runs counter to typical design choices of convolutional layers in neural
networks. To test whether this atypical design choice causes an issue, we
analyze the optimization behavior of ViT models with their original patchify
stem versus a simple counterpart where we replace the ViT stem by a small
number of stacked stride-two 3x3 convolutions. While the vast majority of
computation in the two ViT designs is identical, we find that this small change
in early visual processing results in markedly different training behavior in
terms of the sensitivity to optimization settings as well as the final model
accuracy. Using a convolutional stem in ViT dramatically increases optimization
stability and also improves peak performance (by ~1-2% top-1 accuracy on
ImageNet-1k), while maintaining flops and runtime. The improvement can be
observed across the wide spectrum of model complexities (from 1G to 36G flops)
and dataset scales (from ImageNet-1k to ImageNet-21k). These findings lead us
to recommend using a standard, lightweight convolutional stem for ViT models as
a more robust architectural choice compared to the original ViT model design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1"&gt;Tete Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1"&gt;Mannat Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mintun_E/0/1/0/all/0/1"&gt;Eric Mintun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1"&gt;Trevor Darrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dollar_P/0/1/0/all/0/1"&gt;Piotr Doll&amp;#xe1;r&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Girshick_R/0/1/0/all/0/1"&gt;Ross Girshick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient Matching for Domain Generalization. (arXiv:2104.09937v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09937</id>
        <link href="http://arxiv.org/abs/2104.09937"/>
        <updated>2021-07-13T01:59:34.764Z</updated>
        <summary type="html"><![CDATA[Machine learning systems typically assume that the distributions of training
and test sets match closely. However, a critical requirement of such systems in
the real world is their ability to generalize to unseen domains. Here, we
propose an inter-domain gradient matching objective that targets domain
generalization by maximizing the inner product between gradients from different
domains. Since direct optimization of the gradient inner product can be
computationally prohibitive -- requires computation of second-order derivatives
-- we derive a simpler first-order algorithm named Fish that approximates its
optimization. We demonstrate the efficacy of Fish on 6 datasets from the Wilds
benchmark, which captures distribution shift across a diverse range of
modalities. Our method produces competitive results on these datasets and
surpasses all baselines on 4 of them. We perform experiments on both the Wilds
benchmark, which captures distribution shift in the real world, as well as
datasets in DomainBed benchmark that focuses more on synthetic-to-real
transfer. Our method produces competitive results on both benchmarks,
demonstrating its effectiveness across a wide range of domain generalization
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yuge Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seely_J/0/1/0/all/0/1"&gt;Jeffrey Seely&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip H.S. Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siddharth_N/0/1/0/all/0/1"&gt;N. Siddharth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hannun_A/0/1/0/all/0/1"&gt;Awni Hannun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usunier_N/0/1/0/all/0/1"&gt;Nicolas Usunier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1"&gt;Gabriel Synnaeve&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Harmonization with Flow-based Causal Inference. (arXiv:2106.06845v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06845</id>
        <link href="http://arxiv.org/abs/2106.06845"/>
        <updated>2021-07-13T01:59:34.758Z</updated>
        <summary type="html"><![CDATA[Heterogeneity in medical data, e.g., from data collected at different sites
and with different protocols in a clinical study, is a fundamental hurdle for
accurate prediction using machine learning models, as such models often fail to
generalize well. This paper leverages a recently proposed
normalizing-flow-based method to perform counterfactual inference upon a
structural causal model (SCM), in order to achieve harmonization of such data.
A causal model is used to model observed effects (brain magnetic resonance
imaging data) that result from known confounders (site, gender and age) and
exogenous noise variables. Our formulation exploits the bijection induced by
flow for the purpose of harmonization. We infer the posterior of exogenous
variables, intervene on observations, and draw samples from the resultant SCM
to obtain counterfactuals. This approach is evaluated extensively on multiple,
large, real-world medical datasets and displayed better cross-domain
generalization compared to state-of-the-art algorithms. Further experiments
that evaluate the quality of confounder-independent data generated by our model
using regression and classification tasks are provided.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rongguang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhari_P/0/1/0/all/0/1"&gt;Pratik Chaudhari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davatzikos_C/0/1/0/all/0/1"&gt;Christos Davatzikos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EBBINNOT: A Hardware Efficient Hybrid Event-Frame Tracker for Stationary Neuromorphic Vision Sensors. (arXiv:2006.00422v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.00422</id>
        <link href="http://arxiv.org/abs/2006.00422"/>
        <updated>2021-07-13T01:59:34.740Z</updated>
        <summary type="html"><![CDATA[Neuromorphic vision sensors (NVS) have been recently explored to tackle
scenarios where conventional sensors result in high data rate and processing
time. This paper presents a hybrid event-frame approach for detecting and
tracking objects recorded by a stationary neuromorphic sensor, thereby
exploiting the sparse NVS output in a low-power setting for traffic monitoring.
Specifically, we propose a hardware efficient processing pipeline that
optimizes memory and computational needs. The usage of NVS gives the advantage
of rejecting background while it has a unique disadvantage of fragmented
objects. To exploit the background removal, we propose an event-based binary
image creation that signals presence or absence of events in a frame duration.
This reduces memory requirement and enables usage of simple algorithms like
median filtering and connected component labeling for denoise and region
proposal respectively. To overcome the fragmentation issue, a YOLO-inspired
neural network based detector and classifier to merge fragmented region
proposals has been proposed. Finally, an overlap based tracker exploiting
overlap between detections and tracks is proposed with heuristics to overcome
occlusion. The proposed pipeline is evaluated with more than 5 hours of traffic
recording spanning three different locations on two different NVS and
demonstrate similar performance. Compared to existing event-based feature
trackers, our method provides similar accuracy while needing 6 times less
computes. To the best of our knowledge, this is the first time a stationary NVS
based traffic monitoring solution is extensively compared to simultaneously
recorded RGB frame-methods while showing tremendous promise by outperforming
state-of-the-art deep learning solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singla_D/0/1/0/all/0/1"&gt;Deepak Singla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohan_V/0/1/0/all/0/1"&gt;Vivek Mohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pulluri_T/0/1/0/all/0/1"&gt;Tarun Pulluri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ussa_A/0/1/0/all/0/1"&gt;Andres Ussa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gopalakrishnan_P/0/1/0/all/0/1"&gt;Pradeep Kumar Gopalakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramesh_B/0/1/0/all/0/1"&gt;Bharath Ramesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1"&gt;Arindam Basu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HRVGAN: High Resolution Video Generation using Spatio-Temporal GAN. (arXiv:2008.09646v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.09646</id>
        <link href="http://arxiv.org/abs/2008.09646"/>
        <updated>2021-07-13T01:59:34.734Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a novel network for high resolution video
generation. Our network uses ideas from Wasserstein GANs by enforcing
k-Lipschitz constraint on the loss term and Conditional GANs using class labels
for training and testing. We present Generator and Discriminator network
layerwise details along with the combined network architecture, optimization
details and algorithm used in this work. Our network uses a combination of two
loss terms: mean square pixel loss and an adversarial loss. The datasets used
for training and testing our network are UCF101, Golf and Aeroplane Datasets.
Using Inception Score and Fr\'echet Inception Distance as the evaluation
metrics, our network outperforms previous state of the art networks on
unsupervised video generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sagar_A/0/1/0/all/0/1"&gt;Abhinav Sagar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Occlusion-Aware Depth Estimation with Adaptive Normal Constraints. (arXiv:2004.00845v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.00845</id>
        <link href="http://arxiv.org/abs/2004.00845"/>
        <updated>2021-07-13T01:59:34.728Z</updated>
        <summary type="html"><![CDATA[We present a new learning-based method for multi-frame depth estimation from
a color video, which is a fundamental problem in scene understanding, robot
navigation or handheld 3D reconstruction. While recent learning-based methods
estimate depth at high accuracy, 3D point clouds exported from their depth maps
often fail to preserve important geometric feature (e.g., corners, edges,
planes) of man-made scenes. Widely-used pixel-wise depth errors do not
specifically penalize inconsistency on these features. These inaccuracies are
particularly severe when subsequent depth reconstructions are accumulated in an
attempt to scan a full environment with man-made objects with this kind of
features. Our depth estimation algorithm therefore introduces a Combined Normal
Map (CNM) constraint, which is designed to better preserve high-curvature
features and global planar regions. In order to further improve the depth
estimation accuracy, we introduce a new occlusion-aware strategy that
aggregates initial depth predictions from multiple adjacent views into one
final depth map and one occlusion probability map for the current reference
view. Our method outperforms the state-of-the-art in terms of depth estimation
accuracy, and preserves essential geometric features of man-made indoor scenes
much better than other algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1"&gt;Xiaoxiao Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingjie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenping Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Use of Variational Inference in Music Emotion Recognition. (arXiv:2106.14323v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14323</id>
        <link href="http://arxiv.org/abs/2106.14323"/>
        <updated>2021-07-13T01:59:34.722Z</updated>
        <summary type="html"><![CDATA[This work was developed aiming to employ Statistical techniques to the field
of Music Emotion Recognition, a well-recognized area within the Signal
Processing world, but hardly explored from the statistical point of view. Here,
we opened several possibilities within the field, applying modern Bayesian
Statistics techniques and developing efficient algorithms, focusing on the
applicability of the results obtained. Although the motivation for this project
was the development of a emotion-based music recommendation system, its main
contribution is a highly adaptable multivariate model that can be useful
interpreting any database where there is an interest in applying regularization
in an efficient manner. Broadly speaking, we will explore what role a sound
theoretical statistical analysis can play in the modeling of an algorithm that
is able to understand a well-known database and what can be gained with this
kind of approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Deziderio_N/0/1/0/all/0/1"&gt;Nathalie Deziderio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Carvalho_H/0/1/0/all/0/1"&gt;Hugo Tremonte de Carvalho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classification and understanding of cloud structures via satellite images with EfficientUNet. (arXiv:2009.12931v4 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.12931</id>
        <link href="http://arxiv.org/abs/2009.12931"/>
        <updated>2021-07-13T01:59:34.716Z</updated>
        <summary type="html"><![CDATA[Climate change has been a common interest and the forefront of crucial
political discussion and decision-making for many years. Shallow clouds play a
significant role in understanding the Earth's climate, but they are challenging
to interpret and represent in a climate model. By classifying these cloud
structures, there is a better possibility of understanding the physical
structures of the clouds, which would improve the climate model generation,
resulting in a better prediction of climate change or forecasting weather
update. Clouds organise in many forms, which makes it challenging to build
traditional rule-based algorithms to separate cloud features. In this paper,
classification of cloud organization patterns was performed using a new
scaled-up version of Convolutional Neural Network (CNN) named as EfficientNet
as the encoder and UNet as decoder where they worked as feature extractor and
reconstructor of fine grained feature map and was used as a classifier, which
will help experts to understand how clouds will shape the future climate. By
using a segmentation model in a classification task, it was shown that with a
good encoder alongside UNet, it is possible to obtain good performance from
this dataset. Dice coefficient has been used for the final evaluation metric,
which gave the score of 66.26\% and 66.02\% for public and private (test set)
leaderboard on Kaggle competition respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ahmed_T/0/1/0/all/0/1"&gt;Tashin Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sabab_N/0/1/0/all/0/1"&gt;Noor Hossain Nuri Sabab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TAG: Task-based Accumulated Gradients for Lifelong learning. (arXiv:2105.05155v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05155</id>
        <link href="http://arxiv.org/abs/2105.05155"/>
        <updated>2021-07-13T01:59:34.699Z</updated>
        <summary type="html"><![CDATA[When an agent encounters a continual stream of new tasks in the lifelong
learning setting, it leverages the knowledge it gained from the earlier tasks
to help learn the new tasks better. In such a scenario, identifying an
efficient knowledge representation becomes a challenging problem. Most research
works propose to either store a subset of examples from the past tasks in a
replay buffer, dedicate a separate set of parameters to each task or penalize
excessive updates over parameters by introducing a regularization term. While
existing methods employ the general task-agnostic stochastic gradient descent
update rule, we propose a task-aware optimizer that adapts the learning rate
based on the relatedness among tasks. We utilize the directions taken by the
parameters during the updates by accumulating the gradients specific to each
task. These task-based accumulated gradients act as a knowledge base that is
maintained and updated throughout the stream. We empirically show that our
proposed adaptive learning rate not only accounts for catastrophic forgetting
but also allows positive backward transfer. We also show that our method
performs better than several state-of-the-art methods in lifelong learning on
complex datasets with a large number of tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Malviya_P/0/1/0/all/0/1"&gt;Pranshu Malviya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1"&gt;Sarath Chandar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravindran_B/0/1/0/all/0/1"&gt;Balaraman Ravindran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdderNet: Do We Really Need Multiplications in Deep Learning?. (arXiv:1912.13200v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.13200</id>
        <link href="http://arxiv.org/abs/1912.13200"/>
        <updated>2021-07-13T01:59:34.692Z</updated>
        <summary type="html"><![CDATA[Compared with cheap addition operation, multiplication operation is of much
higher computation complexity. The widely-used convolutions in deep neural
networks are exactly cross-correlation to measure the similarity between input
feature and convolution filters, which involves massive multiplications between
float values. In this paper, we present adder networks (AdderNets) to trade
these massive multiplications in deep neural networks, especially convolutional
neural networks (CNNs), for much cheaper additions to reduce computation costs.
In AdderNets, we take the $\ell_1$-norm distance between filters and input
feature as the output response. The influence of this new similarity measure on
the optimization of neural network have been thoroughly analyzed. To achieve a
better performance, we develop a special back-propagation approach for
AdderNets by investigating the full-precision gradient. We then propose an
adaptive learning rate strategy to enhance the training procedure of AdderNets
according to the magnitude of each neuron's gradient. As a result, the proposed
AdderNets can achieve 74.9% Top-1 accuracy 91.7% Top-5 accuracy using ResNet-50
on the ImageNet dataset without any multiplication in convolution layer. The
codes are publicly available at: https://github.com/huaweinoah/AdderNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hanting Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunhe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chunjing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1"&gt;Boxin Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Visual Models using a Knowledge Graph as a Trainer. (arXiv:2102.08747v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08747</id>
        <link href="http://arxiv.org/abs/2102.08747"/>
        <updated>2021-07-13T01:59:34.686Z</updated>
        <summary type="html"><![CDATA[Traditional computer vision approaches, based on neural networks (NN), are
typically trained on a large amount of image data. By minimizing the
cross-entropy loss between a prediction and a given class label, the NN and its
visual embedding space are learned to fulfill a given task. However, due to the
sole dependence on the image data distribution of the training domain, these
models tend to fail when applied to a target domain that differs from their
source domain. To learn a more robust NN to domain shifts, we propose the
knowledge graph neural network (KG-NN), a neuro-symbolic approach that
supervises the training using image-data-invariant auxiliary knowledge. The
auxiliary knowledge is first encoded in a knowledge graph with respective
concepts and their relationships, which is then transformed into a dense vector
representation via an embedding method. Using a contrastive loss function,
KG-NN learns to adapt its visual embedding space and thus its weights according
to the image-data invariant knowledge graph embedding space. We evaluate KG-NN
on visual transfer learning tasks for classification using the mini-ImageNet
dataset and its derivatives, as well as road sign recognition datasets from
Germany and China. The results show that a visual model trained with a
knowledge graph as a trainer outperforms a model trained with cross-entropy in
all experiments, in particular when the domain gap increases. Besides better
performance and stronger robustness to domain shifts, these KG-NN adapts to
multiple datasets and classes without suffering heavily from catastrophic
forgetting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Monka_S/0/1/0/all/0/1"&gt;Sebastian Monka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Halilaj_L/0/1/0/all/0/1"&gt;Lavdim Halilaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmid_S/0/1/0/all/0/1"&gt;Stefan Schmid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rettinger_A/0/1/0/all/0/1"&gt;Achim Rettinger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Simple Reward-free Approach to Constrained Reinforcement Learning. (arXiv:2107.05216v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05216</id>
        <link href="http://arxiv.org/abs/2107.05216"/>
        <updated>2021-07-13T01:59:34.680Z</updated>
        <summary type="html"><![CDATA[In constrained reinforcement learning (RL), a learning agent seeks to not
only optimize the overall reward but also satisfy the additional safety,
diversity, or budget constraints. Consequently, existing constrained RL
solutions require several new algorithmic ingredients that are notably
different from standard RL. On the other hand, reward-free RL is independently
developed in the unconstrained literature, which learns the transition dynamics
without using the reward information, and thus naturally capable of addressing
RL with multiple objectives under the common dynamics. This paper bridges
reward-free RL and constrained RL. Particularly, we propose a simple
meta-algorithm such that given any reward-free RL oracle, the approachability
and constrained RL problems can be directly solved with negligible overheads in
sample complexity. Utilizing the existing reward-free RL solvers, our framework
provides sharp sample complexity results for constrained RL in the tabular MDP
setting, matching the best existing results up to a factor of horizon
dependence; our framework directly extends to a setting of tabular two-player
Markov games, and gives a new result for constrained RL with linear function
approximation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miryoosefi_S/0/1/0/all/0/1"&gt;Sobhan Miryoosefi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1"&gt;Chi Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smoothed Differential Privacy. (arXiv:2107.01559v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01559</id>
        <link href="http://arxiv.org/abs/2107.01559"/>
        <updated>2021-07-13T01:59:34.674Z</updated>
        <summary type="html"><![CDATA[Differential privacy (DP) is a widely-accepted and widely-applied notion of
privacy based on worst-case analysis. Often, DP classifies most mechanisms
without external noise as non-private [Dwork et al., 2014], and external
noises, such as Gaussian noise or Laplacian noise [Dwork et al., 2006], are
introduced to improve privacy. In many real-world applications, however, adding
external noise is undesirable and sometimes prohibited. For example,
presidential elections often require a deterministic rule to be used [Liu et
al., 2020], and small noises can lead to dramatic decreases in the prediction
accuracy of deep neural networks, especially the underrepresented classes
[Bagdasaryan et al., 2019].

In this paper, we propose a natural extension and relaxation of DP following
the worst average-case idea behind the celebrated smoothed analysis [Spielman
and Teng, 2004]. Our notion, the smoothed DP, can effectively measure the
privacy leakage of mechanisms without external noises under realistic settings.

We prove several strong properties of the smoothed DP, including
composability, robustness to post-processing and etc. We proved that any
discrete mechanism with sampling procedures is more private than what DP
predicts. In comparison, many continuous mechanisms with sampling procedures
are still non-private under smoothed DP. Experimentally, we first verified that
the discrete sampling mechanisms are private in real-world elections. Then, we
apply the smoothed DP notion on quantized gradient descent, which indicates
some neural networks can be private without adding any extra noises. We believe
that these results contribute to the theoretical foundation of realistic
privacy measures beyond worst-case analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Ao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1"&gt;Lirong Xia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smooth Adversarial Training. (arXiv:2006.14536v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.14536</id>
        <link href="http://arxiv.org/abs/2006.14536"/>
        <updated>2021-07-13T01:59:34.656Z</updated>
        <summary type="html"><![CDATA[It is commonly believed that networks cannot be both accurate and robust,
that gaining robustness means losing accuracy. It is also generally believed
that, unless making networks larger, network architectural elements would
otherwise matter little in improving adversarial robustness. Here we present
evidence to challenge these common beliefs by a careful study about adversarial
training. Our key observation is that the widely-used ReLU activation function
significantly weakens adversarial training due to its non-smooth nature. Hence
we propose smooth adversarial training (SAT), in which we replace ReLU with its
smooth approximations to strengthen adversarial training. The purpose of smooth
activation functions in SAT is to allow it to find harder adversarial examples
and compute better gradient updates during adversarial training.

Compared to standard adversarial training, SAT improves adversarial
robustness for "free", i.e., no drop in accuracy and no increase in
computational cost. For example, without introducing additional computations,
SAT significantly enhances ResNet-50's robustness from 33.0% to 42.3%, while
also improving accuracy by 0.9% on ImageNet. SAT also works well with larger
networks: it helps EfficientNet-L1 to achieve 82.2% accuracy and 58.6%
robustness on ImageNet, outperforming the previous state-of-the-art defense by
9.5% for accuracy and 11.6% for robustness. Models are available at
https://github.com/cihangxie/SmoothAdversarialTraining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1"&gt;Cihang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1"&gt;Mingxing Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1"&gt;Boqing Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1"&gt;Alan Yuille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1"&gt;Quoc V. Le&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Explicit Concerning States for Reinforcement Learning in Visual Dialogue. (arXiv:2107.05250v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05250</id>
        <link href="http://arxiv.org/abs/2107.05250"/>
        <updated>2021-07-13T01:59:34.647Z</updated>
        <summary type="html"><![CDATA[To encourage AI agents to conduct meaningful Visual Dialogue (VD), the use of
Reinforcement Learning has been proven potential. In Reinforcement Learning, it
is crucial to represent states and assign rewards based on the action-caused
transitions of states. However, the state representation in previous Visual
Dialogue works uses the textual information only and its transitions are
implicit. In this paper, we propose Explicit Concerning States (ECS) to
represent what visual contents are concerned at each round and what have been
concerned throughout the Visual Dialogue. ECS is modeled from multimodal
information and is represented explicitly. Based on ECS, we formulate two
intuitive and interpretable rewards to encourage the Visual Dialogue agents to
converse on diverse and informative visual information. Experimental results on
the VisDial v1.0 dataset show our method enables the Visual Dialogue agents to
generate more visual coherent, less repetitive and more visual informative
dialogues compared with previous methods, according to multiple automatic
metrics, human study and qualitative analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zipeng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1"&gt;Fandong Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaojie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1"&gt;Duo Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1"&gt;Chenxu Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting sepsis in multi-site, multi-national intensive care cohorts using deep learning. (arXiv:2107.05230v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05230</id>
        <link href="http://arxiv.org/abs/2107.05230"/>
        <updated>2021-07-13T01:59:34.640Z</updated>
        <summary type="html"><![CDATA[Despite decades of clinical research, sepsis remains a global public health
crisis with high mortality, and morbidity. Currently, when sepsis is detected
and the underlying pathogen is identified, organ damage may have already
progressed to irreversible stages. Effective sepsis management is therefore
highly time-sensitive. By systematically analysing trends in the plethora of
clinical data available in the intensive care unit (ICU), an early prediction
of sepsis could lead to earlier pathogen identification, resistance testing,
and effective antibiotic and supportive treatment, and thereby become a
life-saving measure. Here, we developed and validated a machine learning (ML)
system for the prediction of sepsis in the ICU. Our analysis represents the
largest multi-national, multi-centre in-ICU study for sepsis prediction using
ML to date. Our dataset contains $156,309$ unique ICU admissions, which
represent a refined and harmonised subset of five large ICU databases
originating from three countries. Using the international consensus definition
Sepsis-3, we derived hourly-resolved sepsis label annotations, amounting to
$26,734$ ($17.1\%$) septic stays. We compared our approach, a deep
self-attention model, to several clinical baselines as well as ML baselines and
performed an extensive internal and external validation within and across
databases. On average, our model was able to predict sepsis with an AUROC of
$0.847 \pm 0.050$ (internal out-of sample validation) and $0.761 \pm 0.052$
(external validation). For a harmonised prevalence of $17\%$, at $80\%$ recall
our model detects septic patients with $39\%$ precision 3.7 hours in advance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moor_M/0/1/0/all/0/1"&gt;Michael Moor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennet_N/0/1/0/all/0/1"&gt;Nicolas Bennet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plecko_D/0/1/0/all/0/1"&gt;Drago Plecko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horn_M/0/1/0/all/0/1"&gt;Max Horn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rieck_B/0/1/0/all/0/1"&gt;Bastian Rieck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meinshausen_N/0/1/0/all/0/1"&gt;Nicolai Meinshausen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buhlmann_P/0/1/0/all/0/1"&gt;Peter B&amp;#xfc;hlmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borgwardt_K/0/1/0/all/0/1"&gt;Karsten Borgwardt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-Time Super-Resolution System of 4K-Video Based on Deep Learning. (arXiv:2107.05307v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05307</id>
        <link href="http://arxiv.org/abs/2107.05307"/>
        <updated>2021-07-13T01:59:34.634Z</updated>
        <summary type="html"><![CDATA[Video super-resolution (VSR) technology excels in reconstructing low-quality
video, avoiding unpleasant blur effect caused by interpolation-based
algorithms. However, vast computation complexity and memory occupation hampers
the edge of deplorability and the runtime inference in real-life applications,
especially for large-scale VSR task. This paper explores the possibility of
real-time VSR system and designs an efficient and generic VSR network, termed
EGVSR. The proposed EGVSR is based on spatio-temporal adversarial learning for
temporal coherence. In order to pursue faster VSR processing ability up to 4K
resolution, this paper tries to choose lightweight network structure and
efficient upsampling method to reduce the computation required by EGVSR network
under the guarantee of high visual quality. Besides, we implement the batch
normalization computation fusion, convolutional acceleration algorithm and
other neural network acceleration techniques on the actual hardware platform to
optimize the inference process of EGVSR network. Finally, our EGVSR achieves
the real-time processing capacity of 4K@29.61FPS. Compared with TecoGAN, the
most advanced VSR network at present, we achieve 85.04% reduction of
computation density and 7.92x performance speedups. In terms of visual quality,
the proposed EGVSR tops the list of most metrics (such as LPIPS, tOF, tLP,
etc.) on the public test dataset Vid4 and surpasses other state-of-the-art
methods in overall performance score. The source code of this project can be
found on https://github.com/Thmen/EGVSR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yanpeng Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengcheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1"&gt;Changjun Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;He Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yongming Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autonomous Vehicles that Alert Humans to Take-Over Controls: Modeling with Real-World Data. (arXiv:2104.11489v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11489</id>
        <link href="http://arxiv.org/abs/2104.11489"/>
        <updated>2021-07-13T01:59:34.627Z</updated>
        <summary type="html"><![CDATA[With increasing automation in passenger vehicles, the study of safe and
smooth occupant-vehicle interaction and control transitions is key. In this
study, we focus on the development of contextual, semantically meaningful
representations of the driver state, which can then be used to determine the
appropriate timing and conditions for transfer of control between driver and
vehicle. To this end, we conduct a large-scale real-world controlled data study
where participants are instructed to take-over control from an autonomous agent
under different driving conditions while engaged in a variety of distracting
activities. These take-over events are captured using multiple driver-facing
cameras, which when labelled result in a dataset of control transitions and
their corresponding take-over times (TOTs). We then develop and train TOT
models that operate sequentially on mid to high-level features produced by
computer vision algorithms operating on different driver-facing camera views.
The proposed TOT model produces continuous estimates of take-over times without
delay, and shows promising qualitative and quantitative results in complex
real-world scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rangesh_A/0/1/0/all/0/1"&gt;Akshay Rangesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deo_N/0/1/0/all/0/1"&gt;Nachiket Deo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greer_R/0/1/0/all/0/1"&gt;Ross Greer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunaratne_P/0/1/0/all/0/1"&gt;Pujitha Gunaratne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_M/0/1/0/all/0/1"&gt;Mohan M. Trivedi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VM-MODNet: Vehicle Motion aware Moving Object Detection for Autonomous Driving. (arXiv:2104.10985v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10985</id>
        <link href="http://arxiv.org/abs/2104.10985"/>
        <updated>2021-07-13T01:59:34.609Z</updated>
        <summary type="html"><![CDATA[Moving object Detection (MOD) is a critical task in autonomous driving as
moving agents around the ego-vehicle need to be accurately detected for safe
trajectory planning. It also enables appearance agnostic detection of objects
based on motion cues. There are geometric challenges like motion-parallax
ambiguity which makes it a difficult problem. In this work, we aim to leverage
the vehicle motion information and feed it into the model to have an adaptation
mechanism based on ego-motion. The motivation is to enable the model to
implicitly perform ego-motion compensation to improve performance. We convert
the six degrees of freedom vehicle motion into a pixel-wise tensor which can be
fed as input to the CNN model. The proposed model using Vehicle Motion Tensor
(VMT) achieves an absolute improvement of 5.6% in mIoU over the baseline
architecture. We also achieve state-of-the-art results on the public
KITTI_MoSeg_Extended dataset even compared to methods which make use of LiDAR
and additional input frames. Our model is also lightweight and runs at 85 fps
on a TitanX GPU. Qualitative results are provided in
https://youtu.be/ezbfjti-kTk.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rashed_H/0/1/0/all/0/1"&gt;Hazem Rashed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sallab_A/0/1/0/all/0/1"&gt;Ahmad El Sallab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1"&gt;Senthil Yogamani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PDE-based Group Equivariant Convolutional Neural Networks. (arXiv:2001.09046v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.09046</id>
        <link href="http://arxiv.org/abs/2001.09046"/>
        <updated>2021-07-13T01:59:34.603Z</updated>
        <summary type="html"><![CDATA[We present a PDE-based framework that generalizes Group equivariant
Convolutional Neural Networks (G-CNNs). In this framework, a network layer is
seen as a set of PDE-solvers where geometrically meaningful PDE-coefficients
become the layer's trainable weights. Formulating our PDEs on homogeneous
spaces allows these networks to be designed with built-in symmetries such as
rotation in addition to the standard translation equivariance of CNNs.

Having all the desired symmetries included in the design obviates the need to
include them by means of costly techniques such as data augmentation. We will
discuss our PDE-based G-CNNs (PDE-G-CNNs) in a general homogeneous space
setting while also going into the specifics of our primary case of interest:
roto-translation equivariance.

We solve the PDE of interest by a combination of linear group convolutions
and non-linear morphological group convolutions with analytic kernel
approximations that we underpin with formal theorems. Our kernel approximations
allow for fast GPU-implementation of the PDE-solvers, we release our
implementation with this article. Just like for linear convolution a
morphological convolution is specified by a kernel that we train in our
PDE-G-CNNs. In PDE-G-CNNs we do not use non-linearities such as max/min-pooling
and ReLUs as they are already subsumed by morphological convolutions.

We present a set of experiments to demonstrate the strength of the proposed
PDE-G-CNNs in increasing the performance of deep learning based imaging
applications with far fewer parameters than traditional CNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smets_B/0/1/0/all/0/1"&gt;Bart Smets&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Portegies_J/0/1/0/all/0/1"&gt;Jim Portegies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1"&gt;Erik Bekkers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duits_R/0/1/0/all/0/1"&gt;Remco Duits&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepRelativeFusion: Dense Monocular SLAM using Single-Image Relative Depth Prediction. (arXiv:2006.04047v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04047</id>
        <link href="http://arxiv.org/abs/2006.04047"/>
        <updated>2021-07-13T01:59:34.584Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a dense monocular SLAM system, named
DeepRelativeFusion, that is capable to recover a globally consistent 3D
structure. To this end, we use a visual SLAM algorithm to reliably recover the
camera poses and semi-dense depth maps of the keyframes, and then use relative
depth prediction to densify the semi-dense depth maps and refine the keyframe
pose-graph. To improve the semi-dense depth maps, we propose an adaptive
filtering scheme, which is a structure-preserving weighted average smoothing
filter that takes into account the pixel intensity and depth of the
neighbouring pixels, yielding substantial reconstruction accuracy gain in
densification. To perform densification, we introduce two incremental
improvements upon the energy minimization framework proposed by DeepFusion: (1)
an improved cost function, and (2) the use of single-image relative depth
prediction. After densification, we update the keyframes with two-view
consistent optimized semi-dense and dense depth maps to improve pose-graph
optimization, providing a feedback loop to refine the keyframe poses for
accurate scene reconstruction. Our system outperforms the state-of-the-art
dense SLAM systems quantitatively in dense reconstruction accuracy by a large
margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Loo_S/0/1/0/all/0/1"&gt;Shing Yan Loo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mashohor_S/0/1/0/all/0/1"&gt;Syamsiah Mashohor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1"&gt;Sai Hong Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Split, embed and merge: An accurate table structure recognizer. (arXiv:2107.05214v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05214</id>
        <link href="http://arxiv.org/abs/2107.05214"/>
        <updated>2021-07-13T01:59:34.578Z</updated>
        <summary type="html"><![CDATA[The task of table structure recognition is to recognize the internal
structure of a table, which is a key step to make machines understand tables.
However, tabular data in unstructured digital documents, e.g. Portable Document
Format (PDF) and images, are difficult to parse into structured
machine-readable format, due to complexity and diversity in their structure and
style, especially for complex tables. In this paper, we introduce Split, Embed
and Merge (SEM), an accurate table structure recognizer. In the first stage, we
use the FCN to predict the potential regions of the table row (column)
separators, so as to obtain the bounding boxes of the basic grids in the table.
In the second stage, we not only extract the visual features corresponding to
each grid through RoIAlign, but also use the off-the-shelf recognizer and the
BERT to extract the semantic features. The fused features of both are used to
characterize each table grid. We find that by adding additional semantic
features to each grid, the ambiguity problem of the table structure from the
visual perspective can be solved to a certain extent and achieve higher
precision. Finally, we process the merging of these basic grids in a
self-regression manner. The correspondent merging results is learned by the
attention maps in attention mechanism. With the proposed method, we can
recognize the structure of tables well, even for complex tables. SEM can
achieve an average F-Measure of $96.9\%$ on the SciTSR dataset which
outperforms other methods by a large margin. Extensive experiments on other
publicly available table structure recognition datasets show that our model
achieves state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhenrong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianshu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1"&gt;Jun Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Introduction to Deep Morphological Networks. (arXiv:1906.01751v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.01751</id>
        <link href="http://arxiv.org/abs/1906.01751"/>
        <updated>2021-07-13T01:59:34.539Z</updated>
        <summary type="html"><![CDATA[The recent impressive results of deep learning-based methods on computer
vision applications brought fresh air to the research and industrial community.
This success is mainly due to the process that allows those methods to learn
data-driven features, generally based upon linear operations. However, in some
scenarios, such operations do not have a good performance because of their
inherited process that blurs edges, losing notions of corners, borders, and
geometry of objects. Overcoming this, non-linear operations, such as
morphological ones, may preserve such properties of the objects, being
preferable and even state-of-the-art in some applications. Encouraged by this,
in this work, we propose a novel network, called Deep Morphological Network
(DeepMorphNet), capable of doing non-linear morphological operations while
performing the feature learning process by optimizing the structuring elements.
The DeepMorphNets can be trained and optimized end-to-end using traditional
existing techniques commonly employed in the training of deep learning
approaches. A systematic evaluation of the proposed algorithm is conducted
using two synthetic and two traditional image classification datasets. Results
show that the proposed DeepMorphNets is a promising technique that can learn
distinct features when compared to the ones learned by current deep learning
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nogueira_K/0/1/0/all/0/1"&gt;Keiller Nogueira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1"&gt;Jocelyn Chanussot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mura_M/0/1/0/all/0/1"&gt;Mauro Dalla Mura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1"&gt;Jefersson A. dos Santos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classifying, Segmenting, and Tracking Object Instances in Video with Mask Propagation. (arXiv:1912.04573v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.04573</id>
        <link href="http://arxiv.org/abs/1912.04573"/>
        <updated>2021-07-13T01:59:34.522Z</updated>
        <summary type="html"><![CDATA[We introduce a method for simultaneously classifying, segmenting and tracking
object instances in a video sequence. Our method, named MaskProp, adapts the
popular Mask R-CNN to video by adding a mask propagation branch that propagates
frame-level object instance masks from each video frame to all the other frames
in a video clip. This allows our system to predict clip-level instance tracks
with respect to the object instances segmented in the middle frame of the clip.
Clip-level instance tracks generated densely for each frame in the sequence are
finally aggregated to produce video-level object instance segmentation and
classification. Our experiments demonstrate that our clip-level instance
segmentation makes our approach robust to motion blur and object occlusions in
video. MaskProp achieves the best reported accuracy on the YouTube-VIS dataset,
outperforming the ICCV 2019 video instance segmentation challenge winner
despite being much simpler and using orders of magnitude less labeled data
(1.3M vs 1B images and 860K vs 14M bounding boxes).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1"&gt;Gedas Bertasius&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torresani_L/0/1/0/all/0/1"&gt;Lorenzo Torresani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CFTrack: Center-based Radar and Camera Fusion for 3D Multi-Object Tracking. (arXiv:2107.05150v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05150</id>
        <link href="http://arxiv.org/abs/2107.05150"/>
        <updated>2021-07-13T01:59:34.515Z</updated>
        <summary type="html"><![CDATA[3D multi-object tracking is a crucial component in the perception system of
autonomous driving vehicles. Tracking all dynamic objects around the vehicle is
essential for tasks such as obstacle avoidance and path planning. Autonomous
vehicles are usually equipped with different sensor modalities to improve
accuracy and reliability. While sensor fusion has been widely used in object
detection networks in recent years, most existing multi-object tracking
algorithms either rely on a single input modality, or do not fully exploit the
information provided by multiple sensing modalities. In this work, we propose
an end-to-end network for joint object detection and tracking based on radar
and camera sensor fusion. Our proposed method uses a center-based radar-camera
fusion algorithm for object detection and utilizes a greedy algorithm for
object association. The proposed greedy algorithm uses the depth, velocity and
2D displacement of the detected objects to associate them through time. This
makes our tracking algorithm very robust to occluded and overlapping objects,
as the depth and velocity information can help the network in distinguishing
them. We evaluate our method on the challenging nuScenes dataset, where it
achieves 20.0 AMOTA and outperforms all vision-based 3D tracking methods in the
benchmark, as well as the baseline LiDAR-based method. Our method is online
with a runtime of 35ms per image, making it very suitable for autonomous
driving applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nabati_R/0/1/0/all/0/1"&gt;Ramin Nabati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harris_L/0/1/0/all/0/1"&gt;Landon Harris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1"&gt;Hairong Qi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learned super resolution ultrasound for improved breast lesion characterization. (arXiv:2107.05270v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05270</id>
        <link href="http://arxiv.org/abs/2107.05270"/>
        <updated>2021-07-13T01:59:34.505Z</updated>
        <summary type="html"><![CDATA[Breast cancer is the most common malignancy in women. Mammographic findings
such as microcalcifications and masses, as well as morphologic features of
masses in sonographic scans, are the main diagnostic targets for tumor
detection. However, improved specificity of these imaging modalities is
required. A leading alternative target is neoangiogenesis. When pathological,
it contributes to the development of numerous types of tumors, and the
formation of metastases. Hence, demonstrating neoangiogenesis by visualization
of the microvasculature may be of great importance. Super resolution ultrasound
localization microscopy enables imaging of the microvasculature at the
capillary level. Yet, challenges such as long reconstruction time, dependency
on prior knowledge of the system Point Spread Function (PSF), and separability
of the Ultrasound Contrast Agents (UCAs), need to be addressed for translation
of super-resolution US into the clinic. In this work we use a deep neural
network architecture that makes effective use of signal structure to address
these challenges. We present in vivo human results of three different breast
lesions acquired with a clinical US scanner. By leveraging our trained network,
the microvasculature structure is recovered in a short time, without prior PSF
knowledge, and without requiring separability of the UCAs. Each of the
recoveries exhibits a different structure that corresponds with the known
histological structure. This study demonstrates the feasibility of in vivo
human super resolution, based on a clinical scanner, to increase US specificity
for different breast lesions and promotes the use of US in the diagnosis of
breast pathologies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bar_Shira_O/0/1/0/all/0/1"&gt;Or Bar-Shira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grubstein_A/0/1/0/all/0/1"&gt;Ahuva Grubstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rapson_Y/0/1/0/all/0/1"&gt;Yael Rapson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suhami_D/0/1/0/all/0/1"&gt;Dror Suhami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atar_E/0/1/0/all/0/1"&gt;Eli Atar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peri_Hanania_K/0/1/0/all/0/1"&gt;Keren Peri-Hanania&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosen_R/0/1/0/all/0/1"&gt;Ronnie Rosen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eldar_Y/0/1/0/all/0/1"&gt;Yonina C. Eldar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geographical Knowledge-driven Representation Learning for Remote Sensing Images. (arXiv:2107.05276v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05276</id>
        <link href="http://arxiv.org/abs/2107.05276"/>
        <updated>2021-07-13T01:59:34.497Z</updated>
        <summary type="html"><![CDATA[The proliferation of remote sensing satellites has resulted in a massive
amount of remote sensing images. However, due to human and material resource
constraints, the vast majority of remote sensing images remain unlabeled. As a
result, it cannot be applied to currently available deep learning methods. To
fully utilize the remaining unlabeled images, we propose a Geographical
Knowledge-driven Representation learning method for remote sensing images
(GeoKR), improving network performance and reduce the demand for annotated
data. The global land cover products and geographical location associated with
each remote sensing image are regarded as geographical knowledge to provide
supervision for representation learning and network pre-training. An efficient
pre-training framework is proposed to eliminate the supervision noises caused
by imaging times and resolutions difference between remote sensing images and
geographical knowledge. A large scale pre-training dataset Levir-KR is proposed
to support network pre-training. It contains 1,431,950 remote sensing images
from Gaofen series satellites with various resolutions. Experimental results
demonstrate that our proposed method outperforms ImageNet pre-training and
self-supervised representation learning methods and significantly reduces the
burden of data annotation on downstream tasks such as scene classification,
semantic segmentation, object detection, and cloud / snow detection. It
demonstrates that our proposed method can be used as a novel paradigm for
pre-training neural networks. Codes will be available on
https://github.com/flyakon/Geographical-Knowledge-driven-Representaion-Learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Keyan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1"&gt;Zhenwei Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[eGHWT: The extended Generalized Haar-Walsh Transform. (arXiv:2107.05121v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.05121</id>
        <link href="http://arxiv.org/abs/2107.05121"/>
        <updated>2021-07-13T01:59:34.479Z</updated>
        <summary type="html"><![CDATA[Extending computational harmonic analysis tools from the classical setting of
regular lattices to the more general setting of graphs and networks is very
important and much research has been done recently. The Generalized Haar-Walsh
Transform (GHWT) developed by Irion and Saito (2014) is a multiscale transform
for signals on graphs, which is a generalization of the classical Haar and
Walsh-Hadamard Transforms. We propose the extended Generalized Haar-Walsh
Transform (eGHWT), which is a generalization of the adapted time-frequency
tilings of Thiele and Villemoes (1996). The eGHWT examines not only the
efficiency of graph-domain partitions but also that of "sequency-domain"
partitions simultaneously. Consequently, the eGHWT and its associated
best-basis selection algorithm for graph signals significantly improve the
performance of the previous GHWT with the similar computational cost, $O(N \log
N)$, where $N$ is the number of nodes of an input graph. While the GHWT
best-basis algorithm seeks the most suitable orthonormal basis for a given task
among more than $(1.5)^N$ possible orthonormal bases in $\mathbb{R}^N$, the
eGHWT best-basis algorithm can find a better one by searching through more than
$0.618\cdot(1.84)^N$ possible orthonormal bases in $\mathbb{R}^N$. This article
describes the details of the eGHWT best-basis algorithm and demonstrates its
superiority using several examples including genuine graph signals as well as
conventional digital images viewed as graph signals. Furthermore, we also show
how the eGHWT can be extended to 2D signals and matrix-form data by viewing
them as a tensor product of graphs generated from their columns and rows and
demonstrate its effectiveness on applications such as image approximation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Saito_N/0/1/0/all/0/1"&gt;Naoki Saito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yiqun Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human-like Relational Models for Activity Recognition in Video. (arXiv:2107.05319v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05319</id>
        <link href="http://arxiv.org/abs/2107.05319"/>
        <updated>2021-07-13T01:59:34.472Z</updated>
        <summary type="html"><![CDATA[Video activity recognition by deep neural networks is impressive for many
classes. However, it falls short of human performance, especially for
challenging to discriminate activities. Humans differentiate these complex
activities by recognising critical spatio-temporal relations among explicitly
recognised objects and parts, for example, an object entering the aperture of a
container. Deep neural networks can struggle to learn such critical
relationships effectively. Therefore we propose a more human-like approach to
activity recognition, which interprets a video in sequential temporal phases
and extracts specific relationships among objects and hands in those phases.
Random forest classifiers are learnt from these extracted relationships. We
apply the method to a challenging subset of the something-something dataset and
achieve a more robust performance against neural network baselines on
challenging activities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chrol_Cannon_J/0/1/0/all/0/1"&gt;Joseph Chrol-Cannon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gilbert_A/0/1/0/all/0/1"&gt;Andrew Gilbert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lazic_R/0/1/0/all/0/1"&gt;Ranko Lazic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madhusoodanan_A/0/1/0/all/0/1"&gt;Adithya Madhusoodanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1"&gt;Frank Guerin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BrainNNExplainer: An Interpretable Graph Neural Network Framework for Brain Network based Disease Analysis. (arXiv:2107.05097v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05097</id>
        <link href="http://arxiv.org/abs/2107.05097"/>
        <updated>2021-07-13T01:59:34.466Z</updated>
        <summary type="html"><![CDATA[Interpretable brain network models for disease prediction are of great value
for the advancement of neuroscience. GNNs are promising to model complicated
network data, but they are prone to overfitting and suffer from poor
interpretability, which prevents their usage in decision-critical scenarios
like healthcare. To bridge this gap, we propose BrainNNExplainer, an
interpretable GNN framework for brain network analysis. It is mainly composed
of two jointly learned modules: a backbone prediction model that is
specifically designed for brain networks and an explanation generator that
highlights disease-specific prominent brain network connections. Extensive
experimental results with visualizations on two challenging disease prediction
datasets demonstrate the unique interpretability and outstanding performance of
BrainNNExplainer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1"&gt;Hejie Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1"&gt;Wei Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yanqiao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoxiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lifang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Carl Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-end Trainable Deep Neural Network for Robotic Grasp Detection and Semantic Segmentation from RGB. (arXiv:2107.05287v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05287</id>
        <link href="http://arxiv.org/abs/2107.05287"/>
        <updated>2021-07-13T01:59:34.459Z</updated>
        <summary type="html"><![CDATA[In this work, we introduce a novel, end-to-end trainable CNN-based
architecture to deliver high quality results for grasp detection suitable for a
parallel-plate gripper, and semantic segmentation. Utilizing this, we propose a
novel refinement module that takes advantage of previously calculated grasp
detection and semantic segmentation and further increases grasp detection
accuracy. Our proposed network delivers state-of-the-art accuracy on two
popular grasp dataset, namely Cornell and Jacquard. As additional contribution,
we provide a novel dataset extension for the OCID dataset, making it possible
to evaluate grasp detection in highly challenging scenes. Using this dataset,
we show that semantic segmentation can additionally be used to assign grasp
candidates to object classes, which can be used to pick specific objects in the
scene.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ainetter_S/0/1/0/all/0/1"&gt;Stefan Ainetter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fraundorfer_F/0/1/0/all/0/1"&gt;Friedrich Fraundorfer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Delta Sampling R-BERT for limited data and low-light action recognition. (arXiv:2107.05202v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05202</id>
        <link href="http://arxiv.org/abs/2107.05202"/>
        <updated>2021-07-13T01:59:34.453Z</updated>
        <summary type="html"><![CDATA[We present an approach to perform supervised action recognition in the dark.
In this work, we present our results on the ARID dataset. Most previous works
only evaluate performance on large, well illuminated datasets like Kinetics and
HMDB51. We demonstrate that our work is able to achieve a very low error rate
while being trained on a much smaller dataset of dark videos. We also explore a
variety of training and inference strategies including domain transfer
methodologies and also propose a simple but useful frame selection strategy.
Our empirical results demonstrate that we beat previously published baseline
models by 11%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hira_S/0/1/0/all/0/1"&gt;Sanchit Hira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1"&gt;Ritwik Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Modi_A/0/1/0/all/0/1"&gt;Abhinav Modi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pakhomov_D/0/1/0/all/0/1"&gt;Daniil Pakhomov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[R3L: Connecting Deep Reinforcement Learning to Recurrent Neural Networks for Image Denoising via Residual Recovery. (arXiv:2107.05318v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.05318</id>
        <link href="http://arxiv.org/abs/2107.05318"/>
        <updated>2021-07-13T01:59:34.448Z</updated>
        <summary type="html"><![CDATA[State-of-the-art image denoisers exploit various types of deep neural
networks via deterministic training. Alternatively, very recent works utilize
deep reinforcement learning for restoring images with diverse or unknown
corruptions. Though deep reinforcement learning can generate effective policy
networks for operator selection or architecture search in image restoration,
how it is connected to the classic deterministic training in solving inverse
problems remains unclear. In this work, we propose a novel image denoising
scheme via Residual Recovery using Reinforcement Learning, dubbed R3L. We show
that R3L is equivalent to a deep recurrent neural network that is trained using
a stochastic reward, in contrast to many popular denoisers using supervised
learning with deterministic losses. To benchmark the effectiveness of
reinforcement learning in R3L, we train a recurrent neural network with the
same architecture for residual recovery using the deterministic loss, thus to
analyze how the two different training strategies affect the denoising
performance. With such a unified benchmarking system, we demonstrate that the
proposed R3L has better generalizability and robustness in image denoising when
the estimated noise level varies, comparing to its counterparts using
deterministic training, as well as various state-of-the-art image denoising
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rongkai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jiang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zhiyuan Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dauwels_J/0/1/0/all/0/1"&gt;Justin Dauwels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wen_B/0/1/0/all/0/1"&gt;Bihan Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Action Anticipation with RBF Kernelized Feature Mapping RNN. (arXiv:1911.07806v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.07806</id>
        <link href="http://arxiv.org/abs/1911.07806"/>
        <updated>2021-07-13T01:59:34.431Z</updated>
        <summary type="html"><![CDATA[We introduce a novel Recurrent Neural Network-based algorithm for future
video feature generation and action anticipation called feature mapping RNN.
Our novel RNN architecture builds upon three effective principles of machine
learning, namely parameter sharing, Radial Basis Function kernels and
adversarial training. Using only some of the earliest frames of a video, the
feature mapping RNN is able to generate future features with a fraction of the
parameters needed in traditional RNN. By feeding these future features into a
simple multi-layer perceptron facilitated with an RBF kernel layer, we are able
to accurately predict the action in the video. In our experiments, we obtain
18% improvement on JHMDB-21 dataset, 6% on UCF101-24 and 13% improvement on
UT-Interaction datasets over prior state-of-the-art for action anticipation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yuge Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernando_B/0/1/0/all/0/1"&gt;Basura Fernando&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1"&gt;Richard Hartley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Deep Feature Propagation for Early Action Recognition. (arXiv:2107.05122v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05122</id>
        <link href="http://arxiv.org/abs/2107.05122"/>
        <updated>2021-07-13T01:59:34.425Z</updated>
        <summary type="html"><![CDATA[Early action recognition (action prediction) from limited preliminary
observations plays a critical role for streaming vision systems that demand
real-time inference, as video actions often possess elongated temporal spans
which cause undesired latency. In this study, we address action prediction by
investigating how action patterns evolve over time in a spatial feature space.
There are three key components to our system. First, we work with
intermediate-layer ConvNet features, which allow for abstraction from raw data,
while retaining spatial layout. Second, instead of propagating features per se,
we propagate their residuals across time, which allows for a compact
representation that reduces redundancy. Third, we employ a Kalman filter to
combat error build-up and unify across prediction start times. Extensive
experimental results on multiple benchmarks show that our approach leads to
competitive performance in action prediction. Notably, we investigate the
learned components of our system to shed light on their otherwise opaque
natures in two ways. First, we document that our learned feature propagation
module works as a spatial shifting mechanism under convolution to propagate
current observations into the future. Thus, it captures flow-based image motion
information. Second, the learned Kalman filter adaptively updates prior
estimation to aid the sequence learning process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;He Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wildes_R/0/1/0/all/0/1"&gt;Richard P. Wildes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TransAttUnet: Multi-level Attention-guided U-Net with Transformer for Medical Image Segmentation. (arXiv:2107.05274v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.05274</id>
        <link href="http://arxiv.org/abs/2107.05274"/>
        <updated>2021-07-13T01:59:34.417Z</updated>
        <summary type="html"><![CDATA[With the development of deep encoder-decoder architectures and large-scale
annotated medical datasets, great progress has been achieved in the development
of automatic medical image segmentation. Due to the stacking of convolution
layers and the consecutive sampling operations, existing standard models
inevitably encounter the information recession problem of feature
representations, which fails to fully model the global contextual feature
dependencies. To overcome the above challenges, this paper proposes a novel
Transformer based medical image semantic segmentation framework called
TransAttUnet, in which the multi-level guided attention and multi-scale skip
connection are jointly designed to effectively enhance the functionality and
flexibility of traditional U-shaped architecture. Inspired by Transformer, a
novel self-aware attention (SAA) module with both Transformer Self Attention
(TSA) and Global Spatial Attention (GSA) is incorporated into TransAttUnet to
effectively learn the non-local interactions between encoder features. In
particular, we also establish additional multi-scale skip connections between
decoder blocks to aggregate the different semantic-scale upsampling features.
In this way, the representation ability of multi-scale context information is
strengthened to generate discriminative features. Benefitting from these
complementary components, the proposed TransAttUnet can effectively alleviate
the loss of fine details caused by the information recession problem, improving
the diagnostic sensitivity and segmentation quality of medical image analysis.
Extensive experiments on multiple medical image segmentation datasets of
different imaging demonstrate that our method consistently outperforms the
state-of-the-art baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bingzhi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yishu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_G/0/1/0/all/0/1"&gt;Guangming Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1"&gt;David Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TransClaw U-Net: Claw U-Net with Transformers for Medical Image Segmentation. (arXiv:2107.05188v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05188</id>
        <link href="http://arxiv.org/abs/2107.05188"/>
        <updated>2021-07-13T01:59:34.411Z</updated>
        <summary type="html"><![CDATA[In recent years, computer-aided diagnosis has become an increasingly popular
topic. Methods based on convolutional neural networks have achieved good
performance in medical image segmentation and classification. Due to the
limitations of the convolution operation, the long-term spatial features are
often not accurately obtained. Hence, we propose a TransClaw U-Net network
structure, which combines the convolution operation with the transformer
operation in the encoding part. The convolution part is applied for extracting
the shallow spatial features to facilitate the recovery of the image resolution
after upsampling. The transformer part is used to encode the patches, and the
self-attention mechanism is used to obtain global information between
sequences. The decoding part retains the bottom upsampling structure for better
detail segmentation performance. The experimental results on Synapse
Multi-organ Segmentation Datasets show that the performance of TransClaw U-Net
is better than other network structures. The ablation experiments also prove
the generalization performance of TransClaw U-Net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1"&gt;Yao Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menghan_H/0/1/0/all/0/1"&gt;Hu Menghan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guangtao_Z/0/1/0/all/0/1"&gt;Zhai Guangtao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Ping_Z/0/1/0/all/0/1"&gt;Zhang Xiao-Ping&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Details Preserving Deep Collaborative Filtering-Based Method for Image Denoising. (arXiv:2107.05115v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.05115</id>
        <link href="http://arxiv.org/abs/2107.05115"/>
        <updated>2021-07-13T01:59:34.405Z</updated>
        <summary type="html"><![CDATA[In spite of the improvements achieved by the several denoising algorithms
over the years, many of them still fail at preserving the fine details of the
image after denoising. This is as a result of the smooth-out effect they have
on the images. Most neural network-based algorithms have achieved better
quantitative performance than the classical denoising algorithms. However, they
also suffer from qualitative (visual) performance as a result of the smooth-out
effect. In this paper, we propose an algorithm to address this shortcoming. We
propose a deep collaborative filtering-based (Deep-CoFiB) algorithm for image
denoising. This algorithm performs collaborative denoising of image patches in
the sparse domain using a set of optimized neural network models. This results
in a fast algorithm that is able to excellently obtain a trade-off between
noise removal and details preservation. Extensive experiments show that the
DeepCoFiB performed quantitatively (in terms of PSNR and SSIM) and
qualitatively (visually) better than many of the state-of-the-art denoising
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Alawode_B/0/1/0/all/0/1"&gt;Basit O. Alawode&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Masood_M/0/1/0/all/0/1"&gt;Mudassir Masood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ballal_T/0/1/0/all/0/1"&gt;Tarig Ballal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Al_Naffouri_T/0/1/0/all/0/1"&gt;Tareq Al-Naffouri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ICDAR 2021 Competition on Integrated Circuit Text Spotting and Aesthetic Assessment. (arXiv:2107.05279v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05279</id>
        <link href="http://arxiv.org/abs/2107.05279"/>
        <updated>2021-07-13T01:59:34.389Z</updated>
        <summary type="html"><![CDATA[With hundreds of thousands of electronic chip components are being
manufactured every day, chip manufacturers have seen an increasing demand in
seeking a more efficient and effective way of inspecting the quality of printed
texts on chip components. The major problem that deters this area of research
is the lacking of realistic text on chips datasets to act as a strong
foundation. Hence, a text on chips dataset, ICText is used as the main target
for the proposed Robust Reading Challenge on Integrated Circuit Text Spotting
and Aesthetic Assessment (RRC-ICText) 2021 to encourage the research on this
problem. Throughout the entire competition, we have received a total of 233
submissions from 10 unique teams/individuals. Details of the competition and
submission results are presented in this report.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ng_C/0/1/0/all/0/1"&gt;Chun Chet Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nazaruddin_A/0/1/0/all/0/1"&gt;Akmalul Khairi Bin Nazaruddin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Yeong Khang Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1"&gt;Chee Seng Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1"&gt;Lianwen Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yipeng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1"&gt;Lixin Fan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Review of Video Predictive Understanding: Early ActionRecognition and Future Action Prediction. (arXiv:2107.05140v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05140</id>
        <link href="http://arxiv.org/abs/2107.05140"/>
        <updated>2021-07-13T01:59:34.383Z</updated>
        <summary type="html"><![CDATA[Video predictive understanding encompasses a wide range of efforts that are
concerned with the anticipation of the unobserved future from the current as
well as historical video observations. Action prediction is a major sub-area of
video predictive understanding and is the focus of this review. This sub-area
has two major subdivisions: early action recognition and future action
prediction. Early action recognition is concerned with recognizing an ongoing
action as soon as possible. Future action prediction is concerned with the
anticipation of actions that follow those previously observed. In either case,
the \textbf{\textit{causal}} relationship between the past, current, and
potential future information is the main focus. Various mathematical tools such
as Markov Chains, Gaussian Processes, Auto-Regressive modeling, and Bayesian
recursive filtering are widely adopted jointly with computer vision techniques
for these two tasks. However, these approaches face challenges such as the
curse of dimensionality, poor generalization, and constraints from
domain-specific knowledge. Recently, structures that rely on deep convolutional
neural networks and recurrent neural networks have been extensively proposed
for improving the performance of existing vision tasks, in general, and action
prediction tasks, in particular. However, they have their own shortcomings, \eg
reliance on massive training data and lack of strong theoretical underpinnings.
In this survey, we start by introducing the major sub-areas of the broad area
of video predictive understanding, which recently have received intensive
attention and proven to have practical value. Next, a thorough review of
various early action recognition and future action prediction algorithms are
provided with suitably organized divisions. Finally, we conclude our discussion
with future research directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;He Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wildes_R/0/1/0/all/0/1"&gt;Richard P. Wildes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EndoUDA: A modality independent segmentation approach for endoscopy imaging. (arXiv:2107.05342v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.05342</id>
        <link href="http://arxiv.org/abs/2107.05342"/>
        <updated>2021-07-13T01:59:34.376Z</updated>
        <summary type="html"><![CDATA[Gastrointestinal (GI) cancer precursors require frequent monitoring for risk
stratification of patients. Automated segmentation methods can help to assess
risk areas more accurately, and assist in therapeutic procedures or even
removal. In clinical practice, addition to the conventional white-light imaging
(WLI), complimentary modalities such as narrow-band imaging (NBI) and
fluorescence imaging are used. While, today most segmentation approaches are
supervised and only concentrated on a single modality dataset, this work
exploits to use a target-independent unsupervised domain adaptation (UDA)
technique that is capable to generalize to an unseen target modality. In this
context, we propose a novel UDA-based segmentation method that couples the
variational autoencoder and U-Net with a common EfficientNet-B4 backbone, and
uses a joint loss for latent-space optimization for target samples. We show
that our model can generalize to unseen target NBI (target) modality when
trained using only WLI (source) modality. Our experiments on both upper and
lower GI endoscopy data show the effectiveness of our approach compared to
naive supervised approach and state-of-the-art UDA segmentation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Celik_N/0/1/0/all/0/1"&gt;Numan Celik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1"&gt;Sharib Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Soumya Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Braden_B/0/1/0/all/0/1"&gt;Barbara Braden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rittscher_J/0/1/0/all/0/1"&gt;Jens Rittscher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Shot Compositional Concept Learning. (arXiv:2107.05176v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05176</id>
        <link href="http://arxiv.org/abs/2107.05176"/>
        <updated>2021-07-13T01:59:34.369Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the problem of recognizing compositional
attribute-object concepts within the zero-shot learning (ZSL) framework. We
propose an episode-based cross-attention (EpiCA) network which combines merits
of cross-attention mechanism and episode-based training strategy to recognize
novel compositional concepts. Firstly, EpiCA bases on cross-attention to
correlate concept-visual information and utilizes the gated pooling layer to
build contextualized representations for both images and concepts. The updated
representations are used for a more in-depth multi-modal relevance calculation
for concept recognition. Secondly, a two-phase episode training strategy,
especially the transductive phase, is adopted to utilize unlabeled test
examples to alleviate the low-resource learning problem. Experiments on two
widely-used zero-shot compositional learning (ZSCL) benchmarks have
demonstrated the effectiveness of the model compared with recent approaches on
both conventional and generalized ZSCL settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Guangyue Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kordjamshidi_P/0/1/0/all/0/1"&gt;Parisa Kordjamshidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1"&gt;Joyce Y. Chai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LiveView: Dynamic Target-Centered MPI for View Synthesis. (arXiv:2107.05113v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05113</id>
        <link href="http://arxiv.org/abs/2107.05113"/>
        <updated>2021-07-13T01:59:34.362Z</updated>
        <summary type="html"><![CDATA[Existing Multi-Plane Image (MPI) based view-synthesis methods generate an MPI
aligned with the input view using a fixed number of planes in one forward pass.
These methods produce fast, high-quality rendering of novel views, but rely on
slow and computationally expensive MPI generation methods unsuitable for
real-time applications. In addition, most MPI techniques use fixed
depth/disparity planes which cannot be modified once the training is complete,
hence offering very little flexibility at run-time.

We propose LiveView - a novel MPI generation and rendering technique that
produces high-quality view synthesis in real-time. Our method can also offer
the flexibility to select scene-dependent MPI planes (number of planes and
spacing between them) at run-time. LiveView first warps input images to target
view (target-centered) and then learns to generate a target view centered MPI,
one depth plane at a time (dynamically). The method generates high-quality
renderings, while also enabling fast MPI generation and novel view synthesis.
As a result, LiveView enables real-time view synthesis applications where an
MPI needs to be updated frequently based on a video stream of input views. We
demonstrate that LiveView improves the quality of view synthesis while being 70
times faster at run-time compared to state-of-the-art MPI-based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1"&gt;Sushobhan Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_Z/0/1/0/all/0/1"&gt;Zhaoyang Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsuda_N/0/1/0/all/0/1"&gt;Nathan Matsuda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1"&gt;Lei Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berkovich_A/0/1/0/all/0/1"&gt;Andrew Berkovich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cossairt_O/0/1/0/all/0/1"&gt;Oliver Cossairt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effect of Input Size on the Classification of Lung Nodules Using Convolutional Neural Networks. (arXiv:2107.05085v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.05085</id>
        <link href="http://arxiv.org/abs/2107.05085"/>
        <updated>2021-07-13T01:59:34.347Z</updated>
        <summary type="html"><![CDATA[Recent studies have shown that lung cancer screening using annual low-dose
computed tomography (CT) reduces lung cancer mortality by 20% compared to
traditional chest radiography. Therefore, CT lung screening has started to be
used widely all across the world. However, analyzing these images is a serious
burden for radiologists. The number of slices in a CT scan can be up to 600.
Therefore, computer-aided-detection (CAD) systems are very important for faster
and more accurate assessment of the data. In this study, we proposed a
framework that analyzes CT lung screenings using convolutional neural networks
(CNNs) to reduce false positives. We trained our model with different volume
sizes and showed that volume size plays a critical role in the performance of
the system. We also used different fusions in order to show their power and
effect on the overall accuracy. 3D CNNs were preferred over 2D CNNs because 2D
convolutional operations applied to 3D data could result in information loss.
The proposed framework has been tested on the dataset provided by the LUNA16
Challenge and resulted in a sensitivity of 0.831 at 1 false positive per scan.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Polat_G/0/1/0/all/0/1"&gt;Gorkem Polat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Serinagaoglu_Y/0/1/0/all/0/1"&gt;Yesim Dogrusoz Serinagaoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Halici_U/0/1/0/all/0/1"&gt;Ugur Halici&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoFB: Automating Fetal Biometry Estimation from Standard Ultrasound Planes. (arXiv:2107.05255v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05255</id>
        <link href="http://arxiv.org/abs/2107.05255"/>
        <updated>2021-07-13T01:59:34.341Z</updated>
        <summary type="html"><![CDATA[During pregnancy, ultrasound examination in the second trimester can assess
fetal size according to standardized charts. To achieve a reproducible and
accurate measurement, a sonographer needs to identify three standard 2D planes
of the fetal anatomy (head, abdomen, femur) and manually mark the key
anatomical landmarks on the image for accurate biometry and fetal weight
estimation. This can be a time-consuming operator-dependent task, especially
for a trainee sonographer. Computer-assisted techniques can help in automating
the fetal biometry computation process. In this paper, we present a unified
automated framework for estimating all measurements needed for the fetal weight
assessment. The proposed framework semantically segments the key fetal
anatomies using state-of-the-art segmentation models, followed by region
fitting and scale recovery for the biometry estimation. We present an ablation
study of segmentation algorithms to show their robustness through 4-fold
cross-validation on a dataset of 349 ultrasound standard plane images from 42
pregnancies. Moreover, we show that the network with the best segmentation
performance tends to be more accurate for biometry estimation. Furthermore, we
demonstrate that the error between clinically measured and predicted fetal
biometry is lower than the permissible error during routine clinical
measurements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bano_S/0/1/0/all/0/1"&gt;Sophia Bano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dromey_B/0/1/0/all/0/1"&gt;Brian Dromey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasconcelos_F/0/1/0/all/0/1"&gt;Francisco Vasconcelos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Napolitano_R/0/1/0/all/0/1"&gt;Raffaele Napolitano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+David_A/0/1/0/all/0/1"&gt;Anna L. David&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peebles_D/0/1/0/all/0/1"&gt;Donald M. Peebles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1"&gt;Danail Stoyanov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Shot Scene Graph Relation Prediction through Commonsense Knowledge Integration. (arXiv:2107.05080v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05080</id>
        <link href="http://arxiv.org/abs/2107.05080"/>
        <updated>2021-07-13T01:59:34.335Z</updated>
        <summary type="html"><![CDATA[Relation prediction among entities in images is an important step in scene
graph generation (SGG), which further impacts various visual understanding and
reasoning tasks. Existing SGG frameworks, however, require heavy training yet
are incapable of modeling unseen (i.e.,zero-shot) triplets. In this work, we
stress that such incapability is due to the lack of commonsense reasoning,i.e.,
the ability to associate similar entities and infer similar relations based on
general understanding of the world. To fill this gap, we propose
CommOnsense-integrAted sCenegrapHrElation pRediction (COACHER), a framework to
integrate commonsense knowledge for SGG, especially for zero-shot relation
prediction. Specifically, we develop novel graph mining pipelines to model the
neighborhoods and paths around entities in an external commonsense knowledge
graph, and integrate them on top of state-of-the-art SGG frameworks. Extensive
quantitative evaluations and qualitative case studies on both original and
manipulated datasets from Visual Genome demonstrate the effectiveness of our
proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kan_X/0/1/0/all/0/1"&gt;Xuan Kan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1"&gt;Hejie Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Carl Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SE-PSNet: Silhouette-based Enhancement Feature for Panoptic Segmentation Network. (arXiv:2107.05093v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05093</id>
        <link href="http://arxiv.org/abs/2107.05093"/>
        <updated>2021-07-13T01:59:34.328Z</updated>
        <summary type="html"><![CDATA[Recently, there has been a panoptic segmentation task combining semantic and
instance segmentation, in which the goal is to classify each pixel with the
corresponding instance ID. In this work, we propose a solution to tackle the
panoptic segmentation task. The overall structure combines the bottom-up method
and the top-down method. Therefore, not only can there be better performance,
but also the execution speed can be maintained. The network mainly pays
attention to the quality of the mask. In the previous work, we can see that the
uneven contour of the object is more likely to appear, resulting in low-quality
prediction. Accordingly, we propose enhancement features and corresponding loss
functions for the silhouette of objects and backgrounds to improve the mask.
Meanwhile, we use the new proposed confidence score to solve the occlusion
problem and make the network tend to use higher quality masks as prediction
results. To verify our research, we used the COCO dataset and CityScapes
dataset to do experiments and obtained competitive results with fast inference
time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Shuo-En Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi-Cheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1"&gt;En-Ting Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsiao_P/0/1/0/all/0/1"&gt;Pei-Yung Hsiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1"&gt;Li-Chen Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Remote Blood Oxygen Estimation From Videos Using Neural Networks. (arXiv:2107.05087v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05087</id>
        <link href="http://arxiv.org/abs/2107.05087"/>
        <updated>2021-07-13T01:59:34.321Z</updated>
        <summary type="html"><![CDATA[Blood oxygen saturation (SpO$_2$) is an essential indicator of respiratory
functionality and is receiving increasing attention during the COVID-19
pandemic. Clinical findings show that it is possible for COVID-19 patients to
have significantly low SpO$_2$ before any obvious symptoms. The prevalence of
cameras has motivated researchers to investigate methods for monitoring SpO$_2$
using videos. Most prior schemes involving smartphones are contact-based: They
require a fingertip to cover the phone's camera and the nearby light source to
capture re-emitted light from the illuminated tissue. In this paper, we propose
the first convolutional neural network based noncontact SpO$_2$ estimation
scheme using smartphone cameras. The scheme analyzes the videos of a
participant's hand for physiological sensing, which is convenient and
comfortable, and can protect their privacy and allow for keeping face masks on.
We design our neural network architectures inspired by the optophysiological
models for SpO$_2$ measurement and demonstrate the explainability by
visualizing the weights for channel combination. Our proposed models outperform
the state-of-the-art model that is designed for contact-based SpO$_2$
measurement, showing the potential of our proposed method to contribute to
public health. We also analyze the impact of skin type and the side of a hand
on SpO$_2$ estimation performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mathew_J/0/1/0/all/0/1"&gt;Joshua Mathew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1"&gt;Xin Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1"&gt;Min Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1"&gt;Chau-Wai Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prb-GAN: A Probabilistic Framework for GAN Modelling. (arXiv:2107.05241v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05241</id>
        <link href="http://arxiv.org/abs/2107.05241"/>
        <updated>2021-07-13T01:59:34.305Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) are very popular to generate realistic
images, but they often suffer from the training instability issues and the
phenomenon of mode loss. In order to attain greater diversity in GAN
synthesized data, it is critical to solving the problem of mode loss. Our work
explores probabilistic approaches to GAN modelling that could allow us to
tackle these issues. We present Prb-GANs, a new variation that uses dropout to
create a distribution over the network parameters with the posterior learnt
using variational inference. We describe theoretically and validate
experimentally using simple and complex datasets the benefits of such an
approach. We look into further improvements using the concept of uncertainty
measures. Through a set of further modifications to the loss functions for each
network of the GAN, we are able to get results that show the improvement of GAN
performance. Our methods are extremely simple and require very little
modification to existing GAN architecture.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+George_B/0/1/0/all/0/1"&gt;Blessen George&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurmi_V/0/1/0/all/0/1"&gt;Vinod K. Kurmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1"&gt;Vinay P. Namboodiri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Transformer with Statistical Test for COVID-19 Classification. (arXiv:2107.05334v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.05334</id>
        <link href="http://arxiv.org/abs/2107.05334"/>
        <updated>2021-07-13T01:59:34.296Z</updated>
        <summary type="html"><![CDATA[With the massive damage in the world caused by Coronavirus Disease 2019
SARS-CoV-2 (COVID-19), many related research topics have been proposed in the
past two years. The Chest Computed Tomography (CT) scans are the most valuable
materials to diagnose the COVID-19 symptoms. However, most schemes for COVID-19
classification of Chest CT scan is based on a single-slice level, implying that
the most critical CT slice should be selected from the original CT scan volume
manually. We simultaneously propose 2-D and 3-D models to predict the COVID-19
of CT scan to tickle this issue. In our 2-D model, we introduce the Deep
Wilcoxon signed-rank test (DWCC) to determine the importance of each slice of a
CT scan to overcome the issue mentioned previously. Furthermore, a
Convolutional CT scan-Aware Transformer (CCAT) is proposed to discover the
context of the slices fully. The frame-level feature is extracted from each CT
slice based on any backbone network and followed by feeding the features to our
within-slice-Transformer (WST) to discover the context information in the pixel
dimension. The proposed Between-Slice-Transformer (BST) is used to aggregate
the extracted spatial-context features of every CT slice. A simple classifier
is then used to judge whether the Spatio-temporal features are COVID-19 or
non-COVID-19. The extensive experiments demonstrated that the proposed CCAT and
DWCC significantly outperform the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hsu_C/0/1/0/all/0/1"&gt;Chih-Chung Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guan-Lin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_M/0/1/0/all/0/1"&gt;Mei-Hsuan Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training deep cross-modality conversion models with a small amount of data and its application to MVCT to kVCT conversion. (arXiv:2107.05238v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05238</id>
        <link href="http://arxiv.org/abs/2107.05238"/>
        <updated>2021-07-13T01:59:34.288Z</updated>
        <summary type="html"><![CDATA[Deep-learning-based image processing has emerged as a valuable tool in recent
years owing to its high performance. However, the quality of
deep-learning-based methods relies heavily on the amount of training data, and
the cost of acquiring a large amount of data is often prohibitive in medical
fields. Therefore, we performed CT modality conversion based on deep learning
requiring only a small number of unsupervised images. The proposed method is
based on generative adversarial networks (GANs) with several extensions
tailored for CT images. This method emphasizes the preservation of the
structure in the processed images and reduction in the amount of training data.
This method was applied to realize the conversion of mega-voltage computed
tomography (MVCT) to kilo-voltage computed tomography (kVCT) images. Training
was performed using several datasets acquired from patients with head and neck
cancer. The size of the datasets ranged from 16 slices (for two patients) to
2745 slices (for 137 patients) of MVCT and 2824 slices of kVCT for 98 patients.
The quality of the processed MVCT images was considerably enhanced, and the
structural changes in the images were minimized. With an increase in the size
of training data, the image quality exhibited a satisfactory convergence from a
few hundred slices. In addition to statistical and visual evaluations, these
results were clinically evaluated by medical doctors in terms of the accuracy
of contouring. We developed an MVCT to kVCT conversion model based on deep
learning, which can be trained using a few hundred unpaired images. The
stability of the model against the change in the data size was demonstrated.
This research promotes the reliable use of deep learning in clinical medicine
by partially answering the commonly asked questions: "Is our data enough? How
much data must we prepare?"]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozaki_S/0/1/0/all/0/1"&gt;Sho Ozaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaji_S/0/1/0/all/0/1"&gt;Shizuo Kaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nawa_K/0/1/0/all/0/1"&gt;Kanabu Nawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Imae_T/0/1/0/all/0/1"&gt;Toshikazu Imae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aoki_A/0/1/0/all/0/1"&gt;Atsushi Aoki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakamoto_T/0/1/0/all/0/1"&gt;Takahiro Nakamoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ohta_T/0/1/0/all/0/1"&gt;Takeshi Ohta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nozawa_Y/0/1/0/all/0/1"&gt;Yuki Nozawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamashita_H/0/1/0/all/0/1"&gt;Hideomi Yamashita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haga_A/0/1/0/all/0/1"&gt;Akihiro Haga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakagawa_K/0/1/0/all/0/1"&gt;Keiichi Nakagawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Early warning of pedestrians and cyclists. (arXiv:2107.05186v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05186</id>
        <link href="http://arxiv.org/abs/2107.05186"/>
        <updated>2021-07-13T01:59:34.281Z</updated>
        <summary type="html"><![CDATA[State-of-the-art motor vehicles are able to break for pedestrians in an
emergency. We investigate what it would take to issue an early warning to the
driver so he/she has time to react. We have identified that predicting the
intention of a pedestrian reliably by position is a particularly hard
challenge. This paper describes an early pedestrian warning demonstration
system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_J/0/1/0/all/0/1"&gt;Joerg Christian Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep-learning-based Hyperspectral imaging through a RGB camera. (arXiv:2107.05190v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.05190</id>
        <link href="http://arxiv.org/abs/2107.05190"/>
        <updated>2021-07-13T01:59:34.243Z</updated>
        <summary type="html"><![CDATA[Hyperspectral image (HSI) contains both spatial pattern and spectral
information which has been widely used in food safety, remote sensing, and
medical detection. However, the acquisition of hyperspectral images is usually
costly due to the complicated apparatus for the acquisition of optical
spectrum. Recently, it has been reported that HSI can be reconstructed from
single RGB image using convolution neural network (CNN) algorithms. Compared
with the traditional hyperspectral cameras, the method based on CNN algorithms
is simple, portable and low cost. In this study, we focused on the influence of
the RGB camera spectral sensitivity (CSS) on the HSI. A Xenon lamp incorporated
with a monochromator were used as the standard light source to calibrate the
CSS. And the experimental results show that the CSS plays a significant role in
the reconstruction accuracy of an HSI. In addition, we proposed a new HSI
reconstruction network where the dimensional structure of the original
hyperspectral datacube was modified by 3D matrix transpose to improve the
reconstruction accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xinyu Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianlang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tao_J/0/1/0/all/0/1"&gt;Jinchao Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qiu_Y/0/1/0/all/0/1"&gt;Yanqing Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Meng_Y/0/1/0/all/0/1"&gt;Yanlong Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mao_B/0/1/0/all/0/1"&gt;Banging Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_P/0/1/0/all/0/1"&gt;Pengwei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yi Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatial and Temporal Networks for Facial Expression Recognition in the Wild Videos. (arXiv:2107.05160v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05160</id>
        <link href="http://arxiv.org/abs/2107.05160"/>
        <updated>2021-07-13T01:59:34.224Z</updated>
        <summary type="html"><![CDATA[The paper describes our proposed methodology for the seven basic expression
classification track of Affective Behavior Analysis in-the-wild (ABAW)
Competition 2021. In this task, facial expression recognition (FER) methods aim
to classify the correct expression category from a diverse background, but
there are several challenges. First, to adapt the model to in-the-wild
scenarios, we use the knowledge from pre-trained large-scale face recognition
data. Second, we propose an ensemble model with a convolution neural network
(CNN), a CNN-recurrent neural network (CNN-RNN), and a CNN-Transformer
(CNN-Transformer), to incorporate both spatial and temporal information. Our
ensemble model achieved F1 as 0.4133, accuracy as 0.6216 and final metric as
0.4821 on the validation set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1"&gt;Shuyi Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1"&gt;Xinqi Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1"&gt;Xiaojiang Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeoUNet: Towards accurate colon polyp segmentation and neoplasm detection. (arXiv:2107.05023v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.05023</id>
        <link href="http://arxiv.org/abs/2107.05023"/>
        <updated>2021-07-13T01:59:34.204Z</updated>
        <summary type="html"><![CDATA[Automatic polyp segmentation has proven to be immensely helpful for endoscopy
procedures, reducing the missing rate of adenoma detection for endoscopists
while increasing efficiency. However, classifying a polyp as being neoplasm or
not and segmenting it at the pixel level is still a challenging task for
doctors to perform in a limited time. In this work, we propose a fine-grained
formulation for the polyp segmentation problem. Our formulation aims to not
only segment polyp regions, but also identify those at high risk of malignancy
with high accuracy. In addition, we present a UNet-based neural network
architecture called NeoUNet, along with a hybrid loss function to solve this
problem. Experiments show highly competitive results for NeoUNet on our
benchmark dataset compared to existing polyp segmentation models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lan_P/0/1/0/all/0/1"&gt;Phan Ngoc Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+An_N/0/1/0/all/0/1"&gt;Nguyen Sy An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hang_D/0/1/0/all/0/1"&gt;Dao Viet Hang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Long_D/0/1/0/all/0/1"&gt;Dao Van Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Trung_T/0/1/0/all/0/1"&gt;Tran Quang Trung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thuy_N/0/1/0/all/0/1"&gt;Nguyen Thi Thuy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sang_D/0/1/0/all/0/1"&gt;Dinh Viet Sang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Object Detection with Adaptive Class-Rebalancing Self-Training. (arXiv:2107.05031v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05031</id>
        <link href="http://arxiv.org/abs/2107.05031"/>
        <updated>2021-07-13T01:59:34.197Z</updated>
        <summary type="html"><![CDATA[This study delves into semi-supervised object detection (SSOD) to improve
detector performance with additional unlabeled data. State-of-the-art SSOD
performance has been achieved recently by self-training, in which training
supervision consists of ground truths and pseudo-labels. In current studies, we
observe that class imbalance in SSOD severely impedes the effectiveness of
self-training. To address the class imbalance, we propose adaptive
class-rebalancing self-training (ACRST) with a novel memory module called
CropBank. ACRST adaptively rebalances the training data with foreground
instances extracted from the CropBank, thereby alleviating the class imbalance.
Owing to the high complexity of detection tasks, we observe that both
self-training and data-rebalancing suffer from noisy pseudo-labels in SSOD.
Therefore, we propose a novel two-stage filtering algorithm to generate
accurate pseudo-labels. Our method achieves satisfactory improvements on
MS-COCO and VOC benchmarks. When using only 1\% labeled data in MS-COCO, our
method achieves 17.02 mAP improvement over supervised baselines, and 5.32 mAP
improvement compared with state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fangyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_T/0/1/0/all/0/1"&gt;Tianxiang Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction Surface Uncertainty Quantification in Object Detection Models for Autonomous Driving. (arXiv:2107.04991v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04991</id>
        <link href="http://arxiv.org/abs/2107.04991"/>
        <updated>2021-07-13T01:59:34.191Z</updated>
        <summary type="html"><![CDATA[Object detection in autonomous cars is commonly based on camera images and
Lidar inputs, which are often used to train prediction models such as deep
artificial neural networks for decision making for object recognition,
adjusting speed, etc. A mistake in such decision making can be damaging; thus,
it is vital to measure the reliability of decisions made by such prediction
models via uncertainty measurement. Uncertainty, in deep learning models, is
often measured for classification problems. However, deep learning models in
autonomous driving are often multi-output regression models. Hence, we propose
a novel method called PURE (Prediction sURface uncErtainty) for measuring
prediction uncertainty of such regression models. We formulate the object
recognition problem as a regression model with more than one outputs for
finding object locations in a 2-dimensional camera view. For evaluation, we
modified three widely-applied object recognition models (i.e., YoLo, SSD300 and
SSD512) and used the KITTI, Stanford Cars, Berkeley DeepDrive, and NEXET
datasets. Results showed the statistically significant negative correlation
between prediction surface uncertainty and prediction accuracy suggesting that
uncertainty significantly impacts the decisions made by autonomous driving.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Catak_F/0/1/0/all/0/1"&gt;Ferhat Ozgur Catak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_T/0/1/0/all/0/1"&gt;Tao Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1"&gt;Shaukat Ali&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Locality Relationship Constrained Multi-view Clustering Framework. (arXiv:2107.05073v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05073</id>
        <link href="http://arxiv.org/abs/2107.05073"/>
        <updated>2021-07-13T01:59:34.175Z</updated>
        <summary type="html"><![CDATA[In most practical applications, it's common to utilize multiple features from
different views to represent one object. Among these works, multi-view
subspace-based clustering has gained extensive attention from many researchers,
which aims to provide clustering solutions to multi-view data. However, most
existing methods fail to take full use of the locality geometric structure and
similarity relationship among samples under the multi-view scenario. To solve
these issues, we propose a novel multi-view learning method with locality
relationship constraint to explore the problem of multi-view clustering, called
Locality Relationship Constrained Multi-view Clustering Framework (LRC-MCF).
LRC-MCF aims to explore the diversity, geometric, consensus and complementary
information among different views, by capturing the locality relationship
information and the common similarity relationships among multiple views.
Moreover, LRC-MCF takes sufficient consideration to weights of different views
in finding the common-view locality structure and straightforwardly produce the
final clusters. To effectually reduce the redundancy of the learned
representations, the low-rank constraint on the common similarity matrix is
considered additionally. To solve the minimization problem of LRC-MCF, an
Alternating Direction Minimization (ADM) method is provided to iteratively
calculate all variables LRC-MCF. Extensive experimental results on seven
benchmark multi-view datasets validate the effectiveness of the LRC-MCF method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1"&gt;Xiangzhu Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1"&gt;Wei Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wenzhe Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Cloud-Edge-Terminal Collaborative System for Temperature Measurement in COVID-19 Prevention. (arXiv:2107.05078v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05078</id>
        <link href="http://arxiv.org/abs/2107.05078"/>
        <updated>2021-07-13T01:59:34.167Z</updated>
        <summary type="html"><![CDATA[To prevent the spread of coronavirus disease 2019 (COVID-19), preliminary
temperature measurement and mask detection in public areas are conducted.
However, the existing temperature measurement methods face the problems of
safety and deployment. In this paper, to realize safe and accurate temperature
measurement even when a person's face is partially obscured, we propose a
cloud-edge-terminal collaborative system with a lightweight infrared
temperature measurement model. A binocular camera with an RGB lens and a
thermal lens is utilized to simultaneously capture image pairs. Then, a mobile
detection model based on a multi-task cascaded convolutional network (MTCNN) is
proposed to realize face alignment and mask detection on the RGB images. For
accurate temperature measurement, we transform the facial landmarks on the RGB
images to the thermal images by an affine transformation and select a more
accurate temperature measurement area on the forehead. The collected
information is uploaded to the cloud in real time for COVID-19 prevention.
Experiments show that the detection model is only 6.1M and the average
detection speed is 257ms. At a distance of 1m, the error of indoor temperature
measurement is about 3%. That is, the proposed system can realize real-time
temperature measurement in public areas.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zheyi Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1"&gt;Wen Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qingwen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1"&gt;Zhiyong Bu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Blending Pruning Criteria for Convolutional Neural Networks. (arXiv:2107.05033v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05033</id>
        <link href="http://arxiv.org/abs/2107.05033"/>
        <updated>2021-07-13T01:59:34.161Z</updated>
        <summary type="html"><![CDATA[The advancement of convolutional neural networks (CNNs) on various vision
applications has attracted lots of attention. Yet the majority of CNNs are
unable to satisfy the strict requirement for real-world deployment. To overcome
this, the recent popular network pruning is an effective method to reduce the
redundancy of the models. However, the ranking of filters according to their
"importance" on different pruning criteria may be inconsistent. One filter
could be important according to a certain criterion, while it is unnecessary
according to another one, which indicates that each criterion is only a partial
view of the comprehensive "importance". From this motivation, we propose a
novel framework to integrate the existing filter pruning criteria by exploring
the criteria diversity. The proposed framework contains two stages: Criteria
Clustering and Filters Importance Calibration. First, we condense the pruning
criteria via layerwise clustering based on the rank of "importance" score.
Second, within each cluster, we propose a calibration factor to adjust their
significance for each selected blending candidates and search for the optimal
blending criterion via Evolutionary Algorithm. Quantitative results on the
CIFAR-100 and ImageNet benchmarks show that our framework outperforms the
state-of-the-art baselines, regrading to the compact model performance after
pruning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1"&gt;Wei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhongzhan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1"&gt;Mingfu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1"&gt;Senwei Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haizhao Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Accurate Localization by Instance Search. (arXiv:2107.05005v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05005</id>
        <link href="http://arxiv.org/abs/2107.05005"/>
        <updated>2021-07-13T01:59:34.154Z</updated>
        <summary type="html"><![CDATA[Visual object localization is the key step in a series of object detection
tasks. In the literature, high localization accuracy is achieved with the
mainstream strongly supervised frameworks. However, such methods require
object-level annotations and are unable to detect objects of unknown
categories. Weakly supervised methods face similar difficulties. In this paper,
a self-paced learning framework is proposed to achieve accurate object
localization on the rank list returned by instance search. The proposed
framework mines the target instance gradually from the queries and their
corresponding top-ranked search results. Since a common instance is shared
between the query and the images in the rank list, the target visual instance
can be accurately localized even without knowing what the object category is.
In addition to performing localization on instance search, the issue of
few-shot object detection is also addressed under the same framework. Superior
performance over state-of-the-art methods is observed on both tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1"&gt;Yi-Geng Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1"&gt;Hui-Chu Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wan-Lei Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Projector-Camera System Using Hybrid Pixels with Projection and Capturing Capabilities. (arXiv:2107.05043v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05043</id>
        <link href="http://arxiv.org/abs/2107.05043"/>
        <updated>2021-07-13T01:59:34.147Z</updated>
        <summary type="html"><![CDATA[We propose a novel projector-camera system (ProCams) in which each pixel has
both projection and capturing capabilities. Our proposed ProCams solves the
difficulty of obtaining precise pixel correspondence between the projector and
the camera. We implemented a proof-of-concept ProCams prototype and
demonstrated its applicability to a dynamic projection mapping.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yamamoto_K/0/1/0/all/0/1"&gt;Kenta Yamamoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iwai_D/0/1/0/all/0/1"&gt;Daisuke Iwai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sato_K/0/1/0/all/0/1"&gt;Kosuke Sato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of Plant Leaf Disease Directly in the JPEG Compressed Domain using Transfer Learning Technique. (arXiv:2107.04813v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04813</id>
        <link href="http://arxiv.org/abs/2107.04813"/>
        <updated>2021-07-13T01:59:34.141Z</updated>
        <summary type="html"><![CDATA[Plant leaf diseases pose a significant danger to food security and they cause
depletion in quality and volume of production. Therefore accurate and timely
detection of leaf disease is very important to check the loss of the crops and
meet the growing food demand of the people. Conventional techniques depend on
lab investigation and human skills which are generally costly and inaccessible.
Recently, Deep Neural Networks have been exceptionally fruitful in image
classification. In this research paper, plant leaf disease detection employing
transfer learning is explored in the JPEG compressed domain. Here, the JPEG
compressed stream consisting of DCT coefficients is, directly fed into the
Neural Network to improve the efficiency of classification. The experimental
results on JPEG compressed leaf dataset demonstrate the efficacy of the
proposed model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Atul Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajesh_B/0/1/0/all/0/1"&gt;Bulla Rajesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javed_M/0/1/0/all/0/1"&gt;Mohammed Javed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CSL-YOLO: A New Lightweight Object Detection System for Edge Computing. (arXiv:2107.04829v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04829</id>
        <link href="http://arxiv.org/abs/2107.04829"/>
        <updated>2021-07-13T01:59:34.135Z</updated>
        <summary type="html"><![CDATA[The development of lightweight object detectors is essential due to the
limited computation resources. To reduce the computation cost, how to generate
redundant features plays a significant role. This paper proposes a new
lightweight Convolution method Cross-Stage Lightweight (CSL) Module, to
generate redundant features from cheap operations. In the intermediate
expansion stage, we replaced Pointwise Convolution with Depthwise Convolution
to produce candidate features. The proposed CSL-Module can reduce the
computation cost significantly. Experiments conducted at MS-COCO show that the
proposed CSL-Module can approximate the fitting ability of Convolution-3x3.
Finally, we use the module to construct a lightweight detector CSL-YOLO,
achieving better detection performance with only 43% FLOPs and 52% parameters
than Tiny-YOLOv4.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu-Ming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chun-Chieh Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_J/0/1/0/all/0/1"&gt;Jun-Wei Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_K/0/1/0/all/0/1"&gt;Kuo-Chin Fan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Self-Supervised Learning for Medical Image Segmentation Based on Multi-Domain Data Aggregation. (arXiv:2107.04886v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04886</id>
        <link href="http://arxiv.org/abs/2107.04886"/>
        <updated>2021-07-13T01:59:34.128Z</updated>
        <summary type="html"><![CDATA[A large labeled dataset is a key to the success of supervised deep learning,
but for medical image segmentation, it is highly challenging to obtain
sufficient annotated images for model training. In many scenarios, unannotated
images are abundant and easy to acquire. Self-supervised learning (SSL) has
shown great potentials in exploiting raw data information and representation
learning. In this paper, we propose Hierarchical Self-Supervised Learning
(HSSL), a new self-supervised framework that boosts medical image segmentation
by making good use of unannotated data. Unlike the current literature on
task-specific self-supervised pretraining followed by supervised fine-tuning,
we utilize SSL to learn task-agnostic knowledge from heterogeneous data for
various medical image segmentation tasks. Specifically, we first aggregate a
dataset from several medical challenges, then pre-train the network in a
self-supervised manner, and finally fine-tune on labeled data. We develop a new
loss function by combining contrastive loss and classification loss and
pretrain an encoder-decoder architecture for segmentation tasks. Our extensive
experiments show that multi-domain joint pre-training benefits downstream
segmentation tasks and outperforms single-domain pre-training significantly.
Compared to learning from scratch, our new method yields better performance on
various tasks (e.g., +0.69% to +18.60% in Dice scores with 5% of annotated
data). With limited amounts of training data, our method can substantially
bridge the performance gap w.r.t. denser annotations (e.g., 10% vs.~100% of
annotated data).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Hao Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jun Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongxiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhuo Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chaoli Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Danny Z. Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aligning Correlation Information for Domain Adaptation in Action Recognition. (arXiv:2107.04932v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04932</id>
        <link href="http://arxiv.org/abs/2107.04932"/>
        <updated>2021-07-13T01:59:34.097Z</updated>
        <summary type="html"><![CDATA[Domain adaptation (DA) approaches address domain shift and enable networks to
be applied to different scenarios. Although various image DA approaches have
been proposed in recent years, there is limited research towards video DA. This
is partly due to the complexity in adapting the different modalities of
features in videos, which includes the correlation features extracted as
long-term dependencies of pixels across spatiotemporal dimensions. The
correlation features are highly associated with action classes and proven their
effectiveness in accurate video feature extraction through the supervised
action recognition task. Yet correlation features of the same action would
differ across domains due to domain shift. Therefore we propose a novel
Adversarial Correlation Adaptation Network (ACAN) to align action videos by
aligning pixel correlations. ACAN aims to minimize the distribution of
correlation information, termed as Pixel Correlation Discrepancy (PCD).
Additionally, video DA research is also limited by the lack of cross-domain
video datasets with larger domain shifts. We, therefore, introduce a novel
HMDB-ARID dataset with a larger domain shift caused by a larger statistical
difference between domains. This dataset is built in an effort to leverage
current datasets for dark video classification. Empirical results demonstrate
the state-of-the-art performance of our proposed ACAN for both existing and the
new video DA datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yuecong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jianfei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1"&gt;Haozhi Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_K/0/1/0/all/0/1"&gt;Kezhi Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1"&gt;Jianxiong Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+See_S/0/1/0/all/0/1"&gt;Simon See&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Spatial Guided Self-supervised Clustering Network for Medical Image Segmentation. (arXiv:2107.04934v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04934</id>
        <link href="http://arxiv.org/abs/2107.04934"/>
        <updated>2021-07-13T01:59:34.089Z</updated>
        <summary type="html"><![CDATA[The segmentation of medical images is a fundamental step in automated
clinical decision support systems. Existing medical image segmentation methods
based on supervised deep learning, however, remain problematic because of their
reliance on large amounts of labelled training data. Although medical imaging
data repositories continue to expand, there has not been a commensurate
increase in the amount of annotated data. Hence, we propose a new spatial
guided self-supervised clustering network (SGSCN) for medical image
segmentation, where we introduce multiple loss functions designed to aid in
grouping image pixels that are spatially connected and have similar feature
representations. It iteratively learns feature representations and clustering
assignment of each pixel in an end-to-end fashion from a single image. We also
propose a context-based consistency loss that better delineates the shape and
boundaries of image regions. It enforces all the pixels belonging to a cluster
to be spatially close to the cluster centre. We evaluated our method on 2
public medical image datasets and compared it to existing conventional and
self-supervised clustering methods. Experimental results show that our method
was most accurate for medical image segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahn_E/0/1/0/all/0/1"&gt;Euijoon Ahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1"&gt;Dagan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jinman Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Similarity Guided Deep Face Image Retrieval. (arXiv:2107.05025v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05025</id>
        <link href="http://arxiv.org/abs/2107.05025"/>
        <updated>2021-07-13T01:59:34.078Z</updated>
        <summary type="html"><![CDATA[Face image retrieval, which searches for images of the same identity from the
query input face image, is drawing more attention as the size of the image
database increases rapidly. In order to conduct fast and accurate retrieval, a
compact hash code-based methods have been proposed, and recently, deep face
image hashing methods with supervised classification training have shown
outstanding performance. However, classification-based scheme has a
disadvantage in that it cannot reveal complex similarities between face images
into the hash code learning. In this paper, we attempt to improve the face
image retrieval quality by proposing a Similarity Guided Hashing (SGH) method,
which gently considers self and pairwise-similarity simultaneously. SGH employs
various data augmentations designed to explore elaborate similarities between
face images, solving both intra and inter identity-wise difficulties. Extensive
experimental results on the protocols with existing benchmarks and an
additionally proposed large scale higher resolution face image dataset
demonstrate that our SGH delivers state-of-the-art retrieval performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1"&gt;Young Kyun Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_N/0/1/0/all/0/1"&gt;Nam Ik Cho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BEV-MODNet: Monocular Camera based Bird's Eye View Moving Object Detection for Autonomous Driving. (arXiv:2107.04937v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04937</id>
        <link href="http://arxiv.org/abs/2107.04937"/>
        <updated>2021-07-13T01:59:34.067Z</updated>
        <summary type="html"><![CDATA[Detection of moving objects is a very important task in autonomous driving
systems. After the perception phase, motion planning is typically performed in
Bird's Eye View (BEV) space. This would require projection of objects detected
on the image plane to top view BEV plane. Such a projection is prone to errors
due to lack of depth information and noisy mapping in far away areas. CNNs can
leverage the global context in the scene to project better. In this work, we
explore end-to-end Moving Object Detection (MOD) on the BEV map directly using
monocular images as input. To the best of our knowledge, such a dataset does
not exist and we create an extended KITTI-raw dataset consisting of 12.9k
images with annotations of moving object masks in BEV space for five classes.
The dataset is intended to be used for class agnostic motion cue based object
detection and classes are provided as meta-data for better tuning. We design
and implement a two-stream RGB and optical flow fusion architecture which
outputs motion segmentation directly in BEV space. We compare it with inverse
perspective mapping of state-of-the-art motion segmentation predictions on the
image plane. We observe a significant improvement of 13% in mIoU using the
simple baseline implementation. This demonstrates the ability to directly learn
motion segmentation output in BEV space. Qualitative results of our baseline
and the dataset annotations can be found in
https://sites.google.com/view/bev-modnet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rashed_H/0/1/0/all/0/1"&gt;Hazem Rashed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Essam_M/0/1/0/all/0/1"&gt;Mariam Essam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_M/0/1/0/all/0/1"&gt;Maha Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sallab_A/0/1/0/all/0/1"&gt;Ahmad El Sallab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1"&gt;Senthil Yogamani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learn from Anywhere: Rethinking Generalized Zero-Shot Learning with Limited Supervision. (arXiv:2107.04952v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04952</id>
        <link href="http://arxiv.org/abs/2107.04952"/>
        <updated>2021-07-13T01:59:34.057Z</updated>
        <summary type="html"><![CDATA[A common problem with most zero and few-shot learning approaches is they
suffer from bias towards seen classes resulting in sub-optimal performance.
Existing efforts aim to utilize unlabeled images from unseen classes (i.e
transductive zero-shot) during training to enable generalization. However, this
limits their use in practical scenarios where data from target unseen classes
is unavailable or infeasible to collect. In this work, we present a practical
setting of inductive zero and few-shot learning, where unlabeled images from
other out-of-data classes, that do not belong to seen or unseen categories, can
be used to improve generalization in any-shot learning. We leverage a
formulation based on product-of-experts and introduce a new AUD module that
enables us to use unlabeled samples from out-of-data classes which are usually
easily available and practically entail no annotation cost. In addition, we
also demonstrate the applicability of our model to address a more practical and
challenging, Generalized Zero-shot under a limited supervision setting, where
even base seen classes do not have sufficient annotated samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhatt_G/0/1/0/all/0/1"&gt;Gaurav Bhatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandok_S/0/1/0/all/0/1"&gt;Shivam Chandok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1"&gt;Vineeth N Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-linear Visual Knowledge Discovery with Elliptic Paired Coordinates. (arXiv:2107.04974v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04974</id>
        <link href="http://arxiv.org/abs/2107.04974"/>
        <updated>2021-07-13T01:59:34.020Z</updated>
        <summary type="html"><![CDATA[It is challenging for humans to enable visual knowledge discovery in data
with more than 2-3 dimensions with a naked eye. This chapter explores the
efficiency of discovering predictive machine learning models interactively
using new Elliptic Paired coordinates (EPC) visualizations. It is shown that
EPC are capable to visualize multidimensional data and support visual machine
learning with preservation of multidimensional information in 2-D. Relative to
parallel and radial coordinates, EPC visualization requires only a half of the
visual elements for each n-D point. An interactive software system EllipseVis,
which is developed in this work, processes high-dimensional datasets, creates
EPC visualizations, and produces predictive classification models by
discovering dominance rules in EPC. By using interactive and automatic
processes it discovers zones in EPC with a high dominance of a single class.
The EPC methodology has been successful in discovering non-linear predictive
models with high coverage and precision in the computational experiments. This
can benefit multiple domains by producing visually appealing dominance rules.
This chapter presents results of successful testing the EPC non-linear
methodology in experiments using real and simulated data, EPC generalized to
the Dynamic Elliptic Paired Coordinates (DEPC), incorporation of the weights of
coordinates to optimize the visual discovery, introduction of an alternative
EPC design and introduction of the concept of incompact machine learning
methodology based on EPC/DEPC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McDonald_R/0/1/0/all/0/1"&gt;Rose McDonald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1"&gt;Boris Kovalerchuk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature-based Event Stereo Visual Odometry. (arXiv:2107.04921v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.04921</id>
        <link href="http://arxiv.org/abs/2107.04921"/>
        <updated>2021-07-13T01:59:34.006Z</updated>
        <summary type="html"><![CDATA[Event-based cameras are biologically inspired sensors that output events,
i.e., asynchronous pixel-wise brightness changes in the scene. Their high
dynamic range and temporal resolution of a microsecond makes them more reliable
than standard cameras in environments of challenging illumination and in
high-speed scenarios, thus developing odometry algorithms based solely on event
cameras offers exciting new possibilities for autonomous systems and robots. In
this paper, we propose a novel stereo visual odometry method for event cameras
based on feature detection and matching with careful feature management, while
pose estimation is done by reprojection error minimization. We evaluate the
performance of the proposed method on two publicly available datasets: MVSEC
sequences captured by an indoor flying drone and DSEC outdoor driving
sequences. MVSEC offers accurate ground truth from motion capture, while for
DSEC, which does not offer ground truth, in order to obtain a reference
trajectory on the standard camera frames we used our SOFT visual odometry, one
of the highest ranking algorithms on the KITTI scoreboards. We compared our
method to the ESVO method, which is the first and still the only stereo event
odometry method, showing on par performance on the MVSEC sequences, while on
the DSEC dataset ESVO, unlike our method, was unable to handle outdoor driving
scenario with default parameters. Furthermore, two important advantages of our
method over ESVO are that it adapts tracking frequency to the asynchronous
event rate and does not require initialization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hadviger_A/0/1/0/all/0/1"&gt;Antea Hadviger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cvisic_I/0/1/0/all/0/1"&gt;Igor Cvi&amp;#x161;i&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markovic_I/0/1/0/all/0/1"&gt;Ivan Markovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vrazic_S/0/1/0/all/0/1"&gt;Sacha Vra&amp;#x17e;i&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petrovic_I/0/1/0/all/0/1"&gt;Ivan Petrovi&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Geometric Distillation Network for Compressive Sensing MRI. (arXiv:2107.04943v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04943</id>
        <link href="http://arxiv.org/abs/2107.04943"/>
        <updated>2021-07-13T01:59:33.999Z</updated>
        <summary type="html"><![CDATA[Compressed sensing (CS) is an efficient method to reconstruct MR image from
small sampled data in $k$-space and accelerate the acquisition of MRI. In this
work, we propose a novel deep geometric distillation network which combines the
merits of model-based and deep learning-based CS-MRI methods, it can be
theoretically guaranteed to improve geometric texture details of a linear
reconstruction. Firstly, we unfold the model-based CS-MRI optimization problem
into two sub-problems that consist of image linear approximation and image
geometric compensation. Secondly, geometric compensation sub-problem for
distilling lost texture details in approximation stage can be expanded by
Taylor expansion to design a geometric distillation module fusing features of
different geometric characteristic domains. Additionally, we use a learnable
version with adaptive initialization of the step-length parameter, which allows
model more flexibility that can lead to convergent smoothly. Numerical
experiments verify its superiority over other state-of-the-art CS-MRI
reconstruction approaches. The source code will be available at
\url{https://github.com/fanxiaohong/Deep-Geometric-Distillation-Network-for-CS-MRI}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Fan_X/0/1/0/all/0/1"&gt;Xiaohong Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianping Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weaving Attention U-net: A Novel Hybrid CNN and Attention-based Method for Organs-at-risk Segmentation in Head and Neck CT Images. (arXiv:2107.04847v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04847</id>
        <link href="http://arxiv.org/abs/2107.04847"/>
        <updated>2021-07-13T01:59:33.993Z</updated>
        <summary type="html"><![CDATA[In radiotherapy planning, manual contouring is labor-intensive and
time-consuming. Accurate and robust automated segmentation models improve the
efficiency and treatment outcome. We aim to develop a novel hybrid deep
learning approach, combining convolutional neural networks (CNNs) and the
self-attention mechanism, for rapid and accurate multi-organ segmentation on
head and neck computed tomography (CT) images. Head and neck CT images with
manual contours of 115 patients were retrospectively collected and used. We set
the training/validation/testing ratio to 81/9/25 and used the 10-fold
cross-validation strategy to select the best model parameters. The proposed
hybrid model segmented ten organs-at-risk (OARs) altogether for each case. The
performance of the model was evaluated by three metrics, i.e., the Dice
Similarity Coefficient (DSC), Hausdorff distance 95% (HD95), and mean surface
distance (MSD). We also tested the performance of the model on the Head and
Neck 2015 challenge dataset and compared it against several state-of-the-art
automated segmentation algorithms. The proposed method generated contours that
closely resemble the ground truth for ten OARs. Our results of the new Weaving
Attention U-net demonstrate superior or similar performance on the segmentation
of head and neck CT images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhuangzhuang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tianyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gay_H/0/1/0/all/0/1"&gt;Hiram Gay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weixiong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_B/0/1/0/all/0/1"&gt;Baozhou Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Industry and Academic Research in Computer Vision. (arXiv:2107.04902v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04902</id>
        <link href="http://arxiv.org/abs/2107.04902"/>
        <updated>2021-07-13T01:59:33.987Z</updated>
        <summary type="html"><![CDATA[This work aims to study the dynamic between research in the industry and
academia in computer vision. The results are demonstrated on a set of top-5
vision conferences that are representative of the field. Since data for such
analysis was not readily available, significant effort was spent on gathering
and processing meta-data from the original publications. First, this study
quantifies the share of industry-sponsored research. Specifically, it shows
that the proportion of papers published by industry-affiliated researchers is
increasing and that more academics join companies or collaborate with them.
Next, the possible impact of industry presence is further explored, namely in
the distribution of research topics and citation patterns. The results indicate
that the distribution of the research topics is similar in industry and
academic papers. However, there is a strong preference towards citing industry
papers. Finally, possible reasons for citation bias, such as code availability
and influence, are investigated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kotseruba_I/0/1/0/all/0/1"&gt;Iuliia Kotseruba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SynPick: A Dataset for Dynamic Bin Picking Scene Understanding. (arXiv:2107.04852v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.04852</id>
        <link href="http://arxiv.org/abs/2107.04852"/>
        <updated>2021-07-13T01:59:33.969Z</updated>
        <summary type="html"><![CDATA[We present SynPick, a synthetic dataset for dynamic scene understanding in
bin-picking scenarios. In contrast to existing datasets, our dataset is both
situated in a realistic industrial application domain -- inspired by the
well-known Amazon Robotics Challenge (ARC) -- and features dynamic scenes with
authentic picking actions as chosen by our picking heuristic developed for the
ARC 2017. The dataset is compatible with the popular BOP dataset format. We
describe the dataset generation process in detail, including object arrangement
generation and manipulation simulation using the NVIDIA PhysX physics engine.
To cover a large action space, we perform untargeted and targeted picking
actions, as well as random moving actions. To establish a baseline for object
perception, a state-of-the-art pose estimation approach is evaluated on the
dataset. We demonstrate the usefulness of tracking poses during manipulation
instead of single-shot estimation even with a naive filtering approach. The
generator source code and dataset are publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Periyasamy_A/0/1/0/all/0/1"&gt;Arul Selvam Periyasamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwarz_M/0/1/0/all/0/1"&gt;Max Schwarz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1"&gt;Sven Behnke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out of Distribution Detection and Adversarial Attacks on Deep Neural Networks for Robust Medical Image Analysis. (arXiv:2107.04882v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04882</id>
        <link href="http://arxiv.org/abs/2107.04882"/>
        <updated>2021-07-13T01:59:33.962Z</updated>
        <summary type="html"><![CDATA[Deep learning models have become a popular choice for medical image analysis.
However, the poor generalization performance of deep learning models limits
them from being deployed in the real world as robustness is critical for
medical applications. For instance, the state-of-the-art Convolutional Neural
Networks (CNNs) fail to detect adversarial samples or samples drawn
statistically far away from the training distribution. In this work, we
experimentally evaluate the robustness of a Mahalanobis distance-based
confidence score, a simple yet effective method for detecting abnormal input
samples, in classifying malaria parasitized cells and uninfected cells. Results
indicated that the Mahalanobis confidence score detector exhibits improved
performance and robustness of deep learning models, and achieves
stateof-the-art performance on both out-of-distribution (OOD) and adversarial
samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Uwimana1_A/0/1/0/all/0/1"&gt;Anisie Uwimana1&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Senanayake_R/0/1/0/all/0/1"&gt;Ransalu Senanayake&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Real-Time Image Recognition Using Collaborative Swarm of UAVs and Convolutional Networks. (arXiv:2107.04648v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04648</id>
        <link href="http://arxiv.org/abs/2107.04648"/>
        <updated>2021-07-13T01:59:33.955Z</updated>
        <summary type="html"><![CDATA[Unmanned Aerial Vehicles (UAVs) have recently attracted significant attention
due to their outstanding ability to be used in different sectors and serve in
difficult and dangerous areas. Moreover, the advancements in computer vision
and artificial intelligence have increased the use of UAVs in various
applications and solutions, such as forest fires detection and borders
monitoring. However, using deep neural networks (DNNs) with UAVs introduces
several challenges of processing deeper networks and complex models, which
restricts their on-board computation. In this work, we present a strategy
aiming at distributing inference requests to a swarm of resource-constrained
UAVs that classifies captured images on-board and finds the minimum
decision-making latency. We formulate the model as an optimization problem that
minimizes the latency between acquiring images and making the final decisions.
The formulated optimization solution is an NP-hard problem. Hence it is not
adequate for online resource allocation. Therefore, we introduce an online
heuristic solution, namely DistInference, to find the layers placement strategy
that gives the best latency among the available UAVs. The proposed approach is
general enough to be used for different low decision-latency applications as
well as for all CNN types organized into the pipeline of layers (e.g., VGG) or
based on residual blocks (e.g., ResNet).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dhuheir_M/0/1/0/all/0/1"&gt;Marwan Dhuheir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baccour_E/0/1/0/all/0/1"&gt;Emna Baccour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1"&gt;Aiman Erbad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabeeh_S/0/1/0/all/0/1"&gt;Sinan Sabeeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamdi_M/0/1/0/all/0/1"&gt;Mounir Hamdi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Partial Video Domain Adaptation with Partial Adversarial Temporal Attentive Network. (arXiv:2107.04941v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04941</id>
        <link href="http://arxiv.org/abs/2107.04941"/>
        <updated>2021-07-13T01:59:33.948Z</updated>
        <summary type="html"><![CDATA[Partial Domain Adaptation (PDA) is a practical and general domain adaptation
scenario, which relaxes the fully shared label space assumption such that the
source label space subsumes the target one. The key challenge of PDA is the
issue of negative transfer caused by source-only classes. For videos, such
negative transfer could be triggered by both spatial and temporal features,
which leads to a more challenging Partial Video Domain Adaptation (PVDA)
problem. In this paper, we propose a novel Partial Adversarial Temporal
Attentive Network (PATAN) to address the PVDA problem by utilizing both spatial
and temporal features for filtering source-only classes. Besides, PATAN
constructs effective overall temporal features by attending to local temporal
features that contribute more toward the class filtration process. We further
introduce new benchmarks to facilitate research on PVDA problems, covering a
wide range of PVDA scenarios. Empirical results demonstrate the
state-of-the-art performance of our proposed PATAN across the multiple PVDA
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yuecong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jianfei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1"&gt;Haozhi Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_K/0/1/0/all/0/1"&gt;Kezhi Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhenghua Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID Detection in Chest CTs: Improving the Baseline on COV19-CT-DB. (arXiv:2107.04808v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04808</id>
        <link href="http://arxiv.org/abs/2107.04808"/>
        <updated>2021-07-13T01:59:33.940Z</updated>
        <summary type="html"><![CDATA[The paper presents a comparative analysis of three distinct approaches based
on deep learning for COVID-19 detection in chest CTs. The first approach is a
volumetric one, involving 3D convolutions, while the other two approaches
perform at first slice-wise classification and then aggregate the results at
the volume level. The experiments are carried on the COV19-CT-DB dataset, with
the aim of addressing the challenge raised by the MIA-COV19D Competition within
ICCV 2021. Our best results on the validation subset reach a macro-F1 score of
0.92, which improves considerably the baseline score of 0.70 set by the
organizers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Miron_R/0/1/0/all/0/1"&gt;Radu Miron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moisii_C/0/1/0/all/0/1"&gt;Cosmin Moisii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dinu_S/0/1/0/all/0/1"&gt;Sergiu Dinu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Breaban_M/0/1/0/all/0/1"&gt;Mihaela Breaban&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Fiber Clustering: Anatomically Informed Unsupervised Deep Learning for Fast and Effective White Matter Parcellation. (arXiv:2107.04938v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04938</id>
        <link href="http://arxiv.org/abs/2107.04938"/>
        <updated>2021-07-13T01:59:33.914Z</updated>
        <summary type="html"><![CDATA[White matter fiber clustering (WMFC) enables parcellation of white matter
tractography for applications such as disease classification and anatomical
tract segmentation. However, the lack of ground truth and the ambiguity of
fiber data (the points along a fiber can equivalently be represented in forward
or reverse order) pose challenges to this task. We propose a novel WMFC
framework based on unsupervised deep learning. We solve the unsupervised
clustering problem as a self-supervised learning task. Specifically, we use a
convolutional neural network to learn embeddings of input fibers, using
pairwise fiber distances as pseudo annotations. This enables WMFC that is
insensitive to fiber point ordering. In addition, anatomical coherence of fiber
clusters is improved by incorporating brain anatomical segmentation data. The
proposed framework enables outlier removal in a natural way by rejecting fibers
with low cluster assignment probability. We train and evaluate our method using
200 datasets from the Human Connectome Project. Results demonstrate superior
performance and efficiency of the proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuqian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chaoyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makris_N/0/1/0/all/0/1"&gt;Nikos Makris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rathi_Y/0/1/0/all/0/1"&gt;Yogesh Rathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1"&gt;Weidong Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+ODonnell_L/0/1/0/all/0/1"&gt;Lauren J. O&amp;#x27;Donnell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TeliNet, a simple and shallow Convolution Neural Network (CNN) to Classify CT Scans of COVID-19 patients. (arXiv:2107.04930v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04930</id>
        <link href="http://arxiv.org/abs/2107.04930"/>
        <updated>2021-07-13T01:59:33.905Z</updated>
        <summary type="html"><![CDATA[Hundreds of millions of cases and millions of deaths have occurred worldwide
due to COVID-19. The fight against this pandemic is on-going on multiple
fronts. While vaccinations are picking up speed, there are still billions of
unvaccinated people. In this fight diagnosis of the disease and isolation of
the patients to prevent any spreads play a huge role. Machine Learning
approaches have assisted the diagnosis of COVID-19 cases by analyzing chest
X-ray and CT-scan images of patients. In this research we present a simple and
shallow Convolutional Neural Network based approach, TeliNet, to classify
CT-scan images of COVID-19 patients. Our results outperform the F1 score of
VGGNet and the benchmark approaches. Our proposed solution is also more
lightweight in comparison to the other methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Teli_M/0/1/0/all/0/1"&gt;Mohammad Nayeem Teli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning 3D Dense Correspondence via Canonical Point Autoencoder. (arXiv:2107.04867v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04867</id>
        <link href="http://arxiv.org/abs/2107.04867"/>
        <updated>2021-07-13T01:59:33.894Z</updated>
        <summary type="html"><![CDATA[We propose a canonical point autoencoder (CPAE) that predicts dense
correspondences between 3D shapes of the same category. The autoencoder
performs two key functions: (a) encoding an arbitrarily ordered point cloud to
a canonical primitive, e.g., a sphere, and (b) decoding the primitive back to
the original input instance shape. As being placed in the bottleneck, this
primitive plays a key role to map all the unordered point clouds on the
canonical surface and to be reconstructed in an ordered fashion. Once trained,
points from different shape instances that are mapped to the same locations on
the primitive surface are determined to be a pair of correspondence. Our method
does not require any form of annotation or self-supervised part segmentation
network and can handle unaligned input point clouds. Experimental results on 3D
semantic keypoint transfer and part segmentation transfer show that our model
performs favorably against state-of-the-art correspondence learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_A/0/1/0/all/0/1"&gt;An-Chieh Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xueting Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Min Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Ming-Hsuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sifei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anatomy of Domain Shift Impact on U-Net Layers in MRI Segmentation. (arXiv:2107.04914v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04914</id>
        <link href="http://arxiv.org/abs/2107.04914"/>
        <updated>2021-07-13T01:59:33.886Z</updated>
        <summary type="html"><![CDATA[Domain Adaptation (DA) methods are widely used in medical image segmentation
tasks to tackle the problem of differently distributed train (source) and test
(target) data. We consider the supervised DA task with a limited number of
annotated samples from the target domain. It corresponds to one of the most
relevant clinical setups: building a sufficiently accurate model on the minimum
possible amount of annotated data. Existing methods mostly fine-tune specific
layers of the pretrained Convolutional Neural Network (CNN). However, there is
no consensus on which layers are better to fine-tune, e.g. the first layers for
images with low-level domain shift or the deeper layers for images with
high-level domain shift. To this end, we propose SpotTUnet - a CNN architecture
that automatically chooses the layers which should be optimally fine-tuned.
More specifically, on the target domain, our method additionally learns the
policy that indicates whether a specific layer should be fine-tuned or reused
from the pretrained network. We show that our method performs at the same level
as the best of the nonflexible fine-tuning methods even under the extreme
scarcity of annotated data. Secondly, we show that SpotTUnet policy provides a
layer-wise visualization of the domain shift impact on the network, which could
be further used to develop robust domain generalization methods. In order to
extensively evaluate SpotTUnet performance, we use a publicly available dataset
of brain MR images (CC359), characterized by explicit domain shift. We release
a reproducible experimental pipeline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zakazov_I/0/1/0/all/0/1"&gt;Ivan Zakazov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shirokikh_B/0/1/0/all/0/1"&gt;Boris Shirokikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chernyavskiy_A/0/1/0/all/0/1"&gt;Alexey Chernyavskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belyaev_M/0/1/0/all/0/1"&gt;Mikhail Belyaev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BSDA-Net: A Boundary Shape and Distance Aware Joint Learning Framework for Segmenting and Classifying OCTA Images. (arXiv:2107.04823v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04823</id>
        <link href="http://arxiv.org/abs/2107.04823"/>
        <updated>2021-07-13T01:59:33.878Z</updated>
        <summary type="html"><![CDATA[Optical coherence tomography angiography (OCTA) is a novel non-invasive
imaging technique that allows visualizations of vasculature and foveal
avascular zone (FAZ) across retinal layers. Clinical researches suggest that
the morphology and contour irregularity of FAZ are important biomarkers of
various ocular pathologies. Therefore, precise segmentation of FAZ has great
clinical interest. Also, there is no existing research reporting that FAZ
features can improve the performance of deep diagnostic classification
networks. In this paper, we propose a novel multi-level boundary shape and
distance aware joint learning framework, named BSDA-Net, for FAZ segmentation
and diagnostic classification from OCTA images. Two auxiliary branches, namely
boundary heatmap regression and signed distance map reconstruction branches,
are constructed in addition to the segmentation branch to improve the
segmentation performance, resulting in more accurate FAZ contours and fewer
outliers. Moreover, both low-level and high-level features from the
aforementioned three branches, including shape, size, boundary, and signed
directional distance map of FAZ, are fused hierarchically with features from
the diagnostic classifier. Through extensive experiments, the proposed BSDA-Net
is found to yield state-of-the-art segmentation and classification results on
the OCTA-500, OCTAGON, and FAZID datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1"&gt;Li Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhonghua Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiewei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yijin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lyu_J/0/1/0/all/0/1"&gt;Junyan Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_P/0/1/0/all/0/1"&gt;Pujin Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xiaoying Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Weakly-Supervised Depth Estimation Network Using Attention Mechanism. (arXiv:2107.04819v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04819</id>
        <link href="http://arxiv.org/abs/2107.04819"/>
        <updated>2021-07-13T01:59:33.855Z</updated>
        <summary type="html"><![CDATA[Monocular depth estimation (MDE) is a fundamental task in many applications
such as scene understanding and reconstruction. However, most of the existing
methods rely on accurately labeled datasets. A weakly-supervised framework
based on attention nested U-net (ANU) named as ANUW is introduced in this paper
for cases with wrong labels. The ANUW is trained end-to-end to convert an input
single RGB image into a depth image. It consists of a dense residual network
structure, an adaptive weight channel attention (AWCA) module, a patch second
non-local (PSNL) module and a soft label generation method. The dense residual
network is the main body of the network to encode and decode the input. The
AWCA module can adaptively adjust the channel weights to extract important
features. The PSNL module implements the spatial attention mechanism through a
second-order non-local method. The proposed soft label generation method uses
the prior knowledge of the dataset to produce soft labels to replace false
ones. The proposed ANUW is trained on a defective monocular depth dataset and
the trained model is tested on three public datasets, and the results
demonstrate the superiority of ANUW in comparison with the state-of-the-art MDE
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1"&gt;Fang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiabao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jun Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yaoxiong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shuang_F/0/1/0/all/0/1"&gt;Feng Shuang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cumulative Assessment for Urban 3D Modeling. (arXiv:2107.04622v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04622</id>
        <link href="http://arxiv.org/abs/2107.04622"/>
        <updated>2021-07-13T01:59:33.849Z</updated>
        <summary type="html"><![CDATA[Urban 3D modeling from satellite images requires accurate semantic
segmentation to delineate urban features, multiple view stereo for 3D
reconstruction of surface heights, and 3D model fitting to produce compact
models with accurate surface slopes. In this work, we present a cumulative
assessment metric that succinctly captures error contributions from each of
these components. We demonstrate our approach by providing challenging public
datasets and extending two open source projects to provide an end-to-end 3D
modeling baseline solution to stimulate further research and evaluation with a
public leaderboard.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hagstrom_S/0/1/0/all/0/1"&gt;Shea Hagstrom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pak_H/0/1/0/all/0/1"&gt;Hee Won Pak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ku_S/0/1/0/all/0/1"&gt;Stephanie Ku&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sean Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hager_G/0/1/0/all/0/1"&gt;Gregory Hager&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_M/0/1/0/all/0/1"&gt;Myron Brown&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lifelong Twin Generative Adversarial Networks. (arXiv:2107.04708v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04708</id>
        <link href="http://arxiv.org/abs/2107.04708"/>
        <updated>2021-07-13T01:59:33.843Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a new continuously learning generative model,
called the Lifelong Twin Generative Adversarial Networks (LT-GANs). LT-GANs
learns a sequence of tasks from several databases and its architecture consists
of three components: two identical generators, namely the Teacher and
Assistant, and one Discriminator. In order to allow for the LT-GANs to learn
new concepts without forgetting, we introduce a new lifelong training approach,
namely Lifelong Adversarial Knowledge Distillation (LAKD), which encourages the
Teacher and Assistant to alternately teach each other, while learning a new
database. This training approach favours transferring knowledge from a more
knowledgeable player to another player which knows less information about a
previously given task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1"&gt;Fei Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1"&gt;Adrian G. Bors&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Domain Adaptation with Polymorphic Transformers. (arXiv:2107.04805v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04805</id>
        <link href="http://arxiv.org/abs/2107.04805"/>
        <updated>2021-07-13T01:59:33.836Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) trained on one set of medical images often
experience severe performance drop on unseen test images, due to various domain
discrepancy between the training images (source domain) and the test images
(target domain), which raises a domain adaptation issue. In clinical settings,
it is difficult to collect enough annotated target domain data in a short
period. Few-shot domain adaptation, i.e., adapting a trained model with a
handful of annotations, is highly practical and useful in this case. In this
paper, we propose a Polymorphic Transformer (Polyformer), which can be
incorporated into any DNN backbones for few-shot domain adaptation.
Specifically, after the polyformer layer is inserted into a model trained on
the source domain, it extracts a set of prototype embeddings, which can be
viewed as a "basis" of the source-domain features. On the target domain, the
polyformer layer adapts by only updating a projection layer which controls the
interactions between image features and the prototype embeddings. All other
model weights (except BatchNorm parameters) are frozen during adaptation. Thus,
the chance of overfitting the annotations is greatly reduced, and the model can
perform robustly on the target domain after being trained on a few annotated
images. We demonstrate the effectiveness of Polyformer on two medical
segmentation tasks (i.e., optic disc/cup segmentation, and polyp segmentation).
The source code of Polyformer is released at
https://github.com/askerlee/segtran.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shaohua Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sui_X/0/1/0/all/0/1"&gt;Xiuchao Sui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1"&gt;Jie Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1"&gt;Huazhu Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiangde Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yangqin Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xinxing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ting_D/0/1/0/all/0/1"&gt;Daniel Ting&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goh_R/0/1/0/all/0/1"&gt;Rick Siow Mong Goh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifying Layers Susceptible to Adversarial Attacks. (arXiv:2107.04827v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04827</id>
        <link href="http://arxiv.org/abs/2107.04827"/>
        <updated>2021-07-13T01:59:33.829Z</updated>
        <summary type="html"><![CDATA[Common neural network architectures are susceptible to attack by adversarial
samples. Neural network architectures are commonly thought of as divided into
low-level feature extraction layers and high-level classification layers;
susceptibility of networks to adversarial samples is often thought of as a
problem related to classification rather than feature extraction. We test this
idea by selectively retraining different portions of VGG and ResNet
architectures on CIFAR-10, Imagenette and ImageNet using non-adversarial and
adversarial data. Our experimental results show that susceptibility to
adversarial samples is associated with low-level feature extraction layers.
Therefore, retraining high-level layers is insufficient for achieving
robustness. This phenomenon could have two explanations: either, adversarial
attacks yield outputs from early layers that are indistinguishable from
features found in the attack classes, or adversarial attacks yield outputs from
early layers that differ statistically from features for non-adversarial
samples and do not permit consistent classification by subsequent layers. We
test this question by large-scale non-linear dimensionality reduction and
density modeling on distributions of feature vectors in hidden layers and find
that the feature distributions between non-adversarial and adversarial samples
differ substantially. Our results provide new insights into the statistical
origins of adversarial samples and possible defenses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Siddiqui_S/0/1/0/all/0/1"&gt;Shoaib Ahmed Siddiqui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Breuel_T/0/1/0/all/0/1"&gt;Thomas Breuel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Convolutional Neural Networks for Seven Basic Facial Expression Classifications. (arXiv:2107.04834v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04834</id>
        <link href="http://arxiv.org/abs/2107.04834"/>
        <updated>2021-07-13T01:59:33.791Z</updated>
        <summary type="html"><![CDATA[The seven basic facial expression classifications are a basic way to express
complex human emotions and are an important part of artificial intelligence
research. Based on the traditional Bayesian neural network framework, the
ResNet-18_BNN network constructed in this paper has been improved in the
following three aspects: (1) A new objective function is proposed, which is
composed of the KL loss of uncertain parameters and the intersection of
specific parameters. Entropy loss composition. (2) Aiming at a special
objective function, a training scheme for alternately updating these two
parameters is proposed. (3) Only model the parameters of the last convolution
group. According to experimental analysis, our method achieves an accuracy of
98.28% on the evaluation set of the Aff-Wild2 database. Compared with the
traditional Bayesian Neural Network, our method brings the highest
classification accuracy gain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gong_W/0/1/0/all/0/1"&gt;Wei Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hailan Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Not End-to-End: Explore Multi-Stage Architecture for Online Surgical Phase Recognition. (arXiv:2107.04810v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04810</id>
        <link href="http://arxiv.org/abs/2107.04810"/>
        <updated>2021-07-13T01:59:33.780Z</updated>
        <summary type="html"><![CDATA[Surgical phase recognition is of particular interest to computer assisted
surgery systems, in which the goal is to predict what phase is occurring at
each frame for a surgery video. Networks with multi-stage architecture have
been widely applied in many computer vision tasks with rich patterns, where a
predictor stage first outputs initial predictions and an additional refinement
stage operates on the initial predictions to perform further refinement.
Existing works show that surgical video contents are well ordered and contain
rich temporal patterns, making the multi-stage architecture well suited for the
surgical phase recognition task. However, we observe that when simply applying
the multi-stage architecture to the surgical phase recognition task, the
end-to-end training manner will make the refinement ability fall short of its
wishes. To address the problem, we propose a new non end-to-end training
strategy and explore different designs of multi-stage architecture for surgical
phase recognition task. For the non end-to-end training strategy, the
refinement stage is trained separately with proposed two types of disturbed
sequences. Meanwhile, we evaluate three different choices of refinement models
to show that our analysis and solution are robust to the choices of specific
multi-stage models. We conduct experiments on two public benchmarks, the
M2CAI16 Workflow Challenge, and the Cholec80 dataset. Results show that
multi-stage architecture trained with our strategy largely boosts the
performance of the current state-of-the-art single-stage model. Code is
available at \url{https://github.com/ChinaYi/casual_tcn}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yi_F/0/1/0/all/0/1"&gt;Fangqiu Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1"&gt;Tingting Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consensual Collaborative Training And Knowledge Distillation Based Facial Expression Recognition Under Noisy Annotations. (arXiv:2107.04746v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04746</id>
        <link href="http://arxiv.org/abs/2107.04746"/>
        <updated>2021-07-13T01:59:33.759Z</updated>
        <summary type="html"><![CDATA[Presence of noise in the labels of large scale facial expression datasets has
been a key challenge towards Facial Expression Recognition (FER) in the wild.
During early learning stage, deep networks fit on clean data. Then, eventually,
they start overfitting on noisy labels due to their memorization ability, which
limits FER performance. This work proposes an effective training strategy in
the presence of noisy labels, called as Consensual Collaborative Training (CCT)
framework. CCT co-trains three networks jointly using a convex combination of
supervision loss and consistency loss, without making any assumption about the
noise distribution. A dynamic transition mechanism is used to move from
supervision loss in early learning to consistency loss for consensus of
predictions among networks in the later stage. Inference is done using a single
network based on a simple knowledge distillation scheme. Effectiveness of the
proposed framework is demonstrated on synthetic as well as real noisy FER
datasets. In addition, a large test subset of around 5K images is annotated
from the FEC dataset using crowd wisdom of 16 different annotators and reliable
labels are inferred. CCT is also validated on it. State-of-the-art performance
is reported on the benchmark FER datasets RAFDB (90.84%) FERPlus (89.99%) and
AffectNet (66%). Our codes are available at https://github.com/1980x/CCT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gera_D/0/1/0/all/0/1"&gt;Darshan Gera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_S/0/1/0/all/0/1"&gt;S. Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combining GCN and Transformer for Chinese Grammatical Error Detection. (arXiv:2105.09085v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09085</id>
        <link href="http://arxiv.org/abs/2105.09085"/>
        <updated>2021-07-13T01:59:33.737Z</updated>
        <summary type="html"><![CDATA[This paper describes our system at NLPTEA-2020 Task: Chinese Grammatical
Error Diagnosis (CGED). The goal of CGED is to diagnose four types of
grammatical errors: word selection (S), redundant words (R), missing words (M),
and disordered words (W). The automatic CGED system contains two parts
including error detection and error correction and our system is designed to
solve the error detection problem. Our system is built on three models: 1) a
BERT-based model leveraging syntactic information; 2) a BERT-based model
leveraging contextual embeddings; 3) a lexicon-based graph neural network
leveraging lexical information. We also design an ensemble mechanism to improve
the single model's performance. Finally, our system achieves the highest F1
scores at detection level and identification level among all teams
participating in the CGED 2020 task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinhong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Graph Learning via Population Based Self-Tuning GCN. (arXiv:2107.04713v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04713</id>
        <link href="http://arxiv.org/abs/2107.04713"/>
        <updated>2021-07-13T01:59:33.731Z</updated>
        <summary type="html"><![CDATA[Owing to the remarkable capability of extracting effective graph embeddings,
graph convolutional network (GCN) and its variants have been successfully
applied to a broad range of tasks, such as node classification, link
prediction, and graph classification. Traditional GCN models suffer from the
issues of overfitting and oversmoothing, while some recent techniques like
DropEdge could alleviate these issues and thus enable the development of deep
GCN. However, training GCN models is non-trivial, as it is sensitive to the
choice of hyperparameters such as dropout rate and learning weight decay,
especially for deep GCN models. In this paper, we aim to automate the training
of GCN models through hyperparameter optimization. To be specific, we propose a
self-tuning GCN approach with an alternate training algorithm, and further
extend our approach by incorporating the population based training scheme.
Experimental results on three benchmark datasets demonstrate the effectiveness
of our approaches on optimizing multi-layer GCN, compared with several
representative baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1"&gt;Ronghang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1"&gt;Zhiqiang Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yaliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Sheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lifelong Teacher-Student Network Learning. (arXiv:2107.04689v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04689</id>
        <link href="http://arxiv.org/abs/2107.04689"/>
        <updated>2021-07-13T01:59:33.711Z</updated>
        <summary type="html"><![CDATA[A unique cognitive capability of humans consists in their ability to acquire
new knowledge and skills from a sequence of experiences. Meanwhile, artificial
intelligence systems are good at learning only the last given task without
being able to remember the databases learnt in the past. We propose a novel
lifelong learning methodology by employing a Teacher-Student network framework.
While the Student module is trained with a new given database, the Teacher
module would remind the Student about the information learnt in the past. The
Teacher, implemented by a Generative Adversarial Network (GAN), is trained to
preserve and replay past knowledge corresponding to the probabilistic
representations of previously learn databases. Meanwhile, the Student module is
implemented by a Variational Autoencoder (VAE) which infers its latent variable
representation from both the output of the Teacher module as well as from the
newly available database. Moreover, the Student module is trained to capture
both continuous and discrete underlying data representations across different
domains. The proposed lifelong learning framework is applied in supervised,
semi-supervised and unsupervised training. The code is available~:
\url{https://github.com/dtuzi123/Lifelong-Teacher-Student-Network-Learning}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1"&gt;Fei Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1"&gt;Adrian G. Bors&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lex2vec: making Explainable Word Embeddings via Lexical Resources. (arXiv:2103.02269v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02269</id>
        <link href="http://arxiv.org/abs/2103.02269"/>
        <updated>2021-07-13T01:59:33.699Z</updated>
        <summary type="html"><![CDATA[In this technical report, we propose an algorithm, called Lex2vec that
exploits lexical resources to inject information into word embeddings and name
the embedding dimensions by means of knowledge bases. We evaluate the optimal
parameters to extract a number of informative labels that is readable and has a
good coverage for the embedding dimensions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Celli_F/0/1/0/all/0/1"&gt;Fabio Celli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Multilingual Neural Machine Translation For Low-Resource Languages: French,English - Vietnamese. (arXiv:2012.08743v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08743</id>
        <link href="http://arxiv.org/abs/2012.08743"/>
        <updated>2021-07-13T01:59:33.693Z</updated>
        <summary type="html"><![CDATA[Prior works have demonstrated that a low-resource language pair can benefit
from multilingual machine translation (MT) systems, which rely on many language
pairs' joint training. This paper proposes two simple strategies to address the
rare word issue in multilingual MT systems for two low-resource language pairs:
French-Vietnamese and English-Vietnamese. The first strategy is about dynamical
learning word similarity of tokens in the shared space among source languages
while another one attempts to augment the translation ability of rare words
through updating their embeddings during the training. Besides, we leverage
monolingual data for multilingual MT systems to increase the amount of
synthetic parallel corpora while dealing with the data sparsity problem. We
have shown significant improvements of up to +1.62 and +2.54 BLEU points over
the bilingual baseline systems for both language pairs and released our
datasets for the research community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_T/0/1/0/all/0/1"&gt;Thi-Vinh Ngo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1"&gt;Phuong-Thai Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ha_T/0/1/0/all/0/1"&gt;Thanh-Le Ha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dinh_K/0/1/0/all/0/1"&gt;Khac-Quy Dinh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1"&gt;Le-Minh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Resilience of Autonomous Vehicle Object Category Detection to Universal Adversarial Perturbations. (arXiv:2107.04749v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04749</id>
        <link href="http://arxiv.org/abs/2107.04749"/>
        <updated>2021-07-13T01:59:33.685Z</updated>
        <summary type="html"><![CDATA[Due to the vulnerability of deep neural networks to adversarial examples,
numerous works on adversarial attacks and defenses have been burgeoning over
the past several years. However, there seem to be some conventional views
regarding adversarial attacks and object detection approaches that most
researchers take for granted. In this work, we bring a fresh perspective on
those procedures by evaluating the impact of universal perturbations on object
detection at a class-level. We apply it to a carefully curated data set related
to autonomous driving. We use Faster-RCNN object detector on images of five
different categories: person, car, truck, stop sign and traffic light from the
COCO data set, while carefully perturbing the images using Universal Dense
Object Suppression algorithm. Our results indicate that person, car, traffic
light, truck and stop sign are resilient in that order (most to least) to
universal perturbations. To the best of our knowledge, this is the first time
such a ranking has been established which is significant for the security of
the data sets pertaining to autonomous vehicles and object detection in
general.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Teli_M/0/1/0/all/0/1"&gt;Mohammad Nayeem Teli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1"&gt;Seungwon Oh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaled-Time-Attention Robust Edge Network. (arXiv:2107.04688v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04688</id>
        <link href="http://arxiv.org/abs/2107.04688"/>
        <updated>2021-07-13T01:59:33.662Z</updated>
        <summary type="html"><![CDATA[This paper describes a systematic approach towards building a new family of
neural networks based on a delay-loop version of a reservoir neural network.
The resulting architecture, called Scaled-Time-Attention Robust Edge (STARE)
network, exploits hyper dimensional space and non-multiply-and-add computation
to achieve a simpler architecture, which has shallow layers, is simple to
train, and is better suited for Edge applications, such as Internet of Things
(IoT), over traditional deep neural networks. STARE incorporates new AI
concepts such as Attention and Context, and is best suited for temporal feature
extraction and classification. We demonstrate that STARE is applicable to a
variety of applications with improved performance and lower implementation
complexity. In particular, we showed a novel way of applying a dual-loop
configuration to detection and identification of drone vs bird in a counter
Unmanned Air Systems (UAS) detection application by exploiting both spatial
(video frame) and temporal (trajectory) information. We also demonstrated that
the STARE performance approaches that of a State-of-the-Art deep neural network
in classifying RF modulations, and outperforms Long Short-term Memory (LSTM) in
a special case of Mackey Glass time series prediction. To demonstrate hardware
efficiency, we designed and developed an FPGA implementation of the STARE
algorithm to demonstrate its low-power and high-throughput operations. In
addition, we illustrate an efficient structure for integrating a massively
parallel implementation of the STARE algorithm for ASIC implementation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lau_R/0/1/0/all/0/1"&gt;Richard Lau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1"&gt;Lihan Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huster_T/0/1/0/all/0/1"&gt;Todd Huster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johnson_W/0/1/0/all/0/1"&gt;William Johnson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arleth_S/0/1/0/all/0/1"&gt;Stephen Arleth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1"&gt;Justin Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ridge_D/0/1/0/all/0/1"&gt;Devin Ridge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fletcher_M/0/1/0/all/0/1"&gt;Michael Fletcher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Headley_W/0/1/0/all/0/1"&gt;William C. Headley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Learning with Multi-Head Co-Training. (arXiv:2107.04795v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04795</id>
        <link href="http://arxiv.org/abs/2107.04795"/>
        <updated>2021-07-13T01:59:33.650Z</updated>
        <summary type="html"><![CDATA[Co-training, extended from self-training, is one of the frameworks for
semi-supervised learning. It works at the cost of training extra classifiers,
where the algorithm should be delicately designed to prevent individual
classifiers from collapsing into each other. In this paper, we present a simple
and efficient co-training algorithm, named Multi-Head Co-Training, for
semi-supervised image classification. By integrating base learners into a
multi-head structure, the model is in a minimal amount of extra parameters.
Every classification head in the unified model interacts with its peers through
a "Weak and Strong Augmentation" strategy, achieving single-view co-training
without promoting diversity explicitly. The effectiveness of Multi-Head
Co-Training is demonstrated in an empirical study on standard semi-supervised
learning benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mingcai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yuntao Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1"&gt;Shuwei Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chongjun Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifying Helpful Sentences in Product Reviews. (arXiv:2104.09792v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09792</id>
        <link href="http://arxiv.org/abs/2104.09792"/>
        <updated>2021-07-13T01:59:33.642Z</updated>
        <summary type="html"><![CDATA[In recent years online shopping has gained momentum and became an important
venue for customers wishing to save time and simplify their shopping process. A
key advantage of shopping online is the ability to read what other customers
are saying about products of interest. In this work, we aim to maintain this
advantage in situations where extreme brevity is needed, for example, when
shopping by voice. We suggest a novel task of extracting a single
representative helpful sentence from a set of reviews for a given product. The
selected sentence should meet two conditions: first, it should be helpful for a
purchase decision and second, the opinion it expresses should be supported by
multiple reviewers. This task is closely related to the task of Multi Document
Summarization in the product reviews domain but differs in its objective and
its level of conciseness. We collect a dataset in English of sentence
helpfulness scores via crowd-sourcing and demonstrate its reliability despite
the inherent subjectivity involved. Next, we describe a complete model that
extracts representative helpful sentences with positive and negative sentiment
towards the product and demonstrate that it outperforms several baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gamzu_I/0/1/0/all/0/1"&gt;Iftah Gamzu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonen_H/0/1/0/all/0/1"&gt;Hila Gonen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kutiel_G/0/1/0/all/0/1"&gt;Gilad Kutiel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1"&gt;Ran Levy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agichtein_E/0/1/0/all/0/1"&gt;Eugene Agichtein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TTAN: Two-Stage Temporal Alignment Network for Few-shot Action Recognition. (arXiv:2107.04782v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04782</id>
        <link href="http://arxiv.org/abs/2107.04782"/>
        <updated>2021-07-13T01:59:33.636Z</updated>
        <summary type="html"><![CDATA[Few-shot action recognition aims to recognize novel action classes (query)
using just a few samples (support). The majority of current approaches follow
the metric learning paradigm, which learns to compare the similarity between
videos. Recently, it has been observed that directly measuring this similarity
is not ideal since different action instances may show distinctive temporal
distribution, resulting in severe misalignment issues across query and support
videos. In this paper, we arrest this problem from two distinct aspects --
action duration misalignment and motion evolution misalignment. We address them
sequentially through a Two-stage Temporal Alignment Network (TTAN). The first
stage performs temporal transformation with the predicted affine warp
parameters, while the second stage utilizes a cross-attention mechanism to
coordinate the features of the support and query to a consistent evolution.
Besides, we devise a novel multi-shot fusion strategy, which takes the
misalignment among support samples into consideration. Ablation studies and
visualizations demonstrate the role played by both stages in addressing the
misalignment. Extensive experiments on benchmark datasets show the potential of
the proposed method in achieving state-of-the-art performance for few-shot
action recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Huabin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1"&gt;Rui Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuxi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+See_J/0/1/0/all/0/1"&gt;John See&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_M/0/1/0/all/0/1"&gt;Mengjuan Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xiaoyuan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Weiyao Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lifelong Mixture of Variational Autoencoders. (arXiv:2107.04694v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04694</id>
        <link href="http://arxiv.org/abs/2107.04694"/>
        <updated>2021-07-13T01:59:33.629Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an end-to-end lifelong learning mixture of experts.
Each expert is implemented by a Variational Autoencoder (VAE). The experts in
the mixture system are jointly trained by maximizing a mixture of individual
component evidence lower bounds (MELBO) on the log-likelihood of the given
training samples. The mixing coefficients in the mixture, control the
contributions of each expert in the goal representation. These are sampled from
a Dirichlet distribution whose parameters are determined through non-parametric
estimation during lifelong learning. The model can learn new tasks fast when
these are similar to those previously learnt. The proposed Lifelong mixture of
VAE (L-MVAE) expands its architecture with new components when learning a
completely new task. After the training, our model can automatically determine
the relevant expert to be used when fed with new data samples. This mechanism
benefits both the memory efficiency and the required computational cost as only
one expert is used during the inference. The L-MVAE inference model is able to
perform interpolation in the joint latent space across the data domains
associated with different tasks and is shown to be efficient for disentangled
learning representation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1"&gt;Fei Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1"&gt;Adrian G. Bors&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Node Co-occurrence based Dual Quaternion Graph Neural Networks for Knowledge Graph Link Prediction. (arXiv:2104.07396v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07396</id>
        <link href="http://arxiv.org/abs/2104.07396"/>
        <updated>2021-07-13T01:59:33.622Z</updated>
        <summary type="html"><![CDATA[We introduce a novel embedding model, named NoGE, which aims to integrate
co-occurrence among entities and relations into graph neural networks to
improve knowledge graph completion (i.e., link prediction). Given a knowledge
graph, NoGE constructs a single graph considering entities and relations as
individual nodes. NoGE then computes weights for edges among nodes based on the
co-occurrence of entities and relations. Next, NoGE proposes Dual Quaternion
Graph Neural Networks (Dual-QGNN) and utilizes Dual-QGNN to update vector
representations for entity and relation nodes. NoGE then adopts a score
function to produce the triple scores. Comprehensive experimental results show
that NoGE obtains state-of-the-art results on three new and difficult benchmark
datasets CoDEx for knowledge graph completion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Dai Quoc Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_V/0/1/0/all/0/1"&gt;Vinh Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1"&gt;Dinh Phung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Dat Quoc Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local-to-Global Self-Attention in Vision Transformers. (arXiv:2107.04735v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04735</id>
        <link href="http://arxiv.org/abs/2107.04735"/>
        <updated>2021-07-13T01:59:33.605Z</updated>
        <summary type="html"><![CDATA[Transformers have demonstrated great potential in computer vision tasks. To
avoid dense computations of self-attentions in high-resolution visual data,
some recent Transformer models adopt a hierarchical design, where
self-attentions are only computed within local windows. This design
significantly improves the efficiency but lacks global feature reasoning in
early stages. In this work, we design a multi-path structure of the
Transformer, which enables local-to-global reasoning at multiple granularities
in each stage. The proposed framework is computationally efficient and highly
effective. With a marginal increasement in computational overhead, our model
achieves notable improvements in both image classification and semantic
segmentation. Code is available at https://github.com/ljpadam/LG-Transformer]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jinpeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yichao Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1"&gt;Shengcai Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaokang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DualVGR: A Dual-Visual Graph Reasoning Unit for Video Question Answering. (arXiv:2107.04768v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.04768</id>
        <link href="http://arxiv.org/abs/2107.04768"/>
        <updated>2021-07-13T01:59:33.598Z</updated>
        <summary type="html"><![CDATA[Video question answering is a challenging task, which requires agents to be
able to understand rich video contents and perform spatial-temporal reasoning.
However, existing graph-based methods fail to perform multi-step reasoning
well, neglecting two properties of VideoQA: (1) Even for the same video,
different questions may require different amount of video clips or objects to
infer the answer with relational reasoning; (2) During reasoning, appearance
and motion features have complicated interdependence which are correlated and
complementary to each other. Based on these observations, we propose a
Dual-Visual Graph Reasoning Unit (DualVGR) which reasons over videos in an
end-to-end fashion. The first contribution of our DualVGR is the design of an
explainable Query Punishment Module, which can filter out irrelevant visual
features through multiple cycles of reasoning. The second contribution is the
proposed Video-based Multi-view Graph Attention Network, which captures the
relations between appearance and motion features. Our DualVGR network achieves
state-of-the-art performance on the benchmark MSVD-QA and SVQA datasets, and
demonstrates competitive results on benchmark MSRVTT-QA datasets. Our code is
available at https://github.com/MMIR/DualVGR-VideoQA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_B/0/1/0/all/0/1"&gt;Bing-Kun Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Changsheng Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[InfoVAEGAN : learning joint interpretable representations by information maximization and maximum likelihood. (arXiv:2107.04705v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04705</id>
        <link href="http://arxiv.org/abs/2107.04705"/>
        <updated>2021-07-13T01:59:33.590Z</updated>
        <summary type="html"><![CDATA[Learning disentangled and interpretable representations is an important step
towards accomplishing comprehensive data representations on the manifold. In
this paper, we propose a novel representation learning algorithm which combines
the inference abilities of Variational Autoencoders (VAE) with the
generalization capability of Generative Adversarial Networks (GAN). The
proposed model, called InfoVAEGAN, consists of three networks~: Encoder,
Generator and Discriminator. InfoVAEGAN aims to jointly learn discrete and
continuous interpretable representations in an unsupervised manner by using two
different data-free log-likelihood functions onto the variables sampled from
the generator's distribution. We propose a two-stage algorithm for optimizing
the inference network separately from the generator training. Moreover, we
enforce the learning of interpretable representations through the maximization
of the mutual information between the existing latent variables and those
created through generative and inference processes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1"&gt;Fei Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1"&gt;Adrian G. Bors&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer. (arXiv:2102.09550v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09550</id>
        <link href="http://arxiv.org/abs/2102.09550"/>
        <updated>2021-07-13T01:59:33.584Z</updated>
        <summary type="html"><![CDATA[We address the challenging problem of Natural Language Comprehension beyond
plain-text documents by introducing the TILT neural network architecture which
simultaneously learns layout information, visual features, and textual
semantics. Contrary to previous approaches, we rely on a decoder capable of
unifying a variety of problems involving natural language. The layout is
represented as an attention bias and complemented with contextualized visual
information, while the core of our model is a pretrained encoder-decoder
Transformer. Our novel approach achieves state-of-the-art results in extracting
information from documents and answering questions which demand layout
understanding (DocVQA, CORD, SROIE). At the same time, we simplify the process
by employing an end-to-end model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Powalski_R/0/1/0/all/0/1"&gt;Rafa&amp;#x142; Powalski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borchmann_L/0/1/0/all/0/1"&gt;&amp;#x141;ukasz Borchmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jurkiewicz_D/0/1/0/all/0/1"&gt;Dawid Jurkiewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dwojak_T/0/1/0/all/0/1"&gt;Tomasz Dwojak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pietruszka_M/0/1/0/all/0/1"&gt;Micha&amp;#x142; Pietruszka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palka_G/0/1/0/all/0/1"&gt;Gabriela Pa&amp;#x142;ka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anomaly Detection in Residential Video Surveillance on Edge Devices in IoT Framework. (arXiv:2107.04767v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04767</id>
        <link href="http://arxiv.org/abs/2107.04767"/>
        <updated>2021-07-13T01:59:33.576Z</updated>
        <summary type="html"><![CDATA[Intelligent resident surveillance is one of the most essential smart
community services. The increasing demand for security needs surveillance
systems to be able to detect anomalies in surveillance scenes. Employing
high-capacity computational devices for intelligent surveillance in residential
societies is costly and not feasible. Therefore, we propose anomaly detection
for intelligent surveillance using CPU-only edge devices. A modular framework
to capture object-level inferences and tracking is developed. To cope with
partial occlusions, posture deformations, and complex scenes we employed
feature encoding and trajectory associations. Elements of the anomaly detection
framework are optimized to run on CPU-only edge devices with sufficient FPS.
The experimental results indicate the proposed method is feasible and achieves
satisfactory results in real-life scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parate_M/0/1/0/all/0/1"&gt;Mayur R. Parate&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhurchandi_K/0/1/0/all/0/1"&gt;Kishor M. Bhurchandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kothari_A/0/1/0/all/0/1"&gt;Ashwin G. Kothari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Topological-Framework to Improve Analysis of Machine Learning Model Performance. (arXiv:2107.04714v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04714</id>
        <link href="http://arxiv.org/abs/2107.04714"/>
        <updated>2021-07-13T01:59:33.558Z</updated>
        <summary type="html"><![CDATA[As both machine learning models and the datasets on which they are evaluated
have grown in size and complexity, the practice of using a few summary
statistics to understand model performance has become increasingly problematic.
This is particularly true in real-world scenarios where understanding model
failure on certain subpopulations of the data is of critical importance. In
this paper we propose a topological framework for evaluating machine learning
models in which a dataset is treated as a "space" on which a model operates.
This provides us with a principled way to organize information about model
performance at both the global level (over the entire test set) and also the
local level (on specific subpopulations). Finally, we describe a topological
data structure, presheaves, which offer a convenient way to store and analyze
model performance between different subpopulations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1"&gt;Henry Kvinge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wight_C/0/1/0/all/0/1"&gt;Colby Wight&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akers_S/0/1/0/all/0/1"&gt;Sarah Akers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Howland_S/0/1/0/all/0/1"&gt;Scott Howland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_W/0/1/0/all/0/1"&gt;Woongjo Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xiaolong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gosink_L/0/1/0/all/0/1"&gt;Luke Gosink&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jurrus_E/0/1/0/all/0/1"&gt;Elizabeth Jurrus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kappagantula_K/0/1/0/all/0/1"&gt;Keerti Kappagantula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Emerson_T/0/1/0/all/0/1"&gt;Tegan H. Emerson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DDCNet: Deep Dilated Convolutional Neural Network for Dense Prediction. (arXiv:2107.04715v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04715</id>
        <link href="http://arxiv.org/abs/2107.04715"/>
        <updated>2021-07-13T01:59:33.551Z</updated>
        <summary type="html"><![CDATA[Dense pixel matching problems such as optical flow and disparity estimation
are among the most challenging tasks in computer vision. Recently, several deep
learning methods designed for these problems have been successful. A
sufficiently larger effective receptive field (ERF) and a higher resolution of
spatial features within a network are essential for providing higher-resolution
dense estimates. In this work, we present a systemic approach to design network
architectures that can provide a larger receptive field while maintaining a
higher spatial feature resolution. To achieve a larger ERF, we utilized dilated
convolutional layers. By aggressively increasing dilation rates in the deeper
layers, we were able to achieve a sufficiently larger ERF with a significantly
fewer number of trainable parameters. We used optical flow estimation problem
as the primary benchmark to illustrate our network design strategy. The
benchmark results (Sintel, KITTI, and Middlebury) indicate that our compact
networks can achieve comparable performance in the class of lightweight
networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Salehi_A/0/1/0/all/0/1"&gt;Ali Salehi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_M/0/1/0/all/0/1"&gt;Madhusudhanan Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Generative Adversarial Network for Depth Estimation in Laparoscopic Images. (arXiv:2107.04644v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04644</id>
        <link href="http://arxiv.org/abs/2107.04644"/>
        <updated>2021-07-13T01:59:33.541Z</updated>
        <summary type="html"><![CDATA[Dense depth estimation and 3D reconstruction of a surgical scene are crucial
steps in computer assisted surgery. Recent work has shown that depth estimation
from a stereo images pair could be solved with convolutional neural networks.
However, most recent depth estimation models were trained on datasets with
per-pixel ground truth. Such data is especially rare for laparoscopic imaging,
making it hard to apply supervised depth estimation to real surgical
applications. To overcome this limitation, we propose SADepth, a new
self-supervised depth estimation method based on Generative Adversarial
Networks. It consists of an encoder-decoder generator and a discriminator to
incorporate geometry constraints during training. Multi-scale outputs from the
generator help to solve the local minima caused by the photometric reprojection
loss, while the adversarial learning improves the framework generation quality.
Extensive experiments on two public datasets show that SADepth outperforms
recent state-of-the-art unsupervised methods by a large margin, and reduces the
gap between supervised and unsupervised depth estimation in laparoscopic
images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1"&gt;Baoru Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1"&gt;Jianqing Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;Anh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuch_D/0/1/0/all/0/1"&gt;David Tuch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vyas_K/0/1/0/all/0/1"&gt;Kunal Vyas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giannarou_S/0/1/0/all/0/1"&gt;Stamatia Giannarou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elson_D/0/1/0/all/0/1"&gt;Daniel S. Elson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[U-Net with Hierarchical Bottleneck Attention for Landmark Detection in Fundus Images of the Degenerated Retina. (arXiv:2107.04721v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04721</id>
        <link href="http://arxiv.org/abs/2107.04721"/>
        <updated>2021-07-13T01:59:33.534Z</updated>
        <summary type="html"><![CDATA[Fundus photography has routinely been used to document the presence and
severity of retinal degenerative diseases such as age-related macular
degeneration (AMD), glaucoma, and diabetic retinopathy (DR) in clinical
practice, for which the fovea and optic disc (OD) are important retinal
landmarks. However, the occurrence of lesions, drusen, and other retinal
abnormalities during retinal degeneration severely complicates automatic
landmark detection and segmentation. Here we propose HBA-U-Net: a U-Net
backbone enriched with hierarchical bottleneck attention. The network consists
of a novel bottleneck attention block that combines and refines self-attention,
channel attention, and relative-position attention to highlight retinal
abnormalities that may be important for fovea and OD segmentation in the
degenerated retina. HBA-U-Net achieved state-of-the-art results on fovea
detection across datasets and eye conditions (ADAM: Euclidean Distance (ED) of
25.4 pixels, REFUGE: 32.5 pixels, IDRiD: 32.1 pixels), on OD segmentation for
AMD (ADAM: Dice Coefficient (DC) of 0.947), and on OD detection for DR (IDRiD:
ED of 20.5 pixels). Our results suggest that HBA-U-Net may be well suited for
landmark detection in the presence of a variety of retinal degenerative
diseases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tang_S/0/1/0/all/0/1"&gt;Shuyun Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qi_Z/0/1/0/all/0/1"&gt;Ziming Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Granley_J/0/1/0/all/0/1"&gt;Jacob Granley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Beyeler_M/0/1/0/all/0/1"&gt;Michael Beyeler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Streaming End-to-End Framework For Spoken Language Understanding. (arXiv:2105.10042v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10042</id>
        <link href="http://arxiv.org/abs/2105.10042"/>
        <updated>2021-07-13T01:59:33.511Z</updated>
        <summary type="html"><![CDATA[End-to-end spoken language understanding (SLU) has recently attracted
increasing interest. Compared to the conventional tandem-based approach that
combines speech recognition and language understanding as separate modules, the
new approach extracts users' intentions directly from the speech signals,
resulting in joint optimization and low latency. Such an approach, however, is
typically designed to process one intention at a time, which leads users to
take multiple rounds to fulfill their requirements while interacting with a
dialogue system. In this paper, we propose a streaming end-to-end framework
that can process multiple intentions in an online and incremental way. The
backbone of our framework is a unidirectional RNN trained with the
connectionist temporal classification (CTC) criterion. By this design, an
intention can be identified when sufficient evidence has been accumulated, and
multiple intentions can be identified sequentially. We evaluate our solution on
the Fluent Speech Commands (FSC) dataset and the intent detection accuracy is
about 97 % on all multi-intent settings. This result is comparable to the
performance of the state-of-the-art non-streaming models, but is achieved in an
online and incremental way. We also employ our model to a keyword spotting task
using the Google Speech Commands dataset and the results are also highly
promising.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Potdar_N/0/1/0/all/0/1"&gt;Nihal Potdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avila_A/0/1/0/all/0/1"&gt;Anderson R. Avila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1"&gt;Chao Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yiran Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiao Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diverse Video Generation using a Gaussian Process Trigger. (arXiv:2107.04619v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04619</id>
        <link href="http://arxiv.org/abs/2107.04619"/>
        <updated>2021-07-13T01:59:33.493Z</updated>
        <summary type="html"><![CDATA[Generating future frames given a few context (or past) frames is a
challenging task. It requires modeling the temporal coherence of videos and
multi-modality in terms of diversity in the potential future states. Current
variational approaches for video generation tend to marginalize over
multi-modal future outcomes. Instead, we propose to explicitly model the
multi-modality in the future outcomes and leverage it to sample diverse
futures. Our approach, Diverse Video Generator, uses a Gaussian Process (GP) to
learn priors on future states given the past and maintains a probability
distribution over possible futures given a particular sample. In addition, we
leverage the changes in this distribution over time to control the sampling of
diverse future states by estimating the end of ongoing sequences. That is, we
use the variance of GP over the output function space to trigger a change in an
action sequence. We achieve state-of-the-art results on diverse future frame
generation in terms of reconstruction quality and diversity of the generated
sequences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_G/0/1/0/all/0/1"&gt;Gaurav Shrivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Abhinav Shrivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparative analysis of word embeddings in assessing semantic similarity of complex sentences. (arXiv:2010.12637v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12637</id>
        <link href="http://arxiv.org/abs/2010.12637"/>
        <updated>2021-07-13T01:59:33.486Z</updated>
        <summary type="html"><![CDATA[Semantic textual similarity is one of the open research challenges in the
field of Natural Language Processing. Extensive research has been carried out
in this field and near-perfect results are achieved by recent transformer-based
models in existing benchmark datasets like the STS dataset and the SICK
dataset. In this paper, we study the sentences in these datasets and analyze
the sensitivity of various word embeddings with respect to the complexity of
the sentences. We build a complex sentences dataset comprising of 50 sentence
pairs with associated semantic similarity values provided by 15 human
annotators. Readability analysis is performed to highlight the increase in
complexity of the sentences in the existing benchmark datasets and those in the
proposed dataset. Further, we perform a comparative analysis of the performance
of various word embeddings and language models on the existing benchmark
datasets and the proposed dataset. The results show the increase in complexity
of the sentences has a significant impact on the performance of the embedding
models resulting in a 10-20% decrease in Pearson's and Spearman's correlation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chandrasekaran_D/0/1/0/all/0/1"&gt;Dhivya Chandrasekaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mago_V/0/1/0/all/0/1"&gt;Vijay Mago&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Triangulation Method is Not Really Optimal. (arXiv:2107.04618v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04618</id>
        <link href="http://arxiv.org/abs/2107.04618"/>
        <updated>2021-07-13T01:59:33.475Z</updated>
        <summary type="html"><![CDATA[Triangulation refers to the problem of finding a 3D point from its 2D
projections on multiple camera images. For solving this problem, it is the
common practice to use so-called optimal triangulation method, which we call
the L2 method in this paper. But, the method can be optimal only if we assume
no uncertainty in the camera parameters. Through extensive comparison on
synthetic and real data, we observed that the L2 method is actually not the
best choice when there is uncertainty in the camera parameters. Interestingly,
it can be observed that the simple mid-point method outperforms other methods.
Apart from its high performance, the mid-point method has a simple closed
formed solution for multiple camera images while the L2 method is hard to be
used for more than two camera images. Therefore, in contrast to the common
practice, we argue that the simple mid-point method should be used in
structure-from-motion applications where there is uncertainty in camera
parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nasiri_S/0/1/0/all/0/1"&gt;Seyed-Mahdi Nasiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hosseini_R/0/1/0/all/0/1"&gt;Reshad Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moradi_H/0/1/0/all/0/1"&gt;Hadi Moradi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Transformer Growth for Progressive BERT Training. (arXiv:2010.12562v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12562</id>
        <link href="http://arxiv.org/abs/2010.12562"/>
        <updated>2021-07-13T01:59:33.466Z</updated>
        <summary type="html"><![CDATA[Due to the excessive cost of large-scale language model pre-training,
considerable efforts have been made to train BERT progressively -- start from
an inferior but low-cost model and gradually grow the model to increase the
computational complexity. Our objective is to advance the understanding of
Transformer growth and discover principles that guide progressive training.
First, we find that similar to network architecture search, Transformer growth
also favors compound scaling. Specifically, while existing methods only conduct
network growth in a single dimension, we observe that it is beneficial to use
compound growth operators and balance multiple dimensions (e.g., depth, width,
and input length of the model). Moreover, we explore alternative growth
operators in each dimension via controlled comparison to give operator
selection practical guidance. In light of our analyses, the proposed method
speeds up BERT pre-training by 73.6% and 82.2% for the base and large models
respectively, while achieving comparable performances]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1"&gt;Xiaotao Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Liyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hongkun Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiawei Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Noisy Self-Reports to Predict Twitter User Demographics. (arXiv:2005.00635v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00635</id>
        <link href="http://arxiv.org/abs/2005.00635"/>
        <updated>2021-07-13T01:59:33.459Z</updated>
        <summary type="html"><![CDATA[Computational social science studies often contextualize content analysis
within standard demographics. Since demographics are unavailable on many social
media platforms (e.g. Twitter) numerous studies have inferred demographics
automatically. Despite many studies presenting proof of concept inference of
race and ethnicity, training of practical systems remains elusive since there
are few annotated datasets. Existing datasets are small, inaccurate, or fail to
cover the four most common racial and ethnic groups in the United States. We
present a method to identify self-reports of race and ethnicity from Twitter
profile descriptions. Despite errors inherent in automated supervision, we
produce models with good performance when measured on gold standard self-report
survey data. The result is a reproducible method for creating large-scale
training resources for race and ethnicity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wood_Doughty_Z/0/1/0/all/0/1"&gt;Zach Wood-Doughty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1"&gt;Paiheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dredze_M/0/1/0/all/0/1"&gt;Mark Dredze&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Rich Transcription-Style Automatic Speech Recognition with Semi-Supervised Learning. (arXiv:2107.05382v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05382</id>
        <link href="http://arxiv.org/abs/2107.05382"/>
        <updated>2021-07-13T01:59:33.442Z</updated>
        <summary type="html"><![CDATA[We propose a semi-supervised learning method for building end-to-end rich
transcription-style automatic speech recognition (RT-ASR) systems from
small-scale rich transcription-style and large-scale common transcription-style
datasets. In spontaneous speech tasks, various speech phenomena such as
fillers, word fragments, laughter and coughs, etc. are often included. While
common transcriptions do not give special awareness to these phenomena, rich
transcriptions explicitly convert them into special phenomenon tokens as well
as textual tokens. In previous studies, the textual and phenomenon tokens were
simultaneously estimated in an end-to-end manner. However, it is difficult to
build accurate RT-ASR systems because large-scale rich transcription-style
datasets are often unavailable. To solve this problem, our training method uses
a limited rich transcription-style dataset and common transcription-style
dataset simultaneously. The Key process in our semi-supervised learning is to
convert the common transcription-style dataset into a pseudo-rich
transcription-style dataset. To this end, we introduce style tokens which
control phenomenon tokens are generated or not into transformer-based
autoregressive modeling. We use this modeling for generating the pseudo-rich
transcription-style datasets and for building RT-ASR system from the pseudo and
original datasets. Our experiments on spontaneous ASR tasks showed the
effectiveness of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1"&gt;Tomohiro Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masumura_R/0/1/0/all/0/1"&gt;Ryo Masumura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ihori_M/0/1/0/all/0/1"&gt;Mana Ihori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takashima_A/0/1/0/all/0/1"&gt;Akihiko Takashima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orihashi_S/0/1/0/all/0/1"&gt;Shota Orihashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makishima_N/0/1/0/all/0/1"&gt;Naoki Makishima&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can Evolutionary Computation Help us to Crib the Voynich Manuscript ?. (arXiv:2107.05381v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05381</id>
        <link href="http://arxiv.org/abs/2107.05381"/>
        <updated>2021-07-13T01:59:33.435Z</updated>
        <summary type="html"><![CDATA[Departing from the postulate that Voynich Manuscript is not a hoax but rather
encodes authentic contents, our article presents an evolutionary algorithm
which aims to find the most optimal mapping between voynichian glyphs and
candidate phonemic values. Core component of the decoding algorithm is a
process of maximization of a fitness function which aims to find most optimal
set of substitution rules allowing to transcribe the part of the manuscript --
which we call the Calendar -- into lists of feminine names. This leads to sets
of character subsitution rules which allow us to consistently transcribe dozens
among three hundred calendar tokens into feminine names: a result far
surpassing both ``popular'' as well as "state of the art" tentatives to crack
the manuscript. What's more, by using name lists stemming from different
languages as potential cribs, our ``adaptive'' method can also be useful in
identification of the language in which the manuscript is written.

As far as we can currently tell, results of our experiments indicate that the
Calendar part of the manuscript contains names from baltoslavic, balkanic or
hebrew language strata. Two further indications are also given: primo, highest
fitness values were obtained when the crib list contains names with specific
infixes at token's penultimate position as is the case, for example, for slavic
\textbf{feminine diminutives} (i.e. names ending with -ka and not -a). In the
most successful scenario, 240 characters contained in 35 distinct Voynichese
tokens were successfully transcribed. Secundo, in case of crib stemming from
Hebrew language, whole adaptation process converges to significantly better
fitness values when transcribing voynichian tokens whose order of individual
characters have been reversed, and when lists feminine and not masculine names
are used as the crib.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hromada_D/0/1/0/all/0/1"&gt;Daniel Devatman Hromada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DISCO : efficient unsupervised decoding for discrete natural language problems via convex relaxation. (arXiv:2107.05380v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05380</id>
        <link href="http://arxiv.org/abs/2107.05380"/>
        <updated>2021-07-13T01:59:33.426Z</updated>
        <summary type="html"><![CDATA[In this paper we study test time decoding; an ubiquitous step in almost all
sequential text generation task spanning across a wide array of natural
language processing (NLP) problems. Our main contribution is to develop a
continuous relaxation framework for the combinatorial NP-hard decoding problem
and propose Disco - an efficient algorithm based on standard first order
gradient based. We provide tight analysis and show that our proposed algorithm
linearly converges to within $\epsilon$ neighborhood of the optima. Finally, we
perform preliminary experiments on the task of adversarial text generation and
show superior performance of Disco over several popular decoding approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1"&gt;Anish Acharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1"&gt;Rudrajit Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1"&gt;Greg Durrett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1"&gt;Inderjit Dhillon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanghavi_S/0/1/0/all/0/1"&gt;Sujay Sanghavi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Visual Models using a Knowledge Graph as a Trainer. (arXiv:2102.08747v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08747</id>
        <link href="http://arxiv.org/abs/2102.08747"/>
        <updated>2021-07-13T01:59:33.414Z</updated>
        <summary type="html"><![CDATA[Traditional computer vision approaches, based on neural networks (NN), are
typically trained on a large amount of image data. By minimizing the
cross-entropy loss between a prediction and a given class label, the NN and its
visual embedding space are learned to fulfill a given task. However, due to the
sole dependence on the image data distribution of the training domain, these
models tend to fail when applied to a target domain that differs from their
source domain. To learn a more robust NN to domain shifts, we propose the
knowledge graph neural network (KG-NN), a neuro-symbolic approach that
supervises the training using image-data-invariant auxiliary knowledge. The
auxiliary knowledge is first encoded in a knowledge graph with respective
concepts and their relationships, which is then transformed into a dense vector
representation via an embedding method. Using a contrastive loss function,
KG-NN learns to adapt its visual embedding space and thus its weights according
to the image-data invariant knowledge graph embedding space. We evaluate KG-NN
on visual transfer learning tasks for classification using the mini-ImageNet
dataset and its derivatives, as well as road sign recognition datasets from
Germany and China. The results show that a visual model trained with a
knowledge graph as a trainer outperforms a model trained with cross-entropy in
all experiments, in particular when the domain gap increases. Besides better
performance and stronger robustness to domain shifts, these KG-NN adapts to
multiple datasets and classes without suffering heavily from catastrophic
forgetting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Monka_S/0/1/0/all/0/1"&gt;Sebastian Monka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Halilaj_L/0/1/0/all/0/1"&gt;Lavdim Halilaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmid_S/0/1/0/all/0/1"&gt;Stefan Schmid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rettinger_A/0/1/0/all/0/1"&gt;Achim Rettinger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Social Media Information Sharing for Natural Disaster Response. (arXiv:2005.07019v5 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.07019</id>
        <link href="http://arxiv.org/abs/2005.07019"/>
        <updated>2021-07-13T01:59:33.407Z</updated>
        <summary type="html"><![CDATA[Social media has become an essential channel for posting disaster-related
information, which provide governments and relief agencies real-time data for
better disaster management. However, research in this field has not received
sufficient attention and extracting useful information is still challenging.
This paper aims to improve disaster relief efficiency via mining and analyzing
social media data like public attitudes towards disaster response and public
demands for targeted relief supplies during different types of disasters. We
focus on different natural disasters based on properties such as types,
durations, and damages, which contains a total of 41,993 tweets. In this paper,
public perception is assessed qualitatively by manually classified tweets,
which contain information like the demand for targeted relief supplies,
satisfactions of disaster response, and public fear. Public attitudes to
natural disasters are studied via a quantitative analysis using eight machine
learning models. To better provide decision-makers with the appropriate model,
the comparison of machine learning models based on computational time and
prediction accuracy is conducted. The change of public opinion during different
natural disasters and the evolution of people's behavior of using social media
for disaster relief in the face of the identical type of natural disasters as
Twitter continues to evolve are studied. The results in this paper demonstrate
the feasibility and validation of the proposed research approach and provide
relief agencies with insights into better disaster management.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1"&gt;Zhijie Sasha Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1"&gt;Lingyu Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Christenson_L/0/1/0/all/0/1"&gt;Lauren Christenson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fulton_L/0/1/0/all/0/1"&gt;Lawrence Fulton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Oriental Language Recognition (OLR) 2020: Summary and Analysis. (arXiv:2107.05365v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.05365</id>
        <link href="http://arxiv.org/abs/2107.05365"/>
        <updated>2021-07-13T01:59:33.383Z</updated>
        <summary type="html"><![CDATA[The fifth Oriental Language Recognition (OLR) Challenge focuses on language
recognition in a variety of complex environments to promote its development.
The OLR 2020 Challenge includes three tasks: (1) cross-channel language
identification, (2) dialect identification, and (3) noisy language
identification. We choose Cavg as the principle evaluation metric, and the
Equal Error Rate (EER) as the secondary metric. There were 58 teams
participating in this challenge and one third of the teams submitted valid
results. Compared with the best baseline, the Cavg values of Top 1 system for
the three tasks were relatively reduced by 82%, 62% and 48%, respectively. This
paper describes the three tasks, the database profile, and the final results.
We also outline the novel approaches that improve the performance of language
recognition systems most significantly, such as the utilization of auxiliary
information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Binling Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhi_Y/0/1/0/all/0/1"&gt;Yiming Zhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_Q/0/1/0/all/0/1"&gt;Qingyang Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StructFormer: Joint Unsupervised Induction of Dependency and Constituency Structure from Masked Language Modeling. (arXiv:2012.00857v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00857</id>
        <link href="http://arxiv.org/abs/2012.00857"/>
        <updated>2021-07-13T01:59:33.377Z</updated>
        <summary type="html"><![CDATA[There are two major classes of natural language grammar -- the dependency
grammar that models one-to-one correspondences between words and the
constituency grammar that models the assembly of one or several corresponded
words. While previous unsupervised parsing methods mostly focus on only
inducing one class of grammars, we introduce a novel model, StructFormer, that
can simultaneously induce dependency and constituency structure. To achieve
this, we propose a new parsing framework that can jointly generate a
constituency tree and dependency graph. Then we integrate the induced
dependency relations into the transformer, in a differentiable manner, through
a novel dependency-constrained self-attention mechanism. Experimental results
show that our model can achieve strong results on unsupervised constituency
parsing, unsupervised dependency parsing, and masked language modeling at the
same time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yikang Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1"&gt;Yi Tay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Che Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1"&gt;Dara Bahri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1"&gt;Donald Metzler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1"&gt;Aaron Courville&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DaCy: A Unified Framework for Danish NLP. (arXiv:2107.05295v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05295</id>
        <link href="http://arxiv.org/abs/2107.05295"/>
        <updated>2021-07-13T01:59:33.354Z</updated>
        <summary type="html"><![CDATA[Danish natural language processing (NLP) has in recent years obtained
considerable improvements with the addition of multiple new datasets and
models. However, at present, there is no coherent framework for applying
state-of-the-art models for Danish. We present DaCy: a unified framework for
Danish NLP built on SpaCy. DaCy uses efficient multitask models which obtain
state-of-the-art performance on named entity recognition, part-of-speech
tagging, and dependency parsing. DaCy contains tools for easy integration of
existing models such as for polarity, emotion, or subjectivity detection. In
addition, we conduct a series of tests for biases and robustness of Danish NLP
pipelines through augmentation of the test set of DaNE. DaCy large compares
favorably and is especially robust to long input lengths and spelling
variations and errors. All models except DaCy large display significant biases
related to ethnicity while only Polyglot shows a significant gender bias. We
argue that for languages with limited benchmark sets, data augmentation can be
particularly useful for obtaining more realistic and fine-grained performance
estimates. We provide a series of augmenters as a first step towards a more
thorough evaluation of language models for low and medium resource languages
and encourage further development.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Enevoldsen_K/0/1/0/all/0/1"&gt;Kenneth Enevoldsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hansen_L/0/1/0/all/0/1"&gt;Lasse Hansen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nielbo_K/0/1/0/all/0/1"&gt;Kristoffer Nielbo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Flexible Multi-Task Model for BERT Serving. (arXiv:2107.05377v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05377</id>
        <link href="http://arxiv.org/abs/2107.05377"/>
        <updated>2021-07-13T01:59:33.347Z</updated>
        <summary type="html"><![CDATA[In this demonstration, we present an efficient BERT-based multi-task (MT)
framework that is particularly suitable for iterative and incremental
development of the tasks. The proposed framework is based on the idea of
partial fine-tuning, i.e. only fine-tune some top layers of BERT while keep the
other layers frozen. For each task, we train independently a single-task (ST)
model using partial fine-tuning. Then we compress the task-specific layers in
each ST model using knowledge distillation. Those compressed ST models are
finally merged into one MT model so that the frozen layers of the former are
shared across the tasks. We exemplify our approach on eight GLUE tasks,
demonstrating that it is able to achieve both strong performance and
efficiency. We have implemented our method in the utterance understanding
system of XiaoAI, a commercial AI assistant developed by Xiaomi. We estimate
that our model reduces the overall serving cost by 86%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1"&gt;Tianwen Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1"&gt;Jianwei Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shenghuan He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CatVRNN: Generating Category Texts via Multi-task Learning. (arXiv:2107.05219v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05219</id>
        <link href="http://arxiv.org/abs/2107.05219"/>
        <updated>2021-07-13T01:59:33.333Z</updated>
        <summary type="html"><![CDATA[Controlling the model to generate texts of different categories is a
challenging task that is getting more and more attention. Recently, generative
adversarial net (GAN) has shown promising results in category text generation.
However, the texts generated by GANs usually suffer from the problems of mode
collapse and training instability. To avoid the above problems, we propose a
novel model named category-aware variational recurrent neural network
(CatVRNN), which is inspired by multi-task learning. In our model, generation
and classification are trained simultaneously, aiming at generating texts of
different categories. Moreover, the use of multi-task learning can improve the
quality of generated texts, when the classification task is appropriate. And we
propose a function to initialize the hidden state of CatVRNN to force model to
generate texts of a specific category. Experimental results on three datasets
demonstrate that our model can do better than several state-of-the-art text
generation methods based GAN in the category accuracy and quality of generated
texts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1"&gt;Pengsen Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiayong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1"&gt;Jinqiao Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hate versus Politics: Detection of Hate against Policy makers in Italian tweets. (arXiv:2107.05357v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05357</id>
        <link href="http://arxiv.org/abs/2107.05357"/>
        <updated>2021-07-13T01:59:33.314Z</updated>
        <summary type="html"><![CDATA[Accurate detection of hate speech against politicians, policy making and
political ideas is crucial to maintain democracy and free speech.
Unfortunately, the amount of labelled data necessary for training models to
detect hate speech are limited and domain-dependent. In this paper, we address
the issue of classification of hate speech against policy makers from Twitter
in Italian, producing the first resource of this type in this language. We
collected and annotated 1264 tweets, examined the cases of disagreements
between annotators, and performed in-domain and cross-domain hate speech
classifications with different features and algorithms. We achieved a
performance of ROC AUC 0.83 and analyzed the most predictive attributes, also
finding the different language features in the anti-policymakers and
anti-immigration domains. Finally, we visualized networks of hashtags to
capture the topics used in hateful and normal tweets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duzha_A/0/1/0/all/0/1"&gt;Armend Duzha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casadei_C/0/1/0/all/0/1"&gt;Cristiano Casadei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tosi_M/0/1/0/all/0/1"&gt;Michael Tosi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Celli_F/0/1/0/all/0/1"&gt;Fabio Celli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Putting words into the system's mouth: A targeted attack on neural machine translation using monolingual data poisoning. (arXiv:2107.05243v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05243</id>
        <link href="http://arxiv.org/abs/2107.05243"/>
        <updated>2021-07-13T01:59:33.307Z</updated>
        <summary type="html"><![CDATA[Neural machine translation systems are known to be vulnerable to adversarial
test inputs, however, as we show in this paper, these systems are also
vulnerable to training attacks. Specifically, we propose a poisoning attack in
which a malicious adversary inserts a small poisoned sample of monolingual text
into the training set of a system trained using back-translation. This sample
is designed to induce a specific, targeted translation behaviour, such as
peddling misinformation. We present two methods for crafting poisoned examples,
and show that only a tiny handful of instances, amounting to only 0.02% of the
training set, is sufficient to enact a successful attack. We outline a defence
method against said attacks, which partly ameliorates the problem. However, we
stress that this is a blind-spot in modern NMT, demanding immediate attention.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guzman_F/0/1/0/all/0/1"&gt;Francisco Guzman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Kishky_A/0/1/0/all/0/1"&gt;Ahmed El-Kishky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yuqing Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubinstein_B/0/1/0/all/0/1"&gt;Benjamin I. P. Rubinstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1"&gt;Trevor Cohn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computational Paremiology: Charting the temporal, ecological dynamics of proverb use in books, news articles, and tweets. (arXiv:2107.04929v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04929</id>
        <link href="http://arxiv.org/abs/2107.04929"/>
        <updated>2021-07-13T01:59:33.301Z</updated>
        <summary type="html"><![CDATA[Proverbs are an essential component of language and culture, and though much
attention has been paid to their history and currency, there has been
comparatively little quantitative work on changes in the frequency with which
they are used over time. With wider availability of large corpora reflecting
many diverse genres of documents, it is now possible to take a broad and
dynamic view of the importance of the proverb. Here, we measure temporal
changes in the relevance of proverbs within three corpora, differing in kind,
scale, and time frame: Millions of books over centuries; hundreds of millions
of news articles over twenty years; and billions of tweets over a decade. We
find that proverbs present heavy-tailed frequency-of-usage rank distributions
in each venue; exhibit trends reflecting the cultural dynamics of the eras
covered; and have evolved into contemporary forms on social media.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Davis_E/0/1/0/all/0/1"&gt;E. Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Danforth_C/0/1/0/all/0/1"&gt;C. M. Danforth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mieder_W/0/1/0/all/0/1"&gt;W. Mieder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dodds_P/0/1/0/all/0/1"&gt;P. S. Dodds&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Legal Judgment Prediction with Multi-Stage CaseRepresentation Learning in the Real Court Setting. (arXiv:2107.05192v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05192</id>
        <link href="http://arxiv.org/abs/2107.05192"/>
        <updated>2021-07-13T01:59:33.292Z</updated>
        <summary type="html"><![CDATA[Legal judgment prediction(LJP) is an essential task for legal AI. While prior
methods studied on this topic in a pseudo setting by employing the
judge-summarized case narrative as the input to predict the judgment,
neglecting critical case life-cycle information in real court setting could
threaten the case logic representation quality and prediction correctness. In
this paper, we introduce a novel challenging dataset from real courtrooms to
predict the legal judgment in a reasonably encyclopedic manner by leveraging
the genuine input of the case -- plaintiff's claims and court debate data, from
which the case's facts are automatically recognized by comprehensively
understanding the multi-role dialogues of the court debate, and then learnt to
discriminate the claims so as to reach the final judgment through multi-task
learning. An extensive set of experiments with a large civil trial data set
shows that the proposed model can more accurately characterize the interactions
among claims, fact and debate for legal judgment prediction, achieving
significant improvements over strong state-of-the-art baselines. Moreover, the
user study conducted with real judges and law school students shows the neural
predictions can also be interpretable and easily observed, and thus enhancing
the trial efficiency and judgment quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Luyao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yating Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaozhong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1"&gt;Wei Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changlong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shikun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Shot Compositional Concept Learning. (arXiv:2107.05176v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05176</id>
        <link href="http://arxiv.org/abs/2107.05176"/>
        <updated>2021-07-13T01:59:33.279Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the problem of recognizing compositional
attribute-object concepts within the zero-shot learning (ZSL) framework. We
propose an episode-based cross-attention (EpiCA) network which combines merits
of cross-attention mechanism and episode-based training strategy to recognize
novel compositional concepts. Firstly, EpiCA bases on cross-attention to
correlate concept-visual information and utilizes the gated pooling layer to
build contextualized representations for both images and concepts. The updated
representations are used for a more in-depth multi-modal relevance calculation
for concept recognition. Secondly, a two-phase episode training strategy,
especially the transductive phase, is adopted to utilize unlabeled test
examples to alleviate the low-resource learning problem. Experiments on two
widely-used zero-shot compositional learning (ZSCL) benchmarks have
demonstrated the effectiveness of the model compared with recent approaches on
both conventional and generalized ZSCL settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Guangyue Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kordjamshidi_P/0/1/0/all/0/1"&gt;Parisa Kordjamshidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1"&gt;Joyce Y. Chai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilingual and crosslingual speech recognition using phonological-vector based phone embeddings. (arXiv:2107.05038v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05038</id>
        <link href="http://arxiv.org/abs/2107.05038"/>
        <updated>2021-07-13T01:59:33.262Z</updated>
        <summary type="html"><![CDATA[The use of phonological features (PFs) potentially allows language-specific
phones to remain linked in training, which is highly desirable for information
sharing for multilingual and crosslingual speech recognition methods for
low-resourced languages. A drawback suffered by previous methods in using
phonological features is that the acoustic-to-PF extraction in a bottom-up way
is itself difficult. In this paper, we propose to join phonology driven phone
embedding (top-down) and deep neural network (DNN) based acoustic feature
extraction (bottom-up) to calculate phone probabilities. The new method is
called JoinAP (Joining of Acoustics and Phonology). Remarkably, no inversion
from acoustics to phonological features is required for speech recognition. For
each phone in the IPA (International Phonetic Alphabet) table, we encode its
phonological features to a phonological-vector, and then apply linear or
nonlinear transformation of the phonological-vector to obtain the phone
embedding. A series of multilingual and crosslingual (both zero-shot and
few-shot) speech recognition experiments are conducted on the CommonVoice
dataset (German, French, Spanish and Italian) and the AISHLL-1 dataset
(Mandarin), and demonstrate the superiority of JoinAP with nonlinear phone
embeddings over both JoinAP with linear phone embeddings and the traditional
method with flat phone embeddings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1"&gt;Chengrui Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_K/0/1/0/all/0/1"&gt;Keyu An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Huahuan Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1"&gt;Zhijian Ou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computer-assisted construct classification of organizational performance concerning different stakeholder groups. (arXiv:2107.05133v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05133</id>
        <link href="http://arxiv.org/abs/2107.05133"/>
        <updated>2021-07-13T01:59:33.252Z</updated>
        <summary type="html"><![CDATA[The number of research articles in business and management has dramatically
increased along with terminology, constructs, and measures. Proper
classification of organizational performance constructs from research articles
plays an important role in categorizing the literature and understanding to
whom its research implications may be relevant. In this work, we classify
constructs (i.e., concepts and terminology used to capture different aspects of
organizational performance) in research articles into a three-level
categorization: (a) performance and non-performance categories (Level 0); (b)
for performance constructs, stakeholder group-level of performance concerning
investors, customers, employees, and the society (community and natural
environment) (Level 1); and (c) for each stakeholder group-level, subcategories
of different ways of measurement (Level 2). We observed that increasing
contextual information with features extracted from surrounding sentences and
external references improves classification of disaggregate-level labels, given
limited training data. Our research has implications for computer-assisted
construct identification and classification - an essential step for research
synthesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gopalakrishnan_S/0/1/0/all/0/1"&gt;Seethalakshmi Gopalakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_V/0/1/0/all/0/1"&gt;Victor Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hahn_Powell_G/0/1/0/all/0/1"&gt;Gus Hahn-Powell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tirunagar_B/0/1/0/all/0/1"&gt;Bharadwaj Tirunagar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dialogue State Tracking with Multi-Level Fusion of Predicted Dialogue States and Conversations. (arXiv:2107.05168v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05168</id>
        <link href="http://arxiv.org/abs/2107.05168"/>
        <updated>2021-07-13T01:59:33.246Z</updated>
        <summary type="html"><![CDATA[Most recently proposed approaches in dialogue state tracking (DST) leverage
the context and the last dialogue states to track current dialogue states,
which are often slot-value pairs. Although the context contains the complete
dialogue information, the information is usually indirect and even requires
reasoning to obtain. The information in the lastly predicted dialogue states is
direct, but when there is a prediction error, the dialogue information from
this source will be incomplete or erroneous. In this paper, we propose the
Dialogue State Tracking with Multi-Level Fusion of Predicted Dialogue States
and Conversations network (FPDSC). This model extracts information of each
dialogue turn by modeling interactions among each turn utterance, the
corresponding last dialogue states, and dialogue slots. Then the representation
of each dialogue turn is aggregated by a hierarchical structure to form the
passage information, which is utilized in the current turn of DST. Experimental
results validate the effectiveness of the fusion network with 55.03% and 59.07%
joint accuracy on MultiWOZ 2.0 and MultiWOZ 2.1 datasets, which reaches the
state-of-the-art performance. Furthermore, we conduct the deleted-value and
related-slot experiments on MultiWOZ 2.1 to evaluate our model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingyao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Haipang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zehao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guodun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yin Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Low-resource Reading Comprehension via Cross-lingual Transposition Rethinking. (arXiv:2107.05002v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05002</id>
        <link href="http://arxiv.org/abs/2107.05002"/>
        <updated>2021-07-13T01:59:33.238Z</updated>
        <summary type="html"><![CDATA[Extractive Reading Comprehension (ERC) has made tremendous advances enabled
by the availability of large-scale high-quality ERC training data. Despite of
such rapid progress and widespread application, the datasets in languages other
than high-resource languages such as English remain scarce. To address this
issue, we propose a Cross-Lingual Transposition ReThinking (XLTT) model by
modelling existing high-quality extractive reading comprehension datasets in a
multilingual environment. To be specific, we present multilingual adaptive
attention (MAA) to combine intra-attention and inter-attention to learn more
general generalizable semantic and lexical knowledge from each pair of language
families. Furthermore, to make full use of existing datasets, we adopt a new
training framework to train our model by calculating task-level similarities
between each existing dataset and target dataset. The experimental results show
that our XLTT model surpasses six baselines on two multilingual ERC benchmarks,
especially more effective for low-resource languages with 3.9 and 4.1 average
improvement in F1 and EM, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1"&gt;Gaochen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu1_B/0/1/0/all/0/1"&gt;Bin Xu1&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1"&gt;Yuxin Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1"&gt;Fei Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bangchang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hongwen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1"&gt;Dejie Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Formal context reduction in deriving concept hierarchies from corpora using adaptive evolutionary clustering algorithm star. (arXiv:2107.04781v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.04781</id>
        <link href="http://arxiv.org/abs/2107.04781"/>
        <updated>2021-07-13T01:59:33.228Z</updated>
        <summary type="html"><![CDATA[It is beneficial to automate the process of deriving concept hierarchies from
corpora since a manual construction of concept hierarchies is typically a
time-consuming and resource-intensive process. As such, the overall process of
learning concept hierarchies from corpora encompasses a set of steps: parsing
the text into sentences, splitting the sentences and then tokenising it. After
the lemmatisation step, the pairs are extracted using FCA. However, there might
be some uninteresting and erroneous pairs in the formal context. Generating
formal context may lead to a time-consuming process, so formal context size
reduction is required to remove uninterested and erroneous pairs, taking less
time to extract the concept lattice and concept hierarchies accordingly. In
this premise, this study aims to propose two frameworks: (1) A framework to
review the current process of deriving concept hierarchies from corpus
utilising FCA; (2) A framework to decrease the formal contexts ambiguity of the
first framework using an adaptive version of ECA*. Experiments are conducted by
applying 385 sample corpora from Wikipedia on the two frameworks to examine the
reducing size of formal context, which leads to yield concept lattice and
concept hierarchy. The resulting lattice of formal context is evaluated to the
standard one using concept lattice-invariants. Accordingly, the homomorphic
between the two lattices preserves the quality of resulting concept hierarchies
by 89% in contrast to the basic ones, and the reduced concept lattice inherits
the structural relation of the standard one. The adaptive ECA* is examined
against its four counterpart baseline algorithms to measure the execution time
on random datasets with different densities (fill ratios). The results show
that adaptive ECA* performs concept lattice faster than other mentioned
competitive techniques in different fill ratios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hassan_B/0/1/0/all/0/1"&gt;Bryar A. Hassan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rashid_T/0/1/0/all/0/1"&gt;Tarik A. Rashid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mirjalili_S/0/1/0/all/0/1"&gt;Seyedali Mirjalili&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Axiomatic Explanations for Neural Ranking Models. (arXiv:2106.08019v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08019</id>
        <link href="http://arxiv.org/abs/2106.08019"/>
        <updated>2021-07-13T01:59:33.207Z</updated>
        <summary type="html"><![CDATA[Recently, neural networks have been successfully employed to improve upon
state-of-the-art performance in ad-hoc retrieval tasks via machine-learned
ranking functions. While neural retrieval models grow in complexity and impact,
little is understood about their correspondence with well-studied IR
principles. Recent work on interpretability in machine learning has provided
tools and techniques to understand neural models in general, yet there has been
little progress towards explaining ranking models.

We investigate whether one can explain the behavior of neural ranking models
in terms of their congruence with well understood principles of document
ranking by using established theories from axiomatic IR. Axiomatic analysis of
information retrieval models has formalized a set of constraints on ranking
decisions that reasonable retrieval models should fulfill. We operationalize
this axiomatic thinking to reproduce rankings based on combinations of
elementary constraints. This allows us to investigate to what extent the
ranking decisions of neural rankers can be explained in terms of retrieval
axioms, and which axioms apply in which situations. Our experimental study
considers a comprehensive set of axioms over several representative neural
rankers. While the existing axioms can already explain the particularly
confident ranking decisions rather well, future work should extend the axiom
set to also cover the other still "unexplainable" neural IR rank decisions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Volske_M/0/1/0/all/0/1"&gt;Michael V&amp;#xf6;lske&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bondarenko_A/0/1/0/all/0/1"&gt;Alexander Bondarenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frobe_M/0/1/0/all/0/1"&gt;Maik Fr&amp;#xf6;be&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hagen_M/0/1/0/all/0/1"&gt;Matthias Hagen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stein_B/0/1/0/all/0/1"&gt;Benno Stein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1"&gt;Jaspreet Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1"&gt;Avishek Anand&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Layer-wise Analysis of a Self-supervised Speech Representation Model. (arXiv:2107.04734v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04734</id>
        <link href="http://arxiv.org/abs/2107.04734"/>
        <updated>2021-07-13T01:59:33.186Z</updated>
        <summary type="html"><![CDATA[Recently proposed self-supervised learning approaches have been successful
for pre-training speech representation models. The utility of these learned
representations has been observed empirically, but not much has been studied
about the type or extent of information encoded in the pre-trained
representations themselves. Developing such insights can help understand the
capabilities and limits of these models and enable the research community to
more efficiently develop their usage for downstream applications. In this work,
we begin to fill this gap by examining one recent and successful pre-trained
model (wav2vec 2.0), via its intermediate representation vectors, using a suite
of analysis tools. We use the metrics of canonical correlation, mutual
information, and performance on simple downstream tasks with non-parametric
probes, in order to (i) query for acoustic and linguistic information content,
(ii) characterize the evolution of information across model layers, and (iii)
understand how fine-tuning the model for automatic speech recognition (ASR)
affects these observations. Our findings motivate modifying the fine-tuning
protocol for ASR, which produces improved word error rates in a low-resource
setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pasad_A/0/1/0/all/0/1"&gt;Ankita Pasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chou_J/0/1/0/all/0/1"&gt;Ju-Chieh Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1"&gt;Karen Livescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PatentMiner: Patent Vacancy Mining via Context-enhanced and Knowledge-guided Graph Attention. (arXiv:2107.04880v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04880</id>
        <link href="http://arxiv.org/abs/2107.04880"/>
        <updated>2021-07-13T01:59:33.177Z</updated>
        <summary type="html"><![CDATA[Although there are a small number of work to conduct patent research by
building knowledge graph, but without constructing patent knowledge graph using
patent documents and combining latest natural language processing methods to
mine hidden rich semantic relationships in existing patents and predict new
possible patents. In this paper, we propose a new patent vacancy prediction
approach named PatentMiner to mine rich semantic knowledge and predict new
potential patents based on knowledge graph (KG) and graph attention mechanism.
Firstly, patent knowledge graph over time (e.g. year) is constructed by
carrying out named entity recognition and relation extrac-tion from patent
documents. Secondly, Common Neighbor Method (CNM), Graph Attention Networks
(GAT) and Context-enhanced Graph Attention Networks (CGAT) are proposed to
perform link prediction in the constructed knowledge graph to dig out the
potential triples. Finally, patents are defined on the knowledge graph by means
of co-occurrence relationship, that is, each patent is represented as a fully
connected subgraph containing all its entities and co-occurrence relationships
of the patent in the knowledge graph; Furthermore, we propose a new patent
prediction task which predicts a fully connected subgraph with newly added
prediction links as a new pa-tent. The experimental results demonstrate that
our proposed patent predic-tion approach can correctly predict new patents and
Context-enhanced Graph Attention Networks is much better than the baseline.
Meanwhile, our proposed patent vacancy prediction task still has significant
room to im-prove.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1"&gt;Gaochen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1"&gt;Bin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1"&gt;Yuxin Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1"&gt;Fei Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bangchang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hongwen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1"&gt;Dejie Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can Evolutionary Computation Help us to Crib the Voynich Manuscript ?. (arXiv:2107.05381v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05381</id>
        <link href="http://arxiv.org/abs/2107.05381"/>
        <updated>2021-07-13T01:59:33.170Z</updated>
        <summary type="html"><![CDATA[Departing from the postulate that Voynich Manuscript is not a hoax but rather
encodes authentic contents, our article presents an evolutionary algorithm
which aims to find the most optimal mapping between voynichian glyphs and
candidate phonemic values. Core component of the decoding algorithm is a
process of maximization of a fitness function which aims to find most optimal
set of substitution rules allowing to transcribe the part of the manuscript --
which we call the Calendar -- into lists of feminine names. This leads to sets
of character subsitution rules which allow us to consistently transcribe dozens
among three hundred calendar tokens into feminine names: a result far
surpassing both ``popular'' as well as "state of the art" tentatives to crack
the manuscript. What's more, by using name lists stemming from different
languages as potential cribs, our ``adaptive'' method can also be useful in
identification of the language in which the manuscript is written.

As far as we can currently tell, results of our experiments indicate that the
Calendar part of the manuscript contains names from baltoslavic, balkanic or
hebrew language strata. Two further indications are also given: primo, highest
fitness values were obtained when the crib list contains names with specific
infixes at token's penultimate position as is the case, for example, for slavic
\textbf{feminine diminutives} (i.e. names ending with -ka and not -a). In the
most successful scenario, 240 characters contained in 35 distinct Voynichese
tokens were successfully transcribed. Secundo, in case of crib stemming from
Hebrew language, whole adaptation process converges to significantly better
fitness values when transcribing voynichian tokens whose order of individual
characters have been reversed, and when lists feminine and not masculine names
are used as the crib.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hromada_D/0/1/0/all/0/1"&gt;Daniel Devatman Hromada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HCGR: Hyperbolic Contrastive Graph Representation Learning for Session-based Recommendation. (arXiv:2107.05366v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.05366</id>
        <link href="http://arxiv.org/abs/2107.05366"/>
        <updated>2021-07-13T01:59:33.155Z</updated>
        <summary type="html"><![CDATA[Session-based recommendation (SBR) learns users' preferences by capturing the
short-term and sequential patterns from the evolution of user behaviors. Among
the studies in the SBR field, graph-based approaches are a relatively powerful
kind of way, which generally extract item information by message aggregation
under Euclidean space. However, such methods can't effectively extract the
hierarchical information contained among consecutive items in a session, which
is critical to represent users' preferences. In this paper, we present a
hyperbolic contrastive graph recommender (HCGR), a principled session-based
recommendation framework involving Lorentz hyperbolic space to adequately
capture the coherence and hierarchical representations of the items. Within
this framework, we design a novel adaptive hyperbolic attention computation to
aggregate the graph message of each user's preference in a session-based
behavior sequence. In addition, contrastive learning is leveraged to optimize
the item representation by considering the geodesic distance between positive
and negative samples in hyperbolic space. Extensive experiments on four
real-world datasets demonstrate that HCGR consistently outperforms
state-of-the-art baselines by 0.43$\%$-28.84$\%$ in terms of $HitRate$, $NDCG$
and $MRR$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_N/0/1/0/all/0/1"&gt;Naicheng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaolei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shaoshuai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1"&gt;Qiongxu Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yunan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bing Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Lin Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1"&gt;Kaixin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xiaobo Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Initial Investigation of Non-Native Spoken Question-Answering. (arXiv:2107.04691v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04691</id>
        <link href="http://arxiv.org/abs/2107.04691"/>
        <updated>2021-07-13T01:59:33.140Z</updated>
        <summary type="html"><![CDATA[Text-based machine comprehension (MC) systems have a wide-range of
applications, and standard corpora exist for developing and evaluating
approaches. There has been far less research on spoken question answering (SQA)
systems. The SQA task considered in this paper is to extract the answer from a
candidate$\text{'}$s spoken response to a question in a prompt-response style
language assessment test. Applying these MC approaches to this SQA task rather
than, for example, off-topic response detection provides far more detailed
information that can be used for further downstream processing. One significant
challenge is the lack of appropriately annotated speech corpora to train
systems for this task. Hence, a transfer-learning style approach is adopted
where a system trained on text-based MC is evaluated on an SQA task with
non-native speakers. Mismatches must be considered between text documents and
spoken responses; non-native spoken grammar and written grammar. In practical
SQA, ASR systems are used, necessitating an investigation of the impact of ASR
errors. We show that a simple text-based ELECTRA MC model trained on SQuAD2.0
transfers well for SQA. It is found that there is an approximately linear
relationship between ASR errors and the SQA assessment scores but grammar
mismatches have minimal impact.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1"&gt;Vatsal Raina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1"&gt;Mark J.F. Gales&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Reading of Hypotheses for Organizational Research Reviews and Pre-trained Models via R Shiny App for Non-Programmers. (arXiv:2106.16102v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.16102</id>
        <link href="http://arxiv.org/abs/2106.16102"/>
        <updated>2021-07-13T01:59:33.132Z</updated>
        <summary type="html"><![CDATA[The volume of scientific publications in organizational research becomes
exceedingly overwhelming for human researchers who seek to timely extract and
review knowledge. This paper introduces natural language processing (NLP)
models to accelerate the discovery, extraction, and organization of theoretical
developments (i.e., hypotheses) from social science publications. We illustrate
and evaluate NLP models in the context of a systematic review of stakeholder
value constructs and hypotheses. Specifically, we develop NLP models to
automatically 1) detect sentences in scholarly documents as hypotheses or not
(Hypothesis Detection), 2) deconstruct the hypotheses into nodes (constructs)
and links (causal/associative relationships) (Relationship Deconstruction ),
and 3) classify the features of links in terms causality (versus association)
and direction (positive, negative, versus nonlinear) (Feature Classification).
Our models have reported high performance metrics for all three tasks. While
our models are built in Python, we have made the pre-trained models fully
accessible for non-programmers. We have provided instructions on installing and
using our pre-trained models via an R Shiny app graphic user interface (GUI).
Finally, we suggest the next paths to extend our methodology for
computer-assisted knowledge synthesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_V/0/1/0/all/0/1"&gt;Victor Zitian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Montano_Campos_F/0/1/0/all/0/1"&gt;Felipe Montano-Campos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zadrozny_W/0/1/0/all/0/1"&gt;Wlodek Zadrozny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Canfield_E/0/1/0/all/0/1"&gt;Evan Canfield&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparative analysis of word embeddings in assessing semantic similarity of complex sentences. (arXiv:2010.12637v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12637</id>
        <link href="http://arxiv.org/abs/2010.12637"/>
        <updated>2021-07-13T01:59:33.120Z</updated>
        <summary type="html"><![CDATA[Semantic textual similarity is one of the open research challenges in the
field of Natural Language Processing. Extensive research has been carried out
in this field and near-perfect results are achieved by recent transformer-based
models in existing benchmark datasets like the STS dataset and the SICK
dataset. In this paper, we study the sentences in these datasets and analyze
the sensitivity of various word embeddings with respect to the complexity of
the sentences. We build a complex sentences dataset comprising of 50 sentence
pairs with associated semantic similarity values provided by 15 human
annotators. Readability analysis is performed to highlight the increase in
complexity of the sentences in the existing benchmark datasets and those in the
proposed dataset. Further, we perform a comparative analysis of the performance
of various word embeddings and language models on the existing benchmark
datasets and the proposed dataset. The results show the increase in complexity
of the sentences has a significant impact on the performance of the embedding
models resulting in a 10-20% decrease in Pearson's and Spearman's correlation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chandrasekaran_D/0/1/0/all/0/1"&gt;Dhivya Chandrasekaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mago_V/0/1/0/all/0/1"&gt;Vijay Mago&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noisy Training Improves E2E ASR for the Edge. (arXiv:2107.04677v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04677</id>
        <link href="http://arxiv.org/abs/2107.04677"/>
        <updated>2021-07-13T01:59:33.110Z</updated>
        <summary type="html"><![CDATA[Automatic speech recognition (ASR) has become increasingly ubiquitous on
modern edge devices. Past work developed streaming End-to-End (E2E) all-neural
speech recognizers that can run compactly on edge devices. However, E2E ASR
models are prone to overfitting and have difficulties in generalizing to unseen
testing data. Various techniques have been proposed to regularize the training
of ASR models, including layer normalization, dropout, spectrum data
augmentation and speed distortions in the inputs. In this work, we present a
simple yet effective noisy training strategy to further improve the E2E ASR
model training. By introducing random noise to the parameter space during
training, our method can produce smoother models at convergence that generalize
better. We apply noisy training to improve both dense and sparse
state-of-the-art Emformer models and observe consistent WER reduction.
Specifically, when training Emformers with 90% sparsity, we achieve 12% and 14%
WER improvements on the LibriSpeech Test-other and Test-clean data set,
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dilin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shangguan_Y/0/1/0/all/0/1"&gt;Yuan Shangguan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haichuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chuang_P/0/1/0/all/0/1"&gt;Pierce Chuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jiatong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Meng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venkatesh_G/0/1/0/all/0/1"&gt;Ganesh Venkatesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1"&gt;Ozlem Kalinli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1"&gt;Vikas Chandra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessing Data Efficiency in Task-Oriented Semantic Parsing. (arXiv:2107.04736v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04736</id>
        <link href="http://arxiv.org/abs/2107.04736"/>
        <updated>2021-07-13T01:59:33.084Z</updated>
        <summary type="html"><![CDATA[Data efficiency, despite being an attractive characteristic, is often
challenging to measure and optimize for in task-oriented semantic parsing;
unlike exact match, it can require both model- and domain-specific setups,
which have, historically, varied widely across experiments. In our work, as a
step towards providing a unified solution to data-efficiency-related questions,
we introduce a four-stage protocol which gives an approximate measure of how
much in-domain, "target" data a parser requires to achieve a certain quality
bar. Specifically, our protocol consists of (1) sampling target subsets of
different cardinalities, (2) fine-tuning parsers on each subset, (3) obtaining
a smooth curve relating target subset (%) vs. exact match (%), and (4)
referencing the curve to mine ad-hoc (target subset, exact match) points. We
apply our protocol in two real-world case studies -- model generalizability and
intent complexity -- illustrating its flexibility and applicability to
practitioners in task-oriented semantic parsing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1"&gt;Shrey Desai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Akshat Shrivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rill_J/0/1/0/all/0/1"&gt;Justin Rill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moran_B/0/1/0/all/0/1"&gt;Brian Moran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saleem_S/0/1/0/all/0/1"&gt;Safiyyah Saleem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zotov_A/0/1/0/all/0/1"&gt;Alexander Zotov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aly_A/0/1/0/all/0/1"&gt;Ahmed Aly&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noise Stability Regularization for Improving BERT Fine-tuning. (arXiv:2107.04835v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04835</id>
        <link href="http://arxiv.org/abs/2107.04835"/>
        <updated>2021-07-13T01:59:33.076Z</updated>
        <summary type="html"><![CDATA[Fine-tuning pre-trained language models such as BERT has become a common
practice dominating leaderboards across various NLP tasks. Despite its recent
success and wide adoption, this process is unstable when there are only a small
number of training samples available. The brittleness of this process is often
reflected by the sensitivity to random seeds. In this paper, we propose to
tackle this problem based on the noise stability property of deep nets, which
is investigated in recent literature (Arora et al., 2018; Sanyal et al., 2020).
Specifically, we introduce a novel and effective regularization method to
improve fine-tuning on NLP tasks, referred to as Layer-wise Noise Stability
Regularization (LNSR). We extend the theories about adding noise to the input
and prove that our method gives a stabler regularization effect. We provide
supportive evidence by experimentally confirming that well-performing models
show a low sensitivity to noise and fine-tuning with LNSR exhibits clearly
higher generalizability and stability. Furthermore, our method also
demonstrates advantages over other state-of-the-art algorithms including L2-SP
(Li et al., 2018), Mixout (Lee et al., 2020) and SMART (Jiang et al., 2020).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hua_H/0/1/0/all/0/1"&gt;Hang Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xingjian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1"&gt;Dejing Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Cheng-Zhong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jiebo Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Three Phase Semantic Web Matchmaker. (arXiv:2107.05368v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.05368</id>
        <link href="http://arxiv.org/abs/2107.05368"/>
        <updated>2021-07-13T01:59:32.950Z</updated>
        <summary type="html"><![CDATA[Since using environments that are made according to the service oriented
architecture, we have more effective and dynamic applications. Semantic
matchmaking process is finding valuable service candidates for substitution. It
is a very important aspect of using semantic Web Services. Our proposed
matchmaker algorithm performs semantic matching of Web Services on the basis of
input and output descriptions of semantic Web Services matching. This technique
takes advantages from a graph structure and flow networks. Our novel approach
is assigning matchmaking scores to semantics of the inputs and outputs
parameters and their types. It makes a flow network in which the weights of the
edges are these scores, using FordFulkerson algorithm, we find matching rate of
two web services. So, all services should be described in the same Ontology Web
Language. Among these candidates, best one is chosen for substitution in the
case of an execution failure. Our approach uses the algorithm that has the
least running time among all others that can be used for bipartite matching.
The importance of problem is that in real systems, many fundamental problems
will occur by late answering. So system`s service should always be on and if
one of them crashes, it would be replaced fast. Semantic web matchmaker eases
this process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heidari_G/0/1/0/all/0/1"&gt;Golsa Heidari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamanifar_K/0/1/0/all/0/1"&gt;Kamran Zamanifar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sliding Spectrum Decomposition for Diversified Recommendation. (arXiv:2107.05204v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.05204</id>
        <link href="http://arxiv.org/abs/2107.05204"/>
        <updated>2021-07-13T01:59:32.926Z</updated>
        <summary type="html"><![CDATA[Content feed, a type of product that recommends a sequence of items for users
to browse and engage with, has gained tremendous popularity among social media
platforms. In this paper, we propose to study the diversity problem in such a
scenario from an item sequence perspective using time series analysis
techniques. We derive a method called sliding spectrum decomposition (SSD) that
captures users' perception of diversity in browsing a long item sequence. We
also share our experiences in designing and implementing a suitable item
embedding method for accurate similarity measurement under long tail effect.
Combined together, they are now fully implemented and deployed in Xiaohongshu
App's production recommender system that serves the main Explore Feed product
for tens of millions of users every day. We demonstrate the effectiveness and
efficiency of the method through theoretical analysis, offline experiments and
online A/B tests.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yanhua Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weikun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Ruiwen Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning for Cold-Start Recommendation. (arXiv:2107.05315v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.05315</id>
        <link href="http://arxiv.org/abs/2107.05315"/>
        <updated>2021-07-13T01:59:32.896Z</updated>
        <summary type="html"><![CDATA[Recommending cold-start items is a long-standing and fundamental challenge in
recommender systems. Without any historical interaction on cold-start items, CF
scheme fails to use collaborative signals to infer user preference on these
items. To solve this problem, extensive studies have been conducted to
incorporate side information into the CF scheme. Specifically, they employ
modern neural network techniques (e.g., dropout, consistency constraint) to
discover and exploit the coalition effect of content features and collaborative
representations. However, we argue that these works less explore the mutual
dependencies between content features and collaborative representations and
lack sufficient theoretical supports, thus resulting in unsatisfactory
performance. In this work, we reformulate the cold-start item representation
learning from an information-theoretic standpoint. It aims to maximize the
mutual dependencies between item content and collaborative signals.
Specifically, the representation learning is theoretically lower-bounded by the
integration of two terms: mutual information between collaborative embeddings
of users and items, and mutual information between collaborative embeddings and
feature representations of items. To model such a learning process, we devise a
new objective function founded upon contrastive learning and develop a simple
yet effective Contrastive Learning-based Cold-start Recommendation
framework(CLCRec). In particular, CLCRec consists of three components:
contrastive pair organization, contrastive embedding, and contrastive
optimization modules. It allows us to preserve collaborative signals in the
content representations for both warm and cold-start items. Through extensive
experiments on four publicly accessible datasets, we observe that CLCRec
achieves significant improvements over state-of-the-art approaches in both
warm- and cold-start scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yinwei Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1"&gt;Liqiang Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuanping Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1"&gt;Tat-Seng Chua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Designing Recommender Systems to Depolarize. (arXiv:2107.04953v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.04953</id>
        <link href="http://arxiv.org/abs/2107.04953"/>
        <updated>2021-07-13T01:59:32.859Z</updated>
        <summary type="html"><![CDATA[Polarization is implicated in the erosion of democracy and the progression to
violence, which makes the polarization properties of large algorithmic content
selection systems (recommender systems) a matter of concern for peace and
security. While algorithm-driven social media does not seem to be a primary
driver of polarization at the country level, it could be a useful intervention
point in polarized societies. This paper examines algorithmic depolarization
interventions with the goal of conflict transformation: not suppressing or
eliminating conflict but moving towards more constructive conflict. Algorithmic
intervention is considered at three stages: which content is available
(moderation), how content is selected and personalized (ranking), and content
presentation and controls (user interface). Empirical studies of online
conflict suggest that the exposure diversity intervention proposed as an
antidote to "filter bubbles" can be improved and can even worsen polarization
under some conditions. Using civility metrics in conjunction with diversity in
content selection may be more effective. However, diversity-based interventions
have not been tested at scale and may not work in the diverse and dynamic
contexts of real platforms. Instead, intervening in platform polarization
dynamics will likely require continuous monitoring of polarization metrics,
such as the widely used "feeling thermometer." These metrics can be used to
evaluate product features, and potentially engineered as algorithmic
objectives. It may further prove necessary to include polarization measures in
the objective functions of recommender algorithms to prevent optimization
processes from creating conflict as a side effect.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stray_J/0/1/0/all/0/1"&gt;Jonathan Stray&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Position-enhanced and Time-aware Graph Convolutional Network for Sequential Recommendations. (arXiv:2107.05235v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.05235</id>
        <link href="http://arxiv.org/abs/2107.05235"/>
        <updated>2021-07-13T01:59:32.850Z</updated>
        <summary type="html"><![CDATA[Most of the existing deep learning-based sequential recommendation approaches
utilize the recurrent neural network architecture or self-attention to model
the sequential patterns and temporal influence among a user's historical
behavior and learn the user's preference at a specific time. However, these
methods have two main drawbacks. First, they focus on modeling users' dynamic
states from a user-centric perspective and always neglect the dynamics of items
over time. Second, most of them deal with only the first-order user-item
interactions and do not consider the high-order connectivity between users and
items, which has recently been proved helpful for the sequential
recommendation. To address the above problems, in this article, we attempt to
model user-item interactions by a bipartite graph structure and propose a new
recommendation approach based on a Position-enhanced and Time-aware Graph
Convolutional Network (PTGCN) for the sequential recommendation. PTGCN models
the sequential patterns and temporal dynamics between user-item interactions by
defining a position-enhanced and time-aware graph convolution operation and
learning the dynamic representations of users and items simultaneously on the
bipartite graph with a self-attention aggregator. Also, it realizes the
high-order connectivity between users and items by stacking multi-layer graph
convolutions. To demonstrate the effectiveness of PTGCN, we carried out a
comprehensive evaluation of PTGCN on three real-world datasets of different
sizes compared with a few competitive baselines. Experimental results indicate
that PTGCN outperforms several state-of-the-art models in terms of two
commonly-used evaluation metrics for ranking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Liwei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yutao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yanbo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuliang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Deyi Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Critical Features and General Issues of Freely Available mHealth Apps For Dietary Assessment. (arXiv:2008.09883v4 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.09883</id>
        <link href="http://arxiv.org/abs/2008.09883"/>
        <updated>2021-07-13T01:59:32.823Z</updated>
        <summary type="html"><![CDATA[Obesity is known to lower the quality of life substantially. It is often
associated with increased chances of non-communicable diseases such as
diabetes, cardiovascular problems, various cancers, etc. Evidence suggests that
diet-related mobile applications play a vital role in assisting individuals in
making healthier choices and keeping track of food intake. However, due to an
abundance of similar applications, it becomes pertinent to evaluate each of
them in terms of functionality, usability, and possible design issues to truly
determine state-of-the-art solutions for the future. Since these applications
involve implementing multiple user requirements and recommendations from
different dietitians, the evaluation becomes quite complex. Therefore, this
study aims to review existing dietary applications at length to highlight key
features and problems that enhance or undermine an application's usability. For
this purpose, we have examined the published literature from various scientific
databases of the PUBMED, CINAHL (January 2010-December 2019) and Science Direct
(2010-2019). We followed PRISMA guidelines, and out of our findings, fifty-six
primary studies met our inclusion criteria after identification, screening,
eligibility and full-text evaluation. We analyzed 35 apps from the selected
studies and extracted the data of each of the identified apps.Following our
detailed analysis on the comprehensiveness of freely available mHealth
applications, we specified potential future research challenges and stated
recommendations to help grow clinically accurate diet-related applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tahir_G/0/1/0/all/0/1"&gt;Ghalib Ahmed Tahir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loo_C/0/1/0/all/0/1"&gt;Chu Kiong Loo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moy_F/0/1/0/all/0/1"&gt;Foong Ming Moy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_N/0/1/0/all/0/1"&gt;Nadine Kong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MidiBERT-Piano: Large-scale Pre-training for Symbolic Music Understanding. (arXiv:2107.05223v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.05223</id>
        <link href="http://arxiv.org/abs/2107.05223"/>
        <updated>2021-07-13T01:59:32.810Z</updated>
        <summary type="html"><![CDATA[This paper presents an attempt to employ the mask language modeling approach
of BERT to pre-train a 12-layer Transformer model over 4,166 pieces of
polyphonic piano MIDI files for tackling a number of symbolic-domain
discriminative music understanding tasks. These include two note-level
classification tasks, i.e., melody extraction and velocity prediction, as well
as two sequence-level classification tasks, i.e., composer classification and
emotion classification. We find that, given a pre-trained Transformer, our
models outperform recurrent neural network based baselines with less than 10
epochs of fine-tuning. Ablation studies show that the pre-training remains
effective even if none of the MIDI data of the downstream tasks are seen at the
pre-training stage, and that freezing the self-attention layers of the
Transformer at the fine-tuning stage slightly degrades performance. All the
five datasets employed in this work are publicly available, as well as
checkpoints of our pre-trained and fine-tuned models. As such, our research can
be taken as a benchmark for symbolic-domain music understanding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1"&gt;Yi-Hui Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_I/0/1/0/all/0/1"&gt;I-Chun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1"&gt;Chin-Jui Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ching_J/0/1/0/all/0/1"&gt;Joann Ching&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi-Hsuan Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MMSys'21 Grand Challenge on Detecting Cheapfakes. (arXiv:2107.05297v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.05297</id>
        <link href="http://arxiv.org/abs/2107.05297"/>
        <updated>2021-07-13T01:59:32.800Z</updated>
        <summary type="html"><![CDATA[Cheapfake is a recently coined term that encompasses non-AI ("cheap")
manipulations of multimedia content. Cheapfakes are known to be more prevalent
than deepfakes. Cheapfake media can be created using editing software for
image/video manipulations, or even without using any software, by simply
altering the context of an image/video by sharing the media alongside
misleading claims. This alteration of context is referred to as out-of-context
(OOC) misuse} of media. OOC media is much harder to detect than fake media,
since the images and videos are not tampered. In this challenge, we focus on
detecting OOC images, and more specifically the misuse of real photographs with
conflicting image captions in news items. The aim of this challenge is to
develop and benchmark models that can be used to detect whether given samples
(news image and associated captions) are OOC, based on the recently compiled
COSMOS dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aneja_S/0/1/0/all/0/1"&gt;Shivangi Aneja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Midoglu_C/0/1/0/all/0/1"&gt;Cise Midoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dang_Nguyen_D/0/1/0/all/0/1"&gt;Duc-Tien Dang-Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riegler_M/0/1/0/all/0/1"&gt;Michael Alexander Riegler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Halvorsen_P/0/1/0/all/0/1"&gt;Paal Halvorsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1"&gt;Matthias Niessner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adsumilli_B/0/1/0/all/0/1"&gt;Balu Adsumilli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bregler_C/0/1/0/all/0/1"&gt;Chris Bregler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inductive Representation Based Graph Convolution Network for Collaborative Filtering. (arXiv:2107.05247v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.05247</id>
        <link href="http://arxiv.org/abs/2107.05247"/>
        <updated>2021-07-13T01:59:32.788Z</updated>
        <summary type="html"><![CDATA[In recent years, graph neural networks (GNNs) have shown powerful ability in
collaborative filtering, which is a widely adopted recommendation scenario.
While without any side information, existing graph neural network based methods
generally learn a one-hot embedding for each user or item as the initial input
representation of GNNs. However, such one-hot embedding is intrinsically
transductive, making these methods with no inductive ability, i.e., failing to
deal with new users or new items that are unseen during training. Besides, the
number of model parameters depends on the number of users and items, which is
expensive and not scalable. In this paper, we give a formal definition of
inductive recommendation and solve the above problems by proposing Inductive
representation based Graph Convolutional Network (IGCN) for collaborative
filtering. Specifically, we design an inductive representation layer, which
utilizes the interaction behavior with core users or items as the initial
representation, improving the general recommendation performance while bringing
inductive ability. Note that, the number of parameters of IGCN only depends on
the number of core users or items, which is adjustable and scalable. Extensive
experiments on three public benchmarks demonstrate the state-of-the-art
performance of IGCN in both transductive and inductive recommendation
scenarios, while with remarkably fewer model parameters. Our implementations
are available here in PyTorch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yunfan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1"&gt;Qi Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1"&gt;Huawei Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1"&gt;Shuchang Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xueqi Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Accurate Localization by Instance Search. (arXiv:2107.05005v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05005</id>
        <link href="http://arxiv.org/abs/2107.05005"/>
        <updated>2021-07-13T01:59:32.764Z</updated>
        <summary type="html"><![CDATA[Visual object localization is the key step in a series of object detection
tasks. In the literature, high localization accuracy is achieved with the
mainstream strongly supervised frameworks. However, such methods require
object-level annotations and are unable to detect objects of unknown
categories. Weakly supervised methods face similar difficulties. In this paper,
a self-paced learning framework is proposed to achieve accurate object
localization on the rank list returned by instance search. The proposed
framework mines the target instance gradually from the queries and their
corresponding top-ranked search results. Since a common instance is shared
between the query and the images in the rank list, the target visual instance
can be accurately localized even without knowing what the object category is.
In addition to performing localization on instance search, the issue of
few-shot object detection is also addressed under the same framework. Superior
performance over state-of-the-art methods is observed on both tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1"&gt;Yi-Geng Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1"&gt;Hui-Chu Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wan-Lei Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Similarity Guided Deep Face Image Retrieval. (arXiv:2107.05025v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05025</id>
        <link href="http://arxiv.org/abs/2107.05025"/>
        <updated>2021-07-13T01:59:32.754Z</updated>
        <summary type="html"><![CDATA[Face image retrieval, which searches for images of the same identity from the
query input face image, is drawing more attention as the size of the image
database increases rapidly. In order to conduct fast and accurate retrieval, a
compact hash code-based methods have been proposed, and recently, deep face
image hashing methods with supervised classification training have shown
outstanding performance. However, classification-based scheme has a
disadvantage in that it cannot reveal complex similarities between face images
into the hash code learning. In this paper, we attempt to improve the face
image retrieval quality by proposing a Similarity Guided Hashing (SGH) method,
which gently considers self and pairwise-similarity simultaneously. SGH employs
various data augmentations designed to explore elaborate similarities between
face images, solving both intra and inter identity-wise difficulties. Extensive
experimental results on the protocols with existing benchmarks and an
additionally proposed large scale higher resolution face image dataset
demonstrate that our SGH delivers state-of-the-art retrieval performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1"&gt;Young Kyun Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_N/0/1/0/all/0/1"&gt;Nam Ik Cho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformers with multi-modal features and post-fusion context for e-commerce session-based recommendation. (arXiv:2107.05124v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.05124</id>
        <link href="http://arxiv.org/abs/2107.05124"/>
        <updated>2021-07-13T01:59:32.739Z</updated>
        <summary type="html"><![CDATA[Session-based recommendation is an important task for e-commerce services,
where a large number of users browse anonymously or may have very distinct
interests for different sessions. In this paper we present one of the winning
solutions for the Recommendation task of the SIGIR 2021 Workshop on E-commerce
Data Challenge. Our solution was inspired by NLP techniques and consists of an
ensemble of two Transformer architectures - Transformer-XL and XLNet - trained
with autoregressive and autoencoding approaches. To leverage most of the rich
dataset made available for the competition, we describe how we prepared
multi-model features by combining tabular events with textual and image
vectors. We also present a model prediction analysis to better understand the
effectiveness of our architectures for the session-based recommendation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moreira_G/0/1/0/all/0/1"&gt;Gabriel de Souza P. Moreira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabhi_S/0/1/0/all/0/1"&gt;Sara Rabhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ak_R/0/1/0/all/0/1"&gt;Ronay Ak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1"&gt;Md Yasin Kabir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oldridge_E/0/1/0/all/0/1"&gt;Even Oldridge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SVP-CF: Selection via Proxy for Collaborative Filtering Data. (arXiv:2107.04984v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.04984</id>
        <link href="http://arxiv.org/abs/2107.04984"/>
        <updated>2021-07-13T01:59:32.720Z</updated>
        <summary type="html"><![CDATA[We study the practical consequences of dataset sampling strategies on the
performance of recommendation algorithms. Recommender systems are generally
trained and evaluated on samples of larger datasets. Samples are often taken in
a naive or ad-hoc fashion: e.g. by sampling a dataset randomly or by selecting
users or items with many interactions. As we demonstrate, commonly-used data
sampling schemes can have significant consequences on algorithm performance --
masking performance deficiencies in algorithms or altering the relative
performance of algorithms, as compared to models trained on the complete
dataset. Following this observation, this paper makes the following main
contributions: (1) characterizing the effect of sampling on algorithm
performance, in terms of algorithm and dataset characteristics (e.g. sparsity
characteristics, sequential dynamics, etc.); and (2) designing SVP-CF, which is
a data-specific sampling strategy, that aims to preserve the relative
performance of models after sampling, and is especially suited to long-tail
interaction data. Detailed experiments show that SVP-CF is more accurate than
commonly used sampling schemes in retaining the relative ranking of different
recommendation algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sachdeva_N/0/1/0/all/0/1"&gt;Noveen Sachdeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Carole-Jean Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1"&gt;Julian McAuley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Propagation-aware Social Recommendation by Transfer Learning. (arXiv:2107.04846v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.04846</id>
        <link href="http://arxiv.org/abs/2107.04846"/>
        <updated>2021-07-13T01:59:32.708Z</updated>
        <summary type="html"><![CDATA[Social-aware recommendation approaches have been recognized as an effective
way to solve the data sparsity issue of traditional recommender systems. The
assumption behind is that the knowledge in social user-user connections can be
shared and transferred to the domain of user-item interactions, whereby to help
learn user preferences. However, most existing approaches merely adopt the
first-order connections among users during transfer learning, ignoring those
connections in higher orders. We argue that better recommendation performance
can also benefit from high-order social relations. In this paper, we propose a
novel Propagation-aware Transfer Learning Network (PTLN) based on the
propagation of social relations. We aim to better mine the sharing knowledge
hidden in social networks and thus further improve recommendation performance.
Specifically, we explore social influence in two aspects: (a) higher-order
friends have been taken into consideration by order bias; (b) different friends
in the same order will have distinct importance for recommendation by an
attention mechanism. Besides, we design a novel regularization to bridge the
gap between social relations and user-item interactions. We conduct extensive
experiments on two real-world datasets and beat other counterparts in terms of
ranking accuracy, especially for the cold-start users with few historical
interactions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Haodong Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_Y/0/1/0/all/0/1"&gt;Yabo Chu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning for Cold-Start Recommendation. (arXiv:2107.05315v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.05315</id>
        <link href="http://arxiv.org/abs/2107.05315"/>
        <updated>2021-07-13T01:59:32.618Z</updated>
        <summary type="html"><![CDATA[Recommending cold-start items is a long-standing and fundamental challenge in
recommender systems. Without any historical interaction on cold-start items, CF
scheme fails to use collaborative signals to infer user preference on these
items. To solve this problem, extensive studies have been conducted to
incorporate side information into the CF scheme. Specifically, they employ
modern neural network techniques (e.g., dropout, consistency constraint) to
discover and exploit the coalition effect of content features and collaborative
representations. However, we argue that these works less explore the mutual
dependencies between content features and collaborative representations and
lack sufficient theoretical supports, thus resulting in unsatisfactory
performance. In this work, we reformulate the cold-start item representation
learning from an information-theoretic standpoint. It aims to maximize the
mutual dependencies between item content and collaborative signals.
Specifically, the representation learning is theoretically lower-bounded by the
integration of two terms: mutual information between collaborative embeddings
of users and items, and mutual information between collaborative embeddings and
feature representations of items. To model such a learning process, we devise a
new objective function founded upon contrastive learning and develop a simple
yet effective Contrastive Learning-based Cold-start Recommendation
framework(CLCRec). In particular, CLCRec consists of three components:
contrastive pair organization, contrastive embedding, and contrastive
optimization modules. It allows us to preserve collaborative signals in the
content representations for both warm and cold-start items. Through extensive
experiments on four publicly accessible datasets, we observe that CLCRec
achieves significant improvements over state-of-the-art approaches in both
warm- and cold-start scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yinwei Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1"&gt;Liqiang Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuanping Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1"&gt;Tat-Seng Chua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReconVAT: A Semi-Supervised Automatic Music Transcription Framework for Low-Resource Real-World Data. (arXiv:2107.04954v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.04954</id>
        <link href="http://arxiv.org/abs/2107.04954"/>
        <updated>2021-07-13T01:59:32.430Z</updated>
        <summary type="html"><![CDATA[Most of the current supervised automatic music transcription (AMT) models
lack the ability to generalize. This means that they have trouble transcribing
real-world music recordings from diverse musical genres that are not presented
in the labelled training data. In this paper, we propose a semi-supervised
framework, ReconVAT, which solves this issue by leveraging the huge amount of
available unlabelled music recordings. The proposed ReconVAT uses
reconstruction loss and virtual adversarial training. When combined with
existing U-net models for AMT, ReconVAT achieves competitive results on common
benchmark datasets such as MAPS and MusicNet. For example, in the few-shot
setting for the string part version of MusicNet, ReconVAT achieves F1-scores of
61.0% and 41.6% for the note-wise and note-with-offset-wise metrics
respectively, which translates into an improvement of 22.2% and 62.5% compared
to the supervised baseline model. Our proposed framework also demonstrates the
potential of continual learning on new data, which could be useful in
real-world applications whereby new data is constantly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheuk_K/0/1/0/all/0/1"&gt;Kin Wai Cheuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herremans_D/0/1/0/all/0/1"&gt;Dorien Herremans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1"&gt;Li Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computer Vision-assisted Decimeter-level Single-antenna RSSI Localization Harnessing Dynamic Blockage Events. (arXiv:2107.04770v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.04770</id>
        <link href="http://arxiv.org/abs/2107.04770"/>
        <updated>2021-07-13T01:59:32.408Z</updated>
        <summary type="html"><![CDATA[This paper demonstrates the feasibility of received power strength indicator
(RSSI)-based single-antenna localization (R-SAL) with decimeter-level
localization accuracy. To achieve decimeter-level accuracy, either fine-grained
radio frequency (RF) information (e.g., channel state information) or
coarse-grained RF information (e.g., RSSI) from more than multiple antennas is
required. Meanwhile, owing to deficiency of single-antenna RSSI which only
indicates a distance between a receiver and a transmitter, realizing
fine-grained localization accuracy with single coarse-grained RF information is
challenging. Our key idea to address this challenge is to leverage computer
vision (CV) and to estimate the most likely Fresnel zone between the receiver
and transmitter, where the role of RSSI is to detect blockage timings.
Specifically, historical positions of an obstacle that dynamically blocks the
Fresnel zone are detected by the CV technique, and we estimate positions at
which a blockage starts and ends via a time series of RSSI. These estimated
obstacle positions, in principle, coincide with points on the Fresnel zone
boundaries, enabling the estimation of the Fresnel zone and localization of the
transmitter. The experimental evaluation revealed that the proposed R-SAL
achieved decimeter-level localization in an indoor environment, which is
comparable to that of a simple previous RSSI-based localization with three
receivers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sunami_T/0/1/0/all/0/1"&gt;Tomoya Sunami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Itahara_S/0/1/0/all/0/1"&gt;Sohei Itahara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koda_Y/0/1/0/all/0/1"&gt;Yusuke Koda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nishio_T/0/1/0/all/0/1"&gt;Takayuki Nishio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamamoto_K/0/1/0/all/0/1"&gt;Koji Yamamoto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DualVGR: A Dual-Visual Graph Reasoning Unit for Video Question Answering. (arXiv:2107.04768v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.04768</id>
        <link href="http://arxiv.org/abs/2107.04768"/>
        <updated>2021-07-13T01:59:32.384Z</updated>
        <summary type="html"><![CDATA[Video question answering is a challenging task, which requires agents to be
able to understand rich video contents and perform spatial-temporal reasoning.
However, existing graph-based methods fail to perform multi-step reasoning
well, neglecting two properties of VideoQA: (1) Even for the same video,
different questions may require different amount of video clips or objects to
infer the answer with relational reasoning; (2) During reasoning, appearance
and motion features have complicated interdependence which are correlated and
complementary to each other. Based on these observations, we propose a
Dual-Visual Graph Reasoning Unit (DualVGR) which reasons over videos in an
end-to-end fashion. The first contribution of our DualVGR is the design of an
explainable Query Punishment Module, which can filter out irrelevant visual
features through multiple cycles of reasoning. The second contribution is the
proposed Video-based Multi-view Graph Attention Network, which captures the
relations between appearance and motion features. Our DualVGR network achieves
state-of-the-art performance on the benchmark MSVD-QA and SVQA datasets, and
demonstrates competitive results on benchmark MSRVTT-QA datasets. Our code is
available at https://github.com/MMIR/DualVGR-VideoQA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_B/0/1/0/all/0/1"&gt;Bing-Kun Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Changsheng Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly-Supervised Classification and Detection of Bird Sounds in the Wild. A BirdCLEF 2021 Solution. (arXiv:2107.04878v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.04878</id>
        <link href="http://arxiv.org/abs/2107.04878"/>
        <updated>2021-07-13T01:59:32.340Z</updated>
        <summary type="html"><![CDATA[It is easier to hear birds than see them, however, they still play an
essential role in nature and they are excellent indicators of deteriorating
environmental quality and pollution. Recent advances in Machine Learning and
Convolutional Neural Networks allow us to detect and classify bird sounds, by
doing this, we can assist researchers in monitoring the status and trends of
bird populations and biodiversity in ecosystems. We propose a sound detection
and classification pipeline for analyzing complex soundscape recordings and
identify birdcalls in the background. Our pipeline learns from weak labels,
classifies fine-grained bird vocalizations in the wild, and is robust against
background sounds (e.g., airplanes, rain, etc). Our solution achieved 10th
place of 816 teams at the BirdCLEF 2021 Challenge hosted on Kaggle.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Conde_M/0/1/0/all/0/1"&gt;Marcos V. Conde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shubham_K/0/1/0/all/0/1"&gt;Kumar Shubham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agnihotri_P/0/1/0/all/0/1"&gt;Prateek Agnihotri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Movva_N/0/1/0/all/0/1"&gt;Nitin D. Movva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bessenyei_S/0/1/0/all/0/1"&gt;Szilard Bessenyei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Sensor Fusion Algorithms Against Voice Command Attacks in Autonomous Vehicles. (arXiv:2104.09872v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09872</id>
        <link href="http://arxiv.org/abs/2104.09872"/>
        <updated>2021-07-12T01:55:17.162Z</updated>
        <summary type="html"><![CDATA[With recent advances in autonomous driving, Voice Control Systems have become
increasingly adopted as human-vehicle interaction methods. This technology
enables drivers to use voice commands to control the vehicle and will be soon
available in Advanced Driver Assistance Systems (ADAS). Prior work has shown
that Siri, Alexa and Cortana, are highly vulnerable to inaudible command
attacks. This could be extended to ADAS in real-world applications and such
inaudible command threat is difficult to detect due to microphone
nonlinearities. In this paper, we aim to develop a more practical solution by
using camera views to defend against inaudible command attacks where ADAS are
capable of detecting their environment via multi-sensors. To this end, we
propose a novel multimodal deep learning classification system to defend
against inaudible command attacks. Our experimental results confirm the
feasibility of the proposed defense methods and the best classification
accuracy reaches 89.2%. Code is available at
https://github.com/ITSEG-MQ/Sensor-Fusion-Against-VoiceCommand-Attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1"&gt;Jiwei Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1"&gt;Xi Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yipeng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jolfa_A/0/1/0/all/0/1"&gt;Alireza Jolfa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Adaptation to Label Distribution Shift. (arXiv:2107.04520v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04520</id>
        <link href="http://arxiv.org/abs/2107.04520"/>
        <updated>2021-07-12T01:55:17.137Z</updated>
        <summary type="html"><![CDATA[Machine learning models often encounter distribution shifts when deployed in
the real world. In this paper, we focus on adaptation to label distribution
shift in the online setting, where the test-time label distribution is
continually changing and the model must dynamically adapt to it without
observing the true label. Leveraging a novel analysis, we show that the lack of
true label does not hinder estimation of the expected test loss, which enables
the reduction of online label shift adaptation to conventional online learning.
Informed by this observation, we propose adaptation algorithms inspired by
classical online learning techniques such as Follow The Leader (FTL) and Online
Gradient Descent (OGD) and derive their regret bounds. We empirically verify
our findings under both simulated and real world label distribution shifts and
show that OGD is particularly effective and robust to a variety of challenging
label shift scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1"&gt;Ruihan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1"&gt;Chuan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1"&gt;Yi Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1"&gt;Kilian Q. Weinberger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Annealed Flow Transport Monte Carlo. (arXiv:2102.07501v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07501</id>
        <link href="http://arxiv.org/abs/2102.07501"/>
        <updated>2021-07-12T01:55:17.130Z</updated>
        <summary type="html"><![CDATA[Annealed Importance Sampling (AIS) and its Sequential Monte Carlo (SMC)
extensions are state-of-the-art methods for estimating normalizing constants of
probability distributions. We propose here a novel Monte Carlo algorithm,
Annealed Flow Transport (AFT), that builds upon AIS and SMC and combines them
with normalizing flows (NFs) for improved performance. This method transports a
set of particles using not only importance sampling (IS), Markov chain Monte
Carlo (MCMC) and resampling steps - as in SMC, but also relies on NFs which are
learned sequentially to push particles towards the successive annealed targets.
We provide limit theorems for the resulting Monte Carlo estimates of the
normalizing constant and expectations with respect to the target distribution.
Additionally, we show that a continuous-time scaling limit of the population
version of AFT is given by a Feynman--Kac measure which simplifies to the law
of a controlled diffusion for expressive NFs. We demonstrate experimentally the
benefits and limitations of our methodology on a variety of applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Arbel_M/0/1/0/all/0/1"&gt;Michael Arbel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Matthews_A/0/1/0/all/0/1"&gt;Alexander G. D. G. Matthews&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1"&gt;Arnaud Doucet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalization of the Change of Variables Formula with Applications to Residual Flows. (arXiv:2107.04346v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.04346</id>
        <link href="http://arxiv.org/abs/2107.04346"/>
        <updated>2021-07-12T01:55:17.122Z</updated>
        <summary type="html"><![CDATA[Normalizing flows leverage the Change of Variables Formula (CVF) to define
flexible density models. Yet, the requirement of smooth transformations
(diffeomorphisms) in the CVF poses a significant challenge in the construction
of these models. To enlarge the design space of flows, we introduce
$\mathcal{L}$-diffeomorphisms as generalized transformations which may violate
these requirements on zero Lebesgue-measure sets. This relaxation allows e.g.
the use of non-smooth activation functions such as ReLU. Finally, we apply the
obtained results to planar, radial, and contractive residual flows.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Koenen_N/0/1/0/all/0/1"&gt;Niklas Koenen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wright_M/0/1/0/all/0/1"&gt;Marvin N. Wright&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Maass_P/0/1/0/all/0/1"&gt;Peter Maa&amp;#xdf;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Behrmann_J/0/1/0/all/0/1"&gt;Jens Behrmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CMRNet: Camera to LiDAR-Map Registration. (arXiv:1906.10109v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.10109</id>
        <link href="http://arxiv.org/abs/1906.10109"/>
        <updated>2021-07-12T01:55:17.114Z</updated>
        <summary type="html"><![CDATA[In this paper we present CMRNet, a realtime approach based on a Convolutional
Neural Network to localize an RGB image of a scene in a map built from LiDAR
data. Our network is not trained in the working area, i.e. CMRNet does not
learn the map. Instead it learns to match an image to the map. We validate our
approach on the KITTI dataset, processing each frame independently without any
tracking procedure. CMRNet achieves 0.27m and 1.07deg median localization
accuracy on the sequence 00 of the odometry dataset, starting from a rough pose
estimate displaced up to 3.5m and 17deg. To the best of our knowledge this is
the first CNN-based approach that learns to match images from a monocular
camera to a given, preexisting 3D LiDAR-map.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cattaneo_D/0/1/0/all/0/1"&gt;Daniele Cattaneo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vaghi_M/0/1/0/all/0/1"&gt;Matteo Vaghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ballardini_A/0/1/0/all/0/1"&gt;Augusto Luis Ballardini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fontana_S/0/1/0/all/0/1"&gt;Simone Fontana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sorrenti_D/0/1/0/all/0/1"&gt;Domenico Giorgio Sorrenti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1"&gt;Wolfram Burgard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An $O(s^r)$-Resolution ODE Framework for Understanding Discrete-Time Algorithms and Applications to the Linear Convergence of Minimax Problems. (arXiv:2001.08826v7 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.08826</id>
        <link href="http://arxiv.org/abs/2001.08826"/>
        <updated>2021-07-12T01:55:17.107Z</updated>
        <summary type="html"><![CDATA[There has been a long history of using ordinary differential equations (ODEs)
to understand the dynamics of discrete-time algorithms (DTAs). Surprisingly,
there are still two fundamental and unanswered questions: (i) it is unclear how
to obtain a \emph{suitable} ODE from a given DTA, and (ii) it is unclear the
connection between the convergence of a DTA and its corresponding ODEs. In this
paper, we propose a new machinery -- an $O(s^r)$-resolution ODE framework --
for analyzing the behavior of a generic DTA, which (partially) answers the
above two questions. The framework contains three steps: 1. To obtain a
suitable ODE from a given DTA, we define a hierarchy of $O(s^r)$-resolution
ODEs of a DTA parameterized by the degree $r$, where $s$ is the step-size of
the DTA. We present a principal approach to construct the unique
$O(s^r)$-resolution ODEs from a DTA; 2. To analyze the resulting ODE, we
propose the $O(s^r)$-linear-convergence condition of a DTA with respect to an
energy function, under which the $O(s^r)$-resolution ODE converges linearly to
an optimal solution; 3. To bridge the convergence properties of a DTA and its
corresponding ODEs, we define the properness of an energy function and show
that the linear convergence of the $O(s^r)$-resolution ODE with respect to a
proper energy function can automatically guarantee the linear convergence of
the DTA. To better illustrate this machinery, we utilize it to study three
classic algorithms -- gradient descent ascent (GDA), proximal point method
(PPM) and extra-gradient method (EGM) -- for solving the unconstrained minimax
problem $\min_{x\in\RR^n} \max_{y\in \RR^m} L(x,y)$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Lu_H/0/1/0/all/0/1"&gt;Haihao Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bias in Machine Learning Software: Why? How? What to do?. (arXiv:2105.12195v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12195</id>
        <link href="http://arxiv.org/abs/2105.12195"/>
        <updated>2021-07-12T01:55:17.086Z</updated>
        <summary type="html"><![CDATA[Increasingly, software is making autonomous decisions in case of criminal
sentencing, approving credit cards, hiring employees, and so on. Some of these
decisions show bias and adversely affect certain social groups (e.g. those
defined by sex, race, age, marital status). Many prior works on bias mitigation
take the following form: change the data or learners in multiple ways, then see
if any of that improves fairness. Perhaps a better approach is to postulate
root causes of bias and then applying some resolution strategy. This paper
postulates that the root causes of bias are the prior decisions that affect-
(a) what data was selected and (b) the labels assigned to those examples. Our
Fair-SMOTE algorithm removes biased labels; and rebalances internal
distributions such that based on sensitive attribute, examples are equal in
both positive and negative classes. On testing, it was seen that this method
was just as effective at reducing bias as prior approaches. Further, models
generated via Fair-SMOTE achieve higher performance (measured in terms of
recall and F1) than other state-of-the-art fairness improvement algorithms. To
the best of our knowledge, measured in terms of number of analyzed learners and
datasets, this study is one of the largest studies on bias mitigation yet
presented in the literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_J/0/1/0/all/0/1"&gt;Joymallya Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majumder_S/0/1/0/all/0/1"&gt;Suvodeep Majumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menzies_T/0/1/0/all/0/1"&gt;Tim Menzies&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Gradient-based Algorithms for Non-concave Bandit Optimization. (arXiv:2107.04518v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04518</id>
        <link href="http://arxiv.org/abs/2107.04518"/>
        <updated>2021-07-12T01:55:17.079Z</updated>
        <summary type="html"><![CDATA[Bandit problems with linear or concave reward have been extensively studied,
but relatively few works have studied bandits with non-concave reward. This
work considers a large family of bandit problems where the unknown underlying
reward function is non-concave, including the low-rank generalized linear
bandit problems and two-layer neural network with polynomial activation bandit
problem. For the low-rank generalized linear bandit problem, we provide a
minimax-optimal algorithm in the dimension, refuting both conjectures in
[LMT21, JWWN19]. Our algorithms are based on a unified zeroth-order
optimization paradigm that applies in great generality and attains optimal
rates in several structured polynomial settings (in the dimension). We further
demonstrate the applicability of our algorithms in RL in the generative model
setting, resulting in improved sample complexity over prior approaches.
Finally, we show that the standard optimistic algorithms (e.g., UCB) are
sub-optimal by dimension factors. In the neural net setting (with polynomial
activation functions) with noiseless reward, we provide a bandit algorithm with
sample complexity equal to the intrinsic algebraic dimension. Again, we show
that optimistic approaches have worse sample complexity, polynomial in the
extrinsic dimension (which could be exponentially worse in the polynomial
degree).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1"&gt;Baihe Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kaixuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1"&gt;Sham M. Kakade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jason D. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1"&gt;Qi Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Runzhe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jiaqi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HPNet: Deep Primitive Segmentation Using Hybrid Representations. (arXiv:2105.10620v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10620</id>
        <link href="http://arxiv.org/abs/2105.10620"/>
        <updated>2021-07-12T01:55:17.072Z</updated>
        <summary type="html"><![CDATA[This paper introduces HPNet, a novel deep-learning approach for segmenting a
3D shape represented as a point cloud into primitive patches. The key to deep
primitive segmentation is learning a feature representation that can separate
points of different primitives. Unlike utilizing a single feature
representation, HPNet leverages hybrid representations that combine one learned
semantic descriptor, two spectral descriptors derived from predicted geometric
parameters, as well as an adjacency matrix that encodes sharp edges. Moreover,
instead of merely concatenating the descriptors, HPNet optimally combines
hybrid representations by learning combination weights. This weighting module
builds on the entropy of input features. The output primitive segmentation is
obtained from a mean-shift clustering module. Experimental results on benchmark
datasets ANSI and ABCParts show that HPNet leads to significant performance
gains from baseline approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1"&gt;Siming Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhenpei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1"&gt;Chongyang Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Haibin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vouga_E/0/1/0/all/0/1"&gt;Etienne Vouga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qixing Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EPIC-Survival: End-to-end Part Inferred Clustering for Survival Analysis, Featuring Prognostic Stratification Boosting. (arXiv:2101.11085v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11085</id>
        <link href="http://arxiv.org/abs/2101.11085"/>
        <updated>2021-07-12T01:55:17.065Z</updated>
        <summary type="html"><![CDATA[Histopathology-based survival modelling has two major hurdles. Firstly, a
well-performing survival model has minimal clinical application if it does not
contribute to the stratification of a cancer patient cohort into different risk
groups, preferably driven by histologic morphologies. In the clinical setting,
individuals are not given specific prognostic predictions, but are rather
predicted to lie within a risk group which has a general survival trend. Thus,
It is imperative that a survival model produces well-stratified risk groups.
Secondly, until now, survival modelling was done in a two-stage approach
(encoding and aggregation). The massive amount of pixels in digitized whole
slide images were never utilized to their fullest extent due to technological
constraints on data processing, forcing decoupled learning. EPIC-Survival
bridges encoding and aggregation into an end-to-end survival modelling
approach, while introducing stratification boosting to encourage the model to
not only optimize ranking, but also to discriminate between risk groups. In
this study we show that EPIC-Survival performs better than other approaches in
modelling intrahepatic cholangiocarcinoma, a historically difficult cancer to
model. Further, we show that stratification boosting improves further improves
model performance, resulting in a concordance-index of 0.880 on a held-out test
set. Finally, we were able to identify specific histologic differences, not
commonly sought out in ICC, between low and high risk groups.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Muhammad_H/0/1/0/all/0/1"&gt;Hassan Muhammad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1"&gt;Chensu Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sigel_C/0/1/0/all/0/1"&gt;Carlie S. Sigel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doukas_M/0/1/0/all/0/1"&gt;Michael Doukas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alpert_L/0/1/0/all/0/1"&gt;Lindsay Alpert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jarnagin_W/0/1/0/all/0/1"&gt;William R. Jarnagin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simpson_A/0/1/0/all/0/1"&gt;Amber Simpson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fuchs_T/0/1/0/all/0/1"&gt;Thomas J. Fuchs&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Universal Multilayer Network Exploration by Random Walk with Restart. (arXiv:2107.04565v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04565</id>
        <link href="http://arxiv.org/abs/2107.04565"/>
        <updated>2021-07-12T01:55:17.059Z</updated>
        <summary type="html"><![CDATA[The amount and variety of data is increasing drastically for several years.
These data are often represented as networks, which are then explored with
approaches arising from network theory. Recent years have witnessed the
extension of network exploration methods to leverage more complex and richer
network frameworks. Random walks, for instance, have been extended to explore
multilayer networks. However, current random walk approaches are limited in the
combination and heterogeneity of network layers they can handle. New analytical
and numerical random walk methods are needed to cope with the increasing
diversity and complexity of multilayer networks. We propose here MultiXrank, a
Python package that enables Random Walk with Restart (RWR) on any kind of
multilayer network with an optimized implementation. This package is supported
by a universal mathematical formulation of the RWR. We evaluated MultiXrank
with leave-one-out cross-validation and link prediction, and introduced
protocols to measure the impact of the addition or removal of multilayer
network data on prediction performances. We further measured the sensitivity of
MultiXrank to input parameters by in-depth exploration of the parameter space.
Finally, we illustrate the versatility of MultiXrank with different use-cases
of unsupervised node prioritization and supervised classification in the
context of human genetic diseases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baptista_A/0/1/0/all/0/1"&gt;Anthony Baptista&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_A/0/1/0/all/0/1"&gt;Aitor Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baudot_A/0/1/0/all/0/1"&gt;Ana&amp;#xef;s Baudot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GLIB: Towards Automated Test Oracle for Graphically-Rich Applications. (arXiv:2106.10507v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10507</id>
        <link href="http://arxiv.org/abs/2106.10507"/>
        <updated>2021-07-12T01:55:17.040Z</updated>
        <summary type="html"><![CDATA[Graphically-rich applications such as games are ubiquitous with attractive
visual effects of Graphical User Interface (GUI) that offers a bridge between
software applications and end-users. However, various types of graphical
glitches may arise from such GUI complexity and have become one of the main
component of software compatibility issues. Our study on bug reports from game
development teams in NetEase Inc. indicates that graphical glitches frequently
occur during the GUI rendering and severely degrade the quality of
graphically-rich applications such as video games. Existing automated testing
techniques for such applications focus mainly on generating various GUI test
sequences and check whether the test sequences can cause crashes. These
techniques require constant human attention to captures non-crashing bugs such
as bugs causing graphical glitches. In this paper, we present the first step in
automating the test oracle for detecting non-crashing bugs in graphically-rich
applications. Specifically, we propose \texttt{GLIB} based on a code-based data
augmentation technique to detect game GUI glitches. We perform an evaluation of
\texttt{GLIB} on 20 real-world game apps (with bug reports available) and the
result shows that \texttt{GLIB} can achieve 100\% precision and 99.5\% recall
in detecting non-crashing bugs such as game GUI glitches. Practical application
of \texttt{GLIB} on another 14 real-world games (without bug reports) further
demonstrates that \texttt{GLIB} can effectively uncover GUI glitches, with 48
of 53 bugs reported by \texttt{GLIB} having been confirmed and fixed so far.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Ke Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yufei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yingfeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1"&gt;Changjie Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhipeng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wei Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Delegate for Large-scale Vehicle Routing. (arXiv:2107.04139v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04139</id>
        <link href="http://arxiv.org/abs/2107.04139"/>
        <updated>2021-07-12T01:55:17.032Z</updated>
        <summary type="html"><![CDATA[Vehicle routing problems (VRPs) are a class of combinatorial problems with
wide practical applications. While previous heuristic or learning-based works
achieve decent solutions on small problem instances of up to 100 customers,
their performance does not scale to large problems. This article presents a
novel learning-augmented local search algorithm to solve large-scale VRP. The
method iteratively improves the solution by identifying appropriate subproblems
and $\textit{delegating}$ their improvement to a black box subsolver. At each
step, we leverage spatial locality to consider only a linear number of
subproblems, rather than exponential. We frame subproblem selection as a
regression problem and train a Transformer on a generated training set of
problem instances. We show that our method achieves state-of-the-art
performance, with a speed-up of up to 15 times over strong baselines, on VRPs
with sizes ranging from 500 to 3000.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Sirui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zhongxia Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Cathy Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedAdapt: Adaptive Offloading for IoT Devices in Federated Learning. (arXiv:2107.04271v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2107.04271</id>
        <link href="http://arxiv.org/abs/2107.04271"/>
        <updated>2021-07-12T01:55:17.026Z</updated>
        <summary type="html"><![CDATA[Applying Federated Learning (FL) on Internet-of-Things devices is
necessitated by the large volumes of data they produce and growing concerns of
data privacy. However, there are three challenges that need to be addressed to
make FL efficient: (i) execute on devices with limited computational
capabilities, (ii) account for stragglers due to computational heterogeneity of
devices, and (iii) adapt to the changing network bandwidths. This paper
presents FedAdapt, an adaptive offloading FL framework to mitigate the
aforementioned challenges. FedAdapt accelerates local training in
computationally constrained devices by leveraging layer offloading of deep
neural networks (DNNs) to servers. Further, FedAdapt adopts reinforcement
learning-based optimization and clustering to adaptively identify which layers
of the DNN should be offloaded for each individual device on to a server to
tackle the challenges of computational heterogeneity and changing network
bandwidth. Experimental studies are carried out on a lab-based testbed
comprising five IoT devices. By offloading a DNN from the device to the server
FedAdapt reduces the training time of a typical IoT device by over half
compared to classic FL. The training time of extreme stragglers and the overall
training time can be reduced by up to 57%. Furthermore, with changing network
bandwidth, FedAdapt is demonstrated to reduce the training time by up to 40%
when compared to classic FL, without sacrificing accuracy. FedAdapt can be
downloaded from https://github.com/qub-blesson/FedAdapt.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Di Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullah_R/0/1/0/all/0/1"&gt;Rehmat Ullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harvey_P/0/1/0/all/0/1"&gt;Paul Harvey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kilpatrick_P/0/1/0/all/0/1"&gt;Peter Kilpatrick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spence_I/0/1/0/all/0/1"&gt;Ivor Spence&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varghese_B/0/1/0/all/0/1"&gt;Blesson Varghese&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lithography Hotspot Detection via Heterogeneous Federated Learning with Local Adaptation. (arXiv:2107.04367v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04367</id>
        <link href="http://arxiv.org/abs/2107.04367"/>
        <updated>2021-07-12T01:55:17.016Z</updated>
        <summary type="html"><![CDATA[As technology scaling is approaching the physical limit, lithography hotspot
detection has become an essential task in design for manufacturability. While
the deployment of pattern matching or machine learning in hotspot detection can
help save significant simulation time, such methods typically demand for
non-trivial quality data to build the model, which most design houses are short
of. Moreover, the design houses are also unwilling to directly share such data
with the other houses to build a unified model, which can be ineffective for
the design house with unique design patterns due to data insufficiency. On the
other hand, with data homogeneity in each design house, the locally trained
models can be easily over-fitted, losing generalization ability and robustness.
In this paper, we propose a heterogeneous federated learning framework for
lithography hotspot detection that can address the aforementioned issues. On
one hand, the framework can build a more robust centralized global sub-model
through heterogeneous knowledge sharing while keeping local data private. On
the other hand, the global sub-model can be combined with a local sub-model to
better adapt to local data heterogeneity. The experimental results show that
the proposed framework can overcome the challenge of non-independent and
identically distributed (non-IID) data and heterogeneous communication to
achieve very high performance in comparison to other state-of-the-art methods
while guaranteeing a good convergence rate in various scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xuezhong Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1"&gt;Jingyu Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jinming Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiran Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuo_C/0/1/0/all/0/1"&gt;Cheng Zhuo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Domain Adaptation with Self-Training for EEG-based Sleep Stage Classification. (arXiv:2107.04470v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04470</id>
        <link href="http://arxiv.org/abs/2107.04470"/>
        <updated>2021-07-12T01:55:17.009Z</updated>
        <summary type="html"><![CDATA[Sleep staging is of great importance in the diagnosis and treatment of sleep
disorders. Recently, numerous data driven deep learning models have been
proposed for automatic sleep staging. They mainly rely on the assumption that
training and testing data are drawn from the same distribution which may not
hold in real-world scenarios. Unsupervised domain adaption (UDA) has been
recently developed to handle this domain shift problem. However, previous UDA
methods applied for sleep staging has two main limitations. First, they rely on
a totally shared model for the domain alignment, which may lose the
domain-specific information during feature extraction. Second, they only align
the source and target distributions globally without considering the class
information in the target domain, which hinders the classification performance
of the model. In this work, we propose a novel adversarial learning framework
to tackle the domain shift problem in the unlabeled target domain. First, we
develop unshared attention mechanisms to preserve the domain-specific features
in the source and target domains. Second, we design a self-training strategy to
align the fine-grained class distributions for the source and target domains
via target domain pseudo labels. We also propose dual distinct classifiers to
increase the robustness and quality of the pseudo labels. The experimental
results on six cross-domain scenarios validate the efficacy of our proposed
framework for sleep staging and its advantage over state-of-the-art UDA
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eldele_E/0/1/0/all/0/1"&gt;Emadeldeen Eldele&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ragab_M/0/1/0/all/0/1"&gt;Mohamed Ragab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhenghua Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1"&gt;Min Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwoh_C/0/1/0/all/0/1"&gt;Chee-Keong Kwoh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoli Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1"&gt;Cuntai Guan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StyleCariGAN: Caricature Generation via StyleGAN Feature Map Modulation. (arXiv:2107.04331v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04331</id>
        <link href="http://arxiv.org/abs/2107.04331"/>
        <updated>2021-07-12T01:55:16.992Z</updated>
        <summary type="html"><![CDATA[We present a caricature generation framework based on shape and style
manipulation using StyleGAN. Our framework, dubbed StyleCariGAN, automatically
creates a realistic and detailed caricature from an input photo with optional
controls on shape exaggeration degree and color stylization type. The key
component of our method is shape exaggeration blocks that are used for
modulating coarse layer feature maps of StyleGAN to produce desirable
caricature shape exaggerations. We first build a layer-mixed StyleGAN for
photo-to-caricature style conversion by swapping fine layers of the StyleGAN
for photos to the corresponding layers of the StyleGAN trained to generate
caricatures. Given an input photo, the layer-mixed model produces detailed
color stylization for a caricature but without shape exaggerations. We then
append shape exaggeration blocks to the coarse layers of the layer-mixed model
and train the blocks to create shape exaggerations while preserving the
characteristic appearances of the input. Experimental results show that our
StyleCariGAN generates realistic and detailed caricatures compared to the
current state-of-the-art methods. We demonstrate StyleCariGAN also supports
other StyleGAN-based image manipulations, such as facial expression control.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jang_W/0/1/0/all/0/1"&gt;Wonjong Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ju_G/0/1/0/all/0/1"&gt;Gwangjin Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_Y/0/1/0/all/0/1"&gt;Yucheol Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jiaolong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1"&gt;Xin Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seungyong Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attend2Pack: Bin Packing through Deep Reinforcement Learning with Attention. (arXiv:2107.04333v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04333</id>
        <link href="http://arxiv.org/abs/2107.04333"/>
        <updated>2021-07-12T01:55:16.985Z</updated>
        <summary type="html"><![CDATA[This paper seeks to tackle the bin packing problem (BPP) through a learning
perspective. Building on self-attention-based encoding and deep reinforcement
learning algorithms, we propose a new end-to-end learning model for this task
of interest. By decomposing the combinatorial action space, as well as
utilizing a new training technique denoted as prioritized oversampling, which
is a general scheme to speed up on-policy learning, we achieve state-of-the-art
performance in a range of experimental settings. Moreover, although the
proposed approach attend2pack targets offline-BPP, we strip our method down to
the strict online-BPP setting where it is also able to achieve state-of-the-art
performance. With a set of ablation studies as well as comparisons against a
range of previous works, we hope to offer as a valid baseline approach to this
field of study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zi_B/0/1/0/all/0/1"&gt;Bin Zi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1"&gt;Xiaoyu Ge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scopeformer: n-CNN-ViT Hybrid Model for Intracranial Hemorrhage Classification. (arXiv:2107.04575v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04575</id>
        <link href="http://arxiv.org/abs/2107.04575"/>
        <updated>2021-07-12T01:55:16.979Z</updated>
        <summary type="html"><![CDATA[We propose a feature generator backbone composed of an ensemble of
convolutional neuralnetworks (CNNs) to improve the recently emerging Vision
Transformer (ViT) models. We tackled the RSNA intracranial hemorrhage
classification problem, i.e., identifying various hemorrhage types from
computed tomography (CT) slices. We show that by gradually stacking several
feature maps extracted using multiple Xception CNNs, we can develop a
feature-rich input for the ViT model. Our approach allowed the ViT model to pay
attention to relevant features at multiple levels. Moreover, pretraining the n
CNNs using various paradigms leads to a diverse feature set and further
improves the performance of the proposed n-CNN-ViT. We achieved a test accuracy
of 98.04% with a weighted logarithmic loss value of 0.0708. The proposed
architecture is modular and scalable in both the number of CNNs used for
feature extraction and the size of the ViT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Barhoumi_Y/0/1/0/all/0/1"&gt;Yassine Barhoumi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ghulam_R/0/1/0/all/0/1"&gt;Rasool Ghulam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Robust Active Feature Acquisition. (arXiv:2107.04163v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04163</id>
        <link href="http://arxiv.org/abs/2107.04163"/>
        <updated>2021-07-12T01:55:16.972Z</updated>
        <summary type="html"><![CDATA[Truly intelligent systems are expected to make critical decisions with
incomplete and uncertain data. Active feature acquisition (AFA), where features
are sequentially acquired to improve the prediction, is a step towards this
goal. However, current AFA models all deal with a small set of candidate
features and have difficulty scaling to a large feature space. Moreover, they
are ignorant about the valid domains where they can predict confidently, thus
they can be vulnerable to out-of-distribution (OOD) inputs. In order to remedy
these deficiencies and bring AFA models closer to practical use, we propose
several techniques to advance the current AFA approaches. Our framework can
easily handle a large number of features using a hierarchical acquisition
policy and is more robust to OOD inputs with the help of an OOD detector for
partially observed data. Extensive experiments demonstrate the efficacy of our
framework over strong baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1"&gt;Siyuan Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliva_J/0/1/0/all/0/1"&gt;Junier B. Oliva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Event-Based Feature Tracking in Continuous Time with Sliding Window Optimization. (arXiv:2107.04536v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04536</id>
        <link href="http://arxiv.org/abs/2107.04536"/>
        <updated>2021-07-12T01:55:16.966Z</updated>
        <summary type="html"><![CDATA[We propose a novel method for continuous-time feature tracking in event
cameras. To this end, we track features by aligning events along an estimated
trajectory in space-time such that the projection on the image plane results in
maximally sharp event patch images. The trajectory is parameterized by $n^{th}$
order B-splines, which are continuous up to $(n-2)^{th}$ derivative. In
contrast to previous work, we optimize the curve parameters in a sliding window
fashion. On a public dataset we experimentally confirm that the proposed
sliding-window B-spline optimization leads to longer and more accurate feature
tracks than in previous work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chui_J/0/1/0/all/0/1"&gt;Jason Chui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klenk_S/0/1/0/all/0/1"&gt;Simon Klenk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1"&gt;Daniel Cremers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the Distributions of Aggregation Layers in Deep Neural Networks. (arXiv:2107.04458v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04458</id>
        <link href="http://arxiv.org/abs/2107.04458"/>
        <updated>2021-07-12T01:55:16.960Z</updated>
        <summary type="html"><![CDATA[The process of aggregation is ubiquitous in almost all deep nets models. It
functions as an important mechanism for consolidating deep features into a more
compact representation, whilst increasing robustness to overfitting and
providing spatial invariance in deep nets. In particular, the proximity of
global aggregation layers to the output layers of DNNs mean that aggregated
features have a direct influence on the performance of a deep net. A better
understanding of this relationship can be obtained using information theoretic
methods. However, this requires the knowledge of the distributions of the
activations of aggregation layers. To achieve this, we propose a novel
mathematical formulation for analytically modelling the probability
distributions of output values of layers involved with deep feature
aggregation. An important outcome is our ability to analytically predict the
KL-divergence of output nodes in a DNN. We also experimentally verify our
theoretical predictions against empirical observations across a range of
different classification tasks and datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ong_E/0/1/0/all/0/1"&gt;Eng-Jon Ong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Husain_S/0/1/0/all/0/1"&gt;Sameed Husain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bober_M/0/1/0/all/0/1"&gt;Miroslaw Bober&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sli2Vol: Annotate a 3D Volume from a Single Slice with Self-Supervised Learning. (arXiv:2105.12722v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12722</id>
        <link href="http://arxiv.org/abs/2105.12722"/>
        <updated>2021-07-12T01:55:16.942Z</updated>
        <summary type="html"><![CDATA[The objective of this work is to segment any arbitrary structures of interest
(SOI) in 3D volumes by only annotating a single slice, (i.e. semi-automatic 3D
segmentation). We show that high accuracy can be achieved by simply propagating
the 2D slice segmentation with an affinity matrix between consecutive slices,
which can be learnt in a self-supervised manner, namely slice reconstruction.
Specifically, we compare the proposed framework, termed as Sli2Vol, with
supervised approaches and two other unsupervised/ self-supervised slice
registration approaches, on 8 public datasets (both CT and MRI scans), spanning
9 different SOIs. Without any parameter-tuning, the same model achieves
superior performance with Dice scores (0-100 scale) of over 80 for most of the
benchmarks, including the ones that are unseen during training. Our results
show generalizability of the proposed approach across data from different
machines and with different SOIs: a major use case of semi-automatic
segmentation methods where fully supervised approaches would normally struggle.
The source code will be made publicly available at
https://github.com/pakheiyeung/Sli2Vol.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yeung_P/0/1/0/all/0/1"&gt;Pak-Hei Yeung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namburete_A/0/1/0/all/0/1"&gt;Ana I.L. Namburete&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1"&gt;Weidi Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seven Basic Expression Recognition Using ResNet-18. (arXiv:2107.04569v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04569</id>
        <link href="http://arxiv.org/abs/2107.04569"/>
        <updated>2021-07-12T01:55:16.935Z</updated>
        <summary type="html"><![CDATA[We propose to use a ResNet-18 architecture that was pre-trained on the FER+
dataset for tackling the problem of affective behavior analysis in-the-wild
(ABAW) for classification of the seven basic expressions, namely, neutral,
anger, disgust, fear, happiness, sadness and surprise. As part of the second
workshop and competition on affective behavior analysis in-the-wild (ABAW2), a
database consisting of 564 videos with around 2.8M frames is provided along
with labels for these seven basic expressions. We resampled the dataset to
counter class-imbalances by under-sampling the over-represented classes and
over-sampling the under-represented classes along with class-wise weights. To
avoid overfitting we performed data-augmentation and used L2 regularisation.
Our classifier reaches an ABAW2 score of 0.4 and therefore exceeds the baseline
results provided by the hosts of the competition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Satnam Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schicker_D/0/1/0/all/0/1"&gt;Doris Schicker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the Distributions of Aggregation Layers in Deep Neural Networks. (arXiv:2107.04458v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04458</id>
        <link href="http://arxiv.org/abs/2107.04458"/>
        <updated>2021-07-12T01:55:16.929Z</updated>
        <summary type="html"><![CDATA[The process of aggregation is ubiquitous in almost all deep nets models. It
functions as an important mechanism for consolidating deep features into a more
compact representation, whilst increasing robustness to overfitting and
providing spatial invariance in deep nets. In particular, the proximity of
global aggregation layers to the output layers of DNNs mean that aggregated
features have a direct influence on the performance of a deep net. A better
understanding of this relationship can be obtained using information theoretic
methods. However, this requires the knowledge of the distributions of the
activations of aggregation layers. To achieve this, we propose a novel
mathematical formulation for analytically modelling the probability
distributions of output values of layers involved with deep feature
aggregation. An important outcome is our ability to analytically predict the
KL-divergence of output nodes in a DNN. We also experimentally verify our
theoretical predictions against empirical observations across a range of
different classification tasks and datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ong_E/0/1/0/all/0/1"&gt;Eng-Jon Ong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Husain_S/0/1/0/all/0/1"&gt;Sameed Husain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bober_M/0/1/0/all/0/1"&gt;Miroslaw Bober&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memes in the Wild: Assessing the Generalizability of the Hateful Memes Challenge Dataset. (arXiv:2107.04313v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04313</id>
        <link href="http://arxiv.org/abs/2107.04313"/>
        <updated>2021-07-12T01:55:16.922Z</updated>
        <summary type="html"><![CDATA[Hateful memes pose a unique challenge for current machine learning systems
because their message is derived from both text- and visual-modalities. To this
effect, Facebook released the Hateful Memes Challenge, a dataset of memes with
pre-extracted text captions, but it is unclear whether these synthetic examples
generalize to `memes in the wild'. In this paper, we collect hateful and
non-hateful memes from Pinterest to evaluate out-of-sample performance on
models pre-trained on the Facebook dataset. We find that memes in the wild
differ in two key aspects: 1) Captions must be extracted via OCR, injecting
noise and diminishing performance of multimodal models, and 2) Memes are more
diverse than `traditional memes', including screenshots of conversations or
text on a plain background. This paper thus serves as a reality check for the
current benchmark of hateful meme detection and its applicability for detecting
real world hate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1"&gt;Hannah Rose Kirk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jun_Y/0/1/0/all/0/1"&gt;Yennie Jun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rauba_P/0/1/0/all/0/1"&gt;Paulius Rauba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wachtel_G/0/1/0/all/0/1"&gt;Gal Wachtel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruining Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1"&gt;Xingjian Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Broestl_N/0/1/0/all/0/1"&gt;Noah Broestl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doff_Sotta_M/0/1/0/all/0/1"&gt;Martin Doff-Sotta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shtedritski_A/0/1/0/all/0/1"&gt;Aleksandar Shtedritski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1"&gt;Yuki M. Asano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantitative Evaluation of Explainable Graph Neural Networks for Molecular Property Prediction. (arXiv:2107.04119v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2107.04119</id>
        <link href="http://arxiv.org/abs/2107.04119"/>
        <updated>2021-07-12T01:55:16.905Z</updated>
        <summary type="html"><![CDATA[Advances in machine learning have led to graph neural network-based methods
for drug discovery, yielding promising results in molecular design, chemical
synthesis planning, and molecular property prediction. However, current graph
neural networks (GNNs) remain of limited acceptance in drug discovery is
limited due to their lack of interpretability. Although this major weakness has
been mitigated by the development of explainable artificial intelligence (XAI)
techniques, the "ground truth" assignment in most explainable tasks ultimately
rests with subjective judgments by humans so that the quality of model
interpretation is hard to evaluate in quantity. In this work, we first build
three levels of benchmark datasets to quantitatively assess the
interpretability of the state-of-the-art GNN models. Then we implemented recent
XAI methods in combination with different GNN algorithms to highlight the
benefits, limitations, and future opportunities for drug discovery. As a
result, GradInput and IG generally provide the best model interpretability for
GNNs, especially when combined with GraphNet and CMPNN. The integrated and
developed XAI package is fully open-sourced and can be used by practitioners to
train new models on other drug discovery tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Rao_J/0/1/0/all/0/1"&gt;Jiahua Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Shuangjia Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yuedong Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intuitively Assessing ML Model Reliability through Example-Based Explanations and Editing Model Inputs. (arXiv:2102.08540v2 [cs.HC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08540</id>
        <link href="http://arxiv.org/abs/2102.08540"/>
        <updated>2021-07-12T01:55:16.899Z</updated>
        <summary type="html"><![CDATA[Interpretability methods aim to help users build trust in and understand the
capabilities of machine learning models. However, existing approaches often
rely on abstract, complex visualizations that poorly map to the task at hand or
require non-trivial ML expertise to interpret. Here, we present two visual
analytics modules that facilitate an intuitive assessment of model reliability.
To help users better characterize and reason about a model's uncertainty, we
visualize raw and aggregate information about a given input's nearest
neighbors. Using an interactive editor, users can manipulate this input in
semantically-meaningful ways, determine the effect on the output, and compare
against their prior expectations. We evaluate our interface using an
electrocardiogram beat classification case study. Compared to a baseline
feature importance interface, we find that 14 physicians are better able to
align the model's uncertainty with domain-relevant factors and build intuition
about its capabilities and limitations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suresh_H/0/1/0/all/0/1"&gt;Harini Suresh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_K/0/1/0/all/0/1"&gt;Kathleen M. Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guttag_J/0/1/0/all/0/1"&gt;John V. Guttag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Satyanarayan_A/0/1/0/all/0/1"&gt;Arvind Satyanarayan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stock Market Analysis with Text Data: A Review. (arXiv:2106.12985v2 [q-fin.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12985</id>
        <link href="http://arxiv.org/abs/2106.12985"/>
        <updated>2021-07-12T01:55:16.891Z</updated>
        <summary type="html"><![CDATA[Stock market movements are influenced by public and private information
shared through news articles, company reports, and social media discussions.
Analyzing these vast sources of data can give market participants an edge to
make profit. However, the majority of the studies in the literature are based
on traditional approaches that come short in analyzing unstructured, vast
textual data. In this study, we provide a review on the immense amount of
existing literature of text-based stock market analysis. We present input data
types and cover main textual data sources and variations. Feature
representation techniques are then presented. Then, we cover the analysis
techniques and create a taxonomy of the main stock market forecast models.
Importantly, we discuss representative work in each category of the taxonomy,
analyzing their respective contributions. Finally, this paper shows the
findings on unaddressed open problems and gives suggestions for future work.
The aim of this study is to survey the main stock market analysis models, text
representation techniques for financial market prediction, shortcomings of
existing techniques, and propose promising directions for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Fataliyev_K/0/1/0/all/0/1"&gt;Kamaladdin Fataliyev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Chivukula_A/0/1/0/all/0/1"&gt;Aneesh Chivukula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Prasad_M/0/1/0/all/0/1"&gt;Mukesh Prasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SHAP values for Explaining CNN-based Text Classification Models. (arXiv:2008.11825v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.11825</id>
        <link href="http://arxiv.org/abs/2008.11825"/>
        <updated>2021-07-12T01:55:16.884Z</updated>
        <summary type="html"><![CDATA[Deep neural networks are increasingly used in natural language processing
(NLP) models. However, the need to interpret and explain the results from
complex algorithms are limiting their widespread adoption in regulated
industries such as banking. There has been recent work on interpretability of
machine learning algorithms with structured data. But there are only limited
techniques for NLP applications where the problem is more challenging due to
the size of the vocabulary, high-dimensional nature, and the need to consider
textual coherence and language structure. This paper develops a methodology to
compute SHAP values for local explainability of CNN-based text classification
models. The approach is also extended to compute global scores to assess the
importance of features. The results are illustrated on sentiment analysis of
Amazon Electronic Review data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wei Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_T/0/1/0/all/0/1"&gt;Tarun Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_V/0/1/0/all/0/1"&gt;Vijayan N. Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sudjianto_A/0/1/0/all/0/1"&gt;Agus Sudjianto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured Model Pruning of Convolutional Networks on Tensor Processing Units. (arXiv:2107.04191v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04191</id>
        <link href="http://arxiv.org/abs/2107.04191"/>
        <updated>2021-07-12T01:55:16.877Z</updated>
        <summary type="html"><![CDATA[The deployment of convolutional neural networks is often hindered by high
computational and storage requirements. Structured model pruning is a promising
approach to alleviate these requirements. Using the VGG-16 model as an example,
we measure the accuracy-efficiency trade-off for various structured model
pruning methods and datasets (CIFAR-10 and ImageNet) on Tensor Processing Units
(TPUs). To measure the actual performance of models, we develop a structured
model pruning library for TensorFlow2 to modify models in place (instead of
adding mask layers). We show that structured model pruning can significantly
improve model memory usage and speed on TPUs without losing accuracy,
especially for small datasets (e.g., CIFAR-10).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kongtao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franko_K/0/1/0/all/0/1"&gt;Ken Franko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sang_R/0/1/0/all/0/1"&gt;Ruoxin Sang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modality specific U-Net variants for biomedical image segmentation: A survey. (arXiv:2107.04537v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04537</id>
        <link href="http://arxiv.org/abs/2107.04537"/>
        <updated>2021-07-12T01:55:16.869Z</updated>
        <summary type="html"><![CDATA[With the advent of advancements in deep learning approaches, such as deep
convolution neural network, residual neural network, adversarial network; U-Net
architectures are most widely utilized in biomedical image segmentation to
address the automation in identification and detection of the target regions or
sub-regions. In recent studies, U-Net based approaches have illustrated
state-of-the-art performance in different applications for the development of
computer-aided diagnosis systems for early diagnosis and treatment of diseases
such as brain tumor, lung cancer, alzheimer, breast cancer, etc. This article
contributes to present the success of these approaches by describing the U-Net
framework, followed by the comprehensive analysis of the U-Net variants for
different medical imaging or modalities such as magnetic resonance imaging,
X-ray, computerized tomography/computerized axial tomography, ultrasound,
positron emission tomography, etc. Besides, this article also highlights the
contribution of U-Net based frameworks in the on-going pandemic, severe acute
respiratory syndrome coronavirus 2 (SARS-CoV-2) also known as COVID-19.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Punn_N/0/1/0/all/0/1"&gt;Narinder Singh Punn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sonali Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Finite-temperature Kohn-Sham Density Functional Theory with Deep Neural Networks. (arXiv:2010.04905v2 [cond-mat.mtrl-sci] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04905</id>
        <link href="http://arxiv.org/abs/2010.04905"/>
        <updated>2021-07-12T01:55:16.861Z</updated>
        <summary type="html"><![CDATA[We present a numerical modeling workflow based on machine learning (ML) which
reproduces the the total energies produced by Kohn-Sham density functional
theory (DFT) at finite electronic temperature to within chemical accuracy at
negligible computational cost. Based on deep neural networks, our workflow
yields the local density of states (LDOS) for a given atomic configuration.
From the LDOS, spatially-resolved, energy-resolved, and integrated quantities
can be calculated, including the DFT total free energy, which serves as the
Born-Oppenheimer potential energy surface for the atoms. We demonstrate the
efficacy of this approach for both solid and liquid metals and compare results
between independent and unified machine-learning models for solid and liquid
aluminum. Our machine-learning density functional theory framework opens up the
path towards multiscale materials modeling for matter under ambient and extreme
conditions at a computational scale and cost that is unattainable with current
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Ellis_J/0/1/0/all/0/1"&gt;J. Austin Ellis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Fiedler_L/0/1/0/all/0/1"&gt;Lenz Fiedler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Popoola_G/0/1/0/all/0/1"&gt;Gabriel A. Popoola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Modine_N/0/1/0/all/0/1"&gt;Normand A. Modine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Stephens_J/0/1/0/all/0/1"&gt;J. Adam Stephens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Thompson_A/0/1/0/all/0/1"&gt;Aidan P. Thompson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Cangi_A/0/1/0/all/0/1"&gt;Attila Cangi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Rajamanickam_S/0/1/0/all/0/1"&gt;Sivasankaran Rajamanickam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sli2Vol: Annotate a 3D Volume from a Single Slice with Self-Supervised Learning. (arXiv:2105.12722v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12722</id>
        <link href="http://arxiv.org/abs/2105.12722"/>
        <updated>2021-07-12T01:55:16.854Z</updated>
        <summary type="html"><![CDATA[The objective of this work is to segment any arbitrary structures of interest
(SOI) in 3D volumes by only annotating a single slice, (i.e. semi-automatic 3D
segmentation). We show that high accuracy can be achieved by simply propagating
the 2D slice segmentation with an affinity matrix between consecutive slices,
which can be learnt in a self-supervised manner, namely slice reconstruction.
Specifically, we compare the proposed framework, termed as Sli2Vol, with
supervised approaches and two other unsupervised/ self-supervised slice
registration approaches, on 8 public datasets (both CT and MRI scans), spanning
9 different SOIs. Without any parameter-tuning, the same model achieves
superior performance with Dice scores (0-100 scale) of over 80 for most of the
benchmarks, including the ones that are unseen during training. Our results
show generalizability of the proposed approach across data from different
machines and with different SOIs: a major use case of semi-automatic
segmentation methods where fully supervised approaches would normally struggle.
The source code will be made publicly available at
https://github.com/pakheiyeung/Sli2Vol.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yeung_P/0/1/0/all/0/1"&gt;Pak-Hei Yeung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namburete_A/0/1/0/all/0/1"&gt;Ana I.L. Namburete&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1"&gt;Weidi Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring and Improving Model-Moderator Collaboration using Uncertainty Estimation. (arXiv:2107.04212v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04212</id>
        <link href="http://arxiv.org/abs/2107.04212"/>
        <updated>2021-07-12T01:55:16.828Z</updated>
        <summary type="html"><![CDATA[Content moderation is often performed by a collaboration between humans and
machine learning models. However, it is not well understood how to design the
collaborative process so as to maximize the combined moderator-model system
performance. This work presents a rigorous study of this problem, focusing on
an approach that incorporates model uncertainty into the collaborative process.
First, we introduce principled metrics to describe the performance of the
collaborative system under capacity constraints on the human moderator,
quantifying how efficiently the combined system utilizes human decisions. Using
these metrics, we conduct a large benchmark study evaluating the performance of
state-of-the-art uncertainty models under different collaborative review
strategies. We find that an uncertainty-based strategy consistently outperforms
the widely used strategy based on toxicity scores, and moreover that the choice
of review strategy drastically changes the overall system performance. Our
results demonstrate the importance of rigorous metrics for understanding and
developing effective moderator-model systems for content moderation, as well as
the utility of uncertainty estimation in this domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kivlichan_I/0/1/0/all/0/1"&gt;Ian D. Kivlichan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jeremiah Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasserman_L/0/1/0/all/0/1"&gt;Lucy Vasserman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-headed Neural Ensemble Search. (arXiv:2107.04369v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04369</id>
        <link href="http://arxiv.org/abs/2107.04369"/>
        <updated>2021-07-12T01:55:16.817Z</updated>
        <summary type="html"><![CDATA[Ensembles of CNN models trained with different seeds (also known as Deep
Ensembles) are known to achieve superior performance over a single copy of the
CNN. Neural Ensemble Search (NES) can further boost performance by adding
architectural diversity. However, the scope of NES remains prohibitive under
limited computational resources. In this work, we extend NES to multi-headed
ensembles, which consist of a shared backbone attached to multiple prediction
heads. Unlike Deep Ensembles, these multi-headed ensembles can be trained end
to end, which enables us to leverage one-shot NAS methods to optimize an
ensemble objective. With extensive empirical evaluations, we demonstrate that
multi-headed ensemble search finds robust ensembles 3 times faster, while
having comparable performance to other ensemble search methods, in both
predictive performance and uncertainty calibration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_A/0/1/0/all/0/1"&gt;Ashwin Raaghav Narayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zela_A/0/1/0/all/0/1"&gt;Arber Zela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saikia_T/0/1/0/all/0/1"&gt;Tonmoy Saikia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1"&gt;Thomas Brox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1"&gt;Frank Hutter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ARC: Adversarially Robust Control Policies for Autonomous Vehicles. (arXiv:2107.04487v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04487</id>
        <link href="http://arxiv.org/abs/2107.04487"/>
        <updated>2021-07-12T01:55:16.793Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have demonstrated their capability to learn control
policies for a variety of tasks. However, these neural network-based policies
have been shown to be susceptible to exploitation by adversarial agents.
Therefore, there is a need to develop techniques to learn control policies that
are robust against adversaries. We introduce Adversarially Robust Control
(ARC), which trains the protagonist policy and the adversarial policy
end-to-end on the same loss. The aim of the protagonist is to maximise this
loss, whilst the adversary is attempting to minimise it. We demonstrate the
proposed ARC training in a highway driving scenario, where the protagonist
controls the follower vehicle whilst the adversary controls the lead vehicle.
By training the protagonist against an ensemble of adversaries, it learns a
significantly more robust control policy, which generalises to a variety of
adversarial strategies. The approach is shown to reduce the amount of
collisions against new adversaries by up to 90.25%, compared to the original
policy. Moreover, by utilising an auxiliary distillation loss, we show that the
fine-tuned control policy shows no drop in performance across its original
training distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuutti_S/0/1/0/all/0/1"&gt;Sampo Kuutti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fallah_S/0/1/0/all/0/1"&gt;Saber Fallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1"&gt;Richard Bowden&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extracting the Auditory Attention in a Dual-Speaker Scenario from EEG using a Joint CNN-LSTM Model. (arXiv:2102.03957v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03957</id>
        <link href="http://arxiv.org/abs/2102.03957"/>
        <updated>2021-07-12T01:55:16.782Z</updated>
        <summary type="html"><![CDATA[Human brain performs remarkably well in segregating a particular speaker from
interfering ones in a multi-speaker scenario. It has been recently shown that
we can quantitatively evaluate the segregation capability by modelling the
relationship between the speech signals present in an auditory scene and the
cortical signals of the listener measured using electroencephalography (EEG).
This has opened up avenues to integrate neuro-feedback into hearing aids
whereby the device can infer user's attention and enhance the attended speaker.
Commonly used algorithms to infer the auditory attention are based on linear
systems theory where the speech cues such as envelopes are mapped on to the EEG
signals. Here, we present a joint convolutional neural network (CNN) - long
short-term memory (LSTM) model to infer the auditory attention. Our joint
CNN-LSTM model takes the EEG signals and the spectrogram of the multiple
speakers as inputs and classifies the attention to one of the speakers. We
evaluated the reliability of our neural network using three different datasets
comprising of 61 subjects where, each subject undertook a dual-speaker
experiment. The three datasets analysed corresponded to speech stimuli
presented in three different languages namely German, Danish and Dutch. Using
the proposed joint CNN-LSTM model, we obtained a median decoding accuracy of
77.2% at a trial duration of three seconds. Furthermore, we evaluated the
amount of sparsity that our model can tolerate by means of magnitude pruning
and found that the model can tolerate up to 50% sparsity without substantial
loss of decoding accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuruvila_I/0/1/0/all/0/1"&gt;Ivine Kuruvila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muncke_J/0/1/0/all/0/1"&gt;Jan Muncke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fischer_E/0/1/0/all/0/1"&gt;Eghart Fischer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoppe_U/0/1/0/all/0/1"&gt;Ulrich Hoppe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Spherical k-Means. (arXiv:2107.04074v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04074</id>
        <link href="http://arxiv.org/abs/2107.04074"/>
        <updated>2021-07-12T01:55:16.764Z</updated>
        <summary type="html"><![CDATA[Spherical k-means is a widely used clustering algorithm for sparse and
high-dimensional data such as document vectors. While several improvements and
accelerations have been introduced for the original k-means algorithm, not all
easily translate to the spherical variant: Many acceleration techniques, such
as the algorithms of Elkan and Hamerly, rely on the triangle inequality of
Euclidean distances. However, spherical k-means uses Cosine similarities
instead of distances for computational efficiency. In this paper, we
incorporate the Elkan and Hamerly accelerations to the spherical k-means
algorithm working directly with the Cosines instead of Euclidean distances to
obtain a substantial speedup and evaluate these spherical accelerations on real
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schubert_E/0/1/0/all/0/1"&gt;Erich Schubert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lang_A/0/1/0/all/0/1"&gt;Andreas Lang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feher_G/0/1/0/all/0/1"&gt;Gloria Feher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When is Particle Filtering Efficient for Planning in Partially Observed Linear Dynamical Systems?. (arXiv:2006.05975v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05975</id>
        <link href="http://arxiv.org/abs/2006.05975"/>
        <updated>2021-07-12T01:55:16.747Z</updated>
        <summary type="html"><![CDATA[Particle filtering is a popular method for inferring latent states in
stochastic dynamical systems, whose theoretical properties have been well
studied in machine learning and statistics communities. In many control
problems, e.g., partially observed linear dynamical systems (POLDS), oftentimes
the inferred latent state is further used for planning at each step. This paper
initiates a rigorous study on the efficiency of particle filtering for
sequential planning, and gives the first particle complexity bounds. Though
errors in past actions may affect the future, we are able to bound the number
of particles needed so that the long-run reward of the policy based on particle
filtering is close to that based on exact inference. In particular, we show
that, in stable systems, polynomially many particles suffice. Key in our proof
is a coupling of the ideal sequence based on the exact planning and the
sequence generated by approximate planning based on particle filtering. We
believe this technique can be useful in other sequential decision-making
problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1"&gt;Simon S. Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Wei Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhiyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_R/0/1/0/all/0/1"&gt;Ruoqi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1"&gt;Zhao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiajun Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[White-Box Cartoonization Using An Extended GAN Framework. (arXiv:2107.04551v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04551</id>
        <link href="http://arxiv.org/abs/2107.04551"/>
        <updated>2021-07-12T01:55:16.741Z</updated>
        <summary type="html"><![CDATA[In the present study, we propose to implement a new framework for estimating
generative models via an adversarial process to extend an existing GAN
framework and develop a white-box controllable image cartoonization, which can
generate high-quality cartooned images/videos from real-world photos and
videos. The learning purposes of our system are based on three distinct
representations: surface representation, structure representation, and texture
representation. The surface representation refers to the smooth surface of the
images. The structure representation relates to the sparse colour blocks and
compresses generic content. The texture representation shows the texture,
curves, and features in cartoon images. Generative Adversarial Network (GAN)
framework decomposes the images into different representations and learns from
them to generate cartoon images. This decomposition makes the framework more
controllable and flexible which allows users to make changes based on the
required output. This approach overcomes any previous system in terms of
maintaining clarity, colours, textures, shapes of images yet showing the
characteristics of cartoon images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1"&gt;Amey Thakur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rizvi_H/0/1/0/all/0/1"&gt;Hasan Rizvi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Satish_M/0/1/0/all/0/1"&gt;Mega Satish&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fixed-Budget Best-Arm Identification in Contextual Bandits: A Static-Adaptive Algorithm. (arXiv:2106.04763v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04763</id>
        <link href="http://arxiv.org/abs/2106.04763"/>
        <updated>2021-07-12T01:55:16.720Z</updated>
        <summary type="html"><![CDATA[We study the problem of best-arm identification (BAI) in contextual bandits
in the fixed-budget setting. We propose a general successive elimination
algorithm that proceeds in stages and eliminates a fixed fraction of suboptimal
arms in each stage. This design takes advantage of the strengths of static and
adaptive allocations. We analyze the algorithm in linear models and obtain a
better error bound than prior work. We also apply it to generalized linear
models (GLMs) and bound its error. This is the first BAI algorithm for GLMs in
the fixed-budget setting. Our extensive numerical experiments show that our
algorithm outperforms the state of art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Azizi_M/0/1/0/all/0/1"&gt;Mohammad Javad Azizi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1"&gt;Branislav Kveton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1"&gt;Mohammad Ghavamzadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ANCER: Anisotropic Certification via Sample-wise Volume Maximization. (arXiv:2107.04570v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04570</id>
        <link href="http://arxiv.org/abs/2107.04570"/>
        <updated>2021-07-12T01:55:16.713Z</updated>
        <summary type="html"><![CDATA[Randomized smoothing has recently emerged as an effective tool that enables
certification of deep neural network classifiers at scale. All prior art on
randomized smoothing has focused on isotropic $\ell_p$ certification, which has
the advantage of yielding certificates that can be easily compared among
isotropic methods via $\ell_p$-norm radius. However, isotropic certification
limits the region that can be certified around an input to worst-case
adversaries, \ie it cannot reason about other "close", potentially large,
constant prediction safe regions. To alleviate this issue, (i) we theoretically
extend the isotropic randomized smoothing $\ell_1$ and $\ell_2$ certificates to
their generalized anisotropic counterparts following a simplified analysis.
Moreover, (ii) we propose evaluation metrics allowing for the comparison of
general certificates - a certificate is superior to another if it certifies a
superset region - with the quantification of each certificate through the
volume of the certified region. We introduce ANCER, a practical framework for
obtaining anisotropic certificates for a given test set sample via volume
maximization. Our empirical results demonstrate that ANCER achieves
state-of-the-art $\ell_1$ and $\ell_2$ certified accuracy on both CIFAR-10 and
ImageNet at multiple radii, while certifying substantially larger regions in
terms of volume, thus highlighting the benefits of moving away from isotropic
analysis. Code used in our experiments is available in
https://github.com/MotasemAlfarra/ANCER.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eiras_F/0/1/0/all/0/1"&gt;Francisco Eiras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1"&gt;Motasem Alfarra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1"&gt;M. Pawan Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip H. S. Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1"&gt;Puneet K. Dokania&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1"&gt;Bernard Ghanem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1"&gt;Adel Bibi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Dropout Discriminator for Domain Adaptation. (arXiv:2107.04231v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04231</id>
        <link href="http://arxiv.org/abs/2107.04231"/>
        <updated>2021-07-12T01:55:16.706Z</updated>
        <summary type="html"><![CDATA[Adaptation of a classifier to new domains is one of the challenging problems
in machine learning. This has been addressed using many deep and non-deep
learning based methods. Among the methodologies used, that of adversarial
learning is widely applied to solve many deep learning problems along with
domain adaptation. These methods are based on a discriminator that ensures
source and target distributions are close. However, here we suggest that rather
than using a point estimate obtaining by a single discriminator, it would be
useful if a distribution based on ensembles of discriminators could be used to
bridge this gap. This could be achieved using multiple classifiers or using
traditional ensemble methods. In contrast, we suggest that a Monte Carlo
dropout based ensemble discriminator could suffice to obtain the distribution
based discriminator. Specifically, we propose a curriculum based dropout
discriminator that gradually increases the variance of the sample based
distribution and the corresponding reverse gradients are used to align the
source and target feature representations. An ensemble of discriminators helps
the model to learn the data distribution efficiently. It also provides a better
gradient estimates to train the feature extractor. The detailed results and
thorough ablation analysis show that our model outperforms state-of-the-art
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kurmi_V/0/1/0/all/0/1"&gt;Vinod K Kurmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Subramanian_V/0/1/0/all/0/1"&gt;Venkatesh K Subramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1"&gt;Vinay P. Namboodiri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Form2Seq : A Framework for Higher-Order Form Structure Extraction. (arXiv:2107.04419v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04419</id>
        <link href="http://arxiv.org/abs/2107.04419"/>
        <updated>2021-07-12T01:55:16.700Z</updated>
        <summary type="html"><![CDATA[Document structure extraction has been a widely researched area for decades
with recent works performing it as a semantic segmentation task over document
images using fully-convolution networks. Such methods are limited by image
resolution due to which they fail to disambiguate structures in dense regions
which appear commonly in forms. To mitigate this, we propose Form2Seq, a novel
sequence-to-sequence (Seq2Seq) inspired framework for structure extraction
using text, with a specific focus on forms, which leverages relative spatial
arrangement of structures. We discuss two tasks; 1) Classification of low-level
constituent elements (TextBlock and empty fillable Widget) into ten types such
as field captions, list items, and others; 2) Grouping lower-level elements
into higher-order constructs, such as Text Fields, ChoiceFields and
ChoiceGroups, used as information collection mechanism in forms. To achieve
this, we arrange the constituent elements linearly in natural reading order,
feed their spatial and textual representations to Seq2Seq framework, which
sequentially outputs prediction of each element depending on the final task. We
modify Seq2Seq for grouping task and discuss improvements obtained through
cascaded end-to-end training of two tasks versus training in isolation.
Experimental results show the effectiveness of our text-based approach
achieving an accuracy of 90% on classification task and an F1 of 75.82, 86.01,
61.63 on groups discussed above respectively, outperforming segmentation
baselines. Further we show our framework achieves state of the results for
table structure recognition on ICDAR 2013 dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_M/0/1/0/all/0/1"&gt;Milan Aggarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1"&gt;Hiresh Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_M/0/1/0/all/0/1"&gt;Mausoom Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1"&gt;Balaji Krishnamurthy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Likelihood ratio-based policy gradient methods for distorted risk measures: A non-asymptotic analysis. (arXiv:2107.04422v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04422</id>
        <link href="http://arxiv.org/abs/2107.04422"/>
        <updated>2021-07-12T01:55:16.693Z</updated>
        <summary type="html"><![CDATA[We propose policy-gradient algorithms for solving the problem of control in a
risk-sensitive reinforcement learning (RL) context. The objective of our
algorithm is to maximize the distorted risk measure (DRM) of the cumulative
reward in an episodic Markov decision process (MDP). We derive a variant of the
policy gradient theorem that caters to the DRM objective. Using this theorem in
conjunction with a likelihood ratio (LR) based gradient estimation scheme, we
propose policy gradient algorithms for optimizing DRM in both on-policy and
off-policy RL settings. We derive non-asymptotic bounds that establish the
convergence of our algorithms to an approximate stationary point of the DRM
objective.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vijayan_N/0/1/0/all/0/1"&gt;Nithia Vijayan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+A_P/0/1/0/all/0/1"&gt;Prashanth L. A&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Behavior Self-Organization Supports Task Inference for Continual Robot Learning. (arXiv:2107.04533v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.04533</id>
        <link href="http://arxiv.org/abs/2107.04533"/>
        <updated>2021-07-12T01:55:16.687Z</updated>
        <summary type="html"><![CDATA[Recent advances in robot learning have enabled robots to become increasingly
better at mastering a predefined set of tasks. On the other hand, as humans, we
have the ability to learn a growing set of tasks over our lifetime. Continual
robot learning is an emerging research direction with the goal of endowing
robots with this ability. In order to learn new tasks over time, the robot
first needs to infer the task at hand. Task inference, however, has received
little attention in the multi-task learning literature. In this paper, we
propose a novel approach to continual learning of robotic control tasks. Our
approach performs unsupervised learning of behavior embeddings by incrementally
self-organizing demonstrated behaviors. Task inference is made by finding the
nearest behavior embedding to a demonstrated behavior, which is used together
with the environment state as input to a multi-task policy trained with
reinforcement learning to optimize performance over tasks. Unlike previous
approaches, our approach makes no assumptions about task distribution and
requires no task exploration to infer tasks. We evaluate our approach in
experiments with concurrently and sequentially presented tasks and show that it
outperforms other multi-task learning approaches in terms of generalization
performance and convergence speed, particularly in the continual learning
setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hafez_M/0/1/0/all/0/1"&gt;Muhammad Burhan Hafez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1"&gt;Stefan Wermter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[1st Place Solution for YouTubeVOS Challenge 2021:Video Instance Segmentation. (arXiv:2106.06649v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06649</id>
        <link href="http://arxiv.org/abs/2106.06649"/>
        <updated>2021-07-12T01:55:16.668Z</updated>
        <summary type="html"><![CDATA[Video Instance Segmentation (VIS) is a multi-task problem performing
detection, segmentation, and tracking simultaneously. Extended from image set
applications, video data additionally induces the temporal information, which,
if handled appropriately, is very useful to identify and predict object
motions. In this work, we design a unified model to mutually learn these tasks.
Specifically, we propose two modules, named Temporally Correlated Instance
Segmentation (TCIS) and Bidirectional Tracking (BiTrack), to take the benefit
of the temporal correlation between the object's instance masks across adjacent
frames. On the other hand, video data is often redundant due to the frame's
overlap. Our analysis shows that this problem is particularly severe for the
YoutubeVOS-VIS2021 data. Therefore, we propose a Multi-Source Data (MSD)
training mechanism to compensate for the data deficiency. By combining these
techniques with a bag of tricks, the network performance is significantly
boosted compared to the baseline, and outperforms other methods by a
considerable margin on the YoutubeVOS-VIS 2019 and 2021 datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thuy C. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1"&gt;Tuan N. Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phan_N/0/1/0/all/0/1"&gt;Nam LH. Phan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1"&gt;Chuong H. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamazaki_M/0/1/0/all/0/1"&gt;Masayuki Yamazaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamanaka_M/0/1/0/all/0/1"&gt;Masao Yamanaka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NDPNet: A novel non-linear data projection network for few-shot fine-grained image classification. (arXiv:2106.06988v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06988</id>
        <link href="http://arxiv.org/abs/2106.06988"/>
        <updated>2021-07-12T01:55:16.661Z</updated>
        <summary type="html"><![CDATA[Metric-based few-shot fine-grained image classification (FSFGIC) aims to
learn a transferable feature embedding network by estimating the similarities
between query images and support classes from very few examples. In this work,
we propose, for the first time, to introduce the non-linear data projection
concept into the design of FSFGIC architecture in order to address the limited
sample problem in few-shot learning and at the same time to increase the
discriminability of the model for fine-grained image classification.
Specifically, we first design a feature re-abstraction embedding network that
has the ability to not only obtain the required semantic features for effective
metric learning but also re-enhance such features with finer details from input
images. Then the descriptors of the query images and the support classes are
projected into different non-linear spaces in our proposed similarity metric
learning network to learn discriminative projection factors. This design can
effectively operate in the challenging and restricted condition of a FSFGIC
task for making the distance between the samples within the same class smaller
and the distance between samples from different classes larger and for reducing
the coupling relationship between samples from different categories.
Furthermore, a novel similarity measure based on the proposed non-linear data
project is presented for evaluating the relationships of feature information
between a query image and a support set. It is worth to note that our proposed
architecture can be easily embedded into any episodic training mechanisms for
end-to-end training from scratch. Extensive experiments on FSFGIC tasks
demonstrate the superiority of the proposed methods over the state-of-the-art
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weichuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xuefang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1"&gt;Zhe Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yongsheng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changming Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Program and Layout Transformations to enable DNN Operators on Specialized Hardware based on Constraint Programming. (arXiv:2104.04731v3 [cs.DC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04731</id>
        <link href="http://arxiv.org/abs/2104.04731"/>
        <updated>2021-07-12T01:55:16.655Z</updated>
        <summary type="html"><![CDATA[The success of Deep Artificial Neural Networks (DNNs) in many domains created
a rich body of research concerned with hardwareaccelerators for
compute-intensive DNN operators. However, implementing such operators
efficiently with complex hardwareintrinsics such as matrix multiply is a task
not yet automated gracefully. Solving this task often requires joint program
and data layouttransformations. First solutions to this problem have been
proposed, such as TVM, UNIT or ISAMIR, which work on a loop-levelrepresentation
of operators and specify data layout and possible program transformations
before the embedding into the operator isperformed. This top-down approach
creates a tension between exploration range and search space complexity,
especially when alsoexploring data layout transformations such as im2col,
channel packing or padding.In this work, we propose a new approach to this
problem. We created a bottom-up method that allows the joint transformation
ofboth compuation and data layout based on the found embedding. By formulating
the embedding as a constraint satisfaction problemover the scalar dataflow,
every possible embedding solution is contained in the search space. Adding
additional constraints andoptmization targets to the solver generates the
subset of preferable solutions.An evaluation using the VTA hardware accelerator
with the Baidu DeepBench inference benchmark shows that our approach
canautomatically generate code competitive to reference implementations.
Further, we show that dynamically determining the data layoutbased on intrinsic
and workload is beneficial for hardware utilization and performance. In cases
where the reference implementationhas low hardware utilization due to its fixed
deployment strategy, we achieve a geomean speedup of up to x2.813, while
individualoperators can improve as much as x170.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rieber_D/0/1/0/all/0/1"&gt;Dennis Rieber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Acosta_A/0/1/0/all/0/1"&gt;Axel Acosta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Froning_H/0/1/0/all/0/1"&gt;Holger Fr&amp;#xf6;ning&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability. (arXiv:2006.14512v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.14512</id>
        <link href="http://arxiv.org/abs/2006.14512"/>
        <updated>2021-07-12T01:55:16.648Z</updated>
        <summary type="html"><![CDATA[Knowledge transferability, or transfer learning, has been widely adopted to
allow a pre-trained model in the source domain to be effectively adapted to
downstream tasks in the target domain. It is thus important to explore and
understand the factors affecting knowledge transferability. In this paper, as
the first work, we analyze and demonstrate the connections between knowledge
transferability and another important phenomenon--adversarial transferability,
\emph{i.e.}, adversarial examples generated against one model can be
transferred to attack other models. Our theoretical studies show that
adversarial transferability indicates knowledge transferability and vice versa.
Moreover, based on the theoretical insights, we propose two practical
adversarial transferability metrics to characterize this process, serving as
bidirectional indicators between adversarial and knowledge transferability. We
conduct extensive experiments for different scenarios on diverse datasets,
showing a positive correlation between adversarial transferability and
knowledge transferability. Our findings will shed light on future research
about effective knowledge transfer learning and adversarial transferability
analyses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1"&gt;Kaizhao Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jacky Y. Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Boxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhuolin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koyejo_O/0/1/0/all/0/1"&gt;Oluwasanmi Koyejo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aligning an optical interferometer with beam divergence control and continuous action space. (arXiv:2107.04457v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.04457</id>
        <link href="http://arxiv.org/abs/2107.04457"/>
        <updated>2021-07-12T01:55:16.640Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning is finding its way to real-world problem application,
transferring from simulated environments to physical setups. In this work, we
implement vision-based alignment of an optical Mach-Zehnder interferometer with
a confocal telescope in one arm, which controls the diameter and divergence of
the corresponding beam. We use a continuous action space; exponential scaling
enables us to handle actions within a range of over two orders of magnitude.
Our agent trains only in a simulated environment with domain randomizations. In
an experimental evaluation, the agent significantly outperforms an existing
solution and a human expert.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Makarenko_S/0/1/0/all/0/1"&gt;Stepan Makarenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sorokin_D/0/1/0/all/0/1"&gt;Dmitry Sorokin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ulanov_A/0/1/0/all/0/1"&gt;Alexander Ulanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lvovsky_A/0/1/0/all/0/1"&gt;A. I. Lvovsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Counterfactual Explanations on Graph Neural Networks. (arXiv:2107.04086v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04086</id>
        <link href="http://arxiv.org/abs/2107.04086"/>
        <updated>2021-07-12T01:55:16.615Z</updated>
        <summary type="html"><![CDATA[Massive deployment of Graph Neural Networks (GNNs) in high-stake applications
generates a strong demand for explanations that are robust to noise and align
well with human intuition. Most existing methods generate explanations by
identifying a subgraph of an input graph that has a strong correlation with the
prediction. These explanations are not robust to noise because independently
optimizing the correlation for a single input can easily overfit noise.
Moreover, they do not align well with human intuition because removing an
identified subgraph from an input graph does not necessarily change the
prediction result. In this paper, we propose a novel method to generate robust
counterfactual explanations on GNNs by explicitly modelling the common decision
logic of GNNs on similar input graphs. Our explanations are naturally robust to
noise because they are produced from the common decision boundaries of a GNN
that govern the predictions of many similar input graphs. The explanations also
align well with human intuition because removing the set of edges identified by
an explanation from the input graph changes the prediction significantly.
Exhaustive experiments on many public datasets demonstrate the superior
performance of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bajaj_M/0/1/0/all/0/1"&gt;Mohit Bajaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_L/0/1/0/all/0/1"&gt;Lingyang Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1"&gt;Zi Yu Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1"&gt;Jian Pei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lanjun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_P/0/1/0/all/0/1"&gt;Peter Cho-Ho Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Breath Phase and Continuous Adventitious Sound Detection in Lung and Tracheal Sound Using Mixed Set Training and Domain Adaptation. (arXiv:2107.04229v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.04229</id>
        <link href="http://arxiv.org/abs/2107.04229"/>
        <updated>2021-07-12T01:55:16.609Z</updated>
        <summary type="html"><![CDATA[Previously, we established a lung sound database, HF_Lung_V2 and proposed
convolutional bidirectional gated recurrent unit (CNN-BiGRU) models with
adequate ability for inhalation, exhalation, continuous adventitious sound
(CAS), and discontinuous adventitious sound detection in the lung sound. In
this study, we proceeded to build a tracheal sound database, HF_Tracheal_V1,
containing 11107 of 15-second tracheal sound recordings, 23087 inhalation
labels, 16728 exhalation labels, and 6874 CAS labels. The tracheal sound in
HF_Tracheal_V1 and the lung sound in HF_Lung_V2 were either combined or used
alone to train the CNN-BiGRU models for respective lung and tracheal sound
analysis. Different training strategies were investigated and compared: (1)
using full training (training from scratch) to train the lung sound models
using lung sound alone and train the tracheal sound models using tracheal sound
alone, (2) using a mixed set that contains both the lung and tracheal sound to
train the models, and (3) using domain adaptation that finetuned the
pre-trained lung sound models with the tracheal sound data and vice versa.
Results showed that the models trained only by lung sound performed poorly in
the tracheal sound analysis and vice versa. However, the mixed set training and
domain adaptation can improve the performance of exhalation and CAS detection
in the lung sound, and inhalation, exhalation, and CAS detection in the
tracheal sound compared to positive controls (lung models trained only by lung
sound and vice versa). Especially, a model derived from the mixed set training
prevails in the situation of killing two birds with one stone.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_F/0/1/0/all/0/1"&gt;Fu-Shun Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shang-Ran Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1"&gt;Chang-Fu Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chien-Wen Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yuan-Ren Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chun-Chieh Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chun-Yu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chung-Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1"&gt;Yen-Chun Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1"&gt;Tang-Wei Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1"&gt;Nian-Jhen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_W/0/1/0/all/0/1"&gt;Wan-Ling Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Ching-Shiang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_F/0/1/0/all/0/1"&gt;Feipei Lai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Convolutional Networks for Model-Based Learning in Nonlinear Inverse Problems. (arXiv:2103.15138v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15138</id>
        <link href="http://arxiv.org/abs/2103.15138"/>
        <updated>2021-07-12T01:55:16.601Z</updated>
        <summary type="html"><![CDATA[The majority of model-based learned image reconstruction methods in medical
imaging have been limited to uniform domains, such as pixelated images. If the
underlying model is solved on nonuniform meshes, arising from a finite element
method typical for nonlinear inverse problems, interpolation and embeddings are
needed. To overcome this, we present a flexible framework to extend model-based
learning directly to nonuniform meshes, by interpreting the mesh as a graph and
formulating our network architectures using graph convolutional neural
networks. This gives rise to the proposed iterative Graph Convolutional
Newton-type Method (GCNM), which includes the forward model in the solution of
the inverse problem, while all updates are directly computed by the network on
the problem specific mesh. We present results for Electrical Impedance
Tomography, a severely ill-posed nonlinear inverse problem that is frequently
solved via optimization-based methods, where the forward problem is solved by
finite element methods. Results for absolute EIT imaging are compared to
standard iterative methods as well as a graph residual network. We show that
the GCNM has strong generalizability to different domain shapes and meshes, out
of distribution data as well as experimental data, from purely simulated
training data and without transfer training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Herzberg_W/0/1/0/all/0/1"&gt;William Herzberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rowe_D/0/1/0/all/0/1"&gt;Daniel B. Rowe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hauptmann_A/0/1/0/all/0/1"&gt;Andreas Hauptmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hamilton_S/0/1/0/all/0/1"&gt;Sarah J. Hamilton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Variance of the Fisher Information for Deep Learning. (arXiv:2107.04205v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04205</id>
        <link href="http://arxiv.org/abs/2107.04205"/>
        <updated>2021-07-12T01:55:16.594Z</updated>
        <summary type="html"><![CDATA[The Fisher information matrix (FIM) has been applied to the realm of deep
learning. It is closely related to the loss landscape, the variance of the
parameters, second order optimization, and deep learning theory. The exact FIM
is either unavailable in closed form or too expensive to compute. In practice,
it is almost always estimated based on empirical samples. We investigate two
such estimators based on two equivalent representations of the FIM. They are
both unbiased and consistent with respect to the underlying "true" FIM. Their
estimation quality is characterized by their variance given in closed form. We
bound their variances and analyze how the parametric structure of a deep neural
network can impact the variance. We discuss the meaning of this variance
measure and our bounds in the context of deep learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soen_A/0/1/0/all/0/1"&gt;Alexander Soen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1"&gt;Ke Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[(Un)Masked COVID-19 Trends from Social Media. (arXiv:2011.00052v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.00052</id>
        <link href="http://arxiv.org/abs/2011.00052"/>
        <updated>2021-07-12T01:55:16.587Z</updated>
        <summary type="html"><![CDATA[Wearing masks is a useful protection method against COVID-19, which has
caused widespread economic and social impact worldwide. Across the globe,
governments have put mandates for the use of face masks, which have received
both positive and negative reaction. Online social media provides an exciting
platform to study the use of masks and analyze underlying mask-wearing
patterns. In this article, we analyze 2.04 million social media images for six
US cities. An increase in masks worn in images is seen as the COVID-19 cases
rose, particularly when their respective states imposed strict regulations. We
also found a decrease in the posting of group pictures as stay-at-home laws
were put into place. Furthermore, mask compliance in the Black Lives Matter
protest was analyzed, eliciting that 40% of the people in group photos wore
masks, and 45% of them wore the masks with a fit score of greater than 80%. We
introduce two new datasets, VAriety MAsks - Classification (VAMA-C) and VAriety
MAsks - Segmentation (VAMA-S), for mask detection and mask fit analysis tasks,
respectively. For the analysis, we create two frameworks, face mask detector
(for classifying masked and unmasked faces) and mask fit analyzer (a semantic
segmentation based model to calculate a mask-fit score). The face mask detector
achieved a classification accuracy of 98%, and the semantic segmentation model
for the mask fit analyzer achieved an Intersection Over Union (IOU) score of
98%. We conclude that such a framework can be used to evaluate the
effectiveness of such public health strategies using social media platforms in
times of pandemic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Asmit Kumar Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehan_P/0/1/0/all/0/1"&gt;Paras Mehan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1"&gt;Divyanshu Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_R/0/1/0/all/0/1"&gt;Rohan Pandey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sethi_T/0/1/0/all/0/1"&gt;Tavpritesh Sethi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1"&gt;Ponnurangam Kumaraguru&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RGBD-Net: Predicting color and depth images for novel views synthesis. (arXiv:2011.14398v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14398</id>
        <link href="http://arxiv.org/abs/2011.14398"/>
        <updated>2021-07-12T01:55:16.568Z</updated>
        <summary type="html"><![CDATA[We propose a new cascaded architecture for novel view synthesis, called
RGBD-Net, which consists of two core components: a hierarchical depth
regression network and a depth-aware generator network. The former one predicts
depth maps of the target views by using adaptive depth scaling, while the
latter one leverages the predicted depths and renders spatially and temporally
consistent target images. In the experimental evaluation on standard datasets,
RGBD-Net not only outperforms the state-of-the-art by a clear margin, but it
also generalizes well to new scenes without per-scene optimization. Moreover,
we show that RGBD-Net can be optionally trained without depth supervision while
still retaining high-quality rendering. Thanks to the depth regression network,
RGBD-Net can be also used for creating dense 3D point clouds that are more
accurate than those produced by some state-of-the-art multi-view stereo
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1"&gt;Phong Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karnewar_A/0/1/0/all/0/1"&gt;Animesh Karnewar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huynh_L/0/1/0/all/0/1"&gt;Lam Huynh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1"&gt;Esa Rahtu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1"&gt;Jiri Matas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heikkila_J/0/1/0/all/0/1"&gt;Janne Heikkila&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Contrastive Motion Learning for Video Action Recognition. (arXiv:2007.10321v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.10321</id>
        <link href="http://arxiv.org/abs/2007.10321"/>
        <updated>2021-07-12T01:55:16.561Z</updated>
        <summary type="html"><![CDATA[One central question for video action recognition is how to model motion. In
this paper, we present hierarchical contrastive motion learning, a new
self-supervised learning framework to extract effective motion representations
from raw video frames. Our approach progressively learns a hierarchy of motion
features that correspond to different abstraction levels in a network. This
hierarchical design bridges the semantic gap between low-level motion cues and
high-level recognition tasks, and promotes the fusion of appearance and motion
information at multiple levels. At each level, an explicit motion
self-supervision is provided via contrastive learning to enforce the motion
features at the current level to predict the future ones at the previous level.
Thus, the motion features at higher levels are trained to gradually capture
semantic dynamics and evolve more discriminative for action recognition. Our
motion learning module is lightweight and flexible to be embedded into various
backbone networks. Extensive experiments on four benchmarks show that the
proposed approach consistently achieves superior results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xitong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaodong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sifei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1"&gt;Deqing Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1"&gt;Larry Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1"&gt;Jan Kautz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Retinal OCT Denoising with Pseudo-Multimodal Fusion Network. (arXiv:2107.04288v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04288</id>
        <link href="http://arxiv.org/abs/2107.04288"/>
        <updated>2021-07-12T01:55:16.553Z</updated>
        <summary type="html"><![CDATA[Optical coherence tomography (OCT) is a prevalent imaging technique for
retina. However, it is affected by multiplicative speckle noise that can
degrade the visibility of essential anatomical structures, including blood
vessels and tissue layers. Although averaging repeated B-scan frames can
significantly improve the signal-to-noise-ratio (SNR), this requires longer
acquisition time, which can introduce motion artifacts and cause discomfort to
patients. In this study, we propose a learning-based method that exploits
information from the single-frame noisy B-scan and a pseudo-modality that is
created with the aid of the self-fusion method. The pseudo-modality provides
good SNR for layers that are barely perceptible in the noisy B-scan but can
over-smooth fine features such as small vessels. By using a fusion network,
desired features from each modality can be combined, and the weight of their
contribution is adjustable. Evaluated by intensity-based and structural
metrics, the result shows that our method can effectively suppress the speckle
noise and enhance the contrast between retina layers while the overall
structure and small blood vessels are preserved. Compared to the single
modality network, our method improves the structural similarity with low noise
B-scan from 0.559 +\- 0.033 to 0.576 +\- 0.031.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hu_D/0/1/0/all/0/1"&gt;Dewei Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Malone_J/0/1/0/all/0/1"&gt;Joseph D. Malone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Atay_Y/0/1/0/all/0/1"&gt;Yigit Atay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tao_Y/0/1/0/all/0/1"&gt;Yuankai K. Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oguz_I/0/1/0/all/0/1"&gt;Ipek Oguz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Triangle Inequality for Cosine Similarity. (arXiv:2107.04071v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04071</id>
        <link href="http://arxiv.org/abs/2107.04071"/>
        <updated>2021-07-12T01:55:16.546Z</updated>
        <summary type="html"><![CDATA[Similarity search is a fundamental problem for many data analysis techniques.
Many efficient search techniques rely on the triangle inequality of metrics,
which allows pruning parts of the search space based on transitive bounds on
distances. Recently, Cosine similarity has become a popular alternative choice
to the standard Euclidean metric, in particular in the context of textual data
and neural network embeddings. Unfortunately, Cosine similarity is not metric
and does not satisfy the standard triangle inequality. Instead, many search
techniques for Cosine rely on approximation techniques such as locality
sensitive hashing. In this paper, we derive a triangle inequality for Cosine
similarity that is suitable for efficient similarity search with many standard
search structures (such as the VP-tree, Cover-tree, and M-tree); show that this
bound is tight and discuss fast approximations for it. We hope that this spurs
new research on accelerating exact similarity search for cosine similarity, and
possible other similarity measures beyond the existing work for distance
metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schubert_E/0/1/0/all/0/1"&gt;Erich Schubert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Quantum-Secure Authentication and Key Agreement via Abstract Multi-Agent Interaction. (arXiv:2007.09327v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.09327</id>
        <link href="http://arxiv.org/abs/2007.09327"/>
        <updated>2021-07-12T01:55:16.540Z</updated>
        <summary type="html"><![CDATA[Current methods for authentication and key agreement based on public-key
cryptography are vulnerable to quantum computing. We propose a novel approach
based on artificial intelligence research in which communicating parties are
viewed as autonomous agents which interact repeatedly using their private
decision models. Authentication and key agreement are decided based on the
agents' observed behaviors during the interaction. The security of this
approach rests upon the difficulty of modeling the decisions of interacting
agents from limited observations, a problem which we conjecture is also hard
for quantum computing. We release PyAMI, a prototype authentication and key
agreement system based on the proposed method. We empirically validate our
method for authenticating legitimate users while detecting different types of
adversarial attacks. Finally, we show how reinforcement learning techniques can
be used to train server models which effectively probe a client's decisions to
achieve more sample-efficient authentication.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_I/0/1/0/all/0/1"&gt;Ibrahim H. Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanna_J/0/1/0/all/0/1"&gt;Josiah P. Hanna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fosong_E/0/1/0/all/0/1"&gt;Elliot Fosong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1"&gt;Stefano V. Albrecht&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Discontinuity-Preserving Image Registration Network. (arXiv:2107.04440v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04440</id>
        <link href="http://arxiv.org/abs/2107.04440"/>
        <updated>2021-07-12T01:55:16.522Z</updated>
        <summary type="html"><![CDATA[Image registration aims to establish spatial correspondence across pairs, or
groups of images, and is a cornerstone of medical image computing and
computer-assisted-interventions. Currently, most deep learning-based
registration methods assume that the desired deformation fields are globally
smooth and continuous, which is not always valid for real-world scenarios,
especially in medical image registration (e.g. cardiac imaging and abdominal
imaging). Such a global constraint can lead to artefacts and increased errors
at discontinuous tissue interfaces. To tackle this issue, we propose a
weakly-supervised Deep Discontinuity-preserving Image Registration network
(DDIR), to obtain better registration performance and realistic deformation
fields. We demonstrate that our method achieves significant improvements in
registration accuracy and predicts more realistic deformations, in registration
experiments on cardiac magnetic resonance (MR) images from UK Biobank Imaging
Study (UKBB), than state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ravikumar_N/0/1/0/all/0/1"&gt;Nishant Ravikumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yan Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Frangi_A/0/1/0/all/0/1"&gt;Alejandro F Frangi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inter-domain Multi-relational Link Prediction. (arXiv:2106.06171v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06171</id>
        <link href="http://arxiv.org/abs/2106.06171"/>
        <updated>2021-07-12T01:55:16.502Z</updated>
        <summary type="html"><![CDATA[Multi-relational graph is a ubiquitous and important data structure, allowing
flexible representation of multiple types of interactions and relations between
entities. Similar to other graph-structured data, link prediction is one of the
most important tasks on multi-relational graphs and is often used for knowledge
completion. When related graphs coexist, it is of great benefit to build a
larger graph via integrating the smaller ones. The integration requires
predicting hidden relational connections between entities belonged to different
graphs (inter-domain link prediction). However, this poses a real challenge to
existing methods that are exclusively designed for link prediction between
entities of the same graph only (intra-domain link prediction). In this study,
we propose a new approach to tackle the inter-domain link prediction problem by
softly aligning the entity distributions between different domains with optimal
transport and maximum mean discrepancy regularizers. Experiments on real-world
datasets show that optimal transport regularizer is beneficial and considerably
improves the performance of baseline methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Phuc_L/0/1/0/all/0/1"&gt;Luu Huu Phuc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takeuchi_K/0/1/0/all/0/1"&gt;Koh Takeuchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Okajima_S/0/1/0/all/0/1"&gt;Seiji Okajima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tolmachev_A/0/1/0/all/0/1"&gt;Arseny Tolmachev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takebayashi_T/0/1/0/all/0/1"&gt;Tomoyoshi Takebayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maruhashi_K/0/1/0/all/0/1"&gt;Koji Maruhashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1"&gt;Hisashi Kashima&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilingual Byte2Speech Models for Scalable Low-resource Speech Synthesis. (arXiv:2103.03541v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03541</id>
        <link href="http://arxiv.org/abs/2103.03541"/>
        <updated>2021-07-12T01:55:16.495Z</updated>
        <summary type="html"><![CDATA[To scale neural speech synthesis to various real-world languages, we present
a multilingual end-to-end framework that maps byte inputs to spectrograms, thus
allowing arbitrary input scripts. Besides strong results on 40+ languages, the
framework demonstrates capabilities to adapt to new languages under extreme
low-resource and even few-shot scenarios of merely 40s transcribed recording,
without the need of per-language resources like lexicon, extra corpus,
auxiliary models, or linguistic expertise, thus ensuring scalability. While it
retains satisfactory intelligibility and naturalness matching rich-resource
models. Exhaustive comparative and ablation studies are performed to reveal the
potential of the framework for low-resource languages. Furthermore, we propose
a novel method to extract language-specific sub-networks in a multilingual
model for a better understanding of its mechanism.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1"&gt;Mutian He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jingzhou Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soong_F/0/1/0/all/0/1"&gt;Frank K. Soong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Robust General Medical Image Segmentation. (arXiv:2107.04263v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04263</id>
        <link href="http://arxiv.org/abs/2107.04263"/>
        <updated>2021-07-12T01:55:16.489Z</updated>
        <summary type="html"><![CDATA[The reliability of Deep Learning systems depends on their accuracy but also
on their robustness against adversarial perturbations to the input data.
Several attacks and defenses have been proposed to improve the performance of
Deep Neural Networks under the presence of adversarial noise in the natural
image domain. However, robustness in computer-aided diagnosis for volumetric
data has only been explored for specific tasks and with limited attacks. We
propose a new framework to assess the robustness of general medical image
segmentation systems. Our contributions are two-fold: (i) we propose a new
benchmark to evaluate robustness in the context of the Medical Segmentation
Decathlon (MSD) by extending the recent AutoAttack natural image classification
framework to the domain of volumetric data segmentation, and (ii) we present a
novel lattice architecture for RObust Generic medical image segmentation (ROG).
Our results show that ROG is capable of generalizing across different tasks of
the MSD and largely surpasses the state-of-the-art under sophisticated
adversarial attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Daza_L/0/1/0/all/0/1"&gt;Laura Daza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_J/0/1/0/all/0/1"&gt;Juan C. P&amp;#xe9;rez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1"&gt;Pablo Arbel&amp;#xe1;ez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Best of Many Worlds: Dual Mirror Descent for Online Allocation Problems. (arXiv:2011.10124v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10124</id>
        <link href="http://arxiv.org/abs/2011.10124"/>
        <updated>2021-07-12T01:55:16.471Z</updated>
        <summary type="html"><![CDATA[Online allocation problems with resource constraints are central problems in
revenue management and online advertising. In these problems, requests arrive
sequentially during a finite horizon and, for each request, a decision maker
needs to choose an action that consumes a certain amount of resources and
generates reward. The objective is to maximize cumulative rewards subject to a
constraint on the total consumption of resources. In this paper, we consider a
data-driven setting in which the reward and resource consumption of each
request are generated using an input model that is unknown to the decision
maker.

We design a general class of algorithms that attain good performance in
various inputs models without knowing which type of input they are facing. In
particular, our algorithms are asymptotically optimal under stochastic i.i.d.
input model as well as various non-stationary stochastic input models, and they
attain an asymptotically optimal fixed competitive ratio when the input is
adversarial. Our algorithms operate in the Lagrangian dual space: they maintain
a dual multiplier for each resource that is updated using online mirror
descent. By choosing the reference function accordingly, we recover dual
sub-gradient descent and dual exponential weights algorithm. The resulting
algorithms are simple, fast, and have minimal requirements on the reward
functions, consumption functions and the action space, in contrast to existing
methods for online allocation problems. We discuss applications to network
revenue management, online bidding in repeated auctions with budget
constraints, online proportional matching with high entropy, and personalized
assortment optimization with limited inventories.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balseiro_S/0/1/0/all/0/1"&gt;Santiago Balseiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Haihao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mirrokni_V/0/1/0/all/0/1"&gt;Vahab Mirrokni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Negative Transfer. (arXiv:2009.00909v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.00909</id>
        <link href="http://arxiv.org/abs/2009.00909"/>
        <updated>2021-07-12T01:55:16.463Z</updated>
        <summary type="html"><![CDATA[Transfer learning (TL) tries to utilize data or knowledge from one or more
source domains to facilitate the learning in a target domain. It is
particularly useful when the target domain has few or no labeled data, due to
annotation expense, privacy concerns, etc. Unfortunately, the effectiveness of
TL is not always guaranteed. Negative transfer (NT), i.e., the source domain
data/knowledge cause reduced learning performance in the target domain, has
been a long-standing and challenging problem in TL. Various approaches to
handle NT have been proposed in the literature. However, this filed lacks a
systematic survey on the formalization of NT, their factors and the algorithms
that handle NT. This paper proposes to fill this gap. First, the definition of
negative transfer is considered and a taxonomy of the factors are discussed.
Then, near fifty representative approaches for handling NT are categorized and
reviewed, from four perspectives: secure transfer, domain similarity
estimation, distant transfer and negative transfer mitigation. NT in related
fields, e.g., multi-task learning, lifelong learning, and adversarial attacks
are also discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1"&gt;Lingfei Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dongrui Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Speech Recognition from Federated Acoustic Models. (arXiv:2104.14297v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14297</id>
        <link href="http://arxiv.org/abs/2104.14297"/>
        <updated>2021-07-12T01:55:16.456Z</updated>
        <summary type="html"><![CDATA[Training Automatic Speech Recognition (ASR) models under federated learning
(FL) settings has attracted a lot of attention recently. However, the FL
scenarios often presented in the literature are artificial and fail to capture
the complexity of real FL systems. In this paper, we construct a challenging
and realistic ASR federated experimental setup consisting of clients with
heterogeneous data distributions using the French and Italian sets of the
CommonVoice dataset, a large heterogeneous dataset containing thousands of
different speakers, acoustic environments and noises. We present the first
empirical study on attention-based sequence-to-sequence End-to-End (E2E) ASR
model with three aggregation weighting strategies -- standard FedAvg,
loss-based aggregation and a novel word error rate (WER)-based aggregation,
compared in two realistic FL scenarios: cross-silo with 10 clients and
cross-device with 2K and 4K clients. Our analysis on E2E ASR from heterogeneous
and realistic federated acoustic models provides the foundations for future
research and development of realistic FL-based ASR applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parcollet_T/0/1/0/all/0/1"&gt;Titouan Parcollet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaiem_S/0/1/0/all/0/1"&gt;Salah Zaiem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandez_Marques_J/0/1/0/all/0/1"&gt;Javier Fernandez-Marques&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gusmao_P/0/1/0/all/0/1"&gt;Pedro P. B. de Gusmao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beutel_D/0/1/0/all/0/1"&gt;Daniel J. Beutel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1"&gt;Nicholas D. Lane&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does Form Follow Function? An Empirical Exploration of the Impact of Deep Neural Network Architecture Design on Hardware-Specific Acceleration. (arXiv:2107.04144v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04144</id>
        <link href="http://arxiv.org/abs/2107.04144"/>
        <updated>2021-07-12T01:55:16.449Z</updated>
        <summary type="html"><![CDATA[The fine-grained relationship between form and function with respect to deep
neural network architecture design and hardware-specific acceleration is one
area that is not well studied in the research literature, with form often
dictated by accuracy as opposed to hardware function. In this study, a
comprehensive empirical exploration is conducted to investigate the impact of
deep neural network architecture design on the degree of inference speedup that
can be achieved via hardware-specific acceleration. More specifically, we
empirically study the impact of a variety of commonly used macro-architecture
design patterns across different architectural depths through the lens of
OpenVINO microprocessor-specific and GPU-specific acceleration. Experimental
results showed that while leveraging hardware-specific acceleration achieved an
average inference speed-up of 380%, the degree of inference speed-up varied
drastically depending on the macro-architecture design pattern, with the
greatest speedup achieved on the depthwise bottleneck convolution design
pattern at 550%. Furthermore, we conduct an in-depth exploration of the
correlation between FLOPs requirement, level 3 cache efficacy, and network
latency with increasing architectural depth and width. Finally, we analyze the
inference time reductions using hardware-specific acceleration when compared to
native deep learning frameworks across a wide variety of hand-crafted deep
convolutional neural network architecture designs as well as ones found via
neural architecture search strategies. We found that the DARTS-derived
architecture to benefit from the greatest improvement from hardware-specific
software acceleration (1200%) while the depthwise bottleneck convolution-based
MobileNet-V2 to have the lowest overall inference time of around 2.4 ms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abbasi_S/0/1/0/all/0/1"&gt;Saad Abbasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafiee_M/0/1/0/all/0/1"&gt;Mohammad Javad Shafiee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_E/0/1/0/all/0/1"&gt;Ellick Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alexander Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Generative Models for Two-Dimensional Datasets. (arXiv:2106.00203v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00203</id>
        <link href="http://arxiv.org/abs/2106.00203"/>
        <updated>2021-07-12T01:55:16.439Z</updated>
        <summary type="html"><![CDATA[Two-dimensional array-based datasets are pervasive in a variety of domains.
Current approaches for generative modeling have typically been limited to
conventional image datasets and performed in the pixel domain which do not
explicitly capture the correlation between pixels. Additionally, these
approaches do not extend to scientific and other applications where each
element value is continuous and is not limited to a fixed range. In this paper,
we propose a novel approach for generating two-dimensional datasets by moving
the computations to the space of representation bases and show its usefulness
for two different datasets, one from imaging and another from scientific
computing. The proposed approach is general and can be applied to any dataset,
representation basis, or generative model. We provide a comprehensive
performance comparison of various combinations of generative models and
representation basis spaces. We also propose a new evaluation metric which
captures the deficiency of generating images in pixel space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shajari_H/0/1/0/all/0/1"&gt;Hoda Shajari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jaemoon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranka_S/0/1/0/all/0/1"&gt;Sanjay Ranka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rangarajan_A/0/1/0/all/0/1"&gt;Anand Rangarajan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparison of 2D vs. 3D U-Net Organ Segmentation in abdominal 3D CT images. (arXiv:2107.04062v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04062</id>
        <link href="http://arxiv.org/abs/2107.04062"/>
        <updated>2021-07-12T01:55:16.420Z</updated>
        <summary type="html"><![CDATA[A two-step concept for 3D segmentation on 5 abdominal organs inside
volumetric CT images is presented. First each relevant organ's volume of
interest is extracted as bounding box. The extracted volume acts as input for a
second stage, wherein two compared U-Nets with different architectural
dimensions re-construct an organ segmentation as label mask. In this work, we
focus on comparing 2D U-Nets vs. 3D U-Net counterparts. Our initial results
indicate Dice improvements of about 6\% at maximum. In this study to our
surprise, liver and kidneys for instance were tackled significantly better
using the faster and GPU-memory saving 2D U-Nets. For other abdominal key
organs, there were no significant differences, but we observe highly
significant advantages for the 2D U-Net in terms of GPU computational efforts
for all organs under study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zettler_N/0/1/0/all/0/1"&gt;Nico Zettler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mastmeyer_A/0/1/0/all/0/1"&gt;Andre Mastmeyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controlling Hallucinations at Word Level in Data-to-Text Generation. (arXiv:2102.02810v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02810</id>
        <link href="http://arxiv.org/abs/2102.02810"/>
        <updated>2021-07-12T01:55:16.410Z</updated>
        <summary type="html"><![CDATA[Data-to-Text Generation (DTG) is a subfield of Natural Language Generation
aiming at transcribing structured data in natural language descriptions. The
field has been recently boosted by the use of neural-based generators which
exhibit on one side great syntactic skills without the need of hand-crafted
pipelines; on the other side, the quality of the generated text reflects the
quality of the training data, which in realistic settings only offer
imperfectly aligned structure-text pairs. Consequently, state-of-art neural
models include misleading statements - usually called hallucinations - in their
outputs. The control of this phenomenon is today a major challenge for DTG, and
is the problem addressed in the paper.

Previous work deal with this issue at the instance level: using an alignment
score for each table-reference pair. In contrast, we propose a finer-grained
approach, arguing that hallucinations should rather be treated at the word
level. Specifically, we propose a Multi-Branch Decoder which is able to
leverage word-level labels to learn the relevant parts of each training
instance. These labels are obtained following a simple and efficient scoring
procedure based on co-occurrence analysis and dependency parsing. Extensive
evaluations, via automated metrics and human judgment on the standard WikiBio
benchmark, show the accuracy of our alignment labels and the effectiveness of
the proposed Multi-Branch Decoder. Our model is able to reduce and control
hallucinations, while keeping fluency and coherence in generated texts. Further
experiments on a degraded version of ToTTo show that our model could be
successfully used on very noisy settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rebuffel_C/0/1/0/all/0/1"&gt;Cl&amp;#xe9;ment Rebuffel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberti_M/0/1/0/all/0/1"&gt;Marco Roberti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soulier_L/0/1/0/all/0/1"&gt;Laure Soulier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scoutheeten_G/0/1/0/all/0/1"&gt;Geoffrey Scoutheeten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cancelliere_R/0/1/0/all/0/1"&gt;Rossella Cancelliere&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1"&gt;Patrick Gallinari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum-inspired Machine Learning on high-energy physics data. (arXiv:2004.13747v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.13747</id>
        <link href="http://arxiv.org/abs/2004.13747"/>
        <updated>2021-07-12T01:55:16.402Z</updated>
        <summary type="html"><![CDATA[Tensor Networks, a numerical tool originally designed for simulating quantum
many-body systems, have recently been applied to solve Machine Learning
problems. Exploiting a tree tensor network, we apply a quantum-inspired machine
learning technique to a very important and challenging big data problem in high
energy physics: the analysis and classification of data produced by the Large
Hadron Collider at CERN. In particular, we present how to effectively classify
so-called b-jets, jets originating from b-quarks from proton-proton collisions
in the LHCb experiment, and how to interpret the classification results. We
exploit the Tensor Network approach to select important features and adapt the
network geometry based on information acquired in the learning process.
Finally, we show how to adapt the tree tensor network to achieve optimal
precision or fast response in time without the need of repeating the learning
process. These results pave the way to the implementation of high-frequency
real-time applications, a key ingredient needed among others for current and
future LHCb event classification able to trigger events at the tens of MHz
scale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Felser_T/0/1/0/all/0/1"&gt;Timo Felser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Trenti_M/0/1/0/all/0/1"&gt;Marco Trenti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sestini_L/0/1/0/all/0/1"&gt;Lorenzo Sestini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gianelle_A/0/1/0/all/0/1"&gt;Alessio Gianelle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zuliani_D/0/1/0/all/0/1"&gt;Davide Zuliani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lucchesi_D/0/1/0/all/0/1"&gt;Donatella Lucchesi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Montangero_S/0/1/0/all/0/1"&gt;Simone Montangero&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Query-efficient Planning in MDPs under Linear Realizability of the Optimal State-value Function. (arXiv:2102.02049v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02049</id>
        <link href="http://arxiv.org/abs/2102.02049"/>
        <updated>2021-07-12T01:55:16.396Z</updated>
        <summary type="html"><![CDATA[We consider local planning in fixed-horizon MDPs with a generative model
under the assumption that the optimal value function lies close to the span of
a feature map. The generative model provides a local access to the MDP: The
planner can ask for random transitions from previously returned states and
arbitrary actions, and features are only accessible for states that are
encountered in this process. As opposed to previous work (e.g. Lattimore et al.
(2020)) where linear realizability of all policies was assumed, we consider the
significantly relaxed assumption of a single linearly realizable
(deterministic) policy. A recent lower bound by Weisz et al. (2020) established
that the related problem when the action-value function of the optimal policy
is linearly realizable requires an exponential number of queries, either in $H$
(the horizon of the MDP) or $d$ (the dimension of the feature mapping). Their
construction crucially relies on having an exponentially large action set. In
contrast, in this work, we establish that poly$(H,d)$ planning is possible with
state value function realizability whenever the action set has a constant size.
In particular, we present the TensorPlan algorithm which uses
poly$((dH/\delta)^A)$ simulator queries to find a $\delta$-optimal policy
relative to any deterministic policy for which the value function is linearly
realizable with some bounded parameter. This is the first algorithm to give a
polynomial query complexity guarantee using only linear-realizability of a
single competing value function. Whether the computation cost is similarly
bounded remains an open question. We extend the upper bound to the
near-realizable case and to the infinite-horizon discounted setup. We also
present a lower bound in the infinite-horizon episodic setting: Planners that
achieve constant suboptimality need exponentially many queries, either in $d$
or the number of actions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weisz_G/0/1/0/all/0/1"&gt;Gell&amp;#xe9;rt Weisz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amortila_P/0/1/0/all/0/1"&gt;Philip Amortila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Janzer_B/0/1/0/all/0/1"&gt;Barnab&amp;#xe1;s Janzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbasi_Yadkori_Y/0/1/0/all/0/1"&gt;Yasin Abbasi-Yadkori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1"&gt;Nan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1"&gt;Csaba Szepesv&amp;#xe1;ri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autocalibration and Tweedie-dominance for Insurance Pricing with Machine Learning. (arXiv:2103.03635v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03635</id>
        <link href="http://arxiv.org/abs/2103.03635"/>
        <updated>2021-07-12T01:55:16.371Z</updated>
        <summary type="html"><![CDATA[Boosting techniques and neural networks are particularly effective machine
learning methods for insurance pricing. Often in practice, there are
nevertheless endless debates about the choice of the right loss function to be
used to train the machine learning model, as well as about the appropriate
metric to assess the performances of competing models. Also, the sum of fitted
values can depart from the observed totals to a large extent and this often
confuses actuarial analysts. The lack of balance inherent to training models by
minimizing deviance outside the familiar GLM with canonical link setting has
been empirically documented in W\"uthrich (2019, 2020) who attributes it to the
early stopping rule in gradient descent methods for model fitting. The present
paper aims to further study this phenomenon when learning proceeds by
minimizing Tweedie deviance. It is shown that minimizing deviance involves a
trade-off between the integral of weighted differences of lower partial moments
and the bias measured on a specific scale. Autocalibration is then proposed as
a remedy. This new method to correct for bias adds an extra local GLM step to
the analysis. Theoretically, it is shown that it implements the autocalibration
concept in pure premium calculation and ensures that balance also holds on a
local scale, not only at portfolio level as with existing bias-correction
techniques. The convex order appears to be the natural tool to compare
competing models, putting a new light on the diagnostic graphs and associated
metrics proposed by Denuit et al. (2019).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Denuit_M/0/1/0/all/0/1"&gt;Michel Denuit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Charpentier_A/0/1/0/all/0/1"&gt;Arthur Charpentier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Trufin_J/0/1/0/all/0/1"&gt;Julien Trufin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D RegNet: Deep Learning Model for COVID-19 Diagnosis on Chest CT Image. (arXiv:2107.04055v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04055</id>
        <link href="http://arxiv.org/abs/2107.04055"/>
        <updated>2021-07-12T01:55:16.363Z</updated>
        <summary type="html"><![CDATA[In this paper, a 3D-RegNet-based neural network is proposed for diagnosing
the physical condition of patients with coronavirus (Covid-19) infection. In
the application of clinical medicine, lung CT images are utilized by
practitioners to determine whether a patient is infected with coronavirus.
However, there are some laybacks can be considered regarding to this diagnostic
method, such as time consuming and low accuracy. As a relatively large organ of
human body, important spatial features would be lost if the lungs were
diagnosed utilizing two dimensional slice image. Therefore, in this paper, a
deep learning model with 3D image was designed. The 3D image as input data was
comprised of two-dimensional pulmonary image sequence and from which relevant
coronavirus infection 3D features were extracted and classified. The results
show that the test set of the 3D model, the result: f1 score of 0.8379 and AUC
value of 0.8807 have been achieved.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Qi_H/0/1/0/all/0/1"&gt;Haibo Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuhan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xinyu Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantile-Quantile Embedding for Distribution Transformation and Manifold Embedding with Ability to Choose the Embedding Distribution. (arXiv:2006.11385v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.11385</id>
        <link href="http://arxiv.org/abs/2006.11385"/>
        <updated>2021-07-12T01:55:16.356Z</updated>
        <summary type="html"><![CDATA[We propose a new embedding method, named Quantile-Quantile Embedding (QQE),
for distribution transformation and manifold embedding with the ability to
choose the embedding distribution. QQE, which uses the concept of
quantile-quantile plot from visual statistical tests, can transform the
distribution of data to any theoretical desired distribution or empirical
reference sample. Moreover, QQE gives the user a choice of embedding
distribution in embedding the manifold of data into the low dimensional
embedding space. It can also be used for modifying the embedding distribution
of other dimensionality reduction methods, such as PCA, t-SNE, and deep metric
learning, for better representation or visualization of data. We propose QQE in
both unsupervised and supervised forms. QQE can also transform a distribution
to either an exact reference distribution or its shape. We show that QQE allows
for better discrimination of classes in some cases. Our experiments on
different synthetic and image datasets show the effectiveness of the proposed
embedding method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ghojogh_B/0/1/0/all/0/1"&gt;Benyamin Ghojogh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Karray_F/0/1/0/all/0/1"&gt;Fakhri Karray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Crowley_M/0/1/0/all/0/1"&gt;Mark Crowley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PAC Bayesian Performance Guarantees for Deep (Stochastic) Networks in Medical Imaging. (arXiv:2104.05600v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05600</id>
        <link href="http://arxiv.org/abs/2104.05600"/>
        <updated>2021-07-12T01:55:16.349Z</updated>
        <summary type="html"><![CDATA[Application of deep neural networks to medical imaging tasks has in some
sense become commonplace. Still, a "thorn in the side" of the deep learning
movement is the argument that deep networks are prone to overfitting and are
thus unable to generalize well when datasets are small (as is common in medical
imaging tasks). One way to bolster confidence is to provide mathematical
guarantees, or bounds, on network performance after training which explicitly
quantify the possibility of overfitting. In this work, we explore recent
advances using the PAC-Bayesian framework to provide bounds on generalization
error for large (stochastic) networks. While previous efforts focus on
classification in larger natural image datasets (e.g., MNIST and CIFAR-10), we
apply these techniques to both classification and segmentation in a smaller
medical imagining dataset: the ISIC 2018 challenge set. We observe the
resultant bounds are competitive compared to a simpler baseline, while also
being more explainable and alleviating the need for holdout sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sicilia_A/0/1/0/all/0/1"&gt;Anthony Sicilia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xingchen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sosnovskikh_A/0/1/0/all/0/1"&gt;Anastasia Sosnovskikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Seong Jae Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gemmini: Enabling Systematic Deep-Learning Architecture Evaluation via Full-Stack Integration. (arXiv:1911.09925v3 [cs.DC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.09925</id>
        <link href="http://arxiv.org/abs/1911.09925"/>
        <updated>2021-07-12T01:55:16.341Z</updated>
        <summary type="html"><![CDATA[DNN accelerators are often developed and evaluated in isolation without
considering the cross-stack, system-level effects in real-world environments.
This makes it difficult to appreciate the impact of System-on-Chip (SoC)
resource contention, OS overheads, and programming-stack inefficiencies on
overall performance/energy-efficiency. To address this challenge, we present
Gemmini, an open-source*, full-stack DNN accelerator generator. Gemmini
generates a wide design-space of efficient ASIC accelerators from a flexible
architectural template, together with flexible programming stacks and full SoCs
with shared resources that capture system-level effects. Gemmini-generated
accelerators have also been fabricated, delivering up to three
orders-of-magnitude speedups over high-performance CPUs on various DNN
benchmarks.

* https://github.com/ucb-bar/gemmini]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Genc_H/0/1/0/all/0/1"&gt;Hasan Genc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seah Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amid_A/0/1/0/all/0/1"&gt;Alon Amid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haj_Ali_A/0/1/0/all/0/1"&gt;Ameer Haj-Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyer_V/0/1/0/all/0/1"&gt;Vighnesh Iyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prakash_P/0/1/0/all/0/1"&gt;Pranav Prakash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jerry Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grubb_D/0/1/0/all/0/1"&gt;Daniel Grubb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liew_H/0/1/0/all/0/1"&gt;Harrison Liew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1"&gt;Howard Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ou_A/0/1/0/all/0/1"&gt;Albert Ou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_C/0/1/0/all/0/1"&gt;Colin Schmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steffl_S/0/1/0/all/0/1"&gt;Samuel Steffl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wright_J/0/1/0/all/0/1"&gt;John Wright&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1"&gt;Ion Stoica&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ragan_Kelley_J/0/1/0/all/0/1"&gt;Jonathan Ragan-Kelley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asanovic_K/0/1/0/all/0/1"&gt;Krste Asanovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikolic_B/0/1/0/all/0/1"&gt;Borivoje Nikolic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yakun Sophia Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for Mean Field Games and Mean Field Control with Applications to Finance. (arXiv:2107.04568v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2107.04568</id>
        <link href="http://arxiv.org/abs/2107.04568"/>
        <updated>2021-07-12T01:55:16.323Z</updated>
        <summary type="html"><![CDATA[Financial markets and more generally macro-economic models involve a large
number of individuals interacting through variables such as prices resulting
from the aggregate behavior of all the agents. Mean field games have been
introduced to study Nash equilibria for such problems in the limit when the
number of players is infinite. The theory has been extensively developed in the
past decade, using both analytical and probabilistic tools, and a wide range of
applications have been discovered, from economics to crowd motion. More
recently the interaction with machine learning has attracted a growing
interest. This aspect is particularly relevant to solve very large games with
complex structures, in high dimension or with common sources of randomness. In
this chapter, we review the literature on the interplay between mean field
games and deep learning, with a focus on three families of methods. A special
emphasis is given to financial applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Carmona_R/0/1/0/all/0/1"&gt;Ren&amp;#xe9; Carmona&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Lauriere_M/0/1/0/all/0/1"&gt;Mathieu Lauri&amp;#xe8;re&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finite-Sample Analysis of Nonlinear Stochastic Approximation with Applications in Reinforcement Learning. (arXiv:1905.11425v6 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.11425</id>
        <link href="http://arxiv.org/abs/1905.11425"/>
        <updated>2021-07-12T01:55:16.309Z</updated>
        <summary type="html"><![CDATA[Motivated by applications in reinforcement learning (RL), we study a
nonlinear stochastic approximation (SA) algorithm under Markovian noise, and
establish its finite-sample convergence bounds under various stepsizes.
Specifically, we show that when using constant stepsize (i.e.,
$\epsilon_k\equiv \epsilon$), the algorithm achieves exponential fast
convergence with asymptotic accuracy $\mathcal{O}(\epsilon\log(1/\epsilon))$.
When using diminishing stepsizes with appropriate decay rate, the algorithm
converges with rate $\mathcal{O}(\log(k)/k)$. Our proof is based on the
Lyapunov drift arguments, and to handle the Markovian noise, we exploit the
fast mixing of the underlying Markov chain. To demonstrate the generality of
our theoretical results on Markovian SA, we use it to derive the finite-sample
bounds of the popular $Q$-learning with linear function approximation
algorithm, under a condition on the behavior policy. Importantly, we do not
need to make the unrealistic assumption that the samples are i.i.d., and do not
require an additional projection step in the algorithm to maintain the
boundedness of the iterates. Numerical simulations corroborate our theoretical
findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zaiwei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Sheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Doan_T/0/1/0/all/0/1"&gt;Thinh T. Doan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Clarke_J/0/1/0/all/0/1"&gt;John-Paul Clarke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Maguluri_S/0/1/0/all/0/1"&gt;Siva Theja Maguluri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controlling Graph Dynamics with Reinforcement Learning and Graph Neural Networks. (arXiv:2010.05313v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05313</id>
        <link href="http://arxiv.org/abs/2010.05313"/>
        <updated>2021-07-12T01:55:16.299Z</updated>
        <summary type="html"><![CDATA[We consider the problem of controlling a partially-observed dynamic process
on a graph by a limited number of interventions. This problem naturally arises
in contexts such as scheduling virus tests to curb an epidemic; targeted
marketing in order to promote a product; and manually inspecting posts to
detect fake news spreading on social networks.

We formulate this setup as a sequential decision problem over a temporal
graph process. In face of an exponential state space, combinatorial action
space and partial observability, we design a novel tractable scheme to control
dynamical processes on temporal graphs. We successfully apply our approach to
two popular problems that fall into our framework: prioritizing which nodes
should be tested in order to curb the spread of an epidemic, and influence
maximization on a graph.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meirom_E/0/1/0/all/0/1"&gt;Eli A. Meirom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maron_H/0/1/0/all/0/1"&gt;Haggai Maron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1"&gt;Shie Mannor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1"&gt;Gal Chechik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BrainNetGAN: Data augmentation of brain connectivity using generative adversarial network for dementia classification. (arXiv:2103.08494v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08494</id>
        <link href="http://arxiv.org/abs/2103.08494"/>
        <updated>2021-07-12T01:55:16.291Z</updated>
        <summary type="html"><![CDATA[Alzheimer's disease (AD) is the most common age-related dementia. It remains
a challenge to identify the individuals at risk of dementia for precise
management. Brain MRI offers a noninvasive biomarker to detect brain aging.
Previous evidence shows that the brain structural change detected by diffusion
MRI is associated with dementia. Mounting studies has conceptualised the brain
as a complex network, which has shown the utility of this approach in
characterising various neurological and psychiatric disorders. Therefore, the
structural connectivity shows promise in dementia classification. The proposed
BrainNetGAN is a generative adversarial network variant to augment the brain
structural connectivity matrices for binary dementia classification tasks.
Structural connectivity matrices between separated brain regions are
constructed using tractography on diffusion MRI data. The BrainNetGAN model is
trained to generate fake brain connectivity matrices, which are expected to
reflect latent distribution of the real brain network data. Finally, a
convolutional neural network classifier is proposed for binary dementia
classification. Numerical results show that the binary classification
performance in the testing set was improved using the BrainNetGAN augmented
dataset. The proposed methodology allows quick synthesis of an arbitrary number
of augmented connectivity matrices and can be easily transferred to similar
classification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yiran Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1"&gt;Carola-Bibiane Schonlieb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimising cost vs accuracy of decentralised analytics in fog computing environments. (arXiv:2012.05266v2 [cs.DC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05266</id>
        <link href="http://arxiv.org/abs/2012.05266"/>
        <updated>2021-07-12T01:55:16.284Z</updated>
        <summary type="html"><![CDATA[The exponential growth of devices and data at the edges of the Internet is
rising scalability and privacy concerns on approaches based exclusively on
remote cloud platforms. Data gravity, a fundamental concept in Fog Computing,
points towards decentralisation of computation for data analysis, as a viable
alternative to address those concerns. Decentralising AI tasks on several
cooperative devices means identifying the optimal set of locations or
Collection Points (CP for short) to use, in the continuum between full
centralisation (i.e., all data on a single device) and full decentralisation
(i.e., data on source locations). We propose an analytical framework able to
find the optimal operating point in this continuum, linking the accuracy of the
learning task with the corresponding network and computational cost for moving
data and running the distributed training at the CPs. We show through
simulations that the model accurately predicts the optimal trade-off, quite
often an intermediate point between full centralisation and full
decentralisation, showing also a significant cost saving w.r.t. both of them.
Finally, the analytical model admits closed-form or numeric solutions, making
it not only a performance evaluation instrument but also a design tool to
configure a given distributed learning task optimally before its deployment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Valerio_L/0/1/0/all/0/1"&gt;Lorenzo Valerio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Passarella_A/0/1/0/all/0/1"&gt;Andrea Passarella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1"&gt;Marco Conti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manifold Density Estimation via Generalized Dequantization. (arXiv:2102.07143v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07143</id>
        <link href="http://arxiv.org/abs/2102.07143"/>
        <updated>2021-07-12T01:55:16.278Z</updated>
        <summary type="html"><![CDATA[Density estimation is an important technique for characterizing distributions
given observations. Much existing research on density estimation has focused on
cases wherein the data lies in a Euclidean space. However, some kinds of data
are not well-modeled by supposing that their underlying geometry is Euclidean.
Instead, it can be useful to model such data as lying on a {\it manifold} with
some known structure. For instance, some kinds of data may be known to lie on
the surface of a sphere. We study the problem of estimating densities on
manifolds. We propose a method, inspired by the literature on "dequantization,"
which we interpret through the lens of a coordinate transformation of an
ambient Euclidean space and a smooth manifold of interest. Using methods from
normalizing flows, we apply this method to the dequantization of smooth
manifold structures in order to model densities on the sphere, tori, and the
orthogonal group.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Brofos_J/0/1/0/all/0/1"&gt;James A. Brofos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Brubaker_M/0/1/0/all/0/1"&gt;Marcus A. Brubaker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lederman_R/0/1/0/all/0/1"&gt;Roy R. Lederman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Drug-Target Interaction Prediction via an Ensemble of Weighted Nearest Neighbors with Interaction Recovery. (arXiv:2012.12325v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12325</id>
        <link href="http://arxiv.org/abs/2012.12325"/>
        <updated>2021-07-12T01:55:16.261Z</updated>
        <summary type="html"><![CDATA[Predicting drug-target interactions (DTI) via reliable computational methods
is an effective and efficient way to mitigate the enormous costs and time of
the drug discovery process. Structure-based drug similarities and
sequence-based target protein similarities are the commonly used information
for DTI prediction. Among numerous computational methods, neighborhood-based
chemogenomic approaches that leverage drug and target similarities to perform
predictions directly are simple but promising ones. However, existing
similarity-based methods need to be re-trained to predict interactions for any
new drugs or targets and cannot directly perform predictions for both new
drugs, new targets, and new drug-target pairs. Furthermore, a large amount of
missing (undetected) interactions in current DTI datasets hinders most DTI
prediction methods. To address these issues, we propose a new method denoted as
Weighted k-Nearest Neighbor with Interaction Recovery (WkNNIR). Not only can
WkNNIR estimate interactions of any new drugs and/or new targets without any
need of re-training, but it can also recover missing interactions (false
negatives). In addition, WkNNIR exploits local imbalance to promote the
influence of more reliable similarities on the interaction recovery and
prediction processes. We also propose a series of ensemble methods that employ
diverse sampling strategies and could be coupled with WkNNIR as well as any
other DTI prediction method to improve performance. Experimental results over
five benchmark datasets demonstrate the effectiveness of our approaches in
predicting drug-target interactions. Lastly, we confirm the practical
prediction ability of proposed methods to discover reliable interactions that
were not reported in the original benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pliakos_K/0/1/0/all/0/1"&gt;Konstantinos Pliakos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vens_C/0/1/0/all/0/1"&gt;Celine Vens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1"&gt;Grigorios Tsoumakas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ViTGAN: Training GANs with Vision Transformers. (arXiv:2107.04589v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04589</id>
        <link href="http://arxiv.org/abs/2107.04589"/>
        <updated>2021-07-12T01:55:16.254Z</updated>
        <summary type="html"><![CDATA[Recently, Vision Transformers (ViTs) have shown competitive performance on
image recognition while requiring less vision-specific inductive biases. In
this paper, we investigate if such observation can be extended to image
generation. To this end, we integrate the ViT architecture into generative
adversarial networks (GANs). We observe that existing regularization methods
for GANs interact poorly with self-attention, causing serious instability
during training. To resolve this issue, we introduce novel regularization
techniques for training GANs with ViTs. Empirically, our approach, named
ViTGAN, achieves comparable performance to state-of-the-art CNN-based StyleGAN2
on CIFAR-10, CelebA, and LSUN bedroom datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kwonjoon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Huiwen Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Lu Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Han Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1"&gt;Zhuowen Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Ce Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Bayesian Learning Rule. (arXiv:2107.04562v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.04562</id>
        <link href="http://arxiv.org/abs/2107.04562"/>
        <updated>2021-07-12T01:55:16.247Z</updated>
        <summary type="html"><![CDATA[We show that many machine-learning algorithms are specific instances of a
single algorithm called the Bayesian learning rule. The rule, derived from
Bayesian principles, yields a wide-range of algorithms from fields such as
optimization, deep learning, and graphical models. This includes classical
algorithms such as ridge regression, Newton's method, and Kalman filter, as
well as modern deep-learning algorithms such as stochastic-gradient descent,
RMSprop, and Dropout. The key idea in deriving such algorithms is to
approximate the posterior using candidate distributions estimated by using
natural gradients. Different candidate distributions result in different
algorithms and further approximations to natural gradients give rise to
variants of those algorithms. Our work not only unifies, generalizes, and
improves existing algorithms, but also helps us design new ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mohammad Emtiyaz Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rue_H/0/1/0/all/0/1"&gt;H&amp;#xe5;vard Rue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Machine Translation to Localize Task Oriented NLG Output. (arXiv:2107.04512v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04512</id>
        <link href="http://arxiv.org/abs/2107.04512"/>
        <updated>2021-07-12T01:55:16.230Z</updated>
        <summary type="html"><![CDATA[One of the challenges in a task oriented natural language application like
the Google Assistant, Siri, or Alexa is to localize the output to many
languages. This paper explores doing this by applying machine translation to
the English output. Using machine translation is very scalable, as it can work
with any English output and can handle dynamic text, but otherwise the problem
is a poor fit. The required quality bar is close to perfection, the range of
sentences is extremely narrow, and the sentences are often very different than
the ones in the machine translation training data. This combination of
requirements is novel in the field of domain adaptation for machine
translation. We are able to reach the required quality bar by building on
existing ideas and adding new ones: finetuning on in-domain translations,
adding sentences from the Web, adding semantic annotations, and using automatic
error detection. The paper shares our approach and results, together with a
distillation model to serve the translation models at scale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1"&gt;Scott Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brunk_C/0/1/0/all/0/1"&gt;Cliff Brunk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kyu-Young Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Justin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1"&gt;Markus Freitag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1"&gt;Mihir Kale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_G/0/1/0/all/0/1"&gt;Gagan Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mudgal_S/0/1/0/all/0/1"&gt;Sidharth Mudgal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varano_C/0/1/0/all/0/1"&gt;Chris Varano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GLIB: Towards Automated Test Oracle for Graphically-Rich Applications. (arXiv:2106.10507v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10507</id>
        <link href="http://arxiv.org/abs/2106.10507"/>
        <updated>2021-07-12T01:55:16.222Z</updated>
        <summary type="html"><![CDATA[Graphically-rich applications such as games are ubiquitous with attractive
visual effects of Graphical User Interface (GUI) that offers a bridge between
software applications and end-users. However, various types of graphical
glitches may arise from such GUI complexity and have become one of the main
component of software compatibility issues. Our study on bug reports from game
development teams in NetEase Inc. indicates that graphical glitches frequently
occur during the GUI rendering and severely degrade the quality of
graphically-rich applications such as video games. Existing automated testing
techniques for such applications focus mainly on generating various GUI test
sequences and check whether the test sequences can cause crashes. These
techniques require constant human attention to captures non-crashing bugs such
as bugs causing graphical glitches. In this paper, we present the first step in
automating the test oracle for detecting non-crashing bugs in graphically-rich
applications. Specifically, we propose \texttt{GLIB} based on a code-based data
augmentation technique to detect game GUI glitches. We perform an evaluation of
\texttt{GLIB} on 20 real-world game apps (with bug reports available) and the
result shows that \texttt{GLIB} can achieve 100\% precision and 99.5\% recall
in detecting non-crashing bugs such as game GUI glitches. Practical application
of \texttt{GLIB} on another 14 real-world games (without bug reports) further
demonstrates that \texttt{GLIB} can effectively uncover GUI glitches, with 48
of 53 bugs reported by \texttt{GLIB} having been confirmed and fixed so far.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Ke Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yufei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yingfeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1"&gt;Changjie Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhipeng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wei Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Learning of General-Purpose Embeddings for Code Changes. (arXiv:2106.02087v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02087</id>
        <link href="http://arxiv.org/abs/2106.02087"/>
        <updated>2021-07-12T01:55:16.215Z</updated>
        <summary type="html"><![CDATA[Applying machine learning to tasks that operate with code changes requires
their numerical representation. In this work, we propose an approach for
obtaining such representations during pre-training and evaluate them on two
different downstream tasks - applying changes to code and commit message
generation. During pre-training, the model learns to apply the given code
change in a correct way. This task requires only code changes themselves, which
makes it unsupervised. In the task of applying code changes, our model
outperforms baseline models by 5.9 percentage points in accuracy. As for the
commit message generation, our model demonstrated the same results as
supervised models trained for this specific task, which indicates that it can
encode code changes well and can be improved in the future by pre-training on a
larger dataset of easily gathered code changes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pravilov_M/0/1/0/all/0/1"&gt;Mikhail Pravilov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bogomolov_E/0/1/0/all/0/1"&gt;Egor Bogomolov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golubev_Y/0/1/0/all/0/1"&gt;Yaroslav Golubev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bryksin_T/0/1/0/all/0/1"&gt;Timofey Bryksin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ASC-Net : Adversarial-based Selective Network for Unsupervised Anomaly Segmentation. (arXiv:2103.03664v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03664</id>
        <link href="http://arxiv.org/abs/2103.03664"/>
        <updated>2021-07-12T01:55:16.208Z</updated>
        <summary type="html"><![CDATA[We introduce a neural network framework, utilizing adversarial learning to
partition an image into two cuts, with one cut falling into a reference
distribution provided by the user. This concept tackles the task of
unsupervised anomaly segmentation, which has attracted increasing attention in
recent years due to their broad applications in tasks with unlabelled data.
This Adversarial-based Selective Cutting network (ASC-Net) bridges the two
domains of cluster-based deep learning methods and adversarial-based
anomaly/novelty detection algorithms. We evaluate this unsupervised learning
model on BraTS brain tumor segmentation, LiTS liver lesion segmentation, and
MS-SEG2015 segmentation tasks. Compared to existing methods like the AnoGAN
family, our model demonstrates tremendous performance gains in unsupervised
anomaly segmentation tasks. Although there is still room to further improve
performance compared to supervised learning algorithms, the promising
experimental results shed light on building an unsupervised learning algorithm
using user-defined knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dey_R/0/1/0/all/0/1"&gt;Raunak Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1"&gt;Yi Hong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-level Stress Assessment from ECG in a Virtual Reality Environment using Multimodal Fusion. (arXiv:2107.04566v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04566</id>
        <link href="http://arxiv.org/abs/2107.04566"/>
        <updated>2021-07-12T01:55:16.196Z</updated>
        <summary type="html"><![CDATA[ECG is an attractive option to assess stress in serious Virtual Reality (VR)
applications due to its non-invasive nature. However, the existing Machine
Learning (ML) models perform poorly. Moreover, existing studies only perform a
binary stress assessment, while to develop a more engaging biofeedback-based
application, multi-level assessment is necessary. Existing studies annotate and
classify a single experience (e.g. watching a VR video) to a single stress
level, which again prevents design of dynamic experiences where real-time
in-game stress assessment can be utilized. In this paper, we report our
findings on a new study on VR stress assessment, where three stress levels are
assessed. ECG data was collected from 9 users experiencing a VR roller coaster.
The VR experience was then manually labeled in 10-seconds segments to three
stress levels by three raters. We then propose a novel multimodal deep fusion
model utilizing spectrogram and 1D ECG that can provide a stress prediction
from just a 1-second window. Experimental results demonstrate that the proposed
model outperforms the classical HRV-based ML models (9% increase in accuracy)
and baseline deep learning models (2.5% increase in accuracy). We also report
results on the benchmark WESAD dataset to show the supremacy of the model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmad_Z/0/1/0/all/0/1"&gt;Zeeshan Ahmad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabbani_S/0/1/0/all/0/1"&gt;Suha Rabbani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zafar_M/0/1/0/all/0/1"&gt;Muhammad Rehman Zafar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ishaque_S/0/1/0/all/0/1"&gt;Syem Ishaque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnan_S/0/1/0/all/0/1"&gt;Sridhar Krishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1"&gt;Naimul Khan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Offline reinforcement learning with uncertainty for treatment strategies in sepsis. (arXiv:2107.04491v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04491</id>
        <link href="http://arxiv.org/abs/2107.04491"/>
        <updated>2021-07-12T01:55:16.169Z</updated>
        <summary type="html"><![CDATA[Guideline-based treatment for sepsis and septic shock is difficult because
sepsis is a disparate range of life-threatening organ dysfunctions whose
pathophysiology is not fully understood. Early intervention in sepsis is
crucial for patient outcome, yet those interventions have adverse effects and
are frequently overadministered. Greater personalization is necessary, as no
single action is suitable for all patients. We present a novel application of
reinforcement learning in which we identify optimal recommendations for sepsis
treatment from data, estimate their confidence level, and identify treatment
options infrequently observed in training data. Rather than a single
recommendation, our method can present several treatment options. We examine
learned policies and discover that reinforcement learning is biased against
aggressive intervention due to the confounding relationship between mortality
and level of treatment received. We mitigate this bias using subspace learning,
and develop methodology that can yield more accurate learning policies across
healthcare applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Ran Liu&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Greenstein_J/0/1/0/all/0/1"&gt;Joseph L. Greenstein&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Fackler_J/0/1/0/all/0/1"&gt;James C. Fackler&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Bergmann_J/0/1/0/all/0/1"&gt;Jules Bergmann&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Bembea_M/0/1/0/all/0/1"&gt;Melania M. Bembea&lt;/a&gt; (3 and 4), &lt;a href="http://arxiv.org/find/cs/1/au:+Winslow_R/0/1/0/all/0/1"&gt;Raimond L. Winslow&lt;/a&gt; (1 and 2) ((1) Institute for Computational Medicine, the Johns Hopkins University, (2) Department of Biomedical Engineering, the Johns Hopkins University School of Medicine and Whiting School of Engineering, (3) Department of Anesthesiology and Critical Care Medicine, the Johns Hopkins University, (4) Department of Pediatrics, the Johns Hopkins University School of Medicine)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low Rank Saddle Free Newton: Scalable Stochastic Nonconvex Optimization. (arXiv:2002.02881v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.02881</id>
        <link href="http://arxiv.org/abs/2002.02881"/>
        <updated>2021-07-12T01:55:16.163Z</updated>
        <summary type="html"><![CDATA[Newton methods have fallen out of favor for modern optimization problems
(e.g. deep learning) because of concerns about per-iteration computational
complexity. In this setting highly subsampled first order methods are
preferred. In this work we motivate the extension of Newton methods to the
highly stochastic regime, and argue for the use of the scalable low rank saddle
free Newton (LRSFN) method. In this setting, iterative updates are dominated by
stochastic noise, and stability of the method is key. In stability analysis, we
demonstrate that stochastic errors for Newton methods can be greatly amplified
by ill-conditioned matrix operators. The LRSFN algorithm mitigates this issue
by the use of Levenberg-Marquardt damping, but generally second order methods
with stochastic Hessian and gradient information may need to take small steps,
unlike in deterministic problems. Numerical results show that even under
restrictive step-length conditions, LRSFN can outperform popular first order
methods on nontrivial deep learning tasks in terms of generalizability for
equivalent computational work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+OLeary_Roseberry_T/0/1/0/all/0/1"&gt;Thomas O&amp;#x27;Leary-Roseberry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Alger_N/0/1/0/all/0/1"&gt;Nick Alger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ghattas_O/0/1/0/all/0/1"&gt;Omar Ghattas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comparison of Contextual and Non-Contextual Preference Ranking for Set Addition Problems. (arXiv:2107.04438v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.04438</id>
        <link href="http://arxiv.org/abs/2107.04438"/>
        <updated>2021-07-12T01:55:16.118Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the problem of evaluating the addition of elements to
a set. This problem is difficult, because it can, in the general case, not be
reduced to unconditional preferences between the choices. Therefore, we model
preferences based on the context of the decision. We discuss and compare two
different Siamese network architectures for this task: a twin network that
compares the two sets resulting after the addition, and a triplet network that
models the contribution of each candidate to the existing set. We evaluate the
two settings on a real-world task; learning human card preferences for deck
building in the collectible card game Magic: The Gathering. We show that the
triplet approach achieves a better result than the twin network and that both
outperform previous results on this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bertram_T/0/1/0/all/0/1"&gt;Timo Bertram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Furnkranz_J/0/1/0/all/0/1"&gt;Johannes F&amp;#xfc;rnkranz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1"&gt;Martin M&amp;#xfc;ller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for Reduced Order Modelling and Efficient Temporal Evolution of Fluid Simulations. (arXiv:2107.04556v1 [physics.flu-dyn])]]></title>
        <id>http://arxiv.org/abs/2107.04556</id>
        <link href="http://arxiv.org/abs/2107.04556"/>
        <updated>2021-07-12T01:55:16.069Z</updated>
        <summary type="html"><![CDATA[Reduced Order Modelling (ROM) has been widely used to create lower order,
computationally inexpensive representations of higher-order dynamical systems.
Using these representations, ROMs can efficiently model flow fields while using
significantly lesser parameters. Conventional ROMs accomplish this by linearly
projecting higher-order manifolds to lower-dimensional space using
dimensionality reduction techniques such as Proper Orthogonal Decomposition
(POD). In this work, we develop a novel deep learning framework DL-ROM (Deep
Learning - Reduced Order Modelling) to create a neural network capable of
non-linear projections to reduced order states. We then use the learned reduced
state to efficiently predict future time steps of the simulation using 3D
Autoencoder and 3D U-Net based architectures. Our model DL-ROM is able to
create highly accurate reconstructions from the learned ROM and is thus able to
efficiently predict future time steps by temporally traversing in the learned
reduced state. All of this is achieved without ground truth supervision or
needing to iteratively solve the expensive Navier-Stokes(NS) equations thereby
resulting in massive computational savings. To test the effectiveness and
performance of our approach, we evaluate our implementation on five different
Computational Fluid Dynamics (CFD) datasets using reconstruction performance
and computational runtime metrics. DL-ROM can reduce the computational runtimes
of iterative solvers by nearly two orders of magnitude while maintaining an
acceptable error threshold.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Pant_P/0/1/0/all/0/1"&gt;Pranshu Pant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Doshi_R/0/1/0/all/0/1"&gt;Ruchit Doshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Bahl_P/0/1/0/all/0/1"&gt;Pranav Bahl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Farimani_A/0/1/0/all/0/1"&gt;Amir Barati Farimani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BayesSimIG: Scalable Parameter Inference for Adaptive Domain Randomization with IsaacGym. (arXiv:2107.04527v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.04527</id>
        <link href="http://arxiv.org/abs/2107.04527"/>
        <updated>2021-07-12T01:55:16.062Z</updated>
        <summary type="html"><![CDATA[BayesSim is a statistical technique for domain randomization in reinforcement
learning based on likelihood-free inference of simulation parameters. This
paper outlines BayesSimIG: a library that provides an implementation of
BayesSim integrated with the recently released NVIDIA IsaacGym. This
combination allows large-scale parameter inference with end-to-end GPU
acceleration. Both inference and simulation get GPU speedup, with support for
running more than 10K parallel simulation environments for complex robotics
tasks that can have more than 100 simulation parameters to estimate. BayesSimIG
provides an integration with TensorBoard to easily visualize slices of
high-dimensional posteriors. The library is built in a modular way to support
research experiments with novel ways to collect and process the trajectories
from the parallel IsaacGym environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Antonova_R/0/1/0/all/0/1"&gt;Rika Antonova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramos_F/0/1/0/all/0/1"&gt;Fabio Ramos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Possas_R/0/1/0/all/0/1"&gt;Rafael Possas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1"&gt;Dieter Fox&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Redescription Model Mining. (arXiv:2107.04462v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2107.04462</id>
        <link href="http://arxiv.org/abs/2107.04462"/>
        <updated>2021-07-12T01:55:16.054Z</updated>
        <summary type="html"><![CDATA[This paper introduces Redescription Model Mining, a novel approach to
identify interpretable patterns across two datasets that share only a subset of
attributes and have no common instances. In particular, Redescription Model
Mining aims to find pairs of describable data subsets -- one for each dataset
-- that induce similar exceptional models with respect to a prespecified model
class. To achieve this, we combine two previously separate research areas:
Exceptional Model Mining and Redescription Mining. For this new problem
setting, we develop interestingness measures to select promising patterns,
propose efficient algorithms, and demonstrate their potential on synthetic and
real-world data. Uncovered patterns can hint at common underlying phenomena
that manifest themselves across datasets, enabling the discovery of possible
associations between (combinations of) attributes that do not appear in the
same dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stamm_F/0/1/0/all/0/1"&gt;Felix I. Stamm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Becker_M/0/1/0/all/0/1"&gt;Martin Becker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strohmaier_M/0/1/0/all/0/1"&gt;Markus Strohmaier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lemmerich_F/0/1/0/all/0/1"&gt;Florian Lemmerich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Asymptotic Optimality of Conditioned Stochastic Gradient Descent. (arXiv:2006.02745v3 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.02745</id>
        <link href="http://arxiv.org/abs/2006.02745"/>
        <updated>2021-07-12T01:55:16.032Z</updated>
        <summary type="html"><![CDATA[In this paper, we investigate a general class of stochastic gradient descent
(SGD) algorithms, called conditioned SGD, based on a preconditioning of the
gradient direction. Under some mild assumptions, namely the $L$-smoothness of
the non-convex objective function and some weak growth condition on the noise,
we establish the almost sure convergence and the asymptotic normality for a
broad class of conditioning matrices. In particular, when the conditioning
matrix is an estimate of the inverse Hessian at the optimal point, the
algorithm is proved to be asymptotically optimal. The benefits of this approach
are validated on simulated and real datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Leluc_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Leluc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Portier_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Portier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ANCER: Anisotropic Certification via Sample-wise Volume Maximization. (arXiv:2107.04570v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04570</id>
        <link href="http://arxiv.org/abs/2107.04570"/>
        <updated>2021-07-12T01:55:16.024Z</updated>
        <summary type="html"><![CDATA[Randomized smoothing has recently emerged as an effective tool that enables
certification of deep neural network classifiers at scale. All prior art on
randomized smoothing has focused on isotropic $\ell_p$ certification, which has
the advantage of yielding certificates that can be easily compared among
isotropic methods via $\ell_p$-norm radius. However, isotropic certification
limits the region that can be certified around an input to worst-case
adversaries, \ie it cannot reason about other "close", potentially large,
constant prediction safe regions. To alleviate this issue, (i) we theoretically
extend the isotropic randomized smoothing $\ell_1$ and $\ell_2$ certificates to
their generalized anisotropic counterparts following a simplified analysis.
Moreover, (ii) we propose evaluation metrics allowing for the comparison of
general certificates - a certificate is superior to another if it certifies a
superset region - with the quantification of each certificate through the
volume of the certified region. We introduce ANCER, a practical framework for
obtaining anisotropic certificates for a given test set sample via volume
maximization. Our empirical results demonstrate that ANCER achieves
state-of-the-art $\ell_1$ and $\ell_2$ certified accuracy on both CIFAR-10 and
ImageNet at multiple radii, while certifying substantially larger regions in
terms of volume, thus highlighting the benefits of moving away from isotropic
analysis. Code used in our experiments is available in
https://github.com/MotasemAlfarra/ANCER.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eiras_F/0/1/0/all/0/1"&gt;Francisco Eiras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1"&gt;Motasem Alfarra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1"&gt;M. Pawan Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip H. S. Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1"&gt;Puneet K. Dokania&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1"&gt;Bernard Ghanem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1"&gt;Adel Bibi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Group-Node Attention for Community Evolution Prediction. (arXiv:2107.04522v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04522</id>
        <link href="http://arxiv.org/abs/2107.04522"/>
        <updated>2021-07-12T01:55:16.017Z</updated>
        <summary type="html"><![CDATA[Communities in social networks evolve over time as people enter and leave the
network and their activity behaviors shift. The task of predicting structural
changes in communities over time is known as community evolution prediction.
Existing work in this area has focused on the development of frameworks for
defining events while using traditional classification methods to perform the
actual prediction. We present a novel graph neural network for predicting
community evolution events from structural and temporal information. The model
(GNAN) includes a group-node attention component which enables support for
variable-sized inputs and learned representation of groups based on member and
neighbor node features. A comparative evaluation with standard baseline methods
is performed and we demonstrate that our model outperforms the baselines.
Additionally, we show the effects of network trends on model performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Revelle_M/0/1/0/all/0/1"&gt;Matt Revelle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Domeniconi_C/0/1/0/all/0/1"&gt;Carlotta Domeniconi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gelman_B/0/1/0/all/0/1"&gt;Ben Gelman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bib2Auth: Deep Learning Approach for Author Disambiguation using Bibliographic Data. (arXiv:2107.04382v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2107.04382</id>
        <link href="http://arxiv.org/abs/2107.04382"/>
        <updated>2021-07-12T01:55:16.008Z</updated>
        <summary type="html"><![CDATA[Author name ambiguity remains a critical open problem in digital libraries
due to synonymy and homonymy of names. In this paper, we propose a novel
approach to link author names to their real-world entities by relying on their
co-authorship pattern and area of research. Our supervised deep learning model
identifies an author by capturing his/her relationship with his/her co-authors
and area of research, which is represented by the titles and sources of the
target author's publications. These attributes are encoded by their semantic
and symbolic representations. To this end, Bib2Auth uses ~ 22K bibliographic
records from the DBLP repository and is trained with each pair of co-authors.
The extensive experiments have proved the capability of the approach to
distinguish between authors sharing the same name and recognize authors with
different name variations. Bib2Auth has shown good performance on a relatively
large dataset, which qualifies it to be directly integrated into bibliographic
indices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boukhers_Z/0/1/0/all/0/1"&gt;Zeyd Boukhers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bahubali_N/0/1/0/all/0/1"&gt;Nagaraj Bahubali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandrasekaran_A/0/1/0/all/0/1"&gt;Abinaya Thulsi Chandrasekaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1"&gt;Adarsh Anand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prasadand_S/0/1/0/all/0/1"&gt;Soniya Manchenahalli Gnanendra Prasadand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aralappa_S/0/1/0/all/0/1"&gt;Sriram Aralappa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Learning in the Teacher-Student Setup: Impact of Task Similarity. (arXiv:2107.04384v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.04384</id>
        <link href="http://arxiv.org/abs/2107.04384"/>
        <updated>2021-07-12T01:55:16.002Z</updated>
        <summary type="html"><![CDATA[Continual learning-the ability to learn many tasks in sequence-is critical
for artificial learning systems. Yet standard training methods for deep
networks often suffer from catastrophic forgetting, where learning new tasks
erases knowledge of earlier tasks. While catastrophic forgetting labels the
problem, the theoretical reasons for interference between tasks remain unclear.
Here, we attempt to narrow this gap between theory and practice by studying
continual learning in the teacher-student setup. We extend previous analytical
work on two-layer networks in the teacher-student setup to multiple teachers.
Using each teacher to represent a different task, we investigate how the
relationship between teachers affects the amount of forgetting and transfer
exhibited by the student when the task switches. In line with recent work, we
find that when tasks depend on similar features, intermediate task similarity
leads to greatest forgetting. However, feature similarity is only one way in
which tasks may be related. The teacher-student approach allows us to
disentangle task similarity at the level of readouts (hidden-to-output weights)
and features (input-to-hidden weights). We find a complex interplay between
both types of similarity, initial transfer/forgetting rates, maximum
transfer/forgetting, and long-term transfer/forgetting. Together, these results
help illuminate the diverse factors contributing to catastrophic forgetting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sebastian Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Goldt_S/0/1/0/all/0/1"&gt;Sebastian Goldt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Saxe_A/0/1/0/all/0/1"&gt;Andrew Saxe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Mixture Density Networks: Learning to Drive Safely from Collision Data. (arXiv:2107.04485v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04485</id>
        <link href="http://arxiv.org/abs/2107.04485"/>
        <updated>2021-07-12T01:55:15.981Z</updated>
        <summary type="html"><![CDATA[Imitation learning has been widely used to learn control policies for
autonomous driving based on pre-recorded data. However, imitation learning
based policies have been shown to be susceptible to compounding errors when
encountering states outside of the training distribution. Further, these agents
have been demonstrated to be easily exploitable by adversarial road users
aiming to create collisions. To overcome these shortcomings, we introduce
Adversarial Mixture Density Networks (AMDN), which learns two distributions
from separate datasets. The first is a distribution of safe actions learned
from a dataset of naturalistic human driving. The second is a distribution
representing unsafe actions likely to lead to collision, learned from a dataset
of collisions. During training, we leverage these two distributions to provide
an additional loss based on the similarity of the two distributions. By
penalising the safe action distribution based on its similarity to the unsafe
action distribution when training on the collision dataset, a more robust and
safe control policy is obtained. We demonstrate the proposed AMDN approach in a
vehicle following use-case, and evaluate under naturalistic and adversarial
testing environments. We show that despite its simplicity, AMDN provides
significant benefits for the safety of the learned control policy, when
compared to pure imitation learning or standard mixture density network
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuutti_S/0/1/0/all/0/1"&gt;Sampo Kuutti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fallah_S/0/1/0/all/0/1"&gt;Saber Fallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1"&gt;Richard Bowden&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UniRE: A Unified Label Space for Entity Relation Extraction. (arXiv:2107.04292v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04292</id>
        <link href="http://arxiv.org/abs/2107.04292"/>
        <updated>2021-07-12T01:55:15.975Z</updated>
        <summary type="html"><![CDATA[Many joint entity relation extraction models setup two separated label spaces
for the two sub-tasks (i.e., entity detection and relation classification). We
argue that this setting may hinder the information interaction between entities
and relations. In this work, we propose to eliminate the different treatment on
the two sub-tasks' label spaces. The input of our model is a table containing
all word pairs from a sentence. Entities and relations are represented by
squares and rectangles in the table. We apply a unified classifier to predict
each cell's label, which unifies the learning of two sub-tasks. For testing, an
effective (yet fast) approximate decoder is proposed for finding squares and
rectangles from tables. Experiments on three benchmarks (ACE04, ACE05, SciERC)
show that, using only half the number of parameters, our model achieves
competitive accuracy with the best extractor, and is faster.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yijun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changzhi Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yuanbin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junchi Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Batch Inverse-Variance Weighting: Deep Heteroscedastic Regression. (arXiv:2107.04497v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04497</id>
        <link href="http://arxiv.org/abs/2107.04497"/>
        <updated>2021-07-12T01:55:15.968Z</updated>
        <summary type="html"><![CDATA[Heteroscedastic regression is the task of supervised learning where each
label is subject to noise from a different distribution. This noise can be
caused by the labelling process, and impacts negatively the performance of the
learning algorithm as it violates the i.i.d. assumptions. In many situations
however, the labelling process is able to estimate the variance of such
distribution for each label, which can be used as an additional information to
mitigate this impact. We adapt an inverse-variance weighted mean square error,
based on the Gauss-Markov theorem, for parameter optimization on neural
networks. We introduce Batch Inverse-Variance, a loss function which is robust
to near-ground truth samples, and allows to control the effective learning
rate. Our experimental results show that BIV improves significantly the
performance of the networks on two noisy datasets, compared to L2 loss,
inverse-variance weighting, as well as a filtering-based baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mai_V/0/1/0/all/0/1"&gt;Vincent Mai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khamies_W/0/1/0/all/0/1"&gt;Waleed Khamies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paull_L/0/1/0/all/0/1"&gt;Liam Paull&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding surrogate explanations: the interplay between complexity, fidelity and coverage. (arXiv:2107.04309v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04309</id>
        <link href="http://arxiv.org/abs/2107.04309"/>
        <updated>2021-07-12T01:55:15.962Z</updated>
        <summary type="html"><![CDATA[This paper analyses the fundamental ingredients behind surrogate explanations
to provide a better understanding of their inner workings. We start our
exposition by considering global surrogates, describing the trade-off between
complexity of the surrogate and fidelity to the black-box being modelled. We
show that transitioning from global to local - reducing coverage - allows for
more favourable conditions on the Pareto frontier of fidelity-complexity of a
surrogate. We discuss the interplay between complexity, fidelity and coverage,
and consider how different user needs can lead to problem formulations where
these are either constraints or penalties. We also present experiments that
demonstrate how the local surrogate interpretability procedure can be made
interactive and lead to better explanations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Poyiadzi_R/0/1/0/all/0/1"&gt;Rafael Poyiadzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Renard_X/0/1/0/all/0/1"&gt;Xavier Renard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laugel_T/0/1/0/all/0/1"&gt;Thibault Laugel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_Rodriguez_R/0/1/0/all/0/1"&gt;Raul Santos-Rodriguez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Detyniecki_M/0/1/0/all/0/1"&gt;Marcin Detyniecki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiaccurate Proxies for Downstream Fairness. (arXiv:2107.04423v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04423</id>
        <link href="http://arxiv.org/abs/2107.04423"/>
        <updated>2021-07-12T01:55:15.955Z</updated>
        <summary type="html"><![CDATA[We study the problem of training a model that must obey demographic fairness
conditions when the sensitive features are not available at training time -- in
other words, how can we train a model to be fair by race when we don't have
data about race? We adopt a fairness pipeline perspective, in which an
"upstream" learner that does have access to the sensitive features will learn a
proxy model for these features from the other attributes. The goal of the proxy
is to allow a general "downstream" learner -- with minimal assumptions on their
prediction task -- to be able to use the proxy to train a model that is fair
with respect to the true sensitive features. We show that obeying multiaccuracy
constraints with respect to the downstream model class suffices for this
purpose, and provide sample- and oracle efficient-algorithms and generalization
bounds for learning such proxies. In general, multiaccuracy can be much easier
to satisfy than classification accuracy, and can be satisfied even when the
sensitive features are hard to predict.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diana_E/0/1/0/all/0/1"&gt;Emily Diana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gill_W/0/1/0/all/0/1"&gt;Wesley Gill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kearns_M/0/1/0/all/0/1"&gt;Michael Kearns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kenthapadi_K/0/1/0/all/0/1"&gt;Krishnaram Kenthapadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1"&gt;Aaron Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharifi_Malvajerdi_S/0/1/0/all/0/1"&gt;Saeed Sharifi-Malvajerdi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A First Look at Class Incremental Learning in Deep Learning Mobile Traffic Classification. (arXiv:2107.04464v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2107.04464</id>
        <link href="http://arxiv.org/abs/2107.04464"/>
        <updated>2021-07-12T01:55:15.937Z</updated>
        <summary type="html"><![CDATA[The recent popularity growth of Deep Learning (DL) re-ignited the interest
towards traffic classification, with several studies demonstrating the accuracy
of DL-based classifiers to identify Internet applications' traffic. Even with
the aid of hardware accelerators (GPUs, TPUs), DL model training remains
expensive, and limits the ability to operate frequent model updates necessary
to fit to the ever evolving nature of Internet traffic, and mobile traffic in
particular. To address this pain point, in this work we explore Incremental
Learning (IL) techniques to add new classes to models without a full
retraining, hence speeding up model's updates cycle. We consider iCarl, a state
of the art IL method, and MIRAGE-2019, a public dataset with traffic from 40
Android apps, aiming to understand "if there is a case for incremental learning
in traffic classification". By dissecting iCarl internals, we discuss ways to
improve its design, contributing a revised version, namely iCarl+. Despite our
analysis reveals their infancy, IL techniques are a promising research area on
the roadmap towards automated DL-based traffic analysis systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bovenzi_G/0/1/0/all/0/1"&gt;Giampaolo Bovenzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lixuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finamore_A/0/1/0/all/0/1"&gt;Alessandro Finamore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aceto_G/0/1/0/all/0/1"&gt;Giuseppe Aceto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciuonzo_D/0/1/0/all/0/1"&gt;Domenico Ciuonzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pescape_A/0/1/0/all/0/1"&gt;Antonio Pescap&amp;#xe8;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rossi_D/0/1/0/all/0/1"&gt;Dario Rossi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Detect Adversarial Examples Based on Class Scores. (arXiv:2107.04435v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04435</id>
        <link href="http://arxiv.org/abs/2107.04435"/>
        <updated>2021-07-12T01:55:15.928Z</updated>
        <summary type="html"><![CDATA[Given the increasing threat of adversarial attacks on deep neural networks
(DNNs), research on efficient detection methods is more important than ever. In
this work, we take a closer look at adversarial attack detection based on the
class scores of an already trained classification model. We propose to train a
support vector machine (SVM) on the class scores to detect adversarial
examples. Our method is able to detect adversarial examples generated by
various attacks, and can be easily adopted to a plethora of deep classification
models. We show that our approach yields an improved detection rate compared to
an existing method, whilst being easy to implement. We perform an extensive
empirical analysis on different deep classification models, investigating
various state-of-the-art adversarial attacks. Moreover, we observe that our
proposed method is better at detecting a combination of adversarial attacks.
This work indicates the potential of detecting various adversarial attacks
simply by using the class scores of an already trained classification model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Uelwer_T/0/1/0/all/0/1"&gt;Tobias Uelwer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michels_F/0/1/0/all/0/1"&gt;Felix Michels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Candido_O/0/1/0/all/0/1"&gt;Oliver De Candido&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Model Robustness with Latent Distribution Locally and Globally. (arXiv:2107.04401v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04401</id>
        <link href="http://arxiv.org/abs/2107.04401"/>
        <updated>2021-07-12T01:55:15.922Z</updated>
        <summary type="html"><![CDATA[In this work, we consider model robustness of deep neural networks against
adversarial attacks from a global manifold perspective. Leveraging both the
local and global latent information, we propose a novel adversarial training
method through robust optimization, and a tractable way to generate Latent
Manifold Adversarial Examples (LMAEs) via an adversarial game between a
discriminator and a classifier. The proposed adversarial training with latent
distribution (ATLD) method defends against adversarial attacks by crafting
LMAEs with the latent manifold in an unsupervised manner. ATLD preserves the
local and global information of latent manifold and promises improved
robustness against adversarial attacks. To verify the effectiveness of our
proposed method, we conduct extensive experiments over different datasets
(e.g., CIFAR-10, CIFAR-100, SVHN) with different adversarial attacks (e.g.,
PGD, CW), and show that our method substantially outperforms the
state-of-the-art (e.g., Feature Scattering) in adversarial robustness by a
large accuracy margin. The source codes are available at
https://github.com/LitterQ/ATLD-pytorch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1"&gt;Zhuang Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shufei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kaizhu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qiufeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1"&gt;Xinping Yi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autoencoder-driven Spiral Representation Learning for Gravitational Wave Surrogate Modelling. (arXiv:2107.04312v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04312</id>
        <link href="http://arxiv.org/abs/2107.04312"/>
        <updated>2021-07-12T01:55:15.914Z</updated>
        <summary type="html"><![CDATA[Recently, artificial neural networks have been gaining momentum in the field
of gravitational wave astronomy, for example in surrogate modelling of
computationally expensive waveform models for binary black hole inspiral and
merger. Surrogate modelling yields fast and accurate approximations of
gravitational waves and neural networks have been used in the final step of
interpolating the coefficients of the surrogate model for arbitrary waveforms
outside the training sample. We investigate the existence of underlying
structures in the empirical interpolation coefficients using autoencoders. We
demonstrate that when the coefficient space is compressed to only two
dimensions, a spiral structure appears, wherein the spiral angle is linearly
related to the mass ratio. Based on this finding, we design a spiral module
with learnable parameters, that is used as the first layer in a neural network,
which learns to map the input space to the coefficients. The spiral module is
evaluated on multiple neural network architectures and consistently achieves
better speed-accuracy trade-off than baseline models. A thorough experimental
study is conducted and the final result is a surrogate model which can evaluate
millions of input parameters in a single forward pass in under 1ms on a desktop
GPU, while the mismatch between the corresponding generated waveforms and the
ground-truth waveforms is better than the compared baseline methods. We
anticipate the existence of analogous underlying structures and corresponding
computational gains also in the case of spinning black hole binaries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nousi_P/0/1/0/all/0/1"&gt;Paraskevi Nousi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fragkouli_S/0/1/0/all/0/1"&gt;Styliani-Christina Fragkouli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Passalis_N/0/1/0/all/0/1"&gt;Nikolaos Passalis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iosif_P/0/1/0/all/0/1"&gt;Panagiotis Iosif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Apostolatos_T/0/1/0/all/0/1"&gt;Theocharis Apostolatos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pappas_G/0/1/0/all/0/1"&gt;George Pappas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stergioulas_N/0/1/0/all/0/1"&gt;Nikolaos Stergioulas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tefas_A/0/1/0/all/0/1"&gt;Anastasios Tefas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model compression as constrained optimization, with application to neural nets. Part V: combining compressions. (arXiv:2107.04380v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04380</id>
        <link href="http://arxiv.org/abs/2107.04380"/>
        <updated>2021-07-12T01:55:15.906Z</updated>
        <summary type="html"><![CDATA[Model compression is generally performed by using quantization, low-rank
approximation or pruning, for which various algorithms have been researched in
recent years. One fundamental question is: what types of compression work
better for a given model? Or even better: can we improve by combining
compressions in a suitable way? We formulate this generally as a problem of
optimizing the loss but where the weights are constrained to equal an additive
combination of separately compressed parts; and we give an algorithm to learn
the corresponding parts' parameters. Experimentally with deep neural nets, we
observe that 1) we can find significantly better models in the
error-compression space, indicating that different compression types have
complementary benefits, and 2) the best type of combination depends exquisitely
on the type of neural net. For example, we can compress ResNets and AlexNet
using only 1 bit per weight without error degradation at the cost of adding a
few floating point weights. However, VGG nets can be better compressed by
combining low-rank with a few floating point weights.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carreira_Perpinan_M/0/1/0/all/0/1"&gt;Miguel &amp;#xc1;. Carreira-Perpi&amp;#xf1;&amp;#xe1;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Idelbayev_Y/0/1/0/all/0/1"&gt;Yerlan Idelbayev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to choose an Explainability Method? Towards a Methodical Implementation of XAI in Practice. (arXiv:2107.04427v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04427</id>
        <link href="http://arxiv.org/abs/2107.04427"/>
        <updated>2021-07-12T01:55:15.889Z</updated>
        <summary type="html"><![CDATA[Explainability is becoming an important requirement for organizations that
make use of automated decision-making due to regulatory initiatives and a shift
in public awareness. Various and significantly different algorithmic methods to
provide this explainability have been introduced in the field, but the existing
literature in the machine learning community has paid little attention to the
stakeholder whose needs are rather studied in the human-computer interface
community. Therefore, organizations that want or need to provide this
explainability are confronted with the selection of an appropriate method for
their use case. In this paper, we argue there is a need for a methodology to
bridge the gap between stakeholder needs and explanation methods. We present
our ongoing work on creating this methodology to help data scientists in the
process of providing explainability to stakeholders. In particular, our
contributions include documents used to characterize XAI methods and user
requirements (shown in Appendix), which our methodology builds upon.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vermeire_T/0/1/0/all/0/1"&gt;Tom Vermeire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laugel_T/0/1/0/all/0/1"&gt;Thibault Laugel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Renard_X/0/1/0/all/0/1"&gt;Xavier Renard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martens_D/0/1/0/all/0/1"&gt;David Martens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Detyniecki_M/0/1/0/all/0/1"&gt;Marcin Detyniecki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Low-Resource Neural Machine Translation. (arXiv:2107.04239v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04239</id>
        <link href="http://arxiv.org/abs/2107.04239"/>
        <updated>2021-07-12T01:55:15.882Z</updated>
        <summary type="html"><![CDATA[Neural approaches have achieved state-of-the-art accuracy on machine
translation but suffer from the high cost of collecting large scale parallel
data. Thus, a lot of research has been conducted for neural machine translation
(NMT) with very limited parallel data, i.e., the low-resource setting. In this
paper, we provide a survey for low-resource NMT and classify related works into
three categories according to the auxiliary data they used: (1) exploiting
monolingual data of source and/or target languages, (2) exploiting data from
auxiliary languages, and (3) exploiting multi-modal data. We hope that our
survey can help researchers to better understand this field and inspire them to
design better algorithms, and help industry practitioners to choose appropriate
algorithms for their applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xu Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1"&gt;Renqian Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convergence analysis for gradient flows in the training of artificial neural networks with ReLU activation. (arXiv:2107.04479v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04479</id>
        <link href="http://arxiv.org/abs/2107.04479"/>
        <updated>2021-07-12T01:55:15.876Z</updated>
        <summary type="html"><![CDATA[Gradient descent (GD) type optimization schemes are the standard methods to
train artificial neural networks (ANNs) with rectified linear unit (ReLU)
activation. Such schemes can be considered as discretizations of gradient flows
(GFs) associated to the training of ANNs with ReLU activation and most of the
key difficulties in the mathematical convergence analysis of GD type
optimization schemes in the training of ANNs with ReLU activation seem to be
already present in the dynamics of the corresponding GF differential equations.
It is the key subject of this work to analyze such GF differential equations in
the training of ANNs with ReLU activation and three layers (one input layer,
one hidden layer, and one output layer). In particular, in this article we
prove in the case where the target function is possibly multi-dimensional and
continuous and in the case where the probability distribution of the input data
is absolutely continuous with respect to the Lebesgue measure that the risk of
every bounded GF trajectory converges to the risk of a critical point. In
addition, in this article we show in the case of a 1-dimensional affine linear
target function and in the case where the probability distribution of the input
data coincides with the standard uniform distribution that the risk of every
bounded GF trajectory converges to zero if the initial risk is sufficiently
small. Finally, in the special situation where there is only one neuron on the
hidden layer (1-dimensional hidden layer) we strengthen the above named result
for affine linear target functions by proving that that the risk of every (not
necessarily bounded) GF trajectory converges to zero if the initial risk is
sufficiently small.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jentzen_A/0/1/0/all/0/1"&gt;Arnulf Jentzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riekert_A/0/1/0/all/0/1"&gt;Adrian Riekert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic Trajectory Prediction with Structural Constraints. (arXiv:2107.04193v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.04193</id>
        <link href="http://arxiv.org/abs/2107.04193"/>
        <updated>2021-07-12T01:55:15.869Z</updated>
        <summary type="html"><![CDATA[This work addresses the problem of predicting the motion trajectories of
dynamic objects in the environment. Recent advances in predicting motion
patterns often rely on machine learning techniques to extrapolate motion
patterns from observed trajectories, with no mechanism to directly incorporate
known rules. We propose a novel framework, which combines probabilistic
learning and constrained trajectory optimisation. The learning component of our
framework provides a distribution over future motion trajectories conditioned
on observed past coordinates. This distribution is then used as a prior to a
constrained optimisation problem which enforces chance constraints on the
trajectory distribution. This results in constraint-compliant trajectory
distributions which closely resemble the prior. In particular, we focus our
investigation on collision constraints, such that extrapolated future
trajectory distributions conform to the environment structure. We empirically
demonstrate on real-world and simulated datasets the ability of our framework
to learn complex probabilistic motion trajectories for motion data, while
directly enforcing constraints to improve generalisability, producing more
robust and higher quality trajectory distributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhi_W/0/1/0/all/0/1"&gt;Weiming Zhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ott_L/0/1/0/all/0/1"&gt;Lionel Ott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramos_F/0/1/0/all/0/1"&gt;Fabio Ramos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multitask Multi-database Emotion Recognition. (arXiv:2107.04127v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04127</id>
        <link href="http://arxiv.org/abs/2107.04127"/>
        <updated>2021-07-12T01:55:15.862Z</updated>
        <summary type="html"><![CDATA[In this work, we introduce our submission to the 2nd Affective Behavior
Analysis in-the-wild (ABAW) 2021 competition. We train a unified deep learning
model on multi-databases to perform two tasks: seven basic facial expressions
prediction and valence-arousal estimation. Since these databases do not
contains labels for all the two tasks, we have applied the distillation
knowledge technique to train two networks: one teacher and one student model.
The student model will be trained using both ground truth labels and soft
labels derived from the pretrained teacher model. During the training, we add
one more task, which is the combination of the two mentioned tasks, for better
exploiting inter-task correlations. We also exploit the sharing videos between
the two tasks of the AffWild2 database that is used in the competition, to
further improve the performance of the network. Experiment results shows that
the network have achieved promising results on the validation set of the
AffWild2 database. Code and pretrained model are publicly available at
https://github.com/glmanhtu/multitask-abaw-2021]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vu_M/0/1/0/all/0/1"&gt;Manh Tu Vu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beurton_Aimar_M/0/1/0/all/0/1"&gt;Marie Beurton-Aimar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hoechst Is All You Need: LymphocyteClassification with Deep Learning. (arXiv:2107.04388v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04388</id>
        <link href="http://arxiv.org/abs/2107.04388"/>
        <updated>2021-07-12T01:55:15.855Z</updated>
        <summary type="html"><![CDATA[Multiplex immunofluorescence and immunohistochemistry benefit patients by
allowing cancer pathologists to identify several proteins expressed on the
surface of cells, enabling cell classification, better understanding of the
tumour micro-environment, more accurate diagnoses, prognoses, and tailored
immunotherapy based on the immune status of individual patients. However, they
are expensive and time consuming processes which require complex staining and
imaging techniques by expert technicians. Hoechst staining is much cheaper and
easier to perform, but is not typically used in this case as it binds to DNA
rather than to the proteins targeted by immunofluorescent techniques, and it
was not previously thought possible to differentiate cells expressing these
proteins based only on DNA morphology. In this work we show otherwise, training
a deep convolutional neural network to identify cells expressing three proteins
(T lymphocyte markers CD3 and CD8, and the B lymphocyte marker CD20) with
greater than 90% precision and recall, from Hoechst 33342 stained tissue only.
Our model learns previously unknown morphological features associated with
expression of these proteins which can be used to accurately differentiate
lymphocyte subtypes for use in key prognostic metrics such as assessment of
immune cell infiltration,and thereby predict and improve patient outcomes
without the need for costly multiplex immunofluorescence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cooper_J/0/1/0/all/0/1"&gt;Jessica Cooper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Um_I/0/1/0/all/0/1"&gt;In Hwa Um&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1"&gt;Ognjen Arandjelovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harrison_D/0/1/0/all/0/1"&gt;David J Harrison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Image Synthesis from Intuitive User Input: A Review and Perspectives. (arXiv:2107.04240v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04240</id>
        <link href="http://arxiv.org/abs/2107.04240"/>
        <updated>2021-07-12T01:55:15.835Z</updated>
        <summary type="html"><![CDATA[In many applications of computer graphics, art and design, it is desirable
for a user to provide intuitive non-image input, such as text, sketch, stroke,
graph or layout, and have a computer system automatically generate
photo-realistic images that adhere to the input content. While classic works
that allow such automatic image content generation have followed a framework of
image retrieval and composition, recent advances in deep generative models such
as generative adversarial networks (GANs), variational autoencoders (VAEs), and
flow-based methods have enabled more powerful and versatile image generation
tasks. This paper reviews recent works for image synthesis given intuitive user
input, covering advances in input versatility, image generation methodology,
benchmark datasets, and evaluation metrics. This motivates new perspectives on
input representation and interactivity, cross pollination between major image
generation paradigms, and evaluation and comparison of generation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1"&gt;Yuan Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yuan-Chen Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Han Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1"&gt;Tao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Song-Hai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaolei Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Specialists Outperform Generalists in Ensemble Classification. (arXiv:2107.04381v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04381</id>
        <link href="http://arxiv.org/abs/2107.04381"/>
        <updated>2021-07-12T01:55:15.827Z</updated>
        <summary type="html"><![CDATA[Consider an ensemble of $k$ individual classifiers whose accuracies are
known. Upon receiving a test point, each of the classifiers outputs a predicted
label and a confidence in its prediction for this particular test point. In
this paper, we address the question of whether we can determine the accuracy of
the ensemble. Surprisingly, even when classifiers are combined in the
statistically optimal way in this setting, the accuracy of the resulting
ensemble classifier cannot be computed from the accuracies of the individual
classifiers-as would be the case in the standard setting of confidence weighted
majority voting. We prove tight upper and lower bounds on the ensemble
accuracy. We explicitly construct the individual classifiers that attain the
upper and lower bounds: specialists and generalists. Our theoretical results
have very practical consequences: (1) If we use ensemble methods and have the
choice to construct our individual (independent) classifiers from scratch, then
we should aim for specialist classifiers rather than generalists. (2) Our
bounds can be used to determine how many classifiers are at least required to
achieve a desired ensemble accuracy. Finally, we improve our bounds by
considering the mutual information between the true label and the individual
classifier's output.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meyen_S/0/1/0/all/0/1"&gt;Sascha Meyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goppert_F/0/1/0/all/0/1"&gt;Frieder G&amp;#xf6;ppert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alber_H/0/1/0/all/0/1"&gt;Helen Alber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luxburg_U/0/1/0/all/0/1"&gt;Ulrike von Luxburg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franz_V/0/1/0/all/0/1"&gt;Volker H. Franz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IDRLnet: A Physics-Informed Neural Network Library. (arXiv:2107.04320v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04320</id>
        <link href="http://arxiv.org/abs/2107.04320"/>
        <updated>2021-07-12T01:55:15.820Z</updated>
        <summary type="html"><![CDATA[Physics Informed Neural Network (PINN) is a scientific computing framework
used to solve both forward and inverse problems modeled by Partial Differential
Equations (PDEs). This paper introduces IDRLnet, a Python toolbox for modeling
and solving problems through PINN systematically. IDRLnet constructs the
framework for a wide range of PINN algorithms and applications. It provides a
structured way to incorporate geometric objects, data sources, artificial
neural networks, loss metrics, and optimizers within Python. Furthermore, it
provides functionality to solve noisy inverse problems, variational
minimization, and integral differential equations. New PINN variants can be
integrated into the framework easily. Source code, tutorials, and documentation
are available at \url{https://github.com/idrl-lab/idrlnet}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1"&gt;Wei Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Weien Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xiaoyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1"&gt;Wen Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaoqian Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[REX: Revisiting Budgeted Training with an Improved Schedule. (arXiv:2107.04197v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04197</id>
        <link href="http://arxiv.org/abs/2107.04197"/>
        <updated>2021-07-12T01:55:15.807Z</updated>
        <summary type="html"><![CDATA[Deep learning practitioners often operate on a computational and monetary
budget. Thus, it is critical to design optimization algorithms that perform
well under any budget. The linear learning rate schedule is considered the best
budget-aware schedule, as it outperforms most other schedules in the low budget
regime. On the other hand, learning rate schedules -- such as the
\texttt{30-60-90} step schedule -- are known to achieve high performance when
the model can be trained for many epochs. Yet, it is often not known a priori
whether one's budget will be large or small; thus, the optimal choice of
learning rate schedule is made on a case-by-case basis. In this paper, we frame
the learning rate schedule selection problem as a combination of $i)$ selecting
a profile (i.e., the continuous function that models the learning rate
schedule), and $ii)$ choosing a sampling rate (i.e., how frequently the
learning rate is updated/sampled from this profile). We propose a novel profile
and sampling rate combination called the Reflected Exponential (REX) schedule,
which we evaluate across seven different experimental settings with both SGD
and Adam optimizers. REX outperforms the linear schedule in the low budget
regime, while matching or exceeding the performance of several state-of-the-art
learning rate schedules (linear, step, exponential, cosine, step decay on
plateau, and OneCycle) in both high and low budget regimes. Furthermore, REX
requires no added computation, storage, or hyperparameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;John Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1"&gt;Cameron Wolfe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1"&gt;Anastasios Kyrillidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training a Deep Neural Network via Policy Gradients for Blind Source Separation in Polyphonic Music Recordings. (arXiv:2107.04235v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.04235</id>
        <link href="http://arxiv.org/abs/2107.04235"/>
        <updated>2021-07-12T01:55:15.800Z</updated>
        <summary type="html"><![CDATA[We propose a method for the blind separation of sounds of musical instruments
in audio signals. We describe the individual tones via a parametric model,
training a dictionary to capture the relative amplitudes of the harmonics. The
model parameters are predicted via a U-Net, which is a type of deep neural
network. The network is trained without ground truth information, based on the
difference between the model prediction and the individual STFT time frames.
Since some of the model parameters do not yield a useful backpropagation
gradient, we model them stochastically and employ the policy gradient instead.
To provide phase information and account for inaccuracies in the
dictionary-based representation, we also let the network output a direct
prediction, which we then use to resynthesize the audio signals for the
individual instruments. Due to the flexibility of the neural network,
inharmonicity can be incorporated seamlessly and no preprocessing of the input
spectra is required. Our algorithm yields high-quality separation results with
particularly low interference on a variety of different audio samples, both
acoustic and synthetic, provided that the sample contains enough data for the
training and that the spectral characteristics of the musical instruments are
sufficiently stable to be approximated by the dictionary.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Schulze_S/0/1/0/all/0/1"&gt;S&amp;#xf6;ren Schulze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Leuschner_J/0/1/0/all/0/1"&gt;Johannes Leuschner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+King_E/0/1/0/all/0/1"&gt;Emily J. King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sensitivity analysis in differentially private machine learning using hybrid automatic differentiation. (arXiv:2107.04265v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04265</id>
        <link href="http://arxiv.org/abs/2107.04265"/>
        <updated>2021-07-12T01:55:15.780Z</updated>
        <summary type="html"><![CDATA[In recent years, formal methods of privacy protection such as differential
privacy (DP), capable of deployment to data-driven tasks such as machine
learning (ML), have emerged. Reconciling large-scale ML with the closed-form
reasoning required for the principled analysis of individual privacy loss
requires the introduction of new tools for automatic sensitivity analysis and
for tracking an individual's data and their features through the flow of
computation. For this purpose, we introduce a novel \textit{hybrid} automatic
differentiation (AD) system which combines the efficiency of reverse-mode AD
with an ability to obtain a closed-form expression for any given quantity in
the computational graph. This enables modelling the sensitivity of arbitrary
differentiable function compositions, such as the training of neural networks
on private data. We demonstrate our approach by analysing the individual DP
guarantees of statistical database queries. Moreover, we investigate the
application of our technique to the training of DP neural networks. Our
approach can enable the principled reasoning about privacy loss in the setting
of data processing, and further the development of automatic sensitivity
analysis and privacy budgeting systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1"&gt;Alexander Ziller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usynin_D/0/1/0/all/0/1"&gt;Dmitrii Usynin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knolle_M/0/1/0/all/0/1"&gt;Moritz Knolle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prakash_K/0/1/0/all/0/1"&gt;Kritika Prakash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trask_A/0/1/0/all/0/1"&gt;Andrew Trask&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Braren_R/0/1/0/all/0/1"&gt;Rickmer Braren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1"&gt;Marcus Makowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1"&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1"&gt;Georgios Kaissis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially private training of neural networks with Langevin dynamics forcalibrated predictive uncertainty. (arXiv:2107.04296v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04296</id>
        <link href="http://arxiv.org/abs/2107.04296"/>
        <updated>2021-07-12T01:55:15.773Z</updated>
        <summary type="html"><![CDATA[We show that differentially private stochastic gradient descent (DP-SGD) can
yield poorly calibrated, overconfident deep learning models. This represents a
serious issue for safety-critical applications, e.g. in medical diagnosis. We
highlight and exploit parallels between stochastic gradient Langevin dynamics,
a scalable Bayesian inference technique for training deep neural networks, and
DP-SGD, in order to train differentially private, Bayesian neural networks with
minor adjustments to the original (DP-SGD) algorithm. Our approach provides
considerably more reliable uncertainty estimates than DP-SGD, as demonstrated
empirically by a reduction in expected calibration error (MNIST $\sim{5}$-fold,
Pediatric Pneumonia Dataset $\sim{2}$-fold).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Knolle_M/0/1/0/all/0/1"&gt;Moritz Knolle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1"&gt;Alexander Ziller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usynin_D/0/1/0/all/0/1"&gt;Dmitrii Usynin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Braren_R/0/1/0/all/0/1"&gt;Rickmer Braren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1"&gt;Marcus R. Makowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1"&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1"&gt;Georgios Kaissis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safe Exploration by Solving Early Terminated MDP. (arXiv:2107.04200v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04200</id>
        <link href="http://arxiv.org/abs/2107.04200"/>
        <updated>2021-07-12T01:55:15.765Z</updated>
        <summary type="html"><![CDATA[Safe exploration is crucial for the real-world application of reinforcement
learning (RL). Previous works consider the safe exploration problem as
Constrained Markov Decision Process (CMDP), where the policies are being
optimized under constraints. However, when encountering any potential dangers,
human tends to stop immediately and rarely learns to behave safely in danger.
Motivated by human learning, we introduce a new approach to address safe RL
problems under the framework of Early Terminated MDP (ET-MDP). We first define
the ET-MDP as an unconstrained MDP with the same optimal value function as its
corresponding CMDP. An off-policy algorithm based on context models is then
proposed to solve the ET-MDP, which thereby solves the corresponding CMDP with
better asymptotic performance and improved learning efficiency. Experiments on
various CMDP tasks show a substantial improvement over previous methods that
directly solve CMDP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Hao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Ziping Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1"&gt;Meng Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1"&gt;Zhenghao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jiadong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1"&gt;Bo Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bolei Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EasyCom: An Augmented Reality Dataset to Support Algorithms for Easy Communication in Noisy Environments. (arXiv:2107.04174v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.04174</id>
        <link href="http://arxiv.org/abs/2107.04174"/>
        <updated>2021-07-12T01:55:15.758Z</updated>
        <summary type="html"><![CDATA[Augmented Reality (AR) as a platform has the potential to facilitate the
reduction of the cocktail party effect. Future AR headsets could potentially
leverage information from an array of sensors spanning many different
modalities. Training and testing signal processing and machine learning
algorithms on tasks such as beam-forming and speech enhancement require high
quality representative data. To the best of the author's knowledge, as of
publication there are no available datasets that contain synchronized
egocentric multi-channel audio and video with dynamic movement and
conversations in a noisy environment. In this work, we describe, evaluate and
release a dataset that contains over 5 hours of multi-modal data useful for
training and testing algorithms for the application of improving conversations
for an AR glasses wearer. We provide speech intelligibility, quality and
signal-to-noise ratio improvement results for a baseline method and show
improvements across all tested metrics. The dataset we are releasing contains
AR glasses egocentric multi-channel microphone array audio, wide field-of-view
RGB video, speech source pose, headset microphone audio, annotated voice
activity, speech transcriptions, head bounding boxes, target of speech and
source identification labels. We have created and are releasing this dataset to
facilitate research in multi-modal AR solutions to the cocktail party problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Donley_J/0/1/0/all/0/1"&gt;Jacob Donley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tourbabin_V/0/1/0/all/0/1"&gt;Vladimir Tourbabin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jung-Suk Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Broyles_M/0/1/0/all/0/1"&gt;Mark Broyles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Hao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jie Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pantic_M/0/1/0/all/0/1"&gt;Maja Pantic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ithapu_V/0/1/0/all/0/1"&gt;Vamsi Krishna Ithapu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehra_R/0/1/0/all/0/1"&gt;Ravish Mehra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph-based Deep Generative Modelling for Document Layout Generation. (arXiv:2107.04357v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04357</id>
        <link href="http://arxiv.org/abs/2107.04357"/>
        <updated>2021-07-12T01:55:15.751Z</updated>
        <summary type="html"><![CDATA[One of the major prerequisites for any deep learning approach is the
availability of large-scale training data. When dealing with scanned document
images in real world scenarios, the principal information of its content is
stored in the layout itself. In this work, we have proposed an automated deep
generative model using Graph Neural Networks (GNNs) to generate synthetic data
with highly variable and plausible document layouts that can be used to train
document interpretation systems, in this case, specially in digital mailroom
applications. It is also the first graph-based approach for document layout
generation task experimented on administrative document images, in this case,
invoices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1"&gt;Sanket Biswas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riba_P/0/1/0/all/0/1"&gt;Pau Riba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Llados_J/0/1/0/all/0/1"&gt;Josep Llad&amp;#xf3;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1"&gt;Umapada Pal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Greedy structure learning from data that contains systematic missing values. (arXiv:2107.04184v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04184</id>
        <link href="http://arxiv.org/abs/2107.04184"/>
        <updated>2021-07-12T01:55:15.732Z</updated>
        <summary type="html"><![CDATA[Learning from data that contain missing values represents a common phenomenon
in many domains. Relatively few Bayesian Network structure learning algorithms
account for missing data, and those that do tend to rely on standard approaches
that assume missing data are missing at random, such as the
Expectation-Maximisation algorithm. Because missing data are often systematic,
there is a need for more pragmatic methods that can effectively deal with data
sets containing missing values not missing at random. The absence of approaches
that deal with systematic missing data impedes the application of BN structure
learning methods to real-world problems where missingness are not random. This
paper describes three variants of greedy search structure learning that utilise
pairwise deletion and inverse probability weighting to maximally leverage the
observed data and to limit potential bias caused by missing values. The first
two of the variants can be viewed as sub-versions of the third and best
performing variant, but are important in their own in illustrating the
successive improvements in learning accuracy. The empirical investigations show
that the proposed approach outperforms the commonly used and state-of-the-art
Structural EM algorithm, both in terms of learning accuracy and efficiency, as
well as both when data are missing at random and not at random.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Constantinou_A/0/1/0/all/0/1"&gt;Anthony C. Constantinou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Federated Learning over non-IID Data for Indoor Localization. (arXiv:2107.04189v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04189</id>
        <link href="http://arxiv.org/abs/2107.04189"/>
        <updated>2021-07-12T01:55:15.716Z</updated>
        <summary type="html"><![CDATA[Localization and tracking of objects using data-driven methods is a popular
topic due to the complexity in characterizing the physics of wireless channel
propagation models. In these modeling approaches, data needs to be gathered to
accurately train models, at the same time that user's privacy is maintained. An
appealing scheme to cooperatively achieve these goals is known as Federated
Learning (FL). A challenge in FL schemes is the presence of non-independent and
identically distributed (non-IID) data, caused by unevenly exploration of
different areas. In this paper, we consider the use of recent FL schemes to
train a set of personalized models that are then optimally fused through
Bayesian rules, which makes it appropriate in the context of indoor
localization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1"&gt;Peng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Imbiriba_T/0/1/0/all/0/1"&gt;Tales Imbiriba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Junha Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sunwoo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Closas_P/0/1/0/all/0/1"&gt;Pau Closas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-path Convolutional Neural Networks Efficiently Improve Feature Extraction in Continuous Adventitious Lung Sound Detection. (arXiv:2107.04226v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.04226</id>
        <link href="http://arxiv.org/abs/2107.04226"/>
        <updated>2021-07-12T01:55:15.710Z</updated>
        <summary type="html"><![CDATA[We previously established a large lung sound database, HF_Lung_V2 (Lung_V2).
We trained convolutional-bidirectional gated recurrent unit (CNN-BiGRU)
networks for detecting inhalation, exhalation, continuous adventitious sound
(CAS) and discontinuous adventitious sound at the recording level on the basis
of Lung_V2. However, the performance of CAS detection was poor due to many
reasons, one of which is the highly diversified CAS patterns. To make the
original CNN-BiGRU model learn the CAS patterns more effectively and not cause
too much computing burden, three strategies involving minimal modifications of
the network architecture of the CNN layers were investigated: (1) making the
CNN layers a bit deeper by using the residual blocks, (2) making the CNN layers
a bit wider by increasing the number of CNN kernels, and (3) separating the
feature input into multiple paths (the model was denoted by Multi-path
CNN-BiGRU). The performance of CAS segment and event detection were evaluated.
Results showed that improvement in CAS detection was observed among all the
proposed architecture-modified models. The F1 score for CAS event detection of
the proposed models increased from 0.445 to 0.491-0.530, which was deemed
significant. However, the Multi-path CNN-BiGRU model outperformed the other
models in terms of the number of winning titles (five) in total nine evaluation
metrics. In addition, the Multi-path CNN-BiGRU model did not cause extra
computing burden (0.97-fold inference time) compared to the original CNN-BiGRU
model. Conclusively, the Multi-path CNN layers can efficiently improve the
effectiveness of feature extraction and subsequently result in better CAS
detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_F/0/1/0/all/0/1"&gt;Fu-Shun Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shang-Ran Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chien-Wen Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chun-Chieh Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yuan-Ren Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_F/0/1/0/all/0/1"&gt;Feipei Lai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On lattice-free boosted MMI training of HMM and CTC-based full-context ASR models. (arXiv:2107.04154v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.04154</id>
        <link href="http://arxiv.org/abs/2107.04154"/>
        <updated>2021-07-12T01:55:15.687Z</updated>
        <summary type="html"><![CDATA[Hybrid automatic speech recognition (ASR) models are typically sequentially
trained with CTC or LF-MMI criteria. However, they have vastly different
legacies and are usually implemented in different frameworks. In this paper, by
decoupling the concepts of modeling units and label topologies and building
proper numerator/denominator graphs accordingly, we establish a generalized
framework for hybrid acoustic modeling (AM). In this framework, we show that
LF-MMI is a powerful training criterion applicable to both limited-context and
full-context models, for wordpiece/mono-char/bi-char/chenone units, with both
HMM/CTC topologies. From this framework, we propose three novel training
schemes: chenone(ch)/wordpiece(wp)-CTC-bMMI, and wordpiece(wp)-HMM-bMMI with
different advantages in training performance, decoding efficiency and decoding
time-stamp accuracy. The advantages of different training schemes are evaluated
comprehensively on Librispeech, and wp-CTC-bMMI and ch-CTC-bMMI are evaluated
on two real world ASR tasks to show their effectiveness. Besides, we also show
bi-char(bc) HMM-MMI models can serve as better alignment models than
traditional non-neural GMM-HMMs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaohui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Manohar_V/0/1/0/all/0/1"&gt;Vimal Manohar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1"&gt;David Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Frank Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yangyang Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Singhal_N/0/1/0/all/0/1"&gt;Nayan Singhal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chan_J/0/1/0/all/0/1"&gt;Julian Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Peng_F/0/1/0/all/0/1"&gt;Fuchun Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Saraf_Y/0/1/0/all/0/1"&gt;Yatharth Saraf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Seltzer_M/0/1/0/all/0/1"&gt;Mike Seltzer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NDPNet: A novel non-linear data projection network for few-shot fine-grained image classification. (arXiv:2106.06988v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06988</id>
        <link href="http://arxiv.org/abs/2106.06988"/>
        <updated>2021-07-12T01:55:15.679Z</updated>
        <summary type="html"><![CDATA[Metric-based few-shot fine-grained image classification (FSFGIC) aims to
learn a transferable feature embedding network by estimating the similarities
between query images and support classes from very few examples. In this work,
we propose, for the first time, to introduce the non-linear data projection
concept into the design of FSFGIC architecture in order to address the limited
sample problem in few-shot learning and at the same time to increase the
discriminability of the model for fine-grained image classification.
Specifically, we first design a feature re-abstraction embedding network that
has the ability to not only obtain the required semantic features for effective
metric learning but also re-enhance such features with finer details from input
images. Then the descriptors of the query images and the support classes are
projected into different non-linear spaces in our proposed similarity metric
learning network to learn discriminative projection factors. This design can
effectively operate in the challenging and restricted condition of a FSFGIC
task for making the distance between the samples within the same class smaller
and the distance between samples from different classes larger and for reducing
the coupling relationship between samples from different categories.
Furthermore, a novel similarity measure based on the proposed non-linear data
project is presented for evaluating the relationships of feature information
between a query image and a support set. It is worth to note that our proposed
architecture can be easily embedded into any episodic training mechanisms for
end-to-end training from scratch. Extensive experiments on FSFGIC tasks
demonstrate the superiority of the proposed methods over the state-of-the-art
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weichuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xuefang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1"&gt;Zhe Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yongsheng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changming Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Just Noticeable Difference for Machine Perception and Generation of Regularized Adversarial Images with Minimal Perturbation. (arXiv:2102.08079v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08079</id>
        <link href="http://arxiv.org/abs/2102.08079"/>
        <updated>2021-07-12T01:55:15.661Z</updated>
        <summary type="html"><![CDATA[In this study, we introduce a measure for machine perception, inspired by the
concept of Just Noticeable Difference (JND) of human perception. Based on this
measure, we suggest an adversarial image generation algorithm, which
iteratively distorts an image by an additive noise until the machine learning
model detects the change in the image by outputting a false label. The amount
of noise added to the original image is defined as the gradient of the cost
function of the machine learning model. This cost function explicitly minimizes
the amount of perturbation applied on the input image and it is regularized by
bounded range and total variation functions to assure perceptual similarity of
the adversarial image to the input. We evaluate the adversarial images
generated by our algorithm both qualitatively and quantitatively on CIFAR10,
ImageNet, and MS COCO datasets. Our experiments on image classification and
object detection tasks show that adversarial images generated by our method are
both more successful in deceiving the recognition/detection model and less
perturbed compared to the images generated by the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akan_A/0/1/0/all/0/1"&gt;Adil Kaan Akan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akbas_E/0/1/0/all/0/1"&gt;Emre Akbas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vural_F/0/1/0/all/0/1"&gt;Fatos T. Yarman Vural&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Gaussian Processes with Derivative Information Using Variational Inference. (arXiv:2107.04061v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04061</id>
        <link href="http://arxiv.org/abs/2107.04061"/>
        <updated>2021-07-12T01:55:15.652Z</updated>
        <summary type="html"><![CDATA[Gaussian processes with derivative information are useful in many settings
where derivative information is available, including numerous Bayesian
optimization and regression tasks that arise in the natural sciences.
Incorporating derivative observations, however, comes with a dominating
$O(N^3D^3)$ computational cost when training on $N$ points in $D$ input
dimensions. This is intractable for even moderately sized problems. While
recent work has addressed this intractability in the low-$D$ setting, the
high-$N$, high-$D$ setting is still unexplored and of great value, particularly
as machine learning problems increasingly become high dimensional. In this
paper, we introduce methods to achieve fully scalable Gaussian process
regression with derivatives using variational inference. Analogous to the use
of inducing values to sparsify the labels of a training set, we introduce the
concept of inducing directional derivatives to sparsify the partial derivative
information of a training set. This enables us to construct a variational
posterior that incorporates derivative information but whose size depends
neither on the full dataset size $N$ nor the full dimensionality $D$. We
demonstrate the full scalability of our approach on a variety of tasks, ranging
from a high dimensional stellarator fusion regression task to training graph
convolutional neural networks on Pubmed using Bayesian optimization.
Surprisingly, we find that our approach can improve regression performance even
in settings where only label data is available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Padidar_M/0/1/0/all/0/1"&gt;Misha Padidar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xinran Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Leo Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1"&gt;Jacob R. Gardner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bindel_D/0/1/0/all/0/1"&gt;David Bindel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Model-Based Multi-Agent Mean-Field Reinforcement Learning. (arXiv:2107.04050v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.04050</id>
        <link href="http://arxiv.org/abs/2107.04050"/>
        <updated>2021-07-12T01:55:15.644Z</updated>
        <summary type="html"><![CDATA[Learning in multi-agent systems is highly challenging due to the inherent
complexity introduced by agents' interactions. We tackle systems with a huge
population of interacting agents (e.g., swarms) via Mean-Field Control (MFC).
MFC considers an asymptotically infinite population of identical agents that
aim to collaboratively maximize the collective reward. Specifically, we
consider the case of unknown system dynamics where the goal is to
simultaneously optimize for the rewards and learn from experience. We propose
an efficient model-based reinforcement learning algorithm
$\text{M}^3\text{-UCRL}$ that runs in episodes and provably solves this
problem. $\text{M}^3\text{-UCRL}$ uses upper-confidence bounds to balance
exploration and exploitation during policy learning. Our main theoretical
contributions are the first general regret bounds for model-based RL for MFC,
obtained via a novel mean-field type analysis. $\text{M}^3\text{-UCRL}$ can be
instantiated with different models such as neural networks or Gaussian
Processes, and effectively combined with neural network policy learning. We
empirically demonstrate the convergence of $\text{M}^3\text{-UCRL}$ on the
swarm motion problem of controlling an infinite population of agents seeking to
maximize location-dependent reward and avoid congested areas.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Pasztor_B/0/1/0/all/0/1"&gt;Barna Pasztor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bogunovic_I/0/1/0/all/0/1"&gt;Ilija Bogunovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Krause_A/0/1/0/all/0/1"&gt;Andreas Krause&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Psychophysical Oriented Saliency Map Prediction Model. (arXiv:2011.04076v10 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04076</id>
        <link href="http://arxiv.org/abs/2011.04076"/>
        <updated>2021-07-12T01:55:15.637Z</updated>
        <summary type="html"><![CDATA[Visual attention is one of the most significant characteristics for selecting
and understanding the visual redundancy of the external world. Complex scenes
include enormous redundancy. The human vision system cannot process all
information simultaneously, due to the visual information bottleneck. The human
visual system mainly focuses on dominant parts of scenes, in order to reduce
the redundant input of visual information. This is commonly known as visual
attention prediction or visual saliency map prediction. This paper proposes a
new psychophysical saliency prediction architecture, WECSF, inspired by
multi-channel model of visual cortex functioning in humans. The model consists
of opponent color channels, a wavelet transform and wavelet energy map, and a
contrast sensitivity function for extracting low-level image features and
providing maximum approximation to the human visual system. In this paper, the
proposed model is evaluated using several data sets, including the MIT1003,
MIT300, TORONTO, SID4VAM, and UCF Sports data sets, in order to demonstrate its
efficiency. We also quantitatively and qualitatively compare the saliency
prediction performance with that of other state-of-the-art models. Our model
achieved stable and very good performance. Additionally, Fourier and
spectral-inspired saliency prediction models outperformed other
state-of-the-art non-neural networks (and even deep neural network) models on
psychophysical synthetic images. Finally, the proposed model can also be
applied to spatial-temporal saliency prediction and achieved superior
performance in the evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qiang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ViTGAN: Training GANs with Vision Transformers. (arXiv:2107.04589v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04589</id>
        <link href="http://arxiv.org/abs/2107.04589"/>
        <updated>2021-07-12T01:55:15.629Z</updated>
        <summary type="html"><![CDATA[Recently, Vision Transformers (ViTs) have shown competitive performance on
image recognition while requiring less vision-specific inductive biases. In
this paper, we investigate if such observation can be extended to image
generation. To this end, we integrate the ViT architecture into generative
adversarial networks (GANs). We observe that existing regularization methods
for GANs interact poorly with self-attention, causing serious instability
during training. To resolve this issue, we introduce novel regularization
techniques for training GANs with ViTs. Empirically, our approach, named
ViTGAN, achieves comparable performance to state-of-the-art CNN-based StyleGAN2
on CIFAR-10, CelebA, and LSUN bedroom datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kwonjoon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Huiwen Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Lu Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Han Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1"&gt;Zhuowen Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Ce Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Cascaded Detection Tasks with Weakly-Supervised Domain Adaptation. (arXiv:2107.04523v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04523</id>
        <link href="http://arxiv.org/abs/2107.04523"/>
        <updated>2021-07-12T01:55:15.609Z</updated>
        <summary type="html"><![CDATA[In order to handle the challenges of autonomous driving, deep learning has
proven to be crucial in tackling increasingly complex tasks, such as 3D
detection or instance segmentation. State-of-the-art approaches for image-based
detection tasks tackle this complexity by operating in a cascaded fashion: they
first extract a 2D bounding box based on which additional attributes, e.g.
instance masks, are inferred. While these methods perform well, a key challenge
remains the lack of accurate and cheap annotations for the growing variety of
tasks. Synthetic data presents a promising solution but, despite the effort in
domain adaptation research, the gap between synthetic and real data remains an
open problem. In this work, we propose a weakly supervised domain adaptation
setting which exploits the structure of cascaded detection tasks. In
particular, we learn to infer the attributes solely from the source domain
while leveraging 2D bounding boxes as weak labels in both domains to explain
the domain shift. We further encourage domain-invariant features through
class-wise feature alignment using ground-truth class information, which is not
available in the unsupervised setting. As our experiments demonstrate, the
approach is competitive with fully supervised settings while outperforming
unsupervised adaptation approaches by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hanselmann_N/0/1/0/all/0/1"&gt;Niklas Hanselmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1"&gt;Nick Schneider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ortelt_B/0/1/0/all/0/1"&gt;Benedikt Ortelt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1"&gt;Andreas Geiger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensembles of Randomized NNs for Pattern-based Time Series Forecasting. (arXiv:2107.04091v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04091</id>
        <link href="http://arxiv.org/abs/2107.04091"/>
        <updated>2021-07-12T01:55:15.600Z</updated>
        <summary type="html"><![CDATA[In this work, we propose an ensemble forecasting approach based on randomized
neural networks. Improved randomized learning streamlines the fitting abilities
of individual learners by generating network parameters in accordance with the
data and target function features. A pattern-based representation of time
series makes the proposed approach suitable for forecasting time series with
multiple seasonality. We propose six strategies for controlling the diversity
of ensemble members. Case studies conducted on four real-world forecasting
problems verified the effectiveness and superior performance of the proposed
ensemble forecasting approach. It outperformed statistical models as well as
state-of-the-art machine learning models in terms of forecasting accuracy. The
proposed approach has several advantages: fast and easy training, simple
architecture, ease of implementation, high accuracy and the ability to deal
with nonstationarity and multiple seasonality in time series.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1"&gt;Grzegorz Dudek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pelka_P/0/1/0/all/0/1"&gt;Pawe&amp;#x142; Pe&amp;#x142;ka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MCMC Variational Inference via Uncorrected Hamiltonian Annealing. (arXiv:2107.04150v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04150</id>
        <link href="http://arxiv.org/abs/2107.04150"/>
        <updated>2021-07-12T01:55:15.591Z</updated>
        <summary type="html"><![CDATA[Given an unnormalized target distribution we want to obtain approximate
samples from it and a tight lower bound on its (log) normalization constant log
Z. Annealed Importance Sampling (AIS) with Hamiltonian MCMC is a powerful
method that can be used to do this. Its main drawback is that it uses
non-differentiable transition kernels, which makes tuning its many parameters
hard. We propose a framework to use an AIS-like procedure with Uncorrected
Hamiltonian MCMC, called Uncorrected Hamiltonian Annealing. Our method leads to
tight and differentiable lower bounds on log Z. We show empirically that our
method yields better performances than other competing approaches, and that the
ability to tune its parameters using reparameterization gradients may lead to
large performance improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Geffner_T/0/1/0/all/0/1"&gt;Tomas Geffner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Domke_J/0/1/0/all/0/1"&gt;Justin Domke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparison of 2D vs. 3D U-Net Organ Segmentation in abdominal 3D CT images. (arXiv:2107.04062v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04062</id>
        <link href="http://arxiv.org/abs/2107.04062"/>
        <updated>2021-07-12T01:55:15.578Z</updated>
        <summary type="html"><![CDATA[A two-step concept for 3D segmentation on 5 abdominal organs inside
volumetric CT images is presented. First each relevant organ's volume of
interest is extracted as bounding box. The extracted volume acts as input for a
second stage, wherein two compared U-Nets with different architectural
dimensions re-construct an organ segmentation as label mask. In this work, we
focus on comparing 2D U-Nets vs. 3D U-Net counterparts. Our initial results
indicate Dice improvements of about 6\% at maximum. In this study to our
surprise, liver and kidneys for instance were tackled significantly better
using the faster and GPU-memory saving 2D U-Nets. For other abdominal key
organs, there were no significant differences, but we observe highly
significant advantages for the 2D U-Net in terms of GPU computational efforts
for all organs under study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zettler_N/0/1/0/all/0/1"&gt;Nico Zettler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mastmeyer_A/0/1/0/all/0/1"&gt;Andre Mastmeyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Challenges of Open World Recognitionunder Shifting Visual Domains. (arXiv:2107.04461v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04461</id>
        <link href="http://arxiv.org/abs/2107.04461"/>
        <updated>2021-07-12T01:55:15.571Z</updated>
        <summary type="html"><![CDATA[Robotic visual systems operating in the wild must act in unconstrained
scenarios, under different environmental conditions while facing a variety of
semantic concepts, including unknown ones. To this end, recent works tried to
empower visual object recognition methods with the capability to i) detect
unseen concepts and ii) extended their knowledge over time, as images of new
semantic classes arrive. This setting, called Open World Recognition (OWR), has
the goal to produce systems capable of breaking the semantic limits present in
the initial training set. However, this training set imposes to the system not
only its own semantic limits, but also environmental ones, due to its bias
toward certain acquisition conditions that do not necessarily reflect the high
variability of the real-world. This discrepancy between training and test
distribution is called domain-shift. This work investigates whether OWR
algorithms are effective under domain-shift, presenting the first benchmark
setup for assessing fairly the performances of OWR algorithms, with and without
domain-shift. We then use this benchmark to conduct analyses in various
scenarios, showing how existing OWR algorithms indeed suffer a severe
performance degradation when train and test distributions differ. Our analysis
shows that this degradation is only slightly mitigated by coupling OWR with
domain generalization techniques, indicating that the mere plug-and-play of
existing algorithms is not enough to recognize new and unknown categories in
unseen domains. Our results clearly point toward open issues and future
research directions, that need to be investigated for building robot visual
systems able to function reliably under these challenging yet very real
conditions. Code available at
https://github.com/DarioFontanel/OWR-VisualDomains]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fontanel_D/0/1/0/all/0/1"&gt;Dario Fontanel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cermelli_F/0/1/0/all/0/1"&gt;Fabio Cermelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mancini_M/0/1/0/all/0/1"&gt;Massimiliano Mancini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1"&gt;Barbara Caputo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does Form Follow Function? An Empirical Exploration of the Impact of Deep Neural Network Architecture Design on Hardware-Specific Acceleration. (arXiv:2107.04144v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04144</id>
        <link href="http://arxiv.org/abs/2107.04144"/>
        <updated>2021-07-12T01:55:15.547Z</updated>
        <summary type="html"><![CDATA[The fine-grained relationship between form and function with respect to deep
neural network architecture design and hardware-specific acceleration is one
area that is not well studied in the research literature, with form often
dictated by accuracy as opposed to hardware function. In this study, a
comprehensive empirical exploration is conducted to investigate the impact of
deep neural network architecture design on the degree of inference speedup that
can be achieved via hardware-specific acceleration. More specifically, we
empirically study the impact of a variety of commonly used macro-architecture
design patterns across different architectural depths through the lens of
OpenVINO microprocessor-specific and GPU-specific acceleration. Experimental
results showed that while leveraging hardware-specific acceleration achieved an
average inference speed-up of 380%, the degree of inference speed-up varied
drastically depending on the macro-architecture design pattern, with the
greatest speedup achieved on the depthwise bottleneck convolution design
pattern at 550%. Furthermore, we conduct an in-depth exploration of the
correlation between FLOPs requirement, level 3 cache efficacy, and network
latency with increasing architectural depth and width. Finally, we analyze the
inference time reductions using hardware-specific acceleration when compared to
native deep learning frameworks across a wide variety of hand-crafted deep
convolutional neural network architecture designs as well as ones found via
neural architecture search strategies. We found that the DARTS-derived
architecture to benefit from the greatest improvement from hardware-specific
software acceleration (1200%) while the depthwise bottleneck convolution-based
MobileNet-V2 to have the lowest overall inference time of around 2.4 ms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abbasi_S/0/1/0/all/0/1"&gt;Saad Abbasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafiee_M/0/1/0/all/0/1"&gt;Mohammad Javad Shafiee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_E/0/1/0/all/0/1"&gt;Ellick Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alexander Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[White-Box Cartoonization Using An Extended GAN Framework. (arXiv:2107.04551v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04551</id>
        <link href="http://arxiv.org/abs/2107.04551"/>
        <updated>2021-07-12T01:55:15.539Z</updated>
        <summary type="html"><![CDATA[In the present study, we propose to implement a new framework for estimating
generative models via an adversarial process to extend an existing GAN
framework and develop a white-box controllable image cartoonization, which can
generate high-quality cartooned images/videos from real-world photos and
videos. The learning purposes of our system are based on three distinct
representations: surface representation, structure representation, and texture
representation. The surface representation refers to the smooth surface of the
images. The structure representation relates to the sparse colour blocks and
compresses generic content. The texture representation shows the texture,
curves, and features in cartoon images. Generative Adversarial Network (GAN)
framework decomposes the images into different representations and learns from
them to generate cartoon images. This decomposition makes the framework more
controllable and flexible which allows users to make changes based on the
required output. This approach overcomes any previous system in terms of
maintaining clarity, colours, textures, shapes of images yet showing the
characteristics of cartoon images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1"&gt;Amey Thakur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rizvi_H/0/1/0/all/0/1"&gt;Hasan Rizvi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Satish_M/0/1/0/all/0/1"&gt;Mega Satish&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D RegNet: Deep Learning Model for COVID-19 Diagnosis on Chest CT Image. (arXiv:2107.04055v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04055</id>
        <link href="http://arxiv.org/abs/2107.04055"/>
        <updated>2021-07-12T01:55:15.532Z</updated>
        <summary type="html"><![CDATA[In this paper, a 3D-RegNet-based neural network is proposed for diagnosing
the physical condition of patients with coronavirus (Covid-19) infection. In
the application of clinical medicine, lung CT images are utilized by
practitioners to determine whether a patient is infected with coronavirus.
However, there are some laybacks can be considered regarding to this diagnostic
method, such as time consuming and low accuracy. As a relatively large organ of
human body, important spatial features would be lost if the lungs were
diagnosed utilizing two dimensional slice image. Therefore, in this paper, a
deep learning model with 3D image was designed. The 3D image as input data was
comprised of two-dimensional pulmonary image sequence and from which relevant
coronavirus infection 3D features were extracted and classified. The results
show that the test set of the 3D model, the result: f1 score of 0.8379 and AUC
value of 0.8807 have been achieved.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Qi_H/0/1/0/all/0/1"&gt;Haibo Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuhan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xinyu Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object-Centric Representation Learning for Video Question Answering. (arXiv:2104.05166v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05166</id>
        <link href="http://arxiv.org/abs/2104.05166"/>
        <updated>2021-07-12T01:55:15.524Z</updated>
        <summary type="html"><![CDATA[Video question answering (Video QA) presents a powerful testbed for
human-like intelligent behaviors. The task demands new capabilities to
integrate video processing, language understanding, binding abstract linguistic
concepts to concrete visual artifacts, and deliberative reasoning over
spacetime. Neural networks offer a promising approach to reach this potential
through learning from examples rather than handcrafting features and rules.
However, neural networks are predominantly feature-based - they map data to
unstructured vectorial representation and thus can fall into the trap of
exploiting shortcuts through surface statistics instead of true systematic
reasoning seen in symbolic systems. To tackle this issue, we advocate for
object-centric representation as a basis for constructing spatio-temporal
structures from videos, essentially bridging the semantic gap between low-level
pattern recognition and high-level symbolic algebra. To this end, we propose a
new query-guided representation framework to turn a video into an evolving
relational graph of objects, whose features and interactions are dynamically
and conditionally inferred. The object lives are then summarized into resumes,
lending naturally for deliberative relational reasoning that produces an answer
to the query. The framework is evaluated on major Video QA datasets,
demonstrating clear benefits of the object-centric approach to video reasoning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dang_L/0/1/0/all/0/1"&gt;Long Hoang Dang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1"&gt;Thao Minh Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1"&gt;Vuong Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1"&gt;Truyen Tran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Camera Alignment Network for Unsupervised Cross-camera Person Re-identification. (arXiv:1908.00862v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.00862</id>
        <link href="http://arxiv.org/abs/1908.00862"/>
        <updated>2021-07-12T01:55:15.517Z</updated>
        <summary type="html"><![CDATA[In person re-identification (Re-ID), supervised methods usually need a large
amount of expensive label information, while unsupervised ones are still unable
to deliver satisfactory identification performance. In this paper, we introduce
a novel person Re-ID task called unsupervised cross-camera person Re-ID, which
only needs the within-camera (intra-camera) label information but not
cross-camera (inter-camera) labels which are more expensive to obtain. In
real-world applications, the intra-camera label information can be easily
captured by tracking algorithms or few manual annotations. In this situation,
the main challenge becomes the distribution discrepancy across different camera
views, caused by the various body pose, occlusion, image resolution,
illumination conditions, and background noises in different cameras. To address
this situation, we propose a novel Adversarial Camera Alignment Network (ACAN)
for unsupervised cross-camera person Re-ID. It consists of the camera-alignment
task and the supervised within-camera learning task. To achieve the camera
alignment, we develop a Multi-Camera Adversarial Learning (MCAL) to map images
of different cameras into a shared subspace. Particularly, we investigate two
different schemes, including the existing GRL (i.e., gradient reversal layer)
scheme and the proposed scheme called "other camera equiprobability" (OCE), to
conduct the multi-camera adversarial task. Based on this shared subspace, we
then leverage the within-camera labels to train the network. Extensive
experiments on five large-scale datasets demonstrate the superiority of ACAN
over multiple state-of-the-art unsupervised methods that take advantage of
labeled source domains and generated images by GAN-based models. In particular,
we verify that the proposed multi-camera adversarial task does contribute to
the significant improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1"&gt;Lei Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huo_J/0/1/0/all/0/1"&gt;Jing Huo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yinghuan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1"&gt;Xin Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yang Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Detect Adversarial Examples Based on Class Scores. (arXiv:2107.04435v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04435</id>
        <link href="http://arxiv.org/abs/2107.04435"/>
        <updated>2021-07-12T01:55:15.509Z</updated>
        <summary type="html"><![CDATA[Given the increasing threat of adversarial attacks on deep neural networks
(DNNs), research on efficient detection methods is more important than ever. In
this work, we take a closer look at adversarial attack detection based on the
class scores of an already trained classification model. We propose to train a
support vector machine (SVM) on the class scores to detect adversarial
examples. Our method is able to detect adversarial examples generated by
various attacks, and can be easily adopted to a plethora of deep classification
models. We show that our approach yields an improved detection rate compared to
an existing method, whilst being easy to implement. We perform an extensive
empirical analysis on different deep classification models, investigating
various state-of-the-art adversarial attacks. Moreover, we observe that our
proposed method is better at detecting a combination of adversarial attacks.
This work indicates the potential of detecting various adversarial attacks
simply by using the class scores of an already trained classification model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Uelwer_T/0/1/0/all/0/1"&gt;Tobias Uelwer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michels_F/0/1/0/all/0/1"&gt;Felix Michels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Candido_O/0/1/0/all/0/1"&gt;Oliver De Candido&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Audio-visual Attentive Fusion for Continuous Emotion Recognition. (arXiv:2107.01175v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01175</id>
        <link href="http://arxiv.org/abs/2107.01175"/>
        <updated>2021-07-12T01:55:15.486Z</updated>
        <summary type="html"><![CDATA[We propose an audio-visual spatial-temporal deep neural network with: (1) a
visual block containing a pretrained 2D-CNN followed by a temporal
convolutional network (TCN); (2) an aural block containing several parallel
TCNs; and (3) a leader-follower attentive fusion block combining the
audio-visual information. The TCN with large history coverage enables our model
to exploit spatial-temporal information within a much larger window length
(i.e., 300) than that from the baseline and state-of-the-art methods (i.e., 36
or 48). The fusion block emphasizes the visual modality while exploits the
noisy aural modality using the inter-modality attention mechanism. To make full
use of the data and alleviate over-fitting, cross-validation is carried out on
the training and validation set. The concordance correlation coefficient (CCC)
centering is used to merge the results from each fold. On the development set,
the achieved CCC is 0.469 for valence and 0.649 for arousal, which
significantly outperforms the baseline method with the corresponding CCC of
0.210 and 0.230 for valence and arousal, respectively. The code is available at
https://github.com/sucv/ABAW2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Su Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yi Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1"&gt;Ziquan Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1"&gt;Cuntai Guan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Many Objective Bayesian Optimization. (arXiv:2107.04126v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.04126</id>
        <link href="http://arxiv.org/abs/2107.04126"/>
        <updated>2021-07-12T01:55:15.479Z</updated>
        <summary type="html"><![CDATA[Some real problems require the evaluation of expensive and noisy objective
functions. Moreover, the analytical expression of these objective functions may
be unknown. These functions are known as black-boxes, for example, estimating
the generalization error of a machine learning algorithm and computing its
prediction time in terms of its hyper-parameters. Multi-objective Bayesian
optimization (MOBO) is a set of methods that has been successfully applied for
the simultaneous optimization of black-boxes. Concretely, BO methods rely on a
probabilistic model of the objective functions, typically a Gaussian process.
This model generates a predictive distribution of the objectives. However, MOBO
methods have problems when the number of objectives in a multi-objective
optimization problem are 3 or more, which is the many objective setting. In
particular, the BO process is more costly as more objectives are considered,
computing the quality of the solution via the hyper-volume is also more costly
and, most importantly, we have to evaluate every objective function, wasting
expensive computational, economic or other resources. However, as more
objectives are involved in the optimization problem, it is highly probable that
some of them are redundant and not add information about the problem solution.
A measure that represents how similar are GP predictive distributions is
proposed. We also propose a many objective Bayesian optimization algorithm that
uses this metric to determine whether two objectives are redundant. The
algorithm stops evaluating one of them if the similarity is found, saving
resources and not hurting the performance of the multi-objective BO algorithm.
We show empirical evidence in a set of toy, synthetic, benchmark and real
experiments that GPs predictive distributions of the effectiveness of the
metric and the algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Martin_L/0/1/0/all/0/1"&gt;Lucia Asencio Mart&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Garrido_Merchan_E/0/1/0/all/0/1"&gt;Eduardo C. Garrido-Merch&amp;#xe1;n&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information-Theoretic Registration with Explicit Reorientation of Diffusion-Weighted Images. (arXiv:1905.12056v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.12056</id>
        <link href="http://arxiv.org/abs/1905.12056"/>
        <updated>2021-07-12T01:55:15.471Z</updated>
        <summary type="html"><![CDATA[We present an information-theoretic approach to the registration of images
with directional information, and especially for diffusion-Weighted Images
(DWI), with explicit optimization over the directional scale. We call it
Locally Orderless Registration with Directions (LORD). We focus on normalized
mutual information as a robust information-theoretic similarity measure for
DWI. The framework is an extension of the LOR-DWI density-based hierarchical
scale-space model that varies and optimizes the integration, spatial,
directional, and intensity scales. As affine transformations are insufficient
for inter-subject registration, we extend the model to non-rigid deformations.
We illustrate that the proposed model deforms orientation distribution
functions (ODFs) correctly and is capable of handling the classic complex
challenges in DWI-registrations, such as the registration of fiber-crossings
along with kissing, fanning, and interleaving fibers. Our experimental results
clearly illustrate a novel promising regularizing effect, that comes from the
nonlinear orientation-based cost function. We show the properties of the
different image scales and, we show that including orientational information in
our model makes the model better at retrieving deformations in contrast to
standard scalar-based registration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jensen_H/0/1/0/all/0/1"&gt;Henrik Gr&amp;#xf8;nholt Jensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lauze_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Lauze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darkner_S/0/1/0/all/0/1"&gt;Sune Darkner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ResT: An Efficient Transformer for Visual Recognition. (arXiv:2105.13677v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13677</id>
        <link href="http://arxiv.org/abs/2105.13677"/>
        <updated>2021-07-12T01:55:15.452Z</updated>
        <summary type="html"><![CDATA[This paper presents an efficient multi-scale vision Transformer, called ResT,
that capably served as a general-purpose backbone for image recognition. Unlike
existing Transformer methods, which employ standard Transformer blocks to
tackle raw images with a fixed resolution, our ResT have several advantages:
(1) A memory-efficient multi-head self-attention is built, which compresses the
memory by a simple depth-wise convolution, and projects the interaction across
the attention-heads dimension while keeping the diversity ability of
multi-heads; (2) Position encoding is constructed as spatial attention, which
is more flexible and can tackle with input images of arbitrary size without
interpolation or fine-tune; (3) Instead of the straightforward tokenization at
the beginning of each stage, we design the patch embedding as a stack of
overlapping convolution operation with stride on the 2D-reshaped token map. We
comprehensively validate ResT on image classification and downstream tasks.
Experimental results show that the proposed ResT can outperform the recently
state-of-the-art backbones by a large margin, demonstrating the potential of
ResT as strong backbones. The code and models will be made publicly available
at https://github.com/wofmanaf/ResT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qinglong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yubin Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prior-Guided Multi-View 3D Head Reconstruction. (arXiv:2107.04277v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04277</id>
        <link href="http://arxiv.org/abs/2107.04277"/>
        <updated>2021-07-12T01:55:15.445Z</updated>
        <summary type="html"><![CDATA[Recovering a 3D head model including the complete face and hair regions is
still a challenging problem in computer vision and graphics. In this paper, we
consider this problem with a few multi-view portrait images as input. Previous
multi-view stereo methods, either based on the optimization strategies or deep
learning techniques, suffer from low-frequency geometric structures such as
unclear head structures and inaccurate reconstruction in hair regions. To
tackle this problem, we propose a prior-guided implicit neural rendering
network. Specifically, we model the head geometry with a learnable signed
distance field (SDF) and optimize it via an implicit differentiable renderer
with the guidance of some human head priors, including the facial prior
knowledge, head semantic segmentation information and 2D hair orientation maps.
The utilization of these priors can improve the reconstruction accuracy and
robustness, leading to a high-quality integrated 3D head model. Extensive
ablation studies and comparisons with state-of-the-art methods demonstrate that
our method could produce high-fidelity 3D head geometries with the guidance of
these priors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xueying Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yudong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhongqi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Juyong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Even Faster SNN Simulation with Lazy+Event-driven Plasticity and Shared Atomics. (arXiv:2107.04092v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2107.04092</id>
        <link href="http://arxiv.org/abs/2107.04092"/>
        <updated>2021-07-12T01:55:15.437Z</updated>
        <summary type="html"><![CDATA[We present two novel optimizations that accelerate clock-based spiking neural
network (SNN) simulators. The first one targets spike timing dependent
plasticity (STDP). It combines lazy- with event-driven plasticity and
efficiently facilitates the computation of pre- and post-synaptic spikes using
bitfields and integer intrinsics. It offers higher bandwidth than event-driven
plasticity alone and achieves a 1.5x-2x speedup over our closest competitor.
The second optimization targets spike delivery. We partition our graph
representation in a way that bounds the number of neurons that need be updated
at any given time which allows us to perform said update in shared memory
instead of global memory. This is 2x-2.5x faster than our closest competitor.
Both optimizations represent the final evolutionary stages of years of
iteration on STDP and spike delivery inside "Spice" (/spaIk/), our state of the
art SNN simulator. The proposed optimizations are not exclusive to our graph
representation or pipeline but are applicable to a multitude of simulator
designs. We evaluate our performance on three well-established models and
compare ourselves against three other state of the art simulators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bautembach_D/0/1/0/all/0/1"&gt;Dennis Bautembach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oikonomidis_I/0/1/0/all/0/1"&gt;Iason Oikonomidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Argyros_A/0/1/0/all/0/1"&gt;Antonis Argyros&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual-Awareness Attention for Few-Shot Object Detection. (arXiv:2102.12152v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12152</id>
        <link href="http://arxiv.org/abs/2102.12152"/>
        <updated>2021-07-12T01:55:15.430Z</updated>
        <summary type="html"><![CDATA[While recent progress has significantly boosted few-shot classification (FSC)
performance, few-shot object detection (FSOD) remains challenging for modern
learning systems. Existing FSOD systems follow FSC approaches, ignoring the
issues of spatial misalignment and vagueness in class representations, and
consequently result in low performance. Observing this, we propose a novel
Dual-Awareness Attention (DAnA) mechanism that can adaptively generate
query-position-aware (QPA) support features and guide the detection networks
precisely. The generated QPA features represent local information of a support
image conditioned on a given region of the query. By taking the spatial
relationships across different images into consideration, our approach
conspicuously outperforms previous FSOD methods (+6.9 AP relatively) and
achieves remarkable results even under a challenging cross-dataset evaluation
setting. Furthermore, the proposed DAnA component is flexible and adaptable to
multiple existing object detection frameworks. By equipping DAnA, conventional
object detection models, Faster R-CNN and RetinaNet, which are not designed
explicitly for few-shot learning, reach state-of-the-art performance in FSOD
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tung-I Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yueh-Cheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hung-Ting Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1"&gt;Yu-Cheng Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yu-Hsiang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeh_J/0/1/0/all/0/1"&gt;Jia-Fong Yeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Winston H. Hsu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fedlearn-Algo: A flexible open-source privacy-preserving machine learning platform. (arXiv:2107.04129v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04129</id>
        <link href="http://arxiv.org/abs/2107.04129"/>
        <updated>2021-07-12T01:55:15.423Z</updated>
        <summary type="html"><![CDATA[In this paper, we present Fedlearn-Algo, an open-source privacy preserving
machine learning platform. We use this platform to demonstrate our research and
development results on privacy preserving machine learning algorithms. As the
first batch of novel FL algorithm examples, we release vertical federated
kernel binary classification model and vertical federated random forest model.
They have been tested to be more efficient than existing vertical federated
learning models in our practice. Besides the novel FL algorithm examples, we
also release a machine communication module. The uniform data transfer
interface supports transfering widely used data formats between machines. We
will maintain this platform by adding more functional modules and algorithm
examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1"&gt;Chaowei Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiazhou Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_T/0/1/0/all/0/1"&gt;Tao Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1"&gt;Huasong Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1"&gt;Houpu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heng_H/0/1/0/all/0/1"&gt;Huang Heng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1"&gt;Peng Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1"&gt;Liefeng Bo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yanqing Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CMRNet: Camera to LiDAR-Map Registration. (arXiv:1906.10109v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.10109</id>
        <link href="http://arxiv.org/abs/1906.10109"/>
        <updated>2021-07-12T01:55:15.415Z</updated>
        <summary type="html"><![CDATA[In this paper we present CMRNet, a realtime approach based on a Convolutional
Neural Network to localize an RGB image of a scene in a map built from LiDAR
data. Our network is not trained in the working area, i.e. CMRNet does not
learn the map. Instead it learns to match an image to the map. We validate our
approach on the KITTI dataset, processing each frame independently without any
tracking procedure. CMRNet achieves 0.27m and 1.07deg median localization
accuracy on the sequence 00 of the odometry dataset, starting from a rough pose
estimate displaced up to 3.5m and 17deg. To the best of our knowledge this is
the first CNN-based approach that learns to match images from a monocular
camera to a given, preexisting 3D LiDAR-map.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cattaneo_D/0/1/0/all/0/1"&gt;Daniele Cattaneo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vaghi_M/0/1/0/all/0/1"&gt;Matteo Vaghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ballardini_A/0/1/0/all/0/1"&gt;Augusto Luis Ballardini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fontana_S/0/1/0/all/0/1"&gt;Simone Fontana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sorrenti_D/0/1/0/all/0/1"&gt;Domenico Giorgio Sorrenti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1"&gt;Wolfram Burgard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Negative Transfer. (arXiv:2009.00909v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.00909</id>
        <link href="http://arxiv.org/abs/2009.00909"/>
        <updated>2021-07-12T01:55:15.408Z</updated>
        <summary type="html"><![CDATA[Transfer learning (TL) tries to utilize data or knowledge from one or more
source domains to facilitate the learning in a target domain. It is
particularly useful when the target domain has few or no labeled data, due to
annotation expense, privacy concerns, etc. Unfortunately, the effectiveness of
TL is not always guaranteed. Negative transfer (NT), i.e., the source domain
data/knowledge cause reduced learning performance in the target domain, has
been a long-standing and challenging problem in TL. Various approaches to
handle NT have been proposed in the literature. However, this filed lacks a
systematic survey on the formalization of NT, their factors and the algorithms
that handle NT. This paper proposes to fill this gap. First, the definition of
negative transfer is considered and a taxonomy of the factors are discussed.
Then, near fifty representative approaches for handling NT are categorized and
reviewed, from four perspectives: secure transfer, domain similarity
estimation, distant transfer and negative transfer mitigation. NT in related
fields, e.g., multi-task learning, lifelong learning, and adversarial attacks
are also discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1"&gt;Lingfei Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dongrui Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[iGibson, a Simulation Environment for Interactive Tasks in Large Realistic Scenes. (arXiv:2012.02924v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02924</id>
        <link href="http://arxiv.org/abs/2012.02924"/>
        <updated>2021-07-12T01:55:15.395Z</updated>
        <summary type="html"><![CDATA[We present iGibson, a novel simulation environment to develop robotic
solutions for interactive tasks in large-scale realistic scenes. Our
environment contains 15 fully interactive home-sized scenes with 108 rooms
populated with rigid and articulated objects. The scenes are replicas of
real-world homes, with distribution and the layout of objects aligned to those
of the real world. iGibson integrates several key features to facilitate the
study of interactive tasks: i) generation of high-quality virtual sensor
signals (RGB, depth, segmentation, LiDAR, flow and so on), ii) domain
randomization to change the materials of the objects (both visual and physical)
and/or their shapes, iii) integrated sampling-based motion planners to generate
collision-free trajectories for robot bases and arms, and iv) intuitive
human-iGibson interface that enables efficient collection of human
demonstrations. Through experiments, we show that the full interactivity of the
scenes enables agents to learn useful visual representations that accelerate
the training of downstream manipulation tasks. We also show that iGibson
features enable the generalization of navigation agents, and that the
human-iGibson interface and integrated motion planners facilitate efficient
imitation learning of human demonstrated (mobile) manipulation behaviors.
iGibson is open-source, equipped with comprehensive examples and documentation.
For more information, visit our project website:
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1"&gt;Bokui Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1"&gt;Fei Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chengshu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1"&gt;Roberto Mart&amp;#xed;n-Mart&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1"&gt;Linxi Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guanzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_DArpino_C/0/1/0/all/0/1"&gt;Claudia P&amp;#xe9;rez-D&amp;#x27;Arpino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buch_S/0/1/0/all/0/1"&gt;Shyamal Buch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1"&gt;Sanjana Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tchapmi_L/0/1/0/all/0/1"&gt;Lyne P. Tchapmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tchapmi_M/0/1/0/all/0/1"&gt;Micael E. Tchapmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1"&gt;Kent Vainio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1"&gt;Silvio Savarese&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning for Stuttering Identification: Review, Challenges & Future Directions. (arXiv:2107.04057v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.04057</id>
        <link href="http://arxiv.org/abs/2107.04057"/>
        <updated>2021-07-12T01:55:15.363Z</updated>
        <summary type="html"><![CDATA[Stuttering is a speech disorder during which the flow of speech is
interrupted by involuntary pauses and repetition of sounds. Stuttering
identification is an interesting interdisciplinary domain research problem
which involves pathology, psychology, acoustics, and signal processing that
makes it hard and complicated to detect. Recent developments in machine and
deep learning have dramatically revolutionized speech domain, however minimal
attention has been given to stuttering identification. This work fills the gap
by trying to bring researchers together from interdisciplinary fields. In this
paper, we review comprehensively acoustic features, statistical and deep
learning based stuttering/disfluency classification methods. We also present
several challenges and possible future directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sheikh_S/0/1/0/all/0/1"&gt;Shakeel Ahmad Sheikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahidullah_M/0/1/0/all/0/1"&gt;Md Sahidullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hirsch_F/0/1/0/all/0/1"&gt;Fabrice Hirsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouni_S/0/1/0/all/0/1"&gt;Slim Ouni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ASC-Net : Adversarial-based Selective Network for Unsupervised Anomaly Segmentation. (arXiv:2103.03664v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03664</id>
        <link href="http://arxiv.org/abs/2103.03664"/>
        <updated>2021-07-12T01:55:15.344Z</updated>
        <summary type="html"><![CDATA[We introduce a neural network framework, utilizing adversarial learning to
partition an image into two cuts, with one cut falling into a reference
distribution provided by the user. This concept tackles the task of
unsupervised anomaly segmentation, which has attracted increasing attention in
recent years due to their broad applications in tasks with unlabelled data.
This Adversarial-based Selective Cutting network (ASC-Net) bridges the two
domains of cluster-based deep learning methods and adversarial-based
anomaly/novelty detection algorithms. We evaluate this unsupervised learning
model on BraTS brain tumor segmentation, LiTS liver lesion segmentation, and
MS-SEG2015 segmentation tasks. Compared to existing methods like the AnoGAN
family, our model demonstrates tremendous performance gains in unsupervised
anomaly segmentation tasks. Although there is still room to further improve
performance compared to supervised learning algorithms, the promising
experimental results shed light on building an unsupervised learning algorithm
using user-defined knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dey_R/0/1/0/all/0/1"&gt;Raunak Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1"&gt;Yi Hong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MutualEyeContact: A conversation analysis tool with focus on eye contact. (arXiv:2107.04476v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04476</id>
        <link href="http://arxiv.org/abs/2107.04476"/>
        <updated>2021-07-12T01:55:15.326Z</updated>
        <summary type="html"><![CDATA[Eye contact between individuals is particularly important for understanding
human behaviour. To further investigate the importance of eye contact in social
interactions, portable eye tracking technology seems to be a natural choice.
However, the analysis of available data can become quite complex. Scientists
need data that is calculated quickly and accurately. Additionally, the relevant
data must be automatically separated to save time. In this work, we propose a
tool called MutualEyeContact which excels in those tasks and can help
scientists to understand the importance of (mutual) eye contact in social
interactions. We combine state-of-the-art eye tracking with face recognition
based on machine learning and provide a tool for analysis and visualization of
social interaction sessions. This work is a joint collaboration of computer
scientists and cognitive scientists. It combines the fields of social and
behavioural science with computer vision and deep learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schafer_A/0/1/0/all/0/1"&gt;Alexander Sch&amp;#xe4;fer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Isomura_T/0/1/0/all/0/1"&gt;Tomoko Isomura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reis_G/0/1/0/all/0/1"&gt;Gerd Reis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watanabe_K/0/1/0/all/0/1"&gt;Katsumi Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1"&gt;Didier Stricker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Activated Gradients for Deep Neural Networks. (arXiv:2107.04228v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04228</id>
        <link href="http://arxiv.org/abs/2107.04228"/>
        <updated>2021-07-12T01:55:15.306Z</updated>
        <summary type="html"><![CDATA[Deep neural networks often suffer from poor performance or even training
failure due to the ill-conditioned problem, the vanishing/exploding gradient
problem, and the saddle point problem. In this paper, a novel method by acting
the gradient activation function (GAF) on the gradient is proposed to handle
these challenges. Intuitively, the GAF enlarges the tiny gradients and
restricts the large gradient. Theoretically, this paper gives conditions that
the GAF needs to meet, and on this basis, proves that the GAF alleviates the
problems mentioned above. In addition, this paper proves that the convergence
rate of SGD with the GAF is faster than that without the GAF under some
assumptions. Furthermore, experiments on CIFAR, ImageNet, and PASCAL visual
object classes confirm the GAF's effectiveness. The experimental results also
demonstrate that the proposed method is able to be adopted in various deep
neural networks to improve their performance. The source code is publicly
available at
https://github.com/LongJin-lab/Activated-Gradients-for-Deep-Neural-Networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liangming Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1"&gt;Xiaohao Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1"&gt;Long Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shang_M/0/1/0/all/0/1"&gt;Mingsheng Shang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantile-Quantile Embedding for Distribution Transformation and Manifold Embedding with Ability to Choose the Embedding Distribution. (arXiv:2006.11385v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.11385</id>
        <link href="http://arxiv.org/abs/2006.11385"/>
        <updated>2021-07-12T01:55:15.297Z</updated>
        <summary type="html"><![CDATA[We propose a new embedding method, named Quantile-Quantile Embedding (QQE),
for distribution transformation and manifold embedding with the ability to
choose the embedding distribution. QQE, which uses the concept of
quantile-quantile plot from visual statistical tests, can transform the
distribution of data to any theoretical desired distribution or empirical
reference sample. Moreover, QQE gives the user a choice of embedding
distribution in embedding the manifold of data into the low dimensional
embedding space. It can also be used for modifying the embedding distribution
of other dimensionality reduction methods, such as PCA, t-SNE, and deep metric
learning, for better representation or visualization of data. We propose QQE in
both unsupervised and supervised forms. QQE can also transform a distribution
to either an exact reference distribution or its shape. We show that QQE allows
for better discrimination of classes in some cases. Our experiments on
different synthetic and image datasets show the effectiveness of the proposed
embedding method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ghojogh_B/0/1/0/all/0/1"&gt;Benyamin Ghojogh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Karray_F/0/1/0/all/0/1"&gt;Fakhri Karray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Crowley_M/0/1/0/all/0/1"&gt;Mark Crowley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PAC Bayesian Performance Guarantees for Deep (Stochastic) Networks in Medical Imaging. (arXiv:2104.05600v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05600</id>
        <link href="http://arxiv.org/abs/2104.05600"/>
        <updated>2021-07-12T01:55:15.289Z</updated>
        <summary type="html"><![CDATA[Application of deep neural networks to medical imaging tasks has in some
sense become commonplace. Still, a "thorn in the side" of the deep learning
movement is the argument that deep networks are prone to overfitting and are
thus unable to generalize well when datasets are small (as is common in medical
imaging tasks). One way to bolster confidence is to provide mathematical
guarantees, or bounds, on network performance after training which explicitly
quantify the possibility of overfitting. In this work, we explore recent
advances using the PAC-Bayesian framework to provide bounds on generalization
error for large (stochastic) networks. While previous efforts focus on
classification in larger natural image datasets (e.g., MNIST and CIFAR-10), we
apply these techniques to both classification and segmentation in a smaller
medical imagining dataset: the ISIC 2018 challenge set. We observe the
resultant bounds are competitive compared to a simpler baseline, while also
being more explainable and alleviating the need for holdout sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sicilia_A/0/1/0/all/0/1"&gt;Anthony Sicilia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xingchen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sosnovskikh_A/0/1/0/all/0/1"&gt;Anastasia Sosnovskikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Seong Jae Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mutually-aware Sub-Graphs Differentiable Architecture Search. (arXiv:2107.04324v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04324</id>
        <link href="http://arxiv.org/abs/2107.04324"/>
        <updated>2021-07-12T01:55:15.269Z</updated>
        <summary type="html"><![CDATA[Differentiable architecture search is prevalent in the field of NAS because
of its simplicity and efficiency, where two paradigms, multi-path algorithms
and single-path methods, are dominated. Multi-path framework (e.g. DARTS) is
intuitive but suffers from memory usage and training collapse. Single-path
methods (e.g.GDAS and ProxylessNAS) mitigate the memory issue and shrink the
gap between searching and evaluation but sacrifice the performance. In this
paper, we propose a conceptually simple yet efficient method to bridge these
two paradigms, referred as Mutually-aware Sub-Graphs Differentiable
Architecture Search (MSG-DAS). The core of our framework is a differentiable
Gumbel-TopK sampler that produces multiple mutually exclusive single-path
sub-graphs. To alleviate the severer skip-connect issue brought by multiple
sub-graphs setting, we propose a Dropblock-Identity module to stabilize the
optimization. To make best use of the available models (super-net and
sub-graphs), we introduce a memory-efficient super-net guidance distillation to
improve training. The proposed framework strikes a balance between flexible
memory usage and searching quality. We demonstrate the effectiveness of our
methods on ImageNet and CIFAR10, where the searched models show a comparable
performance as the most recent approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1"&gt;Haoxian Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1"&gt;Sheng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1"&gt;Yujie Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Weilin Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Icon Annotation For Mobile Applications. (arXiv:2107.04452v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04452</id>
        <link href="http://arxiv.org/abs/2107.04452"/>
        <updated>2021-07-12T01:55:15.243Z</updated>
        <summary type="html"><![CDATA[Annotating user interfaces (UIs) that involves localization and
classification of meaningful UI elements on a screen is a critical step for
many mobile applications such as screen readers and voice control of devices.
Annotating object icons, such as menu, search, and arrow backward, is
especially challenging due to the lack of explicit labels on screens, their
similarity to pictures, and their diverse shapes. Existing studies either use
view hierarchy or pixel based methods to tackle the task. Pixel based
approaches are more popular as view hierarchy features on mobile platforms are
often incomplete or inaccurate, however it leaves out instructional information
in the view hierarchy such as resource-ids or content descriptions. We propose
a novel deep learning based multi-modal approach that combines the benefits of
both pixel and view hierarchy features as well as leverages the
state-of-the-art object detection techniques. In order to demonstrate the
utility provided, we create a high quality UI dataset by manually annotating
the most commonly used 29 icons in Rico, a large scale mobile design dataset
consisting of 72k UI screenshots. The experimental results indicate the
effectiveness of our multi-modal approach. Our model not only outperforms a
widely used object classification baseline but also pixel based object
detection models. Our study sheds light on how to combine view hierarchy with
pixel features for annotating UI elements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zang_X/0/1/0/all/0/1"&gt;Xiaoxue Zang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Ying Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jindong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Compositional Convolutional Neural Networks. (arXiv:2107.04474v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04474</id>
        <link href="http://arxiv.org/abs/2107.04474"/>
        <updated>2021-07-12T01:55:15.236Z</updated>
        <summary type="html"><![CDATA[The reasonable definition of semantic interpretability presents the core
challenge in explainable AI. This paper proposes a method to modify a
traditional convolutional neural network (CNN) into an interpretable
compositional CNN, in order to learn filters that encode meaningful visual
patterns in intermediate convolutional layers. In a compositional CNN, each
filter is supposed to consistently represent a specific compositional object
part or image region with a clear meaning. The compositional CNN learns from
image labels for classification without any annotations of parts or regions for
supervision. Our method can be broadly applied to different types of CNNs.
Experiments have demonstrated the effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1"&gt;Wen Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1"&gt;Zhihua Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shikun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Binbin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1"&gt;Jiaqi Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Ping Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Quanshi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-modal Attention for MRI and Ultrasound Volume Registration. (arXiv:2107.04548v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04548</id>
        <link href="http://arxiv.org/abs/2107.04548"/>
        <updated>2021-07-12T01:55:15.229Z</updated>
        <summary type="html"><![CDATA[Prostate cancer biopsy benefits from accurate fusion of transrectal
ultrasound (TRUS) and magnetic resonance (MR) images. In the past few years,
convolutional neural networks (CNNs) have been proved powerful in extracting
image features crucial for image registration. However, challenging
applications and recent advances in computer vision suggest that CNNs are quite
limited in its ability to understand spatial correspondence between features, a
task in which the self-attention mechanism excels. This paper aims to develop a
self-attention mechanism specifically for cross-modal image registration. Our
proposed cross-modal attention block effectively maps each of the features in
one volume to all features in the corresponding volume. Our experimental
results demonstrate that a CNN network designed with the cross-modal attention
block embedded outperforms an advanced CNN network 10 times of its size. We
also incorporated visualization techniques to improve the interpretability of
our network. The source code of our work is available at
https://github.com/DIAL-RPI/Attention-Reg .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xinrui Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Hengtao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xuanang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_H/0/1/0/all/0/1"&gt;Hanqing Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Sheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turkbey_B/0/1/0/all/0/1"&gt;Baris Turkbey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wood_B/0/1/0/all/0/1"&gt;Bradford J. Wood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Ge Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_P/0/1/0/all/0/1"&gt;Pingkun Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability. (arXiv:2006.14512v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.14512</id>
        <link href="http://arxiv.org/abs/2006.14512"/>
        <updated>2021-07-12T01:55:15.215Z</updated>
        <summary type="html"><![CDATA[Knowledge transferability, or transfer learning, has been widely adopted to
allow a pre-trained model in the source domain to be effectively adapted to
downstream tasks in the target domain. It is thus important to explore and
understand the factors affecting knowledge transferability. In this paper, as
the first work, we analyze and demonstrate the connections between knowledge
transferability and another important phenomenon--adversarial transferability,
\emph{i.e.}, adversarial examples generated against one model can be
transferred to attack other models. Our theoretical studies show that
adversarial transferability indicates knowledge transferability and vice versa.
Moreover, based on the theoretical insights, we propose two practical
adversarial transferability metrics to characterize this process, serving as
bidirectional indicators between adversarial and knowledge transferability. We
conduct extensive experiments for different scenarios on diverse datasets,
showing a positive correlation between adversarial transferability and
knowledge transferability. Our findings will shed light on future research
about effective knowledge transfer learning and adversarial transferability
analyses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1"&gt;Kaizhao Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jacky Y. Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Boxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhuolin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koyejo_O/0/1/0/all/0/1"&gt;Oluwasanmi Koyejo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hacking VMAF and VMAF NEG: metrics vulnerability to different preprocessing. (arXiv:2107.04510v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.04510</id>
        <link href="http://arxiv.org/abs/2107.04510"/>
        <updated>2021-07-12T01:55:15.207Z</updated>
        <summary type="html"><![CDATA[Video quality measurement plays a critical role in the development of video
processing applications. In this paper, we show how popular quality metrics
VMAF and its tuning-resistant version VMAF NEG can be artificially increased by
video preprocessing. We propose a pipeline for tuning parameters of processing
algorithms that allows increasing VMAF by up to 218.8%. A subjective comparison
of preprocessed videos showed that with the majority of methods visual quality
drops down or stays unchanged. We show that VMAF NEG scores can also be
increased by some preprocessing methods by up to 23.6%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Siniukov_M/0/1/0/all/0/1"&gt;Maksim Siniukov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antsiferova_A/0/1/0/all/0/1"&gt;Anastasia Antsiferova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulikov_D/0/1/0/all/0/1"&gt;Dmitriy Kulikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vatolin_D/0/1/0/all/0/1"&gt;Dmitriy Vatolin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient-Based Quantification of Epistemic Uncertainty for Deep Object Detectors. (arXiv:2107.04517v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04517</id>
        <link href="http://arxiv.org/abs/2107.04517"/>
        <updated>2021-07-12T01:55:15.179Z</updated>
        <summary type="html"><![CDATA[Reliable epistemic uncertainty estimation is an essential component for
backend applications of deep object detectors in safety-critical environments.
Modern network architectures tend to give poorly calibrated confidences with
limited predictive power. Here, we introduce novel gradient-based uncertainty
metrics and investigate them for different object detection architectures.
Experiments on the MS COCO, PASCAL VOC and the KITTI dataset show significant
improvements in true positive / false positive discrimination and prediction of
intersection over union as compared to network confidence. We also find
improvement over Monte-Carlo dropout uncertainty metrics and further
significant boosts by aggregating different sources of uncertainty metrics.The
resulting uncertainty models generate well-calibrated confidences in all
instances. Furthermore, we implement our uncertainty quantification models into
object detection pipelines as a means to discern true against false
predictions, replacing the ordinary score-threshold-based decision rule. In our
experiments, we achieve a significant boost in detection performance in terms
of mean average precision. With respect to computational complexity, we find
that computing gradient uncertainty metrics results in floating point operation
counts similar to those of Monte-Carlo dropout.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Riedlinger_T/0/1/0/all/0/1"&gt;Tobias Riedlinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rottmann_M/0/1/0/all/0/1"&gt;Matthias Rottmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schubert_M/0/1/0/all/0/1"&gt;Marius Schubert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gottschalk_H/0/1/0/all/0/1"&gt;Hanno Gottschalk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Score refinement for confidence-based 3D multi-object tracking. (arXiv:2107.04327v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04327</id>
        <link href="http://arxiv.org/abs/2107.04327"/>
        <updated>2021-07-12T01:55:15.158Z</updated>
        <summary type="html"><![CDATA[Multi-object tracking is a critical component in autonomous navigation, as it
provides valuable information for decision-making. Many researchers tackled the
3D multi-object tracking task by filtering out the frame-by-frame 3D
detections; however, their focus was mainly on finding useful features or
proper matching metrics. Our work focuses on a neglected part of the tracking
system: score refinement and tracklet termination. We show that manipulating
the scores depending on time consistency while terminating the tracklets
depending on the tracklet score improves tracking results. We do this by
increasing the matched tracklets' score with score update functions and
decreasing the unmatched tracklets' score. Compared to count-based methods, our
method consistently produces better AMOTA and MOTA scores when utilizing
various detectors and filtering algorithms on different datasets. The
improvements in AMOTA score went up to 1.83 and 2.96 in MOTA. We also used our
method as a late-fusion ensembling method, and it performed better than
voting-based ensemble methods by a solid margin. It achieved an AMOTA score of
67.6 on nuScenes test evaluation, which is comparable to other state-of-the-art
trackers. Code is publicly available at:
\url{https://github.com/cogsys-tuebingen/CBMOT}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Benbarka_N/0/1/0/all/0/1"&gt;Nuri Benbarka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schroder_J/0/1/0/all/0/1"&gt;Jona Schr&amp;#xf6;der&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1"&gt;Andreas Zell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hoechst Is All You Need: LymphocyteClassification with Deep Learning. (arXiv:2107.04388v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04388</id>
        <link href="http://arxiv.org/abs/2107.04388"/>
        <updated>2021-07-12T01:55:15.150Z</updated>
        <summary type="html"><![CDATA[Multiplex immunofluorescence and immunohistochemistry benefit patients by
allowing cancer pathologists to identify several proteins expressed on the
surface of cells, enabling cell classification, better understanding of the
tumour micro-environment, more accurate diagnoses, prognoses, and tailored
immunotherapy based on the immune status of individual patients. However, they
are expensive and time consuming processes which require complex staining and
imaging techniques by expert technicians. Hoechst staining is much cheaper and
easier to perform, but is not typically used in this case as it binds to DNA
rather than to the proteins targeted by immunofluorescent techniques, and it
was not previously thought possible to differentiate cells expressing these
proteins based only on DNA morphology. In this work we show otherwise, training
a deep convolutional neural network to identify cells expressing three proteins
(T lymphocyte markers CD3 and CD8, and the B lymphocyte marker CD20) with
greater than 90% precision and recall, from Hoechst 33342 stained tissue only.
Our model learns previously unknown morphological features associated with
expression of these proteins which can be used to accurately differentiate
lymphocyte subtypes for use in key prognostic metrics such as assessment of
immune cell infiltration,and thereby predict and improve patient outcomes
without the need for costly multiplex immunofluorescence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cooper_J/0/1/0/all/0/1"&gt;Jessica Cooper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Um_I/0/1/0/all/0/1"&gt;In Hwa Um&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1"&gt;Ognjen Arandjelovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harrison_D/0/1/0/all/0/1"&gt;David J Harrison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[JPGNet: Joint Predictive Filtering and Generative Network for Image Inpainting. (arXiv:2107.04281v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04281</id>
        <link href="http://arxiv.org/abs/2107.04281"/>
        <updated>2021-07-12T01:55:15.142Z</updated>
        <summary type="html"><![CDATA[Image inpainting aims to restore the missing regions and make the recovery
results identical to the originally complete image, which is different from the
common generative task emphasizing the naturalness of generated images.
Nevertheless, existing works usually regard it as a pure generation problem and
employ cutting-edge generative techniques to address it. The generative
networks fill the main missing parts with realistic contents but usually
distort the local structures. In this paper, we formulate image inpainting as a
mix of two problems, i.e., predictive filtering and deep generation. Predictive
filtering is good at preserving local structures and removing artifacts but
falls short to complete the large missing regions. The deep generative network
can fill the numerous missing pixels based on the understanding of the whole
scene but hardly restores the details identical to the original ones. To make
use of their respective advantages, we propose the joint predictive filtering
and generative network (JPGNet) that contains three branches: predictive
filtering & uncertainty network (PFUNet), deep generative network, and
uncertainty-aware fusion network (UAFNet). The PFUNet can adaptively predict
pixel-wise kernels for filtering-based inpainting according to the input image
and output an uncertainty map. This map indicates the pixels should be
processed by filtering or generative networks, which is further fed to the
UAFNet for a smart combination between filtering and generative results. Note
that, our method as a novel framework for the image inpainting problem can
benefit any existing generation-based methods. We validate our method on three
public datasets, i.e., Dunhuang, Places2, and CelebA, and demonstrate that our
method can enhance three state-of-the-art generative methods (i.e., StructFlow,
EdgeConnect, and RFRNet) significantly with the slightly extra time cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoguang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1"&gt;Qing Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1"&gt;Felix Juefei-Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hongkai Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+wang_S/0/1/0/all/0/1"&gt;Song wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Segmentation on Multiple Visual Domains. (arXiv:2107.04326v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04326</id>
        <link href="http://arxiv.org/abs/2107.04326"/>
        <updated>2021-07-12T01:55:15.133Z</updated>
        <summary type="html"><![CDATA[Semantic segmentation models only perform well on the domain they are trained
on and datasets for training are scarce and often have a small label-spaces,
because the pixel level annotations required are expensive to make. Thus
training models on multiple existing domains is desired to increase the output
label-space. Current research shows that there is potential to improve accuracy
across datasets by using multi-domain training, but this has not yet been
successfully extended to datasets of three different non-overlapping domains
without manual labelling. In this paper a method for this is proposed for the
datasets Cityscapes, SUIM and SUN RGB-D, by creating a label-space that spans
all classes of the datasets. Duplicate classes are merged and discrepant
granularity is solved by keeping classes separate. Results show that accuracy
of the multi-domain model has higher accuracy than all baseline models
together, if hardware performance is equalized, as resources are not limitless,
showing that models benefit from additional data even from domains that have
nothing in common.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naber_F/0/1/0/all/0/1"&gt;Floris Naber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Farthest Point Sampling in Point-Wise Analysis. (arXiv:2107.04291v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04291</id>
        <link href="http://arxiv.org/abs/2107.04291"/>
        <updated>2021-07-12T01:55:15.108Z</updated>
        <summary type="html"><![CDATA[Sampling, grouping, and aggregation are three important components in the
multi-scale analysis of point clouds. In this paper, we present a novel
data-driven sampler learning strategy for point-wise analysis tasks. Unlike the
widely used sampling technique, Farthest Point Sampling (FPS), we propose to
learn sampling and downstream applications jointly. Our key insight is that
uniform sampling methods like FPS are not always optimal for different tasks:
sampling more points around boundary areas can make the point-wise
classification easier for segmentation. Towards the end, we propose a novel
sampler learning strategy that learns sampling point displacement supervised by
task-related ground truth information and can be trained jointly with the
underlying tasks. We further demonstrate our methods in various point-wise
analysis architectures, including semantic part segmentation, point cloud
completion, and keypoint detection. Our experiments show that jointly learning
of the sampler and task brings remarkable improvement over previous baseline
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yiqun Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lichang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Haibin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1"&gt;Chongyang Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xiaoguang Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Shuguang Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hepatocellular Carcinoma Segmentation fromDigital Subtraction Angiography Videos usingLearnable Temporal Difference. (arXiv:2107.04306v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04306</id>
        <link href="http://arxiv.org/abs/2107.04306"/>
        <updated>2021-07-12T01:55:15.101Z</updated>
        <summary type="html"><![CDATA[Automatic segmentation of hepatocellular carcinoma (HCC)in Digital
Subtraction Angiography (DSA) videos can assist radiologistsin efficient
diagnosis of HCC and accurate evaluation of tumors in clinical practice. Few
studies have investigated HCC segmentation from DSAvideos. It shows great
challenging due to motion artifacts in filming, ambiguous boundaries of tumor
regions and high similarity in imaging toother anatomical tissues. In this
paper, we raise the problem of HCCsegmentation in DSA videos, and build our own
DSA dataset. We alsopropose a novel segmentation network called DSA-LTDNet,
including asegmentation sub-network, a temporal difference learning (TDL)
moduleand a liver region segmentation (LRS) sub-network for providing
additional guidance. DSA-LTDNet is preferable for learning the latent
motioninformation from DSA videos proactively and boosting segmentation
performance. All of experiments are conducted on our self-collected
dataset.Experimental results show that DSA-LTDNet increases the DICE scoreby
nearly 4% compared to the U-Net baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jiang_W/0/1/0/all/0/1"&gt;Wenting Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yicheng Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1"&gt;Changmiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Han_X/0/1/0/all/0/1"&gt;Xiaoguang Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shuixing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wan_X/0/1/0/all/0/1"&gt;Xiang Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cui_S/0/1/0/all/0/1"&gt;Shuguang Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Multi-task Mean Teacher for Semi-supervised Facial Affective Behavior Analysis. (arXiv:2107.04225v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04225</id>
        <link href="http://arxiv.org/abs/2107.04225"/>
        <updated>2021-07-12T01:55:15.089Z</updated>
        <summary type="html"><![CDATA[Affective Behavior Analysis is an important part in human?computer
interaction. Existing successful affective behavior analysis method such as
TSAV[9] suffer from challenge of incomplete labeled datasets. To boost its
performance, this paper presents a multi-task mean teacher model for
semi?supervised Affective Behavior Analysis to learn from missing labels and
exploring the learning of multiple correlated task simultaneously. To be
specific, we first utilize TSAV as baseline model to simultaneously recognize
the three tasks. We have modified the preprocessing method of rendering mask to
provide better semantics information. After that, we extended TSAV model to
semi-supervised model using mean teacher, which allow it to be benefited from
unlabeled data. Experimental results on validation datasets show that our
method achieves better performance than TSAV model, which verifies that the
proposed network can effectively learn additional unlabeled data to boost the
affective behavior analysis performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lingfeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shisen Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LIFE: A Generalizable Autodidactic Pipeline for 3D OCT-A Vessel Segmentation. (arXiv:2107.04282v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04282</id>
        <link href="http://arxiv.org/abs/2107.04282"/>
        <updated>2021-07-12T01:55:15.082Z</updated>
        <summary type="html"><![CDATA[Optical coherence tomography (OCT) is a non-invasive imaging technique widely
used for ophthalmology. It can be extended to OCT angiography (OCT-A), which
reveals the retinal vasculature with improved contrast. Recent deep learning
algorithms produced promising vascular segmentation results; however, 3D
retinal vessel segmentation remains difficult due to the lack of manually
annotated training data. We propose a learning-based method that is only
supervised by a self-synthesized modality named local intensity fusion (LIF).
LIF is a capillary-enhanced volume computed directly from the input OCT-A. We
then construct the local intensity fusion encoder (LIFE) to map a given OCT-A
volume and its LIF counterpart to a shared latent space. The latent space of
LIFE has the same dimensions as the input data and it contains features common
to both modalities. By binarizing this latent space, we obtain a volumetric
vessel segmentation. Our method is evaluated in a human fovea OCT-A and three
zebrafish OCT-A volumes with manual labels. It yields a Dice score of 0.7736 on
human data and 0.8594 +/- 0.0275 on zebrafish data, a dramatic improvement over
existing unsupervised algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hu_D/0/1/0/all/0/1"&gt;Dewei Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cui_C/0/1/0/all/0/1"&gt;Can Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Larson_K/0/1/0/all/0/1"&gt;Kathleen E. Larson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tao_Y/0/1/0/all/0/1"&gt;Yuankai K. Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oguz_I/0/1/0/all/0/1"&gt;Ipek Oguz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CASPIANET++: A Multidimensional Channel-Spatial Asymmetric Attention Network with Noisy Student Curriculum Learning Paradigm for Brain Tumor Segmentation. (arXiv:2107.04099v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04099</id>
        <link href="http://arxiv.org/abs/2107.04099"/>
        <updated>2021-07-12T01:55:15.075Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) have been used quite successfully for
semantic segmentation of brain tumors. However, current CNNs and attention
mechanisms are stochastic in nature and neglect the morphological indicators
used by radiologists to manually annotate regions of interest. In this paper,
we introduce a channel and spatial wise asymmetric attention (CASPIAN) by
leveraging the inherent structure of tumors to detect regions of saliency. To
demonstrate the efficacy of our proposed layer, we integrate this into a
well-established convolutional neural network (CNN) architecture to achieve
higher Dice scores, with less GPU resources. Also, we investigate the inclusion
of auxiliary multiscale and multiplanar attention branches to increase the
spatial context crucial in semantic segmentation tasks. The resulting
architecture is the new CASPIANET++, which achieves Dice Scores of 91.19% whole
tumor, 87.6% for tumor core and 81.03% for enhancing tumor. Furthermore, driven
by the scarcity of brain tumor data, we investigate the Noisy Student method
for segmentation tasks. Our new Noisy Student Curriculum Learning paradigm,
which infuses noise incrementally to increase the complexity of the training
images exposed to the network, further boosts the enhancing tumor region to
81.53%. Additional validation performed on the BraTS2020 data shows that the
Noisy Student Curriculum Learning method works well without any additional
training or finetuning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liew_A/0/1/0/all/0/1"&gt;Andrea Liew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chun Cheng Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lan_B/0/1/0/all/0/1"&gt;Boon Leong Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tan_M/0/1/0/all/0/1"&gt;Maxine Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Modal Association based Grouping for Form Structure Extraction. (arXiv:2107.04396v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04396</id>
        <link href="http://arxiv.org/abs/2107.04396"/>
        <updated>2021-07-12T01:55:15.055Z</updated>
        <summary type="html"><![CDATA[Document structure extraction has been a widely researched area for decades.
Recent work in this direction has been deep learning-based, mostly focusing on
extracting structure using fully convolution NN through semantic segmentation.
In this work, we present a novel multi-modal approach for form structure
extraction. Given simple elements such as textruns and widgets, we extract
higher-order structures such as TextBlocks, Text Fields, Choice Fields, and
Choice Groups, which are essential for information collection in forms. To
achieve this, we obtain a local image patch around each low-level element
(reference) by identifying candidate elements closest to it. We process textual
and spatial representation of candidates sequentially through a BiLSTM to
obtain context-aware representations and fuse them with image patch features
obtained by processing it through a CNN. Subsequently, the sequential decoder
takes this fused feature vector to predict the association type between
reference and candidates. These predicted associations are utilized to
determine larger structures through connected components analysis. Experimental
results show the effectiveness of our approach achieving a recall of 90.29%,
73.80%, 83.12%, and 52.72% for the above structures, respectively,
outperforming semantic segmentation baselines significantly. We show the
efficacy of our method through ablations, comparing it against using individual
modalities. We also introduce our new rich human-annotated Forms Dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_M/0/1/0/all/0/1"&gt;Milan Aggarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_M/0/1/0/all/0/1"&gt;Mausoom Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1"&gt;Hiresh Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1"&gt;Balaji Krishnamurthy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially private training of neural networks with Langevin dynamics forcalibrated predictive uncertainty. (arXiv:2107.04296v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04296</id>
        <link href="http://arxiv.org/abs/2107.04296"/>
        <updated>2021-07-12T01:55:15.047Z</updated>
        <summary type="html"><![CDATA[We show that differentially private stochastic gradient descent (DP-SGD) can
yield poorly calibrated, overconfident deep learning models. This represents a
serious issue for safety-critical applications, e.g. in medical diagnosis. We
highlight and exploit parallels between stochastic gradient Langevin dynamics,
a scalable Bayesian inference technique for training deep neural networks, and
DP-SGD, in order to train differentially private, Bayesian neural networks with
minor adjustments to the original (DP-SGD) algorithm. Our approach provides
considerably more reliable uncertainty estimates than DP-SGD, as demonstrated
empirically by a reduction in expected calibration error (MNIST $\sim{5}$-fold,
Pediatric Pneumonia Dataset $\sim{2}$-fold).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Knolle_M/0/1/0/all/0/1"&gt;Moritz Knolle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1"&gt;Alexander Ziller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usynin_D/0/1/0/all/0/1"&gt;Dmitrii Usynin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Braren_R/0/1/0/all/0/1"&gt;Rickmer Braren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1"&gt;Marcus R. Makowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1"&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1"&gt;Georgios Kaissis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UrbanScene3D: A Large Scale Urban Scene Dataset and Simulator. (arXiv:2107.04286v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04286</id>
        <link href="http://arxiv.org/abs/2107.04286"/>
        <updated>2021-07-12T01:55:15.039Z</updated>
        <summary type="html"><![CDATA[The ability to perceive the environments in different ways is essential to
robotic research. This involves the analysis of both 2D and 3D data sources. We
present a large scale urban scene dataset associated with a handy simulator
based on Unreal Engine 4 and AirSim, which consists of both man-made and
real-world reconstruction scenes in different scales, referred to as
UrbanScene3D. Unlike previous works that purely based on 2D information or
man-made 3D CAD models, UrbanScene3D contains both compact man-made models and
detailed real-world models reconstructed by aerial images. Each building has
been manually extracted from the entire scene model and then has been assigned
with a unique label, forming an instance segmentation map. The provided 3D
ground-truth textured models with instance segmentation labels in UrbanScene3D
allow users to obtain all kinds of data they would like to have: instance
segmentation map, depth map in arbitrary resolution, 3D point cloud/mesh in
both visible and invisible places, etc. In addition, with the help of AirSim,
users can also simulate the robots (cars/drones)to test a variety of autonomous
tasks in the proposed city environment. Please refer to our paper and
website(https://vcc.tech/UrbanScene3D/) for further details and applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yilin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1"&gt;Fuyou Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hui Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph-based Deep Generative Modelling for Document Layout Generation. (arXiv:2107.04357v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04357</id>
        <link href="http://arxiv.org/abs/2107.04357"/>
        <updated>2021-07-12T01:55:15.032Z</updated>
        <summary type="html"><![CDATA[One of the major prerequisites for any deep learning approach is the
availability of large-scale training data. When dealing with scanned document
images in real world scenarios, the principal information of its content is
stored in the layout itself. In this work, we have proposed an automated deep
generative model using Graph Neural Networks (GNNs) to generate synthetic data
with highly variable and plausible document layouts that can be used to train
document interpretation systems, in this case, specially in digital mailroom
applications. It is also the first graph-based approach for document layout
generation task experimented on administrative document images, in this case,
invoices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1"&gt;Sanket Biswas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riba_P/0/1/0/all/0/1"&gt;Pau Riba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Llados_J/0/1/0/all/0/1"&gt;Josep Llad&amp;#xf3;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1"&gt;Umapada Pal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Pixel-Matching for Video Object Segmentation. (arXiv:2107.04279v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04279</id>
        <link href="http://arxiv.org/abs/2107.04279"/>
        <updated>2021-07-12T01:55:15.025Z</updated>
        <summary type="html"><![CDATA[Video object segmentation, aiming to segment the foreground objects given the
annotation of the first frame, has been attracting increasing attentions. Many
state-of-the-art approaches have achieved great performance by relying on
online model updating or mask-propagation techniques. However, most online
models require high computational cost due to model fine-tuning during
inference. Most mask-propagation based models are faster but with relatively
low performance due to failure to adapt to object appearance variation. In this
paper, we are aiming to design a new model to make a good balance between speed
and performance. We propose a model, called NPMCA-net, which directly localizes
foreground objects based on mask-propagation and non-local technique by
matching pixels in reference and target frames. Since we bring in information
of both first and previous frames, our network is robust to large object
appearance variation, and can better adapt to occlusions. Extensive experiments
show that our approach can achieve a new state-of-the-art performance with a
fast speed at the same time (86.5% IoU on DAVIS-2016 and 72.2% IoU on
DAVIS-2017, with speed of 0.11s per frame) under the same level comparison.
Source code is available at https://github.com/siyueyu/NPMCA-net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Siyue Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1"&gt;Jimin Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;BingFeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1"&gt;Eng Gee Lim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Matrix Decomposition for Deep Convolutional Neural Networks Compression. (arXiv:2107.04386v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04386</id>
        <link href="http://arxiv.org/abs/2107.04386"/>
        <updated>2021-07-12T01:55:15.018Z</updated>
        <summary type="html"><![CDATA[Deep convolutional neural networks (CNNs) with a large number of parameters
requires huge computational resources, which has limited the application of
CNNs on resources constrained appliances. Decomposition-based methods,
therefore, have been utilized to compress CNNs in recent years. However, since
the compression factor and performance are negatively correlated, the
state-of-the-art works either suffer from severe performance degradation or
have limited low compression factors. To overcome these problems, unlike
previous works compressing layers separately, we propose to compress CNNs and
alleviate performance degradation via joint matrix decomposition. The idea is
inspired by the fact that there are lots of repeated modules in CNNs, and by
projecting weights with the same structures into the same subspace, networks
can be further compressed and even accelerated. In particular, three joint
matrix decomposition schemes are developed, and the corresponding optimization
approaches based on Singular Values Decomposition are proposed. Extensive
experiments are conducted across three challenging compact CNNs and 3 benchmark
data sets to demonstrate the superior performance of our proposed algorithms.
As a result, our methods can compress the size of ResNet-34 by 22x with
slighter accuracy degradation compared with several state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shaowu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jihao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Weize Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Lei Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning models for benign and malign Ocular Tumor Growth Estimation. (arXiv:2107.04220v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04220</id>
        <link href="http://arxiv.org/abs/2107.04220"/>
        <updated>2021-07-12T01:55:15.000Z</updated>
        <summary type="html"><![CDATA[Relatively abundant availability of medical imaging data has provided
significant support in the development and testing of Neural Network based
image processing methods. Clinicians often face issues in selecting suitable
image processing algorithm for medical imaging data. A strategy for the
selection of a proper model is presented here. The training data set comprises
optical coherence tomography (OCT) and angiography (OCT-A) images of 50 mice
eyes with more than 100 days follow-up. The data contains images from treated
and untreated mouse eyes. Four deep learning variants are tested for automatic
(a) differentiation of tumor region with healthy retinal layer and (b)
segmentation of 3D ocular tumor volumes. Exhaustive sensitivity analysis of
deep learning models is performed with respect to the number of training and
testing images using 8 eight performance indices to study accuracy,
reliability/reproducibility, and speed. U-net with UVgg16 is best for malign
tumor data set with treatment (having considerable variation) and U-net with
Inception backbone for benign tumor data (with minor variation). Loss value and
root mean square error (R.M.S.E.) are found most and least sensitive
performance indices, respectively. The performance (via indices) is found to be
exponentially improving regarding a number of training images. The segmented
OCT-Angiography data shows that neovascularization drives the tumor volume.
Image analysis shows that photodynamic imaging-assisted tumor treatment
protocol is transforming an aggressively growing tumor into a cyst. An
empirical expression is obtained to help medical professionals to choose a
particular model given the number of images and types of characteristics. We
recommend that the presented exercise should be taken as standard practice
before employing a particular deep learning model for biomedical image
analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Goswami_M/0/1/0/all/0/1"&gt;Mayank Goswami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effectiveness of State-of-the-Art Super Resolution Algorithms in Surveillance Environment. (arXiv:2107.04133v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04133</id>
        <link href="http://arxiv.org/abs/2107.04133"/>
        <updated>2021-07-12T01:55:14.993Z</updated>
        <summary type="html"><![CDATA[Image Super Resolution (SR) finds applications in areas where images need to
be closely inspected by the observer to extract enhanced information. One such
focused application is an offline forensic analysis of surveillance feeds. Due
to the limitations of camera hardware, camera pose, limited bandwidth, varying
illumination conditions, and occlusions, the quality of the surveillance feed
is significantly degraded at times, thereby compromising monitoring of
behavior, activities, and other sporadic information in the scene. For the
proposed research work, we have inspected the effectiveness of four
conventional yet effective SR algorithms and three deep learning-based SR
algorithms to seek the finest method that executes well in a surveillance
environment with limited training data op-tions. These algorithms generate an
enhanced resolution output image from a sin-gle low-resolution (LR) input
image. For performance analysis, a subset of 220 images from six surveillance
datasets has been used, consisting of individuals with varying distances from
the camera, changing illumination conditions, and complex backgrounds. The
performance of these algorithms has been evaluated and compared using both
qualitative and quantitative metrics. These SR algo-rithms have also been
compared based on face detection accuracy. By analyzing and comparing the
performance of all the algorithms, a Convolutional Neural Network (CNN) based
SR technique using an external dictionary proved to be best by achieving robust
face detection accuracy and scoring optimal quantitative metric results under
different surveillance conditions. This is because the CNN layers progressively
learn more complex features using an external dictionary.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farooq_M/0/1/0/all/0/1"&gt;Muhammad Ali Farooq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1"&gt;Ammar Ali Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmad_A/0/1/0/all/0/1"&gt;Ansar Ahmad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raza_R/0/1/0/all/0/1"&gt;Rana Hammad Raza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Action Unit Detection with Joint Adaptive Attention and Graph Relation. (arXiv:2107.04389v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04389</id>
        <link href="http://arxiv.org/abs/2107.04389"/>
        <updated>2021-07-12T01:55:14.983Z</updated>
        <summary type="html"><![CDATA[This paper describes an approach to the facial action unit (AU) detection. In
this work, we present our submission to the Field Affective Behavior Analysis
(ABAW) 2021 competition. The proposed method uses the pre-trained JAA model as
the feature extractor, and extracts global features, face alignment features
and AU local features on the basis of multi-scale features. We take the AU
local features as the input of the graph convolution to further consider the
correlation between AU, and finally use the fused features to classify AU. The
detected accuracy was evaluated by 0.5*accuracy + 0.5*F1. Our model achieves
0.674 on the challenging Aff-Wild2 database.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chenggong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Juan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qingyang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1"&gt;Weilong Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1"&gt;Ruomeng Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhilei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wavelet Transform-assisted Adaptive Generative Modeling for Colorization. (arXiv:2107.04261v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04261</id>
        <link href="http://arxiv.org/abs/2107.04261"/>
        <updated>2021-07-12T01:55:14.977Z</updated>
        <summary type="html"><![CDATA[Unsupervised deep learning has recently demonstrated the promise to produce
high-quality samples. While it has tremendous potential to promote the image
colorization task, the performance is limited owing to the manifold hypothesis
in machine learning. This study presents a novel scheme that exploiting the
score-based generative model in wavelet domain to address the issue. By taking
advantage of the multi-scale and multi-channel representation via wavelet
transform, the proposed model learns the priors from stacked wavelet
coefficient components, thus learns the image characteristics under coarse and
detail frequency spectrums jointly and effectively. Moreover, such a highly
flexible generative model without adversarial optimization can execute
colorization tasks better under dual consistency terms in wavelet domain,
namely data-consistency and structure-consistency. Specifically, in the
training phase, a set of multi-channel tensors consisting of wavelet
coefficients are used as the input to train the network by denoising score
matching. In the test phase, samples are iteratively generated via annealed
Langevin dynamics with data and structure consistencies. Experiments
demonstrated remarkable improvements of the proposed model on colorization
quality, particularly on colorization robustness and diversity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wanyun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zichen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuhao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qiegen Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unity Perception: Generate Synthetic Data for Computer Vision. (arXiv:2107.04259v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04259</id>
        <link href="http://arxiv.org/abs/2107.04259"/>
        <updated>2021-07-12T01:55:14.970Z</updated>
        <summary type="html"><![CDATA[We introduce the Unity Perception package which aims to simplify and
accelerate the process of generating synthetic datasets for computer vision
tasks by offering an easy-to-use and highly customizable toolset. This
open-source package extends the Unity Editor and engine components to generate
perfectly annotated examples for several common computer vision tasks.
Additionally, it offers an extensible Randomization framework that lets the
user quickly construct and configure randomized simulation parameters in order
to introduce variation into the generated datasets. We provide an overview of
the provided tools and how they work, and demonstrate the value of the
generated synthetic datasets by training a 2D object detection model. The model
trained with mostly synthetic data outperforms the model trained using only
real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Borkman_S/0/1/0/all/0/1"&gt;Steve Borkman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crespi_A/0/1/0/all/0/1"&gt;Adam Crespi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhakad_S/0/1/0/all/0/1"&gt;Saurav Dhakad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganguly_S/0/1/0/all/0/1"&gt;Sujoy Ganguly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hogins_J/0/1/0/all/0/1"&gt;Jonathan Hogins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jhang_Y/0/1/0/all/0/1"&gt;You-Cyuan Jhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamalzadeh_M/0/1/0/all/0/1"&gt;Mohsen Kamalzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bowen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leal_S/0/1/0/all/0/1"&gt;Steven Leal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parisi_P/0/1/0/all/0/1"&gt;Pete Parisi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romero_C/0/1/0/all/0/1"&gt;Cesar Romero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_W/0/1/0/all/0/1"&gt;Wesley Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thaman_A/0/1/0/all/0/1"&gt;Alex Thaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Warren_S/0/1/0/all/0/1"&gt;Samuel Warren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_N/0/1/0/all/0/1"&gt;Nupur Yadav&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic and Geometric Unfolding of StyleGAN Latent Space. (arXiv:2107.04481v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04481</id>
        <link href="http://arxiv.org/abs/2107.04481"/>
        <updated>2021-07-12T01:55:14.951Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) have proven to be surprisingly
efficient for image editing by inverting and manipulating the latent code
corresponding to a natural image. This property emerges from the disentangled
nature of the latent space. In this paper, we identify two geometric
limitations of such latent space: (a) euclidean distances differ from image
perceptual distance, and (b) disentanglement is not optimal and facial
attribute separation using linear model is a limiting hypothesis. We thus
propose a new method to learn a proxy latent representation using normalizing
flows to remedy these limitations, and show that this leads to a more efficient
space for face image editing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shukor_M/0/1/0/all/0/1"&gt;Mustafa Shukor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1"&gt;Xu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Damodaran_B/0/1/0/all/0/1"&gt;Bharath Bhushan Damodaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hellier_P/0/1/0/all/0/1"&gt;Pierre Hellier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multitask Multi-database Emotion Recognition. (arXiv:2107.04127v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04127</id>
        <link href="http://arxiv.org/abs/2107.04127"/>
        <updated>2021-07-12T01:55:14.944Z</updated>
        <summary type="html"><![CDATA[In this work, we introduce our submission to the 2nd Affective Behavior
Analysis in-the-wild (ABAW) 2021 competition. We train a unified deep learning
model on multi-databases to perform two tasks: seven basic facial expressions
prediction and valence-arousal estimation. Since these databases do not
contains labels for all the two tasks, we have applied the distillation
knowledge technique to train two networks: one teacher and one student model.
The student model will be trained using both ground truth labels and soft
labels derived from the pretrained teacher model. During the training, we add
one more task, which is the combination of the two mentioned tasks, for better
exploiting inter-task correlations. We also exploit the sharing videos between
the two tasks of the AffWild2 database that is used in the competition, to
further improve the performance of the network. Experiment results shows that
the network have achieved promising results on the validation set of the
AffWild2 database. Code and pretrained model are publicly available at
https://github.com/glmanhtu/multitask-abaw-2021]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vu_M/0/1/0/all/0/1"&gt;Manh Tu Vu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beurton_Aimar_M/0/1/0/all/0/1"&gt;Marie Beurton-Aimar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Multi-modal and Multi-task Learning Method for Action Unit and Expression Recognition. (arXiv:2107.04187v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04187</id>
        <link href="http://arxiv.org/abs/2107.04187"/>
        <updated>2021-07-12T01:55:14.938Z</updated>
        <summary type="html"><![CDATA[Analyzing human affect is vital for human-computer interaction systems. Most
methods are developed in restricted scenarios which are not practical for
in-the-wild settings. The Affective Behavior Analysis in-the-wild (ABAW) 2021
Contest provides a benchmark for this in-the-wild problem. In this paper, we
introduce a multi-modal and multi-task learning method by using both visual and
audio information. We use both AU and expression annotations to train the model
and apply a sequence model to further extract associations between video
frames. We achieve an AU score of 0.712 and an expression score of 0.477 on the
validation set. These results demonstrate the effectiveness of our approach in
improving model performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1"&gt;Yue Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1"&gt;Tianqing Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1"&gt;Chao Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Guoqiang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Dropout Discriminator for Domain Adaptation. (arXiv:2107.04231v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04231</id>
        <link href="http://arxiv.org/abs/2107.04231"/>
        <updated>2021-07-12T01:55:14.929Z</updated>
        <summary type="html"><![CDATA[Adaptation of a classifier to new domains is one of the challenging problems
in machine learning. This has been addressed using many deep and non-deep
learning based methods. Among the methodologies used, that of adversarial
learning is widely applied to solve many deep learning problems along with
domain adaptation. These methods are based on a discriminator that ensures
source and target distributions are close. However, here we suggest that rather
than using a point estimate obtaining by a single discriminator, it would be
useful if a distribution based on ensembles of discriminators could be used to
bridge this gap. This could be achieved using multiple classifiers or using
traditional ensemble methods. In contrast, we suggest that a Monte Carlo
dropout based ensemble discriminator could suffice to obtain the distribution
based discriminator. Specifically, we propose a curriculum based dropout
discriminator that gradually increases the variance of the sample based
distribution and the corresponding reverse gradients are used to align the
source and target feature representations. An ensemble of discriminators helps
the model to learn the data distribution efficiently. It also provides a better
gradient estimates to train the feature extractor. The detailed results and
thorough ablation analysis show that our model outperforms state-of-the-art
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kurmi_V/0/1/0/all/0/1"&gt;Vinod K Kurmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Subramanian_V/0/1/0/all/0/1"&gt;Venkatesh K Subramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1"&gt;Vinay P. Namboodiri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RGB Stream Is Enough for Temporal Action Detection. (arXiv:2107.04362v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04362</id>
        <link href="http://arxiv.org/abs/2107.04362"/>
        <updated>2021-07-12T01:55:14.920Z</updated>
        <summary type="html"><![CDATA[State-of-the-art temporal action detectors to date are based on two-stream
input including RGB frames and optical flow. Although combining RGB frames and
optical flow boosts performance significantly, optical flow is a hand-designed
representation which not only requires heavy computation, but also makes it
methodologically unsatisfactory that two-stream methods are often not learned
end-to-end jointly with the flow. In this paper, we argue that optical flow is
dispensable in high-accuracy temporal action detection and image level data
augmentation (ILDA) is the key solution to avoid performance degradation when
optical flow is removed. To evaluate the effectiveness of ILDA, we design a
simple yet efficient one-stage temporal action detector based on single RGB
stream named DaoTAD. Our results show that when trained with ILDA, DaoTAD has
comparable accuracy with all existing state-of-the-art two-stream detectors
while surpassing the inference speed of previous methods by a large margin and
the inference speed is astounding 6668 fps on GeForce GTX 1080 Ti. Code is
available at \url{https://github.com/Media-Smart/vedatad}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chenhao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1"&gt;Hongxiang Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuxin Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1"&gt;Yichao Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Image Synthesis from Intuitive User Input: A Review and Perspectives. (arXiv:2107.04240v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04240</id>
        <link href="http://arxiv.org/abs/2107.04240"/>
        <updated>2021-07-12T01:55:14.900Z</updated>
        <summary type="html"><![CDATA[In many applications of computer graphics, art and design, it is desirable
for a user to provide intuitive non-image input, such as text, sketch, stroke,
graph or layout, and have a computer system automatically generate
photo-realistic images that adhere to the input content. While classic works
that allow such automatic image content generation have followed a framework of
image retrieval and composition, recent advances in deep generative models such
as generative adversarial networks (GANs), variational autoencoders (VAEs), and
flow-based methods have enabled more powerful and versatile image generation
tasks. This paper reviews recent works for image synthesis given intuitive user
input, covering advances in input versatility, image generation methodology,
benchmark datasets, and evaluation metrics. This motivates new perspectives on
input representation and interactivity, cross pollination between major image
generation paradigms, and evaluation and comparison of generation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1"&gt;Yuan Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yuan-Chen Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Han Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1"&gt;Tao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Song-Hai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaolei Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emotion Recognition with Incomplete Labels Using Modified Multi-task Learning Technique. (arXiv:2107.04192v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04192</id>
        <link href="http://arxiv.org/abs/2107.04192"/>
        <updated>2021-07-12T01:55:14.891Z</updated>
        <summary type="html"><![CDATA[The task of predicting affective information in the wild such as seven basic
emotions or action units from human faces has gradually become more interesting
due to the accessibility and availability of massive annotated datasets. In
this study, we propose a method that utilizes the association between seven
basic emotions and twelve action units from the AffWild2 dataset. The method
based on the architecture of ResNet50 involves the multi-task learning
technique for the incomplete labels of the two tasks. By combining the
knowledge for two correlated tasks, both performances are improved by a large
margin compared to those with the model employing only one kind of label.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thinh_P/0/1/0/all/0/1"&gt;Phan Tran Dac Thinh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hung_H/0/1/0/all/0/1"&gt;Hoang Manh Hung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hyung-Jeong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Soo-Hyung Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1"&gt;Guee-Sang Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EasyCom: An Augmented Reality Dataset to Support Algorithms for Easy Communication in Noisy Environments. (arXiv:2107.04174v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.04174</id>
        <link href="http://arxiv.org/abs/2107.04174"/>
        <updated>2021-07-12T01:55:14.881Z</updated>
        <summary type="html"><![CDATA[Augmented Reality (AR) as a platform has the potential to facilitate the
reduction of the cocktail party effect. Future AR headsets could potentially
leverage information from an array of sensors spanning many different
modalities. Training and testing signal processing and machine learning
algorithms on tasks such as beam-forming and speech enhancement require high
quality representative data. To the best of the author's knowledge, as of
publication there are no available datasets that contain synchronized
egocentric multi-channel audio and video with dynamic movement and
conversations in a noisy environment. In this work, we describe, evaluate and
release a dataset that contains over 5 hours of multi-modal data useful for
training and testing algorithms for the application of improving conversations
for an AR glasses wearer. We provide speech intelligibility, quality and
signal-to-noise ratio improvement results for a baseline method and show
improvements across all tested metrics. The dataset we are releasing contains
AR glasses egocentric multi-channel microphone array audio, wide field-of-view
RGB video, speech source pose, headset microphone audio, annotated voice
activity, speech transcriptions, head bounding boxes, target of speech and
source identification labels. We have created and are releasing this dataset to
facilitate research in multi-modal AR solutions to the cocktail party problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Donley_J/0/1/0/all/0/1"&gt;Jacob Donley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tourbabin_V/0/1/0/all/0/1"&gt;Vladimir Tourbabin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jung-Suk Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Broyles_M/0/1/0/all/0/1"&gt;Mark Broyles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Hao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jie Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pantic_M/0/1/0/all/0/1"&gt;Maja Pantic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ithapu_V/0/1/0/all/0/1"&gt;Vamsi Krishna Ithapu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehra_R/0/1/0/all/0/1"&gt;Ravish Mehra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text-to-Text Pre-Training for Data-to-Text Tasks. (arXiv:2005.10433v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.10433</id>
        <link href="http://arxiv.org/abs/2005.10433"/>
        <updated>2021-07-12T01:55:14.803Z</updated>
        <summary type="html"><![CDATA[We study the pre-train + fine-tune strategy for data-to-text tasks. Our
experiments indicate that text-to-text pre-training in the form of T5, enables
simple, end-to-end transformer based models to outperform pipelined neural
architectures tailored for data-to-text generation, as well as alternative
language model based pre-training techniques such as BERT and GPT-2.
Importantly, T5 pre-training leads to better generalization, as evidenced by
large improvements on out-of-domain test sets. We hope our work serves as a
useful baseline for future research, as transfer learning becomes ever more
prevalent for data-to-text tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1"&gt;Mihir Kale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1"&gt;Abhinav Rastogi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Robust Deep Ensemble Classifier for Figurative Language Detection. (arXiv:2107.04372v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04372</id>
        <link href="http://arxiv.org/abs/2107.04372"/>
        <updated>2021-07-12T01:55:14.789Z</updated>
        <summary type="html"><![CDATA[Recognition and classification of Figurative Language (FL) is an open problem
of Sentiment Analysis in the broader field of Natural Language Processing (NLP)
due to the contradictory meaning contained in phrases with metaphorical
content. The problem itself contains three interrelated FL recognition tasks:
sarcasm, irony and metaphor which, in the present paper, are dealt with
advanced Deep Learning (DL) techniques. First, we introduce a data
prepossessing framework towards efficient data representation formats so that
to optimize the respective inputs to the DL models. In addition, special
features are extracted in order to characterize the syntactic, expressive,
emotional and temper content reflected in the respective social media text
references. These features aim to capture aspects of the social network user's
writing method. Finally, features are fed to a robust, Deep Ensemble Soft
Classifier (DESC) which is based on the combination of different DL techniques.
Using three different benchmark datasets (one of them containing various FL
forms) we conclude that the DESC model achieves a very good performance, worthy
of comparison with relevant methodologies and state-of-the-art technologies in
the challenging field of FL recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Potamias_R/0/1/0/all/0/1"&gt;Rolandos Alexandros Potamias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siolas_G/0/1/0/all/0/1"&gt;Georgios Siolas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stafylopatis_A/0/1/0/all/0/1"&gt;Andreas - Georgios Stafylopatis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Summary-Oriented Question Generation for Informational Queries. (arXiv:2010.09692v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09692</id>
        <link href="http://arxiv.org/abs/2010.09692"/>
        <updated>2021-07-12T01:55:14.669Z</updated>
        <summary type="html"><![CDATA[Users frequently ask simple factoid questions for question answering (QA)
systems, attenuating the impact of myriad recent works that support more
complex questions. Prompting users with automatically generated suggested
questions (SQs) can improve user understanding of QA system capabilities and
thus facilitate more effective use. We aim to produce self-explanatory
questions that focus on main document topics and are answerable with variable
length passages as appropriate. We satisfy these requirements by using a
BERT-based Pointer-Generator Network trained on the Natural Questions (NQ)
dataset. Our model shows SOTA performance of SQ generation on the NQ dataset
(20.1 BLEU-4). We further apply our model on out-of-domain news articles,
evaluating with a QA system due to the lack of gold questions and demonstrate
that our model produces better SQs for news articles -- with further
confirmation via a human evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1"&gt;Xusen Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Li Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Small_K/0/1/0/all/0/1"&gt;Kevin Small&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1"&gt;Jonathan May&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The USTC-NELSLIP Systems for Simultaneous Speech Translation Task at IWSLT 2021. (arXiv:2107.00279v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00279</id>
        <link href="http://arxiv.org/abs/2107.00279"/>
        <updated>2021-07-12T01:55:14.661Z</updated>
        <summary type="html"><![CDATA[This paper describes USTC-NELSLIP's submissions to the IWSLT2021 Simultaneous
Speech Translation task. We proposed a novel simultaneous translation model,
Cross Attention Augmented Transducer (CAAT), which extends conventional RNN-T
to sequence-to-sequence tasks without monotonic constraints, e.g., simultaneous
translation. Experiments on speech-to-text (S2T) and text-to-text (T2T)
simultaneous translation tasks shows CAAT achieves better quality-latency
trade-offs compared to \textit{wait-k}, one of the previous state-of-the-art
approaches. Based on CAAT architecture and data augmentation, we build S2T and
T2T simultaneous translation systems in this evaluation campaign. Compared to
last year's optimal systems, our S2T simultaneous translation system improves
by an average of 11.3 BLEU for all latency regimes, and our T2T simultaneous
translation system improves by an average of 4.6 BLEU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Dan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1"&gt;Mengge Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoxi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yuchen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1"&gt;Lirong Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TAN-NTM: Topic Attention Networks for Neural Topic Modeling. (arXiv:2012.01524v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01524</id>
        <link href="http://arxiv.org/abs/2012.01524"/>
        <updated>2021-07-12T01:55:14.653Z</updated>
        <summary type="html"><![CDATA[Topic models have been widely used to learn text representations and gain
insight into document corpora. To perform topic discovery, most existing neural
models either take document bag-of-words (BoW) or sequence of tokens as input
followed by variational inference and BoW reconstruction to learn topic-word
distribution. However, leveraging topic-word distribution for learning better
features during document encoding has not been explored much. To this end, we
develop a framework TAN-NTM, which processes document as a sequence of tokens
through a LSTM whose contextual outputs are attended in a topic-aware manner.
We propose a novel attention mechanism which factors in topic-word distribution
to enable the model to attend on relevant words that convey topic related cues.
The output of topic attention module is then used to carry out variational
inference. We perform extensive ablations and experiments resulting in ~9-15
percentage improvement over score of existing SOTA topic models in NPMI
coherence on several benchmark datasets - 20Newsgroups, Yelp Review Polarity
and AGNews. Further, we show that our method learns better latent
document-topic features compared to existing topic models through improvement
on two downstream tasks: document classification and topic guided keyphrase
generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Panwar_M/0/1/0/all/0/1"&gt;Madhur Panwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shailabh_S/0/1/0/all/0/1"&gt;Shashank Shailabh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_M/0/1/0/all/0/1"&gt;Milan Aggarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1"&gt;Balaji Krishnamurthy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilingual Byte2Speech Models for Scalable Low-resource Speech Synthesis. (arXiv:2103.03541v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03541</id>
        <link href="http://arxiv.org/abs/2103.03541"/>
        <updated>2021-07-12T01:55:14.639Z</updated>
        <summary type="html"><![CDATA[To scale neural speech synthesis to various real-world languages, we present
a multilingual end-to-end framework that maps byte inputs to spectrograms, thus
allowing arbitrary input scripts. Besides strong results on 40+ languages, the
framework demonstrates capabilities to adapt to new languages under extreme
low-resource and even few-shot scenarios of merely 40s transcribed recording,
without the need of per-language resources like lexicon, extra corpus,
auxiliary models, or linguistic expertise, thus ensuring scalability. While it
retains satisfactory intelligibility and naturalness matching rich-resource
models. Exhaustive comparative and ablation studies are performed to reveal the
potential of the framework for low-resource languages. Furthermore, we propose
a novel method to extract language-specific sub-networks in a multilingual
model for a better understanding of its mechanism.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1"&gt;Mutian He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jingzhou Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soong_F/0/1/0/all/0/1"&gt;Frank K. Soong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-interpretable Convolutional Neural Networks for Text Classification. (arXiv:2105.08589v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08589</id>
        <link href="http://arxiv.org/abs/2105.08589"/>
        <updated>2021-07-12T01:55:14.612Z</updated>
        <summary type="html"><![CDATA[Deep learning models for natural language processing (NLP) are inherently
complex and often viewed as black box in nature. This paper develops an
approach for interpreting convolutional neural networks for text classification
problems by exploiting the local-linear models inherent in ReLU-DNNs. The CNN
model combines the word embedding through convolutional layers, filters them
using max-pooling, and optimizes using a ReLU-DNN for classification. To get an
overall self-interpretable model, the system of local linear models from the
ReLU DNN are mapped back through the max-pool filter to the appropriate
n-grams. Our results on experimental datasets demonstrate that our proposed
technique produce parsimonious models that are self-interpretable and have
comparable performance with respect to a more complex CNN model. We also study
the impact of the complexity of the convolutional layers and the classification
layers on the model performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wei Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rahul Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_T/0/1/0/all/0/1"&gt;Tarun Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sudjianto_A/0/1/0/all/0/1"&gt;Agus Sudjianto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_V/0/1/0/all/0/1"&gt;Vijayan N. Nair&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controlling Hallucinations at Word Level in Data-to-Text Generation. (arXiv:2102.02810v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02810</id>
        <link href="http://arxiv.org/abs/2102.02810"/>
        <updated>2021-07-12T01:55:14.598Z</updated>
        <summary type="html"><![CDATA[Data-to-Text Generation (DTG) is a subfield of Natural Language Generation
aiming at transcribing structured data in natural language descriptions. The
field has been recently boosted by the use of neural-based generators which
exhibit on one side great syntactic skills without the need of hand-crafted
pipelines; on the other side, the quality of the generated text reflects the
quality of the training data, which in realistic settings only offer
imperfectly aligned structure-text pairs. Consequently, state-of-art neural
models include misleading statements - usually called hallucinations - in their
outputs. The control of this phenomenon is today a major challenge for DTG, and
is the problem addressed in the paper.

Previous work deal with this issue at the instance level: using an alignment
score for each table-reference pair. In contrast, we propose a finer-grained
approach, arguing that hallucinations should rather be treated at the word
level. Specifically, we propose a Multi-Branch Decoder which is able to
leverage word-level labels to learn the relevant parts of each training
instance. These labels are obtained following a simple and efficient scoring
procedure based on co-occurrence analysis and dependency parsing. Extensive
evaluations, via automated metrics and human judgment on the standard WikiBio
benchmark, show the accuracy of our alignment labels and the effectiveness of
the proposed Multi-Branch Decoder. Our model is able to reduce and control
hallucinations, while keeping fluency and coherence in generated texts. Further
experiments on a degraded version of ToTTo show that our model could be
successfully used on very noisy settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rebuffel_C/0/1/0/all/0/1"&gt;Cl&amp;#xe9;ment Rebuffel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberti_M/0/1/0/all/0/1"&gt;Marco Roberti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soulier_L/0/1/0/all/0/1"&gt;Laure Soulier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scoutheeten_G/0/1/0/all/0/1"&gt;Geoffrey Scoutheeten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cancelliere_R/0/1/0/all/0/1"&gt;Rossella Cancelliere&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1"&gt;Patrick Gallinari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SHAP values for Explaining CNN-based Text Classification Models. (arXiv:2008.11825v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.11825</id>
        <link href="http://arxiv.org/abs/2008.11825"/>
        <updated>2021-07-12T01:55:14.588Z</updated>
        <summary type="html"><![CDATA[Deep neural networks are increasingly used in natural language processing
(NLP) models. However, the need to interpret and explain the results from
complex algorithms are limiting their widespread adoption in regulated
industries such as banking. There has been recent work on interpretability of
machine learning algorithms with structured data. But there are only limited
techniques for NLP applications where the problem is more challenging due to
the size of the vocabulary, high-dimensional nature, and the need to consider
textual coherence and language structure. This paper develops a methodology to
compute SHAP values for local explainability of CNN-based text classification
models. The approach is also extended to compute global scores to assess the
importance of features. The results are illustrated on sentiment analysis of
Amazon Electronic Review data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wei Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_T/0/1/0/all/0/1"&gt;Tarun Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_V/0/1/0/all/0/1"&gt;Vijayan N. Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sudjianto_A/0/1/0/all/0/1"&gt;Agus Sudjianto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can Deep Neural Networks Predict Data Correlations from Column Names?. (arXiv:2107.04553v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2107.04553</id>
        <link href="http://arxiv.org/abs/2107.04553"/>
        <updated>2021-07-12T01:55:14.578Z</updated>
        <summary type="html"><![CDATA[For humans, it is often possible to predict data correlations from column
names. We conduct experiments to find out whether deep neural networks can
learn to do the same. If so, e.g., it would open up the possibility of tuning
tools that use NLP analysis on schema elements to prioritize their efforts for
correlation detection.

We analyze correlations for around 120,000 column pairs, taken from around
4,000 data sets. We try to predict correlations, based on column names alone.
For predictions, we exploit pre-trained language models, based on the recently
proposed Transformer architecture. We consider different types of correlations,
multiple prediction methods, and various prediction scenarios. We study the
impact of factors such as column name length or the amount of training data on
prediction accuracy. Altogether, we find that deep neural networks can predict
correlations with a relatively high accuracy in many scenarios (e.g., with an
accuracy of 95% for long column names).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trummer_I/0/1/0/all/0/1"&gt;Immanuel Trummer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking for Biomedical Natural Language Processing Tasks with a Domain Specific ALBERT. (arXiv:2107.04374v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04374</id>
        <link href="http://arxiv.org/abs/2107.04374"/>
        <updated>2021-07-12T01:55:14.570Z</updated>
        <summary type="html"><![CDATA[The availability of biomedical text data and advances in natural language
processing (NLP) have made new applications in biomedical NLP possible.
Language models trained or fine tuned using domain specific corpora can
outperform general models, but work to date in biomedical NLP has been limited
in terms of corpora and tasks. We present BioALBERT, a domain-specific
adaptation of A Lite Bidirectional Encoder Representations from Transformers
(ALBERT), trained on biomedical (PubMed and PubMed Central) and clinical
(MIMIC-III) corpora and fine tuned for 6 different tasks across 20 benchmark
datasets. Experiments show that BioALBERT outperforms the state of the art on
named entity recognition (+11.09% BLURB score improvement), relation extraction
(+0.80% BLURB score), sentence similarity (+1.05% BLURB score), document
classification (+0.62% F1-score), and question answering (+2.83% BLURB score).
It represents a new state of the art in 17 out of 20 benchmark datasets. By
making BioALBERT models and data available, our aim is to help the biomedical
NLP community avoid computational costs of training and establish a new set of
baselines for future efforts across a broad range of biomedical NLP tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naseem_U/0/1/0/all/0/1"&gt;Usman Naseem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dunn_A/0/1/0/all/0/1"&gt;Adam G. Dunn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khushi_M/0/1/0/all/0/1"&gt;Matloob Khushi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jinman Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Systematic Survey of Text Worlds as Embodied Natural Language Environments. (arXiv:2107.04132v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04132</id>
        <link href="http://arxiv.org/abs/2107.04132"/>
        <updated>2021-07-12T01:55:14.562Z</updated>
        <summary type="html"><![CDATA[Text Worlds are virtual environments for embodied agents that, unlike 2D or
3D environments, are rendered exclusively using textual descriptions. These
environments offer an alternative to higher-fidelity 3D environments due to
their low barrier to entry, providing the ability to study semantics,
compositional inference, and other high-level tasks with rich high-level action
spaces while controlling for perceptual input. This systematic survey outlines
recent developments in tooling, environments, and agent modeling for Text
Worlds, while examining recent trends in knowledge graphs, common sense
reasoning, transfer learning of Text World performance to higher-fidelity
environments, as well as near-term development targets that, once achieved,
make Text Worlds an attractive general research paradigm for natural language
processing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jansen_P/0/1/0/all/0/1"&gt;Peter A Jansen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Low-Resource Neural Machine Translation. (arXiv:2107.04239v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04239</id>
        <link href="http://arxiv.org/abs/2107.04239"/>
        <updated>2021-07-12T01:55:14.525Z</updated>
        <summary type="html"><![CDATA[Neural approaches have achieved state-of-the-art accuracy on machine
translation but suffer from the high cost of collecting large scale parallel
data. Thus, a lot of research has been conducted for neural machine translation
(NMT) with very limited parallel data, i.e., the low-resource setting. In this
paper, we provide a survey for low-resource NMT and classify related works into
three categories according to the auxiliary data they used: (1) exploiting
monolingual data of source and/or target languages, (2) exploiting data from
auxiliary languages, and (3) exploiting multi-modal data. We hope that our
survey can help researchers to better understand this field and inspire them to
design better algorithms, and help industry practitioners to choose appropriate
algorithms for their applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xu Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1"&gt;Renqian Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural content-aware collaborative filtering for cold-start music recommendation. (arXiv:2102.12369v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12369</id>
        <link href="http://arxiv.org/abs/2102.12369"/>
        <updated>2021-07-12T01:55:14.512Z</updated>
        <summary type="html"><![CDATA[State-of-the-art music recommender systems are based on collaborative
filtering, which builds upon learning similarities between users and songs from
the available listening data. These approaches inherently face the cold-start
problem, as they cannot recommend novel songs with no listening history.
Content-aware recommendation addresses this issue by incorporating content
information about the songs on top of collaborative filtering. However, methods
falling in this category rely on a shallow user/item interaction that
originates from a matrix factorization framework. In this work, we introduce
neural content-aware collaborative filtering, a unified framework which
alleviates these limits, and extends the recently introduced neural
collaborative filtering to its content-aware counterpart. We propose a
generative model which leverages deep learning for both extracting content
information from low-level acoustic features and for modeling the interaction
between users and songs embeddings. The deep content feature extractor can
either directly predict the item embedding, or serve as a regularization prior,
yielding two variants (strict} and relaxed) of our model. Experimental results
show that the proposed method reaches state-of-the-art results for a cold-start
music recommendation task. We notably observe that exploiting deep neural
networks for learning refined user/item interactions outperforms approaches
using a more simple interaction model in a content-aware framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Magron_P/0/1/0/all/0/1"&gt;Paul Magron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fevotte_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric F&amp;#xe9;votte&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Syntactic Dense Embedding with Correlation Graph for Automatic Readability Assessment. (arXiv:2107.04268v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04268</id>
        <link href="http://arxiv.org/abs/2107.04268"/>
        <updated>2021-07-12T01:55:14.501Z</updated>
        <summary type="html"><![CDATA[Deep learning models for automatic readability assessment generally discard
linguistic features traditionally used in machine learning models for the task.
We propose to incorporate linguistic features into neural network models by
learning syntactic dense embeddings based on linguistic features. To cope with
the relationships between the features, we form a correlation graph among
features and use it to learn their embeddings so that similar features will be
represented by similar embeddings. Experiments with six data sets of two
proficiency levels demonstrate that our proposed methodology can complement
BERT-only model to achieve significantly better performances for automatic
readability assessment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1"&gt;Xinying Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hanwu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1"&gt;Jian-Yun Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yuming Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1"&gt;Dawei Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Language Identification Through Cross-Lingual Self-Supervised Learning. (arXiv:2107.04082v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04082</id>
        <link href="http://arxiv.org/abs/2107.04082"/>
        <updated>2021-07-12T01:55:14.471Z</updated>
        <summary type="html"><![CDATA[Language identification greatly impacts the success of downstream tasks such
as automatic speech recognition. Recently, self-supervised speech
representations learned by wav2vec 2.0 have been shown to be very effective for
a range of speech tasks. We extend previous self-supervised work on language
identification by experimenting with pre-trained models which were learned on
real-world unconstrained speech in multiple languages and not just on English.
We show that models pre-trained on many languages perform better and enable
language identification systems that require very little labeled data to
perform well. Results on a 25 languages setup show that with only 10 minutes of
labeled data per language, a cross-lingually pre-trained model can achieve over
93% accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tjandra_A/0/1/0/all/0/1"&gt;Andros Tjandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhury_D/0/1/0/all/0/1"&gt;Diptanu Gon Choudhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Frank Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1"&gt;Kritika Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1"&gt;Alexei Baevski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sela_A/0/1/0/all/0/1"&gt;Assaf Sela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saraf_Y/0/1/0/all/0/1"&gt;Yatharth Saraf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1"&gt;Michael Auli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Levi Graph AMR Parser using Heterogeneous Attention. (arXiv:2107.04152v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04152</id>
        <link href="http://arxiv.org/abs/2107.04152"/>
        <updated>2021-07-12T01:55:14.460Z</updated>
        <summary type="html"><![CDATA[Coupled with biaffine decoders, transformers have been effectively adapted to
text-to-graph transduction and achieved state-of-the-art performance on AMR
parsing. Many prior works, however, rely on the biaffine decoder for either or
both arc and label predictions although most features used by the decoder may
be learned by the transformer already. This paper presents a novel approach to
AMR parsing by combining heterogeneous data (tokens, concepts, labels) as one
input to a transformer to learn attention, and use only attention matrices from
the transformer to predict all elements in AMR graphs (concepts, arcs, labels).
Although our models use significantly fewer parameters than the previous
state-of-the-art graph parser, they show similar or better accuracy on AMR 2.0
and 3.0.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Han He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jinho D. Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Models for Answer Verification in Question Answering Systems. (arXiv:2107.04217v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04217</id>
        <link href="http://arxiv.org/abs/2107.04217"/>
        <updated>2021-07-12T01:55:14.451Z</updated>
        <summary type="html"><![CDATA[This paper studies joint models for selecting correct answer sentences among
the top $k$ provided by answer sentence selection (AS2) modules, which are core
components of retrieval-based Question Answering (QA) systems. Our work shows
that a critical step to effectively exploit an answer set regards modeling the
interrelated information between pair of answers. For this purpose, we build a
three-way multi-classifier, which decides if an answer supports, refutes, or is
neutral with respect to another one. More specifically, our neural architecture
integrates a state-of-the-art AS2 model with the multi-classifier, and a joint
layer connecting all components. We tested our models on WikiQA, TREC-QA, and a
real-world dataset. The results show that our models obtain the new state of
the art in AS2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zeyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1"&gt;Thuy Vu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moschitti_A/0/1/0/all/0/1"&gt;Alessandro Moschitti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Audio-visual Attentive Fusion for Continuous Emotion Recognition. (arXiv:2107.01175v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01175</id>
        <link href="http://arxiv.org/abs/2107.01175"/>
        <updated>2021-07-12T01:55:14.429Z</updated>
        <summary type="html"><![CDATA[We propose an audio-visual spatial-temporal deep neural network with: (1) a
visual block containing a pretrained 2D-CNN followed by a temporal
convolutional network (TCN); (2) an aural block containing several parallel
TCNs; and (3) a leader-follower attentive fusion block combining the
audio-visual information. The TCN with large history coverage enables our model
to exploit spatial-temporal information within a much larger window length
(i.e., 300) than that from the baseline and state-of-the-art methods (i.e., 36
or 48). The fusion block emphasizes the visual modality while exploits the
noisy aural modality using the inter-modality attention mechanism. To make full
use of the data and alleviate over-fitting, cross-validation is carried out on
the training and validation set. The concordance correlation coefficient (CCC)
centering is used to merge the results from each fold. On the development set,
the achieved CCC is 0.469 for valence and 0.649 for arousal, which
significantly outperforms the baseline method with the corresponding CCC of
0.210 and 0.230 for valence and arousal, respectively. The code is available at
https://github.com/sucv/ABAW2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Su Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yi Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1"&gt;Ziquan Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1"&gt;Cuntai Guan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring and Improving Model-Moderator Collaboration using Uncertainty Estimation. (arXiv:2107.04212v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04212</id>
        <link href="http://arxiv.org/abs/2107.04212"/>
        <updated>2021-07-12T01:55:14.406Z</updated>
        <summary type="html"><![CDATA[Content moderation is often performed by a collaboration between humans and
machine learning models. However, it is not well understood how to design the
collaborative process so as to maximize the combined moderator-model system
performance. This work presents a rigorous study of this problem, focusing on
an approach that incorporates model uncertainty into the collaborative process.
First, we introduce principled metrics to describe the performance of the
collaborative system under capacity constraints on the human moderator,
quantifying how efficiently the combined system utilizes human decisions. Using
these metrics, we conduct a large benchmark study evaluating the performance of
state-of-the-art uncertainty models under different collaborative review
strategies. We find that an uncertainty-based strategy consistently outperforms
the widely used strategy based on toxicity scores, and moreover that the choice
of review strategy drastically changes the overall system performance. Our
results demonstrate the importance of rigorous metrics for understanding and
developing effective moderator-model systems for content moderation, as well as
the utility of uncertainty estimation in this domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kivlichan_I/0/1/0/all/0/1"&gt;Ian D. Kivlichan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jeremiah Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasserman_L/0/1/0/all/0/1"&gt;Lucy Vasserman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Machine Translation to Localize Task Oriented NLG Output. (arXiv:2107.04512v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04512</id>
        <link href="http://arxiv.org/abs/2107.04512"/>
        <updated>2021-07-12T01:55:14.390Z</updated>
        <summary type="html"><![CDATA[One of the challenges in a task oriented natural language application like
the Google Assistant, Siri, or Alexa is to localize the output to many
languages. This paper explores doing this by applying machine translation to
the English output. Using machine translation is very scalable, as it can work
with any English output and can handle dynamic text, but otherwise the problem
is a poor fit. The required quality bar is close to perfection, the range of
sentences is extremely narrow, and the sentences are often very different than
the ones in the machine translation training data. This combination of
requirements is novel in the field of domain adaptation for machine
translation. We are able to reach the required quality bar by building on
existing ideas and adding new ones: finetuning on in-domain translations,
adding sentences from the Web, adding semantic annotations, and using automatic
error detection. The paper shares our approach and results, together with a
distillation model to serve the translation models at scale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1"&gt;Scott Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brunk_C/0/1/0/all/0/1"&gt;Cliff Brunk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kyu-Young Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Justin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1"&gt;Markus Freitag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1"&gt;Mihir Kale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_G/0/1/0/all/0/1"&gt;Gagan Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mudgal_S/0/1/0/all/0/1"&gt;Sidharth Mudgal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varano_C/0/1/0/all/0/1"&gt;Chris Varano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UniRE: A Unified Label Space for Entity Relation Extraction. (arXiv:2107.04292v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04292</id>
        <link href="http://arxiv.org/abs/2107.04292"/>
        <updated>2021-07-12T01:55:14.376Z</updated>
        <summary type="html"><![CDATA[Many joint entity relation extraction models setup two separated label spaces
for the two sub-tasks (i.e., entity detection and relation classification). We
argue that this setting may hinder the information interaction between entities
and relations. In this work, we propose to eliminate the different treatment on
the two sub-tasks' label spaces. The input of our model is a table containing
all word pairs from a sentence. Entities and relations are represented by
squares and rectangles in the table. We apply a unified classifier to predict
each cell's label, which unifies the learning of two sub-tasks. For testing, an
effective (yet fast) approximate decoder is proposed for finding squares and
rectangles from tables. Experiments on three benchmarks (ACE04, ACE05, SciERC)
show that, using only half the number of parameters, our model achieves
competitive accuracy with the best extractor, and is faster.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yijun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changzhi Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yuanbin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junchi Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Positional Information for Session-based Recommendation. (arXiv:2107.00846v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00846</id>
        <link href="http://arxiv.org/abs/2107.00846"/>
        <updated>2021-07-12T01:55:14.284Z</updated>
        <summary type="html"><![CDATA[For present e-commerce platforms, session-based recommender systems are
developed to predict users' preference for next-item recommendation. Although a
session can usually reflect a user's current preference, a local shift of the
user's intention within the session may still exist. Specifically, the
interactions that take place in the early positions within a session generally
indicate the user's initial intention, while later interactions are more likely
to represent the latest intention. Such positional information has been rarely
considered in existing methods, which restricts their ability to capture the
significance of interactions at different positions. To thoroughly exploit the
positional information within a session, a theoretical framework is developed
in this paper to provide an in-depth analysis of the positional information. We
formally define the properties of forward-awareness and backward-awareness to
evaluate the ability of positional encoding schemes in capturing the initial
and the latest intention. According to our analysis, existing positional
encoding schemes are generally forward-aware only, which can hardly represent
the dynamics of the intention in a session. To enhance the positional encoding
scheme for the session-based recommendation, a dual positional encoding (DPE)
is proposed to account for both forward-awareness and backward-awareness. Based
on DPE, we propose a novel Positional Recommender (PosRec) model with a
well-designed Position-aware Gated Graph Neural Network module to fully exploit
the positional information for session-based recommendation tasks. Extensive
experiments are conducted on two e-commerce benchmark datasets, Yoochoose and
Diginetica and the experimental results show the superiority of the PosRec by
comparing it with the state-of-the-art session-based recommender models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1"&gt;Ruihong Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Hongzhi Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Cross-Session Information for Session-based Recommendation with Graph Neural Networks. (arXiv:2107.00852v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00852</id>
        <link href="http://arxiv.org/abs/2107.00852"/>
        <updated>2021-07-12T01:55:14.243Z</updated>
        <summary type="html"><![CDATA[Different from the traditional recommender system, the session-based
recommender system introduces the concept of the session, i.e., a sequence of
interactions between a user and multiple items within a period, to preserve the
user's recent interest. The existing work on the session-based recommender
system mainly relies on mining sequential patterns within individual sessions,
which are not expressive enough to capture more complicated dependency
relationships among items. In addition, it does not consider the cross-session
information due to the anonymity of the session data, where the linkage between
different sessions is prevented. In this paper, we solve these problems with
the graph neural networks technique. First, each session is represented as a
graph rather than a linear sequence structure, based on which a novel Full
Graph Neural Network (FGNN) is proposed to learn complicated item dependency.
To exploit and incorporate cross-session information in the individual
session's representation learning, we further construct a Broadly Connected
Session (BCS) graph to link different sessions and a novel Mask-Readout
function to improve session embedding based on the BCS graph. Extensive
experiments have been conducted on two e-commerce benchmark datasets, i.e.,
Yoochoose and Diginetica, and the experimental results demonstrate the
superiority of our proposal through comparisons with state-of-the-art
session-based recommender models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1"&gt;Ruihong Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jingjing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Hongzhi Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GAG: Global Attributed Graph Neural Network for Streaming Session-based Recommendation. (arXiv:2007.02747v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.02747</id>
        <link href="http://arxiv.org/abs/2007.02747"/>
        <updated>2021-07-12T01:55:14.054Z</updated>
        <summary type="html"><![CDATA[Streaming session-based recommendation (SSR) is a challenging task that
requires the recommender system to do the session-based recommendation (SR) in
the streaming scenario. In the real-world applications of e-commerce and social
media, a sequence of user-item interactions generated within a certain period
are grouped as a session, and these sessions consecutively arrive in the form
of streams. Most of the recent SR research has focused on the static setting
where the training data is first acquired and then used to train a
session-based recommender model. They need several epochs of training over the
whole dataset, which is infeasible in the streaming setting. Besides, they can
hardly well capture long-term user interests because of the neglect or the
simple usage of the user information. Although some streaming recommendation
strategies have been proposed recently, they are designed for streams of
individual interactions rather than streams of sessions. In this paper, we
propose a Global Attributed Graph (GAG) neural network model with a Wasserstein
reservoir for the SSR problem. On one hand, when a new session arrives, a
session graph with a global attribute is constructed based on the current
session and its associate user. Thus, the GAG can take both the global
attribute and the current session into consideration to learn more
comprehensive representations of the session and the user, yielding a better
performance in the recommendation. On the other hand, for the adaptation to the
streaming session scenario, a Wasserstein reservoir is proposed to help
preserve a representative sketch of the historical data. Extensive experiments
on two real-world datasets have been conducted to verify the superiority of the
GAG model compared with the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1"&gt;Ruihong Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Hongzhi Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[News Recommender System: A review of recent progress, challenges, and opportunities. (arXiv:2009.04964v4 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04964</id>
        <link href="http://arxiv.org/abs/2009.04964"/>
        <updated>2021-07-12T01:55:14.040Z</updated>
        <summary type="html"><![CDATA[Nowadays, more and more news readers tend to read news online where they have
access to millions of news articles from multiple sources. In order to help
users to find the right and relevant content, news recommender systems (NRS)
are developed to relieve the information overload problem and suggest news
items that users might be interested in. In this paper, we highlight the major
challenges faced by the news recommendation domain and identify the possible
solutions from the state-of-the-art. Due to the rapid growth of building
recommender systems using deep learning models, we divide our discussion in two
parts. In the first part, we present an overview of the conventional
recommendation solutions, datasets, evaluation criteria beyond accuracy and
recommendation platforms being used in NRS. In the second part, we explain the
deep learning-based recommendation solutions applied in NRS. Different from
previous surveys, we also study the effects of news recommendations on user
behavior and try to suggest the possible remedies to mitigate these effects. By
providing the state-of-the-art knowledge, this survey can help researchers and
practical professionals in their understanding of developments in news
recommendation algorithms. It also sheds light on potential new directions]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raza_S/0/1/0/all/0/1"&gt;Shaina Raza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1"&gt;Chen Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hacking VMAF and VMAF NEG: metrics vulnerability to different preprocessing. (arXiv:2107.04510v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.04510</id>
        <link href="http://arxiv.org/abs/2107.04510"/>
        <updated>2021-07-12T01:55:14.002Z</updated>
        <summary type="html"><![CDATA[Video quality measurement plays a critical role in the development of video
processing applications. In this paper, we show how popular quality metrics
VMAF and its tuning-resistant version VMAF NEG can be artificially increased by
video preprocessing. We propose a pipeline for tuning parameters of processing
algorithms that allows increasing VMAF by up to 218.8%. A subjective comparison
of preprocessed videos showed that with the majority of methods visual quality
drops down or stays unchanged. We show that VMAF NEG scores can also be
increased by some preprocessing methods by up to 23.6%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Siniukov_M/0/1/0/all/0/1"&gt;Maksim Siniukov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antsiferova_A/0/1/0/all/0/1"&gt;Anastasia Antsiferova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulikov_D/0/1/0/all/0/1"&gt;Dmitriy Kulikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vatolin_D/0/1/0/all/0/1"&gt;Dmitriy Vatolin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking the Item Order in Session-based Recommendation with Graph Neural Networks. (arXiv:1911.11942v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.11942</id>
        <link href="http://arxiv.org/abs/1911.11942"/>
        <updated>2021-07-12T01:55:13.979Z</updated>
        <summary type="html"><![CDATA[Predicting a user's preference in a short anonymous interaction session
instead of long-term history is a challenging problem in the real-life
session-based recommendation, e.g., e-commerce and media stream. Recent
research of the session-based recommender system mainly focuses on sequential
patterns by utilizing the attention mechanism, which is straightforward for the
session's natural sequence sorted by time. However, the user's preference is
much more complicated than a solely consecutive time pattern in the transition
of item choices. In this paper, therefore, we study the item transition pattern
by constructing a session graph and propose a novel model which collaboratively
considers the sequence order and the latent order in the session graph for a
session-based recommender system. We formulate the next item recommendation
within the session as a graph classification problem. Specifically, we propose
a weighted attention graph layer and a Readout function to learn embeddings of
items and sessions for the next item recommendation. Extensive experiments have
been conducted on two benchmark E-commerce datasets, Yoochoose and Diginetica,
and the experimental results show that our model outperforms other
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1"&gt;Ruihong Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jingjing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Hongzhi Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Crowd Sensing and Living Lab Outdoor Experimentation Made Easy. (arXiv:2107.04117v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2107.04117</id>
        <link href="http://arxiv.org/abs/2107.04117"/>
        <updated>2021-07-12T01:55:13.946Z</updated>
        <summary type="html"><![CDATA[Outdoor `living lab' experimentation using pervasive computing provides new
opportunities: higher realism, external validity and large-scale
socio-spatio-temporal observations. However, experimentation `in the wild' is
highly complex and costly. Noise, biases, privacy concerns to comply with
standards of ethical review boards, remote moderation, control of experimental
conditions and equipment perplex the collection of high-quality data for causal
inference. This article introduces Smart Agora, a novel open-source software
platform for rigorous systematic outdoor experimentation. Without writing a
single line of code, highly complex experimental scenarios are visually
designed and automatically deployed to smart phones. Novel geolocated survey
and sensor data are collected subject of participants verifying desired
experimental conditions, for instance. their presence at certain urban spots.
This new approach drastically improves the quality and purposefulness of crowd
sensing, tailored to conditions that confirm/reject hypotheses. The features
that support this innovative functionality and the broad spectrum of its
applicability are demonstrated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pournaras_E/0/1/0/all/0/1"&gt;Evangelos Pournaras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghulam_A/0/1/0/all/0/1"&gt;Atif Nabi Ghulam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kunz_R/0/1/0/all/0/1"&gt;Renato Kunz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanggli_R/0/1/0/all/0/1"&gt;Regula H&amp;#xe4;nggli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Frequentist Consistency of Variational Bayes. (arXiv:1705.03439v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1705.03439</id>
        <link href="http://arxiv.org/abs/1705.03439"/>
        <updated>2021-07-09T01:58:29.204Z</updated>
        <summary type="html"><![CDATA[A key challenge for modern Bayesian statistics is how to perform scalable
inference of posterior distributions. To address this challenge, variational
Bayes (VB) methods have emerged as a popular alternative to the classical
Markov chain Monte Carlo (MCMC) methods. VB methods tend to be faster while
achieving comparable predictive performance. However, there are few theoretical
results around VB. In this paper, we establish frequentist consistency and
asymptotic normality of VB methods. Specifically, we connect VB methods to
point estimates based on variational approximations, called frequentist
variational approximations, and we use the connection to prove a variational
Bernstein-von Mises theorem. The theorem leverages the theoretical
characterizations of frequentist variational approximations to understand
asymptotic properties of VB. In summary, we prove that (1) the VB posterior
converges to the Kullback-Leibler (KL) minimizer of a normal distribution,
centered at the truth and (2) the corresponding variational expectation of the
parameter is consistent and asymptotically normal. As applications of the
theorem, we derive asymptotic properties of VB posteriors in Bayesian mixture
models, Bayesian generalized linear mixed models, and Bayesian stochastic block
models. We conduct a simulation study to illustrate these theoretical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yixin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Blei_D/0/1/0/all/0/1"&gt;David M. Blei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the Limits of Unsupervised Domain Adaptation via Data Poisoning. (arXiv:2107.03919v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03919</id>
        <link href="http://arxiv.org/abs/2107.03919"/>
        <updated>2021-07-09T01:58:29.191Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation (UDA) enables cross-domain learning without
target domain labels by transferring knowledge from a labeled source domain
whose distribution differs from the target. However, UDA is not always
successful and several accounts of "negative transfer" have been reported in
the literature. In this work, we prove a simple lower bound on the target
domain error that complements the existing upper bound. Our bound shows the
insufficiency of minimizing source domain error and marginal distribution
mismatch for a guaranteed reduction in the target domain error, due to the
possible increase of induced labeling function mismatch. This insufficiency is
further illustrated through simple distributions for which the same UDA
approach succeeds, fails, and may succeed or fail with an equal chance.
Motivated from this, we propose novel data poisoning attacks to fool UDA
methods into learning representations that produce large target domain errors.
We evaluate the effect of these attacks on popular UDA methods using benchmark
datasets where they have been previously shown to be successful. Our results
show that poisoning can significantly decrease the target domain accuracy,
dropping it to almost 0\% in some cases, with the addition of only 10\%
poisoned data in the source domain. The failure of UDA methods demonstrates the
limitations of UDA at guaranteeing cross-domain generalization consistent with
the lower bound. Thus, evaluation of UDA methods in adversarial settings such
as data poisoning can provide a better sense of their robustness in scenarios
unfavorable for UDA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mehra_A/0/1/0/all/0/1"&gt;Akshay Mehra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1"&gt;Bhavya Kailkhura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamm_J/0/1/0/all/0/1"&gt;Jihun Hamm&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparing Supervised Models And Learned Speech Representations For Classifying Intelligibility Of Disordered Speech On Selected Phrases. (arXiv:2107.03985v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.03985</id>
        <link href="http://arxiv.org/abs/2107.03985"/>
        <updated>2021-07-09T01:58:29.177Z</updated>
        <summary type="html"><![CDATA[Automatic classification of disordered speech can provide an objective tool
for identifying the presence and severity of speech impairment. Classification
approaches can also help identify hard-to-recognize speech samples to teach ASR
systems about the variable manifestations of impaired speech. Here, we develop
and compare different deep learning techniques to classify the intelligibility
of disordered speech on selected phrases. We collected samples from a diverse
set of 661 speakers with a variety of self-reported disorders speaking 29 words
or phrases, which were rated by speech-language pathologists for their overall
intelligibility using a five-point Likert scale. We then evaluated classifiers
developed using 3 approaches: (1) a convolutional neural network (CNN) trained
for the task, (2) classifiers trained on non-semantic speech representations
from CNNs that used an unsupervised objective [1], and (3) classifiers trained
on the acoustic (encoder) embeddings from an ASR system trained on typical
speech [2]. We found that the ASR encoder's embeddings considerably outperform
the other two on detecting and classifying disordered speech. Further analysis
shows that the ASR embeddings cluster speech by the spoken phrase, while the
non-semantic embeddings cluster speech by speaker. Also, longer phrases are
more indicative of intelligibility deficits than single words.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Venugopalan_S/0/1/0/all/0/1"&gt;Subhashini Venugopalan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shor_J/0/1/0/all/0/1"&gt;Joel Shor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Plakal_M/0/1/0/all/0/1"&gt;Manoj Plakal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tobin_J/0/1/0/all/0/1"&gt;Jimmy Tobin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tomanek_K/0/1/0/all/0/1"&gt;Katrin Tomanek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Green_J/0/1/0/all/0/1"&gt;Jordan R. Green&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Brenner_M/0/1/0/all/0/1"&gt;Michael P. Brenner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Neural Scene Representations for Visuomotor Control. (arXiv:2107.04004v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.04004</id>
        <link href="http://arxiv.org/abs/2107.04004"/>
        <updated>2021-07-09T01:58:29.146Z</updated>
        <summary type="html"><![CDATA[Humans have a strong intuitive understanding of the 3D environment around us.
The mental model of the physics in our brain applies to objects of different
materials and enables us to perform a wide range of manipulation tasks that are
far beyond the reach of current robots. In this work, we desire to learn models
for dynamic 3D scenes purely from 2D visual observations. Our model combines
Neural Radiance Fields (NeRF) and time contrastive learning with an
autoencoding framework, which learns viewpoint-invariant 3D-aware scene
representations. We show that a dynamics model, constructed over the learned
representation space, enables visuomotor control for challenging manipulation
tasks involving both rigid bodies and fluids, where the target is specified in
a viewpoint different from what the robot operates on. When coupled with an
auto-decoding framework, it can even support goal specification from camera
viewpoints that are outside the training distribution. We further demonstrate
the richness of the learned 3D dynamics model by performing future prediction
and novel view synthesis. Finally, we provide detailed ablation studies
regarding different system designs and qualitative analysis of the learned
representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yunzhu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sitzmann_V/0/1/0/all/0/1"&gt;Vincent Sitzmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1"&gt;Pulkit Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1"&gt;Antonio Torralba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CamTuner: Reinforcement-Learning based System for Camera Parameter Tuning to enhance Analytics. (arXiv:2107.03964v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03964</id>
        <link href="http://arxiv.org/abs/2107.03964"/>
        <updated>2021-07-09T01:58:29.130Z</updated>
        <summary type="html"><![CDATA[Complex sensors like video cameras include tens of configurable parameters,
which can be set by end-users to customize the sensors to specific application
scenarios. Although parameter settings significantly affect the quality of the
sensor output and the accuracy of insights derived from sensor data, most
end-users use a fixed parameter setting because they lack the skill or
understanding to appropriately configure these parameters. We propose CamTuner,
which is a system to automatically, and dynamically adapt the complex sensor to
changing environments. CamTuner includes two key components. First, a bespoke
analytics quality estimator, which is a deep-learning model to automatically
and continuously estimate the quality of insights from an analytics unit as the
environment around a sensor change. Second, a reinforcement learning (RL)
module, which reacts to the changes in quality, and automatically adjusts the
camera parameters to enhance the accuracy of insights. We improve the training
time of the RL module by an order of magnitude by designing virtual models to
mimic essential behavior of the camera: we design virtual knobs that can be set
to different values to mimic the effects of assigning different values to the
camera's configurable parameters, and we design a virtual camera model that
mimics the output from a video camera at different times of the day. These
virtual models significantly accelerate training because (a) frame rates from a
real camera are limited to 25-30 fps while the virtual models enable processing
at 300 fps, (b) we do not have to wait until the real camera sees different
environments, which could take weeks or months, and (c) virtual knobs can be
updated instantly, while it can take 200-500 ms to change the camera parameter
settings. Our dynamic tuning approach results in up to 12% improvement in the
accuracy of insights from several video analytics tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1"&gt;Sibendu Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1"&gt;Kunal Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coviello_G/0/1/0/all/0/1"&gt;Giuseppe Coviello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sankaradas_M/0/1/0/all/0/1"&gt;Murugan Sankaradas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Po_O/0/1/0/all/0/1"&gt;Oliver Po&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Y. Charlie Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakradhar_S/0/1/0/all/0/1"&gt;Srimat T. Chakradhar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manifold Hypothesis in Data Analysis: Double Geometrically-Probabilistic Approach to Manifold Dimension Estimation. (arXiv:2107.03903v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03903</id>
        <link href="http://arxiv.org/abs/2107.03903"/>
        <updated>2021-07-09T01:58:29.096Z</updated>
        <summary type="html"><![CDATA[Manifold hypothesis states that data points in high-dimensional space
actually lie in close vicinity of a manifold of much lower dimension. In many
cases this hypothesis was empirically verified and used to enhance unsupervised
and semi-supervised learning. Here we present new approach to manifold
hypothesis checking and underlying manifold dimension estimation. In order to
do it we use two very different methods simultaneously - one geometric, another
probabilistic - and check whether they give the same result. Our geometrical
method is a modification for sparse data of a well-known box-counting algorithm
for Minkowski dimension calculation. The probabilistic method is new. Although
it exploits standard nearest neighborhood distance, it is different from
methods which were previously used in such situations. This method is robust,
fast and includes special preliminary data transformation. Experiments on real
datasets show that the suggested approach based on two methods combination is
powerful and effective.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ivanov_A/0/1/0/all/0/1"&gt;Alexander Ivanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nosovskiy_G/0/1/0/all/0/1"&gt;Gleb Nosovskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chekunov_A/0/1/0/all/0/1"&gt;Alexey Chekunov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fedoseev_D/0/1/0/all/0/1"&gt;Denis Fedoseev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kibkalo_V/0/1/0/all/0/1"&gt;Vladislav Kibkalo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikulin_M/0/1/0/all/0/1"&gt;Mikhail Nikulin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Popelenskiy_F/0/1/0/all/0/1"&gt;Fedor Popelenskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Komkov_S/0/1/0/all/0/1"&gt;Stepan Komkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazurenko_I/0/1/0/all/0/1"&gt;Ivan Mazurenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petiushko_A/0/1/0/all/0/1"&gt;Aleksandr Petiushko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers. (arXiv:2107.03996v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03996</id>
        <link href="http://arxiv.org/abs/2107.03996"/>
        <updated>2021-07-09T01:58:29.088Z</updated>
        <summary type="html"><![CDATA[We propose to address quadrupedal locomotion tasks using Reinforcement
Learning (RL) with a Transformer-based model that learns to combine
proprioceptive information and high-dimensional depth sensor inputs. While
learning-based locomotion has made great advances using RL, most methods still
rely on domain randomization for training blind agents that generalize to
challenging terrains. Our key insight is that proprioceptive states only offer
contact measurements for immediate reaction, whereas an agent equipped with
visual sensory observations can learn to proactively maneuver environments with
obstacles and uneven terrain by anticipating changes in the environment many
steps ahead. In this paper, we introduce LocoTransformer, an end-to-end RL
method for quadrupedal locomotion that leverages a Transformer-based model for
fusing proprioceptive states and visual observations. We evaluate our method in
challenging simulated environments with different obstacles and uneven terrain.
We show that our method obtains significant improvements over policies with
only proprioceptive state inputs, and that Transformer-based models further
improve generalization across environments. Our project page with videos is at
https://RchalYang.github.io/LocoTransformer .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1"&gt;Ruihan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Minghao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hansen_N/0/1/0/all/0/1"&gt;Nicklas Hansen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Huazhe Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaolong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Patient Embeddings in Healthcare and Insurance Applications. (arXiv:2107.03913v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2107.03913</id>
        <link href="http://arxiv.org/abs/2107.03913"/>
        <updated>2021-07-09T01:58:29.082Z</updated>
        <summary type="html"><![CDATA[The paper researches the problem of concept and patient representations in
the medical domain. We present the patient histories from Electronic Health
Records (EHRs) as temporal sequences of ICD concepts for which embeddings are
learned in an unsupervised setup with a transformer-based neural network model.
The model training was performed on the collection of one million patients'
histories in 6 years. The predictive power of such a model is assessed in
comparison with several baseline methods. A series of experiments on the
MIMIC-III data show the advantage of the presented model compared to a similar
system. Further, we analyze the obtained embedding space with regards to
concept relations and show how knowledge from the medical domain can be
successfully transferred to the practical task of insurance scoring in the form
of patient embeddings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blinov_P/0/1/0/all/0/1"&gt;Pavel Blinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kokh_V/0/1/0/all/0/1"&gt;Vladimir Kokh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RMA: Rapid Motor Adaptation for Legged Robots. (arXiv:2107.04034v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04034</id>
        <link href="http://arxiv.org/abs/2107.04034"/>
        <updated>2021-07-09T01:58:29.075Z</updated>
        <summary type="html"><![CDATA[Successful real-world deployment of legged robots would require them to adapt
in real-time to unseen scenarios like changing terrains, changing payloads,
wear and tear. This paper presents Rapid Motor Adaptation (RMA) algorithm to
solve this problem of real-time online adaptation in quadruped robots. RMA
consists of two components: a base policy and an adaptation module. The
combination of these components enables the robot to adapt to novel situations
in fractions of a second. RMA is trained completely in simulation without using
any domain knowledge like reference trajectories or predefined foot trajectory
generators and is deployed on the A1 robot without any fine-tuning. We train
RMA on a varied terrain generator using bioenergetics-inspired rewards and
deploy it on a variety of difficult terrains including rocky, slippery,
deformable surfaces in environments with grass, long vegetation, concrete,
pebbles, stairs, sand, etc. RMA shows state-of-the-art performance across
diverse real-world as well as simulation experiments. Video results at
https://ashish-kmr.github.io/rma-legged-robots/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Ashish Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1"&gt;Zipeng Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1"&gt;Deepak Pathak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1"&gt;Jitendra Malik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning for Multi-Center Imaging Diagnostics: A Study in Cardiovascular Disease. (arXiv:2107.03901v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.03901</id>
        <link href="http://arxiv.org/abs/2107.03901"/>
        <updated>2021-07-09T01:58:29.068Z</updated>
        <summary type="html"><![CDATA[Deep learning models can enable accurate and efficient disease diagnosis, but
have thus far been hampered by the data scarcity present in the medical world.
Automated diagnosis studies have been constrained by underpowered single-center
datasets, and although some results have shown promise, their generalizability
to other institutions remains questionable as the data heterogeneity between
institutions is not taken into account. By allowing models to be trained in a
distributed manner that preserves patients' privacy, federated learning
promises to alleviate these issues, by enabling diligent multi-center studies.
We present the first federated learning study on the modality of cardiovascular
magnetic resonance (CMR) and use four centers derived from subsets of the M\&M
and ACDC datasets, focusing on the diagnosis of hypertrophic cardiomyopathy
(HCM). We adapt a 3D-CNN network pretrained on action recognition and explore
two different ways of incorporating shape prior information to the model, and
four different data augmentation set-ups, systematically analyzing their impact
on the different collaborative learning choices. We show that despite the small
size of data (180 subjects derived from four centers), the privacy preserving
federated learning achieves promising results that are competitive with
traditional centralized learning. We further find that federatively trained
models exhibit increased robustness and are more sensitive to domain shift
effects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Linardos_A/0/1/0/all/0/1"&gt;Akis Linardos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kushibar_K/0/1/0/all/0/1"&gt;Kaisar Kushibar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Walsh_S/0/1/0/all/0/1"&gt;Sean Walsh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gkontra_P/0/1/0/all/0/1"&gt;Polyxeni Gkontra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1"&gt;Karim Lekadir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable AI (XAI) for PHM of Industrial Asset: A State-of-The-Art, PRISMA-Compliant Systematic Review. (arXiv:2107.03869v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.03869</id>
        <link href="http://arxiv.org/abs/2107.03869"/>
        <updated>2021-07-09T01:58:29.049Z</updated>
        <summary type="html"><![CDATA[A state-of-the-art systematic review on XAI applied to Prognostic and Health
Management (PHM) of industrial asset is presented. The work attempts to provide
an overview of the general trend of XAI in PHM, answers the question of
accuracy versus explainability, investigates the extent of human role,
explainability evaluation and uncertainty management in PHM XAI. Research
articles linked to PHM XAI, in English language, from 2015 to 2021 are selected
from IEEE Xplore, ScienceDirect, SpringerLink, ACM Digital Library and Scopus
databases using PRISMA guidelines. Data was extracted from 35 selected articles
and examined using MS. Excel. Several findings were synthesized. Firstly, while
the discipline is still young, the analysis indicates the growing acceptance of
XAI in PHM domain. Secondly, XAI functions as a double edge sword, where it is
assimilated as a tool to execute PHM tasks as well as a mean of explanation, in
particular in diagnostic and anomaly detection. There is thus a need for XAI in
PHM. Thirdly, the review shows that PHM XAI papers produce either good or
excellent results in general, suggesting that PHM performance is unaffected by
XAI. Fourthly, human role, explainability metrics and uncertainty management
are areas requiring further attention by the PHM community. Adequate
explainability metrics to cater for PHM need are urgently needed. Finally, most
case study featured on the accepted articles are based on real, indicating that
available AI and XAI approaches are equipped to solve complex real-world
challenges, increasing the confidence of AI model adoption in the industry.
This work is funded by the Universiti Teknologi Petronas Foundation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+NOR_A/0/1/0/all/0/1"&gt;Ahmad Kamal BIN MOHD NOR&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+PEDAPATI_S/0/1/0/all/0/1"&gt;Srinivasa Rao PEDAPATI&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+MUHAMMAD_M/0/1/0/all/0/1"&gt;Masdi MUHAMMAD&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessing putative bias in prediction of anti-microbial resistance from real-world genotyping data under explicit causal assumptions. (arXiv:2107.03383v1 [q-bio.GN])]]></title>
        <id>http://arxiv.org/abs/2107.03383</id>
        <link href="http://arxiv.org/abs/2107.03383"/>
        <updated>2021-07-09T01:58:29.038Z</updated>
        <summary type="html"><![CDATA[Whole genome sequencing (WGS) is quickly becoming the customary means for
identification of antimicrobial resistance (AMR) due to its ability to obtain
high resolution information about the genes and mechanisms that are causing
resistance and driving pathogen mobility. By contrast, traditional phenotypic
(antibiogram) testing cannot easily elucidate such information. Yet development
of AMR prediction tools from genotype-phenotype data can be biased, since
sampling is non-randomized. Sample provenience, period of collection, and
species representation can confound the association of genetic traits with AMR.
Thus, prediction models can perform poorly on new data with sampling
distribution shifts. In this work -- under an explicit set of causal
assumptions -- we evaluate the effectiveness of propensity-based rebalancing
and confounding adjustment on AMR prediction using genotype-phenotype AMR data
from the Pathosystems Resource Integration Center (PATRIC). We select bacterial
genotypes (encoded as k-mer signatures, i.e. DNA fragments of length k),
country, year, species, and AMR phenotypes for the tetracycline drug class,
preparing test data with recent genomes coming from a single country. We test
boosted logistic regression (BLR) and random forests (RF) with/without
bias-handling. On 10,936 instances, we find evidence of species, location and
year imbalance with respect to the AMR phenotype. The crude versus
bias-adjusted change in effect of genetic signatures on AMR varies but only
moderately (selecting the top 20,000 out of 40+ million k-mers). The area under
the receiver operating characteristic (AUROC) of the RF (0.95) is comparable to
that of BLR (0.94) on both out-of-bag samples from bootstrap and the external
test (n=1,085), where AUROCs do not decrease. We observe a 1%-5% gain in AUROC
with bias-handling compared to the sole use of genetic signatures. ...]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Prosperi_M/0/1/0/all/0/1"&gt;Mattia Prosperi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Marini_S/0/1/0/all/0/1"&gt;Simone Marini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Boucher_C/0/1/0/all/0/1"&gt;Christina Boucher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Bian_J/0/1/0/all/0/1"&gt;Jiang Bian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalization Error of GAN from the Discriminator's Perspective. (arXiv:2107.03633v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03633</id>
        <link href="http://arxiv.org/abs/2107.03633"/>
        <updated>2021-07-09T01:58:29.031Z</updated>
        <summary type="html"><![CDATA[The generative adversarial network (GAN) is a well-known model for learning
high-dimensional distributions, but the mechanism for its generalization
ability is not understood. In particular, GAN is vulnerable to the memorization
phenomenon, the eventual convergence to the empirical distribution. We consider
a simplified GAN model with the generator replaced by a density, and analyze
how the discriminator contributes to generalization. We show that with early
stopping, the generalization error measured by Wasserstein metric escapes from
the curse of dimensionality, despite that in the long term, memorization is
inevitable. In addition, we present a hardness of learning result for WGAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongkang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+E_W/0/1/0/all/0/1"&gt;Weinan E&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consistency of the Maximal Information Coefficient Estimator. (arXiv:2107.03836v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2107.03836</id>
        <link href="http://arxiv.org/abs/2107.03836"/>
        <updated>2021-07-09T01:58:29.024Z</updated>
        <summary type="html"><![CDATA[The Maximal Information Coefficient (MIC) of Reshef et al. (Science, 2011) is
a statistic for measuring dependence between variable pairs in large datasets.
In this note, we prove that MIC is a consistent estimator of the corresponding
population statistic MIC$_*$. This corrects an error in an argument of Reshef
et al. (JMLR, 2016), which we describe.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lazarsfeld_J/0/1/0/all/0/1"&gt;John Lazarsfeld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Johnson_A/0/1/0/all/0/1"&gt;Aaron Johnson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analytically Tractable Hidden-States Inference in Bayesian Neural Networks. (arXiv:2107.03759v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03759</id>
        <link href="http://arxiv.org/abs/2107.03759"/>
        <updated>2021-07-09T01:58:28.984Z</updated>
        <summary type="html"><![CDATA[With few exceptions, neural networks have been relying on backpropagation and
gradient descent as the inference engine in order to learn the model
parameters, because the closed-form Bayesian inference for neural networks has
been considered to be intractable. In this paper, we show how we can leverage
the tractable approximate Gaussian inference's (TAGI) capabilities to infer
hidden states, rather than only using it for inferring the network's
parameters. One novel aspect it allows is to infer hidden states through the
imposition of constraints designed to achieve specific objectives, as
illustrated through three examples: (1) the generation of adversarial-attack
examples, (2) the usage of a neural network as a black-box optimization method,
and (3) the application of inference on continuous-action reinforcement
learning. These applications showcase how tasks that were previously reserved
to gradient-based optimization approaches can now be approached with
analytically tractable inference]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1"&gt;Luong-Ha Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goulet_J/0/1/0/all/0/1"&gt;James-A. Goulet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Safety Envelopes using Light Curtains with Probabilistic Guarantees. (arXiv:2107.04000v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04000</id>
        <link href="http://arxiv.org/abs/2107.04000"/>
        <updated>2021-07-09T01:58:28.964Z</updated>
        <summary type="html"><![CDATA[To safely navigate unknown environments, robots must accurately perceive
dynamic obstacles. Instead of directly measuring the scene depth with a LiDAR
sensor, we explore the use of a much cheaper and higher resolution sensor:
programmable light curtains. Light curtains are controllable depth sensors that
sense only along a surface that a user selects. We use light curtains to
estimate the safety envelope of a scene: a hypothetical surface that separates
the robot from all obstacles. We show that generating light curtains that sense
random locations (from a particular distribution) can quickly discover the
safety envelope for scenes with unknown objects. Importantly, we produce
theoretical safety guarantees on the probability of detecting an obstacle using
random curtains. We combine random curtains with a machine learning based model
that forecasts and tracks the motion of the safety envelope efficiently. Our
method accurately estimates safety envelopes while providing probabilistic
safety guarantees that can be used to certify the efficacy of a robot
perception system to detect and avoid dynamic obstacles. We evaluate our
approach in a simulated urban driving environment and a real-world environment
with moving pedestrians using a light curtain device and show that we can
estimate safety envelopes efficiently and effectively. Project website:
https://siddancha.github.io/projects/active-safety-envelopes-with-guarantees]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ancha_S/0/1/0/all/0/1"&gt;Siddharth Ancha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pathak_G/0/1/0/all/0/1"&gt;Gaurav Pathak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narasimhan_S/0/1/0/all/0/1"&gt;Srinivasa G. Narasimhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1"&gt;David Held&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Short-term Renewable Energy Forecasting in Greece using Prophet Decomposition and Tree-based Ensembles. (arXiv:2107.03825v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03825</id>
        <link href="http://arxiv.org/abs/2107.03825"/>
        <updated>2021-07-09T01:58:28.957Z</updated>
        <summary type="html"><![CDATA[Energy production using renewable sources exhibits inherent uncertainties due
to their intermittent nature. Nevertheless, the unified European energy market
promotes the increasing penetration of renewable energy sources (RES) by the
regional energy system operators. Consequently, RES forecasting can assist in
the integration of these volatile energy sources, since it leads to higher
reliability and reduced ancillary operational costs for power systems. This
paper presents a new dataset for solar and wind energy generation forecast in
Greece and introduces a feature engineering pipeline that enriches the
dimensional space of the dataset. In addition, we propose a novel method that
utilizes the innovative Prophet model, an end-to-end forecasting tool that
considers several kinds of nonlinear trends in decomposing the energy time
series before a tree-based ensemble provides short-term predictions. The
performance of the system is measured through representative evaluation
metrics, and by estimating the model's generalization under an industryprovided
scheme of absolute error thresholds. The proposed hybrid model competes with
baseline persistence models, tree-based regression ensembles, and the Prophet
model, managing to outperform them, presenting both lower error rates and more
favorable error distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vartholomaios_A/0/1/0/all/0/1"&gt;Argyrios Vartholomaios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlos_S/0/1/0/all/0/1"&gt;Stamatis Karlos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kouloumpris_E/0/1/0/all/0/1"&gt;Eleftherios Kouloumpris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1"&gt;Grigorios Tsoumakas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge Transfer by Discriminative Pre-training for Academic Performance Prediction. (arXiv:2107.04009v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2107.04009</id>
        <link href="http://arxiv.org/abs/2107.04009"/>
        <updated>2021-07-09T01:58:28.939Z</updated>
        <summary type="html"><![CDATA[The needs for precisely estimating a student's academic performance have been
emphasized with an increasing amount of attention paid to Intelligent Tutoring
System (ITS). However, since labels for academic performance, such as test
scores, are collected from outside of ITS, obtaining the labels is costly,
leading to label-scarcity problem which brings challenge in taking machine
learning approaches for academic performance prediction. To this end, inspired
by the recent advancement of pre-training method in natural language processing
community, we propose DPA, a transfer learning framework with Discriminative
Pre-training tasks for Academic performance prediction. DPA pre-trains two
models, a generator and a discriminator, and fine-tunes the discriminator on
academic performance prediction. In DPA's pre-training phase, a sequence of
interactions where some tokens are masked is provided to the generator which is
trained to reconstruct the original sequence. Then, the discriminator takes an
interaction sequence where the masked tokens are replaced by the generator's
outputs, and is trained to predict the originalities of all tokens in the
sequence. Compared to the previous state-of-the-art generative pre-training
method, DPA is more sample efficient, leading to fast convergence to lower
academic performance prediction error. We conduct extensive experimental
studies on a real-world dataset obtained from a multi-platform ITS application
and show that DPA outperforms the previous state-of-the-art generative
pre-training method with a reduction of 4.05% in mean absolute error and more
robust to increased label-scarcity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1"&gt;Byungsoo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hangyeol Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1"&gt;Dongmin Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Youngduck Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bootstrapping Generalization of Process Models Discovered From Event Data. (arXiv:2107.03876v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.03876</id>
        <link href="http://arxiv.org/abs/2107.03876"/>
        <updated>2021-07-09T01:58:28.917Z</updated>
        <summary type="html"><![CDATA[Process mining studies ways to derive value from process executions recorded
in event logs of IT-systems, with process discovery the task of inferring a
process model for an event log emitted by some unknown system. One quality
criterion for discovered process models is generalization. Generalization seeks
to quantify how well the discovered model describes future executions of the
system, and is perhaps the least understood quality criterion in process
mining. The lack of understanding is primarily a consequence of generalization
seeking to measure properties over the entire future behavior of the system,
when the only available sample of behavior is that provided by the event log
itself. In this paper, we draw inspiration from computational statistics, and
employ a bootstrap approach to estimate properties of a population based on a
sample. Specifically, we define an estimator of the model's generalization
based on the event log it was discovered from, and then use bootstrapping to
measure the generalization of the model with respect to the system, and its
statistical significance. Experiments demonstrate the feasibility of the
approach in industrial settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Polyvyanyy_A/0/1/0/all/0/1"&gt;Artem Polyvyanyy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moffat_A/0/1/0/all/0/1"&gt;Alistair Moffat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_Banuelos_L/0/1/0/all/0/1"&gt;Luciano Garc&amp;#xed;a-Ba&amp;#xf1;uelos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Price of Diversity. (arXiv:2107.03900v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2107.03900</id>
        <link href="http://arxiv.org/abs/2107.03900"/>
        <updated>2021-07-09T01:58:28.910Z</updated>
        <summary type="html"><![CDATA[Systemic bias with respect to gender, race and ethnicity, often unconscious,
is prevalent in datasets involving choices among individuals. Consequently,
society has found it challenging to alleviate bias and achieve diversity in a
way that maintains meritocracy in such settings. We propose (a) a novel
optimization approach based on optimally flipping outcome labels and training
classification models simultaneously to discover changes to be made in the
selection process so as to achieve diversity without significantly affecting
meritocracy, and (b) a novel implementation tool employing optimal
classification trees to provide insights on which attributes of individuals
lead to flipping of their labels, and to help make changes in the current
selection processes in a manner understandable by human decision makers. We
present case studies on three real-world datasets consisting of parole,
admissions to the bar and lending decisions, and demonstrate that the price of
diversity is low and sometimes negative, that is we can modify our selection
processes in a way that enhances diversity without affecting meritocracy
significantly, and sometimes improving it.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bandi_H/0/1/0/all/0/1"&gt;Hari Bandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bertsimas_D/0/1/0/all/0/1"&gt;Dimitris Bertsimas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Locally differentially private estimation of nonlinear functionals of discrete distributions. (arXiv:2107.03940v1 [math.ST])]]></title>
        <id>http://arxiv.org/abs/2107.03940</id>
        <link href="http://arxiv.org/abs/2107.03940"/>
        <updated>2021-07-09T01:58:28.891Z</updated>
        <summary type="html"><![CDATA[We study the problem of estimating non-linear functionals of discrete
distributions in the context of local differential privacy. The initial data
$x_1,\ldots,x_n \in [K]$ are supposed i.i.d. and distributed according to an
unknown discrete distribution $p = (p_1,\ldots,p_K)$. Only $\alpha$-locally
differentially private (LDP) samples $z_1,...,z_n$ are publicly available,
where the term 'local' means that each $z_i$ is produced using one individual
attribute $x_i$. We exhibit privacy mechanisms (PM) that are interactive (i.e.
they are allowed to use already published confidential data) or
non-interactive. We describe the behavior of the quadratic risk for estimating
the power sum functional $F_{\gamma} = \sum_{k=1}^K p_k^{\gamma}$, $\gamma >0$
as a function of $K, \, n$ and $\alpha$. In the non-interactive case, we study
two plug-in type estimators of $F_{\gamma}$, for all $\gamma >0$, that are
similar to the MLE analyzed by Jiao et al. (2017) in the multinomial model.
However, due to the privacy constraint the rates we attain are slower and
similar to those obtained in the Gaussian model by Collier et al. (2020). In
the interactive case, we introduce for all $\gamma >1$ a two-step procedure
which attains the faster parametric rate $(n \alpha^2)^{-1/2}$ when $\gamma
\geq 2$. We give lower bounds results over all $\alpha$-LDP mechanisms and all
estimators using the private samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Butucea_C/0/1/0/all/0/1"&gt;Cristina Butucea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Issartel_Y/0/1/0/all/0/1"&gt;Yann Issartel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring Financial Time Series Similarity With a View to Identifying Profitable Stock Market Opportunities. (arXiv:2107.03926v1 [q-fin.ST])]]></title>
        <id>http://arxiv.org/abs/2107.03926</id>
        <link href="http://arxiv.org/abs/2107.03926"/>
        <updated>2021-07-09T01:58:28.883Z</updated>
        <summary type="html"><![CDATA[Forecasting stock returns is a challenging problem due to the highly
stochastic nature of the market and the vast array of factors and events that
can influence trading volume and prices. Nevertheless it has proven to be an
attractive target for machine learning research because of the potential for
even modest levels of prediction accuracy to deliver significant benefits. In
this paper, we describe a case-based reasoning approach to predicting stock
market returns using only historical pricing data. We argue that one of the
impediments for case-based stock prediction has been the lack of a suitable
similarity metric when it comes to identifying similar pricing histories as the
basis for a future prediction -- traditional Euclidean and correlation based
approaches are not effective for a variety of reasons -- and in this regard, a
key contribution of this work is the development of a novel similarity metric
for comparing historical pricing data. We demonstrate the benefits of this
metric and the case-based approach in a real-world application in comparison to
a variety of conventional benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Dolphin_R/0/1/0/all/0/1"&gt;Rian Dolphin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Smyth_B/0/1/0/all/0/1"&gt;Barry Smyth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Dong_R/0/1/0/all/0/1"&gt;Ruihai Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Data Processing in Space for Object Detection in Satellite Imagery. (arXiv:2107.03774v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03774</id>
        <link href="http://arxiv.org/abs/2107.03774"/>
        <updated>2021-07-09T01:58:28.875Z</updated>
        <summary type="html"><![CDATA[There is a proliferation in the number of satellites launched each year,
resulting in downlinking of terabytes of data each day. The data received by
ground stations is often unprocessed, making this an expensive process
considering the large data sizes and that not all of the data is useful. This,
coupled with the increasing demand for real-time data processing, has led to a
growing need for on-orbit processing solutions. In this work, we investigate
the performance of CNN-based object detectors on constrained devices by
applying different image compression techniques to satellite data. We examine
the capabilities of the NVIDIA Jetson Nano and NVIDIA Jetson AGX Xavier;
low-power, high-performance computers, with integrated GPUs, small enough to
fit on-board a nanosatellite. We take a closer look at object detection
networks, including the Single Shot MultiBox Detector (SSD) and Region-based
Fully Convolutional Network (R-FCN) models that are pre-trained on DOTA - a
Large Scale Dataset for Object Detection in Aerial Images. The performance is
measured in terms of execution time, memory consumption, and accuracy, and are
compared against a baseline containing a server with two powerful GPUs. The
results show that by applying image compression techniques, we are able to
improve the execution time and memory consumption, achieving a fully runnable
dataset. A lossless compression technique achieves roughly a 10% reduction in
execution time and about a 3% reduction in memory consumption, with no impact
on the accuracy. While a lossy compression technique improves the execution
time by up to 144% and the memory consumption is reduced by as much as 97%.
However, it has a significant impact on accuracy, varying depending on the
compression ratio. Thus the application and ratio of these compression
techniques may differ depending on the required level of accuracy for a
particular task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lofqvist_M/0/1/0/all/0/1"&gt;Martina Lofqvist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cano_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Cano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Modality Task Cascade for 3D Object Detection. (arXiv:2107.04013v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04013</id>
        <link href="http://arxiv.org/abs/2107.04013"/>
        <updated>2021-07-09T01:58:28.868Z</updated>
        <summary type="html"><![CDATA[Point clouds and RGB images are naturally complementary modalities for 3D
visual understanding - the former provides sparse but accurate locations of
points on objects, while the latter contains dense color and texture
information. Despite this potential for close sensor fusion, many methods train
two models in isolation and use simple feature concatenation to represent 3D
sensor data. This separated training scheme results in potentially sub-optimal
performance and prevents 3D tasks from being used to benefit 2D tasks that are
often useful on their own. To provide a more integrated approach, we propose a
novel Multi-Modality Task Cascade network (MTC-RCNN) that leverages 3D box
proposals to improve 2D segmentation predictions, which are then used to
further refine the 3D boxes. We show that including a 2D network between two
stages of 3D modules significantly improves both 2D and 3D task performance.
Moreover, to prevent the 3D module from over-relying on the overfitted 2D
predictions, we propose a dual-head 2D segmentation training and inference
scheme, allowing the 2nd 3D module to learn to interpret imperfect 2D
segmentation predictions. Evaluating our model on the challenging SUN RGB-D
dataset, we improve upon state-of-the-art results of both single modality and
fusion networks by a large margin ($\textbf{+3.8}$ mAP@0.5). Code will be
released $\href{https://github.com/Divadi/MTC_RCNN}{\text{here.}}$]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jinhyung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weng_X/0/1/0/all/0/1"&gt;Xinshuo Weng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Man_Y/0/1/0/all/0/1"&gt;Yunze Man&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1"&gt;Kris Kitani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Margins and Derandomisation in PAC-Bayes. (arXiv:2107.03955v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03955</id>
        <link href="http://arxiv.org/abs/2107.03955"/>
        <updated>2021-07-09T01:58:28.861Z</updated>
        <summary type="html"><![CDATA[We develop a framework for derandomising PAC-Bayesian generalisation bounds
achieving a margin on training data, relating this process to the
concentration-of-measure phenomenon. We apply these tools to linear prediction,
single-hidden-layer neural networks with an unusual erf activation function,
and deep ReLU networks, obtaining new bounds. The approach is also extended to
the idea of "partial-derandomisation" where only some layers are derandomised
and the others are stochastic. This allows empirical evaluation of
single-hidden-layer networks on more complex datasets, and helps bridge the gap
between generalisation bounds for non-stochastic deep networks and those for
randomised deep networks as generally examined in PAC-Bayes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biggs_F/0/1/0/all/0/1"&gt;Felix Biggs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guedj_B/0/1/0/all/0/1"&gt;Benjamin Guedj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Augmented Data as an Auxiliary Plug-in Towards Categorization of Crowdsourced Heritage Data. (arXiv:2107.03852v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03852</id>
        <link href="http://arxiv.org/abs/2107.03852"/>
        <updated>2021-07-09T01:58:28.841Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a strategy to mitigate the problem of inefficient
clustering performance by introducing data augmentation as an auxiliary
plug-in. Classical clustering techniques such as K-means, Gaussian mixture
model and spectral clustering are central to many data-driven applications.
However, recently unsupervised simultaneous feature learning and clustering
using neural networks also known as Deep Embedded Clustering (DEC) has gained
prominence. Pioneering works on deep feature clustering focus on defining
relevant clustering loss function and choosing the right neural network for
extracting features. A central problem in all these cases is data sparsity
accompanied by high intra-class and low inter-class variance, which
subsequently leads to poor clustering performance and erroneous candidate
assignments. Towards this, we employ data augmentation techniques to improve
the density of the clusters, thus improving the overall performance. We train a
variant of Convolutional Autoencoder (CAE) with augmented data to construct the
initial feature space as a novel model for deep clustering. We demonstrate the
results of proposed strategy on crowdsourced Indian Heritage dataset. Extensive
experiments show consistent improvements over existing works.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kudari_S/0/1/0/all/0/1"&gt;Shashidhar Veerappa Kudari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunari_A/0/1/0/all/0/1"&gt;Akshaykumar Gunari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jamadandi_A/0/1/0/all/0/1"&gt;Adarsh Jamadandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tabib_R/0/1/0/all/0/1"&gt;Ramesh Ashok Tabib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mudenagudi_U/0/1/0/all/0/1"&gt;Uma Mudenagudi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation. (arXiv:2107.03502v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03502</id>
        <link href="http://arxiv.org/abs/2107.03502"/>
        <updated>2021-07-09T01:58:28.805Z</updated>
        <summary type="html"><![CDATA[The imputation of missing values in time series has many applications in
healthcare and finance. While autoregressive models are natural candidates for
time series imputation, score-based diffusion models have recently outperformed
existing counterparts including autoregressive models in many tasks such as
image generation and audio synthesis, and would be promising for time series
imputation. In this paper, we propose Conditional Score-based Diffusion models
for Imputation (CSDI), a novel time series imputation method that utilizes
score-based diffusion models conditioned on observed data. Unlike existing
score-based approaches, the conditional diffusion model is explicitly trained
for imputation and can exploit correlations between observed values. On
healthcare and environmental data, CSDI improves by 40-70% over existing
probabilistic imputation methods on popular performance metrics. In addition,
deterministic imputation by CSDI reduces the error by 5-20% compared to the
state-of-the-art deterministic imputation methods. Furthermore, CSDI can also
be applied to time series interpolation and probabilistic forecasting, and is
competitive with existing baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tashiro_Y/0/1/0/all/0/1"&gt;Yusuke Tashiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jiaming Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1"&gt;Stefano Ermon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[$S^3$: Sign-Sparse-Shift Reparametrization for Effective Training of Low-bit Shift Networks. (arXiv:2107.03453v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03453</id>
        <link href="http://arxiv.org/abs/2107.03453"/>
        <updated>2021-07-09T01:58:28.783Z</updated>
        <summary type="html"><![CDATA[Shift neural networks reduce computation complexity by removing expensive
multiplication operations and quantizing continuous weights into low-bit
discrete values, which are fast and energy efficient compared to conventional
neural networks. However, existing shift networks are sensitive to the weight
initialization, and also yield a degraded performance caused by vanishing
gradient and weight sign freezing problem. To address these issues, we propose
S low-bit re-parameterization, a novel technique for training low-bit shift
networks. Our method decomposes a discrete parameter in a sign-sparse-shift
3-fold manner. In this way, it efficiently learns a low-bit network with a
weight dynamics similar to full-precision networks and insensitive to weight
initialization. Our proposed training method pushes the boundaries of shift
neural networks and shows 3-bit shift networks out-performs their
full-precision counterparts in terms of top-1 accuracy on ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xinlin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yaoliang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wulong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chunjing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nia_V/0/1/0/all/0/1"&gt;Vahid Partovi Nia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral decoupling allows training transferable neural networks in medical imaging. (arXiv:2103.17171v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.17171</id>
        <link href="http://arxiv.org/abs/2103.17171"/>
        <updated>2021-07-09T01:58:28.572Z</updated>
        <summary type="html"><![CDATA[Many current neural networks for medical imaging generalise poorly to data
unseen during training. Such behaviour can be caused by networks overfitting
easy-to-learn, or statistically dominant, features while disregarding other
potentially informative features. For example, indistinguishable differences in
the sharpness of the images from two different scanners can degrade the
performance of the network significantly. All neural networks intended for
clinical practice need to be robust to variation in data caused by differences
in imaging equipment, sample preparation and patient populations.

To address these challenges, we evaluate the utility of spectral decoupling
as an implicit bias mitigation method. Spectral decoupling encourages the
neural network to learn more features by simply regularising the networks'
unnormalised prediction scores with an L2 penalty, thus having no added
computational costs.

We show that spectral decoupling allows training neural networks on datasets
with strong spurious correlations. Networks trained without spectral decoupling
do not learn the original task and appear to make false predictions based on
the spurious correlations. Spectral decoupling also increases networks'
robustness for data distribution shifts. To validate our findings, we train
networks with and without spectral decoupling to detect prostate cancer tissue
slides and COVID-19 in chest radiographs. Networks trained with spectral
decoupling achieve substantially higher performance on all evaluation datasets.

Our results show that spectral decoupling helps with generalisation issues
associated with neural networks. We recommend using spectral decoupling as an
implicit bias mitigation method in any neural network intended for clinical
use.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Pohjonen_J/0/1/0/all/0/1"&gt;Joona Pohjonen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sturenberg_C/0/1/0/all/0/1"&gt;Carolin St&amp;#xfc;renberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rannikko_A/0/1/0/all/0/1"&gt;Antti Rannikko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mirtti_T/0/1/0/all/0/1"&gt;Tuomas Mirtti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pitkanen_E/0/1/0/all/0/1"&gt;Esa Pitk&amp;#xe4;nen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID19-HPSMP: COVID-19 Adopted Hybrid and Parallel Deep Information Fusion Framework for Stock Price Movement Prediction. (arXiv:2101.02287v2 [q-fin.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02287</id>
        <link href="http://arxiv.org/abs/2101.02287"/>
        <updated>2021-07-09T01:58:28.564Z</updated>
        <summary type="html"><![CDATA[The novel of coronavirus (COVID-19) has suddenly and abruptly changed the
world as we knew at the start of the 3rd decade of the 21st century.
Particularly, COVID-19 pandemic has negatively affected financial econometrics
and stock markets across the globe. Artificial Intelligence (AI) and Machine
Learning (ML)-based prediction models, especially Deep Neural Network (DNN)
architectures, have the potential to act as a key enabling factor to reduce the
adverse effects of the COVID-19 pandemic and future possible ones on financial
markets. In this regard, first, a unique COVID-19 related PRIce MOvement
prediction (COVID19 PRIMO) dataset is introduced in this paper, which
incorporates effects of social media trends related to COVID-19 on stock market
price movements. Afterwards, a novel hybrid and parallel DNN-based framework is
proposed that integrates different and diversified learning architectures.
Referred to as the COVID-19 adopted Hybrid and Parallel deep fusion framework
for Stock price Movement Prediction (COVID19-HPSMP), innovative fusion
strategies are used to combine scattered social media news related to COVID-19
with historical mark data. The proposed COVID19-HPSMP consists of two parallel
paths (hence hybrid), one based on Convolutional Neural Network (CNN) with
Local/Global Attention modules, and one integrated CNN and Bi-directional Long
Short term Memory (BLSTM) path. The two parallel paths are followed by a
multilayer fusion layer acting as a fusion centre that combines localized
features. Performance evaluations are performed based on the introduced COVID19
PRIMO dataset illustrating superior performance of the proposed framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Ronaghi_F/0/1/0/all/0/1"&gt;Farnoush Ronaghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Salimibeni_M/0/1/0/all/0/1"&gt;Mohammad Salimibeni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Naderkhani_F/0/1/0/all/0/1"&gt;Farnoosh Naderkhani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Mohammadi_A/0/1/0/all/0/1"&gt;Arash Mohammadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fourier Sparse Leverage Scores and Approximate Kernel Learning. (arXiv:2006.07340v3 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07340</id>
        <link href="http://arxiv.org/abs/2006.07340"/>
        <updated>2021-07-09T01:58:28.546Z</updated>
        <summary type="html"><![CDATA[We prove new explicit upper bounds on the leverage scores of Fourier sparse
functions under both the Gaussian and Laplace measures. In particular, we study
$s$-sparse functions of the form $f(x) = \sum_{j=1}^s a_j e^{i \lambda_j x}$
for coefficients $a_j \in \mathbb{C}$ and frequencies $\lambda_j \in
\mathbb{R}$. Bounding Fourier sparse leverage scores under various measures is
of pure mathematical interest in approximation theory, and our work extends
existing results for the uniform measure [Erd17,CP19a]. Practically, our bounds
are motivated by two important applications in machine learning:

1. Kernel Approximation. They yield a new random Fourier features algorithm
for approximating Gaussian and Cauchy (rational quadratic) kernel matrices. For
low-dimensional data, our method uses a near optimal number of features, and
its runtime is polynomial in the $statistical\ dimension$ of the approximated
kernel matrix. It is the first "oblivious sketching method" with this property
for any kernel besides the polynomial kernel, resolving an open question of
[AKM+17,AKK+20b].

2. Active Learning. They can be used as non-uniform sampling distributions
for robust active learning when data follows a Gaussian or Laplace
distribution. Using the framework of [AKM+19], we provide essentially optimal
results for bandlimited and multiband interpolation, and Gaussian process
regression. These results generalize existing work that only applies to
uniformly distributed data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Erdelyi_T/0/1/0/all/0/1"&gt;Tam&amp;#xe1;s Erd&amp;#xe9;lyi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1"&gt;Cameron Musco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1"&gt;Christopher Musco&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The value of text for small business default prediction: A deep learning approach. (arXiv:2003.08964v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.08964</id>
        <link href="http://arxiv.org/abs/2003.08964"/>
        <updated>2021-07-09T01:58:28.540Z</updated>
        <summary type="html"><![CDATA[Compared to consumer lending, Micro, Small and Medium Enterprise (mSME)
credit risk modelling is particularly challenging, as, often, the same sources
of information are not available. Therefore, it is standard policy for a loan
officer to provide a textual loan assessment to mitigate limited data
availability. In turn, this statement is analysed by a credit expert alongside
any available standard credit data. In our paper, we exploit recent advances
from the field of Deep Learning and Natural Language Processing (NLP),
including the BERT (Bidirectional Encoder Representations from Transformers)
model, to extract information from 60 000 textual assessments provided by a
lender. We consider the performance in terms of the AUC (Area Under the
receiver operating characteristic Curve) and Brier Score metrics and find that
the text alone is surprisingly effective for predicting default. However, when
combined with traditional data, it yields no additional predictive capability,
with performance dependent on the text's length. Our proposed deep learning
model does, however, appear to be robust to the quality of the text and
therefore suitable for partly automating the mSME lending process. We also
demonstrate how the content of loan assessments influences performance, leading
us to a series of recommendations on a new strategy for collecting future mSME
loan assessments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stevenson_M/0/1/0/all/0/1"&gt;Matthew Stevenson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mues_C/0/1/0/all/0/1"&gt;Christophe Mues&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bravo_C/0/1/0/all/0/1"&gt;Cristi&amp;#xe1;n Bravo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extending Lagrangian and Hamiltonian Neural Networks with Differentiable Contact Models. (arXiv:2102.06794v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06794</id>
        <link href="http://arxiv.org/abs/2102.06794"/>
        <updated>2021-07-09T01:58:28.533Z</updated>
        <summary type="html"><![CDATA[The incorporation of appropriate inductive bias plays a critical role in
learning dynamics from data. A growing body of work has been exploring ways to
enforce energy conservation in the learned dynamics by encoding Lagrangian or
Hamiltonian dynamics into the neural network architecture. These existing
approaches are based on differential equations, which do not allow
discontinuity in the states and thereby limit the class of systems one can
learn. However, in reality, most physical systems, such as legged robots and
robotic manipulators, involve contacts and collisions, which introduce
discontinuities in the states. In this paper, we introduce a differentiable
contact model, which can capture contact mechanics: frictionless/frictional, as
well as elastic/inelastic. This model can also accommodate inequality
constraints, such as limits on the joint angles. The proposed contact model
extends the scope of Lagrangian and Hamiltonian neural networks by allowing
simultaneous learning of contact and system properties. We demonstrate this
framework on a series of challenging 2D and 3D physical systems with different
coefficients of restitution and friction. The learned dynamics can be used as a
differentiable physics simulator for downstream gradient-based optimization
tasks, such as planning and control.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1"&gt;Yaofeng Desmond Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dey_B/0/1/0/all/0/1"&gt;Biswadip Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1"&gt;Amit Chakraborty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bag of Tricks for Neural Architecture Search. (arXiv:2107.03719v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03719</id>
        <link href="http://arxiv.org/abs/2107.03719"/>
        <updated>2021-07-09T01:58:28.498Z</updated>
        <summary type="html"><![CDATA[While neural architecture search methods have been successful in previous
years and led to new state-of-the-art performance on various problems, they
have also been criticized for being unstable, being highly sensitive with
respect to their hyperparameters, and often not performing better than random
search. To shed some light on this issue, we discuss some practical
considerations that help improve the stability, efficiency and overall
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elsken_T/0/1/0/all/0/1"&gt;Thomas Elsken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Staffler_B/0/1/0/all/0/1"&gt;Benedikt Staffler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zela_A/0/1/0/all/0/1"&gt;Arber Zela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metzen_J/0/1/0/all/0/1"&gt;Jan Hendrik Metzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1"&gt;Frank Hutter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Impossibility results for fair representations. (arXiv:2107.03483v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03483</id>
        <link href="http://arxiv.org/abs/2107.03483"/>
        <updated>2021-07-09T01:58:28.468Z</updated>
        <summary type="html"><![CDATA[With the growing awareness to fairness in machine learning and the
realization of the central role that data representation has in data processing
tasks, there is an obvious interest in notions of fair data representations.
The goal of such representations is that a model trained on data under the
representation (e.g., a classifier) will be guaranteed to respect some fairness
constraints.

Such representations are useful when they can be fixed for training models on
various different tasks and also when they serve as data filtering between the
raw data (known to the representation designer) and potentially malicious
agents that use the data under the representation to learn predictive models
and make decisions.

A long list of recent research papers strive to provide tools for achieving
these goals.

However, we prove that this is basically a futile effort. Roughly stated, we
prove that no representation can guarantee the fairness of classifiers for
different tasks trained using it; even the basic goal of achieving
label-independent Demographic Parity fairness fails once the marginal data
distribution shifts. More refined notions of fairness, like Odds Equality,
cannot be guaranteed by a representation that does not take into account the
task specific labeling rule with respect to which such fairness will be
evaluated (even if the marginal data distribution is known a priory).
Furthermore, except for trivial cases, no representation can guarantee Odds
Equality fairness for any two different tasks, while allowing accurate label
predictions for both.

While some of our conclusions are intuitive, we formulate (and prove) crisp
statements of such impossibilities, often contrasting impressions conveyed by
many recent works on fair representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lechner_T/0/1/0/all/0/1"&gt;Tosca Lechner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ben_David_S/0/1/0/all/0/1"&gt;Shai Ben-David&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sushant Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ananthakrishnan_N/0/1/0/all/0/1"&gt;Nivasini Ananthakrishnan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Network Compression. (arXiv:2008.08733v3 [q-fin.RM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.08733</id>
        <link href="http://arxiv.org/abs/2008.08733"/>
        <updated>2021-07-09T01:58:28.448Z</updated>
        <summary type="html"><![CDATA[This paper introduces a formulation of the optimal network compression
problem for financial systems. This general formulation is presented for
different levels of network compression or rerouting allowed from the initial
interbank network. We prove that this problem is, generically, NP-hard. We
focus on objective functions generated by systemic risk measures under
systematic shocks to the financial network. We conclude by studying the optimal
compression problem for specific networks; this permits us to study the
so-called robust fragility of certain network topologies more generally as well
as the potential benefits and costs of network compression. In particular,
under systematic shocks and heterogeneous financial networks the typical
heuristics of robust fragility no longer hold generally.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Amini_H/0/1/0/all/0/1"&gt;Hamed Amini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Feinstein_Z/0/1/0/all/0/1"&gt;Zachary Feinstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Certifying clusters from sum-of-norms clustering. (arXiv:2006.11355v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.11355</id>
        <link href="http://arxiv.org/abs/2006.11355"/>
        <updated>2021-07-09T01:58:28.441Z</updated>
        <summary type="html"><![CDATA[Sum-of-norms clustering is a clustering formulation based on convex
optimization that automatically induces hierarchy. Multiple algorithms have
been proposed to solve the optimization problem: subgradient descent by Hocking
et al., ADMM and ADA by Chi and Lange, stochastic incremental algorithm by
Panahi et al. and semismooth Newton-CG augmented Lagrangian method by Sun et
al. All algorithms yield approximate solutions, even though an exact solution
is demanded to determine the correct cluster assignment. The purpose of this
paper is to close the gap between the output from existing algorithms and the
exact solution to the optimization problem. We present a clustering test that
identifies and certifies the correct cluster assignment from an approximate
solution yielded by any primal-dual algorithm. Our certification validates
clustering for both unit and multiplicative weights. The test may not succeed
if the approximation is inaccurate. However, we show the correct cluster
assignment is guaranteed to be certified by a primal-dual path following
algorithm after sufficiently many iterations, provided that the model parameter
$\lambda$ avoids a finite number of bad values. Numerical experiments are
conducted on Gaussian mixture and half-moon data, which indicate that carefully
chosen multiplicative weights increase the recovery power of sum-of-norms
clustering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1"&gt;Tao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vavasis_S/0/1/0/all/0/1"&gt;Stephen Vavasis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline. (arXiv:2106.06054v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06054</id>
        <link href="http://arxiv.org/abs/2106.06054"/>
        <updated>2021-07-09T01:58:28.434Z</updated>
        <summary type="html"><![CDATA[In recent years, many incidents have been reported where machine learning
models exhibited discrimination among people based on race, sex, age, etc.
Research has been conducted to measure and mitigate unfairness in machine
learning models. For a machine learning task, it is a common practice to build
a pipeline that includes an ordered set of data preprocessing stages followed
by a classifier. However, most of the research on fairness has considered a
single classifier based prediction task. What are the fairness impacts of the
preprocessing stages in machine learning pipeline? Furthermore, studies showed
that often the root cause of unfairness is ingrained in the data itself, rather
than the model. But no research has been conducted to measure the unfairness
caused by a specific transformation made in the data preprocessing stage. In
this paper, we introduced the causal method of fairness to reason about the
fairness impact of data preprocessing stages in ML pipeline. We leveraged
existing metrics to define the fairness measures of the stages. Then we
conducted a detailed fairness evaluation of the preprocessing stages in 37
pipelines collected from three different sources. Our results show that certain
data transformers are causing the model to exhibit unfairness. We identified a
number of fairness patterns in several categories of data transformers.
Finally, we showed how the local fairness of a preprocessing stage composes in
the global fairness of the pipeline. We used the fairness composition to choose
appropriate downstream transformer that mitigates unfairness in the machine
learning pipeline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1"&gt;Sumon Biswas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajan_H/0/1/0/all/0/1"&gt;Hridesh Rajan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Default Values: a Low Cost and Efficient Strategy to Define Hyperparameters. (arXiv:2008.00025v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.00025</id>
        <link href="http://arxiv.org/abs/2008.00025"/>
        <updated>2021-07-09T01:58:28.426Z</updated>
        <summary type="html"><![CDATA[Machine Learning (ML) algorithms have been increasingly applied to problems
from several different areas. Despite their growing popularity, their
predictive performance is usually affected by the values assigned to their
hyperparameters (HPs). As consequence, researchers and practitioners face the
challenge of how to set these values. Many users have limited knowledge about
ML algorithms and the effect of their HP values and, therefore, do not take
advantage of suitable settings. They usually define the HP values by trial and
error, which is very subjective, not guaranteed to find good values and
dependent on the user experience. Tuning techniques search for HP values able
to maximize the predictive performance of induced models for a given dataset,
but have the drawback of a high computational cost. Thus, practitioners use
default values suggested by the algorithm developer or by tools implementing
the algorithm. Although default values usually result in models with acceptable
predictive performance, different implementations of the same algorithm can
suggest distinct default values. To maintain a balance between tuning and using
default values, we propose a strategy to generate new optimized default values.
Our approach is grounded on a small set of optimized values able to obtain
predictive performance values better than default settings provided by popular
tools. After performing a large experiment and a careful analysis of the
results, we concluded that our approach delivers better default values.
Besides, it leads to competitive solutions when compared to tuned values,
making it easier to use and having a lower cost. We also extracted simple rules
to guide practitioners in deciding whether to use our new methodology or a HP
tuning approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mantovani_R/0/1/0/all/0/1"&gt;Rafael Gomes Mantovani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rossi_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Luis Debiaso Rossi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alcobaca_E/0/1/0/all/0/1"&gt;Edesio Alcoba&amp;#xe7;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gertrudes_J/0/1/0/all/0/1"&gt;Jadson Castro Gertrudes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Junior_S/0/1/0/all/0/1"&gt;Sylvio Barbon Junior&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carvalho_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Carlos Ponce de Leon Ferreira de Carvalho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Use of Machine Learning Technique to maximize the signal over background for $H \rightarrow \tau \tau$. (arXiv:2106.14257v2 [physics.data-an] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14257</id>
        <link href="http://arxiv.org/abs/2106.14257"/>
        <updated>2021-07-09T01:58:28.418Z</updated>
        <summary type="html"><![CDATA[In recent years, artificial neural networks (ANNs) have won numerous contests
in pattern recognition and machine learning. ANNS have been applied to problems
ranging from speech recognition to prediction of protein secondary structure,
classification of cancers, and gene prediction. Here, we intend to maximize the
chances of finding the Higgs boson decays to two $\tau$ leptons in the pseudo
dataset using a Machine Learning technique to classify the recorded events as
signal or background.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Gupta_K/0/1/0/all/0/1"&gt;Kanhaiya Gupta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepShift: Towards Multiplication-Less Neural Networks. (arXiv:1905.13298v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.13298</id>
        <link href="http://arxiv.org/abs/1905.13298"/>
        <updated>2021-07-09T01:58:28.398Z</updated>
        <summary type="html"><![CDATA[The high computation, memory, and power budgets of inferring convolutional
neural networks (CNNs) are major bottlenecks of model deployment to edge
computing platforms, e.g., mobile devices and IoT. Moreover, training CNNs is
time and energy-intensive even on high-grade servers. Convolution layers and
fully connected layers, because of their intense use of multiplications, are
the dominant contributor to this computation budget.

We propose to alleviate this problem by introducing two new operations:
convolutional shifts and fully-connected shifts which replace multiplications
with bitwise shift and sign flipping during both training and inference. During
inference, both approaches require only 5 bits (or less) to represent the
weights. This family of neural network architectures (that use convolutional
shifts and fully connected shifts) is referred to as DeepShift models. We
propose two methods to train DeepShift models: DeepShift-Q which trains regular
weights constrained to powers of 2, and DeepShift-PS that trains the values of
the shifts and sign flips directly.

Very close accuracy, and in some cases higher accuracy, to baselines are
achieved. Converting pre-trained 32-bit floating-point baseline models of
ResNet18, ResNet50, VGG16, and GoogleNet to DeepShift and training them for 15
to 30 epochs, resulted in Top-1/Top-5 accuracies higher than that of the
original model.

Last but not least, we implemented the convolutional shifts and fully
connected shift GPU kernels and showed a reduction in latency time of 25% when
inferring ResNet18 compared to unoptimized multiplication-based GPU kernels.
The code can be found at https://github.com/mostafaelhoushi/DeepShift.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elhoushi_M/0/1/0/all/0/1"&gt;Mostafa Elhoushi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zihao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafiq_F/0/1/0/all/0/1"&gt;Farhan Shafiq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Ye Henry Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Joey Yiwei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Probabilistic Interpretation of Self-Paced Learning with Applications to Reinforcement Learning. (arXiv:2102.13176v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13176</id>
        <link href="http://arxiv.org/abs/2102.13176"/>
        <updated>2021-07-09T01:58:28.391Z</updated>
        <summary type="html"><![CDATA[Across machine learning, the use of curricula has shown strong empirical
potential to improve learning from data by avoiding local optima of training
objectives. For reinforcement learning (RL), curricula are especially
interesting, as the underlying optimization has a strong tendency to get stuck
in local optima due to the exploration-exploitation trade-off. Recently, a
number of approaches for an automatic generation of curricula for RL have been
shown to increase performance while requiring less expert knowledge compared to
manually designed curricula. However, these approaches are seldomly
investigated from a theoretical perspective, preventing a deeper understanding
of their mechanics. In this paper, we present an approach for automated
curriculum generation in RL with a clear theoretical underpinning. More
precisely, we formalize the well-known self-paced learning paradigm as inducing
a distribution over training tasks, which trades off between task complexity
and the objective to match a desired task distribution. Experiments show that
training on this induced distribution helps to avoid poor local optima across
RL algorithms in different tasks with uninformative rewards and challenging
exploration requirements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Klink_P/0/1/0/all/0/1"&gt;Pascal Klink&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdulsamad_H/0/1/0/all/0/1"&gt;Hany Abdulsamad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belousov_B/0/1/0/all/0/1"&gt;Boris Belousov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DEramo_C/0/1/0/all/0/1"&gt;Carlo D&amp;#x27;Eramo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1"&gt;Jan Peters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pajarinen_J/0/1/0/all/0/1"&gt;Joni Pajarinen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthetic Data -- Anonymisation Groundhog Day. (arXiv:2011.07018v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.07018</id>
        <link href="http://arxiv.org/abs/2011.07018"/>
        <updated>2021-07-09T01:58:28.384Z</updated>
        <summary type="html"><![CDATA[Synthetic data has been advertised as a silver-bullet solution to
privacy-preserving data publishing that addresses the shortcomings of
traditional anonymisation techniques. The promise is that synthetic data drawn
from generative models preserves the statistical properties of the original
dataset but, at the same time, provides perfect protection against privacy
attacks. In this work, we present the first quantitative evaluation of the
privacy gain of synthetic data publishing and compare it to that of previous
anonymisation techniques.

Our evaluation of a wide range of state-of-the-art generative models
demonstrates that synthetic data either does not prevent inference attacks or
does not retain data utility. In other words, we empirically show that
synthetic data suffers from the same limitations as traditional anonymisation
techniques.

Furthermore, we find that, in contrast to traditional anonymisation, the
privacy-utility tradeoff of synthetic data publishing is hard to predict.
Because it is impossible to predict what signals a synthetic dataset will
preserve and what information will be lost, synthetic data leads to a highly
variable privacy gain and unpredictable utility loss. In summary, we find that
synthetic data is far from the holy grail of privacy-preserving data
publishing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stadler_T/0/1/0/all/0/1"&gt;Theresa Stadler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oprisanu_B/0/1/0/all/0/1"&gt;Bristena Oprisanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Troncoso_C/0/1/0/all/0/1"&gt;Carmela Troncoso&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey. (arXiv:2009.13303v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.13303</id>
        <link href="http://arxiv.org/abs/2009.13303"/>
        <updated>2021-07-09T01:58:28.378Z</updated>
        <summary type="html"><![CDATA[Deep reinforcement learning has recently seen huge success across multiple
areas in the robotics domain. Owing to the limitations of gathering real-world
data, i.e., sample inefficiency and the cost of collecting it, simulation
environments are utilized for training the different agents. This not only aids
in providing a potentially infinite data source, but also alleviates safety
concerns with real robots. Nonetheless, the gap between the simulated and real
worlds degrades the performance of the policies once the models are transferred
into real robots. Multiple research efforts are therefore now being directed
towards closing this sim-to-real gap and accomplish more efficient policy
transfer. Recent years have seen the emergence of multiple methods applicable
to different domains, but there is a lack, to the best of our knowledge, of a
comprehensive review summarizing and putting into context the different
methods. In this survey paper, we cover the fundamental background behind
sim-to-real transfer in deep reinforcement learning and overview the main
methods being utilized at the moment: domain randomization, domain adaptation,
imitation learning, meta-learning and knowledge distillation. We categorize
some of the most relevant recent works, and outline the main application
scenarios. Finally, we discuss the main opportunities and challenges of the
different approaches and point to the most promising directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wenshuai Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Queralta_J/0/1/0/all/0/1"&gt;Jorge Pe&amp;#xf1;a Queralta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Westerlund_T/0/1/0/all/0/1"&gt;Tomi Westerlund&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Reinforcement Learning Methods for Structure-Guided Processing Path Optimization. (arXiv:2009.09706v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.09706</id>
        <link href="http://arxiv.org/abs/2009.09706"/>
        <updated>2021-07-09T01:58:28.368Z</updated>
        <summary type="html"><![CDATA[A major goal of materials design is to find material structures with desired
properties and in a second step to find a processing path to reach one of these
structures. In this paper, we propose and investigate a deep reinforcement
learning approach for the optimization of processing paths. The goal is to find
optimal processing paths in the material structure space that lead to
target-structures, which have been identified beforehand to result in desired
material properties. There exists a target set containing one or multiple
different structures. Our proposed methods can find an optimal path from a
start structure to a single target structure, or optimize the processing paths
to one of the equivalent target-structures in the set. In the latter case, the
algorithm learns during processing to simultaneously identify the best
reachable target structure and the optimal path to it. The proposed methods
belong to the family of model-free deep reinforcement learning algorithms. They
are guided by structure representations as features of the process state and by
a reward signal, which is formulated based on a distance function in the
structure space. Model-free reinforcement learning algorithms learn through
trial and error while interacting with the process. Thereby, they are not
restricted to information from a priori sampled processing data and are able to
adapt to the specific process. The optimization itself is model-free and does
not require any prior knowledge about the process itself. We instantiate and
evaluate the proposed methods by optimizing paths of a generic metal forming
process. We show the ability of both methods to find processing paths leading
close to target structures and the ability of the extended method to identify
target-structures that can be reached effectively and efficiently and to focus
on these targets for sample efficient processing path optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dornheim_J/0/1/0/all/0/1"&gt;Johannes Dornheim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morand_L/0/1/0/all/0/1"&gt;Lukas Morand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeitvogel_S/0/1/0/all/0/1"&gt;Samuel Zeitvogel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iraki_T/0/1/0/all/0/1"&gt;Tarek Iraki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Link_N/0/1/0/all/0/1"&gt;Norbert Link&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Helm_D/0/1/0/all/0/1"&gt;Dirk Helm&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond BatchNorm: Towards a General Understanding of Normalization in Deep Learning. (arXiv:2106.05956v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05956</id>
        <link href="http://arxiv.org/abs/2106.05956"/>
        <updated>2021-07-09T01:58:28.349Z</updated>
        <summary type="html"><![CDATA[Inspired by BatchNorm, there has been an explosion of normalization layers
for deep neural networks (DNNs). However, these alternative normalization
layers have seen minimal use, partially due to a lack of guiding principles
that can help identify when these layers can serve as a replacement for
BatchNorm. To address this problem, we take a theoretical approach,
generalizing the known beneficial mechanisms of BatchNorm to several recently
proposed normalization techniques. Our generalized theory leads to the
following set of principles: (i) similar to BatchNorm, activations-based
normalization layers can prevent exponential growth of activations in ResNets,
but parametric layers require explicit remedies; (ii) use of GroupNorm can
ensure informative forward propagation, with different samples being assigned
dissimilar activations, but increasing group size results in increasingly
indistinguishable activations for different samples, explaining slow
convergence speed in models with LayerNorm; (iii) small group sizes result in
large gradient norm in earlier layers, hence explaining training instability
issues in Instance Normalization and illustrating a speed-stability tradeoff in
GroupNorm. Overall, our analysis reveals a unified set of mechanisms that
underpin the success of normalization methods in deep learning, providing us
with a compass to systematically explore the vast design space of DNN
normalization layers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lubana_E/0/1/0/all/0/1"&gt;Ekdeep Singh Lubana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dick_R/0/1/0/all/0/1"&gt;Robert P. Dick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tanaka_H/0/1/0/all/0/1"&gt;Hidenori Tanaka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TensorFlow RiemOpt: a library for optimization on Riemannian manifolds. (arXiv:2105.13921v2 [cs.MS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13921</id>
        <link href="http://arxiv.org/abs/2105.13921"/>
        <updated>2021-07-09T01:58:28.343Z</updated>
        <summary type="html"><![CDATA[The adoption of neural networks and deep learning in non-Euclidean domains
has been hindered until recently by the lack of scalable and efficient learning
frameworks. Existing toolboxes in this space were mainly motivated by research
and education use cases, whereas practical aspects, such as deploying and
maintaining machine learning models, were often overlooked.

We attempt to bridge this gap by proposing TensorFlow RiemOpt, a Python
library for optimization on Riemannian manifolds in TensorFlow. The library is
designed with the aim for a seamless integration with the TensorFlow ecosystem,
targeting not only research, but also streamlining production machine learning
pipelines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smirnov_O/0/1/0/all/0/1"&gt;Oleg Smirnov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cascaded Diffusion Models for High Fidelity Image Generation. (arXiv:2106.15282v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15282</id>
        <link href="http://arxiv.org/abs/2106.15282"/>
        <updated>2021-07-09T01:58:28.336Z</updated>
        <summary type="html"><![CDATA[We show that cascaded diffusion models are capable of generating high
fidelity images on the class-conditional ImageNet generation challenge, without
any assistance from auxiliary image classifiers to boost sample quality. A
cascaded diffusion model comprises a pipeline of multiple diffusion models that
generate images of increasing resolution, beginning with a standard diffusion
model at the lowest resolution, followed by one or more super-resolution
diffusion models that successively upsample the image and add higher resolution
details. We find that the sample quality of a cascading pipeline relies
crucially on conditioning augmentation, our proposed method of data
augmentation of the lower resolution conditioning inputs to the
super-resolution models. Our experiments show that conditioning augmentation
prevents compounding error during sampling in a cascaded model, helping us to
train cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at
128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep, and
classification accuracy scores of 63.02% (top-1) and 84.06% (top-5) at 256x256,
outperforming VQ-VAE-2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1"&gt;Jonathan Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saharia_C/0/1/0/all/0/1"&gt;Chitwan Saharia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1"&gt;William Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1"&gt;David J. Fleet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1"&gt;Mohammad Norouzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salimans_T/0/1/0/all/0/1"&gt;Tim Salimans&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Teacher's pet: understanding and mitigating biases in distillation. (arXiv:2106.10494v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10494</id>
        <link href="http://arxiv.org/abs/2106.10494"/>
        <updated>2021-07-09T01:58:28.330Z</updated>
        <summary type="html"><![CDATA[Knowledge distillation is widely used as a means of improving the performance
of a relatively simple student model using the predictions from a complex
teacher model. Several works have shown that distillation significantly boosts
the student's overall performance; however, are these gains uniform across all
data subgroups? In this paper, we show that distillation can harm performance
on certain subgroups, e.g., classes with few associated samples. We trace this
behaviour to errors made by the teacher distribution being transferred to and
amplified by the student model. To mitigate this problem, we present techniques
which soften the teacher influence for subgroups where it is less reliable.
Experiments on several image classification benchmarks show that these
modifications of distillation maintain boost in overall accuracy, while
additionally ensuring improvement in subgroup performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lukasik_M/0/1/0/all/0/1"&gt;Michal Lukasik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhojanapalli_S/0/1/0/all/0/1"&gt;Srinadh Bhojanapalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1"&gt;Aditya Krishna Menon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sanjiv Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-world Ride-hailing Vehicle Repositioning using Deep Reinforcement Learning. (arXiv:2103.04555v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04555</id>
        <link href="http://arxiv.org/abs/2103.04555"/>
        <updated>2021-07-09T01:58:28.323Z</updated>
        <summary type="html"><![CDATA[We present a new practical framework based on deep reinforcement learning and
decision-time planning for real-world vehicle repositioning on ride-hailing (a
type of mobility-on-demand, MoD) platforms. Our approach learns the
spatiotemporal state-value function using a batch training algorithm with deep
value networks. The optimal repositioning action is generated on-demand through
value-based policy search, which combines planning and bootstrapping with the
value networks. For the large-fleet problems, we develop several algorithmic
features that we incorporate into our framework and that we demonstrate to
induce coordination among the algorithmically-guided vehicles. We benchmark our
algorithm with baselines in a ride-hailing simulation environment to
demonstrate its superiority in improving income efficiency meausred by
income-per-hour. We have also designed and run a real-world experiment program
with regular drivers on a major ride-hailing platform. We have observed
significantly positive results on key metrics comparing our method with
experienced drivers who performed idle-time repositioning based on their own
expertise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1"&gt;Yan Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xiaocheng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1"&gt;Zhiwei Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuaiji Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hongtu Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jieping Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer-based Conditional Variational Autoencoder for Controllable Story Generation. (arXiv:2101.00828v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00828</id>
        <link href="http://arxiv.org/abs/2101.00828"/>
        <updated>2021-07-09T01:58:28.305Z</updated>
        <summary type="html"><![CDATA[We investigate large-scale latent variable models (LVMs) for neural story
generation -- an under-explored application for open-domain long text -- with
objectives in two threads: generation effectiveness and controllability. LVMs,
especially the variational autoencoder (VAE), have achieved both effective and
controllable generation through exploiting flexible distributional latent
representations. Recently, Transformers and its variants have achieved
remarkable effectiveness without explicit latent representation learning, thus
lack satisfying controllability in generation. In this paper, we advocate to
revive latent variable modeling, essentially the power of representation
learning, in the era of Transformers to enhance controllability without hurting
state-of-the-art generation effectiveness. Specifically, we integrate latent
representation vectors with a Transformer-based pre-trained architecture to
build conditional variational autoencoder (CVAE). Model components such as
encoder, decoder and the variational posterior are all built on top of
pre-trained language models -- GPT2 specifically in this paper. Experiments
demonstrate state-of-the-art conditional generation ability of our model, as
well as its excellent representation learning capability and controllability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1"&gt;Le Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_T/0/1/0/all/0/1"&gt;Tao Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chaochun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1"&gt;Liefeng Bo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1"&gt;Wen Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Changyou Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Defending Against Multiple and Unforeseen Adversarial Videos. (arXiv:2009.05244v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.05244</id>
        <link href="http://arxiv.org/abs/2009.05244"/>
        <updated>2021-07-09T01:58:28.295Z</updated>
        <summary type="html"><![CDATA[Adversarial robustness of deep neural networks has been actively
investigated. However, most existing defense approaches are limited to a
specific type of adversarial perturbations. Specifically, they often fail to
offer resistance to multiple attack types simultaneously, i.e., they lack
multi-perturbation robustness. Furthermore, compared to image recognition
problems, the adversarial robustness of video recognition models is relatively
unexplored. While several studies have proposed how to generate adversarial
videos, only a handful of approaches about the defense strategies have been
published in the literature. In this paper, we propose one of the first defense
strategies against multiple types of adversarial videos for video recognition.
The proposed method, referred to as MultiBN, performs adversarial training on
multiple adversarial video types using multiple independent batch normalization
(BN) layers with a learning-based BN selection module. With a multiple BN
structure, each BN brach is responsible for learning the distribution of a
single perturbation type and thus provides more precise distribution
estimations. This mechanism benefits dealing with multiple perturbation types.
The BN selection module detects the attack type of an input video and sends it
to the corresponding BN branch, making MultiBN fully automatic and allow
end-to-end training. Compared to present adversarial training approaches, the
proposed MultiBN exhibits stronger multi-perturbation robustness against
different and even unforeseen adversarial video types, ranging from Lp-bounded
attacks and physically realizable attacks. This holds true on different
datasets and target models. Moreover, we conduct an extensive analysis to study
the properties of the multiple BN structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1"&gt;Shao-Yuan Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1"&gt;Vishal M. Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flexibly Regularized Mixture Models and Application to Image Segmentation. (arXiv:1905.10629v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.10629</id>
        <link href="http://arxiv.org/abs/1905.10629"/>
        <updated>2021-07-09T01:58:28.288Z</updated>
        <summary type="html"><![CDATA[Probabilistic finite mixture models are widely used for unsupervised
clustering. These models can often be improved by adapting them to the topology
of the data. For instance, in order to classify spatially adjacent data points
similarly, it is common to introduce a Laplacian constraint on the posterior
probability that each data point belongs to a class. Alternatively, the mixing
probabilities can be treated as free parameters, while assuming Gauss-Markov or
more complex priors to regularize those mixing probabilities. However, these
approaches are constrained by the shape of the prior and often lead to
complicated or intractable inference. Here, we propose a new parametrization of
the Dirichlet distribution to flexibly regularize the mixing probabilities of
over-parametrized mixture distributions. Using the Expectation-Maximization
algorithm, we show that our approach allows us to define any linear update rule
for the mixing probabilities, including spatial smoothing regularization as a
special case. We then show that this flexible design can be extended to share
class information between multiple mixture models. We apply our algorithm to
artificial and natural image segmentation tasks, and we provide quantitative
and qualitative comparison of the performance of Gaussian and Student-t
mixtures on the Berkeley Segmentation Dataset. We also demonstrate how to
propagate class information across the layers of deep convolutional neural
networks in a probabilistically optimal way, suggesting a new interpretation
for feedback signals in biological visual systems. Our flexible approach can be
easily generalized to adapt probabilistic mixture models to arbitrary data
topologies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vacher_J/0/1/0/all/0/1"&gt;Jonathan Vacher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Launay_C/0/1/0/all/0/1"&gt;Claire Launay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coen_Cagli_R/0/1/0/all/0/1"&gt;Ruben Coen-Cagli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the Effectiveness and Efficiency of Stochastic Neighbour Embedding with Isolation Kernel. (arXiv:1906.09744v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.09744</id>
        <link href="http://arxiv.org/abs/1906.09744"/>
        <updated>2021-07-09T01:58:28.281Z</updated>
        <summary type="html"><![CDATA[This paper presents a new insight into improving the performance of
Stochastic Neighbour Embedding (t-SNE) by using Isolation kernel instead of
Gaussian kernel. Isolation kernel outperforms Gaussian kernel in two aspects.
First, the use of Isolation kernel in t-SNE overcomes the drawback of
misrepresenting some structures in the data, which often occurs when Gaussian
kernel is applied in t-SNE. This is because Gaussian kernel determines each
local bandwidth based on one local point only, while Isolation kernel is
derived directly from the data based on space partitioning. Second, the use of
Isolation kernel yields a more efficient similarity computation because
data-dependent Isolation kernel has only one parameter that needs to be tuned.
In contrast, the use of data-independent Gaussian kernel increases the
computational cost by determining n bandwidths for a dataset of n points. As
the root cause of these deficiencies in t-SNE is Gaussian kernel, we show that
simply replacing Gaussian kernel with Isolation kernel in t-SNE significantly
improves the quality of the final visualisation output (without creating
misrepresented structures) and removes one key obstacle that prevents t-SNE
from processing large datasets. Moreover, Isolation kernel enables t-SNE to
deal with large-scale datasets in less runtime without trading off accuracy,
unlike existing methods in speeding up t-SNE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Ye Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ting_K/0/1/0/all/0/1"&gt;Kai Ming Ting&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Matrix Factorization with Grouping Effect. (arXiv:2106.13681v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13681</id>
        <link href="http://arxiv.org/abs/2106.13681"/>
        <updated>2021-07-09T01:58:28.273Z</updated>
        <summary type="html"><![CDATA[Although many techniques have been applied to matrix factorization (MF), they
may not fully exploit the feature structure. In this paper, we incorporate the
grouping effect into MF and propose a novel method called Robust Matrix
Factorization with Grouping effect (GRMF). The grouping effect is a
generalization of the sparsity effect, which conducts denoising by clustering
similar values around multiple centers instead of just around 0. Compared with
existing algorithms, the proposed GRMF can automatically learn the grouping
structure and sparsity in MF without prior knowledge, by introducing a
naturally adjustable non-convex regularization to achieve simultaneous sparsity
and grouping effect. Specifically, GRMF uses an efficient alternating
minimization framework to perform MF, in which the original non-convex problem
is first converted into a convex problem through Difference-of-Convex (DC)
programming, and then solved by Alternating Direction Method of Multipliers
(ADMM). In addition, GRMF can be easily extended to the Non-negative Matrix
Factorization (NMF) settings. Extensive experiments have been conducted using
real-world data sets with outliers and contaminated noise, where the
experimental results show that GRMF has promoted performance and robustness,
compared to five benchmark algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haiyan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Luwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Haoyi Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1"&gt;Dejing Dou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consensus Clustering With Unsupervised Representation Learning. (arXiv:2010.01245v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.01245</id>
        <link href="http://arxiv.org/abs/2010.01245"/>
        <updated>2021-07-09T01:58:28.252Z</updated>
        <summary type="html"><![CDATA[Recent advances in deep clustering and unsupervised representation learning
are based on the idea that different views of an input image (generated through
data augmentation techniques) must either be closer in the representation
space, or have a similar cluster assignment. Bootstrap Your Own Latent (BYOL)
is one such representation learning algorithm that has achieved
state-of-the-art results in self-supervised image classification on ImageNet
under the linear evaluation protocol. However, the utility of the learnt
features of BYOL to perform clustering is not explored. In this work, we study
the clustering ability of BYOL and observe that features learnt using BYOL may
not be optimal for clustering. We propose a novel consensus clustering based
loss function, and train BYOL with the proposed loss in an end-to-end way that
improves the clustering ability and outperforms similar clustering based
methods on some popular computer vision datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Regatti_J/0/1/0/all/0/1"&gt;Jayanth Reddy Regatti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deshmukh_A/0/1/0/all/0/1"&gt;Aniket Anand Deshmukh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manavoglu_E/0/1/0/all/0/1"&gt;Eren Manavoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dogan_U/0/1/0/all/0/1"&gt;Urun Dogan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classification of Urban Morphology with Deep Learning: Application on Urban Vitality. (arXiv:2105.09908v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09908</id>
        <link href="http://arxiv.org/abs/2105.09908"/>
        <updated>2021-07-09T01:58:28.245Z</updated>
        <summary type="html"><![CDATA[There is a prevailing trend to study urban morphology quantitatively thanks
to the growing accessibility to various forms of spatial big data, increasing
computing power, and use cases benefiting from such information. The methods
developed up to now measure urban morphology with numerical indices describing
density, proportion, and mixture, but they do not directly represent
morphological features from the human's visual and intuitive perspective. We
take the first step to bridge the gap by proposing a deep learning-based
technique to automatically classify road networks into four classes on a visual
basis. The method is implemented by generating an image of the street network
(Colored Road Hierarchy Diagram), which we introduce in this paper, and
classifying it using a deep convolutional neural network (ResNet-34). The model
achieves an overall classification accuracy of 0.875. Nine cities around the
world are selected as the study areas with their road networks acquired from
OpenStreetMap. Latent subgroups among the cities are uncovered through
clustering on the percentage of each road network category. In the subsequent
part of the paper, we focus on the usability of such classification: we apply
our method in a case study of urban vitality prediction. An advanced tree-based
regression model (LightGBM) is for the first time designated to establish the
relationship between morphological indices and vitality indicators. The effect
of road network classification is found to be small but positively associated
with urban vitality. This work expands the toolkit of quantitative urban
morphology study with new techniques, supporting further studies in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wangyang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1"&gt;Abraham Noah Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biljecki_F/0/1/0/all/0/1"&gt;Filip Biljecki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Multi-Step Critiquing for VAE-based Recommender Systems. (arXiv:2105.00774v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00774</id>
        <link href="http://arxiv.org/abs/2105.00774"/>
        <updated>2021-07-09T01:58:28.228Z</updated>
        <summary type="html"><![CDATA[Recent studies have shown that providing personalized explanations alongside
recommendations increases trust and perceived quality. Furthermore, it gives
users an opportunity to refine the recommendations by critiquing parts of the
explanations. On one hand, current recommender systems model the
recommendation, explanation, and critiquing objectives jointly, but this
creates an inherent trade-off between their respective performance. On the
other hand, although recent latent linear critiquing approaches are built upon
an existing recommender system, they suffer from computational inefficiency at
inference due to the objective optimized at each conversation's turn. We
address these deficiencies with M&Ms-VAE, a novel variational autoencoder for
recommendation and explanation that is based on multimodal modeling
assumptions. We train the model under a weak supervision scheme to simulate
both fully and partially observed variables. Then, we leverage the
generalization ability of a trained M&Ms-VAE model to embed the user preference
and the critique separately. Our work's most important innovation is our
critiquing module, which is built upon and trained in a self-supervised manner
with a simple ranking objective. Experiments on four real-world datasets
demonstrate that among state-of-the-art models, our system is the first to
dominate or match the performance in terms of recommendation, explanation, and
multi-step critiquing. Moreover, M&Ms-VAE processes the critiques up to 25.6x
faster than the best baselines. Finally, we show that our model infers coherent
joint and cross generation, even under weak supervision, thanks to our
multimodal-based modeling and training scheme.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1"&gt;Diego Antognini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1"&gt;Boi Faltings&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ChemRL-GEM: Geometry Enhanced Molecular Representation Learning for Property Prediction. (arXiv:2106.06130v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06130</id>
        <link href="http://arxiv.org/abs/2106.06130"/>
        <updated>2021-07-09T01:58:28.202Z</updated>
        <summary type="html"><![CDATA[Effective molecular representation learning is of great importance to
facilitate molecular property prediction, which is a fundamental task for the
drug and material industry. Recent advances in graph neural networks (GNNs)
have shown great promise in applying GNNs for molecular representation
learning. Moreover, a few recent studies have also demonstrated successful
applications of self-supervised learning methods to pre-train the GNNs to
overcome the problem of insufficient labeled molecules. However, existing GNNs
and pre-training strategies usually treat molecules as topological graph data
without fully utilizing the molecular geometry information. Whereas, the
three-dimensional (3D) spatial structure of a molecule, a.k.a molecular
geometry, is one of the most critical factors for determining molecular
physical, chemical, and biological properties. To this end, we propose a novel
Geometry Enhanced Molecular representation learning method (GEM) for Chemical
Representation Learning (ChemRL). At first, we design a geometry-based GNN
architecture that simultaneously models atoms, bonds, and bond angles in a
molecule. To be specific, we devised double graphs for a molecule: The first
one encodes the atom-bond relations; The second one encodes bond-angle
relations. Moreover, on top of the devised GNN architecture, we propose several
novel geometry-level self-supervised learning strategies to learn spatial
knowledge by utilizing the local and global molecular 3D structures. We compare
ChemRL-GEM with various state-of-the-art (SOTA) baselines on different
molecular benchmarks and exhibit that ChemRL-GEM can significantly outperform
all baselines in both regression and classification tasks. For example, the
experimental results show an overall improvement of 8.8% on average compared to
SOTA baselines on the regression tasks, demonstrating the superiority of the
proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1"&gt;Xiaomin Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lihang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jieqiong Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Donglong He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shanzhuo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingbo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haifeng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convergence of Langevin Monte Carlo in Chi-Squared and Renyi Divergence. (arXiv:2007.11612v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.11612</id>
        <link href="http://arxiv.org/abs/2007.11612"/>
        <updated>2021-07-09T01:58:28.182Z</updated>
        <summary type="html"><![CDATA[We study sampling from a target distribution $\nu_* = e^{-f}$ using the
unadjusted Langevin Monte Carlo (LMC) algorithm when the potential $f$
satisfies a strong dissipativity condition and it is first-order smooth with a
Lipschitz gradient. We prove that, initialized with a Gaussian random vector
that has sufficiently small variance, iterating the LMC algorithm for
$\widetilde{\mathcal{O}}(\lambda^2 d\epsilon^{-1})$ steps is sufficient to
reach $\epsilon$-neighborhood of the target in both Chi-squared and Renyi
divergence, where $\lambda$ is the logarithmic Sobolev constant of $\nu_*$. Our
results do not require warm-start to deal with the exponential dimension
dependency in Chi-squared divergence at initialization. In particular, for
strongly convex and first-order smooth potentials, we show that the LMC
algorithm achieves the rate estimate $\widetilde{\mathcal{O}}(d\epsilon^{-1})$
which improves the previously known rates in both of these metrics, under the
same assumptions. Translating this rate to other metrics, our results also
recover the state-of-the-art rate estimates in KL divergence, total variation
and $2$-Wasserstein distance in the same setup. Finally, as we rely on the
logarithmic Sobolev inequality, our framework covers a range of non-convex
potentials that are first-order smooth and exhibit strong convexity outside of
a compact region.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Erdogdu_M/0/1/0/all/0/1"&gt;Murat A. Erdogdu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hosseinzadeh_R/0/1/0/all/0/1"&gt;Rasa Hosseinzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Matthew S. Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AC-VRNN: Attentive Conditional-VRNN for Multi-Future Trajectory Prediction. (arXiv:2005.08307v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.08307</id>
        <link href="http://arxiv.org/abs/2005.08307"/>
        <updated>2021-07-09T01:58:28.173Z</updated>
        <summary type="html"><![CDATA[Anticipating human motion in crowded scenarios is essential for developing
intelligent transportation systems, social-aware robots and advanced video
surveillance applications. A key component of this task is represented by the
inherently multi-modal nature of human paths which makes socially acceptable
multiple futures when human interactions are involved. To this end, we propose
a generative architecture for multi-future trajectory predictions based on
Conditional Variational Recurrent Neural Networks (C-VRNNs). Conditioning
mainly relies on prior belief maps, representing most likely moving directions
and forcing the model to consider past observed dynamics in generating future
positions. Human interactions are modeled with a graph-based attention
mechanism enabling an online attentive hidden state refinement of the recurrent
estimation. To corroborate our model, we perform extensive experiments on
publicly-available datasets (e.g., ETH/UCY, Stanford Drone Dataset, STATS
SportVU NBA, Intersection Drone Dataset and TrajNet++) and demonstrate its
effectiveness in crowded scenes compared to several state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bertugli_A/0/1/0/all/0/1"&gt;Alessia Bertugli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calderara_S/0/1/0/all/0/1"&gt;Simone Calderara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coscia_P/0/1/0/all/0/1"&gt;Pasquale Coscia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ballan_L/0/1/0/all/0/1"&gt;Lamberto Ballan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1"&gt;Rita Cucchiara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond BatchNorm: Towards a General Understanding of Normalization in Deep Learning. (arXiv:2106.05956v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05956</id>
        <link href="http://arxiv.org/abs/2106.05956"/>
        <updated>2021-07-09T01:58:27.809Z</updated>
        <summary type="html"><![CDATA[Inspired by BatchNorm, there has been an explosion of normalization layers
for deep neural networks (DNNs). However, these alternative normalization
layers have seen minimal use, partially due to a lack of guiding principles
that can help identify when these layers can serve as a replacement for
BatchNorm. To address this problem, we take a theoretical approach,
generalizing the known beneficial mechanisms of BatchNorm to several recently
proposed normalization techniques. Our generalized theory leads to the
following set of principles: (i) similar to BatchNorm, activations-based
normalization layers can prevent exponential growth of activations in ResNets,
but parametric layers require explicit remedies; (ii) use of GroupNorm can
ensure informative forward propagation, with different samples being assigned
dissimilar activations, but increasing group size results in increasingly
indistinguishable activations for different samples, explaining slow
convergence speed in models with LayerNorm; (iii) small group sizes result in
large gradient norm in earlier layers, hence explaining training instability
issues in Instance Normalization and illustrating a speed-stability tradeoff in
GroupNorm. Overall, our analysis reveals a unified set of mechanisms that
underpin the success of normalization methods in deep learning, providing us
with a compass to systematically explore the vast design space of DNN
normalization layers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lubana_E/0/1/0/all/0/1"&gt;Ekdeep Singh Lubana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dick_R/0/1/0/all/0/1"&gt;Robert P. Dick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tanaka_H/0/1/0/all/0/1"&gt;Hidenori Tanaka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GPNAS: A Neural Network Architecture Search Framework Based on Graphical Predictor. (arXiv:2103.11820v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11820</id>
        <link href="http://arxiv.org/abs/2103.11820"/>
        <updated>2021-07-09T01:58:27.802Z</updated>
        <summary type="html"><![CDATA[In practice, the problems encountered in Neural Architecture Search (NAS)
training are not simple problems, but often a series of difficult combinations
(wrong compensation estimation, curse of dimension, overfitting, high
complexity, etc.). In this paper, we propose a framework to decouple network
structure from operator search space, and use two BOHBs to search
alternatively. Considering that activation function and initialization are also
important parts of neural network, the generalization ability of the model will
be affected. We introduce an activation function and an initialization method
domain, and add them into the operator search space to form a generalized
search space, so as to improve the generalization ability of the child model.
We then trained a GCN-based predictor using feedback from the child model. This
can not only improve the search efficiency, but also solve the problem of
dimension curse. Next, unlike other NAS studies, we used predictors to analyze
the stability of different network structures. Finally, we applied our
framework to neural structure search and achieved significant improvements on
multiple datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ai_D/0/1/0/all/0/1"&gt;Dige Ai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchpress: a scalable and platform-independent workflow for benchmarking structure learning algorithms for graphical models. (arXiv:2107.03863v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.03863</id>
        <link href="http://arxiv.org/abs/2107.03863"/>
        <updated>2021-07-09T01:58:27.796Z</updated>
        <summary type="html"><![CDATA[Describing the relationship between the variables in a study domain and
modelling the data generating mechanism is a fundamental problem in many
empirical sciences. Probabilistic graphical models are one common approach to
tackle the problem. Learning the graphical structure is computationally
challenging and a fervent area of current research with a plethora of
algorithms being developed. To facilitate the benchmarking of different
methods, we present a novel automated workflow, called benchpress for producing
scalable, reproducible, and platform-independent benchmarks of structure
learning algorithms for probabilistic graphical models. Benchpress is
interfaced via a simple JSON-file, which makes it accessible for all users,
while the code is designed in a fully modular fashion to enable researchers to
contribute additional methodologies. Benchpress currently provides an interface
to a large number of state-of-the-art algorithms from libraries such as BiDAG,
bnlearn, GOBNILP, pcalg, r.blip, scikit-learn, TETRAD, and trilearn as well as
a variety of methods for data generating models and performance evaluation.
Alongside user-defined models and randomly generated datasets, the software
tool also includes a number of standard datasets and graphical models from the
literature, which may be included in a benchmarking workflow. We demonstrate
the applicability of this workflow for learning Bayesian networks in four
typical data scenarios. The source code and documentation is publicly available
from this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Rios_F/0/1/0/all/0/1"&gt;Felix L. Rios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Moffa_G/0/1/0/all/0/1"&gt;Giusi Moffa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kuipers_J/0/1/0/all/0/1"&gt;Jack Kuipers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Neural Scene Representations for Visuomotor Control. (arXiv:2107.04004v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.04004</id>
        <link href="http://arxiv.org/abs/2107.04004"/>
        <updated>2021-07-09T01:58:27.789Z</updated>
        <summary type="html"><![CDATA[Humans have a strong intuitive understanding of the 3D environment around us.
The mental model of the physics in our brain applies to objects of different
materials and enables us to perform a wide range of manipulation tasks that are
far beyond the reach of current robots. In this work, we desire to learn models
for dynamic 3D scenes purely from 2D visual observations. Our model combines
Neural Radiance Fields (NeRF) and time contrastive learning with an
autoencoding framework, which learns viewpoint-invariant 3D-aware scene
representations. We show that a dynamics model, constructed over the learned
representation space, enables visuomotor control for challenging manipulation
tasks involving both rigid bodies and fluids, where the target is specified in
a viewpoint different from what the robot operates on. When coupled with an
auto-decoding framework, it can even support goal specification from camera
viewpoints that are outside the training distribution. We further demonstrate
the richness of the learned 3D dynamics model by performing future prediction
and novel view synthesis. Finally, we provide detailed ablation studies
regarding different system designs and qualitative analysis of the learned
representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yunzhu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sitzmann_V/0/1/0/all/0/1"&gt;Vincent Sitzmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1"&gt;Pulkit Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1"&gt;Antonio Torralba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identification and Adaptation with Binary-Valued Observations under Non-Persistent Excitation Condition. (arXiv:2107.03588v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2107.03588</id>
        <link href="http://arxiv.org/abs/2107.03588"/>
        <updated>2021-07-09T01:58:27.770Z</updated>
        <summary type="html"><![CDATA[Dynamical systems with binary-valued observations are widely used in
information industry, technology of biological pharmacy and other fields.
Though there have been much efforts devoted to the identification of such
systems, most of the previous investigations are based on first-order gradient
algorithm which usually has much slower convergence rate than the Quasi-Newton
algorithm. Moreover, persistence of excitation(PE) conditions are usually
required to guarantee consistent parameter estimates in the existing
literature, which are hard to be verified or guaranteed for feedback control
systems. In this paper, we propose an online projected Quasi-Newton type
algorithm for parameter estimation of stochastic regression models with
binary-valued observations and varying thresholds. By using both the stochastic
Lyapunov function and martingale estimation methods, we establish the strong
consistency of the estimation algorithm and provide the convergence rate, under
a signal condition which is considerably weaker than the traditional PE
condition and coincides with the weakest possible excitation known for the
classical least square algorithm of stochastic regression models. Convergence
of adaptive predictors and their applications in adaptive control are also
discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lantian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yanlong Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Guo_L/0/1/0/all/0/1"&gt;Lei Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Conditioned Imitation Learning over Unstructured Data. (arXiv:2005.07648v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.07648</id>
        <link href="http://arxiv.org/abs/2005.07648"/>
        <updated>2021-07-09T01:58:27.761Z</updated>
        <summary type="html"><![CDATA[Natural language is perhaps the most flexible and intuitive way for humans to
communicate tasks to a robot. Prior work in imitation learning typically
requires each task be specified with a task id or goal image -- something that
is often impractical in open-world environments. On the other hand, previous
approaches in instruction following allow agent behavior to be guided by
language, but typically assume structure in the observations, actuators, or
language that limit their applicability to complex settings like robotics. In
this work, we present a method for incorporating free-form natural language
conditioning into imitation learning. Our approach learns perception from
pixels, natural language understanding, and multitask continuous control
end-to-end as a single neural network. Unlike prior work in imitation learning,
our method is able to incorporate unlabeled and unstructured demonstration data
(i.e. no task or language labels). We show this dramatically improves language
conditioned performance, while reducing the cost of language annotation to less
than 1% of total data. At test time, a single language conditioned visuomotor
policy trained with our method can perform a wide variety of robotic
manipulation skills in a 3D environment, specified only with natural language
descriptions of each task (e.g. "open the drawer...now pick up the block...now
press the green button..."). To scale up the number of instructions an agent
can follow, we propose combining text conditioned policies with large
pretrained neural language models. We find this allows a policy to be robust to
many out-of-distribution synonym instructions, without requiring new
demonstrations. See videos of a human typing live text commands to our agent at
language-play.github.io]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lynch_C/0/1/0/all/0/1"&gt;Corey Lynch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sermanet_P/0/1/0/all/0/1"&gt;Pierre Sermanet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Direct detection of plasticity onset through total-strain profile evolution. (arXiv:2107.03738v1 [cond-mat.mtrl-sci])]]></title>
        <id>http://arxiv.org/abs/2107.03738</id>
        <link href="http://arxiv.org/abs/2107.03738"/>
        <updated>2021-07-09T01:58:27.755Z</updated>
        <summary type="html"><![CDATA[Plastic yielding in solids strongly depends on various conditions, such as
temperature and loading rate and indeed, sample-dependent knowledge of yield
points in structural materials promotes reliability in mechanical behavior.
Commonly, yielding is measured through controlled mechanical testing at small
or large scales, in ways that either distinguish elastic (stress) from total
deformation measurements, or by identifying plastic slip contributions. In this
paper we argue that instead of separate elastic/plastic measurements, yielding
can be unraveled through statistical analysis of total strain fluctuations
during the evolution sequence of profiles measured in-situ, through digital
image correlation. We demonstrate two distinct ways of precisely quantifying
yield locations in widely applicable crystal plasticity models, that apply in
polycrystalline solids, either by using principal component analysis or
discrete wavelet transforms. We test and compare these approaches in synthetic
data of polycrystal simulations and a variety of yielding responses, through
changes of the applied loading rates and the strain-rate sensitivity exponents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Papanikolaou_S/0/1/0/all/0/1"&gt;Stefanos Papanikolaou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Alava_M/0/1/0/all/0/1"&gt;Mikko J. Alava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Offline Meta-Reinforcement Learning with Online Self-Supervision. (arXiv:2107.03974v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03974</id>
        <link href="http://arxiv.org/abs/2107.03974"/>
        <updated>2021-07-09T01:58:27.748Z</updated>
        <summary type="html"><![CDATA[Meta-reinforcement learning (RL) can be used to train policies that quickly
adapt to new tasks with orders of magnitude less data than standard RL, but
this fast adaptation often comes at the cost of greatly increasing the amount
of reward supervision during meta-training time. Offline meta-RL removes the
need to continuously provide reward supervision because rewards must only be
provided once when the offline dataset is generated. In addition to the
challenges of offline RL, a unique distribution shift is present in meta RL:
agents learn exploration strategies that can gather the experience needed to
learn a new task, and also learn adaptation strategies that work well when
presented with the trajectories in the dataset, but the adaptation strategies
are not adapted to the data distribution that the learned exploration
strategies collect. Unlike the online setting, the adaptation and exploration
strategies cannot effectively adapt to each other, resulting in poor
performance. In this paper, we propose a hybrid offline meta-RL algorithm,
which uses offline data with rewards to meta-train an adaptive policy, and then
collects additional unsupervised online data, without any ground truth reward
labels, to bridge this distribution shift problem. Our method uses the offline
data to learn the distribution of reward functions, which is then sampled to
self-supervise reward labels for the additional online data. By removing the
need to provide reward labels for the online experience, our approach can be
more practical to use in settings where reward supervision would otherwise be
provided manually. We compare our method to prior work on offline meta-RL on
simulated robot locomotion and manipulation tasks and find that using
additional data and self-generated rewards significantly improves an agent's
ability to generalize.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pong_V/0/1/0/all/0/1"&gt;Vitchyr H. Pong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1"&gt;Ashvin Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_L/0/1/0/all/0/1"&gt;Laura Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Catherine Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Management of Resource at the Network Edge for Federated Learning. (arXiv:2107.03428v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2107.03428</id>
        <link href="http://arxiv.org/abs/2107.03428"/>
        <updated>2021-07-09T01:58:27.731Z</updated>
        <summary type="html"><![CDATA[Federated learning has been explored as a promising solution for training at
the edge, where end devices collaborate to train models without sharing data
with other entities. Since the execution of these learning models occurs at the
edge, where resources are limited, new solutions must be developed. In this
paper, we describe the recent work on resource management at the edge, and
explore the challenges and future directions to allow the execution of
federated learning at the edge. Some of the problems of this management, such
as discovery of resources, deployment, load balancing, migration, and energy
efficiency will be discussed in the paper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trindade_S/0/1/0/all/0/1"&gt;Silvana Trindade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bittencourt_L/0/1/0/all/0/1"&gt;Luiz F. Bittencourt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fonseca_N/0/1/0/all/0/1"&gt;Nelson L. S. da Fonseca&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification. (arXiv:2107.03423v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03423</id>
        <link href="http://arxiv.org/abs/2107.03423"/>
        <updated>2021-07-09T01:58:27.725Z</updated>
        <summary type="html"><![CDATA[Machine learning solutions for pattern classification problems are nowadays
widely deployed in society and industry. However, the lack of transparency and
accountability of most accurate models often hinders their meaningful and safe
use. Thus, there is a clear need for developing explainable artificial
intelligence mechanisms. There exist model-agnostic methods that summarize
feature contributions, but their interpretability is limited to specific
predictions made by black-box models. An open challenge is to develop models
that have intrinsic interpretability and produce their own explanations, even
for classes of models that are traditionally considered black boxes like
(recurrent) neural networks. In this paper, we propose an LTCN-based model for
interpretable pattern classification of structured data. Our method brings its
own mechanism for providing explanations by quantifying the relevance of each
feature in the decision process. For supporting the interpretability without
affecting the performance, the model incorporates more flexibility through a
quasi-nonlinear reasoning rule that allows controlling nonlinearity. Besides,
we propose a recurrence-aware decision model that evades the issues posed by
unique fixed points while introducing a deterministic learning method to
compute the learnable parameters. The simulations show that our interpretable
model obtains competitive performance when compared to the state-of-the-art
white and black boxes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Napoles_G/0/1/0/all/0/1"&gt;Gonzalo N&amp;#xe1;poles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salgueiro_Y/0/1/0/all/0/1"&gt;Yamisleydi Salgueiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grau_I/0/1/0/all/0/1"&gt;Isel Grau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Espinosa_M/0/1/0/all/0/1"&gt;Maikel Leon Espinosa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Generative Model for Raw Audio Using Transformer Architectures. (arXiv:2106.16036v3 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.16036</id>
        <link href="http://arxiv.org/abs/2106.16036"/>
        <updated>2021-07-09T01:58:27.719Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel way of doing audio synthesis at the waveform
level using Transformer architectures. We propose a deep neural network for
generating waveforms, similar to wavenet. This is fully probabilistic,
auto-regressive, and causal, i.e. each sample generated depends only on the
previously observed samples. Our approach outperforms a widely used wavenet
architecture by up to 9% on a similar dataset for predicting the next step.
Using the attention mechanism, we enable the architecture to learn which audio
samples are important for the prediction of the future sample. We show how
causal transformer generative models can be used for raw waveform synthesis. We
also show that this performance can be improved by another 2% by conditioning
samples over a wider context. The flexibility of the current model to
synthesize audio from latent representations suggests a large number of
potential applications. The novel approach of using generative transformer
architectures for raw audio synthesis is, however, still far away from
generating any meaningful music, without using latent codes/meta-data to aid
the generation process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1"&gt;Prateek Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chafe_C/0/1/0/all/0/1"&gt;Chris Chafe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[No-Reference Quality Assessment for Colored Point Cloud and Mesh Based on Natural Scene Statistics. (arXiv:2107.02041v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02041</id>
        <link href="http://arxiv.org/abs/2107.02041"/>
        <updated>2021-07-09T01:58:27.712Z</updated>
        <summary type="html"><![CDATA[To improve the viewer's quality of experience and optimize processing systems
in computer graphics applications, the 3D quality assessment (3D-QA) has become
an important task in the multimedia area. Point cloud and mesh are the two most
widely used electronic representation formats of 3D models, the quality of
which is quite sensitive to operations like simplification and compression.
Therefore, many studies concerning point cloud quality assessment (PCQA) and
mesh quality assessment (MQA) have been carried out to measure the visual
quality degradations caused by lossy operations. However, a large part of
previous studies utilizes full-reference (FR) metrics, which means they may
fail to predict the accurate quality level of 3D models when the reference 3D
model is not available. Furthermore, limited numbers of 3D-QA metrics are
carried out to take color features into consideration, which significantly
restricts the effectiveness and scope of application. In many quality
assessment studies, natural scene statistics (NSS) have shown a good ability to
quantify the distortion of natural scenes to statistical parameters. Therefore,
we propose an NSS-based no-reference quality assessment metric for colored 3D
models. In this paper, quality-aware features are extracted from the aspects of
color and geometry directly from the 3D models. Then the statistic parameters
are estimated using different distribution models to describe the
characteristic of the 3D models. Our method is mainly validated on the colored
point cloud quality assessment database (SJTU-PCQA) and the colored mesh
quality assessment database (CMDM). The experimental results show that the
proposed method outperforms all the state-of-art NR 3D-QA metrics and obtains
an acceptable gap with the state-of-art FR 3D-QA metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zicheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+sun_W/0/1/0/all/0/1"&gt;Wei sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1"&gt;Xiongkuo Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wei Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1"&gt;Guangtao Zhai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse and Structured Visual Attention. (arXiv:2002.05556v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.05556</id>
        <link href="http://arxiv.org/abs/2002.05556"/>
        <updated>2021-07-09T01:58:27.706Z</updated>
        <summary type="html"><![CDATA[Visual attention mechanisms are widely used in multimodal tasks, as visual
question answering (VQA). One drawback of softmax-based attention mechanisms is
that they assign some probability mass to all image regions, regardless of
their adjacency structure and of their relevance to the text. In this paper, to
better link the image structure with the text, we replace the traditional
softmax attention mechanism with two alternative sparsity-promoting
transformations: sparsemax, which is able to select only the relevant regions
(assigning zero weight to the rest), and a newly proposed Total-Variation
Sparse Attention (TVmax), which further encourages the joint selection of
adjacent spatial locations. Experiments in VQA show gains in accuracy as well
as higher similarity to human attention, which suggests better
interpretability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Martins_P/0/1/0/all/0/1"&gt;Pedro Henrique Martins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niculae_V/0/1/0/all/0/1"&gt;Vlad Niculae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marinho_Z/0/1/0/all/0/1"&gt;Zita Marinho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Martins&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Energy Efficient Federated Learning in Integrated Fog-Cloud Computing Enabled Internet-of-Things Networks. (arXiv:2107.03520v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.03520</id>
        <link href="http://arxiv.org/abs/2107.03520"/>
        <updated>2021-07-09T01:58:27.699Z</updated>
        <summary type="html"><![CDATA[We investigate resource allocation scheme to reduce the energy consumption of
federated learning (FL) in the integrated fog-cloud computing enabled
Internet-of-things (IoT) networks. In the envisioned system, IoT devices are
connected with the centralized cloud server (CS) via multiple fog access points
(F-APs). We consider two different scenarios for training the local models. In
the first scenario, local models are trained at the IoT devices and the F-APs
upload the local model parameters to the CS. In the second scenario, local
models are trained at the F-APs based on the collected data from the IoT
devices and the F-APs collaborate with the CS for updating the model
parameters. Our objective is to minimize the overall energy-consumption of both
scenarios subject to FL time constraint. Towards this goal, we devise a joint
optimization of scheduling of IoT devices with the F-APs, transmit power
allocation, computation frequency allocation at the devices and F-APs and
decouple it into two subproblems. In the first subproblem, we optimize the IoT
device scheduling and power allocation, while in the second subproblem, we
optimize the computation frequency allocation. For each scenario, we develop a
conflict graph based solution to iteratively solve the two subproblems.
Simulation results show that the proposed two schemes achieve a considerable
performance gain in terms of the energy consumption minimization. The presented
simulation results interestingly reveal that for a large number of IoT devices
and large data sizes, it is more energy efficient to train the local models at
the IoT devices instead of the F-APs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Al_Abiad_M/0/1/0/all/0/1"&gt;Mohammed S. Al-Abiad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hassan_M/0/1/0/all/0/1"&gt;Md. Zoheb Hassan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hossain_M/0/1/0/all/0/1"&gt;Md. Jahangir Hossain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OXnet: Omni-supervised Thoracic Disease Detection from Chest X-rays. (arXiv:2104.03218v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03218</id>
        <link href="http://arxiv.org/abs/2104.03218"/>
        <updated>2021-07-09T01:58:27.692Z</updated>
        <summary type="html"><![CDATA[Chest X-ray (CXR) is the most typical diagnostic X-ray examination for
screening various thoracic diseases. Automatically localizing lesions from CXR
is promising for alleviating radiologists' reading burden. However, CXR
datasets are often with massive image-level annotations and scarce lesion-level
annotations, and more often, without annotations. Thus far, unifying different
supervision granularities to develop thoracic disease detection algorithms has
not been comprehensively addressed. In this paper, we present OXnet, the first
deep omni-supervised thoracic disease detection network to our best knowledge
that uses as much available supervision as possible for CXR diagnosis. We first
introduce supervised learning via a one-stage detection model. Then, we inject
a global classification head to the detection model and propose dual attention
alignment to guide the global gradient to the local detection branch, which
enables learning lesion detection from image-level annotations. We also impose
intra-class compactness and inter-class separability with global prototype
alignment to further enhance the global information learning. Moreover, we
leverage a soft focal loss to distill the soft pseudo-labels of unlabeled data
generated by a teacher model. Extensive experiments on a large-scale chest
X-ray dataset show the proposed OXnet outperforms competitive methods with
significant margins. Further, we investigate omni-supervision under various
annotation granularities and corroborate OXnet is a promising choice to
mitigate the plight of annotation shortage for medical image diagnosis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1"&gt;Luyang Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yanning Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Huangjing Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pheng_P/0/1/0/all/0/1"&gt;Pheng-Ann Pheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Long Short-Term Memory for AI Applications in Spike-based Neuromorphic Hardware. (arXiv:2107.03992v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2107.03992</id>
        <link href="http://arxiv.org/abs/2107.03992"/>
        <updated>2021-07-09T01:58:27.686Z</updated>
        <summary type="html"><![CDATA[In spite of intensive efforts it has remained an open problem to what extent
current Artificial Intelligence (AI) methods that employ Deep Neural Networks
(DNNs) can be implemented more energy-efficiently on spike-based neuromorphic
hardware. This holds in particular for AI methods that solve sequence
processing tasks, a primary application target for spike-based neuromorphic
hardware. One difficulty is that DNNs for such tasks typically employ Long
Short-Term Memory (LSTM) units. Yet an efficient emulation of these units in
spike-based hardware has been missing. We present a biologically inspired
solution that solves this problem. This solution enables us to implement a
major class of DNNs for sequence processing tasks such as time series
classification and question answering with substantial energy savings on
neuromorphic hardware. In fact, the Relational Network for reasoning about
relations between objects that we use for question answering is the first
example of a large DNN that carries out a sequence processing task with
substantial energy-saving on neuromorphic hardware.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Plank_P/0/1/0/all/0/1"&gt;Philipp Plank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1"&gt;Arjun Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wild_A/0/1/0/all/0/1"&gt;Andreas Wild&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maass_W/0/1/0/all/0/1"&gt;Wolfgang Maass&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Likelihood-Free Frequentist Inference: Bridging Classical Statistics and Machine Learning in Simulation and Uncertainty Quantification. (arXiv:2107.03920v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.03920</id>
        <link href="http://arxiv.org/abs/2107.03920"/>
        <updated>2021-07-09T01:58:27.643Z</updated>
        <summary type="html"><![CDATA[Many areas of science make extensive use of computer simulators that
implicitly encode likelihood functions for complex systems. Classical
statistical methods are poorly suited for these so-called likelihood-free
inference (LFI) settings, outside the asymptotic and low-dimensional regimes.
Although new machine learning methods, such as normalizing flows, have
revolutionized the sample efficiency and capacity of LFI methods, it remains an
open question whether they produce reliable measures of uncertainty. In this
paper, we present a statistical framework for LFI that unifies classical
statistics with modern machine learning to: (1) construct frequentist
confidence sets and hypothesis tests with finite-sample guarantees of nominal
coverage (type I error control) and power, and (2) provide rigorous diagnostics
for assessing empirical coverage over the entire parameter space. We refer to
our framework as likelihood-free frequentist inference (LF2I). Any method that
estimates a test statistic, such as the likelihood ratio, can be plugged into
our framework to create powerful tests and confidence sets with correct
coverage. In this work, we specifically study two test statistics (ACORE and
BFF), which, respectively, maximize versus integrate an odds function over the
parameter space. Our theoretical and empirical results offer multifaceted
perspectives on error sources and challenges in likelihood-free frequentist
inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Dalmasso_N/0/1/0/all/0/1"&gt;Niccol&amp;#xf2; Dalmasso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhao_D/0/1/0/all/0/1"&gt;David Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Izbicki_R/0/1/0/all/0/1"&gt;Rafael Izbicki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lee_A/0/1/0/all/0/1"&gt;Ann B. Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Matrix Sketching for Secure Collaborative Machine Learning. (arXiv:1909.11201v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.11201</id>
        <link href="http://arxiv.org/abs/1909.11201"/>
        <updated>2021-07-09T01:58:27.634Z</updated>
        <summary type="html"><![CDATA[Collaborative learning allows participants to jointly train a model without
data sharing. To update the model parameters, the central server broadcasts
model parameters to the clients, and the clients send updating directions such
as gradients to the server. While data do not leave a client device, the
communicated gradients and parameters will leak a client's privacy. Attacks
that infer clients' privacy from gradients and parameters have been developed
by prior work. Simple defenses such as dropout and differential privacy either
fail to defend the attacks or seriously hurt test accuracy.

We propose a practical defense which we call Double-Blind Collaborative
Learning (DBCL). The high-level idea is to apply random matrix sketching to the
parameters (aka weights) and re-generate random sketching after each iteration.
DBCL prevents clients from conducting gradient-based privacy inferences which
are the most effective attacks. DBCL works because from the attacker's
perspective, sketching is effectively random noise that outweighs the signal.
Notably, DBCL does not much increase computation and communication costs and
does not hurt test accuracy at all.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mengjiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shusen Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Design Smells in Deep Learning Programs: An Empirical Study. (arXiv:2107.02279v2 [cs.SE] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2107.02279</id>
        <link href="http://arxiv.org/abs/2107.02279"/>
        <updated>2021-07-09T01:58:27.622Z</updated>
        <summary type="html"><![CDATA[Nowadays, we are witnessing an increasing adoption of Deep Learning (DL)
based software systems in many industries. Designing a DL program requires
constructing a deep neural network (DNN) and then training it on a dataset.
This process requires that developers make multiple architectural (e.g., type,
size, number, and order of layers) and configuration (e.g., optimizer,
regularization methods, and activation functions) choices that affect the
quality of the DL models, and consequently software quality. An under-specified
or poorly-designed DL model may train successfully but is likely to perform
poorly when deployed in production. Design smells in DL programs are poor
design and-or configuration decisions taken during the development of DL
components, that are likely to have a negative impact on the performance (i.e.,
prediction accuracy) and then quality of DL based software systems. In this
paper, we present a catalogue of 8 design smells for a popular DL architecture,
namely deep Feedforward Neural Networks which is widely employed in industrial
applications. The design smells were identified through a review of the
existing literature on DL design and a manual inspection of 659 DL programs
with performance issues and design inefficiencies. The smells are specified by
describing their context, consequences, and recommended refactorings. To
provide empirical evidence on the relevance and perceived impact of the
proposed design smells, we conducted a survey with 81 DL developers. In
general, the developers perceived the proposed design smells as reflective of
design or implementation problems, with agreement levels varying between 47\%
and 68\%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nikanjam_A/0/1/0/all/0/1"&gt;Amin Nikanjam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1"&gt;Foutse Khomh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training. (arXiv:2103.06561v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06561</id>
        <link href="http://arxiv.org/abs/2103.06561"/>
        <updated>2021-07-09T01:58:27.613Z</updated>
        <summary type="html"><![CDATA[Multi-modal pre-training models have been intensively explored to bridge
vision and language in recent years. However, most of them explicitly model the
cross-modal interaction between image-text pairs, by assuming that there exists
strong semantic correlation between the text and image modalities. Since this
strong assumption is often invalid in real-world scenarios, we choose to
implicitly model the cross-modal correlation for large-scale multi-modal
pre-training, which is the focus of the Chinese project `WenLan' led by our
team. Specifically, with the weak correlation assumption over image-text pairs,
we propose a two-tower pre-training model called BriVL within the cross-modal
contrastive learning framework. Unlike OpenAI CLIP that adopts a simple
contrastive learning method, we devise a more advanced algorithm by adapting
the latest method MoCo into the cross-modal scenario. By building a large
queue-based dictionary, our BriVL can incorporate more negative samples in
limited GPU resources. We further construct a large Chinese multi-source
image-text dataset called RUC-CAS-WenLan for pre-training our BriVL model.
Extensive experiments demonstrate that the pre-trained BriVL model outperforms
both UNITER and OpenAI CLIP on various downstream tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1"&gt;Yuqi Huo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Manli Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guangzhen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Haoyu Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yizhao Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Guoxing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Jingyuan Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Heng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1"&gt;Baogui Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1"&gt;Weihao Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1"&gt;Zongzheng Xi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yueqian Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1"&gt;Anwen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jinming Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruichen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yida Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yuqing Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1"&gt;Xin Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1"&gt;Wanqing Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_D/0/1/0/all/0/1"&gt;Danyang Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yingyan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Junyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Peiyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1"&gt;Zheng Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1"&gt;Chuhao Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yuchong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shizhe Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zhiwu Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1"&gt;Zhicheng Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1"&gt;Qin Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1"&gt;Yanyan Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wayne Xin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1"&gt;Ruihua Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Ji-Rong Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Task Fingerprinting for Meta Learning in Biomedical Image Analysis. (arXiv:2107.03949v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03949</id>
        <link href="http://arxiv.org/abs/2107.03949"/>
        <updated>2021-07-09T01:58:27.588Z</updated>
        <summary type="html"><![CDATA[Shortage of annotated data is one of the greatest bottlenecks in biomedical
image analysis. Meta learning studies how learning systems can increase in
efficiency through experience and could thus evolve as an important concept to
overcome data sparsity. However, the core capability of meta learning-based
approaches is the identification of similar previous tasks given a new task - a
challenge largely unexplored in the biomedical imaging domain. In this paper,
we address the problem of quantifying task similarity with a concept that we
refer to as task fingerprinting. The concept involves converting a given task,
represented by imaging data and corresponding labels, to a fixed-length vector
representation. In fingerprint space, different tasks can be directly compared
irrespective of their data set sizes, types of labels or specific resolutions.
An initial feasibility study in the field of surgical data science (SDS) with
26 classification tasks from various medical and non-medical domains suggests
that task fingerprinting could be leveraged for both (1) selecting appropriate
data sets for pretraining and (2) selecting appropriate architectures for a new
task. Task fingerprinting could thus become an important tool for meta learning
in SDS and other fields of biomedical image analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Godau_P/0/1/0/all/0/1"&gt;Patrick Godau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maier_Hein_L/0/1/0/all/0/1"&gt;Lena Maier-Hein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Malware Classification Using Deep Boosted Learning. (arXiv:2107.04008v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.04008</id>
        <link href="http://arxiv.org/abs/2107.04008"/>
        <updated>2021-07-09T01:58:27.569Z</updated>
        <summary type="html"><![CDATA[Malicious activities in cyberspace have gone further than simply hacking
machines and spreading viruses. It has become a challenge for a nations
survival and hence has evolved to cyber warfare. Malware is a key component of
cyber-crime, and its analysis is the first line of defence against attack. This
work proposes a novel deep boosted hybrid learning-based malware classification
framework and named as Deep boosted Feature Space-based Malware classification
(DFS-MC). In the proposed framework, the discrimination power is enhanced by
fusing the feature spaces of the best performing customized CNN architectures
models and its discrimination by an SVM for classification. The discrimination
capacity of the proposed classification framework is assessed by comparing it
against the standard customized CNNs. The customized CNN models are implemented
in two ways: softmax classifier and deep hybrid learning-based malware
classification. In the hybrid learning, Deep features are extracted from
customized CNN architectures and fed into the conventional machine learning
classifier to improve the classification performance. We also introduced the
concept of transfer learning in a customized CNN architecture based malware
classification framework through fine-tuning. The performance of the proposed
malware classification approaches are validated on the MalImg malware dataset
using the hold-out cross-validation technique. Experimental comparisons were
conducted by employing innovative, customized CNN, trained from scratch and
fine-tuning the customized CNN using transfer learning. The proposed
classification framework DFS-MC showed improved results, Accuracy: 98.61%,
F-score: 0.96, Precision: 0.96, and Recall: 0.96.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Asam_M/0/1/0/all/0/1"&gt;Muhammad Asam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1"&gt;Saddam Hussain Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jamal_T/0/1/0/all/0/1"&gt;Tauseef Jamal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zahoora_U/0/1/0/all/0/1"&gt;Umme Zahoora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1"&gt;Asifullah Khan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Digitizing Handwriting with a Sensor Pen: A Writer-Independent Recognizer. (arXiv:2107.03704v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03704</id>
        <link href="http://arxiv.org/abs/2107.03704"/>
        <updated>2021-07-09T01:58:27.552Z</updated>
        <summary type="html"><![CDATA[Online handwriting recognition has been studied for a long time with only few
practicable results when writing on normal paper. Previous approaches using
sensor-based devices encountered problems that limited the usage of the
developed systems in real-world applications. This paper presents a
writer-independent system that recognizes characters written on plain paper
with the use of a sensor-equipped pen. This system is applicable in real-world
applications and requires no user-specific training for recognition. The pen
provides linear acceleration, angular velocity, magnetic field, and force
applied by the user, and acts as a digitizer that transforms the analogue
signals of the sensors into timeseries data while writing on regular paper. The
dataset we collected with this pen consists of Latin lower-case and upper-case
alphabets. We present the results of a convolutional neural network model for
letter classification and show that this approach is practical and achieves
promising results for writer-independent character recognition. This work aims
at providing a realtime handwriting recognition system to be used for writing
on normal paper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wehbi_M/0/1/0/all/0/1"&gt;Mohamad Wehbi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamann_T/0/1/0/all/0/1"&gt;Tim Hamann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barth_J/0/1/0/all/0/1"&gt;Jens Barth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eskofier_B/0/1/0/all/0/1"&gt;Bjoern Eskofier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-stationary Reinforcement Learning without Prior Knowledge: An Optimal Black-box Approach. (arXiv:2102.05406v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05406</id>
        <link href="http://arxiv.org/abs/2102.05406"/>
        <updated>2021-07-09T01:58:27.546Z</updated>
        <summary type="html"><![CDATA[We propose a black-box reduction that turns a certain reinforcement learning
algorithm with optimal regret in a (near-)stationary environment into another
algorithm with optimal dynamic regret in a non-stationary environment,
importantly without any prior knowledge on the degree of non-stationarity. By
plugging different algorithms into our black-box, we provide a list of examples
showing that our approach not only recovers recent results for (contextual)
multi-armed bandits achieved by very specialized algorithms, but also
significantly improves the state of the art for (generalized) linear bandits,
episodic MDPs, and infinite-horizon MDPs in various ways. Specifically, in most
cases our algorithm achieves the optimal dynamic regret
$\widetilde{\mathcal{O}}(\min\{\sqrt{LT}, \Delta^{1/3}T^{2/3}\})$ where $T$ is
the number of rounds and $L$ and $\Delta$ are the number and amount of changes
of the world respectively, while previous works only obtain suboptimal bounds
and/or require the knowledge of $L$ and $\Delta$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1"&gt;Chen-Yu Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Haipeng Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recommendation system using a deep learning and graph analysis approach. (arXiv:2004.08100v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.08100</id>
        <link href="http://arxiv.org/abs/2004.08100"/>
        <updated>2021-07-09T01:58:27.539Z</updated>
        <summary type="html"><![CDATA[When a user connects to the Internet to fulfill his needs, he often
encounters a huge amount of related information. Recommender systems are the
techniques for massively filtering information and offering the items that
users find them satisfying and interesting. The advances in machine learning
methods, especially deep learning, have led to great achievements in
recommender systems, although these systems still suffer from challenges such
as cold-start and sparsity problems. To solve these problems, context
information such as user communication network is usually used. In this paper,
we have proposed a novel recommendation method based on Matrix Factorization
and graph analysis methods. In addition, we leverage deep Autoencoders to
initialize users and items latent factors, and deep embedding method gathers
users' latent factors from the user trust graph. The proposed method is
implemented on two standard datasets. The experimental results and comparisons
demonstrate that the proposed approach is superior to the existing
state-of-the-art recommendation methods. Our approach outperforms other
comparative methods and achieves great improvements. This work has been
submitted to the IEEE for possible publication. Copyright may be transferred
without notice, after which this version may no longer be accessible]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kherad_M/0/1/0/all/0/1"&gt;Mahdi Kherad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bidgoly_A/0/1/0/all/0/1"&gt;Amir Jalaly Bidgoly&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rating and aspect-based opinion graph embeddings for explainable recommendations. (arXiv:2107.03385v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.03385</id>
        <link href="http://arxiv.org/abs/2107.03385"/>
        <updated>2021-07-09T01:58:27.533Z</updated>
        <summary type="html"><![CDATA[The success of neural network embeddings has entailed a renewed interest in
using knowledge graphs for a wide variety of machine learning and information
retrieval tasks. In particular, recent recommendation methods based on graph
embeddings have shown state-of-the-art performance. In general, these methods
encode latent rating patterns and content features. Differently from previous
work, in this paper, we propose to exploit embeddings extracted from graphs
that combine information from ratings and aspect-based opinions expressed in
textual reviews. We then adapt and evaluate state-of-the-art graph embedding
techniques over graphs generated from Amazon and Yelp reviews on six domains,
outperforming baseline recommenders. Additionally, our method has the advantage
of providing explanations that involve the coverage of aspect-based opinions
given by users about recommended items.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cantador_I/0/1/0/all/0/1"&gt;Iv&amp;#xe1;n Cantador&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carvallo_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9;s Carvallo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diez_F/0/1/0/all/0/1"&gt;Fernando Diez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TGHop: An Explainable, Efficient and Lightweight Method for Texture Generation. (arXiv:2107.04020v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04020</id>
        <link href="http://arxiv.org/abs/2107.04020"/>
        <updated>2021-07-09T01:58:27.515Z</updated>
        <summary type="html"><![CDATA[An explainable, efficient and lightweight method for texture generation,
called TGHop (an acronym of Texture Generation PixelHop), is proposed in this
work. Although synthesis of visually pleasant texture can be achieved by deep
neural networks, the associated models are large in size, difficult to explain
in theory, and computationally expensive in training. In contrast, TGHop is
small in its model size, mathematically transparent, efficient in training and
inference, and able to generate high quality texture. Given an exemplary
texture, TGHop first crops many sample patches out of it to form a collection
of sample patches called the source. Then, it analyzes pixel statistics of
samples from the source and obtains a sequence of fine-to-coarse subspaces for
these patches by using the PixelHop++ framework. To generate texture patches
with TGHop, we begin with the coarsest subspace, which is called the core, and
attempt to generate samples in each subspace by following the distribution of
real samples. Finally, texture patches are stitched to form texture images of a
large size. It is demonstrated by experimental results that TGHop can generate
texture images of superior quality with a small model size and at a fast speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1"&gt;Xuejing Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1"&gt;Ganning Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kaitai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1"&gt;C.-C. Jay Kuo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Machine Learning Approach to Safer Airplane Landings: Predicting Runway Conditions using Weather and Flight Data. (arXiv:2107.04010v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2107.04010</id>
        <link href="http://arxiv.org/abs/2107.04010"/>
        <updated>2021-07-09T01:58:27.508Z</updated>
        <summary type="html"><![CDATA[The presence of snow and ice on runway surfaces reduces the available
tire-pavement friction needed for retardation and directional control and
causes potential economic and safety threats for the aviation industry during
the winter seasons. To activate appropriate safety procedures, pilots need
accurate and timely information on the actual runway surface conditions. In
this study, XGBoost is used to create a combined runway assessment system,
which includes a classifcation model to predict slippery conditions and a
regression model to predict the level of slipperiness. The models are trained
on weather data and data from runway reports. The runway surface conditions are
represented by the tire-pavement friction coefficient, which is estimated from
flight sensor data from landing aircrafts. To evaluate the performance of the
models, they are compared to several state-of-the-art runway assessment
methods. The XGBoost models identify slippery runway conditions with a ROC AUC
of 0.95, predict the friction coefficient with a MAE of 0.0254, and outperforms
all the previous methods. The results show the strong abilities of machine
learning methods to model complex, physical phenomena with a good accuracy when
domain knowledge is used in the variable extraction. The XGBoost models are
combined with SHAP (SHapley Additive exPlanations) approximations to provide a
comprehensible decision support system for airport operators and pilots, which
can contribute to safer and more economic operations of airport runways.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Midtfjord_A/0/1/0/all/0/1"&gt;Alise Danielle Midtfjord&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bin_R/0/1/0/all/0/1"&gt;Riccardo De Bin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huseby_A/0/1/0/all/0/1"&gt;Arne Bang Huseby&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-organized criticality in neural networks. (arXiv:2107.03402v1 [cond-mat.stat-mech])]]></title>
        <id>http://arxiv.org/abs/2107.03402</id>
        <link href="http://arxiv.org/abs/2107.03402"/>
        <updated>2021-07-09T01:58:27.501Z</updated>
        <summary type="html"><![CDATA[We demonstrate, both analytically and numerically, that learning dynamics of
neural networks is generically attracted towards a self-organized critical
state. The effect can be modeled with quartic interactions between
non-trainable variables (e.g. states of neurons) and trainable variables (e.g.
weight matrix). Non-trainable variables are rapidly driven towards stochastic
equilibrium and trainable variables are slowly driven towards learning
equilibrium described by a scale-invariant distribution on a wide range of
scales. Our results suggest that the scale invariance observed in many physical
and biological systems might be due to some kind of learning dynamics and
support the claim that the universe might be a neural network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Katsnelson_M/0/1/0/all/0/1"&gt;Mikhail I. Katsnelson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Vanchurin_V/0/1/0/all/0/1"&gt;Vitaly Vanchurin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Westerhout_T/0/1/0/all/0/1"&gt;Tom Westerhout&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Three Ensemble Clustering (3EC) Algorithm for Pattern Discovery in Unsupervised Learning. (arXiv:2107.03729v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03729</id>
        <link href="http://arxiv.org/abs/2107.03729"/>
        <updated>2021-07-09T01:58:27.494Z</updated>
        <summary type="html"><![CDATA[This paper presents a multiple learner algorithm called the 'Three Ensemble
Clustering 3EC' algorithm that classifies unlabeled data into quality clusters
as a part of unsupervised learning. It offers the flexibility to explore the
context of new clusters formed by an ensemble of algorithms based on internal
validation indices.

It is worth mentioning that the input data set is considered to be a cluster
of clusters. An anomaly can possibly manifest as a cluster as well. Each
partitioned cluster is considered to be a new data set and is a candidate to
explore the most optimal algorithm and its number of partition splits until a
predefined stopping criteria is met. The algorithms independently partition the
data set into clusters and the quality of the partitioning is assessed by an
ensemble of internal cluster validation indices. The 3EC algorithm presents the
validation index scores from a choice of algorithms and its configuration of
partitions and it is called the Tau Grid. 3EC chooses the most optimal score.
The 3EC algorithm owes its name to the two input ensembles of algorithms and
internal validation indices and an output ensemble of final clusters.

Quality plays an important role in this clustering approach and it also acts
as a stopping criteria from further partitioning. Quality is determined based
on the quality of the clusters provided by an algorithm and its optimal number
of splits. The 3EC algorithm determines this from the score of the ensemble of
validation indices. The user can configure the stopping criteria by providing
quality thresholds for the score range of each of the validation indices and
the optimal size of the output cluster. The users can experiment with different
sets of stopping criteria and choose the most 'sensible group' of quality
clusters]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kundu/0/1/0/all/0/1"&gt;Kundu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Debasish/0/1/0/all/0/1"&gt;Debasish&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Output Randomization: A Novel Defense for both White-box and Black-box Adversarial Models. (arXiv:2107.03806v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03806</id>
        <link href="http://arxiv.org/abs/2107.03806"/>
        <updated>2021-07-09T01:58:27.476Z</updated>
        <summary type="html"><![CDATA[Adversarial examples pose a threat to deep neural network models in a variety
of scenarios, from settings where the adversary has complete knowledge of the
model in a "white box" setting and to the opposite in a "black box" setting. In
this paper, we explore the use of output randomization as a defense against
attacks in both the black box and white box models and propose two defenses. In
the first defense, we propose output randomization at test time to thwart
finite difference attacks in black box settings. Since this type of attack
relies on repeated queries to the model to estimate gradients, we investigate
the use of randomization to thwart such adversaries from successfully creating
adversarial examples. We empirically show that this defense can limit the
success rate of a black box adversary using the Zeroth Order Optimization
attack to 0%. Secondly, we propose output randomization training as a defense
against white box adversaries. Unlike prior approaches that use randomization,
our defense does not require its use at test time, eliminating the Backward
Pass Differentiable Approximation attack, which was shown to be effective
against other randomization defenses. Additionally, this defense has low
overhead and is easily implemented, allowing it to be used together with other
defenses across various model architectures. We evaluate output randomization
training against the Projected Gradient Descent attacker and show that the
defense can reduce the PGD attack's success rate down to 12% when using
cross-entropy loss.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1"&gt;Daniel Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_H/0/1/0/all/0/1"&gt;Haidar Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1"&gt;Azer Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gittens_A/0/1/0/all/0/1"&gt;Alex Gittens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yener_B/0/1/0/all/0/1"&gt;B&amp;#xfc;lent Yener&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Community detection in the sparse hypergraph stochastic block model. (arXiv:1904.05981v6 [math.PR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.05981</id>
        <link href="http://arxiv.org/abs/1904.05981"/>
        <updated>2021-07-09T01:58:27.468Z</updated>
        <summary type="html"><![CDATA[We consider the community detection problem in sparse random hypergraphs.
Angelini et al. (2015) conjectured the existence of a sharp threshold on model
parameters for community detection in sparse hypergraphs generated by a
hypergraph stochastic block model. We solve the positive part of the conjecture
for the case of two blocks: above the threshold, there is a spectral algorithm
which asymptotically almost surely constructs a partition of the hypergraph
correlated with the true partition. Our method is a generalization to random
hypergraphs of the method developed by Massouli\'{e} (2014) for sparse random
graphs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Pal_S/0/1/0/all/0/1"&gt;Soumik Pal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yizhe Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Imitation by Predicting Observations. (arXiv:2107.03851v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03851</id>
        <link href="http://arxiv.org/abs/2107.03851"/>
        <updated>2021-07-09T01:58:27.456Z</updated>
        <summary type="html"><![CDATA[Imitation learning enables agents to reuse and adapt the hard-won expertise
of others, offering a solution to several key challenges in learning behavior.
Although it is easy to observe behavior in the real-world, the underlying
actions may not be accessible. We present a new method for imitation solely
from observations that achieves comparable performance to experts on
challenging continuous control tasks while also exhibiting robustness in the
presence of observations unrelated to the task. Our method, which we call FORM
(for "Future Observation Reward Model") is derived from an inverse RL objective
and imitates using a model of expert behavior learned by generative modelling
of the expert's observations, without needing ground truth actions. We show
that FORM performs comparably to a strong baseline IRL method (GAIL) on the
DeepMind Control Suite benchmark, while outperforming GAIL in the presence of
task-irrelevant features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1"&gt;Andrew Jaegle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sulsky_Y/0/1/0/all/0/1"&gt;Yury Sulsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahuja_A/0/1/0/all/0/1"&gt;Arun Ahuja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruce_J/0/1/0/all/0/1"&gt;Jake Bruce&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fergus_R/0/1/0/all/0/1"&gt;Rob Fergus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wayne_G/0/1/0/all/0/1"&gt;Greg Wayne&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Optimized Speech Coding with Deep Neural Networks. (arXiv:1710.09064v3 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1710.09064</id>
        <link href="http://arxiv.org/abs/1710.09064"/>
        <updated>2021-07-09T01:58:27.445Z</updated>
        <summary type="html"><![CDATA[Modern compression algorithms are often the result of laborious
domain-specific research; industry standards such as MP3, JPEG, and AMR-WB took
years to develop and were largely hand-designed. We present a deep neural
network model which optimizes all the steps of a wideband speech coding
pipeline (compression, quantization, entropy coding, and decompression)
end-to-end directly from raw speech data -- no manual feature engineering
necessary, and it trains in hours. In testing, our DNN-based coder performs on
par with the AMR-WB standard at a variety of bitrates (~9kbps up to ~24kbps).
It also runs in realtime on a 3.8GhZ Intel CPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kankanahalli_S/0/1/0/all/0/1"&gt;Srihari Kankanahalli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MOD-Net: A Machine Learning Approach via Model-Operator-Data Network for Solving PDEs. (arXiv:2107.03673v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2107.03673</id>
        <link href="http://arxiv.org/abs/2107.03673"/>
        <updated>2021-07-09T01:58:27.432Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a model-operator-data network (MOD-Net) for solving
PDEs. A MOD-Net is driven by a model to solve PDEs based on operator
representation with regularization from data. In this work, we use a deep
neural network to parameterize the Green's function. The empirical risk
consists of the mean square of the governing equation, boundary conditions, and
a few labels, which are numerically computed by traditional schemes on coarse
grid points with cheap computation cost. With only the labeled dataset or only
the model constraints, it is insufficient to accurately train a MOD-Net for
complicate problems. Intuitively, the labeled dataset works as a regularization
in addition to the model constraints. The MOD-Net is much efficient than
original neural operator because the MOD-Net also uses the information of
governing equation and the boundary conditions of the PDE rather than purely
the expensive labels. Since the MOD-Net learns the Green's function of a PDE,
it solves a type of PDEs but not a specific case. We numerically show MOD-Net
is very efficient in solving Poisson equation and one-dimensional Boltzmann
equation. For non-linear PDEs, where the concept of the Green's function does
not apply, the non-linear MOD-Net can be similarly used as an ansatz for
solving non-linear PDEs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lulu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Luo_T/0/1/0/all/0/1"&gt;Tao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yaoyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhi-Qin John Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zheng Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptation of Quadruped Robot Locomotion with Meta-Learning. (arXiv:2107.03741v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.03741</id>
        <link href="http://arxiv.org/abs/2107.03741"/>
        <updated>2021-07-09T01:58:27.327Z</updated>
        <summary type="html"><![CDATA[Animals have remarkable abilities to adapt locomotion to different terrains
and tasks. However, robots trained by means of reinforcement learning are
typically able to solve only a single task and a transferred policy is usually
inferior to that trained from scratch. In this work, we demonstrate that
meta-reinforcement learning can be used to successfully train a robot capable
to solve a wide range of locomotion tasks. The performance of the meta-trained
robot is similar to that of a robot that is trained on a single task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuzhamuratov_A/0/1/0/all/0/1"&gt;Arsen Kuzhamuratov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sorokin_D/0/1/0/all/0/1"&gt;Dmitry Sorokin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ulanov_A/0/1/0/all/0/1"&gt;Alexander Ulanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lvovsky_A/0/1/0/all/0/1"&gt;A. I. Lvovsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic Time Series Forecasting with Implicit Quantile Networks. (arXiv:2107.03743v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03743</id>
        <link href="http://arxiv.org/abs/2107.03743"/>
        <updated>2021-07-09T01:58:27.320Z</updated>
        <summary type="html"><![CDATA[Here, we propose a general method for probabilistic time series forecasting.
We combine an autoregressive recurrent neural network to model temporal
dynamics with Implicit Quantile Networks to learn a large class of
distributions over a time-series target. When compared to other probabilistic
neural forecasting models on real- and simulated data, our approach is
favorable in terms of point-wise prediction accuracy as well as on estimating
the underlying temporal distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gouttes_A/0/1/0/all/0/1"&gt;Ad&amp;#xe8;le Gouttes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasul_K/0/1/0/all/0/1"&gt;Kashif Rasul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koren_M/0/1/0/all/0/1"&gt;Mateusz Koren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stephan_J/0/1/0/all/0/1"&gt;Johannes Stephan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naghibi_T/0/1/0/all/0/1"&gt;Tofigh Naghibi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grid Partitioned Attention: Efficient TransformerApproximation with Inductive Bias for High Resolution Detail Generation. (arXiv:2107.03742v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03742</id>
        <link href="http://arxiv.org/abs/2107.03742"/>
        <updated>2021-07-09T01:58:27.228Z</updated>
        <summary type="html"><![CDATA[Attention is a general reasoning mechanism than can flexibly deal with image
information, but its memory requirements had made it so far impractical for
high resolution image generation. We present Grid Partitioned Attention (GPA),
a new approximate attention algorithm that leverages a sparse inductive bias
for higher computational and memory efficiency in image domains: queries attend
only to few keys, spatially close queries attend to close keys due to
correlations. Our paper introduces the new attention layer, analyzes its
complexity and how the trade-off between memory usage and model power can be
tuned by the hyper-parameters.We will show how such attention enables novel
deep learning architectures with copying modules that are especially useful for
conditional image generation tasks like pose morphing. Our contributions are
(i) algorithm and code1of the novel GPA layer, (ii) a novel deep
attention-copying architecture, and (iii) new state-of-the art experimental
results in human pose morphing generation benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jetchev_N/0/1/0/all/0/1"&gt;Nikolay Jetchev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yildirim_G/0/1/0/all/0/1"&gt;G&amp;#xf6;khan Yildirim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bracher_C/0/1/0/all/0/1"&gt;Christian Bracher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vollgraf_R/0/1/0/all/0/1"&gt;Roland Vollgraf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SSSE: Efficiently Erasing Samples from Trained Machine Learning Models. (arXiv:2107.03860v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03860</id>
        <link href="http://arxiv.org/abs/2107.03860"/>
        <updated>2021-07-09T01:58:27.174Z</updated>
        <summary type="html"><![CDATA[The availability of large amounts of user-provided data has been key to the
success of machine learning for many real-world tasks. Recently, an increasing
awareness has emerged that users should be given more control about how their
data is used. In particular, users should have the right to prohibit the use of
their data for training machine learning systems, and to have it erased from
already trained systems. While several sample erasure methods have been
proposed, all of them have drawbacks which have prevented them from gaining
widespread adoption. Most methods are either only applicable to very specific
families of models, sacrifice too much of the original model's accuracy, or
they have prohibitive memory or computational requirements. In this paper, we
propose an efficient and effective algorithm, SSSE, for samples erasure, that
is applicable to a wide class of machine learning models. From a second-order
analysis of the model's loss landscape we derive a closed-form update step of
the model parameters that only requires access to the data to be erased, not to
the original training set. Experiments on three datasets, CelebFaces attributes
(CelebA), Animals with Attributes 2 (AwA2) and CIFAR10, show that in certain
cases SSSE can erase samples almost as well as the optimal, yet impractical,
gold standard of training a new model from scratch with only the permitted
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peste_A/0/1/0/all/0/1"&gt;Alexandra Peste&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1"&gt;Dan Alistarh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lampert_C/0/1/0/all/0/1"&gt;Christoph H. Lampert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Encoding Domain Information with Sparse Priors for Inferring Explainable Latent Variables. (arXiv:2107.03730v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.03730</id>
        <link href="http://arxiv.org/abs/2107.03730"/>
        <updated>2021-07-09T01:58:27.163Z</updated>
        <summary type="html"><![CDATA[Latent variable models are powerful statistical tools that can uncover
relevant variation between patients or cells, by inferring unobserved hidden
states from observable high-dimensional data. A major shortcoming of current
methods, however, is their inability to learn sparse and interpretable hidden
states. Additionally, in settings where partial knowledge on the latent
structure of the data is readily available, a statistically sound integration
of prior information into current methods is challenging. To address these
issues, we propose spex-LVM, a factorial latent variable model with sparse
priors to encourage the inference of explainable factors driven by
domain-relevant information. spex-LVM utilizes existing knowledge of curated
biomedical pathways to automatically assign annotated attributes to latent
factors, yielding interpretable results tailored to the corresponding domain of
interest. Evaluations on simulated and real single-cell RNA-seq datasets
demonstrate that our model robustly identifies relevant structure in an
inherently explainable manner, distinguishes technical noise from sources of
biomedical variation, and provides dataset-specific adaptations of existing
pathway annotations. Implementation is available at
https://github.com/MLO-lab/spexlvm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Qoku_A/0/1/0/all/0/1"&gt;Arber Qoku&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Buettner_F/0/1/0/all/0/1"&gt;Florian Buettner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cascaded Diffusion Models for High Fidelity Image Generation. (arXiv:2106.15282v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15282</id>
        <link href="http://arxiv.org/abs/2106.15282"/>
        <updated>2021-07-09T01:58:27.118Z</updated>
        <summary type="html"><![CDATA[We show that cascaded diffusion models are capable of generating high
fidelity images on the class-conditional ImageNet generation challenge, without
any assistance from auxiliary image classifiers to boost sample quality. A
cascaded diffusion model comprises a pipeline of multiple diffusion models that
generate images of increasing resolution, beginning with a standard diffusion
model at the lowest resolution, followed by one or more super-resolution
diffusion models that successively upsample the image and add higher resolution
details. We find that the sample quality of a cascading pipeline relies
crucially on conditioning augmentation, our proposed method of data
augmentation of the lower resolution conditioning inputs to the
super-resolution models. Our experiments show that conditioning augmentation
prevents compounding error during sampling in a cascaded model, helping us to
train cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at
128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep, and
classification accuracy scores of 63.02% (top-1) and 84.06% (top-5) at 256x256,
outperforming VQ-VAE-2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1"&gt;Jonathan Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saharia_C/0/1/0/all/0/1"&gt;Chitwan Saharia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1"&gt;William Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1"&gt;David J. Fleet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1"&gt;Mohammad Norouzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salimans_T/0/1/0/all/0/1"&gt;Tim Salimans&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpecGrav -- Detection of Gravitational Waves using Deep Learning. (arXiv:2107.03607v1 [astro-ph.IM])]]></title>
        <id>http://arxiv.org/abs/2107.03607</id>
        <link href="http://arxiv.org/abs/2107.03607"/>
        <updated>2021-07-09T01:58:27.110Z</updated>
        <summary type="html"><![CDATA[Gravitational waves are ripples in the fabric of space-time that travel at
the speed of light. The detection of gravitational waves by LIGO is a major
breakthrough in the field of astronomy. Deep Learning has revolutionized many
industries including health care, finance and education. Deep Learning
techniques have also been explored for detection of gravitational waves to
overcome the drawbacks of traditional matched filtering method. However, in
several researches, the training phase of neural network is very time consuming
and hardware devices with large memory are required for the task. In order to
reduce the extensive amount of hardware resources and time required in training
a neural network for detecting gravitational waves, we made SpecGrav. We use 2D
Convolutional Neural Network and spectrograms of gravitational waves embedded
in noise to detect gravitational waves from binary black hole merger and binary
neutron star merger. The training phase of our neural network was of about just
19 minutes on a 2GB GPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Dodia_H/0/1/0/all/0/1"&gt;Hrithika Dodia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Tandel_H/0/1/0/all/0/1"&gt;Himanshu Tandel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+DMello_L/0/1/0/all/0/1"&gt;Lynette D&amp;#x27;Mello&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MAFIA: Machine Learning Acceleration on FPGAs for IoT Applications. (arXiv:2107.03653v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2107.03653</id>
        <link href="http://arxiv.org/abs/2107.03653"/>
        <updated>2021-07-09T01:58:27.104Z</updated>
        <summary type="html"><![CDATA[Recent breakthroughs in ML have produced new classes of models that allow ML
inference to run directly on milliwatt-powered IoT devices. On one hand,
existing ML-to-FPGA compilers are designed for deep neural-networks on large
FPGAs. On the other hand, general-purpose HLS tools fail to exploit properties
specific to ML inference, thereby resulting in suboptimal performance. We
propose MAFIA, a tool to compile ML inference on small form-factor FPGAs for
IoT applications. MAFIA provides native support for linear algebra operations
and can express a variety of ML algorithms, including state-of-the-art models.
We show that MAFIA-generated programs outperform best-performing variant of a
commercial HLS compiler by 2.5x on average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghanathe_N/0/1/0/all/0/1"&gt;Nikhil Pratap Ghanathe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seshadri_V/0/1/0/all/0/1"&gt;Vivek Seshadri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1"&gt;Rahul Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilton_S/0/1/0/all/0/1"&gt;Steve Wilton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Aayan Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BumbleBee: A Transformer for Music. (arXiv:2107.03443v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.03443</id>
        <link href="http://arxiv.org/abs/2107.03443"/>
        <updated>2021-07-09T01:58:27.098Z</updated>
        <summary type="html"><![CDATA[We will introduce BumbleBee, a transformer model that will generate MIDI
music data . We will tackle the issue of transformers applied to long sequences
by implementing a longformer generative model that uses dilating sliding
windows to compute the attention layers. We will compare our results to that of
the music transformer and Long-Short term memory (LSTM) to benchmark our
results. This analysis will be performed using piano MIDI files, in particular
, the JSB Chorales dataset that has already been used for other research works
(Huang et al., 2018)]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fenaux_L/0/1/0/all/0/1"&gt;Lucas Fenaux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quintero_M/0/1/0/all/0/1"&gt;Maria Juliana Quintero&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sublinear Regret for Learning POMDPs. (arXiv:2107.03635v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03635</id>
        <link href="http://arxiv.org/abs/2107.03635"/>
        <updated>2021-07-09T01:58:27.091Z</updated>
        <summary type="html"><![CDATA[We study the model-based undiscounted reinforcement learning for partially
observable Markov decision processes (POMDPs). The oracle we consider is the
optimal policy of the POMDP with a known environment in terms of the average
reward over an infinite horizon. We propose a learning algorithm for this
problem, building on spectral method-of-moments estimations for hidden Markov
models, the belief error control in POMDPs and upper-confidence-bound methods
for online learning. We establish a regret bound of $O(T^{2/3}\sqrt{\log T})$
for the proposed learning algorithm where $T$ is the learning horizon. This is,
to the best of our knowledge, the first algorithm achieving sublinear regret
with respect to our oracle for learning general POMDPs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1"&gt;Yi Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Ningyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xuefeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xiang Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Disease Progress with Imprecise Lab Test Results. (arXiv:2107.03620v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03620</id>
        <link href="http://arxiv.org/abs/2107.03620"/>
        <updated>2021-07-09T01:58:27.069Z</updated>
        <summary type="html"><![CDATA[In existing deep learning methods, almost all loss functions assume that
sample data values used to be predicted are the only correct ones. This
assumption does not hold for laboratory test data. Test results are often
within tolerable or imprecision ranges, with all values in the ranges
acceptable. By considering imprecision samples, we propose an imprecision range
loss (IR loss) method and incorporate it into Long Short Term Memory (LSTM)
model for disease progress prediction. In this method, each sample in
imprecision range space has a certain probability to be the real value,
participating in the loss calculation. The loss is defined as the integral of
the error of each point in the impression range space. A sampling method for
imprecision space is formulated. The continuous imprecision space is
discretized, and a sequence of imprecise data sets are obtained, which is
convenient for gradient descent learning. A heuristic learning algorithm is
developed to learn the model parameters based on the imprecise data sets.
Experimental results on real data show that the prediction method based on IR
loss can provide more stable and consistent prediction result when test samples
are generated from imprecision range.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1"&gt;Jianwen Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhihua Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BANet: Blur-aware Attention Networks for Dynamic Scene Deblurring. (arXiv:2101.07518v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.07518</id>
        <link href="http://arxiv.org/abs/2101.07518"/>
        <updated>2021-07-09T01:58:27.061Z</updated>
        <summary type="html"><![CDATA[Image motion blur usually results from moving objects or camera shakes. Such
blur is generally directional and non-uniform. Previous research efforts
attempt to solve non-uniform blur by using self-recurrent multi-scale or
multi-patch architectures accompanying with self-attention. However, using
self-recurrent frameworks typically leads to a longer inference time, while
inter-pixel or inter-channel self-attention may cause excessive memory usage.
This paper proposes blur-aware attention networks (BANet) that accomplish
accurate and efficient deblurring via a single forward pass. Our BANet utilizes
region-based self-attention with multi-kernel strip pooling to disentangle blur
patterns of different degrees and with cascaded parallel dilated convolution to
aggregate multi-scale content features. Extensive experimental results on the
GoPro and HIDE benchmarks demonstrate that the proposed BANet performs
favorably against the state-of-the-art in blurred image restoration and can
provide deblurred results in real-time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsai%2A_F/0/1/0/all/0/1"&gt;Fu-Jen Tsai*&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng%2A_Y/0/1/0/all/0/1"&gt;Yan-Tsung Peng*&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yen-Yu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_C/0/1/0/all/0/1"&gt;Chung-Chi Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chia-Wen Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Label-set Loss Functions for Partial Supervision: Application to Fetal Brain 3D MRI Parcellation. (arXiv:2107.03846v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.03846</id>
        <link href="http://arxiv.org/abs/2107.03846"/>
        <updated>2021-07-09T01:58:27.054Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have increased the accuracy of automatic segmentation,
however, their accuracy depends on the availability of a large number of fully
segmented images. Methods to train deep neural networks using images for which
some, but not all, regions of interest are segmented are necessary to make
better use of partially annotated datasets. In this paper, we propose the first
axiomatic definition of label-set loss functions that are the loss functions
that can handle partially segmented images. We prove that there is one and only
one method to convert a classical loss function for fully segmented images into
a proper label-set loss function. Our theory also allows us to define the
leaf-Dice loss, a label-set generalization of the Dice loss particularly suited
for partial supervision with only missing labels. Using the leaf-Dice loss, we
set a new state of the art in partially supervised learning for fetal brain 3D
MRI segmentation. We achieve a deep neural network able to segment white
matter, ventricles, cerebellum, extra-ventricular CSF, cortical gray matter,
deep gray matter, brainstem, and corpus callosum based on fetal brain 3D MRI of
anatomically normal fetuses or with open spina bifida. Our implementation of
the proposed label-set loss functions is available at
https://github.com/LucasFidon/label-set-loss-functions]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Fidon_L/0/1/0/all/0/1"&gt;Lucas Fidon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Aertsen_M/0/1/0/all/0/1"&gt;Michael Aertsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Emam_D/0/1/0/all/0/1"&gt;Doaa Emam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mufti_N/0/1/0/all/0/1"&gt;Nada Mufti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Guffens_F/0/1/0/all/0/1"&gt;Frederic Guffens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Deprest_T/0/1/0/all/0/1"&gt;Thomas Deprest&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Demaerel_P/0/1/0/all/0/1"&gt;Philippe Demaerel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+David_A/0/1/0/all/0/1"&gt;Anna L. David&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Melbourne_A/0/1/0/all/0/1"&gt;Andrew Melbourne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1"&gt;Sebastien Ourselin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Deprest_J/0/1/0/all/0/1"&gt;Jam Deprest&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1"&gt;Tom Vercauteren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IowaRain: A Statewide Rain Event Dataset Based on Weather Radars and Quantitative Precipitation Estimation. (arXiv:2107.03432v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03432</id>
        <link href="http://arxiv.org/abs/2107.03432"/>
        <updated>2021-07-09T01:58:27.048Z</updated>
        <summary type="html"><![CDATA[Effective environmental planning and management to address climate change
could be achieved through extensive environmental modeling with machine
learning and conventional physical models. In order to develop and improve
these models, practitioners and researchers need comprehensive benchmark
datasets that are prepared and processed with environmental expertise that they
can rely on. This study presents an extensive dataset of rainfall events for
the state of Iowa (2016-2019) acquired from the National Weather Service Next
Generation Weather Radar (NEXRAD) system and processed by a quantitative
precipitation estimation system. The dataset presented in this study could be
used for better disaster monitoring, response and recovery by paving the way
for both predictive and prescriptive modeling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sit_M/0/1/0/all/0/1"&gt;Muhammed Sit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seo_B/0/1/0/all/0/1"&gt;Bong-Chul Seo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demir_I/0/1/0/all/0/1"&gt;Ibrahim Demir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Random Access Memory using Lattices. (arXiv:2107.03474v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03474</id>
        <link href="http://arxiv.org/abs/2107.03474"/>
        <updated>2021-07-09T01:58:27.041Z</updated>
        <summary type="html"><![CDATA[We introduce a differentiable random access memory module with $O(1)$
performance regardless of size, scaling to billions of entries. The design
stores entries on points of a chosen lattice to calculate nearest neighbours of
arbitrary points efficiently by exploiting symmetries. Augmenting a standard
neural network architecture with a single memory layer based on this, we can
scale the parameter count up to memory limits with negligible computational
overhead, giving better accuracy at similar cost. On large language modelling
tasks, these enhanced models with larger capacity significantly outperform the
unmodified transformer baseline. We found continued scaling with memory size up
to the limits tested.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goucher_A/0/1/0/all/0/1"&gt;Adam P. Goucher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Troll_R/0/1/0/all/0/1"&gt;Rajan Troll&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modality Completion via Gaussian Process Prior Variational Autoencoders for Multi-Modal Glioma Segmentation. (arXiv:2107.03442v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.03442</id>
        <link href="http://arxiv.org/abs/2107.03442"/>
        <updated>2021-07-09T01:58:27.021Z</updated>
        <summary type="html"><![CDATA[In large studies involving multi protocol Magnetic Resonance Imaging (MRI),
it can occur to miss one or more sub-modalities for a given patient owing to
poor quality (e.g. imaging artifacts), failed acquisitions, or hallway
interrupted imaging examinations. In some cases, certain protocols are
unavailable due to limited scan time or to retrospectively harmonise the
imaging protocols of two independent studies. Missing image modalities pose a
challenge to segmentation frameworks as complementary information contributed
by the missing scans is then lost. In this paper, we propose a novel model,
Multi-modal Gaussian Process Prior Variational Autoencoder (MGP-VAE), to impute
one or more missing sub-modalities for a patient scan. MGP-VAE can leverage the
Gaussian Process (GP) prior on the Variational Autoencoder (VAE) to utilize the
subjects/patients and sub-modalities correlations. Instead of designing one
network for each possible subset of present sub-modalities or using frameworks
to mix feature maps, missing data can be generated from a single model based on
all the available samples. We show the applicability of MGP-VAE on brain tumor
segmentation where either, two, or three of four sub-modalities may be missing.
Our experiments against competitive segmentation baselines with missing
sub-modality on BraTS'19 dataset indicate the effectiveness of the MGP-VAE
model for segmentation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hamghalam_M/0/1/0/all/0/1"&gt;Mohammad Hamghalam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Frangi_A/0/1/0/all/0/1"&gt;Alejandro F. Frangi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lei_B/0/1/0/all/0/1"&gt;Baiying Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Simpson_A/0/1/0/all/0/1"&gt;Amber L. Simpson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quadruplet Deep Metric Learning Model for Imbalanced Time-series Fault Diagnosis. (arXiv:2107.03786v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03786</id>
        <link href="http://arxiv.org/abs/2107.03786"/>
        <updated>2021-07-09T01:58:27.013Z</updated>
        <summary type="html"><![CDATA[Intelligent diagnosis method based on data-driven and deep learning is an
attractive and meaningful field in recent years. However, in practical
application scenarios, the imbalance of time-series fault is an urgent problem
to be solved. From the perspective of Bayesian probability, this paper analyzes
how to improve the performance of imbalanced classification by adjusting the
distance between classes and the distribution within a class and proposes a
time-series fault diagnosis model based on deep metric learning. As a core of
deep metric learning, a novel quadruplet data pair design considering imbalance
class is proposed with reference to traditional deep metric learning. Based on
such data pair, this paper proposes a quadruplet loss function which takes into
account the inter-class distance and the intra-class data distribution, and
pays special attention to imbalanced sample pairs. The reasonable combination
of quadruplet loss and softmax loss function can reduce the impact of
imbalance. Experiments on two open datasets are carried out to verify the
effectiveness and robustness of the model. Experimental results show that the
proposed method can effectively improve the performance of imbalanced
classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gui_X/0/1/0/all/0/1"&gt;Xingtai Gui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiyang Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collaboration of Experts: Achieving 80% Top-1 Accuracy on ImageNet with 100M FLOPs. (arXiv:2107.03815v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03815</id>
        <link href="http://arxiv.org/abs/2107.03815"/>
        <updated>2021-07-09T01:58:27.006Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a Collaboration of Experts (CoE) framework to pool
together the expertise of multiple networks towards a common aim. Each expert
is an individual network with expertise on a unique portion of the dataset,
which enhances the collective capacity. Given a sample, an expert is selected
by the delegator, which simultaneously outputs a rough prediction to support
early termination. To fulfill this framework, we propose three modules to impel
each model to play its role, namely weight generation module (WGM), label
generation module (LGM) and variance calculation module (VCM). Our method
achieves the state-of-the-art performance on ImageNet, 80.7% top-1 accuracy
with 194M FLOPs. Combined with PWLU activation function and CondConv, CoE
further achieves the accuracy of 80.0% with only 100M FLOPs for the first time.
More importantly, our method is hardware friendly and achieves a 3-6x speedup
compared with some existing conditional computation approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yikang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1"&gt;Zhao Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flexibly Regularized Mixture Models and Application to Image Segmentation. (arXiv:1905.10629v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.10629</id>
        <link href="http://arxiv.org/abs/1905.10629"/>
        <updated>2021-07-09T01:58:26.998Z</updated>
        <summary type="html"><![CDATA[Probabilistic finite mixture models are widely used for unsupervised
clustering. These models can often be improved by adapting them to the topology
of the data. For instance, in order to classify spatially adjacent data points
similarly, it is common to introduce a Laplacian constraint on the posterior
probability that each data point belongs to a class. Alternatively, the mixing
probabilities can be treated as free parameters, while assuming Gauss-Markov or
more complex priors to regularize those mixing probabilities. However, these
approaches are constrained by the shape of the prior and often lead to
complicated or intractable inference. Here, we propose a new parametrization of
the Dirichlet distribution to flexibly regularize the mixing probabilities of
over-parametrized mixture distributions. Using the Expectation-Maximization
algorithm, we show that our approach allows us to define any linear update rule
for the mixing probabilities, including spatial smoothing regularization as a
special case. We then show that this flexible design can be extended to share
class information between multiple mixture models. We apply our algorithm to
artificial and natural image segmentation tasks, and we provide quantitative
and qualitative comparison of the performance of Gaussian and Student-t
mixtures on the Berkeley Segmentation Dataset. We also demonstrate how to
propagate class information across the layers of deep convolutional neural
networks in a probabilistically optimal way, suggesting a new interpretation
for feedback signals in biological visual systems. Our flexible approach can be
easily generalized to adapt probabilistic mixture models to arbitrary data
topologies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vacher_J/0/1/0/all/0/1"&gt;Jonathan Vacher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Launay_C/0/1/0/all/0/1"&gt;Claire Launay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coen_Cagli_R/0/1/0/all/0/1"&gt;Ruben Coen-Cagli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model Selection for Generic Contextual Bandits. (arXiv:2107.03455v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.03455</id>
        <link href="http://arxiv.org/abs/2107.03455"/>
        <updated>2021-07-09T01:58:26.990Z</updated>
        <summary type="html"><![CDATA[We consider the problem of model selection for the general stochastic
contextual bandits under the realizability assumption. We propose a successive
refinement based algorithm called Adaptive Contextual Bandit ({\ttfamily ACB}),
that works in phases and successively eliminates model classes that are too
simple to fit the given instance. We prove that this algorithm is adaptive,
i.e., the regret rate order-wise matches that of {\ttfamily FALCON}, the
state-of-art contextual bandit algorithm of Levi et. al '20, that needs
knowledge of the true model class. The price of not knowing the correct model
class is only an additive term contributing to the second order term in the
regret bound. This cost possess the intuitive property that it becomes smaller
as the model class becomes easier to identify, and vice-versa. We then show
that a much simpler explore-then-commit (ETC) style algorithm also obtains a
regret rate of matching that of {\ttfamily FALCON}, despite not knowing the
true model class. However, the cost of model selection is higher in ETC as
opposed to in {\ttfamily ACB}, as expected. Furthermore, {\ttfamily ACB}
applied to the linear bandit setting with unknown sparsity, order-wise recovers
the model selection guarantees previously established by algorithms tailored to
the linear setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ghosh_A/0/1/0/all/0/1"&gt;Avishek Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sankararaman_A/0/1/0/all/0/1"&gt;Abishek Sankararaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ramchandran_K/0/1/0/all/0/1"&gt;Kannan Ramchandran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proceedings of the First Workshop on Weakly Supervised Learning (WeaSuL). (arXiv:2107.03690v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03690</id>
        <link href="http://arxiv.org/abs/2107.03690"/>
        <updated>2021-07-09T01:58:26.960Z</updated>
        <summary type="html"><![CDATA[Welcome to WeaSuL 2021, the First Workshop on Weakly Supervised Learning,
co-located with ICLR 2021. In this workshop, we want to advance theory, methods
and tools for allowing experts to express prior coded knowledge for automatic
data annotations that can be used to train arbitrary deep neural networks for
prediction. The ICLR 2021 Workshop on Weak Supervision aims at advancing
methods that help modern machine-learning methods to generalize from knowledge
provided by experts, in interaction with observable (unlabeled) data. In total,
15 papers were accepted. All the accepted contributions are listed in these
Proceedings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hedderich_M/0/1/0/all/0/1"&gt;Michael A. Hedderich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1"&gt;Benjamin Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kann_K/0/1/0/all/0/1"&gt;Katharina Kann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1"&gt;Barbara Plank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ratner_A/0/1/0/all/0/1"&gt;Alex Ratner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1"&gt;Dietrich Klakow&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[In-Network Learning: Distributed Training and Inference in Networks. (arXiv:2107.03433v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03433</id>
        <link href="http://arxiv.org/abs/2107.03433"/>
        <updated>2021-07-09T01:58:26.946Z</updated>
        <summary type="html"><![CDATA[It is widely perceived that leveraging the success of modern machine learning
techniques to mobile devices and wireless networks has the potential of
enabling important new services. This, however, poses significant challenges,
essentially due to that both data and processing power are highly distributed
in a wireless network. In this paper, we develop a learning algorithm and an
architecture that make use of multiple data streams and processing units, not
only during the training phase but also during the inference phase. In
particular, the analysis reveals how inference propagates and fuses across a
network. We study the design criterion of our proposed method and its bandwidth
requirements. Also, we discuss implementation aspects using neural networks in
typical wireless radio access; and provide experiments that illustrate benefits
over state-of-the-art techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moldoveanu_M/0/1/0/all/0/1"&gt;Matei Moldoveanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaidi_A/0/1/0/all/0/1"&gt;Abdellatif Zaidi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning as a Mean-Field Game. (arXiv:2107.03770v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.03770</id>
        <link href="http://arxiv.org/abs/2107.03770"/>
        <updated>2021-07-09T01:58:26.939Z</updated>
        <summary type="html"><![CDATA[We establish a connection between federated learning, a concept from machine
learning, and mean-field games, a concept from game theory and control theory.
In this analogy, the local federated learners are considered as the players and
the aggregation of the gradients in a central server is the mean-field effect.
We present federated learning as a differential game and discuss the properties
of the equilibrium of this game. We hope this novel view to federated learning
brings together researchers from these two distinct areas to work on
fundamental problems of large-scale distributed and privacy-preserving learning
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Mehrjou_A/0/1/0/all/0/1"&gt;Arash Mehrjou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-sentence Neural Language Models for Conversational Speech Recognition. (arXiv:2106.06922v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06922</id>
        <link href="http://arxiv.org/abs/2106.06922"/>
        <updated>2021-07-09T01:58:26.931Z</updated>
        <summary type="html"><![CDATA[An important research direction in automatic speech recognition (ASR) has
centered around the development of effective methods to rerank the output
hypotheses of an ASR system with more sophisticated language models (LMs) for
further gains. A current mainstream school of thoughts for ASR N-best
hypothesis reranking is to employ a recurrent neural network (RNN)-based LM or
its variants, with performance superiority over the conventional n-gram LMs
across a range of ASR tasks. In real scenarios such as a long conversation, a
sequence of consecutive sentences may jointly contain ample cues of
conversation-level information such as topical coherence, lexical entrainment
and adjacency pairs, which however remains to be underexplored. In view of
this, we first formulate ASR N-best reranking as a prediction problem, putting
forward an effective cross-sentence neural LM approach that reranks the ASR
N-best hypotheses of an upcoming sentence by taking into consideration the word
usage in its precedent sentences. Furthermore, we also explore to extract
task-specific global topical information of the cross-sentence history in an
unsupervised manner for better ASR performance. Extensive experiments conducted
on the AMI conversational benchmark corpus indicate the effectiveness and
feasibility of our methods in comparison to several state-of-the-art reranking
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chiu_S/0/1/0/all/0/1"&gt;Shih-Hsuan Chiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lo_T/0/1/0/all/0/1"&gt;Tien-Hong Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Berlin Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for Two-Sided Matching. (arXiv:2107.03427v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2107.03427</id>
        <link href="http://arxiv.org/abs/2107.03427"/>
        <updated>2021-07-09T01:58:26.924Z</updated>
        <summary type="html"><![CDATA[We initiate the use of a multi-layer neural network to model two-sided
matching and to explore the design space between strategy-proofness and
stability. It is well known that both properties cannot be achieved
simultaneously but the efficient frontier in this design space is not
understood. We show empirically that it is possible to achieve a good
compromise between stability and strategy-proofness-substantially better than
that achievable through a convex combination of deferred acceptance (stable and
strategy-proof for only one side of the market) and randomized serial
dictatorship (strategy-proof but not stable).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ravindranath_S/0/1/0/all/0/1"&gt;Sai Srivatsa Ravindranath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zhe Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shira Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jonathan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kominers_S/0/1/0/all/0/1"&gt;Scott D. Kominers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parkes_D/0/1/0/all/0/1"&gt;David C. Parkes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CHASE: Robust Visual Tracking via Cell-Level Differentiable Neural Architecture Search. (arXiv:2107.03463v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03463</id>
        <link href="http://arxiv.org/abs/2107.03463"/>
        <updated>2021-07-09T01:58:26.905Z</updated>
        <summary type="html"><![CDATA[A strong visual object tracker nowadays relies on its well-crafted modules,
which typically consist of manually-designed network architectures to deliver
high-quality tracking results. Not surprisingly, the manual design process
becomes a particularly challenging barrier, as it demands sufficient prior
experience, enormous effort, intuition and perhaps some good luck. Meanwhile,
neural architecture search has gaining grounds in practical applications such
as image segmentation, as a promising method in tackling the issue of automated
search of feasible network structures. In this work, we propose a novel
cell-level differentiable architecture search mechanism to automate the network
design of the tracking module, aiming to adapt backbone features to the
objective of a tracking network during offline training. The proposed approach
is simple, efficient, and with no need to stack a series of modules to
construct a network. Our approach is easy to be incorporated into existing
trackers, which is empirically validated using different differentiable
architecture search-based methods and tracking objectives. Extensive
experimental evaluations demonstrate the superior performance of our approach
over five commonly-used benchmarks. Meanwhile, our automated searching process
takes 41 (18) hours for the second (first) order DARTS method on the
TrackingNet dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marvasti_Zadeh_S/0/1/0/all/0/1"&gt;Seyed Mojtaba Marvasti-Zadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khaghani_J/0/1/0/all/0/1"&gt;Javad Khaghani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1"&gt;Li Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghanei_Yakhdan_H/0/1/0/all/0/1"&gt;Hossein Ghanei-Yakhdan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasaei_S/0/1/0/all/0/1"&gt;Shohreh Kasaei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elastic deformation of optical coherence tomography images of diabetic macular edema for deep-learning models training: how far to go?. (arXiv:2107.03651v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.03651</id>
        <link href="http://arxiv.org/abs/2107.03651"/>
        <updated>2021-07-09T01:58:26.899Z</updated>
        <summary type="html"><![CDATA[To explore the clinical validity of elastic deformation of optical coherence
tomography (OCT) images for data augmentation in the development of
deep-learning model for detection of diabetic macular edema (DME).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bar_David_D/0/1/0/all/0/1"&gt;Daniel Bar-David&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bar_David_L/0/1/0/all/0/1"&gt;Laura Bar-David&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shapira_Y/0/1/0/all/0/1"&gt;Yinon Shapira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Leibu_R/0/1/0/all/0/1"&gt;Rina Leibu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dori_D/0/1/0/all/0/1"&gt;Dalia Dori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schneor_R/0/1/0/all/0/1"&gt;Ronit Schneor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fischer_A/0/1/0/all/0/1"&gt;Anath Fischer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Soudry_S/0/1/0/all/0/1"&gt;Shiri Soudry&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Stress Testing for Adversarial Learning in a Financial Environment. (arXiv:2107.03577v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.03577</id>
        <link href="http://arxiv.org/abs/2107.03577"/>
        <updated>2021-07-09T01:58:26.891Z</updated>
        <summary type="html"><![CDATA[We demonstrate the use of Adaptive Stress Testing to detect and address
potential vulnerabilities in a financial environment. We develop a simplified
model for credit card fraud detection that utilizes a linear regression
classifier based on historical payment transaction data coupled with business
rules. We then apply the reinforcement learning model known as Adaptive Stress
Testing to train an agent, that can be thought of as a potential fraudster, to
find the most likely path to system failure -- successfully defrauding the
system. We show the connection between this most likely failure path and the
limits of the classifier and discuss how the fraud detection system's business
rules can be further augmented to mitigate these failure modes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+El_Awady_K/0/1/0/all/0/1"&gt;Khalid El-Awady&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sleep syndromes onset detection based on automatic sleep staging algorithm. (arXiv:2107.03387v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2107.03387</id>
        <link href="http://arxiv.org/abs/2107.03387"/>
        <updated>2021-07-09T01:58:26.885Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel method and a practical approach to
predicting early onsets of sleep syndromes, including restless leg syndrome,
insomnia, based on an algorithm that is comprised of two modules. A Fast
Fourier Transform is applied to 30 seconds long epochs of EEG recordings to
provide localized time-frequency information, and a deep convolutional LSTM
neural network is trained for sleep stage classification. Automating sleep
stages detection from EEG data offers great potential to tackling sleep
irregularities on a daily basis. Thereby, a novel approach for sleep stage
classification is proposed which combines the best of signal processing and
statistics. In this study, we used the PhysioNet Sleep European Data Format
(EDF) Database. The code evaluation showed impressive results, reaching an
accuracy of 86.43, precision of 77.76, recall of 93,32, F1-score of 89.12 with
the final mean false error loss of 0.09.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Cvetko_T/0/1/0/all/0/1"&gt;Tim Cvetko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Robek_T/0/1/0/all/0/1"&gt;Tinkara Robek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modality Completion via Gaussian Process Prior Variational Autoencoders for Multi-Modal Glioma Segmentation. (arXiv:2107.03442v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.03442</id>
        <link href="http://arxiv.org/abs/2107.03442"/>
        <updated>2021-07-09T01:58:26.878Z</updated>
        <summary type="html"><![CDATA[In large studies involving multi protocol Magnetic Resonance Imaging (MRI),
it can occur to miss one or more sub-modalities for a given patient owing to
poor quality (e.g. imaging artifacts), failed acquisitions, or hallway
interrupted imaging examinations. In some cases, certain protocols are
unavailable due to limited scan time or to retrospectively harmonise the
imaging protocols of two independent studies. Missing image modalities pose a
challenge to segmentation frameworks as complementary information contributed
by the missing scans is then lost. In this paper, we propose a novel model,
Multi-modal Gaussian Process Prior Variational Autoencoder (MGP-VAE), to impute
one or more missing sub-modalities for a patient scan. MGP-VAE can leverage the
Gaussian Process (GP) prior on the Variational Autoencoder (VAE) to utilize the
subjects/patients and sub-modalities correlations. Instead of designing one
network for each possible subset of present sub-modalities or using frameworks
to mix feature maps, missing data can be generated from a single model based on
all the available samples. We show the applicability of MGP-VAE on brain tumor
segmentation where either, two, or three of four sub-modalities may be missing.
Our experiments against competitive segmentation baselines with missing
sub-modality on BraTS'19 dataset indicate the effectiveness of the MGP-VAE
model for segmentation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hamghalam_M/0/1/0/all/0/1"&gt;Mohammad Hamghalam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Frangi_A/0/1/0/all/0/1"&gt;Alejandro F. Frangi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lei_B/0/1/0/all/0/1"&gt;Baiying Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Simpson_A/0/1/0/all/0/1"&gt;Amber L. Simpson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Resolution Susceptibility of Face Recognition Models. (arXiv:2107.03769v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03769</id>
        <link href="http://arxiv.org/abs/2107.03769"/>
        <updated>2021-07-09T01:58:26.861Z</updated>
        <summary type="html"><![CDATA[Face recognition approaches often rely on equal image resolution for
verification faces on two images. However, in practical applications, those
image resolutions are usually not in the same range due to different image
capture mechanisms or sources. In this work, we first analyze the impact of
image resolutions on the face verification performance with a state-of-the-art
face recognition model. For images, synthetically reduced to $5\, \times 5\,
\mathrm{px}$ resolution, the verification performance drops from $99.23\%$
increasingly down to almost $55\%$. Especially, for cross-resolution image
pairs (one high- and one low-resolution image), the verification accuracy
decreases even further. We investigate this behavior more in-depth by looking
at the feature distances for every 2-image test pair. To tackle this problem,
we propose the following two methods: 1) Train a state-of-the-art
face-recognition model straightforward with $50\%$ low-resolution images
directly within each batch. \\ 2) Train a siamese-network structure and adding
a cosine distance feature loss between high- and low-resolution features. Both
methods show an improvement for cross-resolution scenarios and can increase the
accuracy at very low resolution to approximately $70\%$. However, a
disadvantage is that a specific model needs to be trained for every
resolution-pair ...]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Knoche_M/0/1/0/all/0/1"&gt;Martin Knoche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hormann_S/0/1/0/all/0/1"&gt;Stefan H&amp;#xf6;rmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1"&gt;Gerhard Rigoll&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MotionRNN: A Flexible Model for Video Prediction with Spacetime-Varying Motions. (arXiv:2103.02243v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02243</id>
        <link href="http://arxiv.org/abs/2103.02243"/>
        <updated>2021-07-09T01:58:26.854Z</updated>
        <summary type="html"><![CDATA[This paper tackles video prediction from a new dimension of predicting
spacetime-varying motions that are incessantly changing across both space and
time. Prior methods mainly capture the temporal state transitions but overlook
the complex spatiotemporal variations of the motion itself, making them
difficult to adapt to ever-changing motions. We observe that physical world
motions can be decomposed into transient variation and motion trend, while the
latter can be regarded as the accumulation of previous motions. Thus,
simultaneously capturing the transient variation and the motion trend is the
key to make spacetime-varying motions more predictable. Based on these
observations, we propose the MotionRNN framework, which can capture the complex
variations within motions and adapt to spacetime-varying scenarios. MotionRNN
has two main contributions. The first is that we design the MotionGRU unit,
which can model the transient variation and motion trend in a unified way. The
second is that we apply the MotionGRU to RNN-based predictive models and
indicate a new flexible video prediction architecture with a Motion Highway
that can significantly improve the ability to predict changeable motions and
avoid motion vanishing for stacked multiple-layer predictive models. With high
flexibility, this framework can adapt to a series of models for deterministic
spatiotemporal prediction. Our MotionRNN can yield significant improvements on
three challenging benchmarks for video prediction with spacetime-varying
motions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Haixu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1"&gt;Zhiyu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianmin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1"&gt;Mingsheng Long&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-Time Ellipse Detection for Robotics Applications. (arXiv:2102.12670v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12670</id>
        <link href="http://arxiv.org/abs/2102.12670"/>
        <updated>2021-07-09T01:58:26.846Z</updated>
        <summary type="html"><![CDATA[We propose a new algorithm for real-time detection and tracking of elliptic
patterns suitable for real-world robotics applications. The method fits
ellipses to each contour in the image frame and rejects ellipses that do not
yield a good fit. The resulting detection and tracking method is lightweight
enough to be used on robots' resource-limited onboard computers, can deal with
lighting variations and detect the pattern even when the view is partial. The
method is tested on an example application of an autonomous UAV landing on a
fast-moving vehicle to show its performance indoors, outdoors, and in
simulation on a real-world robotics task. The comparison with other well-known
ellipse detection methods shows that our proposed algorithm outperforms other
methods with the F1 score of 0.981 on a dataset with over 1500 frames. The
videos of experiments, the source codes, and the collected dataset are provided
with the paper at https://theairlab.org/landing-on-vehicle .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Keipour_A/0/1/0/all/0/1"&gt;Azarakhsh Keipour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pereira_G/0/1/0/all/0/1"&gt;Guilherme A. S. Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1"&gt;Sebastian Scherer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Bangla Natural Language Processing Tasks and the Utility of Transformer Models. (arXiv:2107.03844v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.03844</id>
        <link href="http://arxiv.org/abs/2107.03844"/>
        <updated>2021-07-09T01:58:26.838Z</updated>
        <summary type="html"><![CDATA[Bangla -- ranked as the 6th most widely spoken language across the world
(https://www.ethnologue.com/guides/ethnologue200), with 230 million native
speakers -- is still considered as a low-resource language in the natural
language processing (NLP) community. With three decades of research, Bangla NLP
(BNLP) is still lagging behind mainly due to the scarcity of resources and the
challenges that come with it. There is sparse work in different areas of BNLP;
however, a thorough survey reporting previous work and recent advances is yet
to be done. In this study, we first provide a review of Bangla NLP tasks,
resources, and tools available to the research community; we benchmark datasets
collected from various platforms for nine NLP tasks using current
state-of-the-art algorithms (i.e., transformer-based models). We provide
comparative results for the studied NLP tasks by comparing monolingual vs.
multilingual models of varying sizes. We report our results using both
individual and consolidated datasets and provide data splits for future
research. We reviewed a total of 108 papers and conducted 175 sets of
experiments. Our results show promising performance using transformer-based
models while highlighting the trade-off with computational costs. We hope that
such a comprehensive survey will motivate the community to build on and further
advance the research on Bangla NLP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1"&gt;Firoj Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1"&gt;Arid Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1"&gt;Tanvir Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1"&gt;Akib Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tajrin_J/0/1/0/all/0/1"&gt;Janntatul Tajrin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1"&gt;Naira Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1"&gt;Shammur Absar Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A hybrid virtual sensing approach for approximating non-linear dynamic system behavior using LSTM networks. (arXiv:2107.03645v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.03645</id>
        <link href="http://arxiv.org/abs/2107.03645"/>
        <updated>2021-07-09T01:58:26.831Z</updated>
        <summary type="html"><![CDATA[Modern Internet of Things solutions are used in a variety of different areas,
ranging from connected vehicles and healthcare to industrial applications. They
rely on a large amount of interconnected sensors, which can lead to both
technical and economical challenges. Virtual sensing techniques aim to reduce
the number of physical sensors in a system by using data from available
measurements to estimate additional unknown quantities of interest. Successful
model-based solutions include Kalman filters or the combination of finite
element models and modal analysis, while many data-driven methods rely on
machine learning algorithms. The presented hybrid virtual sensing approach
combines Long Short-Term Memory networks with frequency response function
models in order to estimate the behavior of non-linear dynamic systems with
multiple input and output channels. Network training and prediction make use of
short signal subsequences, which are later recombined by applying a windowing
technique. The frequency response function model acts as a baseline estimate
which perfectly captures linear dynamic systems and is augmented by the
non-linear Long Short-Term Memory network following two different hybrid
modeling strategies. The approach is tested using a non-linear experimental
dataset, which results from measurements of a three-component servo-hydraulic
fatigue test bench. A variety of metrics in time and frequency domains, as well
as fatigue strength under variable amplitudes are used to evaluate the
approximation quality of the proposed method. In addition to virtual sensing,
the algorithm is also applied to a forward prediction task. Synthetic data are
used in a separate study to estimate the prediction quality on datasets of
different size.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Heindel_L/0/1/0/all/0/1"&gt;Leonhard Heindel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hantschke_P/0/1/0/all/0/1"&gt;Peter Hantschke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kastner_M/0/1/0/all/0/1"&gt;Markus K&amp;#xe4;stner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking annotation granularity for overcoming deep shortcut learning: A retrospective study on chest radiographs. (arXiv:2104.10553v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10553</id>
        <link href="http://arxiv.org/abs/2104.10553"/>
        <updated>2021-07-09T01:58:26.823Z</updated>
        <summary type="html"><![CDATA[Deep learning has demonstrated radiograph screening performances that are
comparable or superior to radiologists. However, recent studies show that deep
models for thoracic disease classification usually show degraded performance
when applied to external data. Such phenomena can be categorized into shortcut
learning, where the deep models learn unintended decision rules that can fit
the identically distributed training and test set but fail to generalize to
other distributions. A natural way to alleviate this defect is explicitly
indicating the lesions and focusing the model on learning the intended
features. In this paper, we conduct extensive retrospective experiments to
compare a popular thoracic disease classification model, CheXNet, and a
thoracic lesion detection model, CheXDet. We first showed that the two models
achieved similar image-level classification performance on the internal test
set with no significant differences under many scenarios. Meanwhile, we found
incorporating external training data even led to performance degradation for
CheXNet. Then, we compared the models' internal performance on the lesion
localization task and showed that CheXDet achieved significantly better
performance than CheXNet even when given 80% less training data. By further
visualizing the models' decision-making regions, we revealed that CheXNet
learned patterns other than the target lesions, demonstrating its shortcut
learning defect. Moreover, CheXDet achieved significantly better external
performance than CheXNet on both the image-level classification task and the
lesion localization task. Our findings suggest improving annotation granularity
for training deep learning systems as a promising way to elevate future deep
learning-based diagnosis systems for clinical usage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Luo_L/0/1/0/all/0/1"&gt;Luyang Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xiao_Y/0/1/0/all/0/1"&gt;Yongjie Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yanning Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vardhanabhuti_V/0/1/0/all/0/1"&gt;Varut Vardhanabhuti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_M/0/1/0/all/0/1"&gt;Mingxiang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Heng_P/0/1/0/all/0/1"&gt;Pheng-Ann Heng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer-based Conditional Variational Autoencoder for Controllable Story Generation. (arXiv:2101.00828v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00828</id>
        <link href="http://arxiv.org/abs/2101.00828"/>
        <updated>2021-07-09T01:58:26.805Z</updated>
        <summary type="html"><![CDATA[We investigate large-scale latent variable models (LVMs) for neural story
generation -- an under-explored application for open-domain long text -- with
objectives in two threads: generation effectiveness and controllability. LVMs,
especially the variational autoencoder (VAE), have achieved both effective and
controllable generation through exploiting flexible distributional latent
representations. Recently, Transformers and its variants have achieved
remarkable effectiveness without explicit latent representation learning, thus
lack satisfying controllability in generation. In this paper, we advocate to
revive latent variable modeling, essentially the power of representation
learning, in the era of Transformers to enhance controllability without hurting
state-of-the-art generation effectiveness. Specifically, we integrate latent
representation vectors with a Transformer-based pre-trained architecture to
build conditional variational autoencoder (CVAE). Model components such as
encoder, decoder and the variational posterior are all built on top of
pre-trained language models -- GPT2 specifically in this paper. Experiments
demonstrate state-of-the-art conditional generation ability of our model, as
well as its excellent representation learning capability and controllability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1"&gt;Le Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_T/0/1/0/all/0/1"&gt;Tao Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chaochun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1"&gt;Liefeng Bo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1"&gt;Wen Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Changyou Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral decoupling allows training transferable neural networks in medical imaging. (arXiv:2103.17171v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.17171</id>
        <link href="http://arxiv.org/abs/2103.17171"/>
        <updated>2021-07-09T01:58:26.798Z</updated>
        <summary type="html"><![CDATA[Many current neural networks for medical imaging generalise poorly to data
unseen during training. Such behaviour can be caused by networks overfitting
easy-to-learn, or statistically dominant, features while disregarding other
potentially informative features. For example, indistinguishable differences in
the sharpness of the images from two different scanners can degrade the
performance of the network significantly. All neural networks intended for
clinical practice need to be robust to variation in data caused by differences
in imaging equipment, sample preparation and patient populations.

To address these challenges, we evaluate the utility of spectral decoupling
as an implicit bias mitigation method. Spectral decoupling encourages the
neural network to learn more features by simply regularising the networks'
unnormalised prediction scores with an L2 penalty, thus having no added
computational costs.

We show that spectral decoupling allows training neural networks on datasets
with strong spurious correlations. Networks trained without spectral decoupling
do not learn the original task and appear to make false predictions based on
the spurious correlations. Spectral decoupling also increases networks'
robustness for data distribution shifts. To validate our findings, we train
networks with and without spectral decoupling to detect prostate cancer tissue
slides and COVID-19 in chest radiographs. Networks trained with spectral
decoupling achieve substantially higher performance on all evaluation datasets.

Our results show that spectral decoupling helps with generalisation issues
associated with neural networks. We recommend using spectral decoupling as an
implicit bias mitigation method in any neural network intended for clinical
use.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Pohjonen_J/0/1/0/all/0/1"&gt;Joona Pohjonen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sturenberg_C/0/1/0/all/0/1"&gt;Carolin St&amp;#xfc;renberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rannikko_A/0/1/0/all/0/1"&gt;Antti Rannikko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mirtti_T/0/1/0/all/0/1"&gt;Tuomas Mirtti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pitkanen_E/0/1/0/all/0/1"&gt;Esa Pitk&amp;#xe4;nen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly-Supervised Online Hashing. (arXiv:2009.07436v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.07436</id>
        <link href="http://arxiv.org/abs/2009.07436"/>
        <updated>2021-07-09T01:58:26.790Z</updated>
        <summary type="html"><![CDATA[With the rapid development of social websites, recent years have witnessed an
explosive growth of social images with user-provided tags which continuously
arrive in a streaming fashion. Due to the fast query speed and low storage
cost, hashing-based methods for image search have attracted increasing
attention. However, existing hashing methods for social image retrieval are
based on batch mode which violates the nature of social images, i.e., social
images are usually generated periodically or collected in a stream fashion.
Although there exist many online image hashing methods, they either adopt
unsupervised learning which ignore the relevant tags, or are designed in the
supervised manner which needs high-quality labels. In this paper, to overcome
the above limitations, we propose a new method named Weakly-supervised Online
Hashing (WOH). In order to learn high-quality hash codes, WOH exploits the weak
supervision by considering the semantics of tags and removing the noise.
Besides, We develop a discrete online optimization algorithm for WOH, which is
efficient and scalable. Extensive experiments conducted on two real-world
datasets demonstrate the superiority of WOH compared with several
state-of-the-art hashing baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1"&gt;Yu-Wei Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xin Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yongxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhen-Duo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xin-Shun Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Worsening Perception: Real-time Degradation of Autonomous Vehicle Perception Performance for Simulation of Adverse Weather Conditions. (arXiv:2103.02760v4 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02760</id>
        <link href="http://arxiv.org/abs/2103.02760"/>
        <updated>2021-07-09T01:58:26.770Z</updated>
        <summary type="html"><![CDATA[Autonomous vehicles rely heavily upon their perception subsystems to see the
environment in which they operate. Unfortunately, the effect of variable
weather conditions presents a significant challenge to object detection
algorithms, and thus it is imperative to test the vehicle extensively in all
conditions which it may experience. However, development of robust autonomous
vehicle subsystems requires repeatable, controlled testing - while real weather
is unpredictable and cannot be scheduled. Real-world testing in adverse
conditions is an expensive and time-consuming task, often requiring access to
specialist facilities. Simulation is commonly relied upon as a substitute, with
increasingly visually realistic representations of the real-world being
developed. In the context of the complete autonomous vehicle control pipeline,
subsystems downstream of perception need to be tested with accurate recreations
of the perception system output, rather than focusing on subjective visual
realism of the input - whether in simulation or the real world. This study
develops the untapped potential of a lightweight weather augmentation method in
an autonomous racing vehicle - focusing not on visual accuracy, but rather the
effect upon perception subsystem performance in real time. With minimal
adjustment, the prototype developed in this study can replicate the effects of
water droplets on the camera lens, and fading light conditions. This approach
introduces a latency of less than 8 ms using compute hardware well suited to
being carried in the vehicle - rendering it ideal for real-time implementation
that can be run during experiments in simulation, and augmented reality testing
in the real world.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fursa_I/0/1/0/all/0/1"&gt;Ivan Fursa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fandi_E/0/1/0/all/0/1"&gt;Elias Fandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musat_V/0/1/0/all/0/1"&gt;Valentina Musat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Culley_J/0/1/0/all/0/1"&gt;Jacob Culley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gil_E/0/1/0/all/0/1"&gt;Enric Gil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teeti_I/0/1/0/all/0/1"&gt;Izzeddin Teeti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bilous_L/0/1/0/all/0/1"&gt;Louise Bilous&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sluis_I/0/1/0/all/0/1"&gt;Isaac Vander Sluis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rast_A/0/1/0/all/0/1"&gt;Alexander Rast&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bradley_A/0/1/0/all/0/1"&gt;Andrew Bradley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Safety Envelopes using Light Curtains with Probabilistic Guarantees. (arXiv:2107.04000v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04000</id>
        <link href="http://arxiv.org/abs/2107.04000"/>
        <updated>2021-07-09T01:58:26.762Z</updated>
        <summary type="html"><![CDATA[To safely navigate unknown environments, robots must accurately perceive
dynamic obstacles. Instead of directly measuring the scene depth with a LiDAR
sensor, we explore the use of a much cheaper and higher resolution sensor:
programmable light curtains. Light curtains are controllable depth sensors that
sense only along a surface that a user selects. We use light curtains to
estimate the safety envelope of a scene: a hypothetical surface that separates
the robot from all obstacles. We show that generating light curtains that sense
random locations (from a particular distribution) can quickly discover the
safety envelope for scenes with unknown objects. Importantly, we produce
theoretical safety guarantees on the probability of detecting an obstacle using
random curtains. We combine random curtains with a machine learning based model
that forecasts and tracks the motion of the safety envelope efficiently. Our
method accurately estimates safety envelopes while providing probabilistic
safety guarantees that can be used to certify the efficacy of a robot
perception system to detect and avoid dynamic obstacles. We evaluate our
approach in a simulated urban driving environment and a real-world environment
with moving pedestrians using a light curtain device and show that we can
estimate safety envelopes efficiently and effectively. Project website:
https://siddancha.github.io/projects/active-safety-envelopes-with-guarantees]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ancha_S/0/1/0/all/0/1"&gt;Siddharth Ancha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pathak_G/0/1/0/all/0/1"&gt;Gaurav Pathak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narasimhan_S/0/1/0/all/0/1"&gt;Srinivasa G. Narasimhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1"&gt;David Held&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-frame Collaboration for Effective Endoscopic Video Polyp Detection via Spatial-Temporal Feature Transformation. (arXiv:2107.03609v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03609</id>
        <link href="http://arxiv.org/abs/2107.03609"/>
        <updated>2021-07-09T01:58:26.755Z</updated>
        <summary type="html"><![CDATA[Precise localization of polyp is crucial for early cancer screening in
gastrointestinal endoscopy. Videos given by endoscopy bring both richer
contextual information as well as more challenges than still images. The
camera-moving situation, instead of the common camera-fixed-object-moving one,
leads to significant background variation between frames. Severe internal
artifacts (e.g. water flow in the human body, specular reflection by tissues)
can make the quality of adjacent frames vary considerately. These factors
hinder a video-based model to effectively aggregate features from neighborhood
frames and give better predictions. In this paper, we present Spatial-Temporal
Feature Transformation (STFT), a multi-frame collaborative framework to address
these issues. Spatially, STFT mitigates inter-frame variations in the
camera-moving situation with feature alignment by proposal-guided deformable
convolutions. Temporally, STFT proposes a channel-aware attention module to
simultaneously estimate the quality and correlation of adjacent frames for
adaptive feature aggregation. Empirical studies and superior results
demonstrate the effectiveness and stability of our method. For example, STFT
improves the still image baseline FCOS by 10.6% and 20.6% on the comprehensive
F1-score of the polyp localization task in CVC-Clinic and ASUMayo datasets,
respectively, and outperforms the state-of-the-art video-based method by 3.6%
and 8.0%, respectively. Code is available at
\url{https://github.com/lingyunwu14/STFT}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lingyun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhiqiang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1"&gt;Yuanfeng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1"&gt;Ping Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shaoting Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Based Image Retrieval in the JPEG Compressed Domain. (arXiv:2107.03648v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.03648</id>
        <link href="http://arxiv.org/abs/2107.03648"/>
        <updated>2021-07-09T01:58:26.748Z</updated>
        <summary type="html"><![CDATA[Content-based image retrieval (CBIR) systems on pixel domain use low-level
features, such as colour, texture and shape, to retrieve images. In this
context, two types of image representations i.e. local and global image
features have been studied in the literature. Extracting these features from
pixel images and comparing them with images from the database is very
time-consuming. Therefore, in recent years, there has been some effort to
accomplish image analysis directly in the compressed domain with lesser
computations. Furthermore, most of the images in our daily transactions are
stored in the JPEG compressed format. Therefore, it would be ideal if we could
retrieve features directly from the partially decoded or compressed data and
use them for retrieval. Here, we propose a unified model for image retrieval
which takes DCT coefficients as input and efficiently extracts global and local
features directly in the JPEG compressed domain for accurate image retrieval.
The experimental findings indicate that our proposed model performed similarly
to the current DELG model which takes RGB features as an input with reference
to mean average precision while having a faster training and retrieval speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Temburwar_S/0/1/0/all/0/1"&gt;Shrikant Temburwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rajesh_B/0/1/0/all/0/1"&gt;Bulla Rajesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Javed_M/0/1/0/all/0/1"&gt;Mohammed Javed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers. (arXiv:2107.03996v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03996</id>
        <link href="http://arxiv.org/abs/2107.03996"/>
        <updated>2021-07-09T01:58:26.708Z</updated>
        <summary type="html"><![CDATA[We propose to address quadrupedal locomotion tasks using Reinforcement
Learning (RL) with a Transformer-based model that learns to combine
proprioceptive information and high-dimensional depth sensor inputs. While
learning-based locomotion has made great advances using RL, most methods still
rely on domain randomization for training blind agents that generalize to
challenging terrains. Our key insight is that proprioceptive states only offer
contact measurements for immediate reaction, whereas an agent equipped with
visual sensory observations can learn to proactively maneuver environments with
obstacles and uneven terrain by anticipating changes in the environment many
steps ahead. In this paper, we introduce LocoTransformer, an end-to-end RL
method for quadrupedal locomotion that leverages a Transformer-based model for
fusing proprioceptive states and visual observations. We evaluate our method in
challenging simulated environments with different obstacles and uneven terrain.
We show that our method obtains significant improvements over policies with
only proprioceptive state inputs, and that Transformer-based models further
improve generalization across environments. Our project page with videos is at
https://RchalYang.github.io/LocoTransformer .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1"&gt;Ruihan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Minghao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hansen_N/0/1/0/all/0/1"&gt;Nicklas Hansen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Huazhe Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaolong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decoding Methods for Neural Narrative Generation. (arXiv:2010.07375v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07375</id>
        <link href="http://arxiv.org/abs/2010.07375"/>
        <updated>2021-07-09T01:58:26.695Z</updated>
        <summary type="html"><![CDATA[Narrative generation is an open-ended NLP task in which a model generates a
story given a prompt. The task is similar to neural response generation for
chatbots; however, innovations in response generation are often not applied to
narrative generation, despite the similarity between these tasks. We aim to
bridge this gap by applying and evaluating advances in decoding methods for
neural response generation to neural narrative generation. In particular, we
employ GPT-2 and perform ablations across nucleus sampling thresholds and
diverse decoding hyperparameters -- specifically, maximum mutual information --
analyzing results over multiple criteria with automatic and human evaluation.
We find that (1) nucleus sampling is generally best with thresholds between 0.7
and 0.9; (2) a maximum mutual information objective can improve the quality of
generated stories; and (3) established automatic metrics do not correlate well
with human judgments of narrative quality on any qualitative metric.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+DeLucia_A/0/1/0/all/0/1"&gt;Alexandra DeLucia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mueller_A/0/1/0/all/0/1"&gt;Aaron Mueller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang Lisa Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Sedoc&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consensus Clustering With Unsupervised Representation Learning. (arXiv:2010.01245v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.01245</id>
        <link href="http://arxiv.org/abs/2010.01245"/>
        <updated>2021-07-09T01:58:26.540Z</updated>
        <summary type="html"><![CDATA[Recent advances in deep clustering and unsupervised representation learning
are based on the idea that different views of an input image (generated through
data augmentation techniques) must either be closer in the representation
space, or have a similar cluster assignment. Bootstrap Your Own Latent (BYOL)
is one such representation learning algorithm that has achieved
state-of-the-art results in self-supervised image classification on ImageNet
under the linear evaluation protocol. However, the utility of the learnt
features of BYOL to perform clustering is not explored. In this work, we study
the clustering ability of BYOL and observe that features learnt using BYOL may
not be optimal for clustering. We propose a novel consensus clustering based
loss function, and train BYOL with the proposed loss in an end-to-end way that
improves the clustering ability and outperforms similar clustering based
methods on some popular computer vision datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Regatti_J/0/1/0/all/0/1"&gt;Jayanth Reddy Regatti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deshmukh_A/0/1/0/all/0/1"&gt;Aniket Anand Deshmukh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manavoglu_E/0/1/0/all/0/1"&gt;Eren Manavoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dogan_U/0/1/0/all/0/1"&gt;Urun Dogan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse and Structured Visual Attention. (arXiv:2002.05556v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.05556</id>
        <link href="http://arxiv.org/abs/2002.05556"/>
        <updated>2021-07-09T01:58:26.488Z</updated>
        <summary type="html"><![CDATA[Visual attention mechanisms are widely used in multimodal tasks, as visual
question answering (VQA). One drawback of softmax-based attention mechanisms is
that they assign some probability mass to all image regions, regardless of
their adjacency structure and of their relevance to the text. In this paper, to
better link the image structure with the text, we replace the traditional
softmax attention mechanism with two alternative sparsity-promoting
transformations: sparsemax, which is able to select only the relevant regions
(assigning zero weight to the rest), and a newly proposed Total-Variation
Sparse Attention (TVmax), which further encourages the joint selection of
adjacent spatial locations. Experiments in VQA show gains in accuracy as well
as higher similarity to human attention, which suggests better
interpretability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Martins_P/0/1/0/all/0/1"&gt;Pedro Henrique Martins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niculae_V/0/1/0/all/0/1"&gt;Vlad Niculae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marinho_Z/0/1/0/all/0/1"&gt;Zita Marinho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Martins&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video 3D Sampling for Self-supervised Representation Learning. (arXiv:2107.03578v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03578</id>
        <link href="http://arxiv.org/abs/2107.03578"/>
        <updated>2021-07-09T01:58:26.456Z</updated>
        <summary type="html"><![CDATA[Most of the existing video self-supervised methods mainly leverage temporal
signals of videos, ignoring that the semantics of moving objects and
environmental information are all critical for video-related tasks. In this
paper, we propose a novel self-supervised method for video representation
learning, referred to as Video 3D Sampling (V3S). In order to sufficiently
utilize the information (spatial and temporal) provided in videos, we
pre-process a video from three dimensions (width, height, time). As a result,
we can leverage the spatial information (the size of objects), temporal
information (the direction and magnitude of motions) as our learning target. In
our implementation, we combine the sampling of the three dimensions and propose
the scale and projection transformations in space and time respectively. The
experimental results show that, when applied to action recognition, video
retrieval and action similarity labeling, our approach improves the
state-of-the-arts with significant margins.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1"&gt;Dezhao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1"&gt;Bo Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weiping Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The value of text for small business default prediction: A deep learning approach. (arXiv:2003.08964v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.08964</id>
        <link href="http://arxiv.org/abs/2003.08964"/>
        <updated>2021-07-09T01:58:26.416Z</updated>
        <summary type="html"><![CDATA[Compared to consumer lending, Micro, Small and Medium Enterprise (mSME)
credit risk modelling is particularly challenging, as, often, the same sources
of information are not available. Therefore, it is standard policy for a loan
officer to provide a textual loan assessment to mitigate limited data
availability. In turn, this statement is analysed by a credit expert alongside
any available standard credit data. In our paper, we exploit recent advances
from the field of Deep Learning and Natural Language Processing (NLP),
including the BERT (Bidirectional Encoder Representations from Transformers)
model, to extract information from 60 000 textual assessments provided by a
lender. We consider the performance in terms of the AUC (Area Under the
receiver operating characteristic Curve) and Brier Score metrics and find that
the text alone is surprisingly effective for predicting default. However, when
combined with traditional data, it yields no additional predictive capability,
with performance dependent on the text's length. Our proposed deep learning
model does, however, appear to be robust to the quality of the text and
therefore suitable for partly automating the mSME lending process. We also
demonstrate how the content of loan assessments influences performance, leading
us to a series of recommendations on a new strategy for collecting future mSME
loan assessments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stevenson_M/0/1/0/all/0/1"&gt;Matthew Stevenson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mues_C/0/1/0/all/0/1"&gt;Christophe Mues&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bravo_C/0/1/0/all/0/1"&gt;Cristi&amp;#xe1;n Bravo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting the relationship between visual and textual features in social networks for image classification with zero-shot deep learning. (arXiv:2107.03751v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03751</id>
        <link href="http://arxiv.org/abs/2107.03751"/>
        <updated>2021-07-09T01:58:26.406Z</updated>
        <summary type="html"><![CDATA[One of the main issues related to unsupervised machine learning is the cost
of processing and extracting useful information from large datasets. In this
work, we propose a classifier ensemble based on the transferable learning
capabilities of the CLIP neural network architecture in multimodal environments
(image and text) from social media. For this purpose, we used the InstaNY100K
dataset and proposed a validation approach based on sampling techniques. Our
experiments, based on image classification tasks according to the labels of the
Places dataset, are performed by first considering only the visual part, and
then adding the associated texts as support. The results obtained demonstrated
that trained neural networks such as CLIP can be successfully applied to image
classification with little fine-tuning, and considering the associated texts to
the images can help to improve the accuracy depending on the goal. The results
demonstrated what seems to be a promising research direction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lucas_L/0/1/0/all/0/1"&gt;Luis Lucas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomas_D/0/1/0/all/0/1"&gt;David Tomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_Rodriguez_J/0/1/0/all/0/1"&gt;Jose Garcia-Rodriguez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Technical Report for Valence-Arousal Estimation in ABAW2 Challenge. (arXiv:2107.03891v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03891</id>
        <link href="http://arxiv.org/abs/2107.03891"/>
        <updated>2021-07-09T01:58:26.398Z</updated>
        <summary type="html"><![CDATA[In this work, we describe our method for tackling the valence-arousal
estimation challenge from ABAW2 ICCV-2021 Competition. The competition
organizers provide an in-the-wild Aff-Wild2 dataset for participants to analyze
affective behavior in real-life settings. We use a two stream model to learn
emotion features from appearance and action respectively. To solve data
imbalanced problem, we apply label distribution smoothing (LDS) to re-weight
labels. Our proposed method achieves Concordance Correlation Coefficient (CCC)
of 0.591 and 0.617 for valence and arousal on the validation set of Aff-wild2
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1"&gt;Hong-Xia Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1"&gt;I-Hsuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lo_L/0/1/0/all/0/1"&gt;Ling Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shuai_H/0/1/0/all/0/1"&gt;Hong-Han Shuai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1"&gt;Wen-Huang Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FarsTail: A Persian Natural Language Inference Dataset. (arXiv:2009.08820v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08820</id>
        <link href="http://arxiv.org/abs/2009.08820"/>
        <updated>2021-07-09T01:58:26.389Z</updated>
        <summary type="html"><![CDATA[Natural language inference (NLI) is known as one of the central tasks in
natural language processing (NLP) which encapsulates many fundamental aspects
of language understanding. With the considerable achievements of data-hungry
deep learning methods in NLP tasks, a great amount of e ort has been devoted to
develop more diverse datasets for di erent languages. In this paper, we present
a new dataset for the NLI task in the Persian language, also known as Farsi,
which is one of the dominant languages in the Middle East. This dataset, named
FarsTail, includes 10,367 samples which are provided in both the Persian
language as well as the indexed format to be useful for non-Persian
researchers. The samples are generated from 3,539 multiple-choice questions
with the least amount of annotator interventions in a way similar to the
SciTail dataset. A carefully designed multi-step process is adopted to ensure
the quality of the dataset. We also present the results of traditional and
state-of-the-art methods on FarsTail including di erent embedding methods such
as word2vec, fastText, ELMo, BERT, and LASER, as well as di erent modeling
approaches such as DecompAtt, ESIM, HBMP, and ULMFiT to provide a solid
baseline for the future research. The best obtained test accuracy is 83.38%
which shows that there is a big room for improving the current methods to be
useful for real-world NLP applications in di erent languages. We also
investigate the extent to which the models exploit super cial clues, also known
as dataset biases, in FarsTail, and partition the test set into easy and hard
subsets according to the success of biased models. The dataset is available at
https://github.com/dml-qom/ FarsTail.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Amirkhani_H/0/1/0/all/0/1"&gt;Hossein Amirkhani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+AzariJafari_M/0/1/0/all/0/1"&gt;Mohammad AzariJafari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pourjafari_Z/0/1/0/all/0/1"&gt;Zohreh Pourjafari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faridan_Jahromi_S/0/1/0/all/0/1"&gt;Soroush Faridan-Jahromi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kouhkan_Z/0/1/0/all/0/1"&gt;Zeinab Kouhkan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amirak_A/0/1/0/all/0/1"&gt;Azadeh Amirak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Case-based similar image retrieval for weakly annotated large histopathological images of malignant lymphoma using deep metric learning. (arXiv:2107.03602v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03602</id>
        <link href="http://arxiv.org/abs/2107.03602"/>
        <updated>2021-07-09T01:58:26.367Z</updated>
        <summary type="html"><![CDATA[In the present study, we propose a novel case-based similar image retrieval
(SIR) method for hematoxylin and eosin (H&E)-stained histopathological images
of malignant lymphoma. When a whole slide image (WSI) is used as an input
query, it is desirable to be able to retrieve similar cases by focusing on
image patches in pathologically important regions such as tumor cells. To
address this problem, we employ attention-based multiple instance learning,
which enables us to focus on tumor-specific regions when the similarity between
cases is computed. Moreover, we employ contrastive distance metric learning to
incorporate immunohistochemical (IHC) staining patterns as useful supervised
information for defining appropriate similarity between heterogeneous malignant
lymphoma cases. In the experiment with 249 malignant lymphoma patients, we
confirmed that the proposed method exhibited higher evaluation measures than
the baseline case-based SIR methods. Furthermore, the subjective evaluation by
pathologists revealed that our similarity measure using IHC staining patterns
is appropriate for representing the similarity of H&E-stained tissue images for
malignant lymphoma.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hashimoto_N/0/1/0/all/0/1"&gt;Noriaki Hashimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takagi_Y/0/1/0/all/0/1"&gt;Yusuke Takagi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masuda_H/0/1/0/all/0/1"&gt;Hiroki Masuda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miyoshi_H/0/1/0/all/0/1"&gt;Hiroaki Miyoshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kohno_K/0/1/0/all/0/1"&gt;Kei Kohno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagaishi_M/0/1/0/all/0/1"&gt;Miharu Nagaishi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sato_K/0/1/0/all/0/1"&gt;Kensaku Sato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takeuchi_M/0/1/0/all/0/1"&gt;Mai Takeuchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Furuta_T/0/1/0/all/0/1"&gt;Takuya Furuta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawamoto_K/0/1/0/all/0/1"&gt;Keisuke Kawamoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamada_K/0/1/0/all/0/1"&gt;Kyohei Yamada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moritsubo_M/0/1/0/all/0/1"&gt;Mayuko Moritsubo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Inoue_K/0/1/0/all/0/1"&gt;Kanako Inoue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shimasaki_Y/0/1/0/all/0/1"&gt;Yasumasa Shimasaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ogura_Y/0/1/0/all/0/1"&gt;Yusuke Ogura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Imamoto_T/0/1/0/all/0/1"&gt;Teppei Imamoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishina_T/0/1/0/all/0/1"&gt;Tatsuzo Mishina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ohshima_K/0/1/0/all/0/1"&gt;Koichi Ohshima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hontani_H/0/1/0/all/0/1"&gt;Hidekata Hontani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takeuchi_I/0/1/0/all/0/1"&gt;Ichiro Takeuchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comprehensive Assessment of Dialog Evaluation Metrics. (arXiv:2106.03706v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03706</id>
        <link href="http://arxiv.org/abs/2106.03706"/>
        <updated>2021-07-09T01:58:26.360Z</updated>
        <summary type="html"><![CDATA[Automatic evaluation metrics are a crucial component of dialog systems
research. Standard language evaluation metrics are known to be ineffective for
evaluating dialog. As such, recent research has proposed a number of novel,
dialog-specific metrics that correlate better with human judgements. Due to the
fast pace of research, many of these metrics have been assessed on different
datasets and there has as yet been no time for a systematic comparison between
them. To this end, this paper provides a comprehensive assessment of recently
proposed dialog evaluation metrics on a number of datasets. In this paper, 23
different automatic evaluation metrics are evaluated on 10 different datasets.
Furthermore, the metrics are assessed in different settings, to better qualify
their respective strengths and weaknesses. Metrics are assessed (1) on both the
turn level and the dialog level, (2) for different dialog lengths, (3) for
different dialog qualities (e.g., coherence, engaging), (4) for different types
of response generation models (i.e., generative, retrieval, simple models and
state-of-the-art models), (5) taking into account the similarity of different
metrics and (6) exploring combinations of different metrics. This comprehensive
assessment offers several takeaways pertaining to dialog evaluation metrics in
general. It also suggests how to best assess evaluation metrics and indicates
promising directions for future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yeh_Y/0/1/0/all/0/1"&gt;Yi-Ting Yeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eskenazi_M/0/1/0/all/0/1"&gt;Maxine Eskenazi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehri_S/0/1/0/all/0/1"&gt;Shikib Mehri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CamTuner: Reinforcement-Learning based System for Camera Parameter Tuning to enhance Analytics. (arXiv:2107.03964v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03964</id>
        <link href="http://arxiv.org/abs/2107.03964"/>
        <updated>2021-07-09T01:58:26.350Z</updated>
        <summary type="html"><![CDATA[Complex sensors like video cameras include tens of configurable parameters,
which can be set by end-users to customize the sensors to specific application
scenarios. Although parameter settings significantly affect the quality of the
sensor output and the accuracy of insights derived from sensor data, most
end-users use a fixed parameter setting because they lack the skill or
understanding to appropriately configure these parameters. We propose CamTuner,
which is a system to automatically, and dynamically adapt the complex sensor to
changing environments. CamTuner includes two key components. First, a bespoke
analytics quality estimator, which is a deep-learning model to automatically
and continuously estimate the quality of insights from an analytics unit as the
environment around a sensor change. Second, a reinforcement learning (RL)
module, which reacts to the changes in quality, and automatically adjusts the
camera parameters to enhance the accuracy of insights. We improve the training
time of the RL module by an order of magnitude by designing virtual models to
mimic essential behavior of the camera: we design virtual knobs that can be set
to different values to mimic the effects of assigning different values to the
camera's configurable parameters, and we design a virtual camera model that
mimics the output from a video camera at different times of the day. These
virtual models significantly accelerate training because (a) frame rates from a
real camera are limited to 25-30 fps while the virtual models enable processing
at 300 fps, (b) we do not have to wait until the real camera sees different
environments, which could take weeks or months, and (c) virtual knobs can be
updated instantly, while it can take 200-500 ms to change the camera parameter
settings. Our dynamic tuning approach results in up to 12% improvement in the
accuracy of insights from several video analytics tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1"&gt;Sibendu Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1"&gt;Kunal Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coviello_G/0/1/0/all/0/1"&gt;Giuseppe Coviello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sankaradas_M/0/1/0/all/0/1"&gt;Murugan Sankaradas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Po_O/0/1/0/all/0/1"&gt;Oliver Po&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Y. Charlie Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakradhar_S/0/1/0/all/0/1"&gt;Srimat T. Chakradhar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the Effectiveness and Efficiency of Stochastic Neighbour Embedding with Isolation Kernel. (arXiv:1906.09744v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.09744</id>
        <link href="http://arxiv.org/abs/1906.09744"/>
        <updated>2021-07-09T01:58:26.343Z</updated>
        <summary type="html"><![CDATA[This paper presents a new insight into improving the performance of
Stochastic Neighbour Embedding (t-SNE) by using Isolation kernel instead of
Gaussian kernel. Isolation kernel outperforms Gaussian kernel in two aspects.
First, the use of Isolation kernel in t-SNE overcomes the drawback of
misrepresenting some structures in the data, which often occurs when Gaussian
kernel is applied in t-SNE. This is because Gaussian kernel determines each
local bandwidth based on one local point only, while Isolation kernel is
derived directly from the data based on space partitioning. Second, the use of
Isolation kernel yields a more efficient similarity computation because
data-dependent Isolation kernel has only one parameter that needs to be tuned.
In contrast, the use of data-independent Gaussian kernel increases the
computational cost by determining n bandwidths for a dataset of n points. As
the root cause of these deficiencies in t-SNE is Gaussian kernel, we show that
simply replacing Gaussian kernel with Isolation kernel in t-SNE significantly
improves the quality of the final visualisation output (without creating
misrepresented structures) and removes one key obstacle that prevents t-SNE
from processing large datasets. Moreover, Isolation kernel enables t-SNE to
deal with large-scale datasets in less runtime without trading off accuracy,
unlike existing methods in speeding up t-SNE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Ye Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ting_K/0/1/0/all/0/1"&gt;Kai Ming Ting&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectroscopic Approach to Correction and Visualisation of Bright-Field Light Transmission Microscopy Biological Data. (arXiv:1903.06519v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1903.06519</id>
        <link href="http://arxiv.org/abs/1903.06519"/>
        <updated>2021-07-09T01:58:26.336Z</updated>
        <summary type="html"><![CDATA[The most realistic information about the transparent sample such as a live
cell can be obtained only using bright-field light microscopy. At
high-intensity pulsing LED illumination, we captured a primary
12-bit-per-channel (bpc) response from an observed sample using a bright-field
microscope equipped with a high-resolution (4872x3248) image sensor. In order
to suppress data distortions originating from the light interactions with
elements in the optical path, poor sensor reproduction (geometrical defects of
the camera sensor and some peculiarities of sensor sensitivity), we propose a
spectroscopic approach for the correction of this uncompressed 12-bpc data by
simultaneous calibration of all parts of the experimental arrangement.
Moreover, the final intensities of the corrected images are proportional to the
photon fluxes detected by a camera sensor. It can be visualized in 8-bpc
intensity depth after the Least Information Loss compression [Lect. Notes
Bioinform. 9656, 527 (2016)].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Platonova_G/0/1/0/all/0/1"&gt;Ganna Platonova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Stys_D/0/1/0/all/0/1"&gt;Dalibor Stys&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Soucek_P/0/1/0/all/0/1"&gt;Pavel Soucek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lonhus_K/0/1/0/all/0/1"&gt;Kirill Lonhus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Valenta_J/0/1/0/all/0/1"&gt;Jan Valenta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rychtarikova_R/0/1/0/all/0/1"&gt;Renata Rychtarikova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regional Differential Information Entropy for Super-Resolution Image Quality Assessment. (arXiv:2107.03642v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.03642</id>
        <link href="http://arxiv.org/abs/2107.03642"/>
        <updated>2021-07-09T01:58:26.316Z</updated>
        <summary type="html"><![CDATA[PSNR and SSIM are the most widely used metrics in super-resolution problems,
because they are easy to use and can evaluate the similarities between
generated images and reference images. However, single image super-resolution
is an ill-posed problem, there are multiple corresponding high-resolution
images for the same low-resolution image. The similarities can't totally
reflect the restoration effect. The perceptual quality of generated images is
also important, but PSNR and SSIM do not reflect perceptual quality well. To
solve the problem, we proposed a method called regional differential
information entropy to measure both of the similarities and perceptual quality.
To overcome the problem that traditional image information entropy can't
reflect the structure information, we proposed to measure every region's
information entropy with sliding window. Considering that the human visual
system is more sensitive to the brightness difference at low brightness, we
take $\gamma$ quantization rather than linear quantization. To accelerate the
method, we reorganized the calculation procedure of information entropy with a
neural network. Through experiments on our IQA dataset and PIPAL, this paper
proves that RDIE can better quantify perceptual quality of images especially
GAN-based images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xu_N/0/1/0/all/0/1"&gt;Ningyuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhuang_J/0/1/0/all/0/1"&gt;Jiayan Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1"&gt;Jiangjian Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Peng_C/0/1/0/all/0/1"&gt;Chengbin Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Embedded Iris Recognition System Optimization using Dynamically ReconfigurableDecoder with LDPC Codes. (arXiv:2107.03688v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03688</id>
        <link href="http://arxiv.org/abs/2107.03688"/>
        <updated>2021-07-09T01:58:26.305Z</updated>
        <summary type="html"><![CDATA[Extracting and analyzing iris textures for biometric recognition has been
extensively studied. As the transition of iris recognition from lab technology
to nation-scale applications, most systems are facing high complexity in either
time or space, leading to unfitness for embedded devices. In this paper, the
proposed design includes a minimal set of computer vision modules and
multi-mode QC-LDPC decoder which can alleviate variability and noise caused by
iris acquisition and follow-up process. Several classes of QC-LDPC code from
IEEE 802.16 are tested for the validity of accuracy improvement. Some of the
codes mentioned above are used for further QC-LDPC decoder quantization,
validation and comparison to each other. We show that we can apply Dynamic
Partial Reconfiguration technology to implement the multi-mode QC-LDPC decoder
for the iris recognition system. The results show that the implementation is
power-efficient and good for edge applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Longyu Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sham_C/0/1/0/all/0/1"&gt;Chiu-Wing Sham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lo_C/0/1/0/all/0/1"&gt;Chun Yan Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1"&gt;Xinchao Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty-Aware Camera Pose Estimation from Points and Lines. (arXiv:2107.03890v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03890</id>
        <link href="http://arxiv.org/abs/2107.03890"/>
        <updated>2021-07-09T01:58:26.298Z</updated>
        <summary type="html"><![CDATA[Perspective-n-Point-and-Line (P$n$PL) algorithms aim at fast, accurate, and
robust camera localization with respect to a 3D model from 2D-3D feature
correspondences, being a major part of modern robotic and AR/VR systems.
Current point-based pose estimation methods use only 2D feature detection
uncertainties, and the line-based methods do not take uncertainties into
account. In our setup, both 3D coordinates and 2D projections of the features
are considered uncertain. We propose PnP(L) solvers based on EPnP and DLS for
the uncertainty-aware pose estimation. We also modify motion-only bundle
adjustment to take 3D uncertainties into account. We perform exhaustive
synthetic and real experiments on two different visual odometry datasets. The
new PnP(L) methods outperform the state-of-the-art on real data in isolation,
showing an increase in mean translation accuracy by 18% on a representative
subset of KITTI, while the new uncertain refinement improves pose accuracy for
most of the solvers, e.g. decreasing mean translation error for the EPnP by 16%
compared to the standard refinement on the same dataset. The code is available
at https://alexandervakhitov.github.io/uncertain-pnp/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vakhitov_A/0/1/0/all/0/1"&gt;Alexander Vakhitov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colomina_L/0/1/0/all/0/1"&gt;Luis Ferraz Colomina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agudo_A/0/1/0/all/0/1"&gt;Antonio Agudo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1"&gt;Francesc Moreno-Noguer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grid Partitioned Attention: Efficient TransformerApproximation with Inductive Bias for High Resolution Detail Generation. (arXiv:2107.03742v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03742</id>
        <link href="http://arxiv.org/abs/2107.03742"/>
        <updated>2021-07-09T01:58:26.290Z</updated>
        <summary type="html"><![CDATA[Attention is a general reasoning mechanism than can flexibly deal with image
information, but its memory requirements had made it so far impractical for
high resolution image generation. We present Grid Partitioned Attention (GPA),
a new approximate attention algorithm that leverages a sparse inductive bias
for higher computational and memory efficiency in image domains: queries attend
only to few keys, spatially close queries attend to close keys due to
correlations. Our paper introduces the new attention layer, analyzes its
complexity and how the trade-off between memory usage and model power can be
tuned by the hyper-parameters.We will show how such attention enables novel
deep learning architectures with copying modules that are especially useful for
conditional image generation tasks like pose morphing. Our contributions are
(i) algorithm and code1of the novel GPA layer, (ii) a novel deep
attention-copying architecture, and (iii) new state-of-the art experimental
results in human pose morphing generation benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jetchev_N/0/1/0/all/0/1"&gt;Nikolay Jetchev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yildirim_G/0/1/0/all/0/1"&gt;G&amp;#xf6;khan Yildirim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bracher_C/0/1/0/all/0/1"&gt;Christian Bracher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vollgraf_R/0/1/0/all/0/1"&gt;Roland Vollgraf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adiabatic Quantum Graph Matching with Permutation Matrix Constraints. (arXiv:2107.04032v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04032</id>
        <link href="http://arxiv.org/abs/2107.04032"/>
        <updated>2021-07-09T01:58:26.284Z</updated>
        <summary type="html"><![CDATA[Matching problems on 3D shapes and images are challenging as they are
frequently formulated as combinatorial quadratic assignment problems (QAPs)
with permutation matrix constraints, which are NP-hard. In this work, we
address such problems with emerging quantum computing technology and propose
several reformulations of QAPs as unconstrained problems suitable for efficient
execution on quantum hardware. We investigate several ways to inject
permutation matrix constraints in a quadratic unconstrained binary optimization
problem which can be mapped to quantum hardware. We focus on obtaining a
sufficient spectral gap, which further increases the probability to measure
optimal solutions and valid permutation matrices in a single run. We perform
our experiments on the quantum computer D-Wave 2000Q (2^11 qubits, adiabatic).
Despite the observed discrepancy between simulated adiabatic quantum computing
and execution on real quantum hardware, our reformulation of permutation matrix
constraints increases the robustness of the numerical computations over other
penalty approaches in our experiments. The proposed algorithm has the potential
to scale to higher dimensions on future quantum computing architectures, which
opens up multiple new directions for solving matching problems in 3D computer
vision and graphics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Benkner_M/0/1/0/all/0/1"&gt;Marcel Seelbach Benkner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1"&gt;Vladislav Golyanik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moeller_M/0/1/0/all/0/1"&gt;Michael Moeller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[4D Attention: Comprehensive Framework for Spatio-Temporal Gaze Mapping. (arXiv:2107.03606v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.03606</id>
        <link href="http://arxiv.org/abs/2107.03606"/>
        <updated>2021-07-09T01:58:26.261Z</updated>
        <summary type="html"><![CDATA[This study presents a framework for capturing human attention in the
spatio-temporal domain using eye-tracking glasses. Attention mapping is a key
technology for human perceptual activity analysis or Human-Robot Interaction
(HRI) to support human visual cognition; however, measuring human attention in
dynamic environments is challenging owing to the difficulty in localizing the
subject and dealing with moving objects. To address this, we present a
comprehensive framework, 4D Attention, for unified gaze mapping onto static and
dynamic objects. Specifically, we estimate the glasses pose by leveraging a
loose coupling of direct visual localization and Inertial Measurement Unit
(IMU) values. Further, by installing reconstruction components into our
framework, dynamic objects not captured in the 3D environment map are
instantiated based on the input images. Finally, a scene rendering component
synthesizes a first-person view with identification (ID) textures and performs
direct 2D-3D gaze association. Quantitative evaluations showed the
effectiveness of our framework. Additionally, we demonstrated the applications
of 4D Attention through experiments in real situations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oishi_S/0/1/0/all/0/1"&gt;Shuji Oishi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koide_K/0/1/0/all/0/1"&gt;Kenji Koide&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yokozuka_M/0/1/0/all/0/1"&gt;Masashi Yokozuka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banno_A/0/1/0/all/0/1"&gt;Atsuhiko Banno&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AC-VRNN: Attentive Conditional-VRNN for Multi-Future Trajectory Prediction. (arXiv:2005.08307v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.08307</id>
        <link href="http://arxiv.org/abs/2005.08307"/>
        <updated>2021-07-09T01:58:26.255Z</updated>
        <summary type="html"><![CDATA[Anticipating human motion in crowded scenarios is essential for developing
intelligent transportation systems, social-aware robots and advanced video
surveillance applications. A key component of this task is represented by the
inherently multi-modal nature of human paths which makes socially acceptable
multiple futures when human interactions are involved. To this end, we propose
a generative architecture for multi-future trajectory predictions based on
Conditional Variational Recurrent Neural Networks (C-VRNNs). Conditioning
mainly relies on prior belief maps, representing most likely moving directions
and forcing the model to consider past observed dynamics in generating future
positions. Human interactions are modeled with a graph-based attention
mechanism enabling an online attentive hidden state refinement of the recurrent
estimation. To corroborate our model, we perform extensive experiments on
publicly-available datasets (e.g., ETH/UCY, Stanford Drone Dataset, STATS
SportVU NBA, Intersection Drone Dataset and TrajNet++) and demonstrate its
effectiveness in crowded scenes compared to several state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bertugli_A/0/1/0/all/0/1"&gt;Alessia Bertugli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calderara_S/0/1/0/all/0/1"&gt;Simone Calderara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coscia_P/0/1/0/all/0/1"&gt;Pasquale Coscia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ballan_L/0/1/0/all/0/1"&gt;Lamberto Ballan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1"&gt;Rita Cucchiara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Resolution Susceptibility of Face Recognition Models. (arXiv:2107.03769v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03769</id>
        <link href="http://arxiv.org/abs/2107.03769"/>
        <updated>2021-07-09T01:58:26.248Z</updated>
        <summary type="html"><![CDATA[Face recognition approaches often rely on equal image resolution for
verification faces on two images. However, in practical applications, those
image resolutions are usually not in the same range due to different image
capture mechanisms or sources. In this work, we first analyze the impact of
image resolutions on the face verification performance with a state-of-the-art
face recognition model. For images, synthetically reduced to $5\, \times 5\,
\mathrm{px}$ resolution, the verification performance drops from $99.23\%$
increasingly down to almost $55\%$. Especially, for cross-resolution image
pairs (one high- and one low-resolution image), the verification accuracy
decreases even further. We investigate this behavior more in-depth by looking
at the feature distances for every 2-image test pair. To tackle this problem,
we propose the following two methods: 1) Train a state-of-the-art
face-recognition model straightforward with $50\%$ low-resolution images
directly within each batch. \\ 2) Train a siamese-network structure and adding
a cosine distance feature loss between high- and low-resolution features. Both
methods show an improvement for cross-resolution scenarios and can increase the
accuracy at very low resolution to approximately $70\%$. However, a
disadvantage is that a specific model needs to be trained for every
resolution-pair ...]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Knoche_M/0/1/0/all/0/1"&gt;Martin Knoche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hormann_S/0/1/0/all/0/1"&gt;Stefan H&amp;#xf6;rmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1"&gt;Gerhard Rigoll&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Atlas-Based Segmentation of Intracochlear Anatomy in Metal Artifact Affected CT Images of the Ear with Co-trained Deep Neural Networks. (arXiv:2107.03987v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.03987</id>
        <link href="http://arxiv.org/abs/2107.03987"/>
        <updated>2021-07-09T01:58:26.241Z</updated>
        <summary type="html"><![CDATA[We propose an atlas-based method to segment the intracochlear anatomy (ICA)
in the post-implantation CT (Post-CT) images of cochlear implant (CI)
recipients that preserves the point-to-point correspondence between the meshes
in the atlas and the segmented volumes. To solve this problem, which is
challenging because of the strong artifacts produced by the implant, we use a
pair of co-trained deep networks that generate dense deformation fields (DDFs)
in opposite directions. One network is tasked with registering an atlas image
to the Post-CT images and the other network is tasked with registering the
Post-CT images to the atlas image. The networks are trained using loss
functions based on voxel-wise labels, image content, fiducial registration
error, and cycle-consistency constraint. The segmentation of the ICA in the
Post-CT images is subsequently obtained by transferring the predefined
segmentation meshes of the ICA in the atlas image to the Post-CT images using
the corresponding DDFs generated by the trained registration networks. Our
model can learn the underlying geometric features of the ICA even though they
are obscured by the metal artifacts. We show that our end-to-end network
produces results that are comparable to the current state of the art (SOTA)
that relies on a two-steps approach that first uses conditional generative
adversarial networks to synthesize artifact-free images from the Post-CT images
and then uses an active shape model-based method to segment the ICA in the
synthetic images. Our method requires a fraction of the time needed by the
SOTA, which is important for end-user acceptance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Su_D/0/1/0/all/0/1"&gt;Dingjie Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1"&gt;Yubo Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chakravorti_S/0/1/0/all/0/1"&gt;Srijata Chakravorti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Noble_J/0/1/0/all/0/1"&gt;Jack H. Noble&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dawant_B/0/1/0/all/0/1"&gt;Be-noit M. Dawant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Label-set Loss Functions for Partial Supervision: Application to Fetal Brain 3D MRI Parcellation. (arXiv:2107.03846v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.03846</id>
        <link href="http://arxiv.org/abs/2107.03846"/>
        <updated>2021-07-09T01:58:26.233Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have increased the accuracy of automatic segmentation,
however, their accuracy depends on the availability of a large number of fully
segmented images. Methods to train deep neural networks using images for which
some, but not all, regions of interest are segmented are necessary to make
better use of partially annotated datasets. In this paper, we propose the first
axiomatic definition of label-set loss functions that are the loss functions
that can handle partially segmented images. We prove that there is one and only
one method to convert a classical loss function for fully segmented images into
a proper label-set loss function. Our theory also allows us to define the
leaf-Dice loss, a label-set generalization of the Dice loss particularly suited
for partial supervision with only missing labels. Using the leaf-Dice loss, we
set a new state of the art in partially supervised learning for fetal brain 3D
MRI segmentation. We achieve a deep neural network able to segment white
matter, ventricles, cerebellum, extra-ventricular CSF, cortical gray matter,
deep gray matter, brainstem, and corpus callosum based on fetal brain 3D MRI of
anatomically normal fetuses or with open spina bifida. Our implementation of
the proposed label-set loss functions is available at
https://github.com/LucasFidon/label-set-loss-functions]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Fidon_L/0/1/0/all/0/1"&gt;Lucas Fidon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Aertsen_M/0/1/0/all/0/1"&gt;Michael Aertsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Emam_D/0/1/0/all/0/1"&gt;Doaa Emam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mufti_N/0/1/0/all/0/1"&gt;Nada Mufti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Guffens_F/0/1/0/all/0/1"&gt;Frederic Guffens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Deprest_T/0/1/0/all/0/1"&gt;Thomas Deprest&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Demaerel_P/0/1/0/all/0/1"&gt;Philippe Demaerel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+David_A/0/1/0/all/0/1"&gt;Anna L. David&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Melbourne_A/0/1/0/all/0/1"&gt;Andrew Melbourne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1"&gt;Sebastien Ourselin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Deprest_J/0/1/0/all/0/1"&gt;Jam Deprest&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1"&gt;Tom Vercauteren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ACAE-REMIND for Online Continual Learning with Compressed Feature Replay. (arXiv:2105.08595v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08595</id>
        <link href="http://arxiv.org/abs/2105.08595"/>
        <updated>2021-07-09T01:58:26.213Z</updated>
        <summary type="html"><![CDATA[Online continual learning aims to learn from a non-IID stream of data from a
number of different tasks, where the learner is only allowed to consider data
once. Methods are typically allowed to use a limited buffer to store some of
the images in the stream. Recently, it was found that feature replay, where an
intermediate layer representation of the image is stored (or generated) leads
to superior results than image replay, while requiring less memory. Quantized
exemplars can further reduce the memory usage. However, a drawback of these
methods is that they use a fixed (or very intransigent) backbone network. This
significantly limits the learning of representations that can discriminate
between all tasks. To address this problem, we propose an auxiliary classifier
auto-encoder (ACAE) module for feature replay at intermediate layers with high
compression rates. The reduced memory footprint per image allows us to save
more exemplars for replay. In our experiments, we conduct task-agnostic
evaluation under online continual learning setting and get state-of-the-art
performance on ImageNet-Subset, CIFAR100 and CIFAR10 dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herranz_L/0/1/0/all/0/1"&gt;Luis Herranz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1"&gt;Joost van de Weijer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relation-Based Associative Joint Location for Human Pose Estimation in Videos. (arXiv:2107.03591v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03591</id>
        <link href="http://arxiv.org/abs/2107.03591"/>
        <updated>2021-07-09T01:58:26.206Z</updated>
        <summary type="html"><![CDATA[Video-based human pose estimation (HPE) is a vital yet challenging task.
While deep learning methods have made significant progress for the HPE, most
approaches to this task detect each joint independently, damaging the pose
structural information. In this paper, unlike the prior methods, we propose a
Relation-based Pose Semantics Transfer Network (RPSTN) to locate joints
associatively. Specifically, we design a lightweight joint relation extractor
(JRE) to model the pose structural features and associatively generate heatmaps
for joints by modeling the relation between any two joints heuristically
instead of building each joint heatmap independently. Actually, the proposed
JRE module models the spatial configuration of human poses through the
relationship between any two joints. Moreover, considering the temporal
semantic continuity of videos, the pose semantic information in the current
frame is beneficial for guiding the location of joints in the next frame.
Therefore, we use the idea of knowledge reuse to propagate the pose semantic
information between consecutive frames. In this way, the proposed RPSTN
captures temporal dynamics of poses. On the one hand, the JRE module can infer
invisible joints according to the relationship between the invisible joints and
other visible joints in space. On the other hand, in the time, the propose
model can transfer the pose semantic features from the non-occluded frame to
the occluded frame to locate occluded joints. Therefore, our method is robust
to the occlusion and achieves state-of-the-art results on the two challenging
datasets, which demonstrates its effectiveness for video-based human pose
estimation. We will release the code and models publicly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dang_Y/0/1/0/all/0/1"&gt;Yonghao Dang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1"&gt;Jianqin Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tensor Methods in Computer Vision and Deep Learning. (arXiv:2107.03436v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03436</id>
        <link href="http://arxiv.org/abs/2107.03436"/>
        <updated>2021-07-09T01:58:26.199Z</updated>
        <summary type="html"><![CDATA[Tensors, or multidimensional arrays, are data structures that can naturally
represent visual data of multiple dimensions. Inherently able to efficiently
capture structured, latent semantic spaces and high-order interactions, tensors
have a long history of applications in a wide span of computer vision problems.
With the advent of the deep learning paradigm shift in computer vision, tensors
have become even more fundamental. Indeed, essential ingredients in modern deep
learning architectures, such as convolutions and attention mechanisms, can
readily be considered as tensor mappings. In effect, tensor methods are
increasingly finding significant applications in deep learning, including the
design of memory and compute efficient network architectures, improving
robustness to random noise and adversarial attacks, and aiding the theoretical
understanding of deep networks.

This article provides an in-depth and practical review of tensors and tensor
methods in the context of representation learning and deep learning, with a
particular focus on visual data analysis and computer vision applications.
Concretely, besides fundamental work in tensor-based visual data analysis
methods, we focus on recent developments that have brought on a gradual
increase of tensor methods, especially in deep learning architectures, and
their implications in computer vision applications. To further enable the
newcomer to grasp such concepts quickly, we provide companion Python notebooks,
covering key aspects of the paper and implementing them, step-by-step with
TensorLy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Panagakis_Y/0/1/0/all/0/1"&gt;Yannis Panagakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kossaifi_J/0/1/0/all/0/1"&gt;Jean Kossaifi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chrysos_G/0/1/0/all/0/1"&gt;Grigorios G. Chrysos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oldfield_J/0/1/0/all/0/1"&gt;James Oldfield&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nicolaou_M/0/1/0/all/0/1"&gt;Mihalis A. Nicolaou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1"&gt;Stefanos Zafeiriou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Complete Scanning Application Using OpenCv. (arXiv:2107.03700v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03700</id>
        <link href="http://arxiv.org/abs/2107.03700"/>
        <updated>2021-07-09T01:58:26.192Z</updated>
        <summary type="html"><![CDATA[In the following paper, we have combined the various basic functionalities
provided by the NumPy library and OpenCv library, which is an open source for
Computer Vision applications, like conversion of colored images to grayscale,
calculating threshold, finding contours and using those contour points to take
perspective transform of the image inputted by the user, using Python version
3.7. Additional features include cropping, rotating and saving as well. All
these functions and features, when implemented step by step, results in a
complete scanning application. The applied procedure involves the following
steps: Finding contours, applying Perspective transform and brightening the
image, Adaptive Thresholding and applying filters for noise cancellation, and
Rotation features and perspective transform for a special cropping algorithm.
The described technique is implemented on various samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gangal_A/0/1/0/all/0/1"&gt;Ayushe Gangal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1"&gt;Peeyush Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumari_S/0/1/0/all/0/1"&gt;Sunita Kumari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Pyramid Network for Multi-task Affective Analysis. (arXiv:2107.03670v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03670</id>
        <link href="http://arxiv.org/abs/2107.03670"/>
        <updated>2021-07-09T01:58:26.184Z</updated>
        <summary type="html"><![CDATA[Affective Analysis is not a single task, and the valence-arousal value,
expression class and action unit can be predicted at the same time. Previous
researches failed to take them as a whole task or ignore the entanglement and
hierarchical relation of this three facial attributes. We propose a novel model
named feature pyramid networks for multi-task affect analysis. The hierarchical
features are extracted to predict three labels and we apply teacher-student
training strategy to learn from pretrained single-task models. Extensive
experiment results demonstrate the proposed model outperform other models. The
code and model are available for research purposes at
$\href{https://github.com/ryanhe312/ABAW2-FPNMAA}{\text{this link}}$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1"&gt;Ruian He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1"&gt;Zhen Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1"&gt;Bo Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EEG-ConvTransformer for Single-Trial EEG based Visual Stimuli Classification. (arXiv:2107.03983v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03983</id>
        <link href="http://arxiv.org/abs/2107.03983"/>
        <updated>2021-07-09T01:58:26.147Z</updated>
        <summary type="html"><![CDATA[Different categories of visual stimuli activate different responses in the
human brain. These signals can be captured with EEG for utilization in
applications such as Brain-Computer Interface (BCI). However, accurate
classification of single-trial data is challenging due to low signal-to-noise
ratio of EEG. This work introduces an EEG-ConvTranformer network that is based
on multi-headed self-attention. Unlike other transformers, the model
incorporates self-attention to capture inter-region interactions. It further
extends to adjunct convolutional filters with multi-head attention as a single
module to learn temporal patterns. Experimental results demonstrate that
EEG-ConvTransformer achieves improved classification accuracy over the
state-of-the-art techniques across five different visual stimuli classification
tasks. Finally, quantitative analysis of inter-head diversity also shows low
similarity in representational subspaces, emphasizing the implicit diversity of
multi-head attention.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bagchi_S/0/1/0/all/0/1"&gt;Subhranil Bagchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bathula_D/0/1/0/all/0/1"&gt;Deepti R. Bathula&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[$S^3$: Sign-Sparse-Shift Reparametrization for Effective Training of Low-bit Shift Networks. (arXiv:2107.03453v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03453</id>
        <link href="http://arxiv.org/abs/2107.03453"/>
        <updated>2021-07-09T01:58:26.141Z</updated>
        <summary type="html"><![CDATA[Shift neural networks reduce computation complexity by removing expensive
multiplication operations and quantizing continuous weights into low-bit
discrete values, which are fast and energy efficient compared to conventional
neural networks. However, existing shift networks are sensitive to the weight
initialization, and also yield a degraded performance caused by vanishing
gradient and weight sign freezing problem. To address these issues, we propose
S low-bit re-parameterization, a novel technique for training low-bit shift
networks. Our method decomposes a discrete parameter in a sign-sparse-shift
3-fold manner. In this way, it efficiently learns a low-bit network with a
weight dynamics similar to full-precision networks and insensitive to weight
initialization. Our proposed training method pushes the boundaries of shift
neural networks and shows 3-bit shift networks out-performs their
full-precision counterparts in terms of top-1 accuracy on ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xinlin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yaoliang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wulong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chunjing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nia_V/0/1/0/all/0/1"&gt;Vahid Partovi Nia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SCSS-Net: Superpoint Constrained Semi-supervised Segmentation Network for 3D Indoor Scenes. (arXiv:2107.03601v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03601</id>
        <link href="http://arxiv.org/abs/2107.03601"/>
        <updated>2021-07-09T01:58:26.133Z</updated>
        <summary type="html"><![CDATA[Many existing deep neural networks (DNNs) for 3D point cloud semantic
segmentation require a large amount of fully labeled training data. However,
manually assigning point-level labels on the complex scenes is time-consuming.
While unlabeled point clouds can be easily obtained from sensors or
reconstruction, we propose a superpoint constrained semi-supervised
segmentation network for 3D point clouds, named as SCSS-Net. Specifically, we
use the pseudo labels predicted from unlabeled point clouds for self-training,
and the superpoints produced by geometry-based and color-based Region Growing
algorithms are combined to modify and delete pseudo labels with low confidence.
Additionally, we propose an edge prediction module to constrain the features
from edge points of geometry and color. A superpoint feature aggregation module
and superpoint feature consistency loss functions are introduced to smooth the
point features in each superpoint. Extensive experimental results on two 3D
public indoor datasets demonstrate that our method can achieve better
performance than some state-of-the-art point cloud segmentation networks and
some popular semi-supervised segmentation methods with few labeled scenes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1"&gt;Shuang Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1"&gt;Qiulei Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bo Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An audiovisual and contextual approach for categorical and continuous emotion recognition in-the-wild. (arXiv:2107.03465v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03465</id>
        <link href="http://arxiv.org/abs/2107.03465"/>
        <updated>2021-07-09T01:58:26.124Z</updated>
        <summary type="html"><![CDATA[In this work we tackle the task of video-based audio-visual emotion
recognition, within the premises of the 2nd Workshop and Competition on
Affective Behavior Analysis in-the-wild (ABAW). Standard methodologies that
rely solely on the extraction of facial features often fall short of accurate
emotion prediction in cases where the aforementioned source of affective
information is inaccessible due to head/body orientation, low resolution and
poor illumination. We aspire to alleviate this problem by leveraging bodily as
well as contextual features, as part of a broader emotion recognition
framework. A standard CNN-RNN cascade constitutes the backbone of our proposed
model for sequence-to-sequence (seq2seq) learning. Apart from learning through
the \textit{RGB} input modality, we construct an aural stream which operates on
sequences of extracted mel-spectrograms. Our extensive experiments on the
challenging and newly assembled Affect-in-the-wild-2 (Aff-Wild2) dataset verify
the superiority of our methods over existing approaches, while by properly
incorporating all of the aforementioned modules in a network ensemble, we
manage to surpass the previous best published recognition scores, in the
official validation set. All the code was implemented using
PyTorch\footnote{\url{https://pytorch.org/}} and is publicly
available\footnote{\url{https://github.com/PanosAntoniadis/NTUA-ABAW2021}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Antoniadis_P/0/1/0/all/0/1"&gt;Panagiotis Antoniadis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pikoulis_I/0/1/0/all/0/1"&gt;Ioannis Pikoulis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filntisis_P/0/1/0/all/0/1"&gt;Panagiotis P. Filntisis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maragos_P/0/1/0/all/0/1"&gt;Petros Maragos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking of Pedestrian Attribute Recognition: A Reliable Evaluation under Zero-Shot Pedestrian Identity Setting. (arXiv:2107.03576v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03576</id>
        <link href="http://arxiv.org/abs/2107.03576"/>
        <updated>2021-07-09T01:58:26.056Z</updated>
        <summary type="html"><![CDATA[Pedestrian attribute recognition aims to assign multiple attributes to one
pedestrian image captured by a video surveillance camera. Although numerous
methods are proposed and make tremendous progress, we argue that it is time to
step back and analyze the status quo of the area. We review and rethink the
recent progress from three perspectives. First, given that there is no explicit
and complete definition of pedestrian attribute recognition, we formally define
and distinguish pedestrian attribute recognition from other similar tasks.
Second, based on the proposed definition, we expose the limitations of the
existing datasets, which violate the academic norm and are inconsistent with
the essential requirement of practical industry application. Thus, we propose
two datasets, PETA\textsubscript{$ZS$} and RAP\textsubscript{$ZS$}, constructed
following the zero-shot settings on pedestrian identity. In addition, we also
introduce several realistic criteria for future pedestrian attribute dataset
construction. Finally, we reimplement existing state-of-the-art methods and
introduce a strong baseline method to give reliable evaluations and fair
comparisons. Experiments are conducted on four existing datasets and two
proposed datasets to measure progress on pedestrian attribute recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1"&gt;Jian Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Houjing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaotang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kaiqi Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A hybrid deep learning framework for Covid-19 detection via 3D Chest CT Images. (arXiv:2107.03904v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.03904</id>
        <link href="http://arxiv.org/abs/2107.03904"/>
        <updated>2021-07-09T01:58:26.034Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a hybrid deep learning framework named CTNet which
combines convolutional neural network and transformer together for the
detection of COVID-19 via 3D chest CT images. It consists of a CNN feature
extractor module with SE attention to extract sufficient features from CT
scans, together with a transformer model to model the discriminative features
of the 3D CT scans. Compared to previous works, CTNet provides an effective and
efficient method to perform COVID-19 diagnosis via 3D CT scans with data
resampling strategy. Advanced results on a large and public benchmarks,
COV19-CT-DB database was achieved by the proposed CTNet, over the
state-of-the-art baseline approachproposed together with the dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liang_S/0/1/0/all/0/1"&gt;Shuang Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal affect prediction model using a facial image sequence. (arXiv:2107.03886v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03886</id>
        <link href="http://arxiv.org/abs/2107.03886"/>
        <updated>2021-07-09T01:58:26.022Z</updated>
        <summary type="html"><![CDATA[Among human affective behavior research, facial expression recognition
research is improving in performance along with the development of deep
learning. However, for improved performance, not only past images but also
future images should be used along with corresponding facial images, but there
are obstacles to the application of this technique to real-time environments.
In this paper, we propose the causal affect prediction network (CAPNet), which
uses only past facial images to predict corresponding affective valence and
arousal. We train CAPNet to learn causal inference between past images and
corresponding affective valence and arousal through supervised learning by
pairing the sequence of past images with the current label using the Aff-Wild2
dataset. We show through experiments that the well-trained CAPNet outperforms
the baseline of the second challenge of the Affective Behavior Analysis
in-the-wild (ABAW2) Competition by predicting affective valence and arousal
only with past facial images one-third of a second earlier. Therefore, in
real-time application, CAPNet can reliably predict affective valence and
arousal only with past data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oh_G/0/1/0/all/0/1"&gt;Geesung Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_E/0/1/0/all/0/1"&gt;Euiseok Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1"&gt;Sejoon Lim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GPNAS: A Neural Network Architecture Search Framework Based on Graphical Predictor. (arXiv:2103.11820v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11820</id>
        <link href="http://arxiv.org/abs/2103.11820"/>
        <updated>2021-07-09T01:58:25.953Z</updated>
        <summary type="html"><![CDATA[In practice, the problems encountered in Neural Architecture Search (NAS)
training are not simple problems, but often a series of difficult combinations
(wrong compensation estimation, curse of dimension, overfitting, high
complexity, etc.). In this paper, we propose a framework to decouple network
structure from operator search space, and use two BOHBs to search
alternatively. Considering that activation function and initialization are also
important parts of neural network, the generalization ability of the model will
be affected. We introduce an activation function and an initialization method
domain, and add them into the operator search space to form a generalized
search space, so as to improve the generalization ability of the child model.
We then trained a GCN-based predictor using feedback from the child model. This
can not only improve the search efficiency, but also solve the problem of
dimension curse. Next, unlike other NAS studies, we used predictors to analyze
the stability of different network structures. Finally, we applied our
framework to neural structure search and achieved significant improvements on
multiple datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ai_D/0/1/0/all/0/1"&gt;Dige Ai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Modality Task Cascade for 3D Object Detection. (arXiv:2107.04013v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04013</id>
        <link href="http://arxiv.org/abs/2107.04013"/>
        <updated>2021-07-09T01:58:25.882Z</updated>
        <summary type="html"><![CDATA[Point clouds and RGB images are naturally complementary modalities for 3D
visual understanding - the former provides sparse but accurate locations of
points on objects, while the latter contains dense color and texture
information. Despite this potential for close sensor fusion, many methods train
two models in isolation and use simple feature concatenation to represent 3D
sensor data. This separated training scheme results in potentially sub-optimal
performance and prevents 3D tasks from being used to benefit 2D tasks that are
often useful on their own. To provide a more integrated approach, we propose a
novel Multi-Modality Task Cascade network (MTC-RCNN) that leverages 3D box
proposals to improve 2D segmentation predictions, which are then used to
further refine the 3D boxes. We show that including a 2D network between two
stages of 3D modules significantly improves both 2D and 3D task performance.
Moreover, to prevent the 3D module from over-relying on the overfitted 2D
predictions, we propose a dual-head 2D segmentation training and inference
scheme, allowing the 2nd 3D module to learn to interpret imperfect 2D
segmentation predictions. Evaluating our model on the challenging SUN RGB-D
dataset, we improve upon state-of-the-art results of both single modality and
fusion networks by a large margin ($\textbf{+3.8}$ mAP@0.5). Code will be
released $\href{https://github.com/Divadi/MTC_RCNN}{\text{here.}}$]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jinhyung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weng_X/0/1/0/all/0/1"&gt;Xinshuo Weng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Man_Y/0/1/0/all/0/1"&gt;Yunze Man&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1"&gt;Kris Kitani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Semantic Hallucination for Domain Generalized Semantic Segmentation. (arXiv:2106.04144v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04144</id>
        <link href="http://arxiv.org/abs/2106.04144"/>
        <updated>2021-07-09T01:58:25.857Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks may perform poorly when the test and train data
are from different domains. While this problem can be mitigated by using the
target domain data to align the source and target domain feature
representations, the target domain data may be unavailable due to privacy
concerns. Consequently, there is a need for methods that generalize well
without access to target domain data during training. In this work, we propose
an adversarial hallucination approach, which combines a class-wise
hallucination module and a semantic segmentation module. Since the segmentation
performance varies across different classes, we design a semantic-conditioned
style hallucination layer to adaptively stylize each class. The classwise
stylization parameters are generated from the semantic knowledge in the
segmentation probability maps of the source domain image. Both modules compete
adversarially, with the hallucination module generating increasingly
'difficult' style images to challenge the segmentation module. In response, the
segmentation module improves its performance as it is trained with generated
samples at an appropriate class-wise difficulty level. Experiments on state of
the art domain adaptation work demonstrate the efficacy of our proposed method
when no target domain data are available for training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tjio_G/0/1/0/all/0/1"&gt;Gabriel Tjio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Ping Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Joey Tianyi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goh_R/0/1/0/all/0/1"&gt;Rick Siow Mong Goh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CHASE: Robust Visual Tracking via Cell-Level Differentiable Neural Architecture Search. (arXiv:2107.03463v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03463</id>
        <link href="http://arxiv.org/abs/2107.03463"/>
        <updated>2021-07-09T01:58:25.837Z</updated>
        <summary type="html"><![CDATA[A strong visual object tracker nowadays relies on its well-crafted modules,
which typically consist of manually-designed network architectures to deliver
high-quality tracking results. Not surprisingly, the manual design process
becomes a particularly challenging barrier, as it demands sufficient prior
experience, enormous effort, intuition and perhaps some good luck. Meanwhile,
neural architecture search has gaining grounds in practical applications such
as image segmentation, as a promising method in tackling the issue of automated
search of feasible network structures. In this work, we propose a novel
cell-level differentiable architecture search mechanism to automate the network
design of the tracking module, aiming to adapt backbone features to the
objective of a tracking network during offline training. The proposed approach
is simple, efficient, and with no need to stack a series of modules to
construct a network. Our approach is easy to be incorporated into existing
trackers, which is empirically validated using different differentiable
architecture search-based methods and tracking objectives. Extensive
experimental evaluations demonstrate the superior performance of our approach
over five commonly-used benchmarks. Meanwhile, our automated searching process
takes 41 (18) hours for the second (first) order DARTS method on the
TrackingNet dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marvasti_Zadeh_S/0/1/0/all/0/1"&gt;Seyed Mojtaba Marvasti-Zadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khaghani_J/0/1/0/all/0/1"&gt;Javad Khaghani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1"&gt;Li Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghanei_Yakhdan_H/0/1/0/all/0/1"&gt;Hossein Ghanei-Yakhdan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasaei_S/0/1/0/all/0/1"&gt;Shohreh Kasaei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Motion Correction and Super Resolution for Cardiac Segmentation via Latent Optimisation. (arXiv:2107.03887v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.03887</id>
        <link href="http://arxiv.org/abs/2107.03887"/>
        <updated>2021-07-09T01:58:25.761Z</updated>
        <summary type="html"><![CDATA[In cardiac magnetic resonance (CMR) imaging, a 3D high-resolution
segmentation of the heart is essential for detailed description of its
anatomical structures. However, due to the limit of acquisition duration and
respiratory/cardiac motion, stacks of multi-slice 2D images are acquired in
clinical routine. The segmentation of these images provides a low-resolution
representation of cardiac anatomy, which may contain artefacts caused by
motion. Here we propose a novel latent optimisation framework that jointly
performs motion correction and super resolution for cardiac image
segmentations. Given a low-resolution segmentation as input, the framework
accounts for inter-slice motion in cardiac MR imaging and super-resolves the
input into a high-resolution segmentation consistent with input. A multi-view
loss is incorporated to leverage information from both short-axis view and
long-axis view of cardiac imaging. To solve the inverse problem, iterative
optimisation is performed in a latent space, which ensures the anatomical
plausibility. This alleviates the need of paired low-resolution and
high-resolution images for supervised learning. Experiments on two cardiac MR
datasets show that the proposed framework achieves high performance, comparable
to state-of-the-art super-resolution approaches and with better cross-domain
generalisability and anatomical plausibility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qin_C/0/1/0/all/0/1"&gt;Chen Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Savioli_N/0/1/0/all/0/1"&gt;Nicolo Savioli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+ORegan_D/0/1/0/all/0/1"&gt;Declan O&amp;#x27;Regan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cook_S/0/1/0/all/0/1"&gt;Stuart Cook&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yike Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1"&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bai_W/0/1/0/all/0/1"&gt;Wenjia Bai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Long-Short Temporal Contrastive Learning of Video Transformers. (arXiv:2106.09212v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09212</id>
        <link href="http://arxiv.org/abs/2106.09212"/>
        <updated>2021-07-09T01:58:25.751Z</updated>
        <summary type="html"><![CDATA[Video transformers have recently emerged as a competitive alternative to 3D
CNNs for video understanding. However, due to their large number of parameters
and reduced inductive biases, these models require supervised pretraining on
large-scale image datasets to achieve top performance. In this paper, we
empirically demonstrate that self-supervised pretraining of video transformers
on video-only datasets can lead to action recognition results that are on par
or better than those obtained with supervised pretraining on large-scale image
datasets, even massive ones such as ImageNet-21K. Since transformer-based
models are effective at capturing dependencies over extended temporal spans, we
propose a simple learning procedure that forces the model to match a long-term
view to a short-term view of the same video. Our approach, named Long-Short
Temporal Contrastive Learning (LSTCL), enables video transformers to learn an
effective clip-level representation by predicting temporal context captured
from a longer temporal extent. To demonstrate the generality of our findings,
we implement and validate our approach under three different self-supervised
contrastive learning frameworks (MoCo v3, BYOL, SimSiam) using two distinct
video-transformer architectures, including an improved variant of the Swin
Transformer augmented with space-time attention. We conduct a thorough ablation
study and show that LSTCL achieves competitive performance on multiple video
benchmarks and represents a convincing alternative to supervised image-based
pretraining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1"&gt;Gedas Bertasius&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1"&gt;Du Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torresani_L/0/1/0/all/0/1"&gt;Lorenzo Torresani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collaboration of Experts: Achieving 80% Top-1 Accuracy on ImageNet with 100M FLOPs. (arXiv:2107.03815v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03815</id>
        <link href="http://arxiv.org/abs/2107.03815"/>
        <updated>2021-07-09T01:58:25.744Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a Collaboration of Experts (CoE) framework to pool
together the expertise of multiple networks towards a common aim. Each expert
is an individual network with expertise on a unique portion of the dataset,
which enhances the collective capacity. Given a sample, an expert is selected
by the delegator, which simultaneously outputs a rough prediction to support
early termination. To fulfill this framework, we propose three modules to impel
each model to play its role, namely weight generation module (WGM), label
generation module (LGM) and variance calculation module (VCM). Our method
achieves the state-of-the-art performance on ImageNet, 80.7% top-1 accuracy
with 194M FLOPs. Combined with PWLU activation function and CondConv, CoE
further achieves the accuracy of 80.0% with only 100M FLOPs for the first time.
More importantly, our method is hardware friendly and achieves a 3-6x speedup
compared with some existing conditional computation approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yikang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1"&gt;Zhao Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Conditioned Imitation Learning over Unstructured Data. (arXiv:2005.07648v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.07648</id>
        <link href="http://arxiv.org/abs/2005.07648"/>
        <updated>2021-07-09T01:58:25.724Z</updated>
        <summary type="html"><![CDATA[Natural language is perhaps the most flexible and intuitive way for humans to
communicate tasks to a robot. Prior work in imitation learning typically
requires each task be specified with a task id or goal image -- something that
is often impractical in open-world environments. On the other hand, previous
approaches in instruction following allow agent behavior to be guided by
language, but typically assume structure in the observations, actuators, or
language that limit their applicability to complex settings like robotics. In
this work, we present a method for incorporating free-form natural language
conditioning into imitation learning. Our approach learns perception from
pixels, natural language understanding, and multitask continuous control
end-to-end as a single neural network. Unlike prior work in imitation learning,
our method is able to incorporate unlabeled and unstructured demonstration data
(i.e. no task or language labels). We show this dramatically improves language
conditioned performance, while reducing the cost of language annotation to less
than 1% of total data. At test time, a single language conditioned visuomotor
policy trained with our method can perform a wide variety of robotic
manipulation skills in a 3D environment, specified only with natural language
descriptions of each task (e.g. "open the drawer...now pick up the block...now
press the green button..."). To scale up the number of instructions an agent
can follow, we propose combining text conditioned policies with large
pretrained neural language models. We find this allows a policy to be robust to
many out-of-distribution synonym instructions, without requiring new
demonstrations. See videos of a human typing live text commands to our agent at
language-play.github.io]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lynch_C/0/1/0/all/0/1"&gt;Corey Lynch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sermanet_P/0/1/0/all/0/1"&gt;Pierre Sermanet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prior Aided Streaming Network for Multi-task Affective Recognitionat the 2nd ABAW2 Competition. (arXiv:2107.03708v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03708</id>
        <link href="http://arxiv.org/abs/2107.03708"/>
        <updated>2021-07-09T01:58:25.717Z</updated>
        <summary type="html"><![CDATA[Automatic affective recognition has been an important research topic in human
computer interaction (HCI) area. With recent development of deep learning
techniques and large scale in-the-wild annotated datasets, the facial emotion
analysis is now aimed at challenges in the real world settings. In this paper,
we introduce our submission to the 2nd Affective Behavior Analysis in-the-wild
(ABAW2) Competition. In dealing with different emotion representations,
including Categorical Emotions (CE), Action Units (AU), and Valence Arousal
(VA), we propose a multi-task streaming network by a heuristic that the three
representations are intrinsically associated with each other. Besides, we
leverage an advanced facial expression embedding as prior knowledge, which is
capable of capturing identity-invariant expression features while preserving
the expression similarities, to aid the down-streaming recognition tasks. The
extensive quantitative evaluations as well as ablation studies on the Aff-Wild2
dataset prove the effectiveness of our proposed prior aided streaming network
approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1"&gt;Zunhu Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Keyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lincheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhimeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yu Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Object Behavioral Feature Extraction for Potential Risk Analysis based on Video Sensor. (arXiv:2107.03554v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03554</id>
        <link href="http://arxiv.org/abs/2107.03554"/>
        <updated>2021-07-09T01:58:25.710Z</updated>
        <summary type="html"><![CDATA[Pedestrians are exposed to risk of death or serious injuries on roads,
especially unsignalized crosswalks, for a variety of reasons. To date, an
extensive variety of studies have reported on vision based traffic safety
system. However, many studies required manual inspection of the volumes of
traffic video to reliably obtain traffic related objects behavioral factors. In
this paper, we propose an automated and simpler system for effectively
extracting object behavioral features from video sensors deployed on the road.
We conduct basic statistical analysis on these features, and show how they can
be useful for monitoring the traffic behavior on the road. We confirm the
feasibility of the proposed system by applying our prototype to two
unsignalized crosswalks in Osan city, South Korea. To conclude, we compare
behaviors of vehicles and pedestrians in those two areas by simple statistical
analysis. This study demonstrates the potential for a network of connected
video sensors to provide actionable data for smart cities to improve pedestrian
safety in dangerous road environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Noh_B/0/1/0/all/0/1"&gt;Byeongjoon Noh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noh_W/0/1/0/all/0/1"&gt;Wonjun Noh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;David Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeo_H/0/1/0/all/0/1"&gt;Hwasoo Yeo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Malware Classification Using Deep Boosted Learning. (arXiv:2107.04008v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.04008</id>
        <link href="http://arxiv.org/abs/2107.04008"/>
        <updated>2021-07-09T01:58:25.703Z</updated>
        <summary type="html"><![CDATA[Malicious activities in cyberspace have gone further than simply hacking
machines and spreading viruses. It has become a challenge for a nations
survival and hence has evolved to cyber warfare. Malware is a key component of
cyber-crime, and its analysis is the first line of defence against attack. This
work proposes a novel deep boosted hybrid learning-based malware classification
framework and named as Deep boosted Feature Space-based Malware classification
(DFS-MC). In the proposed framework, the discrimination power is enhanced by
fusing the feature spaces of the best performing customized CNN architectures
models and its discrimination by an SVM for classification. The discrimination
capacity of the proposed classification framework is assessed by comparing it
against the standard customized CNNs. The customized CNN models are implemented
in two ways: softmax classifier and deep hybrid learning-based malware
classification. In the hybrid learning, Deep features are extracted from
customized CNN architectures and fed into the conventional machine learning
classifier to improve the classification performance. We also introduced the
concept of transfer learning in a customized CNN architecture based malware
classification framework through fine-tuning. The performance of the proposed
malware classification approaches are validated on the MalImg malware dataset
using the hold-out cross-validation technique. Experimental comparisons were
conducted by employing innovative, customized CNN, trained from scratch and
fine-tuning the customized CNN using transfer learning. The proposed
classification framework DFS-MC showed improved results, Accuracy: 98.61%,
F-score: 0.96, Precision: 0.96, and Recall: 0.96.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Asam_M/0/1/0/all/0/1"&gt;Muhammad Asam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1"&gt;Saddam Hussain Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jamal_T/0/1/0/all/0/1"&gt;Tauseef Jamal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zahoora_U/0/1/0/all/0/1"&gt;Umme Zahoora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1"&gt;Asifullah Khan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty-aware Human Motion Prediction. (arXiv:2107.03575v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03575</id>
        <link href="http://arxiv.org/abs/2107.03575"/>
        <updated>2021-07-09T01:58:25.692Z</updated>
        <summary type="html"><![CDATA[Human motion prediction is essential for tasks such as human motion analysis
and human-robot interactions. Most existing approaches have been proposed to
realize motion prediction. However, they ignore an important task, the
evaluation of the quality of the predicted result. It is far more enough for
current approaches in actual scenarios because people can't know how to
interact with the machine without the evaluation of prediction, and unreliable
predictions may mislead the machine to harm the human. Hence, we propose an
uncertainty-aware framework for human motion prediction (UA-HMP). Concretely,
we first design an uncertainty-aware predictor through Gaussian modeling to
achieve the value and the uncertainty of predicted motion. Then, an
uncertainty-guided learning scheme is proposed to quantitate the uncertainty
and reduce the negative effect of the noisy samples during optimization for
better performance. Our proposed framework is easily combined with current SOTA
baselines to overcome their weakness in uncertainty modeling with slight
parameters increment. Extensive experiments also show that they can achieve
better performance in both short and long-term predictions in H3.6M, CMU-Mocap.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_P/0/1/0/all/0/1"&gt;Pengxiang Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1"&gt;Jianqin Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Crowd Counting via Perspective-Guided Fractional-Dilation Convolution. (arXiv:2107.03665v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03665</id>
        <link href="http://arxiv.org/abs/2107.03665"/>
        <updated>2021-07-09T01:58:25.685Z</updated>
        <summary type="html"><![CDATA[Crowd counting is critical for numerous video surveillance scenarios. One of
the main issues in this task is how to handle the dramatic scale variations of
pedestrians caused by the perspective effect. To address this issue, this paper
proposes a novel convolution neural network-based crowd counting method, termed
Perspective-guided Fractional-Dilation Network (PFDNet). By modeling the
continuous scale variations, the proposed PFDNet is able to select the proper
fractional dilation kernels for adapting to different spatial locations. It
significantly improves the flexibility of the state-of-the-arts that only
consider the discrete representative scales. In addition, by avoiding the
multi-scale or multi-column architecture that used in other methods, it is
computationally more efficient. In practice, the proposed PFDNet is constructed
by stacking multiple Perspective-guided Fractional-Dilation Convolutions (PFC)
on a VGG16-BN backbone. By introducing a novel generalized dilation convolution
operation, the PFC can handle fractional dilation ratios in the spatial domain
under the guidance of perspective annotations, achieving continuous scales
modeling of pedestrians. To deal with the problem of unavailable perspective
information in some cases, we further introduce an effective perspective
estimation branch to the proposed PFDNet, which can be trained in either
supervised or weakly-supervised setting once the branch has been pre-trained.
Extensive experiments show that the proposed PFDNet outperforms
state-of-the-art methods on ShanghaiTech A, ShanghaiTech B, WorldExpo'10,
UCF-QNRF, UCF_CC_50 and TRANCOS dataset, achieving MAE 53.8, 6.5, 6.8, 84.3,
205.8, and 3.06 respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zhaoyi Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruimao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongzhi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qingfu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1"&gt;Wangmeng Zuo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Spoken Language Understanding using RNN-Transducer ASR. (arXiv:2106.15919v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15919</id>
        <link href="http://arxiv.org/abs/2106.15919"/>
        <updated>2021-07-09T01:58:25.658Z</updated>
        <summary type="html"><![CDATA[We propose an end-to-end trained spoken language understanding (SLU) system
that extracts transcripts, intents and slots from an input speech utterance. It
consists of a streaming recurrent neural network transducer (RNNT) based
automatic speech recognition (ASR) model connected to a neural natural language
understanding (NLU) model through a neural interface. This interface allows for
end-to-end training using multi-task RNNT and NLU losses. Additionally, we
introduce semantic sequence loss training for the joint RNNT-NLU system that
allows direct optimization of non-differentiable SLU metrics. This end-to-end
SLU model paradigm can leverage state-of-the-art advancements and pretrained
models in both ASR and NLU research communities, outperforming recently
proposed direct speech-to-semantics models, and conventional pipelined ASR and
NLU systems. We show that this method improves both ASR and NLU metrics on both
public SLU datasets and large proprietary datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raju_A/0/1/0/all/0/1"&gt;Anirudh Raju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tiwari_G/0/1/0/all/0/1"&gt;Gautam Tiwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_M/0/1/0/all/0/1"&gt;Milind Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dheram_P/0/1/0/all/0/1"&gt;Pranav Dheram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_B/0/1/0/all/0/1"&gt;Bryan Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhe Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_B/0/1/0/all/0/1"&gt;Bach Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rastrow_A/0/1/0/all/0/1"&gt;Ariya Rastrow&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elastic deformation of optical coherence tomography images of diabetic macular edema for deep-learning models training: how far to go?. (arXiv:2107.03651v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.03651</id>
        <link href="http://arxiv.org/abs/2107.03651"/>
        <updated>2021-07-09T01:58:25.650Z</updated>
        <summary type="html"><![CDATA[To explore the clinical validity of elastic deformation of optical coherence
tomography (OCT) images for data augmentation in the development of
deep-learning model for detection of diabetic macular edema (DME).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bar_David_D/0/1/0/all/0/1"&gt;Daniel Bar-David&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bar_David_L/0/1/0/all/0/1"&gt;Laura Bar-David&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shapira_Y/0/1/0/all/0/1"&gt;Yinon Shapira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Leibu_R/0/1/0/all/0/1"&gt;Rina Leibu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dori_D/0/1/0/all/0/1"&gt;Dalia Dori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schneor_R/0/1/0/all/0/1"&gt;Ronit Schneor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fischer_A/0/1/0/all/0/1"&gt;Anath Fischer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Soudry_S/0/1/0/all/0/1"&gt;Shiri Soudry&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Use of Affective Visual Information for Summarization of Human-Centric Videos. (arXiv:2107.03783v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03783</id>
        <link href="http://arxiv.org/abs/2107.03783"/>
        <updated>2021-07-09T01:58:25.642Z</updated>
        <summary type="html"><![CDATA[Increasing volume of user-generated human-centric video content and their
applications, such as video retrieval and browsing, require compact
representations that are addressed by the video summarization literature.
Current supervised studies formulate video summarization as a
sequence-to-sequence learning problem and the existing solutions often neglect
the surge of human-centric view, which inherently contains affective content.
In this study, we investigate the affective-information enriched supervised
video summarization task for human-centric videos. First, we train a visual
input-driven state-of-the-art continuous emotion recognition model (CER-NET) on
the RECOLA dataset to estimate emotional attributes. Then, we integrate the
estimated emotional attributes and the high-level representations from the
CER-NET with the visual information to define the proposed affective video
summarization architectures (AVSUM). In addition, we investigate the use of
attention to improve the AVSUM architectures and propose two new architectures
based on temporal attention (TA-AVSUM) and spatial attention (SA-AVSUM). We
conduct video summarization experiments on the TvSum database. The proposed
AVSUM-GRU architecture with an early fusion of high level GRU embeddings and
the temporal attention based TA-AVSUM architecture attain competitive video
summarization performances by bringing strong performance improvements for the
human-centric videos compared to the state-of-the-art in terms of F-score and
self-defined face recall metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kopru_B/0/1/0/all/0/1"&gt;Berkay K&amp;#xf6;pr&amp;#xfc;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erzin_E/0/1/0/all/0/1"&gt;Engin Erzin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Defending Against Multiple and Unforeseen Adversarial Videos. (arXiv:2009.05244v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.05244</id>
        <link href="http://arxiv.org/abs/2009.05244"/>
        <updated>2021-07-09T01:58:25.621Z</updated>
        <summary type="html"><![CDATA[Adversarial robustness of deep neural networks has been actively
investigated. However, most existing defense approaches are limited to a
specific type of adversarial perturbations. Specifically, they often fail to
offer resistance to multiple attack types simultaneously, i.e., they lack
multi-perturbation robustness. Furthermore, compared to image recognition
problems, the adversarial robustness of video recognition models is relatively
unexplored. While several studies have proposed how to generate adversarial
videos, only a handful of approaches about the defense strategies have been
published in the literature. In this paper, we propose one of the first defense
strategies against multiple types of adversarial videos for video recognition.
The proposed method, referred to as MultiBN, performs adversarial training on
multiple adversarial video types using multiple independent batch normalization
(BN) layers with a learning-based BN selection module. With a multiple BN
structure, each BN brach is responsible for learning the distribution of a
single perturbation type and thus provides more precise distribution
estimations. This mechanism benefits dealing with multiple perturbation types.
The BN selection module detects the attack type of an input video and sends it
to the corresponding BN branch, making MultiBN fully automatic and allow
end-to-end training. Compared to present adversarial training approaches, the
proposed MultiBN exhibits stronger multi-perturbation robustness against
different and even unforeseen adversarial video types, ranging from Lp-bounded
attacks and physically realizable attacks. This holds true on different
datasets and target models. Moreover, we conduct an extensive analysis to study
the properties of the multiple BN structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1"&gt;Shao-Yuan Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1"&gt;Vishal M. Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inspiration through Observation: Demonstrating the Influence of Automatically Generated Text on Creative Writing. (arXiv:2107.04007v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04007</id>
        <link href="http://arxiv.org/abs/2107.04007"/>
        <updated>2021-07-09T01:58:25.614Z</updated>
        <summary type="html"><![CDATA[Getting machines to generate text perceived as creative is a long-pursued
goal. A growing body of research directs this goal towards augmenting the
creative writing abilities of human authors. In this paper, we pursue this
objective by analyzing how observing examples of automatically generated text
influences writing. In particular, we examine a task referred to as sentence
infilling, which involves transforming a list of words into a complete
sentence. We emphasize "storiability" as a desirable feature of the resulting
sentences, where "storiable" sentences are those that suggest a story a reader
would be curious to hear about. Both humans and an automated system (based on a
neural language model) performed this sentence infilling task. In one setting,
people wrote sentences on their own; in a different setting, people observed
the sentences produced by the model while writing their own sentences. Readers
then assigned storiability preferences to the resulting sentences in a
subsequent evaluation. We find that human-authored sentences were judged as
more storiable when authors observed the generated examples, and that
storiability increased as authors derived more semantic content from the
examples. This result gives evidence of an "inspiration through observation"
paradigm for human-computer collaborative writing, through which human writing
can be enhanced by text generation models without directly copying their
output.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roemmele_M/0/1/0/all/0/1"&gt;Melissa Roemmele&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Instance-Level Relative Saliency Ranking with Graph Reasoning. (arXiv:2107.03824v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03824</id>
        <link href="http://arxiv.org/abs/2107.03824"/>
        <updated>2021-07-09T01:58:25.607Z</updated>
        <summary type="html"><![CDATA[Conventional salient object detection models cannot differentiate the
importance of different salient objects. Recently, two works have been proposed
to detect saliency ranking by assigning different degrees of saliency to
different objects. However, one of these models cannot differentiate object
instances and the other focuses more on sequential attention shift order
inference. In this paper, we investigate a practical problem setting that
requires simultaneously segment salient instances and infer their relative
saliency rank order. We present a novel unified model as the first end-to-end
solution, where an improved Mask R-CNN is first used to segment salient
instances and a saliency ranking branch is then added to infer the relative
saliency. For relative saliency ranking, we build a new graph reasoning module
by combining four graphs to incorporate the instance interaction relation,
local contrast, global contrast, and a high-level semantic prior, respectively.
A novel loss function is also proposed to effectively train the saliency
ranking branch. Besides, a new dataset and an evaluation metric are proposed
for this task, aiming at pushing forward this field of research. Finally,
experimental results demonstrate that our proposed model is more effective than
previous methods. We also show an example of its practical usage on adaptive
image retargeting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1"&gt;Nian Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Long Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wangbo Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Junwei Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Lingual Transfer in Zero-Shot Cross-Language Entity Linking. (arXiv:2010.09828v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09828</id>
        <link href="http://arxiv.org/abs/2010.09828"/>
        <updated>2021-07-09T01:58:25.599Z</updated>
        <summary type="html"><![CDATA[Cross-language entity linking grounds mentions in multiple languages to a
single-language knowledge base. We propose a neural ranking architecture for
this task that uses multilingual BERT representations of the mention and the
context in a neural network. We find that the multilingual ability of BERT
leads to robust performance in monolingual and multilingual settings.
Furthermore, we explore zero-shot language transfer and find surprisingly
robust performance. We investigate the zero-shot degradation and find that it
can be partially mitigated by a proposed auxiliary training objective, but that
the remaining error can best be attributed to domain shift rather than language
transfer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schumacher_E/0/1/0/all/0/1"&gt;Elliot Schumacher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mayfield_J/0/1/0/all/0/1"&gt;James Mayfield&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dredze_M/0/1/0/all/0/1"&gt;Mark Dredze&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Staying in Shape: Learning Invariant Shape Representations using Contrastive Learning. (arXiv:2107.03552v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03552</id>
        <link href="http://arxiv.org/abs/2107.03552"/>
        <updated>2021-07-09T01:58:25.592Z</updated>
        <summary type="html"><![CDATA[Creating representations of shapes that are invari-ant to isometric or
almost-isometric transforma-tions has long been an area of interest in shape
anal-ysis, since enforcing invariance allows the learningof more effective and
robust shape representations.Most existing invariant shape representations
arehandcrafted, and previous work on learning shaperepresentations do not focus
on producing invariantrepresentations. To solve the problem of
learningunsupervised invariant shape representations, weuse contrastive
learning, which produces discrimi-native representations through learning
invarianceto user-specified data augmentations. To producerepresentations that
are specifically isometry andalmost-isometry invariant, we propose new
dataaugmentations that randomly sample these transfor-mations. We show
experimentally that our methodoutperforms previous unsupervised learning
ap-proaches in both effectiveness and robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jeffrey Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1"&gt;Serena Yeung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classification of Urban Morphology with Deep Learning: Application on Urban Vitality. (arXiv:2105.09908v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09908</id>
        <link href="http://arxiv.org/abs/2105.09908"/>
        <updated>2021-07-09T01:58:25.584Z</updated>
        <summary type="html"><![CDATA[There is a prevailing trend to study urban morphology quantitatively thanks
to the growing accessibility to various forms of spatial big data, increasing
computing power, and use cases benefiting from such information. The methods
developed up to now measure urban morphology with numerical indices describing
density, proportion, and mixture, but they do not directly represent
morphological features from the human's visual and intuitive perspective. We
take the first step to bridge the gap by proposing a deep learning-based
technique to automatically classify road networks into four classes on a visual
basis. The method is implemented by generating an image of the street network
(Colored Road Hierarchy Diagram), which we introduce in this paper, and
classifying it using a deep convolutional neural network (ResNet-34). The model
achieves an overall classification accuracy of 0.875. Nine cities around the
world are selected as the study areas with their road networks acquired from
OpenStreetMap. Latent subgroups among the cities are uncovered through
clustering on the percentage of each road network category. In the subsequent
part of the paper, we focus on the usability of such classification: we apply
our method in a case study of urban vitality prediction. An advanced tree-based
regression model (LightGBM) is for the first time designated to establish the
relationship between morphological indices and vitality indicators. The effect
of road network classification is found to be small but positively associated
with urban vitality. This work expands the toolkit of quantitative urban
morphology study with new techniques, supporting further studies in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wangyang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1"&gt;Abraham Noah Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biljecki_F/0/1/0/all/0/1"&gt;Filip Biljecki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RMA: Rapid Motor Adaptation for Legged Robots. (arXiv:2107.04034v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04034</id>
        <link href="http://arxiv.org/abs/2107.04034"/>
        <updated>2021-07-09T01:58:25.577Z</updated>
        <summary type="html"><![CDATA[Successful real-world deployment of legged robots would require them to adapt
in real-time to unseen scenarios like changing terrains, changing payloads,
wear and tear. This paper presents Rapid Motor Adaptation (RMA) algorithm to
solve this problem of real-time online adaptation in quadruped robots. RMA
consists of two components: a base policy and an adaptation module. The
combination of these components enables the robot to adapt to novel situations
in fractions of a second. RMA is trained completely in simulation without using
any domain knowledge like reference trajectories or predefined foot trajectory
generators and is deployed on the A1 robot without any fine-tuning. We train
RMA on a varied terrain generator using bioenergetics-inspired rewards and
deploy it on a variety of difficult terrains including rocky, slippery,
deformable surfaces in environments with grass, long vegetation, concrete,
pebbles, stairs, sand, etc. RMA shows state-of-the-art performance across
diverse real-world as well as simulation experiments. Video results at
https://ashish-kmr.github.io/rma-legged-robots/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Ashish Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1"&gt;Zipeng Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1"&gt;Deepak Pathak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1"&gt;Jitendra Malik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weight Reparametrization for Budget-Aware Network Pruning. (arXiv:2107.03909v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03909</id>
        <link href="http://arxiv.org/abs/2107.03909"/>
        <updated>2021-07-09T01:58:25.561Z</updated>
        <summary type="html"><![CDATA[Pruning seeks to design lightweight architectures by removing redundant
weights in overparameterized networks. Most of the existing techniques first
remove structured sub-networks (filters, channels,...) and then fine-tune the
resulting networks to maintain a high accuracy. However, removing a whole
structure is a strong topological prior and recovering the accuracy, with
fine-tuning, is highly cumbersome. In this paper, we introduce an "end-to-end"
lightweight network design that achieves training and pruning simultaneously
without fine-tuning. The design principle of our method relies on
reparametrization that learns not only the weights but also the topological
structure of the lightweight sub-network. This reparametrization acts as a
prior (or regularizer) that defines pruning masks implicitly from the weights
of the underlying network, without increasing the number of training
parameters. Sparsity is induced with a budget loss that provides an accurate
pruning. Extensive experiments conducted on the CIFAR10 and the TinyImageNet
datasets, using standard architectures (namely Conv4, VGG19 and ResNet18), show
compelling results without fine-tuning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dupont_R/0/1/0/all/0/1"&gt;Robin Dupont&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahbi_H/0/1/0/all/0/1"&gt;Hichem Sahbi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michel_G/0/1/0/all/0/1"&gt;Guillaume Michel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Data Processing in Space for Object Detection in Satellite Imagery. (arXiv:2107.03774v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03774</id>
        <link href="http://arxiv.org/abs/2107.03774"/>
        <updated>2021-07-09T01:58:25.507Z</updated>
        <summary type="html"><![CDATA[There is a proliferation in the number of satellites launched each year,
resulting in downlinking of terabytes of data each day. The data received by
ground stations is often unprocessed, making this an expensive process
considering the large data sizes and that not all of the data is useful. This,
coupled with the increasing demand for real-time data processing, has led to a
growing need for on-orbit processing solutions. In this work, we investigate
the performance of CNN-based object detectors on constrained devices by
applying different image compression techniques to satellite data. We examine
the capabilities of the NVIDIA Jetson Nano and NVIDIA Jetson AGX Xavier;
low-power, high-performance computers, with integrated GPUs, small enough to
fit on-board a nanosatellite. We take a closer look at object detection
networks, including the Single Shot MultiBox Detector (SSD) and Region-based
Fully Convolutional Network (R-FCN) models that are pre-trained on DOTA - a
Large Scale Dataset for Object Detection in Aerial Images. The performance is
measured in terms of execution time, memory consumption, and accuracy, and are
compared against a baseline containing a server with two powerful GPUs. The
results show that by applying image compression techniques, we are able to
improve the execution time and memory consumption, achieving a fully runnable
dataset. A lossless compression technique achieves roughly a 10% reduction in
execution time and about a 3% reduction in memory consumption, with no impact
on the accuracy. While a lossy compression technique improves the execution
time by up to 144% and the memory consumption is reduced by as much as 97%.
However, it has a significant impact on accuracy, varying depending on the
compression ratio. Thus the application and ratio of these compression
techniques may differ depending on the required level of accuracy for a
particular task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lofqvist_M/0/1/0/all/0/1"&gt;Martina Lofqvist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cano_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Cano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparing ML based Segmentation Models on Jet Fire Radiation Zone. (arXiv:2107.03461v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03461</id>
        <link href="http://arxiv.org/abs/2107.03461"/>
        <updated>2021-07-09T01:58:25.403Z</updated>
        <summary type="html"><![CDATA[Risk assessment is relevant in any workplace, however there is a degree of
unpredictability when dealing with flammable or hazardous materials so that
detection of fire accidents by itself may not be enough. An example of this is
the impingement of jet fires, where the heat fluxes of the flame could reach
nearby equipment and dramatically increase the probability of a domino effect
with catastrophic results. Because of this, the characterization of such fire
accidents is important from a risk management point of view. One such
characterization would be the segmentation of different radiation zones within
the flame, so this paper presents an exploratory research regarding several
traditional computer vision and Deep Learning segmentation approaches to solve
this specific problem. A data set of propane jet fires is used to train and
evaluate the different approaches and given the difference in the distribution
of the zones and background of the images, different loss functions, that seek
to alleviate data imbalance, are also explored. Additionally, different metrics
are correlated to a manual ranking performed by experts to make an evaluation
that closely resembles the expert's criteria. The Hausdorff Distance and
Adjsted Random Index were the metrics with the highest correlation and the best
results were obtained from the UNet architecture with a Weighted Cross-Entropy
Loss. These results can be used in future research to extract more geometric
information from the segmentation masks or could even be implemented on other
types of fire accidents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Guerrero_C/0/1/0/all/0/1"&gt;Carmina P&amp;#xe9;rez-Guerrero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palacios_A/0/1/0/all/0/1"&gt;Adriana Palacios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1"&gt;Gilberto Ochoa-Ruiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mata_C/0/1/0/all/0/1"&gt;Christian Mata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Mendoza_M/0/1/0/all/0/1"&gt;Miguel Gonzalez-Mendoza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Falcon_Morales_L/0/1/0/all/0/1"&gt;Luis Eduardo Falc&amp;#xf3;n-Morales&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Investigate the Essence of Long-Tailed Recognition from a Unified Perspective. (arXiv:2107.03758v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03758</id>
        <link href="http://arxiv.org/abs/2107.03758"/>
        <updated>2021-07-09T01:58:25.295Z</updated>
        <summary type="html"><![CDATA[As the data scale grows, deep recognition models often suffer from
long-tailed data distributions due to the heavy imbalanced sample number across
categories. Indeed, real-world data usually exhibit some similarity relation
among different categories (e.g., pigeons and sparrows), called category
similarity in this work. It is doubly difficult when the imbalance occurs
between such categories with similar appearances. However, existing solutions
mainly focus on the sample number to re-balance data distribution. In this
work, we systematically investigate the essence of the long-tailed problem from
a unified perspective. Specifically, we demonstrate that long-tailed
recognition suffers from both sample number and category similarity.
Intuitively, using a toy example, we first show that sample number is not the
unique influence factor for performance dropping of long-tailed recognition.
Theoretically, we demonstrate that (1) category similarity, as an inevitable
factor, would also influence the model learning under long-tailed distribution
via similar samples, (2) using more discriminative representation methods
(e.g., self-supervised learning) for similarity reduction, the classifier bias
can be further alleviated with greatly improved performance. Extensive
experiments on several long-tailed datasets verify the rationality of our
theoretical analysis, and show that based on existing state-of-the-arts
(SOTAs), the performance could be further improved by similarity reduction. Our
investigations highlight the essence behind the long-tailed problem, and claim
several feasible directions for future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Li Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NccFlow: Unsupervised Learning of Optical Flow With Non-occlusion from Geometry. (arXiv:2107.03610v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03610</id>
        <link href="http://arxiv.org/abs/2107.03610"/>
        <updated>2021-07-09T01:58:25.283Z</updated>
        <summary type="html"><![CDATA[Optical flow estimation is a fundamental problem of computer vision and has
many applications in the fields of robot learning and autonomous driving. This
paper reveals novel geometric laws of optical flow based on the insight and
detailed definition of non-occlusion. Then, two novel loss functions are
proposed for the unsupervised learning of optical flow based on the geometric
laws of non-occlusion. Specifically, after the occlusion part of the images are
masked, the flowing process of pixels is carefully considered and geometric
constraints are conducted based on the geometric laws of optical flow. First,
neighboring pixels in the first frame will not intersect during the pixel
displacement to the second frame. Secondly, when the cluster containing
adjacent four pixels in the first frame moves to the second frame, no other
pixels will flow into the quadrilateral formed by them. According to the two
geometrical constraints, the optical flow non-intersection loss and the optical
flow non-blocking loss in the non-occlusion regions are proposed. Two loss
functions punish the irregular and inexact optical flows in the non-occlusion
regions. The experiments on datasets demonstrated that the proposed
unsupervised losses of optical flow based on the geometric laws in
non-occlusion regions make the estimated optical flow more refined in detail,
and improve the performance of unsupervised learning of optical flow. In
addition, the experiments training on synthetic data and evaluating on real
data show that the generalization ability of optical flow network is improved
by our proposed unsupervised approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guangming Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Shuaiqi Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hesheng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combining Transformer Generators with Convolutional Discriminators. (arXiv:2105.10189v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10189</id>
        <link href="http://arxiv.org/abs/2105.10189"/>
        <updated>2021-07-09T01:58:25.268Z</updated>
        <summary type="html"><![CDATA[Transformer models have recently attracted much interest from computer vision
researchers and have since been successfully employed for several problems
traditionally addressed with convolutional neural networks. At the same time,
image synthesis using generative adversarial networks (GANs) has drastically
improved over the last few years. The recently proposed TransGAN is the first
GAN using only transformer-based architectures and achieves competitive results
when compared to convolutional GANs. However, since transformers are
data-hungry architectures, TransGAN requires data augmentation, an auxiliary
super-resolution task during training, and a masking prior to guide the
self-attention mechanism. In this paper, we study the combination of a
transformer-based generator and convolutional discriminator and successfully
remove the need of the aforementioned required design choices. We evaluate our
approach by conducting a benchmark of well-known CNN discriminators, ablate the
size of the transformer-based generator, and show that combining both
architectural elements into a hybrid model leads to better results.
Furthermore, we investigate the frequency spectrum properties of generated
images and observe that our model retains the benefits of an attention based
generator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Durall_R/0/1/0/all/0/1"&gt;Ricard Durall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frolov_S/0/1/0/all/0/1"&gt;Stanislav Frolov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1"&gt;Andreas Dengel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1"&gt;Janis Keuper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meeting the SDGs : Enabling the Goals by Cooperation with Crowd using a Conversational AI Platform. (arXiv:2107.04011v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2107.04011</id>
        <link href="http://arxiv.org/abs/2107.04011"/>
        <updated>2021-07-09T01:58:25.216Z</updated>
        <summary type="html"><![CDATA[In this paper, we report about a large-scale online discussion with 1099
citizens on the Afghanistan Sustainable Development Goals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haqbeen_J/0/1/0/all/0/1"&gt;J. Haqbeen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ito_T/0/1/0/all/0/1"&gt;T. Ito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahab_S/0/1/0/all/0/1"&gt;S. Sahab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hadfi_R/0/1/0/all/0/1"&gt;R. Hadfi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sato_T/0/1/0/all/0/1"&gt;T. Sato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Okuhara_S/0/1/0/all/0/1"&gt;S. Okuhara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vector Space Morphology with Linear Discriminative Learning. (arXiv:2107.03950v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.03950</id>
        <link href="http://arxiv.org/abs/2107.03950"/>
        <updated>2021-07-09T01:58:25.209Z</updated>
        <summary type="html"><![CDATA[This paper presents three case studies of modeling aspects of lexical
processing with Linear Discriminative Learning (LDL), the computational engine
of the Discriminative Lexicon model (Baayen et al., 2019). With numeric
representations of word forms and meanings, LDL learns to map one vector space
onto the other, without being informed about any morphological structure or
inflectional classes. The modeling results demonstrated that LDL not only
performs well for understanding and producing morphologically complex words,
but also generates quantitative measures that are predictive for human
behavioral data. LDL models are straightforward to implement with the JudiLing
package (Luo et al., 2021). Worked examples are provided for three modeling
challenges: producing and understanding Korean verb inflection, predicting
primed Dutch lexical decision latencies, and predicting the acoustic duration
of Mandarin words.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1"&gt;Yu-Ying Chuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1"&gt;Mihi Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xuefeng Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baayen_R/0/1/0/all/0/1"&gt;R. Harald Baayen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Dataset and Method for Hallux Valgus Angle Estimation Based on Deep Learing. (arXiv:2107.03640v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03640</id>
        <link href="http://arxiv.org/abs/2107.03640"/>
        <updated>2021-07-09T01:58:25.202Z</updated>
        <summary type="html"><![CDATA[Angular measurements is essential to make a resonable treatment for Hallux
valgus (HV), a common forefoot deformity. However, it still depends on manual
labeling and measurement, which is time-consuming and sometimes unreliable.
Automating this process is a thing of concern. However, it lack of dataset and
the keypoints based method which made a great success in pose estimation is not
suitable for this field.To solve the problems, we made a dataset and developed
an algorithm based on deep learning and linear regression. It shows great
fitting ability to the ground truth.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1"&gt;Ningyuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1"&gt;Jiayan Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yaojun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1"&gt;Jiangjian Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual-Consistency Semi-Supervised Learning with Uncertainty Quantification for COVID-19 Lesion Segmentation from CT Images. (arXiv:2104.03225v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03225</id>
        <link href="http://arxiv.org/abs/2104.03225"/>
        <updated>2021-07-09T01:58:25.194Z</updated>
        <summary type="html"><![CDATA[The novel coronavirus disease 2019 (COVID-19) characterized by atypical
pneumonia has caused millions of deaths worldwide. Automatically segmenting
lesions from chest Computed Tomography (CT) is a promising way to assist
doctors in COVID-19 screening, treatment planning, and follow-up monitoring.
However, voxel-wise annotations are extremely expert-demanding and scarce,
especially when it comes to novel diseases, while an abundance of unlabeled
data could be available. To tackle the challenge of limited annotations, in
this paper, we propose an uncertainty-guided dual-consistency learning network
(UDC-Net) for semi-supervised COVID-19 lesion segmentation from CT images.
Specifically, we present a dual-consistency learning scheme that simultaneously
imposes image transformation equivalence and feature perturbation invariance to
effectively harness the knowledge from unlabeled data. We then quantify the
segmentation uncertainty in two forms and employ them together to guide the
consistency regularization for more reliable unsupervised learning. Extensive
experiments showed that our proposed UDC-Net improves the fully supervised
method by 6.3% in Dice and outperforms other competitive semi-supervised
approaches by significant margins, demonstrating high potential in real-world
clinical practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yanwen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Luo_L/0/1/0/all/0/1"&gt;Luyang Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lin_H/0/1/0/all/0/1"&gt;Huangjing Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Heng_P/0/1/0/all/0/1"&gt;Pheng-Ann Heng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Worry, coping and resignation -- A repeated-measures study on emotional responses after a year in the pandemic. (arXiv:2107.03466v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.03466</id>
        <link href="http://arxiv.org/abs/2107.03466"/>
        <updated>2021-07-09T01:58:25.183Z</updated>
        <summary type="html"><![CDATA[The introduction of COVID-19 lockdown measures and an outlook on return to
normality are demanding societal changes. Among the most pressing questions is
how individuals adjust to the pandemic. This paper examines the emotional
responses to the pandemic in a repeated-measures design. Data (n=1698) were
collected in April 2020 (during strict lockdown measures) and in April 2021
(when vaccination programmes gained traction). We asked participants to report
their emotions and express these in text data. Statistical tests revealed an
average trend towards better adjustment to the pandemic. However, clustering
analyses suggested a more complex heterogeneous pattern with a well-coping and
a resigning subgroup of participants. Linguistic computational analyses
uncovered that topics and n-gram frequencies shifted towards attention to the
vaccination programme and away from general worrying. Implications for public
mental health efforts in identifying people at heightened risk are discussed.
The dataset is made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mozes_M/0/1/0/all/0/1"&gt;Maximilian Mozes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vegt_I/0/1/0/all/0/1"&gt;Isabelle van der Vegt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kleinberg_B/0/1/0/all/0/1"&gt;Bennett Kleinberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LanguageRefer: Spatial-Language Model for 3D Visual Grounding. (arXiv:2107.03438v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.03438</id>
        <link href="http://arxiv.org/abs/2107.03438"/>
        <updated>2021-07-09T01:58:25.164Z</updated>
        <summary type="html"><![CDATA[To realize robots that can understand human instructions and perform
meaningful tasks in the near future, it is important to develop learned models
that can understand referential language to identify common objects in
real-world 3D scenes. In this paper, we develop a spatial-language model for a
3D visual grounding problem. Specifically, given a reconstructed 3D scene in
the form of a point cloud with 3D bounding boxes of potential object
candidates, and a language utterance referring to a target object in the scene,
our model identifies the target object from a set of potential candidates. Our
spatial-language model uses a transformer-based architecture that combines
spatial embedding from bounding-box with a finetuned language embedding from
DistilBert and reasons among the objects in the 3D scene to find the target
object. We show that our model performs competitively on visio-linguistic
datasets proposed by ReferIt3D. We provide additional analysis of performance
in spatial reasoning tasks decoupled from perception noise, the effect of
view-dependent utterances in terms of accuracy, and view-point annotations for
potential robotics applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roh_J/0/1/0/all/0/1"&gt;Junha Roh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desingh_K/0/1/0/all/0/1"&gt;Karthik Desingh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1"&gt;Ali Farhadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1"&gt;Dieter Fox&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy Concerns in Chatbot Interactions: When to Trust and When to Worry. (arXiv:2107.03959v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2107.03959</id>
        <link href="http://arxiv.org/abs/2107.03959"/>
        <updated>2021-07-09T01:58:25.157Z</updated>
        <summary type="html"><![CDATA[Through advances in their conversational abilities, chatbots have started to
request and process an increasing variety of sensitive personal information.
The accurate disclosure of sensitive information is essential where it is used
to provide advice and support to users in the healthcare and finance sectors.
In this study, we explore users' concerns regarding factors associated with the
use of sensitive data by chatbot providers. We surveyed a representative sample
of 491 British citizens. Our results show that the user concerns focus on
deleting personal information and concerns about their data's inappropriate
use. We also identified that individuals were concerned about losing control
over their data after a conversation with conversational agents. We found no
effect from a user's gender or education but did find an effect from the user's
age, with those over 45 being more concerned than those under 45. We also
considered the factors that engender trust in a chatbot. Our respondents'
primary focus was on the chatbot's technical elements, with factors such as the
response quality being identified as the most critical factor. We again found
no effect from the user's gender or education level; however, when we
considered some social factors (e.g. avatars or perceived 'friendliness'), we
found those under 45 years old rated these as more important than those over
45. The paper concludes with a discussion of these results within the context
of designing inclusive, digital systems that support a wide range of users.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saglam_R/0/1/0/all/0/1"&gt;Rahime Belen Saglam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nurse_J/0/1/0/all/0/1"&gt;Jason R.C. Nurse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hodges_D/0/1/0/all/0/1"&gt;Duncan Hodges&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CANDLE: Decomposing Conditional and Conjunctive Queries for Task-Oriented Dialogue Systems. (arXiv:2107.03884v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.03884</id>
        <link href="http://arxiv.org/abs/2107.03884"/>
        <updated>2021-07-09T01:58:25.149Z</updated>
        <summary type="html"><![CDATA[Domain-specific dialogue systems generally determine user intents by relying
on sentence-level classifiers which mainly focus on single action sentences.
Such classifiers are not designed to effectively handle complex queries
composed of conditional and sequential clauses that represent multiple actions.
We attempt to decompose such queries into smaller single-action sub-queries
that are reasonable for intent classifiers to understand in a dialogue
pipeline. We release CANDLE (Conditional & AND type Expressions), a dataset
consisting of 3124 utterances manually tagged with conditional and sequential
labels and demonstrates this decomposition by training two baseline taggers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Aadesh Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhole_K/0/1/0/all/0/1"&gt;Kaustubh D.Dhole&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tarway_R/0/1/0/all/0/1"&gt;Rahul Tarway&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prabhakar_S/0/1/0/all/0/1"&gt;Swetha Prabhakar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Ashish Shrivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using CollGram to Compare Formulaic Language in Human and Neural Machine Translation. (arXiv:2107.03625v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.03625</id>
        <link href="http://arxiv.org/abs/2107.03625"/>
        <updated>2021-07-09T01:58:25.139Z</updated>
        <summary type="html"><![CDATA[A comparison of formulaic sequences in human and neural machine translation
of quality newspaper articles shows that neural machine translations contain
less lower-frequency, but strongly-associated formulaic sequences, and more
high-frequency formulaic sequences. These differences were statistically
significant and the effect sizes were almost always medium or large. These
observations can be related to the differences between second language learners
of various levels and between translated and untranslated texts. The comparison
between the neural machine translation systems indicates that some systems
produce more formulaic sequences of both types than other systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bestgen_Y/0/1/0/all/0/1"&gt;Yves Bestgen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Medical Matting: A New Perspective on Medical Segmentation with Uncertainty. (arXiv:2106.09887v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09887</id>
        <link href="http://arxiv.org/abs/2106.09887"/>
        <updated>2021-07-09T01:58:25.128Z</updated>
        <summary type="html"><![CDATA[In medical image segmentation, it is difficult to mark ambiguous areas
accurately with binary masks, especially when dealing with small lesions.
Therefore, it is a challenge for radiologists to reach a consensus by using
binary masks under the condition of multiple annotations. However, these areas
may contain anatomical structures that are conducive to diagnosis. Uncertainty
is introduced to study these situations. Nevertheless, the uncertainty is
usually measured by the variances between predictions in a multiple trial way.
It is not intuitive, and there is no exact correspondence in the image.
Inspired by image matting, we introduce matting as a soft segmentation method
and a new perspective to deal with and represent uncertain regions into medical
scenes, namely medical matting. More specifically, because there is no
available medical matting dataset, we first labeled two medical datasets with
alpha matte. Secondly, the matting method applied to the natural image is not
suitable for the medical scene, so we propose a new architecture to generate
binary masks and alpha matte in a row. Thirdly, the uncertainty map is
introduced to highlight the ambiguous regions from the binary results and
improve the matting performance. Evaluated on these datasets, the proposed
model outperformed state-of-the-art matting algorithms by a large margin, and
alpha matte is proved to be a more efficient labeling form than a binary mask.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ju_L/0/1/0/all/0/1"&gt;Lie Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Donghao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1"&gt;Wanji He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yelin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhiwen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1"&gt;Xuan Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xiufen Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1"&gt;Zongyuan Ge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Bangla Natural Language Processing Tasks and the Utility of Transformer Models. (arXiv:2107.03844v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.03844</id>
        <link href="http://arxiv.org/abs/2107.03844"/>
        <updated>2021-07-09T01:58:25.107Z</updated>
        <summary type="html"><![CDATA[Bangla -- ranked as the 6th most widely spoken language across the world
(https://www.ethnologue.com/guides/ethnologue200), with 230 million native
speakers -- is still considered as a low-resource language in the natural
language processing (NLP) community. With three decades of research, Bangla NLP
(BNLP) is still lagging behind mainly due to the scarcity of resources and the
challenges that come with it. There is sparse work in different areas of BNLP;
however, a thorough survey reporting previous work and recent advances is yet
to be done. In this study, we first provide a review of Bangla NLP tasks,
resources, and tools available to the research community; we benchmark datasets
collected from various platforms for nine NLP tasks using current
state-of-the-art algorithms (i.e., transformer-based models). We provide
comparative results for the studied NLP tasks by comparing monolingual vs.
multilingual models of varying sizes. We report our results using both
individual and consolidated datasets and provide data splits for future
research. We reviewed a total of 108 papers and conducted 175 sets of
experiments. Our results show promising performance using transformer-based
models while highlighting the trade-off with computational costs. We hope that
such a comprehensive survey will motivate the community to build on and further
advance the research on Bangla NLP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1"&gt;Firoj Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1"&gt;Arid Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1"&gt;Tanvir Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1"&gt;Akib Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tajrin_J/0/1/0/all/0/1"&gt;Janntatul Tajrin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1"&gt;Naira Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1"&gt;Shammur Absar Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Handling Heavily Abbreviated Manuscripts: HTR engines vs text normalisation approaches. (arXiv:2107.03450v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.03450</id>
        <link href="http://arxiv.org/abs/2107.03450"/>
        <updated>2021-07-09T01:58:25.098Z</updated>
        <summary type="html"><![CDATA[Although abbreviations are fairly common in handwritten sources, particularly
in medieval and modern Western manuscripts, previous research dealing with
computational approaches to their expansion is scarce. Yet abbreviations
present particular challenges to computational approaches such as handwritten
text recognition and natural language processing tasks. Often, pre-processing
ultimately aims to lead from a digitised image of the source to a normalised
text, which includes expansion of the abbreviations. We explore different
setups to obtain such a normalised text, either directly, by training HTR
engines on normalised (i.e., expanded, disabbreviated) text, or by decomposing
the process into discrete steps, each making use of specialist models for
recognition, word segmentation and normalisation. The case studies considered
here are drawn from the medieval Latin tradition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Camps_J/0/1/0/all/0/1"&gt;Jean-Baptiste Camps&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vidal_Gorene_C/0/1/0/all/0/1"&gt;Chahan Vidal-Gor&amp;#xe8;ne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vernet_M/0/1/0/all/0/1"&gt;Marguerite Vernet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilingual Speech Evaluation: Case Studies on English, Malay and Tamil. (arXiv:2107.03675v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.03675</id>
        <link href="http://arxiv.org/abs/2107.03675"/>
        <updated>2021-07-09T01:58:25.090Z</updated>
        <summary type="html"><![CDATA[Speech evaluation is an essential component in computer-assisted language
learning (CALL). While speech evaluation on English has been popular, automatic
speech scoring on low resource languages remains challenging. Work in this area
has focused on monolingual specific designs and handcrafted features stemming
from resource-rich languages like English. Such approaches are often difficult
to generalize to other languages, especially if we also want to consider
suprasegmental qualities such as rhythm. In this work, we examine three
different languages that possess distinct rhythm patterns: English
(stress-timed), Malay (syllable-timed), and Tamil (mora-timed). We exploit
robust feature representations inspired by music processing and vector
representation learning. Empirical validations show consistent gains for all
three languages when predicting pronunciation, rhythm and intonation
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huayun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1"&gt;Ke Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nancy F. Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COMBO: a new module for EUD parsing. (arXiv:2107.03809v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.03809</id>
        <link href="http://arxiv.org/abs/2107.03809"/>
        <updated>2021-07-09T01:58:25.082Z</updated>
        <summary type="html"><![CDATA[We introduce the COMBO-based approach for EUD parsing and its implementation,
which took part in the IWPT 2021 EUD shared task. The goal of this task is to
parse raw texts in 17 languages into Enhanced Universal Dependencies (EUD). The
proposed approach uses COMBO to predict UD trees and EUD graphs. These
structures are then merged into the final EUD graphs. Some EUD edge labels are
extended with case information using a single language-independent expansion
rule. In the official evaluation, the solution ranked fourth, achieving an
average ELAS of 83.79%. The source code is available at
https://gitlab.clarin-pl.eu/syntactic-tools/combo.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Klimaszewski_M/0/1/0/all/0/1"&gt;Mateusz Klimaszewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wroblewska_A/0/1/0/all/0/1"&gt;Alina Wr&amp;#xf3;blewska&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anticipating Safety Issues in E2E Conversational AI: Framework and Tooling. (arXiv:2107.03451v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.03451</id>
        <link href="http://arxiv.org/abs/2107.03451"/>
        <updated>2021-07-09T01:58:25.069Z</updated>
        <summary type="html"><![CDATA[Over the last several years, end-to-end neural conversational agents have
vastly improved in their ability to carry a chit-chat conversation with humans.
However, these models are often trained on large datasets from the internet,
and as a result, may learn undesirable behaviors from this data, such as toxic
or otherwise harmful language. Researchers must thus wrestle with the issue of
how and when to release these models. In this paper, we survey the problem
landscape for safety for end-to-end conversational AI and discuss recent and
related work. We highlight tensions between values, potential positive impact
and potential harms, and provide a framework for making decisions about whether
and how to release these models, following the tenets of value-sensitive
design. We additionally provide a suite of tools to enable researchers to make
better-informed decisions about training and releasing end-to-end
conversational AI models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dinan_E/0/1/0/all/0/1"&gt;Emily Dinan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abercrombie_G/0/1/0/all/0/1"&gt;Gavin Abercrombie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bergman_A/0/1/0/all/0/1"&gt;A. Stevie Bergman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spruit_S/0/1/0/all/0/1"&gt;Shannon Spruit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1"&gt;Dirk Hovy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boureau_Y/0/1/0/all/0/1"&gt;Y-Lan Boureau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rieser_V/0/1/0/all/0/1"&gt;Verena Rieser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[POSLAN: Disentangling Chat with Positional and Language encoded Post Embeddings. (arXiv:2107.03529v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.03529</id>
        <link href="http://arxiv.org/abs/2107.03529"/>
        <updated>2021-07-09T01:58:25.046Z</updated>
        <summary type="html"><![CDATA[Most online message threads inherently will be cluttered and any new user or
an existing user visiting after a hiatus will have a difficult time
understanding whats being discussed in the thread. Similarly cluttered
responses in a message thread makes analyzing the messages a difficult problem.
The need for disentangling the clutter is much higher when the platform where
the discussion is taking place does not provide functions to retrieve reply
relations of the messages. This introduces an interesting problem to which
\cite{wang2011learning} phrases as a structural learning problem. We create
vector embeddings for posts in a thread so that it captures both linguistic and
positional features in relation to a context of where a given message is in.
Using these embeddings for posts we compute a similarity based connectivity
matrix which then converted into a graph. After employing a pruning mechanisms
the resultant graph can be used to discover the reply relation for the posts in
the thread. The process of discovering or disentangling chat is kept as an
unsupervised mechanism. We present our experimental results on a data set
obtained from Telegram with limited meta data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abeysinghe_B/0/1/0/all/0/1"&gt;Bhashithe Abeysinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1"&gt;Dhara Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freas_C/0/1/0/all/0/1"&gt;Chris Freas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harrison_R/0/1/0/all/0/1"&gt;Robert Harrison&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sunderraman_R/0/1/0/all/0/1"&gt;Rajshekhar Sunderraman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HinGE: A Dataset for Generation and Evaluation of Code-Mixed Hinglish Text. (arXiv:2107.03760v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.03760</id>
        <link href="http://arxiv.org/abs/2107.03760"/>
        <updated>2021-07-09T01:58:24.979Z</updated>
        <summary type="html"><![CDATA[Text generation is a highly active area of research in the computational
linguistic community. The evaluation of the generated text is a challenging
task and multiple theories and metrics have been proposed over the years.
Unfortunately, text generation and evaluation are relatively understudied due
to the scarcity of high-quality resources in code-mixed languages where the
words and phrases from multiple languages are mixed in a single utterance of
text and speech. To address this challenge, we present a corpus (HinGE) for a
widely popular code-mixed language Hinglish (code-mixing of Hindi and English
languages). HinGE has Hinglish sentences generated by humans as well as two
rule-based algorithms corresponding to the parallel Hindi-English sentences. In
addition, we demonstrate the inefficacy of widely-used evaluation metrics on
the code-mixed data. The HinGE dataset will facilitate the progress of natural
language generation research in code-mixed languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_V/0/1/0/all/0/1"&gt;Vivek Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1"&gt;Mayank Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Keep it Simple: Unsupervised Simplification of Multi-Paragraph Text. (arXiv:2107.03444v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.03444</id>
        <link href="http://arxiv.org/abs/2107.03444"/>
        <updated>2021-07-09T01:58:24.879Z</updated>
        <summary type="html"><![CDATA[This work presents Keep it Simple (KiS), a new approach to unsupervised text
simplification which learns to balance a reward across three properties:
fluency, salience and simplicity. We train the model with a novel algorithm to
optimize the reward (k-SCST), in which the model proposes several candidate
simplifications, computes each candidate's reward, and encourages candidates
that outperform the mean reward. Finally, we propose a realistic text
comprehension task as an evaluation method for text simplification. When tested
on the English news domain, the KiS model outperforms strong supervised
baselines by more than 4 SARI points, and can help people complete a
comprehension task an average of 18% faster while retaining accuracy, when
compared to the original text. Code available:
https://github.com/tingofurro/keep_it_simple]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Laban_P/0/1/0/all/0/1"&gt;Philippe Laban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schnabel_T/0/1/0/all/0/1"&gt;Tobias Schnabel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennett_P/0/1/0/all/0/1"&gt;Paul Bennett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hearst_M/0/1/0/all/0/1"&gt;Marti A. Hearst&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Field-Aware Neural Ranking Models for Recipe Search. (arXiv:2105.05710v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05710</id>
        <link href="http://arxiv.org/abs/2105.05710"/>
        <updated>2021-07-09T01:58:24.869Z</updated>
        <summary type="html"><![CDATA[Explicitly modelling field interactions and correlations in complex document
structures has recently gained popularity in neural document embedding and
retrieval tasks. Although this requires the specification of bespoke
task-dependent models, encouraging empirical results are beginning to emerge.
We present the first in-depth analyses of non-linear multi-field interaction
(NL-MFI) ranking in the cooking domain in this work. Our results show that
field-weighted factorisation machines models provide a statistically
significant improvement over baselines in recipe retrieval tasks. Additionally,
we show that sparsely capturing subsets of field interactions based on domain
knowledge and feature selection heuristics offers significant advantages over
baselines and exhaustive alternatives. Although field-interaction aware models
are more elaborate from an architectural basis, they are often more
data-efficient in optimisation and are better suited for explainability due to
mirrored document and model factorisation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Takiguchi_K/0/1/0/all/0/1"&gt;Kentaro Takiguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fain_M/0/1/0/all/0/1"&gt;Mikhail Fain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Twomey_N/0/1/0/all/0/1"&gt;Niall Twomey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vaquero_L/0/1/0/all/0/1"&gt;Luis M Vaquero&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can Transformer Models Measure Coherence In Text? Re-Thinking the Shuffle Test. (arXiv:2107.03448v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.03448</id>
        <link href="http://arxiv.org/abs/2107.03448"/>
        <updated>2021-07-09T01:58:24.700Z</updated>
        <summary type="html"><![CDATA[The Shuffle Test is the most common task to evaluate whether NLP models can
measure coherence in text. Most recent work uses direct supervision on the
task; we show that by simply finetuning a RoBERTa model, we can achieve a near
perfect accuracy of 97.8%, a state-of-the-art. We argue that this outstanding
performance is unlikely to lead to a good model of text coherence, and suggest
that the Shuffle Test should be approached in a Zero-Shot setting: models
should be evaluated without being trained on the task itself. We evaluate
common models in this setting, such as Generative and Bi-directional
Transformers, and find that larger architectures achieve high-performance
out-of-the-box. Finally, we suggest the k-Block Shuffle Test, a modification of
the original by increasing the size of blocks shuffled. Even though human
reader performance remains high (around 95% accuracy), model performance drops
from 94% to 78% as block size increases, creating a conceptually simple
challenge to benchmark NLP models. Code available:
https://github.com/tingofurro/shuffle_test/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Laban_P/0/1/0/all/0/1"&gt;Philippe Laban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1"&gt;Luke Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bandarkar_L/0/1/0/all/0/1"&gt;Lucas Bandarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hearst_M/0/1/0/all/0/1"&gt;Marti A. Hearst&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Multi-Step Critiquing for VAE-based Recommender Systems. (arXiv:2105.00774v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00774</id>
        <link href="http://arxiv.org/abs/2105.00774"/>
        <updated>2021-07-09T01:58:24.689Z</updated>
        <summary type="html"><![CDATA[Recent studies have shown that providing personalized explanations alongside
recommendations increases trust and perceived quality. Furthermore, it gives
users an opportunity to refine the recommendations by critiquing parts of the
explanations. On one hand, current recommender systems model the
recommendation, explanation, and critiquing objectives jointly, but this
creates an inherent trade-off between their respective performance. On the
other hand, although recent latent linear critiquing approaches are built upon
an existing recommender system, they suffer from computational inefficiency at
inference due to the objective optimized at each conversation's turn. We
address these deficiencies with M&Ms-VAE, a novel variational autoencoder for
recommendation and explanation that is based on multimodal modeling
assumptions. We train the model under a weak supervision scheme to simulate
both fully and partially observed variables. Then, we leverage the
generalization ability of a trained M&Ms-VAE model to embed the user preference
and the critique separately. Our work's most important innovation is our
critiquing module, which is built upon and trained in a self-supervised manner
with a simple ranking objective. Experiments on four real-world datasets
demonstrate that among state-of-the-art models, our system is the first to
dominate or match the performance in terms of recommendation, explanation, and
multi-step critiquing. Moreover, M&Ms-VAE processes the critiques up to 25.6x
faster than the best baselines. Finally, we show that our model infers coherent
joint and cross generation, even under weak supervision, thanks to our
multimodal-based modeling and training scheme.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1"&gt;Diego Antognini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1"&gt;Boi Faltings&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training. (arXiv:2103.06561v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06561</id>
        <link href="http://arxiv.org/abs/2103.06561"/>
        <updated>2021-07-09T01:58:24.680Z</updated>
        <summary type="html"><![CDATA[Multi-modal pre-training models have been intensively explored to bridge
vision and language in recent years. However, most of them explicitly model the
cross-modal interaction between image-text pairs, by assuming that there exists
strong semantic correlation between the text and image modalities. Since this
strong assumption is often invalid in real-world scenarios, we choose to
implicitly model the cross-modal correlation for large-scale multi-modal
pre-training, which is the focus of the Chinese project `WenLan' led by our
team. Specifically, with the weak correlation assumption over image-text pairs,
we propose a two-tower pre-training model called BriVL within the cross-modal
contrastive learning framework. Unlike OpenAI CLIP that adopts a simple
contrastive learning method, we devise a more advanced algorithm by adapting
the latest method MoCo into the cross-modal scenario. By building a large
queue-based dictionary, our BriVL can incorporate more negative samples in
limited GPU resources. We further construct a large Chinese multi-source
image-text dataset called RUC-CAS-WenLan for pre-training our BriVL model.
Extensive experiments demonstrate that the pre-trained BriVL model outperforms
both UNITER and OpenAI CLIP on various downstream tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1"&gt;Yuqi Huo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Manli Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guangzhen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Haoyu Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yizhao Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Guoxing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Jingyuan Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Heng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1"&gt;Baogui Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1"&gt;Weihao Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1"&gt;Zongzheng Xi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yueqian Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1"&gt;Anwen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jinming Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruichen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yida Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yuqing Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1"&gt;Xin Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1"&gt;Wanqing Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_D/0/1/0/all/0/1"&gt;Danyang Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yingyan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Junyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Peiyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1"&gt;Zheng Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1"&gt;Chuhao Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yuchong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shizhe Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zhiwu Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1"&gt;Zhicheng Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1"&gt;Qin Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1"&gt;Yanyan Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wayne Xin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1"&gt;Ruihua Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Ji-Rong Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized News Recommendation: A Survey. (arXiv:2106.08934v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08934</id>
        <link href="http://arxiv.org/abs/2106.08934"/>
        <updated>2021-07-09T01:58:24.665Z</updated>
        <summary type="html"><![CDATA[Personalized news recommendation is an important technique to help users find
their interested news information and alleviate their information overload. It
has been extensively studied over decades and has achieved notable success in
improving users' news reading experience. However, there are still many
unsolved problems and challenges that need to be further studied. To help
researchers master the advances in personalized news recommendation over the
past years, in this paper we present a comprehensive overview of personalized
news recommendation. Instead of following the conventional taxonomy of news
recommendation methods, in this paper we propose a novel perspective to
understand personalized news recommendation based on its core problems and the
associated techniques and challenges. We first review the techniques for
tackling each core problem in a personalized news recommender system and the
challenges they face. Next, we introduce the public datasets and evaluation
methods for personalized news recommendation. We then discuss the key points on
improving the responsibility of personalized news recommender systems. Finally,
we raise several research directions that are worth investigating in the
future. This paper can provide up-to-date and comprehensive views to help
readers understand the personalized news recommendation field. We hope this
paper can facilitate research on personalized news recommendation and as well
as related fields in natural language processing and data mining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chuhan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fangzhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yongfeng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xing Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Graph-based Approach for Mitigating Multi-sided Exposure Bias in Recommender Systems. (arXiv:2107.03415v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.03415</id>
        <link href="http://arxiv.org/abs/2107.03415"/>
        <updated>2021-07-09T01:58:24.650Z</updated>
        <summary type="html"><![CDATA[Fairness is a critical system-level objective in recommender systems that has
been the subject of extensive recent research. A specific form of fairness is
supplier exposure fairness where the objective is to ensure equitable coverage
of items across all suppliers in recommendations provided to users. This is
especially important in multistakeholder recommendation scenarios where it may
be important to optimize utilities not just for the end-user, but also for
other stakeholders such as item sellers or producers who desire a fair
representation of their items. This type of supplier fairness is sometimes
accomplished by attempting to increasing aggregate diversity in order to
mitigate popularity bias and to improve the coverage of long-tail items in
recommendations. In this paper, we introduce FairMatch, a general graph-based
algorithm that works as a post processing approach after recommendation
generation to improve exposure fairness for items and suppliers. The algorithm
iteratively adds high quality items that have low visibility or items from
suppliers with low exposure to the users' final recommendation lists. A
comprehensive set of experiments on two datasets and comparison with
state-of-the-art baselines show that FairMatch, while significantly improves
exposure fairness and aggregate diversity, maintains an acceptable level of
relevance of the recommendations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mansoury_M/0/1/0/all/0/1"&gt;Masoud Mansoury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdollahpouri_H/0/1/0/all/0/1"&gt;Himan Abdollahpouri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1"&gt;Mykola Pechenizkiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mobasher_B/0/1/0/all/0/1"&gt;Bamshad Mobasher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burke_R/0/1/0/all/0/1"&gt;Robin Burke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rating and aspect-based opinion graph embeddings for explainable recommendations. (arXiv:2107.03385v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.03385</id>
        <link href="http://arxiv.org/abs/2107.03385"/>
        <updated>2021-07-09T01:58:24.618Z</updated>
        <summary type="html"><![CDATA[The success of neural network embeddings has entailed a renewed interest in
using knowledge graphs for a wide variety of machine learning and information
retrieval tasks. In particular, recent recommendation methods based on graph
embeddings have shown state-of-the-art performance. In general, these methods
encode latent rating patterns and content features. Differently from previous
work, in this paper, we propose to exploit embeddings extracted from graphs
that combine information from ratings and aspect-based opinions expressed in
textual reviews. We then adapt and evaluate state-of-the-art graph embedding
techniques over graphs generated from Amazon and Yelp reviews on six domains,
outperforming baseline recommenders. Additionally, our method has the advantage
of providing explanations that involve the coverage of aspect-based opinions
given by users about recommended items.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cantador_I/0/1/0/all/0/1"&gt;Iv&amp;#xe1;n Cantador&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carvallo_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9;s Carvallo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diez_F/0/1/0/all/0/1"&gt;Fernando Diez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Neural Pre-training for Enhancing Recommendations using Side Information. (arXiv:2107.03936v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.03936</id>
        <link href="http://arxiv.org/abs/2107.03936"/>
        <updated>2021-07-09T01:58:24.600Z</updated>
        <summary type="html"><![CDATA[Leveraging the side information associated with entities (i.e.\ users and
items) to enhance the performance of recommendation systems has been widely
recognized as an important modelling dimension. While many existing approaches
focus on the \emph{integration scheme} to incorporate entity side information
-- by combining the recommendation loss function with an extra side
information-aware loss -- in this paper, we propose instead a novel
\emph{pre-training scheme} for leveraging the side information. In particular,
we first pre-train a representation model using the side information of the
entities, and then fine-tune it using an existing general representation-based
recommendation model. Specifically, we propose two pre-training models, named
\gcn{} and \com{}, by considering the entities and their relations constructed
from side information as two different types of graphs respectively, to
pre-train entity embeddings. For the \gcn{} model, two single-relational graphs
are constructed from all the users' and items' side information respectively,
to pre-train entity representations by using the Graph Convolutional Networks.
For the \com{} model, two multi-relational graphs are constructed to pre-train
the entity representations by using the Composition-based Graph Convolutional
Networks. An extensive evaluation of our pre-training models fine-tuned under
four general representation-based recommender models, i.e.\ MF, NCF, NGCF and
LightGCN, shows that effectively pre-training embeddings with both the user's
and item's side information can significantly improve these original models in
terms of both effectiveness and stability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1"&gt;Zaiqiao Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Siwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Macdonald_C/0/1/0/all/0/1"&gt;Craig Macdonald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ounis_I/0/1/0/all/0/1"&gt;Iadh Ounis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Proxy Selection for Session-based Recommender Systems. (arXiv:2107.03564v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.03564</id>
        <link href="http://arxiv.org/abs/2107.03564"/>
        <updated>2021-07-09T01:58:24.586Z</updated>
        <summary type="html"><![CDATA[Session-based Recommender Systems (SRSs) have been actively developed to
recommend the next item of an anonymous short item sequence (i.e., session).
Unlike sequence-aware recommender systems where the whole interaction sequence
of each user can be used to model both the short-term interest and the general
interest of the user, the absence of user-dependent information in SRSs makes
it difficult to directly derive the user's general interest from data.
Therefore, existing SRSs have focused on how to effectively model the
information about short-term interest within the sessions, but they are
insufficient to capture the general interest of users. To this end, we propose
a novel framework to overcome the limitation of SRSs, named ProxySR, which
imitates the missing information in SRSs (i.e., general interest of users) by
modeling proxies of sessions. ProxySR selects a proxy for the input session in
an unsupervised manner, and combines it with the encoded short-term interest of
the session. As a proxy is jointly learned with the short-term interest and
selected by multiple sessions, a proxy learns to play the role of the general
interest of a user and ProxySR learns how to select a suitable proxy for an
input session. Moreover, we propose another real-world situation of SRSs where
a few users are logged-in and leave their identifiers in sessions, and a
revision of ProxySR for the situation. Our experiments on real-world datasets
show that ProxySR considerably outperforms the state-of-the-art competitors,
and the proxies successfully imitate the general interest of the users without
any user-dependent information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1"&gt;Junsu Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1"&gt;SeongKu Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hyun_D/0/1/0/all/0/1"&gt;Dongmin Hyun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hwanjo Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Bangla Natural Language Processing Tasks and the Utility of Transformer Models. (arXiv:2107.03844v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.03844</id>
        <link href="http://arxiv.org/abs/2107.03844"/>
        <updated>2021-07-09T01:58:24.571Z</updated>
        <summary type="html"><![CDATA[Bangla -- ranked as the 6th most widely spoken language across the world
(https://www.ethnologue.com/guides/ethnologue200), with 230 million native
speakers -- is still considered as a low-resource language in the natural
language processing (NLP) community. With three decades of research, Bangla NLP
(BNLP) is still lagging behind mainly due to the scarcity of resources and the
challenges that come with it. There is sparse work in different areas of BNLP;
however, a thorough survey reporting previous work and recent advances is yet
to be done. In this study, we first provide a review of Bangla NLP tasks,
resources, and tools available to the research community; we benchmark datasets
collected from various platforms for nine NLP tasks using current
state-of-the-art algorithms (i.e., transformer-based models). We provide
comparative results for the studied NLP tasks by comparing monolingual vs.
multilingual models of varying sizes. We report our results using both
individual and consolidated datasets and provide data splits for future
research. We reviewed a total of 108 papers and conducted 175 sets of
experiments. Our results show promising performance using transformer-based
models while highlighting the trade-off with computational costs. We hope that
such a comprehensive survey will motivate the community to build on and further
advance the research on Bangla NLP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1"&gt;Firoj Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1"&gt;Arid Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1"&gt;Tanvir Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1"&gt;Akib Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tajrin_J/0/1/0/all/0/1"&gt;Janntatul Tajrin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1"&gt;Naira Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1"&gt;Shammur Absar Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heterogeneous Global Graph Neural Networks for Personalized Session-based Recommendation. (arXiv:2107.03813v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.03813</id>
        <link href="http://arxiv.org/abs/2107.03813"/>
        <updated>2021-07-09T01:58:24.546Z</updated>
        <summary type="html"><![CDATA[Predicting the next interaction of a short-term interaction session is a
challenging task in session-based recommendation. Almost all existing works
rely on item transition patterns, and neglect the impact of user historical
sessions while modeling user preference, which often leads to non-personalized
recommendation. Additionally, existing personalized session-based recommenders
capture user preference only based on the sessions of the current user, but
ignore the useful item-transition patterns from other user's historical
sessions. To address these issues, we propose a novel Heterogeneous Global
Graph Neural Networks (HG-GNN) to exploit the item transitions over all
sessions in a subtle manner for better inferring user preference from the
current and historical sessions. To effectively exploit the item transitions
over all sessions from users, we propose a novel heterogeneous global graph
that contains item transitions of sessions, user-item interactions and global
co-occurrence items. Moreover, to capture user preference from sessions
comprehensively, we propose to learn two levels of user representations from
the global graph via two graph augmented preference encoders. Specifically, we
design a novel heterogeneous graph neural network (HGNN) on the heterogeneous
global graph to learn the long-term user preference and item representations
with rich semantics. Based on the HGNN, we propose the Current Preference
Encoder and the Historical Preference Encoder to capture the different levels
of user preference from the current and historical sessions, respectively. To
achieve personalized recommendation, we integrate the representations of the
user current preference and historical interests to generate the final user
preference representation. Extensive experimental results on three real-world
datasets show that our model outperforms other state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1"&gt;Yitong Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lingfei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Q/0/1/0/all/0/1"&gt;Qi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yiming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1"&gt;Zhihua Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1"&gt;Fangli Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1"&gt;Ethan Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1"&gt;Bo Long&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LanguageRefer: Spatial-Language Model for 3D Visual Grounding. (arXiv:2107.03438v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.03438</id>
        <link href="http://arxiv.org/abs/2107.03438"/>
        <updated>2021-07-09T01:58:24.521Z</updated>
        <summary type="html"><![CDATA[To realize robots that can understand human instructions and perform
meaningful tasks in the near future, it is important to develop learned models
that can understand referential language to identify common objects in
real-world 3D scenes. In this paper, we develop a spatial-language model for a
3D visual grounding problem. Specifically, given a reconstructed 3D scene in
the form of a point cloud with 3D bounding boxes of potential object
candidates, and a language utterance referring to a target object in the scene,
our model identifies the target object from a set of potential candidates. Our
spatial-language model uses a transformer-based architecture that combines
spatial embedding from bounding-box with a finetuned language embedding from
DistilBert and reasons among the objects in the 3D scene to find the target
object. We show that our model performs competitively on visio-linguistic
datasets proposed by ReferIt3D. We provide additional analysis of performance
in spatial reasoning tasks decoupled from perception noise, the effect of
view-dependent utterances in terms of accuracy, and view-point annotations for
potential robotics applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roh_J/0/1/0/all/0/1"&gt;Junha Roh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desingh_K/0/1/0/all/0/1"&gt;Karthik Desingh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1"&gt;Ali Farhadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1"&gt;Dieter Fox&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Generative Model for Raw Audio Using Transformer Architectures. (arXiv:2106.16036v3 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.16036</id>
        <link href="http://arxiv.org/abs/2106.16036"/>
        <updated>2021-07-09T01:58:24.045Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel way of doing audio synthesis at the waveform
level using Transformer architectures. We propose a deep neural network for
generating waveforms, similar to wavenet. This is fully probabilistic,
auto-regressive, and causal, i.e. each sample generated depends only on the
previously observed samples. Our approach outperforms a widely used wavenet
architecture by up to 9% on a similar dataset for predicting the next step.
Using the attention mechanism, we enable the architecture to learn which audio
samples are important for the prediction of the future sample. We show how
causal transformer generative models can be used for raw waveform synthesis. We
also show that this performance can be improved by another 2% by conditioning
samples over a wider context. The flexibility of the current model to
synthesize audio from latent representations suggests a large number of
potential applications. The novel approach of using generative transformer
architectures for raw audio synthesis is, however, still far away from
generating any meaningful music, without using latent codes/meta-data to aid
the generation process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1"&gt;Prateek Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chafe_C/0/1/0/all/0/1"&gt;Chris Chafe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Episodic Bandits with Stochastic Experts. (arXiv:2107.03263v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03263</id>
        <link href="http://arxiv.org/abs/2107.03263"/>
        <updated>2021-07-08T01:58:00.342Z</updated>
        <summary type="html"><![CDATA[We study a version of the contextual bandit problem where an agent is given
soft control of a node in a graph-structured environment through a set of
stochastic expert policies. The agent interacts with the environment over
episodes, with each episode having different context distributions; this
results in the `best expert' changing across episodes. Our goal is to develop
an agent that tracks the best expert over episodes. We introduce the Empirical
Divergence-based UCB (ED-UCB) algorithm in this setting where the agent does
not have any knowledge of the expert policies or changes in context
distributions. With mild assumptions, we show that bootstrapping from
$\tilde{O}(N\log(NT^2\sqrt{E}))$ samples results in a regret of
$\tilde{O}(E(N+1) + \frac{N\sqrt{E}}{T^2})$. If the expert policies are known
to the agent a priori, then we can improve the regret to $\tilde{O}(EN)$
without requiring any bootstrapping. Our analysis also tightens pre-existing
logarithmic regret bounds to a problem-dependent constant in the non-episodic
setting when expert policies are known. We finally empirically validate our
findings through simulations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_N/0/1/0/all/0/1"&gt;Nihal Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1"&gt;Soumya Basu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1"&gt;Karthikeyan Shanmugam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1"&gt;Sanjay Shakkottai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KaFiStO: A Kalman Filtering Framework for Stochastic Optimization. (arXiv:2107.03331v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03331</id>
        <link href="http://arxiv.org/abs/2107.03331"/>
        <updated>2021-07-08T01:58:00.335Z</updated>
        <summary type="html"><![CDATA[Optimization is often cast as a deterministic problem, where the solution is
found through some iterative procedure such as gradient descent. However, when
training neural networks the loss function changes over (iteration) time due to
the randomized selection of a subset of the samples. This randomization turns
the optimization problem into a stochastic one. We propose to consider the loss
as a noisy observation with respect to some reference optimum. This
interpretation of the loss allows us to adopt Kalman filtering as an optimizer,
as its recursive formulation is designed to estimate unknown parameters from
noisy measurements. Moreover, we show that the Kalman Filter dynamical model
for the evolution of the unknown parameters can be used to capture the gradient
dynamics of advanced methods such as Momentum and Adam. We call this stochastic
optimization method KaFiStO. KaFiStO is an easy to implement, scalable, and
efficient method to train neural networks. We show that it also yields
parameter estimates that are on par with or better than existing optimization
algorithms across several neural network architectures and machine learning
tasks, such as computer vision and language modeling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Davtyan_A/0/1/0/all/0/1"&gt;Aram Davtyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sameni_S/0/1/0/all/0/1"&gt;Sepehr Sameni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cerkezi_L/0/1/0/all/0/1"&gt;Llukman Cerkezi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meishvilli_G/0/1/0/all/0/1"&gt;Givi Meishvilli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bielski_A/0/1/0/all/0/1"&gt;Adam Bielski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Favaro_P/0/1/0/all/0/1"&gt;Paolo Favaro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularization-based Continual Learning for Fault Prediction in Lithium-Ion Batteries. (arXiv:2107.03336v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03336</id>
        <link href="http://arxiv.org/abs/2107.03336"/>
        <updated>2021-07-08T01:58:00.328Z</updated>
        <summary type="html"><![CDATA[In recent years, the use of lithium-ion batteries has greatly expanded into
products from many industrial sectors, e.g. cars, power tools or medical
devices. An early prediction and robust understanding of battery faults could
therefore greatly increase product quality in those fields. While current
approaches for data-driven fault prediction provide good results on the exact
processes they were trained on, they often lack the ability to flexibly adapt
to changes, e.g. in operational or environmental parameters. Continual learning
promises such flexibility, allowing for an automatic adaption of previously
learnt knowledge to new tasks. Therefore, this article discusses different
continual learning approaches from the group of regularization strategies,
which are implemented, evaluated and compared based on a real battery wear
dataset. Online elastic weight consolidation delivers the best results, but, as
with all examined approaches, its performance appears to be strongly dependent
on task characteristics and task sequence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maschler_B/0/1/0/all/0/1"&gt;Benjamin Maschler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tatiyosyan_S/0/1/0/all/0/1"&gt;Sophia Tatiyosyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weyrich_M/0/1/0/all/0/1"&gt;Michael Weyrich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices. (arXiv:2008.02790v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.02790</id>
        <link href="http://arxiv.org/abs/2008.02790"/>
        <updated>2021-07-08T01:58:00.302Z</updated>
        <summary type="html"><![CDATA[The goal of meta-reinforcement learning (meta-RL) is to build agents that can
quickly learn new tasks by leveraging prior experience on related tasks.
Learning a new task often requires both exploring to gather task-relevant
information and exploiting this information to solve the task. In principle,
optimal exploration and exploitation can be learned end-to-end by simply
maximizing task performance. However, such meta-RL approaches struggle with
local optima due to a chicken-and-egg problem: learning to explore requires
good exploitation to gauge the exploration's utility, but learning to exploit
requires information gathered via exploration. Optimizing separate objectives
for exploration and exploitation can avoid this problem, but prior meta-RL
exploration objectives yield suboptimal policies that gather information
irrelevant to the task. We alleviate both concerns by constructing an
exploitation objective that automatically identifies task-relevant information
and an exploration objective to recover only this information. This avoids
local optima in end-to-end training, without sacrificing optimal exploration.
Empirically, DREAM substantially outperforms existing approaches on complex
meta-RL problems, such as sparse-reward 3D visual navigation. Videos of DREAM:
https://ezliu.github.io/dream/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_E/0/1/0/all/0/1"&gt;Evan Zheran Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1"&gt;Aditi Raghunathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Percy Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1"&gt;Chelsea Finn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Universal Approximation for Log-concave Distributions using Well-conditioned Normalizing Flows. (arXiv:2107.02951v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.02951</id>
        <link href="http://arxiv.org/abs/2107.02951"/>
        <updated>2021-07-08T01:58:00.183Z</updated>
        <summary type="html"><![CDATA[Normalizing flows are a widely used class of latent-variable generative
models with a tractable likelihood. Affine-coupling (Dinh et al, 2014-16)
models are a particularly common type of normalizing flows, for which the
Jacobian of the latent-to-observable-variable transformation is triangular,
allowing the likelihood to be computed in linear time. Despite the widespread
usage of affine couplings, the special structure of the architecture makes
understanding their representational power challenging. The question of
universal approximation was only recently resolved by three parallel papers
(Huang et al.,2020;Zhang et al.,2020;Koehler et al.,2020) -- who showed
reasonably regular distributions can be approximated arbitrarily well using
affine couplings -- albeit with networks with a nearly-singular Jacobian. As
ill-conditioned Jacobians are an obstacle for likelihood-based training, the
fundamental question remains: which distributions can be approximated using
well-conditioned affine coupling flows?

In this paper, we show that any log-concave distribution can be approximated
using well-conditioned affine-coupling flows. In terms of proof techniques, we
uncover and leverage deep connections between affine coupling architectures,
underdamped Langevin dynamics (a stochastic differential equation often used to
sample from Gibbs measures) and H\'enon maps (a structured dynamical system
that appears in the study of symplectic diffeomorphisms). Our results also
inform the practice of training affine couplings: we approximate a padded
version of the input distribution with iid Gaussians -- a strategy which
Koehler et al.(2020) empirically observed to result in better-conditioned
flows, but had hitherto no theoretical grounding. Our proof can thus be seen as
providing theoretical evidence for the benefits of Gaussian padding when
training normalizing flows.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Holden Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pabbaraju_C/0/1/0/all/0/1"&gt;Chirag Pabbaraju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sevekari_A/0/1/0/all/0/1"&gt;Anish Sevekari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Risteski_A/0/1/0/all/0/1"&gt;Andrej Risteski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust learning under clean-label attack. (arXiv:2103.00671v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00671</id>
        <link href="http://arxiv.org/abs/2103.00671"/>
        <updated>2021-07-08T01:58:00.012Z</updated>
        <summary type="html"><![CDATA[We study the problem of robust learning under clean-label data-poisoning
attacks, where the attacker injects (an arbitrary set of) correctly-labeled
examples to the training set to fool the algorithm into making mistakes on
specific test instances at test time. The learning goal is to minimize the
attackable rate (the probability mass of attackable test instances), which is
more difficult than optimal PAC learning. As we show, any robust algorithm with
diminishing attackable rate can achieve the optimal dependence on $\epsilon$ in
its PAC sample complexity, i.e., $O(1/\epsilon)$. On the other hand, the
attackable rate might be large even for some optimal PAC learners, e.g., SVM
for linear classifiers. Furthermore, we show that the class of linear
hypotheses is not robustly learnable when the data distribution has zero margin
and is robustly learnable in the case of positive margin but requires sample
complexity exponential in the dimension. For a general hypothesis class with
bounded VC dimension, if the attacker is limited to add at most $t>0$ poison
examples, the optimal robust learning sample complexity grows almost linearly
with $t$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blum_A/0/1/0/all/0/1"&gt;Avrim Blum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanneke_S/0/1/0/all/0/1"&gt;Steve Hanneke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1"&gt;Jian Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1"&gt;Han Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SoundStream: An End-to-End Neural Audio Codec. (arXiv:2107.03312v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.03312</id>
        <link href="http://arxiv.org/abs/2107.03312"/>
        <updated>2021-07-08T01:58:00.005Z</updated>
        <summary type="html"><![CDATA[We present SoundStream, a novel neural audio codec that can efficiently
compress speech, music and general audio at bitrates normally targeted by
speech-tailored codecs. SoundStream relies on a model architecture composed by
a fully convolutional encoder/decoder network and a residual vector quantizer,
which are trained jointly end-to-end. Training leverages recent advances in
text-to-speech and speech enhancement, which combine adversarial and
reconstruction losses to allow the generation of high-quality audio content
from quantized embeddings. By training with structured dropout applied to
quantizer layers, a single model can operate across variable bitrates from
3kbps to 18kbps, with a negligible quality loss when compared with models
trained at fixed bitrates. In addition, the model is amenable to a low latency
implementation, which supports streamable inference and runs in real time on a
smartphone CPU. In subjective evaluations using audio at 24kHz sampling rate,
SoundStream at 3kbps outperforms Opus at 12kbps and approaches EVS at 9.6kbps.
Moreover, we are able to perform joint compression and enhancement either at
the encoder or at the decoder side with no additional latency, which we
demonstrate through background noise suppression for speech.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeghidour_N/0/1/0/all/0/1"&gt;Neil Zeghidour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luebs_A/0/1/0/all/0/1"&gt;Alejandro Luebs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Omran_A/0/1/0/all/0/1"&gt;Ahmed Omran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Skoglund_J/0/1/0/all/0/1"&gt;Jan Skoglund&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tagliasacchi_M/0/1/0/all/0/1"&gt;Marco Tagliasacchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic semi-nonnegative matrix factorization: a Skellam-based framework. (arXiv:2107.03317v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03317</id>
        <link href="http://arxiv.org/abs/2107.03317"/>
        <updated>2021-07-08T01:57:59.999Z</updated>
        <summary type="html"><![CDATA[We present a new probabilistic model to address semi-nonnegative matrix
factorization (SNMF), called Skellam-SNMF. It is a hierarchical generative
model consisting of prior components, Skellam-distributed hidden variables and
observed data. Two inference algorithms are derived: Expectation-Maximization
(EM) algorithm for maximum \emph{a posteriori} estimation and Variational Bayes
EM (VBEM) for full Bayesian inference, including the estimation of parameters
prior distribution. From this Skellam-based model, we also introduce a new
divergence $\mathcal{D}$ between a real-valued target data $x$ and two
nonnegative parameters $\lambda_{0}$ and $\lambda_{1}$ such that
$\mathcal{D}\left(x\mid\lambda_{0},\lambda_{1}\right)=0\Leftrightarrow
x=\lambda_{0}-\lambda_{1}$, which is a generalization of the Kullback-Leibler
(KL) divergence. Finally, we conduct experimental studies on those new
algorithms in order to understand their behavior and prove that they can
outperform the classic SNMF approach on real data in a task of automatic
clustering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fuentes_B/0/1/0/all/0/1"&gt;Benoit Fuentes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richard_G/0/1/0/all/0/1"&gt;Ga&amp;#xeb;l Richard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lossy Compression for Lossless Prediction. (arXiv:2106.10800v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10800</id>
        <link href="http://arxiv.org/abs/2106.10800"/>
        <updated>2021-07-08T01:57:59.980Z</updated>
        <summary type="html"><![CDATA[Most data is automatically collected and only ever "seen" by algorithms. Yet,
data compressors preserve perceptual fidelity rather than just the information
needed by algorithms performing downstream tasks. In this paper, we
characterize the bit-rate required to ensure high performance on all predictive
tasks that are invariant under a set of transformations, such as data
augmentations. Based on our theory, we design unsupervised objectives for
training neural compressors. Using these objectives, we train a generic image
compressor that achieves substantial rate savings (more than $1000\times$ on
ImageNet) compared to JPEG on 8 datasets, without decreasing downstream
classification performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dubois_Y/0/1/0/all/0/1"&gt;Yann Dubois&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bloem_Reddy_B/0/1/0/all/0/1"&gt;Benjamin Bloem-Reddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullrich_K/0/1/0/all/0/1"&gt;Karen Ullrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maddison_C/0/1/0/all/0/1"&gt;Chris J. Maddison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Monocular Depth Estimation via Listwise Ranking using the Plackett-Luce Model. (arXiv:2010.13118v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.13118</id>
        <link href="http://arxiv.org/abs/2010.13118"/>
        <updated>2021-07-08T01:57:59.974Z</updated>
        <summary type="html"><![CDATA[In many real-world applications, the relative depth of objects in an image is
crucial for scene understanding. Recent approaches mainly tackle the problem of
depth prediction in monocular images by treating the problem as a regression
task. Yet, being interested in an order relation in the first place, ranking
methods suggest themselves as a natural alternative to regression, and indeed,
ranking approaches leveraging pairwise comparisons as training information
("object A is closer to the camera than B") have shown promising performance on
this problem. In this paper, we elaborate on the use of so-called listwise
ranking as a generalization of the pairwise approach. Our method is based on
the Plackett-Luce (PL) model, a probability distribution on rankings, which we
combine with a state-of-the-art neural network architecture and a simple
sampling strategy to reduce training complexity. Moreover, taking advantage of
the representation of PL as a random utility model, the proposed predictor
offers a natural way to recover (shift-invariant) metric depth information from
ranking-only data provided at training time. An empirical evaluation on several
benchmark datasets in a "zero-shot" setting demonstrates the effectiveness of
our approach compared to existing ranking and regression methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lienen_J/0/1/0/all/0/1"&gt;Julian Lienen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1"&gt;Eyke H&amp;#xfc;llermeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1"&gt;Ralph Ewerth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nommensen_N/0/1/0/all/0/1"&gt;Nils Nommensen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simulated Data Generation Through Algorithmic Force Coefficient Estimation for AI-Based Robotic Projectile Launch Modeling. (arXiv:2105.12833v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12833</id>
        <link href="http://arxiv.org/abs/2105.12833"/>
        <updated>2021-07-08T01:57:59.967Z</updated>
        <summary type="html"><![CDATA[Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1"&gt;Sajiv Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haque_A/0/1/0/all/0/1"&gt;Ayaan Haque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Locally Private Graph Neural Networks. (arXiv:2006.05535v9 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05535</id>
        <link href="http://arxiv.org/abs/2006.05535"/>
        <updated>2021-07-08T01:57:59.961Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) have demonstrated superior performance in
learning node representations for various graph inference tasks. However,
learning over graph data can raise privacy concerns when nodes represent people
or human-related variables that involve sensitive or personal information.
While numerous techniques have been proposed for privacy-preserving deep
learning over non-relational data, there is less work addressing the privacy
issues pertained to applying deep learning algorithms on graphs. In this paper,
we study the problem of node data privacy, where graph nodes have potentially
sensitive data that is kept private, but they could be beneficial for a central
server for training a GNN over the graph. To address this problem, we develop a
privacy-preserving, architecture-agnostic GNN learning algorithm with formal
privacy guarantees based on Local Differential Privacy (LDP). Specifically, we
propose an LDP encoder and an unbiased rectifier, by which the server can
communicate with the graph nodes to privately collect their data and
approximate the GNN's first layer. To further reduce the effect of the injected
noise, we propose to prepend a simple graph convolution layer, called KProp,
which is based on the multi-hop aggregation of the nodes' features acting as a
denoising mechanism. Finally, we propose a robust training framework, in which
we benefit from KProp's denoising capability to increase the accuracy of
inference in the presence of noisy labels. Extensive experiments conducted over
real-world datasets demonstrate that our method can maintain a satisfying level
of accuracy with low privacy loss.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sajadmanesh_S/0/1/0/all/0/1"&gt;Sina Sajadmanesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gatica_Perez_D/0/1/0/all/0/1"&gt;Daniel Gatica-Perez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comparative Study of Using Spatial-Temporal Graph Convolutional Networks for Predicting Availability in Bike Sharing Schemes. (arXiv:2104.10644v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10644</id>
        <link href="http://arxiv.org/abs/2104.10644"/>
        <updated>2021-07-08T01:57:59.952Z</updated>
        <summary type="html"><![CDATA[Accurately forecasting transportation demand is crucial for efficient urban
traffic guidance, control and management. One solution to enhance the level of
prediction accuracy is to leverage graph convolutional networks (GCN), a neural
network based modelling approach with the ability to process data contained in
graph based structures. As a powerful extension of GCN, a spatial-temporal
graph convolutional network (ST-GCN) aims to capture the relationship of data
contained in the graphical nodes across both spatial and temporal dimensions,
which presents a novel deep learning paradigm for the analysis of complex
time-series data that also involves spatial information as present in
transportation use cases. In this paper, we present an Attention-based ST-GCN
(AST-GCN) for predicting the number of available bikes in bike-sharing systems
in cities, where the attention-based mechanism is introduced to further improve
the performance of an ST-GCN. Furthermore, we also discuss the impacts of
different modelling methods of adjacency matrices on the proposed architecture.
Our experimental results are presented using two real-world datasets,
Dublinbikes and NYC-Citi Bike, to illustrate the efficacy of our proposed model
which outperforms the majority of existing approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhengyong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hongde Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1"&gt;Noel E. O&amp;#x27;Connor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mingming Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Convergence of Overlapping Schwarz Decomposition for Nonlinear Optimal Control. (arXiv:2005.06674v4 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.06674</id>
        <link href="http://arxiv.org/abs/2005.06674"/>
        <updated>2021-07-08T01:57:59.934Z</updated>
        <summary type="html"><![CDATA[We study the convergence properties of an overlapping Schwarz
decomposition~algorithm for solving nonlinear optimal control problems (OCPs).
The approach decomposes the time domain into a set of overlapping subdomains,
and solves subproblems defined over such subdomains in parallel. Convergence is
attained by updating primal-dual information at the boundaries of the
overlapping regions. We show that the algorithm exhibits local linear
convergence and that the convergence rate improves exponentially with the
overlap size. Our convergence results rely on a sensitivity result for OCPs
that we call "exponential decay of sensitivity" (EDS). Intuitively, EDS states
that the impact of parametric perturbations at the boundaries of the domain
(initial and final time) decays exponentially as one moves into the domain. We
show that EDS holds for nonlinear OCPs under a uniform second-order sufficient
condition, a controllability condition, and a uniform boundedness condition. We
conduct numerical experiments using a quadrotor motion planning problem and a
PDE control problem; and show that the approach is significantly more efficient
than ADMM and as efficient as the centralized solver Ipopt.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Na_S/0/1/0/all/0/1"&gt;Sen Na&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Shin_S/0/1/0/all/0/1"&gt;Sungho Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Anitescu_M/0/1/0/all/0/1"&gt;Mihai Anitescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zavala_V/0/1/0/all/0/1"&gt;Victor M. Zavala&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Faults in Deep Reinforcement Learning Programs: A Taxonomy and A Detection Approach. (arXiv:2101.00135v2 [cs.SE] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2101.00135</id>
        <link href="http://arxiv.org/abs/2101.00135"/>
        <updated>2021-07-08T01:57:59.927Z</updated>
        <summary type="html"><![CDATA[A growing demand is witnessed in both industry and academia for employing
Deep Learning (DL) in various domains to solve real-world problems. Deep
Reinforcement Learning (DRL) is the application of DL in the domain of
Reinforcement Learning (RL). Like any software systems, DRL applications can
fail because of faults in their programs. In this paper, we present the first
attempt to categorize faults occurring in DRL programs. We manually analyzed
761 artifacts of DRL programs (from Stack Overflow posts and GitHub issues)
developed using well-known DRL frameworks (OpenAI Gym, Dopamine, Keras-rl,
Tensorforce) and identified faults reported by developers/users. We labeled and
taxonomized the identified faults through several rounds of discussions. The
resulting taxonomy is validated using an online survey with 19
developers/researchers. To allow for the automatic detection of faults in DRL
programs, we have defined a meta-model of DRL programs and developed DRLinter,
a model-based fault detection approach that leverages static analysis and graph
transformations. The execution flow of DRLinter consists in parsing a DRL
program to generate a model conforming to our meta-model and applying detection
rules on the model to identify faults occurrences. The effectiveness of
DRLinter is evaluated using 15 synthetic DRLprograms in which we injected
faults observed in the analyzed artifacts of the taxonomy. The results show
that DRLinter can successfully detect faults in all synthetic faulty programs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nikanjam_A/0/1/0/all/0/1"&gt;Amin Nikanjam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morovati_M/0/1/0/all/0/1"&gt;Mohammad Mehdi Morovati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1"&gt;Foutse Khomh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Braiek_H/0/1/0/all/0/1"&gt;Houssem Ben Braiek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Test for non-negligible adverse shifts. (arXiv:2107.02990v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.02990</id>
        <link href="http://arxiv.org/abs/2107.02990"/>
        <updated>2021-07-08T01:57:59.920Z</updated>
        <summary type="html"><![CDATA[Statistical tests for dataset shift are susceptible to false alarms: they are
sensitive to minor differences where there is in fact adequate sample coverage
and predictive performance. We propose instead a robust framework for tests of
dataset shift based on outlier scores, D-SOS for short. D-SOS detects adverse
shifts and can identify false alarms caused by benign ones. It posits that a
new (test) sample is not substantively worse than an old (training) sample, and
not that the two are equal. The key idea is to reduce observations to outlier
scores and compare contamination rates. Beyond comparing distributions, users
can define what worse means in terms of predictive performance and other
relevant notions. We show how versatile and practical D-SOS is for a wide range
of real and simulated datasets. Unlike tests of equal distribution and of
goodness-of-fit, the D-SOS tests are uniquely tailored to serve as robust
performance metrics to monitor model drift and dataset shift.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kamulete_V/0/1/0/all/0/1"&gt;Vathy M. Kamulete&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RoFL: Attestable Robustness for Secure Federated Learning. (arXiv:2107.03311v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.03311</id>
        <link href="http://arxiv.org/abs/2107.03311"/>
        <updated>2021-07-08T01:57:59.914Z</updated>
        <summary type="html"><![CDATA[Federated Learning is an emerging decentralized machine learning paradigm
that allows a large number of clients to train a joint model without the need
to share their private data. Participants instead only share ephemeral updates
necessary to train the model. To ensure the confidentiality of the client
updates, Federated Learning systems employ secure aggregation; clients encrypt
their gradient updates, and only the aggregated model is revealed to the
server. Achieving this level of data protection, however, presents new
challenges to the robustness of Federated Learning, i.e., the ability to
tolerate failures and attacks. Unfortunately, in this setting, a malicious
client can now easily exert influence on the model behavior without being
detected. As Federated Learning is being deployed in practice in a range of
sensitive applications, its robustness is growing in importance. In this paper,
we take a step towards understanding and improving the robustness of secure
Federated Learning. We start this paper with a systematic study that evaluates
and analyzes existing attack vectors and discusses potential defenses and
assesses their effectiveness. We then present RoFL, a secure Federated Learning
system that improves robustness against malicious clients through input checks
on the encrypted model updates. RoFL extends Federated Learning's secure
aggregation protocol to allow expressing a variety of properties and
constraints on model updates using zero-knowledge proofs. To enable RoFL to
scale to typical Federated Learning settings, we introduce several ML and
cryptographic optimizations specific to Federated Learning. We implement and
evaluate a prototype of RoFL and show that realistic ML models can be trained
in a reasonable time while improving robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Burkhalter_L/0/1/0/all/0/1"&gt;Lukas Burkhalter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nijeholt_H/0/1/0/all/0/1"&gt;Hidde Lycklama &amp;#xe0; Nijeholt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viand_A/0/1/0/all/0/1"&gt;Alexander Viand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuchler_N/0/1/0/all/0/1"&gt;Nicolas K&amp;#xfc;chler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hithnawi_A/0/1/0/all/0/1"&gt;Anwar Hithnawi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A data-driven approach to the forecasting of ground-level ozone concentration. (arXiv:2012.00685v4 [physics.ao-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00685</id>
        <link href="http://arxiv.org/abs/2012.00685"/>
        <updated>2021-07-08T01:57:59.907Z</updated>
        <summary type="html"><![CDATA[The ability to forecast the concentration of air pollutants in an urban
region is crucial for decision-makers wishing to reduce the impact of pollution
on public health through active measures (e.g. temporary traffic closures). In
this study, we present a machine learning approach applied to the forecast of
the day-ahead maximum value of the ozone concentration for several geographical
locations in southern Switzerland. Due to the low density of measurement
stations and to the complex orography of the use case terrain, we adopted
feature selection methods instead of explicitly restricting relevant features
to a neighbourhood of the prediction sites, as common in spatio-temporal
forecasting methods. We then used Shapley values to assess the explainability
of the learned models in terms of feature importance and feature interactions
in relation to ozone predictions; our analysis suggests that the trained models
effectively learned explanatory cross-dependencies among atmospheric variables.
Finally, we show how weighting observations helps in increasing the accuracy of
the forecasts for specific ranges of ozone's daily peak values.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Marvin_D/0/1/0/all/0/1"&gt;Dario Marvin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Nespoli_L/0/1/0/all/0/1"&gt;Lorenzo Nespoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Strepparava_D/0/1/0/all/0/1"&gt;Davide Strepparava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Medici_V/0/1/0/all/0/1"&gt;Vasco Medici&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MD-split+: Practical Local Conformal Inference in High Dimensions. (arXiv:2107.03280v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.03280</id>
        <link href="http://arxiv.org/abs/2107.03280"/>
        <updated>2021-07-08T01:57:59.887Z</updated>
        <summary type="html"><![CDATA[Quantifying uncertainty in model predictions is a common goal for
practitioners seeking more than just point predictions. One tool for
uncertainty quantification that requires minimal assumptions is conformal
inference, which can help create probabilistically valid prediction regions for
black box models. Classical conformal prediction only provides marginal
validity, whereas in many situations locally valid prediction regions are
desirable. Deciding how best to partition the feature space X when applying
localized conformal prediction is still an open question. We present MD-split+,
a practical local conformal approach that creates X partitions based on
localized model performance of conditional density estimation models. Our
method handles complex real-world data settings where such models may be
misspecified, and scales to high-dimensional inputs. We discuss how our local
partitions philosophically align with expected behavior from an unattainable
conditional conformal inference approach. We also empirically compare our
method against other local conformal approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+LeRoy_B/0/1/0/all/0/1"&gt;Benjamin LeRoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhao_D/0/1/0/all/0/1"&gt;David Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A comparative study of various Deep Learning techniques for spatio-temporal Super-Resolution reconstruction of Forced Isotropic Turbulent flows. (arXiv:2107.03361v1 [physics.flu-dyn])]]></title>
        <id>http://arxiv.org/abs/2107.03361</id>
        <link href="http://arxiv.org/abs/2107.03361"/>
        <updated>2021-07-08T01:57:59.880Z</updated>
        <summary type="html"><![CDATA[Super-resolution is an innovative technique that upscales the resolution of
an image or a video and thus enables us to reconstruct high-fidelity images
from low-resolution data. This study performs super-resolution analysis on
turbulent flow fields spatially and temporally using various state-of-the-art
machine learning techniques like ESPCN, ESRGAN and TecoGAN to reconstruct
high-resolution flow fields from low-resolution flow field data, especially
keeping in mind the need for low resource consumption and rapid results
production/verification. The dataset used for this study is extracted from the
'isotropic 1024 coarse' dataset which is a part of Johns Hopkins Turbulence
Databases (JHTDB). We have utilized pre-trained models and fine tuned them to
our needs, so as to minimize the computational resources and the time required
for the implementation of the super-resolution models. The advantages presented
by this method far exceed the expectations and the outcomes of regular single
structure models. The results obtained through these models are then compared
using MSE, PSNR, SAM, VIF and SCC metrics in order to evaluate the upscaled
results, find the balance between computational power and output quality, and
then identify the most accurate and efficient model for spatial and temporal
super-resolution of turbulent flow fields.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Venkatesh_T/0/1/0/all/0/1"&gt;T.S.Sachin Venkatesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Srivastava_R/0/1/0/all/0/1"&gt;Rajat Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Bhatt_P/0/1/0/all/0/1"&gt;Pratyush Bhatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Tyagi_P/0/1/0/all/0/1"&gt;Prince Tyagi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Singh_R/0/1/0/all/0/1"&gt;Raj Kumar Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Dynamic Quantization with Mixed Precision and Adaptive Resolution. (arXiv:2106.02295v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02295</id>
        <link href="http://arxiv.org/abs/2106.02295"/>
        <updated>2021-07-08T01:57:59.874Z</updated>
        <summary type="html"><![CDATA[Model quantization is challenging due to many tedious hyper-parameters such
as precision (bitwidth), dynamic range (minimum and maximum discrete values)
and stepsize (interval between discrete values). Unlike prior arts that
carefully tune these values, we present a fully differentiable approach to
learn all of them, named Differentiable Dynamic Quantization (DDQ), which has
several benefits. (1) DDQ is able to quantize challenging lightweight
architectures like MobileNets, where different layers prefer different
quantization parameters. (2) DDQ is hardware-friendly and can be easily
implemented using low-precision matrix-vector multiplication, making it capable
in many hardware such as ARM. (3) Extensive experiments show that DDQ
outperforms prior arts on many networks and benchmarks, especially when models
are already efficient and compact. e.g., DDQ is the first approach that
achieves lossless 4-bit quantization for MobileNetV2 on ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhaoyang_Z/0/1/0/all/0/1"&gt;Zhang Zhaoyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wenqi_S/0/1/0/all/0/1"&gt;Shao Wenqi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jinwei_G/0/1/0/all/0/1"&gt;Gu Jinwei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiaogang_W/0/1/0/all/0/1"&gt;Wang Xiaogang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ping_L/0/1/0/all/0/1"&gt;Luo Ping&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GRIMGEP: Learning Progress for Robust Goal Sampling in Visual Deep Reinforcement Learning. (arXiv:2008.04388v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.04388</id>
        <link href="http://arxiv.org/abs/2008.04388"/>
        <updated>2021-07-08T01:57:59.867Z</updated>
        <summary type="html"><![CDATA[Designing agents, capable of learning autonomously a wide range of skills is
critical in order to increase the scope of reinforcement learning. It will both
increase the diversity of learned skills and reduce the burden of manually
designing reward functions for each skill. Self-supervised agents, setting
their own goals, and trying to maximize the diversity of those goals have shown
great promise towards this end. However, a currently known limitation of agents
trying to maximize the diversity of sampled goals is that they tend to get
attracted to noise or more generally to parts of the environments that cannot
be controlled (distractors). When agents have access to predefined goal
features or expert knowledge, absolute Learning Progress (ALP) provides a way
to distinguish between regions that can be controlled and those that cannot.
However, those methods often fall short when the agents are only provided with
raw sensory inputs such as images. In this work we extend those concepts to
unsupervised image-based goal exploration. We propose a framework that allows
agents to autonomously identify and ignore noisy distracting regions while
searching for novelty in the learnable regions to both improve overall
performance and avoid catastrophic forgetting. Our framework can be combined
with any state-of-the-art novelty seeking goal exploration approaches. We
construct a rich 3D image based environment with distractors. Experiments on
this environment show that agents using our framework successfully identify
interesting regions of the environment, resulting in drastically improved
performances. The source code is available at
https://sites.google.com/view/grimgep.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kovac_G/0/1/0/all/0/1"&gt;Grgur Kova&amp;#x10d;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laversanne_Finot_A/0/1/0/all/0/1"&gt;Adrien Laversanne-Finot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1"&gt;Pierre-Yves Oudeyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Samplets: A new paradigm for data compression. (arXiv:2107.03337v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2107.03337</id>
        <link href="http://arxiv.org/abs/2107.03337"/>
        <updated>2021-07-08T01:57:59.857Z</updated>
        <summary type="html"><![CDATA[In this article, we introduce the novel concept of samplets by transferring
the construction of Tausch-White wavelets to the realm of data. This way we
obtain a multilevel representation of discrete data which directly enables data
compression, detection of singularities and adaptivity. Applying samplets to
represent kernel matrices, as they arise in kernel based learning or Gaussian
process regression, we end up with quasi-sparse matrices. By thresholding small
entries, these matrices are compressible to O(N log N) relevant entries, where
N is the number of data points. This feature allows for the use of fill-in
reducing reorderings to obtain a sparse factorization of the compressed
matrices. Besides the comprehensive introduction to samplets and their
properties, we present extensive numerical studies to benchmark the approach.
Our results demonstrate that samplets mark a considerable step in the direction
of making large data sets accessible for analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Harbrecht_H/0/1/0/all/0/1"&gt;Helmut Harbrecht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Multerer_M/0/1/0/all/0/1"&gt;Michael Multerer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UAV-assisted Online Machine Learning over Multi-Tiered Networks: A Hierarchical Nested Personalized Federated Learning Approach. (arXiv:2106.15734v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15734</id>
        <link href="http://arxiv.org/abs/2106.15734"/>
        <updated>2021-07-08T01:57:59.838Z</updated>
        <summary type="html"><![CDATA[We consider distributed machine learning (ML) through unmanned aerial
vehicles (UAVs) for geo-distributed device clusters. We propose five new
technologies/techniques: (i) stratified UAV swarms with leader, worker, and
coordinator UAVs, (ii) hierarchical nested personalized federated learning
(HN-PFL): a holistic distributed ML framework for personalized model training
across the worker-leader-core network hierarchy, (iii) cooperative UAV resource
pooling for distributed ML using the UAVs' local computational capabilities,
(iv) aerial data caching and relaying for efficient data relaying to conduct
ML, and (v) concept/model drift, capturing online data variations at the
devices. We split the UAV-enabled model training problem as two parts. (a)
Network-aware HN-PFL, where we optimize a tradeoff between energy consumption
and ML model performance by configuring data offloading among devices-UAVs and
UAV-UAVs, UAVs' CPU frequencies, and mini-batch sizes subject to
communication/computation network heterogeneity. We tackle this optimization
problem via the method of posynomial condensation and propose a distributed
algorithm with a performance guarantee. (b) Macro-trajectory and learning
duration design, which we formulate as a sequential decision making problem,
tackled via deep reinforcement learning. Our simulations demonstrate the
superiority of our methodology with regards to the distributed ML performance,
the optimization of network resources, and the swarm trajectory efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Su Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hosseinalipour_S/0/1/0/all/0/1"&gt;Seyyedali Hosseinalipour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gorlatova_M/0/1/0/all/0/1"&gt;Maria Gorlatova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brinton_C/0/1/0/all/0/1"&gt;Christopher G. Brinton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiang_M/0/1/0/all/0/1"&gt;Mung Chiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Jitter: Random Jittering Loss Function. (arXiv:2106.13749v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13749</id>
        <link href="http://arxiv.org/abs/2106.13749"/>
        <updated>2021-07-08T01:57:59.831Z</updated>
        <summary type="html"><![CDATA[Regularization plays a vital role in machine learning optimization. One novel
regularization method called flooding makes the training loss fluctuate around
the flooding level. It intends to make the model continue to random walk until
it comes to a flat loss landscape to enhance generalization. However, the
hyper-parameter flooding level of the flooding method fails to be selected
properly and uniformly. We propose a novel method called Jitter to improve it.
Jitter is essentially a kind of random loss function. Before training, we
randomly sample the Jitter Point from a specific probability distribution. The
flooding level should be replaced by Jitter point to obtain a new target
function and train the model accordingly. As Jitter point acting as a random
factor, we actually add some randomness to the loss function, which is
consistent with the fact that there exists innumerable random behaviors in the
learning process of the machine learning model and is supposed to make the
model more robust. In addition, Jitter performs random walk randomly which
divides the loss curve into small intervals and then flipping them over,
ideally making the loss curve much flatter and enhancing generalization
ability. Moreover, Jitter can be a domain-, task-, and model-independent
regularization method and train the model effectively after the training error
reduces to zero. Our experimental results show that Jitter method can improve
model performance more significantly than the previous flooding method and make
the test loss curve descend twice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhicheng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1"&gt;Chenglei Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1"&gt;Sidan Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Pseudo-Metric between Probability Distributions based on Depth-Trimmed Regions. (arXiv:2103.12711v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12711</id>
        <link href="http://arxiv.org/abs/2103.12711"/>
        <updated>2021-07-08T01:57:59.825Z</updated>
        <summary type="html"><![CDATA[Data depth is a non parametric statistical tool that measures centrality of
any element $x\in\mathbb{R}^d$ with respect to (w.r.t.) a probability
distribution or a data set. It is a natural median-oriented extension of the
cumulative distribution function (cdf) to the multivariate case. Consequently,
its upper level sets -- the depth-trimmed regions -- give rise to a definition
of multivariate quantiles. In this work, we propose two new pseudo-metrics
between continuous probability measures based on data depth and its associated
central regions. The first one is constructed as the Lp-distance between data
depth w.r.t. each distribution while the second one relies on the Hausdorff
distance between their quantile regions. It can further be seen as an original
way to extend the one-dimensional formulae of the Wasserstein distance, which
involves quantiles and cdfs, to the multivariate space. After discussing the
properties of these pseudo-metrics and providing conditions under which they
define a distance, we highlight similarities with the Wasserstein distance.
Interestingly, the derived non-asymptotic bounds show that in contrast to the
Wasserstein distance, the proposed pseudo-metrics do not suffer from the curse
of dimensionality. Moreover, based on the support function of a convex body, we
propose an efficient approximation possessing linear time complexity w.r.t. the
size of the data set and its dimension. The quality of this approximation as
well as the performance of the proposed approach are illustrated in
experiments. Furthermore, by construction the regions-based pseudo-metric
appears to be robust w.r.t. both outliers and heavy tails, a behavior witnessed
in the numerical experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Staerman_G/0/1/0/all/0/1"&gt;Guillaume Staerman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mozharovskyi_P/0/1/0/all/0/1"&gt;Pavlo Mozharovskyi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Clemencon_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phan Cl&amp;#xe9;men&amp;#xe7;on&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+dAlche_Buc_F/0/1/0/all/0/1"&gt;Florence d&amp;#x27;Alch&amp;#xe9;-Buc&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How and Why to Use Experimental Data to Evaluate Methods for Observational Causal Inference. (arXiv:2010.03051v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03051</id>
        <link href="http://arxiv.org/abs/2010.03051"/>
        <updated>2021-07-08T01:57:59.817Z</updated>
        <summary type="html"><![CDATA[Methods that infer causal dependence from observational data are central to
many areas of science, including medicine, economics, and the social sciences.
A variety of theoretical properties of these methods have been proven, but
empirical evaluation remains a challenge, largely due to the lack of
observational data sets for which treatment effect is known. We describe and
analyze observational sampling from randomized controlled trials (OSRCT), a
method for evaluating causal inference methods using data from randomized
controlled trials (RCTs). This method can be used to create constructed
observational data sets with corresponding unbiased estimates of treatment
effect, substantially increasing the number of data sets available for
empirical evaluation of causal inference methods. We show that, in expectation,
OSRCT creates data sets that are equivalent to those produced by randomly
sampling from empirical data sets in which all potential outcomes are
available. We then perform a large-scale evaluation of seven causal inference
methods over 37 data sets, drawn from RCTs, as well as simulators, real-world
computational systems, and observational data sets augmented with a synthetic
response variable. We find notable performance differences when comparing
across data from different sources, demonstrating the importance of using data
from a variety of sources when evaluating any causal inference method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Gentzel_A/0/1/0/all/0/1"&gt;Amanda Gentzel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pruthi_P/0/1/0/all/0/1"&gt;Purva Pruthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jensen_D/0/1/0/all/0/1"&gt;David Jensen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey of Uncertainty in Deep Neural Networks. (arXiv:2107.03342v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03342</id>
        <link href="http://arxiv.org/abs/2107.03342"/>
        <updated>2021-07-08T01:57:59.810Z</updated>
        <summary type="html"><![CDATA[Due to their increasing spread, confidence in neural network predictions
became more and more important. However, basic neural networks do not deliver
certainty estimates or suffer from over or under confidence. Many researchers
have been working on understanding and quantifying uncertainty in a neural
network's prediction. As a result, different types and sources of uncertainty
have been identified and a variety of approaches to measure and quantify
uncertainty in neural networks have been proposed. This work gives a
comprehensive overview of uncertainty estimation in neural networks, reviews
recent advances in the field, highlights current challenges, and identifies
potential research opportunities. It is intended to give anyone interested in
uncertainty estimation in neural networks a broad overview and introduction,
without presupposing prior knowledge in this field. A comprehensive
introduction to the most crucial sources of uncertainty is given and their
separation into reducible model uncertainty and not reducible data uncertainty
is presented. The modeling of these uncertainties based on deterministic neural
networks, Bayesian neural networks, ensemble of neural networks, and test-time
data augmentation approaches is introduced and different branches of these
fields as well as the latest developments are discussed. For a practical
application, we discuss different measures of uncertainty, approaches for the
calibration of neural networks and give an overview of existing baselines and
implementations. Different examples from the wide spectrum of challenges in
different fields give an idea of the needs and challenges regarding
uncertainties in practical applications. Additionally, the practical
limitations of current methods for mission- and safety-critical real world
applications are discussed and an outlook on the next steps towards a broader
usage of such methods is given.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gawlikowski_J/0/1/0/all/0/1"&gt;Jakob Gawlikowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tassi_C/0/1/0/all/0/1"&gt;Cedrique Rovile Njieutcheu Tassi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1"&gt;Mohsin Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jongseok Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Humt_M/0/1/0/all/0/1"&gt;Matthias Humt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jianxiang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kruspe_A/0/1/0/all/0/1"&gt;Anna Kruspe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Triebel_R/0/1/0/all/0/1"&gt;Rudolph Triebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_P/0/1/0/all/0/1"&gt;Peter Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roscher_R/0/1/0/all/0/1"&gt;Ribana Roscher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahzad_M/0/1/0/all/0/1"&gt;Muhammad Shahzad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bamler_R/0/1/0/all/0/1"&gt;Richard Bamler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiao Xiang Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifying Training Stop Point with Noisy Labeled Data. (arXiv:2012.13435v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13435</id>
        <link href="http://arxiv.org/abs/2012.13435"/>
        <updated>2021-07-08T01:57:59.790Z</updated>
        <summary type="html"><![CDATA[Training deep neural networks (DNNs) with noisy labels is a challenging
problem due to over-parameterization. DNNs tend to essentially fit on clean
samples at a higher rate in the initial stages, and later fit on the noisy
samples at a relatively lower rate. Thus, with a noisy dataset, the test
accuracy increases initially and drops in the later stages. To find an early
stopping point at the maximum obtainable test accuracy (MOTA), recent studies
assume either that i) a clean validation set is available or ii) the noise
ratio is known, or, both. However, often a clean validation set is unavailable,
and the noise estimation can be inaccurate. To overcome these issues, we
provide a novel training solution, free of these conditions. We analyze the
rate of change of the training accuracy for different noise ratios under
different conditions to identify a training stop region. We further develop a
heuristic algorithm based on a small-learning assumption to find a training
stop point (TSP) at or close to MOTA. To the best of our knowledge, our method
is the first to rely solely on the \textit{training behavior}, while utilizing
the entire training set, to automatically find a TSP. We validated the
robustness of our algorithm (AutoTSP) through several experiments on CIFAR-10,
CIFAR-100, and a real-world noisy dataset for different noise ratios, noise
types, and architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kamabattula_S/0/1/0/all/0/1"&gt;Sree Ram Kamabattula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Devarajan_V/0/1/0/all/0/1"&gt;Venkat Devarajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namazi_B/0/1/0/all/0/1"&gt;Babak Namazi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sankaranarayanan_G/0/1/0/all/0/1"&gt;Ganesh Sankaranarayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Convolution for Semi-Supervised Classification: Improved Linear Separability and Out-of-Distribution Generalization. (arXiv:2102.06966v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06966</id>
        <link href="http://arxiv.org/abs/2102.06966"/>
        <updated>2021-07-08T01:57:59.783Z</updated>
        <summary type="html"><![CDATA[Recently there has been increased interest in semi-supervised classification
in the presence of graphical information. A new class of learning models has
emerged that relies, at its most basic level, on classifying the data after
first applying a graph convolution. To understand the merits of this approach,
we study the classification of a mixture of Gaussians, where the data
corresponds to the node attributes of a stochastic block model. We show that
graph convolution extends the regime in which the data is linearly separable by
a factor of roughly $1/\sqrt{D}$, where $D$ is the expected degree of a node,
as compared to the mixture model data on its own. Furthermore, we find that the
linear classifier obtained by minimizing the cross-entropy loss after the graph
convolution generalizes to out-of-distribution data where the unseen data can
have different intra- and inter-class edge probabilities from the training
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baranwal_A/0/1/0/all/0/1"&gt;Aseem Baranwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fountoulakis_K/0/1/0/all/0/1"&gt;Kimon Fountoulakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jagannath_A/0/1/0/all/0/1"&gt;Aukosh Jagannath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AGD-Autoencoder: Attention Gated Deep Convolutional Autoencoder for Brain Tumor Segmentation. (arXiv:2107.03323v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.03323</id>
        <link href="http://arxiv.org/abs/2107.03323"/>
        <updated>2021-07-08T01:57:59.776Z</updated>
        <summary type="html"><![CDATA[Brain tumor segmentation is a challenging problem in medical image analysis.
The endpoint is to generate the salient masks that accurately identify brain
tumor regions in an fMRI screening. In this paper, we propose a novel attention
gate (AG model) for brain tumor segmentation that utilizes both the edge
detecting unit and the attention gated network to highlight and segment the
salient regions from fMRI images. This feature enables us to eliminate the
necessity of having to explicitly point towards the damaged area(external
tissue localization) and classify(classification) as per classical computer
vision techniques. AGs can easily be integrated within the deep convolutional
neural networks(CNNs). Minimal computional overhead is required while the AGs
increase the sensitivity scores significantly. We show that the edge detector
along with an attention gated mechanism provide a sufficient enough method for
brain segmentation reaching an IOU of 0.78]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Cvetko_T/0/1/0/all/0/1"&gt;Tim Cvetko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoDebias: Learning to Debias for Recommendation. (arXiv:2105.04170v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04170</id>
        <link href="http://arxiv.org/abs/2105.04170"/>
        <updated>2021-07-08T01:57:59.769Z</updated>
        <summary type="html"><![CDATA[Recommender systems rely on user behavior data like ratings and clicks to
build personalization model. However, the collected data is observational
rather than experimental, causing various biases in the data which
significantly affect the learned model. Most existing work for recommendation
debiasing, such as the inverse propensity scoring and imputation approaches,
focuses on one or two specific biases, lacking the universal capacity that can
account for mixed or even unknown biases in the data.

Towards this research gap, we first analyze the origin of biases from the
perspective of \textit{risk discrepancy} that represents the difference between
the expectation empirical risk and the true risk. Remarkably, we derive a
general learning framework that well summarizes most existing debiasing
strategies by specifying some parameters of the general framework. This
provides a valuable opportunity to develop a universal solution for debiasing,
e.g., by learning the debiasing parameters from data. However, the training
data lacks important signal of how the data is biased and what the unbiased
data looks like. To move this idea forward, we propose \textit{AotoDebias} that
leverages another (small) set of uniform data to optimize the debiasing
parameters by solving the bi-level optimization problem with meta-learning.
Through theoretical analyses, we derive the generalization bound for AutoDebias
and prove its ability to acquire the appropriate debiasing strategy. Extensive
experiments on two real datasets and a simulated dataset demonstrated
effectiveness of AutoDebias. The code is available at
\url{https://github.com/DongHande/AutoDebias}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiawei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Hande Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1"&gt;Yang Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangnan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1"&gt;Xin Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1"&gt;Guli Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Keping Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Dynamic Multi-Modal Data Fusion: A Model Uncertainty Perspective. (arXiv:2105.06018v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06018</id>
        <link href="http://arxiv.org/abs/2105.06018"/>
        <updated>2021-07-08T01:57:59.761Z</updated>
        <summary type="html"><![CDATA[This paper is concerned with multi-modal data fusion (MMDF) under unexpected
modality failures in nonlinear non-Gaussian dynamic processes. An efficient
framework to tackle this problem is proposed. In particular, a notion termed
modality "\emph{usefulness}", which takes a value of 1 or 0, is used for
indicating whether the observation of this modality is useful or not. For $n$
modalities involved, $2^n$ combinations of their "\emph{usefulness}" values
exist. Each combination defines one hypothetical model of the true data
generative process. Then the problem of concern is formalized as a task of
nonlinear non-Gaussian state filtering under model uncertainty, which is
addressed by a dynamic model averaging (DMA) based particle filter (PF)
algorithm. This DMA algorithm employs $2^n$ models, while all models share the
same state-transition function and a unique set of particle values. That makes
the computational complexity of this algorithm only slightly larger than a
single model based PF algorithm, especially for scenarios in which $n$ is
small. Experimental results show that the proposed solution outperforms
remarkably state-of-the-art methods. Code and data are available at
https://github.com/robinlau1981/fusion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bin Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing the Spatio-temporal Observability of Grid-Edge Resources in Distribution Grids. (arXiv:2102.07801v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07801</id>
        <link href="http://arxiv.org/abs/2102.07801"/>
        <updated>2021-07-08T01:57:59.742Z</updated>
        <summary type="html"><![CDATA[Enhancing the spatio-temporal observability of distributed energy resources
(DERs) is crucial for achieving secure and efficient operations in distribution
grids. This paper puts forth a joint recovery framework for residential loads
by leveraging the complimentary strengths of heterogeneous types of
measurements. The proposed approaches integrate the low-resolution smart meter
data collected for every load node with the fast-sampled feeder-level
measurements provided by limited number of phasor measurement units. To address
the lack of data, we exploit two key characteristics for the loads and DERs,
namely the sparse changes due to infrequent activities of appliances and
electric vehicles (EVs) and the locational dependence of solar photovoltaic
(PV) generation. Accordingly, meaningful regularization terms are introduced to
cast a convex load recovery problem, which will be further simplified to reduce
computational complexity. The load recovery solutions can be utilized to
identify the EV charging events at each load node and to infer the total
behind-the-meter PV output. Numerical tests using real-world data have
demonstrated the effectiveness of the proposed approaches in enhancing the
visibility of these grid-edge DERs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lin_S/0/1/0/all/0/1"&gt;Shanny Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hao Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[POLAR: A Polynomial Arithmetic Framework for Verifying Neural-Network Controlled Systems. (arXiv:2106.13867v3 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13867</id>
        <link href="http://arxiv.org/abs/2106.13867"/>
        <updated>2021-07-08T01:57:59.736Z</updated>
        <summary type="html"><![CDATA[We propose POLAR, a \textbf{pol}ynomial \textbf{ar}ithmetic framework that
leverages polynomial overapproximations with interval remainders for
bounded-time reachability analysis of neural network-controlled systems
(NNCSs). Compared with existing arithmetic approaches that use standard Taylor
models, our framework uses a novel approach to iteratively overapproximate the
neuron output ranges layer-by-layer with a combination of Bernstein polynomial
interpolation for continuous activation functions and Taylor model arithmetic
for the other operations. This approach can overcome the main drawback in the
standard Taylor model arithmetic, i.e. its inability to handle functions that
cannot be well approximated by Taylor polynomials, and significantly improve
the accuracy and efficiency of reachable states computation for NNCSs. To
further tighten the overapproximation, our method keeps the Taylor model
remainders symbolic under the linear mappings when estimating the output range
of a neural network. We show that POLAR can be seamlessly integrated with
existing Taylor model flowpipe construction techniques, and demonstrate that
POLAR significantly outperforms the current state-of-the-art techniques on a
suite of benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fan_J/0/1/0/all/0/1"&gt;Jiameng Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenchao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Qi Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Neural Nets in Chemical Engineering: Foundations, Computations, and Applications. (arXiv:2101.04869v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.04869</id>
        <link href="http://arxiv.org/abs/2101.04869"/>
        <updated>2021-07-08T01:57:59.728Z</updated>
        <summary type="html"><![CDATA[In this paper we review the mathematical foundations of convolutional neural
nets (CNNs) with the goals of: i) highlighting connections with techniques from
statistics, signal processing, linear algebra, differential equations, and
optimization, ii) demystifying underlying computations, and iii) identifying
new types of applications. CNNs are powerful machine learning models that
highlight features from grid data to make predictions (regression and
classification). The grid data object can be represented as vectors (in 1D),
matrices (in 2D), or tensors (in 3D or higher dimensions) and can incorporate
multiple channels (thus providing high flexibility in the input data
representation). CNNs highlight features from the grid data by performing
convolution operations with different types of operators. The operators
highlight different types of features (e.g., patterns, gradients, geometrical
features) and are learned by using optimization techniques. In other words,
CNNs seek to identify optimal operators that best map the input data to the
output data. A common misconception is that CNNs are only capable of processing
image or video data but their application scope is much wider; specifically,
datasets encountered in diverse applications can be expressed as grid data.
Here, we show how to apply CNNs to new types of applications such as optimal
control, flow cytometry, multivariate process monitoring, and molecular
simulations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Shengli Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zavala_V/0/1/0/all/0/1"&gt;Victor M. Zavala&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Up Exact Neural Network Compression by ReLU Stability. (arXiv:2102.07804v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07804</id>
        <link href="http://arxiv.org/abs/2102.07804"/>
        <updated>2021-07-08T01:57:59.720Z</updated>
        <summary type="html"><![CDATA[We can compress a neural network while exactly preserving its underlying
functionality with respect to a given input domain if some of its neurons are
stable. However, current approaches to determine the stability of neurons with
Rectified Linear Unit (ReLU) activations require solving or finding a good
approximation to multiple discrete optimization problems. In this work, we
introduce an algorithm based on solving a single optimization problem to
identify all stable neurons. Our approach is on median 100 times faster than
the state-of-art method, which allows us to explore exact compression on deeper
(5 x 100) and wider (2 x 800) networks within minutes. For classifiers trained
under an amount of L1 regularization that does not worsen accuracy, we can
remove up to 40% of the connections]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Serra_T/0/1/0/all/0/1"&gt;Thiago Serra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Abhinav Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramalingam_S/0/1/0/all/0/1"&gt;Srikumar Ramalingam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable Prediction of Text Complexity: The Missing Preliminaries for Text Simplification. (arXiv:2007.15823v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.15823</id>
        <link href="http://arxiv.org/abs/2007.15823"/>
        <updated>2021-07-08T01:57:59.713Z</updated>
        <summary type="html"><![CDATA[Text simplification reduces the language complexity of professional content
for accessibility purposes. End-to-end neural network models have been widely
adopted to directly generate the simplified version of input text, usually
functioning as a blackbox. We show that text simplification can be decomposed
into a compact pipeline of tasks to ensure the transparency and explainability
of the process. The first two steps in this pipeline are often neglected: 1) to
predict whether a given piece of text needs to be simplified, and 2) if yes, to
identify complex parts of the text. The two tasks can be solved separately
using either lexical or deep learning methods, or solved jointly. Simply
applying explainable complexity prediction as a preliminary step, the
out-of-sample text simplification performance of the state-of-the-art,
black-box simplification models can be improved by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garbacea_C/0/1/0/all/0/1"&gt;Cristina Garbacea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Mengtian Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carton_S/0/1/0/all/0/1"&gt;Samuel Carton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_Q/0/1/0/all/0/1"&gt;Qiaozhu Mei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Codomain Separability and Label Inference from (Noisy) Loss Functions. (arXiv:2107.03022v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03022</id>
        <link href="http://arxiv.org/abs/2107.03022"/>
        <updated>2021-07-08T01:57:59.694Z</updated>
        <summary type="html"><![CDATA[Machine learning classifiers rely on loss functions for performance
evaluation, often on a private (hidden) dataset. Label inference was recently
introduced as the problem of reconstructing the ground truth labels of this
private dataset from just the (possibly perturbed) loss function values
evaluated at chosen prediction vectors, without any other access to the hidden
dataset. Existing results have demonstrated this inference is possible on
specific loss functions like the cross-entropy loss. In this paper, we
introduce the notion of codomain separability to formally study the necessary
and sufficient conditions under which label inference is possible from any
(noisy) loss function values. Using this notion, we show that for many commonly
used loss functions, including multiclass cross-entropy with common activation
functions and some Bregman divergence-based losses, it is possible to design
label inference attacks for arbitrary noise levels. We demonstrate that these
attacks can also be carried out through actual neural network models, and
argue, both formally and empirically, the role of finite precision arithmetic
in this setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_A/0/1/0/all/0/1"&gt;Abhinav Aggarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasiviswanathan_S/0/1/0/all/0/1"&gt;Shiva Prasad Kasiviswanathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zekun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feyisetan_O/0/1/0/all/0/1"&gt;Oluwaseyi Feyisetan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teissier_N/0/1/0/all/0/1"&gt;Nathanael Teissier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Residual Star Generative Adversarial Network for multi-domain Image Super-Resolution. (arXiv:2107.03145v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.03145</id>
        <link href="http://arxiv.org/abs/2107.03145"/>
        <updated>2021-07-08T01:57:59.687Z</updated>
        <summary type="html"><![CDATA[Recently, most of state-of-the-art single image super-resolution (SISR)
methods have attained impressive performance by using deep convolutional neural
networks (DCNNs). The existing SR methods have limited performance due to a
fixed degradation settings, i.e. usually a bicubic downscaling of
low-resolution (LR) image. However, in real-world settings, the LR degradation
process is unknown which can be bicubic LR, bilinear LR, nearest-neighbor LR,
or real LR. Therefore, most SR methods are ineffective and inefficient in
handling more than one degradation settings within a single network. To handle
the multiple degradation, i.e. refers to multi-domain image super-resolution,
we propose a deep Super-Resolution Residual StarGAN (SR2*GAN), a novel and
scalable approach that super-resolves the LR images for the multiple LR domains
using only a single model. The proposed scheme is trained in a StarGAN like
network topology with a single generator and discriminator networks. We
demonstrate the effectiveness of our proposed approach in quantitative and
qualitative experiments compared to other state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Umer_R/0/1/0/all/0/1"&gt;Rao Muhammad Umer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Munir_A/0/1/0/all/0/1"&gt;Asad Munir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Micheloni_C/0/1/0/all/0/1"&gt;Christian Micheloni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Promises and Pitfalls of Deep Kernel Learning. (arXiv:2102.12108v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12108</id>
        <link href="http://arxiv.org/abs/2102.12108"/>
        <updated>2021-07-08T01:57:59.657Z</updated>
        <summary type="html"><![CDATA[Deep kernel learning (DKL) and related techniques aim to combine the
representational power of neural networks with the reliable uncertainty
estimates of Gaussian processes. One crucial aspect of these models is an
expectation that, because they are treated as Gaussian process models optimized
using the marginal likelihood, they are protected from overfitting. However, we
identify situations where this is not the case. We explore this behavior,
explain its origins and consider how it applies to real datasets. Through
careful experimentation on the UCI, CIFAR-10, and the UTKFace datasets, we find
that the overfitting from overparameterized maximum marginal likelihood, in
which the model is "somewhat Bayesian", can in certain scenarios be worse than
that from not being Bayesian at all. We explain how and when DKL can still be
successful by investigating optimization dynamics. We also find that failures
of DKL can be rectified by a fully Bayesian treatment, which leads to the
desired performance improvements over standard neural networks and Gaussian
processes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ober_S/0/1/0/all/0/1"&gt;Sebastian W. Ober&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rasmussen_C/0/1/0/all/0/1"&gt;Carl E. Rasmussen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wilk_M/0/1/0/all/0/1"&gt;Mark van der Wilk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Path Planning using Neural A* Search. (arXiv:2009.07476v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.07476</id>
        <link href="http://arxiv.org/abs/2009.07476"/>
        <updated>2021-07-08T01:57:59.650Z</updated>
        <summary type="html"><![CDATA[We present Neural A*, a novel data-driven search method for path planning
problems. Despite the recent increasing attention to data-driven path planning,
machine learning approaches to search-based planning are still challenging due
to the discrete nature of search algorithms. In this work, we reformulate a
canonical A* search algorithm to be differentiable and couple it with a
convolutional encoder to form an end-to-end trainable neural network planner.
Neural A* solves a path planning problem by encoding a problem instance to a
guidance map and then performing the differentiable A* search with the guidance
map. By learning to match the search results with ground-truth paths provided
by experts, Neural A* can produce a path consistent with the ground truth
accurately and efficiently. Our extensive experiments confirmed that Neural A*
outperformed state-of-the-art data-driven planners in terms of the search
optimality and efficiency trade-off. Furthermore, Neural A* successfully
predicted realistic human trajectories by directly performing search-based
planning on natural image inputs. Project page:
https://omron-sinicx.github.io/neural-astar/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yonetani_R/0/1/0/all/0/1"&gt;Ryo Yonetani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taniai_T/0/1/0/all/0/1"&gt;Tatsunori Taniai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barekatain_M/0/1/0/all/0/1"&gt;Mohammadamin Barekatain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nishimura_M/0/1/0/all/0/1"&gt;Mai Nishimura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanezaki_A/0/1/0/all/0/1"&gt;Asako Kanezaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distribution-free uncertainty quantification for classification under label shift. (arXiv:2103.03323v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03323</id>
        <link href="http://arxiv.org/abs/2103.03323"/>
        <updated>2021-07-08T01:57:59.588Z</updated>
        <summary type="html"><![CDATA[Trustworthy deployment of ML models requires a proper measure of uncertainty,
especially in safety-critical applications. We focus on uncertainty
quantification (UQ) for classification problems via two avenues -- prediction
sets using conformal prediction and calibration of probabilistic predictors by
post-hoc binning -- since these possess distribution-free guarantees for i.i.d.
data. Two common ways of generalizing beyond the i.i.d. setting include
handling covariate and label shift. Within the context of distribution-free UQ,
the former has already received attention, but not the latter. It is known that
label shift hurts prediction, and we first argue that it also hurts UQ, by
showing degradation in coverage and calibration. Piggybacking on recent
progress in addressing label shift (for better prediction), we examine the
right way to achieve UQ by reweighting the aforementioned conformal and
calibration procedures whenever some unlabeled data from the target
distribution is available. We examine these techniques theoretically in a
distribution-free framework and demonstrate their excellent practical
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Podkopaev_A/0/1/0/all/0/1"&gt;Aleksandr Podkopaev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1"&gt;Aaditya Ramdas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporating Label Uncertainty in Understanding Adversarial Robustness. (arXiv:2107.03250v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03250</id>
        <link href="http://arxiv.org/abs/2107.03250"/>
        <updated>2021-07-08T01:57:59.567Z</updated>
        <summary type="html"><![CDATA[A fundamental question in adversarial machine learning is whether a robust
classifier exists for a given task. A line of research has made progress
towards this goal by studying concentration of measure, but without considering
data labels. We argue that the standard concentration fails to fully
characterize the intrinsic robustness of a classification problem, since it
ignores data labels which are essential to any classification task. Building on
a novel definition of label uncertainty, we empirically demonstrate that error
regions induced by state-of-the-art models tend to have much higher label
uncertainty compared with randomly-selected subsets. This observation motivates
us to adapt a concentration estimation algorithm to account for label
uncertainty, resulting in more accurate intrinsic robustness measures for
benchmark image classification problems. We further provide empirical evidence
showing that adding an abstain option for classifiers based on label
uncertainty can help improve both the clean and robust accuracies of models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Evans_D/0/1/0/all/0/1"&gt;David Evans&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalization Error Analysis of Neural networks with Gradient Based Regularization. (arXiv:2107.02797v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.02797</id>
        <link href="http://arxiv.org/abs/2107.02797"/>
        <updated>2021-07-08T01:57:59.555Z</updated>
        <summary type="html"><![CDATA[We study gradient-based regularization methods for neural networks. We mainly
focus on two regularization methods: the total variation and the Tikhonov
regularization. Applying these methods is equivalent to using neural networks
to solve some partial differential equations, mostly in high dimensions in
practical applications. In this work, we introduce a general framework to
analyze the generalization error of regularized networks. The error estimate
relies on two assumptions on the approximation error and the quadrature error.
Moreover, we conduct some experiments on the image classification tasks to show
that gradient-based methods can significantly improve the generalization
ability and adversarial robustness of neural networks. A graphical extension of
the gradient-based methods are also considered in the experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lingfeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tai_X/0/1/0/all/0/1"&gt;Xue-Cheng Tai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jiang Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Deterministic Annealing for Classification and Clustering. (arXiv:2102.05836v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05836</id>
        <link href="http://arxiv.org/abs/2102.05836"/>
        <updated>2021-07-08T01:57:59.428Z</updated>
        <summary type="html"><![CDATA[Inherent in virtually every iterative machine learning algorithm is the
problem of hyper-parameter tuning, which includes three major design
parameters: (a) the complexity of the model, e.g., the number of neurons in a
neural network, (b) the initial conditions, which heavily affect the behavior
of the algorithm, and (c) the dissimilarity measure used to quantify its
performance. We introduce an online prototype-based learning algorithm that can
be viewed as a progressively growing competitive-learning neural network
architecture for classification and clustering. The learning rule of the
proposed approach is formulated as an online gradient-free stochastic
approximation algorithm that solves a sequence of appropriately defined
optimization problems, simulating an annealing process. The annealing nature of
the algorithm contributes to avoiding poor local minima, offers robustness with
respect to the initial conditions, and provides a means to progressively
increase the complexity of the learning model, through an intuitive bifurcation
phenomenon. The proposed approach is interpretable, requires minimal
hyper-parameter tuning, and allows online control over the
performance-complexity trade-off. Finally, we show that Bregman divergences
appear naturally as a family of dissimilarity measures that play a central role
in both the performance and the computational complexity of the learning
algorithm. Experimental results illustrate the properties and evaluate the
performance of the proposed learning algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mavridis_C/0/1/0/all/0/1"&gt;Christos Mavridis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baras_J/0/1/0/all/0/1"&gt;John Baras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Your GAN is Secretly an Energy-based Model and You Should use Discriminator Driven Latent Sampling. (arXiv:2003.06060v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.06060</id>
        <link href="http://arxiv.org/abs/2003.06060"/>
        <updated>2021-07-08T01:57:59.349Z</updated>
        <summary type="html"><![CDATA[We show that the sum of the implicit generator log-density $\log p_g$ of a
GAN with the logit score of the discriminator defines an energy function which
yields the true data density when the generator is imperfect but the
discriminator is optimal, thus making it possible to improve on the typical
generator (with implicit density $p_g$). To make that practical, we show that
sampling from this modified density can be achieved by sampling in latent space
according to an energy-based model induced by the sum of the latent prior
log-density and the discriminator output score. This can be achieved by running
a Langevin MCMC in latent space and then applying the generator function, which
we call Discriminator Driven Latent Sampling~(DDLS). We show that DDLS is
highly efficient compared to previous methods which work in the
high-dimensional pixel space and can be applied to improve on previously
trained GANs of many types. We evaluate DDLS on both synthetic and real-world
datasets qualitatively and quantitatively. On CIFAR-10, DDLS substantially
improves the Inception Score of an off-the-shelf pre-trained
SN-GAN~\citep{sngan} from $8.22$ to $9.09$ which is even comparable to the
class-conditional BigGAN~\citep{biggan} model. This achieves a new
state-of-the-art in unconditional image synthesis setting without introducing
extra parameters or additional training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Che_T/0/1/0/all/0/1"&gt;Tong Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruixiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1"&gt;Jascha Sohl-Dickstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Larochelle_H/0/1/0/all/0/1"&gt;Hugo Larochelle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paull_L/0/1/0/all/0/1"&gt;Liam Paull&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yuan Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intensity Prediction of Tropical Cyclones using Long Short-Term Memory Network. (arXiv:2107.03187v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03187</id>
        <link href="http://arxiv.org/abs/2107.03187"/>
        <updated>2021-07-08T01:57:59.304Z</updated>
        <summary type="html"><![CDATA[Tropical cyclones can be of varied intensity and cause a huge loss of lives
and property if the intensity is high enough. Therefore, the prediction of the
intensity of tropical cyclones advance in time is of utmost importance. We
propose a novel stacked bidirectional long short-term memory network (BiLSTM)
based model architecture to predict the intensity of a tropical cyclone in
terms of Maximum surface sustained wind speed (MSWS). The proposed model can
predict MSWS well advance in time (up to 72 h) with very high accuracy. We have
applied the model on tropical cyclones in the North Indian Ocean from 1982 to
2018 and checked its performance on two recent tropical cyclones, namely, Fani
and Vayu. The model predicts MSWS (in knots) for the next 3, 12, 24, 36, 48,
60, and 72 hours with a mean absolute error of 1.52, 3.66, 5.88, 7.42, 8.96,
10.15, and 11.92, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biswas_K/0/1/0/all/0/1"&gt;Koushik Biswas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sandeep Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1"&gt;Ashish Kumar Pandey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Fault Detection for Deep Learning Programs Using Graph Transformations. (arXiv:2105.08095v2 [cs.SE] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2105.08095</id>
        <link href="http://arxiv.org/abs/2105.08095"/>
        <updated>2021-07-08T01:57:59.286Z</updated>
        <summary type="html"><![CDATA[Nowadays, we are witnessing an increasing demand in both corporates and
academia for exploiting Deep Learning (DL) to solve complex real-world
problems. A DL program encodes the network structure of a desirable DL model
and the process by which the model learns from the training dataset. Like any
software, a DL program can be faulty, which implies substantial challenges of
software quality assurance, especially in safety-critical domains. It is
therefore crucial to equip DL development teams with efficient fault detection
techniques and tools. In this paper, we propose NeuraLint, a model-based fault
detection approach for DL programs, using meta-modelling and graph
transformations. First, we design a meta-model for DL programs that includes
their base skeleton and fundamental properties. Then, we construct a
graph-based verification process that covers 23 rules defined on top of the
meta-model and implemented as graph transformations to detect faults and design
inefficiencies in the generated models (i.e., instances of the meta-model).
First, the proposed approach is evaluated by finding faults and design
inefficiencies in 28 synthesized examples built from common problems reported
in the literature. Then NeuraLint successfully finds 64 faults and design
inefficiencies in 34 real-world DL programs extracted from Stack Overflow posts
and GitHub repositories. The results show that NeuraLint effectively detects
faults and design issues in both synthesized and real-world examples with a
recall of 70.5 % and a precision of 100 %. Although the proposed meta-model is
designed for feedforward neural networks, it can be extended to support other
neural network architectures such as recurrent neural networks. Researchers can
also expand our set of verification rules to cover more types of issues in DL
programs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nikanjam_A/0/1/0/all/0/1"&gt;Amin Nikanjam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Braiek_H/0/1/0/all/0/1"&gt;Houssem Ben Braiek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morovati_M/0/1/0/all/0/1"&gt;Mohammad Mehdi Morovati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1"&gt;Foutse Khomh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Opioid Use Disorder from Longitudinal Healthcare Data using Multi-stream Transformer. (arXiv:2103.08800v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08800</id>
        <link href="http://arxiv.org/abs/2103.08800"/>
        <updated>2021-07-08T01:57:59.279Z</updated>
        <summary type="html"><![CDATA[Opioid Use Disorder (OUD) is a public health crisis costing the US billions
of dollars annually in healthcare, lost workplace productivity, and crime.
Analyzing longitudinal healthcare data is critical in addressing many
real-world problems in healthcare. Leveraging the real-world longitudinal
healthcare data, we propose a novel multi-stream transformer model called MUPOD
for OUD identification. MUPOD is designed to simultaneously analyze multiple
types of healthcare data streams, such as medications and diagnoses, by
attending to segments within and across these data streams. Our model tested on
the data from 392,492 patients with long-term back pain problems showed
significantly better performance than the traditional models and recently
developed deep learning models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fouladvand_S/0/1/0/all/0/1"&gt;Sajjad Fouladvand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talbert_J/0/1/0/all/0/1"&gt;Jeffery Talbert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dwoskin_L/0/1/0/all/0/1"&gt;Linda P. Dwoskin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bush_H/0/1/0/all/0/1"&gt;Heather Bush&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meadows_A/0/1/0/all/0/1"&gt;Amy Lynn Meadows&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peterson_L/0/1/0/all/0/1"&gt;Lars E. Peterson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kavuluru_R/0/1/0/all/0/1"&gt;Ramakanth Kavuluru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jin Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graphing else matters: exploiting aspect opinions and ratings in explainable graph-based recommendations. (arXiv:2107.03226v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.03226</id>
        <link href="http://arxiv.org/abs/2107.03226"/>
        <updated>2021-07-08T01:57:59.268Z</updated>
        <summary type="html"><![CDATA[The success of neural network embeddings has entailed a renewed interest in
using knowledge graphs for a wide variety of machine learning and information
retrieval tasks. In particular, current recommendation methods based on graph
embeddings have shown state-of-the-art performance. These methods commonly
encode latent rating patterns and content features. Different from previous
work, in this paper, we propose to exploit embeddings extracted from graphs
that combine information from ratings and aspect-based opinions expressed in
textual reviews. We then adapt and evaluate state-of-the-art graph embedding
techniques over graphs generated from Amazon and Yelp reviews on six domains,
outperforming baseline recommenders. Our approach has the advantage of
providing explanations which leverage aspect-based opinions given by users
about recommended items. Furthermore, we also provide examples of the
applicability of recommendations utilizing aspect opinions as explanations in a
visualization dashboard, which allows obtaining information about the most and
least liked aspects of similar users obtained from the embeddings of an input
graph.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cantador_I/0/1/0/all/0/1"&gt;Iv&amp;#xe1;n Cantador&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carvallo_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9;s Carvallo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diez_F/0/1/0/all/0/1"&gt;Fernando Diez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parra_D/0/1/0/all/0/1"&gt;Denis Parra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simple Agent, Complex Environment: Efficient Reinforcement Learning with Agent States. (arXiv:2102.05261v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05261</id>
        <link href="http://arxiv.org/abs/2102.05261"/>
        <updated>2021-07-08T01:57:59.262Z</updated>
        <summary type="html"><![CDATA[We design a simple reinforcement learning (RL) agent that implements an
optimistic version of $Q$-learning and establish through regret analysis that
this agent can operate with some level of competence in any environment. While
we leverage concepts from the literature on provably efficient RL, we consider
a general agent-environment interface and provide a novel agent design and
analysis. This level of generality positions our results to inform the design
of future agents for operation in complex real environments. We establish that,
as time progresses, our agent performs competitively relative to policies that
require longer times to evaluate. The time it takes to approach asymptotic
performance is polynomial in the complexity of the agent's state representation
and the time required to evaluate the best policy that the agent can represent.
Notably, there is no dependence on the complexity of the environment. The
ultimate per-period performance loss of the agent is bounded by a constant
multiple of a measure of distortion introduced by the agent's state
representation. This work is the first to establish that an algorithm
approaches this asymptotic condition within a tractable time frame.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1"&gt;Shi Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1"&gt;Benjamin Van Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhengyuan Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed stochastic optimization with large delays. (arXiv:2107.02919v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2107.02919</id>
        <link href="http://arxiv.org/abs/2107.02919"/>
        <updated>2021-07-08T01:57:59.255Z</updated>
        <summary type="html"><![CDATA[One of the most widely used methods for solving large-scale stochastic
optimization problems is distributed asynchronous stochastic gradient descent
(DASGD), a family of algorithms that result from parallelizing stochastic
gradient descent on distributed computing architectures (possibly)
asychronously. However, a key obstacle in the efficient implementation of DASGD
is the issue of delays: when a computing node contributes a gradient update,
the global model parameter may have already been updated by other nodes several
times over, thereby rendering this gradient information stale. These delays can
quickly add up if the computational throughput of a node is saturated, so the
convergence of DASGD may be compromised in the presence of large delays. Our
first contribution is that, by carefully tuning the algorithm's step-size,
convergence to the critical set is still achieved in mean square, even if the
delays grow unbounded at a polynomial rate. We also establish finer results in
a broad class of structured optimization problems (called variationally
coherent), where we show that DASGD converges to a global optimum with
probability $1$ under the same delay assumptions. Together, these results
contribute to the broad landscape of large-scale non-convex stochastic
optimization by offering state-of-the-art theoretical guarantees and providing
insights for algorithm design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhengyuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Mertikopoulos_P/0/1/0/all/0/1"&gt;Panayotis Mertikopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Bambos_N/0/1/0/all/0/1"&gt;Nicholas Bambos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Glynn_P/0/1/0/all/0/1"&gt;Peter W. Glynn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yinyu Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DER Forecast using Privacy Preserving Federated Learning. (arXiv:2107.03248v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2107.03248</id>
        <link href="http://arxiv.org/abs/2107.03248"/>
        <updated>2021-07-08T01:57:59.229Z</updated>
        <summary type="html"><![CDATA[With increasing penetration of Distributed Energy Resources (DERs) in grid
edge including renewable generation, flexible loads, and storage, accurate
prediction of distributed generation and consumption at the consumer level
becomes important. However, DER prediction based on the transmission of
customer level data, either repeatedly or in large amounts, is not feasible due
to privacy concerns. In this paper, a distributed machine learning approach,
Federated Learning, is proposed to carry out DER forecasting using a network of
IoT nodes, each of which transmits a model of the consumption and generation
patterns without revealing consumer data. We consider a simulation study which
includes 1000 DERs, and show that our method leads to an accurate prediction of
preserve consumer privacy, while still leading to an accurate forecast. We also
evaluate grid-specific performance metrics such as load swings and load
curtailment and show that our FL algorithm leads to satisfactory performance.
Simulations are also performed on the Pecan street dataset to demonstrate the
validity of the proposed approach on real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Venkataramanan_V/0/1/0/all/0/1"&gt;Venkatesh Venkataramanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kaza_S/0/1/0/all/0/1"&gt;Sridevi Kaza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Annaswamy_A/0/1/0/all/0/1"&gt;Anuradha M. Annaswamy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Large Language Models Trained on Code. (arXiv:2107.03374v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03374</id>
        <link href="http://arxiv.org/abs/2107.03374"/>
        <updated>2021-07-08T01:57:59.223Z</updated>
        <summary type="html"><![CDATA[We introduce Codex, a GPT language model fine-tuned on publicly available
code from GitHub, and study its Python code-writing capabilities. A distinct
production version of Codex powers GitHub Copilot. On HumanEval, a new
evaluation set we release to measure functional correctness for synthesizing
programs from docstrings, our model solves 28.8% of the problems, while GPT-3
solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling
from the model is a surprisingly effective strategy for producing working
solutions to difficult prompts. Using this method, we solve 70.2% of our
problems with 100 samples per problem. Careful investigation of our model
reveals its limitations, including difficulty with docstrings describing long
chains of operations and with binding operations to variables. Finally, we
discuss the potential broader impacts of deploying powerful code generation
technologies, covering safety, security, and economics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mark Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tworek_J/0/1/0/all/0/1"&gt;Jerry Tworek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jun_H/0/1/0/all/0/1"&gt;Heewoo Jun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Q/0/1/0/all/0/1"&gt;Qiming Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ponde_H/0/1/0/all/0/1"&gt;Henrique Ponde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaplan_J/0/1/0/all/0/1"&gt;Jared Kaplan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Edwards_H/0/1/0/all/0/1"&gt;Harri Edwards&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burda_Y/0/1/0/all/0/1"&gt;Yura Burda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joseph_N/0/1/0/all/0/1"&gt;Nicholas Joseph&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brockman_G/0/1/0/all/0/1"&gt;Greg Brockman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1"&gt;Alex Ray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Puri_R/0/1/0/all/0/1"&gt;Raul Puri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krueger_G/0/1/0/all/0/1"&gt;Gretchen Krueger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petrov_M/0/1/0/all/0/1"&gt;Michael Petrov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khlaaf_H/0/1/0/all/0/1"&gt;Heidy Khlaaf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sastry_G/0/1/0/all/0/1"&gt;Girish Sastry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishkin_P/0/1/0/all/0/1"&gt;Pamela Mishkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_B/0/1/0/all/0/1"&gt;Brooke Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gray_S/0/1/0/all/0/1"&gt;Scott Gray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ryder_N/0/1/0/all/0/1"&gt;Nick Ryder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pavlov_M/0/1/0/all/0/1"&gt;Mikhail Pavlov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Power_A/0/1/0/all/0/1"&gt;Alethea Power&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaiser_L/0/1/0/all/0/1"&gt;Lukasz Kaiser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bavarian_M/0/1/0/all/0/1"&gt;Mohammad Bavarian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winter_C/0/1/0/all/0/1"&gt;Clemens Winter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tillet_P/0/1/0/all/0/1"&gt;Philippe Tillet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Such_F/0/1/0/all/0/1"&gt;Felipe Such&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cummings_D/0/1/0/all/0/1"&gt;Dave Cummings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plappert_M/0/1/0/all/0/1"&gt;Matthias Plappert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chantzis_F/0/1/0/all/0/1"&gt;Fotios Chantzis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barnes_E/0/1/0/all/0/1"&gt;Elizabeth Barnes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herbert_Voss_A/0/1/0/all/0/1"&gt;Ariel Herbert-Voss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guss_W/0/1/0/all/0/1"&gt;Will Guss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nichol_A/0/1/0/all/0/1"&gt;Alex Nichol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babuschkin_I/0/1/0/all/0/1"&gt;Igor Babuschkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balaji_S/0/1/0/all/0/1"&gt;Suchir Balaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1"&gt;Shantanu Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carr_A/0/1/0/all/0/1"&gt;Andrew Carr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leike_J/0/1/0/all/0/1"&gt;Jan Leike&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Achiam_J/0/1/0/all/0/1"&gt;Josh Achiam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misra_V/0/1/0/all/0/1"&gt;Vedant Misra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morikawa_E/0/1/0/all/0/1"&gt;Evan Morikawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radford_A/0/1/0/all/0/1"&gt;Alec Radford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knight_M/0/1/0/all/0/1"&gt;Matthew Knight&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brundage_M/0/1/0/all/0/1"&gt;Miles Brundage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murati_M/0/1/0/all/0/1"&gt;Mira Murati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mayer_K/0/1/0/all/0/1"&gt;Katie Mayer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welinder_P/0/1/0/all/0/1"&gt;Peter Welinder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McGrew_B/0/1/0/all/0/1"&gt;Bob McGrew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amodei_D/0/1/0/all/0/1"&gt;Dario Amodei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McCandlish_S/0/1/0/all/0/1"&gt;Sam McCandlish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sutskever_I/0/1/0/all/0/1"&gt;Ilya Sutskever&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaremba_W/0/1/0/all/0/1"&gt;Wojciech Zaremba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An empirical evaluation of active inference in multi-armed bandits. (arXiv:2101.08699v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08699</id>
        <link href="http://arxiv.org/abs/2101.08699"/>
        <updated>2021-07-08T01:57:59.215Z</updated>
        <summary type="html"><![CDATA[A key feature of sequential decision making under uncertainty is a need to
balance between exploiting--choosing the best action according to the current
knowledge, and exploring--obtaining information about values of other actions.
The multi-armed bandit problem, a classical task that captures this trade-off,
served as a vehicle in machine learning for developing bandit algorithms that
proved to be useful in numerous industrial applications. The active inference
framework, an approach to sequential decision making recently developed in
neuroscience for understanding human and animal behaviour, is distinguished by
its sophisticated strategy for resolving the exploration-exploitation
trade-off. This makes active inference an exciting alternative to already
established bandit algorithms. Here we derive an efficient and scalable
approximate active inference algorithm and compare it to two state-of-the-art
bandit algorithms: Bayesian upper confidence bound and optimistic Thompson
sampling. This comparison is done on two types of bandit problems: a stationary
and a dynamic switching bandit. Our empirical evaluation shows that the active
inference algorithm does not produce efficient long-term behaviour in
stationary bandits. However, in the more challenging switching bandit problem
active inference performs substantially better than the two state-of-the-art
bandit algorithms. The results open exciting venues for further research in
theoretical and applied machine learning, as well as lend additional
credibility to active inference as a general framework for studying human and
animal behaviour.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Markovic_D/0/1/0/all/0/1"&gt;Dimitrije Markovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stojic_H/0/1/0/all/0/1"&gt;Hrvoje Stojic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwoebel_S/0/1/0/all/0/1"&gt;Sarah Schwoebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiebel_S/0/1/0/all/0/1"&gt;Stefan J. Kiebel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[XPDNet for MRI Reconstruction: an application to the 2020 fastMRI challenge. (arXiv:2010.07290v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07290</id>
        <link href="http://arxiv.org/abs/2010.07290"/>
        <updated>2021-07-08T01:57:59.206Z</updated>
        <summary type="html"><![CDATA[We present a new neural network, the XPDNet, for MRI reconstruction from
periodically under-sampled multi-coil data. We inform the design of this
network by taking best practices from MRI reconstruction and computer vision.
We show that this network can achieve state-of-the-art reconstruction results,
as shown by its ranking of second in the fastMRI 2020 challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ramzi_Z/0/1/0/all/0/1"&gt;Zaccharie Ramzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ciuciu_P/0/1/0/all/0/1"&gt;Philippe Ciuciu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Starck_J/0/1/0/all/0/1"&gt;Jean-Luc Starck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hub and Spoke Logistics Network Design for Urban Region with Clustering-Based Approach. (arXiv:2107.03080v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03080</id>
        <link href="http://arxiv.org/abs/2107.03080"/>
        <updated>2021-07-08T01:57:59.199Z</updated>
        <summary type="html"><![CDATA[This study aims to propose effective modeling and approach for designing a
logistics network in the urban area in order to offer an efficient flow
distribution network as a competitive strategy in the logistics industry where
demand is sensitive to both price and time. A multi-stage approach is
introduced to select the number of hubs and allocate spokes to the hubs for
flow distribution and hubs' location detection. Specifically, a fuzzy
clustering model with the objective function is to minimize the approximate
transportation cost is employed, in the next phase is to focus on balancing the
demand capacity among the hubs with the help of domain experts, afterward, the
facility location vehicle routing problems within the network is introduced. To
demonstrate the approach's advantages, an experiment was performed on the
designed network and its actual transportation cost for the real operational
data in which specific to the Ho Chi Minh city infrastructure conditions.
Additionally, we show the flexibility of the designed network in the flow
distribution and its computational experiments to develop the managerial
insights which contribute to the network design decision-making process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duong_Q/0/1/0/all/0/1"&gt;Quan Duong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Dang Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1"&gt;Quoc Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection and Mitigation of Rare Subclasses in Deep Neural Network Classifiers. (arXiv:1911.12780v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.12780</id>
        <link href="http://arxiv.org/abs/1911.12780"/>
        <updated>2021-07-08T01:57:59.182Z</updated>
        <summary type="html"><![CDATA[Regions of high-dimensional input spaces that are underrepresented in
training datasets reduce machine-learnt classifier performance, and may lead to
corner cases and unwanted bias for classifiers used in decision making systems.
When these regions belong to otherwise well-represented classes, their presence
and negative impact are very hard to identify. We propose an approach for the
detection and mitigation of such rare subclasses in deep neural network
classifiers. The new approach is underpinned by an easy-to-compute commonality
metric that supports the detection of rare subclasses, and comprises methods
for reducing the impact of these subclasses during both model training and
model exploitation. We demonstrate our approach using two well-known datasets,
MNIST's handwritten digits and Kaggle's cats/dogs, identifying rare subclasses
and producing models which compensate for subclass rarity. In addition we
demonstrate how our run-time approach increases the ability of users to
identify samples likely to be misclassified at run-time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paterson_C/0/1/0/all/0/1"&gt;Colin Paterson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calinescu_R/0/1/0/all/0/1"&gt;Radu Calinescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Picardi_C/0/1/0/all/0/1"&gt;Chiara Picardi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coastal water quality prediction based on machine learning with feature interpretation and spatio-temporal analysis. (arXiv:2107.03230v1 [stat.AP])]]></title>
        <id>http://arxiv.org/abs/2107.03230</id>
        <link href="http://arxiv.org/abs/2107.03230"/>
        <updated>2021-07-08T01:57:59.176Z</updated>
        <summary type="html"><![CDATA[Coastal water quality management is a public health concern, as poor coastal
water quality can harbor pathogens that are dangerous to human health.
Tourism-oriented countries need to actively monitor the condition of coastal
water at tourist popular sites during the summer season. In this study, routine
monitoring data of $Escherichia\ Coli$ and enterococci across 15 public beaches
in the city of Rijeka, Croatia, were used to build machine learning models for
predicting their levels based on environmental parameters as well as to
investigate their relationships with environmental stressors. Gradient Boosting
(Catboost, Xgboost), Random Forests, Support Vector Regression and Artificial
Neural Networks were trained with measurements from all sampling sites and used
to predict $E.\ Coli$ and enterococci values based on environmental features.
The evaluation of stability and generalizability with 10-fold cross validation
analysis of the machine learning models, showed that the Catboost algorithm
performed best with R$^2$ values of 0.71 and 0.68 for predicting $E.\ Coli$ and
enterococci, respectively, compared to other evaluated ML algorithms including
Xgboost, Random Forests, Support Vector Regression and Artificial Neural
Networks. We also use the SHapley Additive exPlanations technique to identify
and interpret which features have the most predictive power. The results show
that site salinity measured is the most important feature for forecasting both
$E.\ Coli$ and enterococci levels. Finally, the spatial and temporal accuracy
of both ML models were examined at sites with the lowest coastal water quality.
The spatial $E. Coli$ and enterococci models achieved strong R$^2$ values of
0.85 and 0.83, while the temporal models achieved R$^2$ values of 0.74 and
0.67. The temporal model also achieved moderate R$^2$ values of 0.44 and 0.46
at a site with high coastal water quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Grbcic_L/0/1/0/all/0/1"&gt;Luka Grb&amp;#x10d;i&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Druzeta_S/0/1/0/all/0/1"&gt;Sini&amp;#x161;a Dru&amp;#x17e;eta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mausa_G/0/1/0/all/0/1"&gt;Goran Mau&amp;#x161;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lipic_T/0/1/0/all/0/1"&gt;Tomislav Lipi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lusic_D/0/1/0/all/0/1"&gt;Darija Vuki&amp;#x107; Lu&amp;#x161;i&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Alvir_M/0/1/0/all/0/1"&gt;Marta Alvir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lucin_I/0/1/0/all/0/1"&gt;Ivana Lu&amp;#x10d;in&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sikirica_A/0/1/0/all/0/1"&gt;Ante Sikirica&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Davidovic_D/0/1/0/all/0/1"&gt;Davor Davidovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Travas_V/0/1/0/all/0/1"&gt;Vanja Trava&amp;#x161;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kalafatovic_D/0/1/0/all/0/1"&gt;Daniela Kalafatovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pikelj_K/0/1/0/all/0/1"&gt;Kristina Pikelj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Fajkovic_H/0/1/0/all/0/1"&gt;Hana Fajkovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kranjcevic_L/0/1/0/all/0/1"&gt;Lado Kranj&amp;#x10d;evi&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Closed-Form Approximation to the Conjugate Prior of the Dirichlet and Beta Distributions. (arXiv:2107.03183v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.03183</id>
        <link href="http://arxiv.org/abs/2107.03183"/>
        <updated>2021-07-08T01:57:59.167Z</updated>
        <summary type="html"><![CDATA[We derive the conjugate prior of the Dirichlet and beta distributions and
explore it with numerical examples to gain an intuitive understanding of the
distribution itself, its hyperparameters, and conditions concerning its
convergence. Due to the prior's intractability, we proceed to define and
analyze a closed-form approximation. Finally, we provide an algorithm
implementing this approximation that enables fully tractable Bayesian conjugate
treatment of Dirichlet and beta likelihoods without the need for Monte Carlo
simulations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Thommen_K/0/1/0/all/0/1"&gt;Kaspar Thommen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Impulse data models for the inverse problem of electrocardiography. (arXiv:2102.00570v2 [q-bio.QM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00570</id>
        <link href="http://arxiv.org/abs/2102.00570"/>
        <updated>2021-07-08T01:57:59.159Z</updated>
        <summary type="html"><![CDATA[The proposed method re-frames traditional inverse problems of
electrocardiography into regression problems, constraining the solution space
by decomposing signals with multidimensional Gaussian impulse basis functions.
Impulse HSPs were generated with single Gaussian basis functions at discrete
heart surface locations and projected to corresponding BSPs using a volume
conductor torso model. Both BSP (inputs) and HSP (outputs) were mapped to
regular 2D surface meshes and used to train a neural network. Predictive
capabilities of the network were tested with unseen synthetic and experimental
data. A dense full connected single hidden layer neural network was trained to
map body surface impulses to heart surface Gaussian basis functions for
reconstructing HSP. Synthetic pulses moving across the heart surface were
predicted from the neural network with root mean squared error of $9.1\pm1.4$%.
Predicted signals were robust to noise up to 20 dB and errors due to
displacement and rotation of the heart within the torso were bounded and
predictable. A shift of the heart 40 mm toward the spine resulted in a 4\%
increase in signal feature localization error. The set of training impulse
function data could be reduced and prediction error remained bounded. Recorded
HSPs from in-vitro pig hearts were reliably decomposed using space-time
Gaussian basis functions. Predicted HSPs for left-ventricular pacing had a mean
absolute error of $10.4\pm11.4$ ms. Other pacing scenarios were analyzed with
similar success. Conclusion: Impulses from Gaussian basis functions are
potentially an effective and robust way to train simple neural network data
models for reconstructing HSPs from decomposed BSPs. The HSPs predicted by the
neural network can be used to generate activation maps that non-invasively
identify features of cardiac electrical dysfunction and can guide subsequent
treatment options.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Peng_T/0/1/0/all/0/1"&gt;Tommy Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Malik_A/0/1/0/all/0/1"&gt;Avinash Malik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Bear_L/0/1/0/all/0/1"&gt;Laura Bear&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Trew_M/0/1/0/all/0/1"&gt;Mark L. Trew&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing an Intelligent Digital Twin with a Self-organized Reconfiguration Management based on Adaptive Process Models. (arXiv:2107.03324v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.03324</id>
        <link href="http://arxiv.org/abs/2107.03324"/>
        <updated>2021-07-08T01:57:59.152Z</updated>
        <summary type="html"><![CDATA[Shorter product life cycles and increasing individualization of production
leads to an increased reconfiguration demand in the domain of industrial
automation systems, which will be dominated by cyber-physical production
systems in the future. In constantly changing systems, however, not all
configuration alternatives of the almost infinite state space are fully
understood. Thus, certain configurations can lead to process instability, a
reduction in quality or machine failures. Therefore, this paper presents an
approach that enhances an intelligent Digital Twin with a self-organized
reconfiguration management based on adaptive process models in order to find
optimized configurations more comprehensively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1"&gt;Timo M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lindemann_B/0/1/0/all/0/1"&gt;Benjamin Lindemann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_T/0/1/0/all/0/1"&gt;Tobias Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jazdi_N/0/1/0/all/0/1"&gt;Nasser Jazdi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weyrich_M/0/1/0/all/0/1"&gt;Michael Weyrich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PHOTONAI -- A Python API for Rapid Machine Learning Model Development. (arXiv:2002.05426v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.05426</id>
        <link href="http://arxiv.org/abs/2002.05426"/>
        <updated>2021-07-08T01:57:59.144Z</updated>
        <summary type="html"><![CDATA[PHOTONAI is a high-level Python API designed to simplify and accelerate
machine learning model development. It functions as a unifying framework
allowing the user to easily access and combine algorithms from different
toolboxes into custom algorithm sequences. It is especially designed to support
the iterative model development process and automates the repetitive training,
hyperparameter optimization and evaluation tasks. Importantly, the workflow
ensures unbiased performance estimates while still allowing the user to fully
customize the machine learning analysis. PHOTONAI extends existing solutions
with a novel pipeline implementation supporting more complex data streams,
feature combinations, and algorithm selection. Metrics and results can be
conveniently visualized using the PHOTONAI Explorer and predictive models are
shareable in a standardized format for further external validation or
application. A growing add-on ecosystem allows researchers to offer data
modality specific algorithms to the community and enhance machine learning in
the areas of the life sciences. Its practical utility is demonstrated on an
exemplary medical machine learning problem, achieving a state-of-the-art
solution in few lines of code. Source code is publicly available on Github,
while examples and documentation can be found at www.photon-ai.com.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leenings_R/0/1/0/all/0/1"&gt;Ramona Leenings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winter_N/0/1/0/all/0/1"&gt;Nils Ralf Winter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plagwitz_L/0/1/0/all/0/1"&gt;Lucas Plagwitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holstein_V/0/1/0/all/0/1"&gt;Vincent Holstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ernsting_J/0/1/0/all/0/1"&gt;Jan Ernsting&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steenweg_J/0/1/0/all/0/1"&gt;Jakob Steenweg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gebker_J/0/1/0/all/0/1"&gt;Julian Gebker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarink_K/0/1/0/all/0/1"&gt;Kelvin Sarink&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Emden_D/0/1/0/all/0/1"&gt;Daniel Emden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grotegerd_D/0/1/0/all/0/1"&gt;Dominik Grotegerd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Opel_N/0/1/0/all/0/1"&gt;Nils Opel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Risse_B/0/1/0/all/0/1"&gt;Benjamin Risse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xiaoyi Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dannlowski_U/0/1/0/all/0/1"&gt;Udo Dannlowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hahn_T/0/1/0/all/0/1"&gt;Tim Hahn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mitigating Performance Saturation in Neural Marked Point Processes: Architectures and Loss Functions. (arXiv:2107.03354v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03354</id>
        <link href="http://arxiv.org/abs/2107.03354"/>
        <updated>2021-07-08T01:57:59.125Z</updated>
        <summary type="html"><![CDATA[Attributed event sequences are commonly encountered in practice. A recent
research line focuses on incorporating neural networks with the statistical
model -- marked point processes, which is the conventional tool for dealing
with attributed event sequences. Neural marked point processes possess good
interpretability of probabilistic models as well as the representational power
of neural networks. However, we find that performance of neural marked point
processes is not always increasing as the network architecture becomes more
complicated and larger, which is what we call the performance saturation
phenomenon. This is due to the fact that the generalization error of neural
marked point processes is determined by both the network representational
ability and the model specification at the same time. Therefore we can draw two
major conclusions: first, simple network structures can perform no worse than
complicated ones for some cases; second, using a proper probabilistic
assumption is as equally, if not more, important as improving the complexity of
the network. Based on this observation, we propose a simple graph-based network
structure called GCHP, which utilizes only graph convolutional layers, thus it
can be easily accelerated by the parallel mechanism. We directly consider the
distribution of interarrival times instead of imposing a specific assumption on
the conditional intensity function, and propose to use a likelihood ratio loss
with a moment matching mechanism for optimization and model selection.
Experimental results show that GCHP can significantly reduce training time and
the likelihood ratio loss with interarrival time probability assumptions can
greatly improve the model performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianbo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1"&gt;Tianze Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ke_Y/0/1/0/all/0/1"&gt;Yiping Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Sinno Jialin Pan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[L2P: An Algorithm for Estimating Heavy-tailed Outcomes. (arXiv:1908.04628v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.04628</id>
        <link href="http://arxiv.org/abs/1908.04628"/>
        <updated>2021-07-08T01:57:59.118Z</updated>
        <summary type="html"><![CDATA[Many real-world prediction tasks have outcome variables that have
characteristic heavy-tail distributions. Examples include copies of books sold,
auction prices of art pieces, demand for commodities in warehouses, etc. By
learning heavy-tailed distributions, "big and rare" instances (e.g., the
best-sellers) will have accurate predictions. Most existing approaches are not
dedicated to learning heavy-tailed distribution; thus, they heavily
under-predict such instances. To tackle this problem, we introduce Learning to
Place (L2P), which exploits the pairwise relationships between instances for
learning. In its training phase, L2P learns a pairwise preference classifier:
is instance A > instance B? In its placing phase, L2P obtains a prediction by
placing the new instance among the known instances. Based on its placement, the
new instance is then assigned a value for its outcome variable. Experiments on
real data show that L2P outperforms competing approaches in terms of accuracy
and ability to reproduce heavy-tailed outcome distribution. In addition, L2P
provides an interpretable model by placing each predicted instance in relation
to its comparable neighbors. Interpretable models are highly desirable when
lives and treasure are at stake.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xindi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varol_O/0/1/0/all/0/1"&gt;Onur Varol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eliassi_Rad_T/0/1/0/all/0/1"&gt;Tina Eliassi-Rad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Last-iterate Convergence of Decentralized Optimistic Gradient Descent/Ascent in Infinite-horizon Competitive Markov Games. (arXiv:2102.04540v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04540</id>
        <link href="http://arxiv.org/abs/2102.04540"/>
        <updated>2021-07-08T01:57:59.111Z</updated>
        <summary type="html"><![CDATA[We study infinite-horizon discounted two-player zero-sum Markov games, and
develop a decentralized algorithm that provably converges to the set of Nash
equilibria under self-play. Our algorithm is based on running an Optimistic
Gradient Descent Ascent algorithm on each state to learn the policies, with a
critic that slowly learns the value of each state. To the best of our
knowledge, this is the first algorithm in this setting that is simultaneously
rational (converging to the opponent's best response when it uses a stationary
policy), convergent (converging to the set of Nash equilibria under self-play),
agnostic (no need to know the actions played by the opponent), symmetric
(players taking symmetric roles in the algorithm), and enjoying a finite-time
last-iterate convergence guarantee, all of which are desirable properties of
decentralized algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1"&gt;Chen-Yu Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chung-Wei Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mengxiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Haipeng Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A case for new neural network smoothness constraints. (arXiv:2012.07969v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07969</id>
        <link href="http://arxiv.org/abs/2012.07969"/>
        <updated>2021-07-08T01:57:59.089Z</updated>
        <summary type="html"><![CDATA[How sensitive should machine learning models be to input changes? We tackle
the question of model smoothness and show that it is a useful inductive bias
which aids generalization, adversarial robustness, generative modeling and
reinforcement learning. We explore current methods of imposing smoothness
constraints and observe they lack the flexibility to adapt to new tasks, they
don't account for data modalities, they interact with losses, architectures and
optimization in ways not yet fully understood. We conclude that new advances in
the field are hinging on finding ways to incorporate data, tasks and learning
into our definitions of smoothness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Rosca_M/0/1/0/all/0/1"&gt;Mihaela Rosca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Weber_T/0/1/0/all/0/1"&gt;Theophane Weber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1"&gt;Arthur Gretton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mohamed_S/0/1/0/all/0/1"&gt;Shakir Mohamed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning 1-Dimensional Submanifolds for Subsequent Inference on Random Dot Product Graphs. (arXiv:2004.07348v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.07348</id>
        <link href="http://arxiv.org/abs/2004.07348"/>
        <updated>2021-07-08T01:57:59.082Z</updated>
        <summary type="html"><![CDATA[A random dot product graph (RDPG) is a generative model for networks in which
vertices correspond to positions in a latent Euclidean space and edge
probabilities are determined by the dot products of the latent positions. We
consider RDPGs for which the latent positions are randomly sampled from an
unknown $1$-dimensional submanifold of the latent space. In principle,
restricted inference, i.e., procedures that exploit the structure of the
submanifold, should be more effective than unrestricted inference; however, it
is not clear how to conduct restricted inference when the submanifold is
unknown. We submit that techniques for manifold learning can be used to learn
the unknown submanifold well enough to realize benefit from restricted
inference. To illustrate, we test $1$- and $2$-sample hypotheses about the
Fr\'{e}chet means of small communities of vertices, using the complete set of
vertices to infer latent structure. We propose test statistics that deploy the
Isomap procedure for manifold learning, using shortest path distances on
neighborhood graphs constructed from estimated latent positions to estimate arc
lengths on the unknown $1$-dimensional submanifold. Unlike conventional
applications of Isomap, the estimated latent positions do not lie on the
submanifold of interest. We extend existing convergence results for Isomap to
this setting and use them to demonstrate that, as the number of auxiliary
vertices increases, the power of our test converges to the power of the
corresponding test when the submanifold is known.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Trosset_M/0/1/0/all/0/1"&gt;Michael W. Trosset&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gao_M/0/1/0/all/0/1"&gt;Mingyue Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tang_M/0/1/0/all/0/1"&gt;Minh Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Priebe_C/0/1/0/all/0/1"&gt;Carey E. Priebe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RAM-VO: Less is more in Visual Odometry. (arXiv:2107.02974v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.02974</id>
        <link href="http://arxiv.org/abs/2107.02974"/>
        <updated>2021-07-08T01:57:59.075Z</updated>
        <summary type="html"><![CDATA[Building vehicles capable of operating without human supervision requires the
determination of the agent's pose. Visual Odometry (VO) algorithms estimate the
egomotion using only visual changes from the input images. The most recent VO
methods implement deep-learning techniques using convolutional neural networks
(CNN) extensively, which add a substantial cost when dealing with
high-resolution images. Furthermore, in VO tasks, more input data does not mean
a better prediction; on the contrary, the architecture may filter out useless
information. Therefore, the implementation of computationally efficient and
lightweight architectures is essential. In this work, we propose the RAM-VO, an
extension of the Recurrent Attention Model (RAM) for visual odometry tasks.
RAM-VO improves the visual and temporal representation of information and
implements the Proximal Policy Optimization (PPO) algorithm to learn robust
policies. The results indicate that RAM-VO can perform regressions with six
degrees of freedom from monocular input images using approximately 3 million
parameters. In addition, experiments on the KITTI dataset demonstrate that
RAM-VO achieves competitive results using only 5.7% of the available visual
information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cleveston_I/0/1/0/all/0/1"&gt;Iury Cleveston&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colombini_E/0/1/0/all/0/1"&gt;Esther L. Colombini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Keiki: Towards Realistic Danmaku Generation via Sequential GANs. (arXiv:2107.02991v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.02991</id>
        <link href="http://arxiv.org/abs/2107.02991"/>
        <updated>2021-07-08T01:57:59.068Z</updated>
        <summary type="html"><![CDATA[Search-based procedural content generation methods have recently been
introduced for the autonomous creation of bullet hell games. Search-based
methods, however, can hardly model patterns of danmakus -- the bullet hell
shooting entity -- explicitly and the resulting levels often look
non-realistic. In this paper, we present a novel bullet hell game platform
named Keiki, which allows the representation of danmakus as a parametric
sequence which, in turn, can model the sequential behaviours of danmakus. We
employ three types of generative adversarial networks (GANs) and test Keiki
across three metrics designed to quantify the quality of the generated
danmakus. The time-series GAN and periodic spatial GAN show different yet
competitive performance in terms of the evaluation metrics adopted, their
deviation from human-designed danmakus, and the diversity of generated
danmakus. The preliminary experimental studies presented here showcase that
potential of time-series GANs for sequential content generation in games.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jialin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yannakakis_G/0/1/0/all/0/1"&gt;Georgios N. Yannakakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Are you sure?": Preliminary Insights from Scaling Product Comparisons to Multiple Shops. (arXiv:2107.03256v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.03256</id>
        <link href="http://arxiv.org/abs/2107.03256"/>
        <updated>2021-07-08T01:57:59.060Z</updated>
        <summary type="html"><![CDATA[Large eCommerce players introduced comparison tables as a new type of
recommendations. However, building comparisons at scale without pre-existing
training/taxonomy data remains an open challenge, especially within the
operational constraints of shops in the long tail. We present preliminary
results from building a comparison pipeline designed to scale in a multi-shop
scenario: we describe our design choices and run extensive benchmarks on
multiple shops to stress-test it. Finally, we run a small user study on
property selection and conclude by discussing potential improvements and
highlighting the questions that remain to be addressed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chia_P/0/1/0/all/0/1"&gt;Patrick John Chia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1"&gt;Bingqing Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tagliabue_J/0/1/0/all/0/1"&gt;Jacopo Tagliabue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diametrical Risk Minimization: Theory and Computations. (arXiv:1910.10844v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.10844</id>
        <link href="http://arxiv.org/abs/1910.10844"/>
        <updated>2021-07-08T01:57:59.053Z</updated>
        <summary type="html"><![CDATA[The theoretical and empirical performance of Empirical Risk Minimization
(ERM) often suffers when loss functions are poorly behaved with large Lipschitz
moduli and spurious sharp minimizers. We propose and analyze a counterpart to
ERM called Diametrical Risk Minimization (DRM), which accounts for worst-case
empirical risks within neighborhoods in parameter space. DRM has generalization
bounds that are independent of Lipschitz moduli for convex as well as nonconvex
problems and it can be implemented using a practical algorithm based on
stochastic gradient descent. Numerical results illustrate the ability of DRM to
find quality solutions with low generalization error in sharp empirical risk
landscapes from benchmark neural network classification problems with corrupted
labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Norton_M/0/1/0/all/0/1"&gt;Matthew Norton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Royset_J/0/1/0/all/0/1"&gt;Johannes O. Royset&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting with Confidence on Unseen Distributions. (arXiv:2107.03315v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03315</id>
        <link href="http://arxiv.org/abs/2107.03315"/>
        <updated>2021-07-08T01:57:59.047Z</updated>
        <summary type="html"><![CDATA[Recent work has shown that the performance of machine learning models can
vary substantially when models are evaluated on data drawn from a distribution
that is close to but different from the training distribution. As a result,
predicting model performance on unseen distributions is an important challenge.
Our work connects techniques from domain adaptation and predictive uncertainty
literature, and allows us to predict model accuracy on challenging unseen
distributions without access to labeled data. In the context of distribution
shift, distributional distances are often used to adapt models and improve
their performance on new domains, however accuracy estimation, or other forms
of predictive uncertainty, are often neglected in these investigations. Through
investigating a wide range of established distributional distances, such as
Frechet distance or Maximum Mean Discrepancy, we determine that they fail to
induce reliable estimates of performance under distribution shift. On the other
hand, we find that the difference of confidences (DoC) of a classifier's
predictions successfully estimates the classifier's performance change over a
variety of shifts. We specifically investigate the distinction between
synthetic and natural distribution shifts and observe that despite its
simplicity DoC consistently outperforms other quantifications of distributional
difference. $DoC$ reduces predictive error by almost half ($46\%$) on several
realistic and challenging distribution shifts, e.g., on the ImageNet-Vid-Robust
and ImageNet-Rendition datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guillory_D/0/1/0/all/0/1"&gt;Devin Guillory&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1"&gt;Vaishaal Shankar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ebrahimi_S/0/1/0/all/0/1"&gt;Sayna Ebrahimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1"&gt;Trevor Darrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1"&gt;Ludwig Schmidt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Extrapolation for Attribute-Enhanced Generation. (arXiv:2107.02968v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.02968</id>
        <link href="http://arxiv.org/abs/2107.02968"/>
        <updated>2021-07-08T01:57:59.040Z</updated>
        <summary type="html"><![CDATA[Attribute extrapolation in sample generation is challenging for deep neural
networks operating beyond the training distribution. We formulate a new task
for extrapolation in sequence generation, focusing on natural language and
proteins, and propose GENhance, a generative framework that enhances attributes
through a learned latent space. Trained on movie reviews and a computed protein
stability dataset, GENhance can generate strongly-positive text reviews and
highly stable protein sequences without being exposed to similar data during
training. We release our benchmark tasks and models to contribute to the study
of generative modeling extrapolation and data-driven design in biology and
chemistry.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1"&gt;Alvin Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madani_A/0/1/0/all/0/1"&gt;Ali Madani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_B/0/1/0/all/0/1"&gt;Ben Krause&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naik_N/0/1/0/all/0/1"&gt;Nikhil Naik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SelfCF: A Simple Framework for Self-supervised Collaborative Filtering. (arXiv:2107.03019v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.03019</id>
        <link href="http://arxiv.org/abs/2107.03019"/>
        <updated>2021-07-08T01:57:59.034Z</updated>
        <summary type="html"><![CDATA[Collaborative filtering (CF) is widely used to learn an informative latent
representation of a user or item from observed interactions. Existing CF-based
methods commonly adopt negative sampling to discriminate different items. That
is, observed user-item pairs are treated as positive instances; unobserved
pairs are considered as negative instances and are sampled under a defined
distribution for training. Training with negative sampling on large datasets is
computationally expensive. Further, negative items should be carefully sampled
under the defined distribution, in order to avoid selecting an observed
positive item in the training dataset. Unavoidably, some negative items sampled
from the training dataset could be positive in the test set. Recently,
self-supervised learning (SSL) has emerged as a powerful tool to learn a model
without negative samples. In this paper, we propose a self-supervised
collaborative filtering framework (SelfCF), that is specially designed for
recommender scenario with implicit feedback. The main idea of SelfCF is to
augment the output embeddings generated by backbone networks, because it is
infeasible to augment raw input of user/item ids. We propose and study three
output perturbation techniques that can be applied to different types of
backbone networks including both traditional CF models and graph-based models.
By encapsulating two popular recommendation models into the framework, our
experiments on three datasets show that the best performance of our framework
is comparable or better than the supervised counterpart. We also show that
SelfCF can boost up the performance by up to 8.93\% on average, compared with
another self-supervised framework as the baseline. Source codes are available
at: https://github.com/enoche/SelfCF.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1"&gt;Aixin Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1"&gt;Chunyan Miao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Modified Drake Equation for Assessing Adversarial Risk to Machine Learning Models. (arXiv:2103.02718v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02718</id>
        <link href="http://arxiv.org/abs/2103.02718"/>
        <updated>2021-07-08T01:57:58.989Z</updated>
        <summary type="html"><![CDATA[Machine learning models present a risk of adversarial attack when deployed in
production. Quantifying the contributing factors and uncertainties using
empirical measures could assist the industry with assessing the risk of
downloading and deploying common model types. This work proposes modifying the
traditional Drake Equation's formalism to estimate the number of potentially
successful adversarial attacks on a deployed model. The Drake Equation is
famously used for parameterizing uncertainties and it has been used in many
research fields outside of its original intentions to estimate the number of
radio-capable extra-terrestrial civilizations. While previous work has outlined
methods for discovering vulnerabilities in public model architectures, the
proposed equation seeks to provide a semi-quantitative benchmark for evaluating
and estimating the potential risk factors for adversarial attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kalin_J/0/1/0/all/0/1"&gt;Josh Kalin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noever_D/0/1/0/all/0/1"&gt;David Noever&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciolino_M/0/1/0/all/0/1"&gt;Matthew Ciolino&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantically Controllable Scene Generation with Guidance of Explicit Knowledge. (arXiv:2106.04066v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04066</id>
        <link href="http://arxiv.org/abs/2106.04066"/>
        <updated>2021-07-08T01:57:58.977Z</updated>
        <summary type="html"><![CDATA[Deep Generative Models (DGMs) are known for their superior capability in
generating realistic data. Extending purely data-driven approaches, recent
specialized DGMs may satisfy additional controllable requirements such as
embedding a traffic sign in a driving scene, by manipulating patterns
\textit{implicitly} in the neuron or feature level. In this paper, we introduce
a novel method to incorporate domain knowledge \textit{explicitly} in the
generation process to achieve semantically controllable scene generation. We
categorize our knowledge into two types to be consistent with the composition
of natural scenes, where the first type represents the property of objects and
the second type represents the relationship among objects. We then propose a
tree-structured generative model to learn complex scene representation, whose
nodes and edges are naturally corresponding to the two types of knowledge
respectively. Knowledge can be explicitly integrated to enable semantically
controllable scene generation by imposing semantic rules on properties of nodes
and edges in the tree structure. We construct a synthetic example to illustrate
the controllability and explainability of our method in a clean setting. We
further extend the synthetic example to realistic autonomous vehicle driving
environments and conduct extensive experiments to show that our method
efficiently identifies adversarial traffic scenes against different
state-of-the-art 3D point cloud segmentation models satisfying the traffic
rules specified as the explicit knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1"&gt;Wenhao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eun_K/0/1/0/all/0/1"&gt;Kim Ji Eun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Ding Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Supervised Bayesian Specification Inference from Demonstrations. (arXiv:2107.02912v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.02912</id>
        <link href="http://arxiv.org/abs/2107.02912"/>
        <updated>2021-07-08T01:57:58.964Z</updated>
        <summary type="html"><![CDATA[When observing task demonstrations, human apprentices are able to identify
whether a given task is executed correctly long before they gain expertise in
actually performing that task. Prior research into learning from demonstrations
(LfD) has failed to capture this notion of the acceptability of a task's
execution; meanwhile, temporal logics provide a flexible language for
expressing task specifications. Inspired by this, we present Bayesian
specification inference, a probabilistic model for inferring task specification
as a temporal logic formula. We incorporate methods from probabilistic
programming to define our priors, along with a domain-independent likelihood
function to enable sampling-based inference. We demonstrate the efficacy of our
model for inferring specifications, with over 90% similarity observed between
the inferred specification and the ground truth, both within a synthetic domain
and during a real-world table setting task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1"&gt;Ankit Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamath_P/0/1/0/all/0/1"&gt;Pritish Kamath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Craven_P/0/1/0/all/0/1"&gt;Patrick Craven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Landers_K/0/1/0/all/0/1"&gt;Kevin Landers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oden_K/0/1/0/all/0/1"&gt;Kevin Oden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1"&gt;Julie Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving the scalarization issues of Advantage-based Reinforcement Learning Algorithms. (arXiv:2004.04120v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.04120</id>
        <link href="http://arxiv.org/abs/2004.04120"/>
        <updated>2021-07-08T01:57:58.956Z</updated>
        <summary type="html"><![CDATA[In this research, some of the issues that arise from the scalarization of the
multi-objective optimization problem in the Advantage Actor Critic (A2C)
reinforcement learning algorithm are investigated. The paper shows how a naive
scalarization can lead to gradients overlapping. Furthermore, the possibility
that the entropy regularization term can be a source of uncontrolled noise is
discussed. With respect to the above issues, a technique to avoid gradient
overlapping is proposed, while keeping the same loss formulation. Moreover, a
method to avoid the uncontrolled noise, by sampling the actions from
distributions with a desired minimum entropy, is investigated. Pilot
experiments have been carried out to show how the proposed method speeds up the
training. The proposed approach can be applied to any Advantage-based
Reinforcement Learning algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Galatolo_F/0/1/0/all/0/1"&gt;Federico A. Galatolo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cimino_M/0/1/0/all/0/1"&gt;Mario G.C.A. Cimino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vaglini_G/0/1/0/all/0/1"&gt;Gigliola Vaglini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Matrix-Free Approximations of Second-Order Information, with Applications to Pruning and Optimization. (arXiv:2107.03356v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03356</id>
        <link href="http://arxiv.org/abs/2107.03356"/>
        <updated>2021-07-08T01:57:58.937Z</updated>
        <summary type="html"><![CDATA[Efficiently approximating local curvature information of the loss function is
a key tool for optimization and compression of deep neural networks. Yet, most
existing methods to approximate second-order information have high
computational or storage costs, which can limit their practicality. In this
work, we investigate matrix-free, linear-time approaches for estimating
Inverse-Hessian Vector Products (IHVPs) for the case when the Hessian can be
approximated as a sum of rank-one matrices, as in the classic approximation of
the Hessian by the empirical Fisher matrix. We propose two new algorithms as
part of a framework called M-FAC: the first algorithm is tailored towards
network compression and can compute the IHVP for dimension $d$, if the Hessian
is given as a sum of $m$ rank-one matrices, using $O(dm^2)$ precomputation,
$O(dm)$ cost for computing the IHVP, and query cost $O(m)$ for any single
element of the inverse Hessian. The second algorithm targets an optimization
setting, where we wish to compute the product between the inverse Hessian,
estimated over a sliding window of optimization steps, and a given gradient
direction, as required for preconditioned SGD. We give an algorithm with cost
$O(dm + m^2)$ for computing the IHVP and $O(dm + m^3)$ for adding or removing
any gradient from the sliding window. These two algorithms yield
state-of-the-art results for network pruning and optimization with lower
computational overhead relative to existing second-order methods.
Implementations are available at [10] and [18].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Frantar_E/0/1/0/all/0/1"&gt;Elias Frantar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurtic_E/0/1/0/all/0/1"&gt;Eldar Kurtic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1"&gt;Dan Alistarh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discriminative Mutual Information Estimators for Channel Capacity Learning. (arXiv:2107.03084v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2107.03084</id>
        <link href="http://arxiv.org/abs/2107.03084"/>
        <updated>2021-07-08T01:57:58.909Z</updated>
        <summary type="html"><![CDATA[Channel capacity plays a crucial role in the development of modern
communication systems as it represents the maximum rate at which information
can be reliably transmitted over a communication channel. Nevertheless, for the
majority of channels, finding a closed-form capacity expression remains an open
challenge. This is because it requires to carry out two formidable tasks a) the
computation of the mutual information between the channel input and output, and
b) its maximization with respect to the signal distribution at the channel
input. In this paper, we address both tasks. Inspired by implicit generative
models, we propose a novel cooperative framework to automatically learn the
channel capacity, for any type of memory-less channel. In particular, we
firstly develop a new methodology to estimate the mutual information directly
from a discriminator typically deployed to train adversarial networks, referred
to as discriminative mutual information estimator (DIME). Secondly, we include
the discriminator in a cooperative channel capacity learning framework,
referred to as CORTICAL, where a discriminator learns to distinguish between
dependent and independent channel input-output samples while a generator learns
to produce the optimal channel input distribution for which the discriminator
exhibits the best performance. Lastly, we prove that a particular choice of the
cooperative value function solves the channel capacity estimation problem.
Simulation results demonstrate that the proposed method offers high accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Letizia_N/0/1/0/all/0/1"&gt;Nunzio A. Letizia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tonello_A/0/1/0/all/0/1"&gt;Andrea M. Tonello&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-VAE: Learning Disentangled View-common and View-peculiar Visual Representations for Multi-view Clustering. (arXiv:2106.11232v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11232</id>
        <link href="http://arxiv.org/abs/2106.11232"/>
        <updated>2021-07-08T01:57:58.646Z</updated>
        <summary type="html"><![CDATA[Multi-view clustering, a long-standing and important research problem,
focuses on mining complementary information from diverse views. However,
existing works often fuse multiple views' representations or handle clustering
in a common feature space, which may result in their entanglement especially
for visual representations. To address this issue, we present a novel VAE-based
multi-view clustering framework (Multi-VAE) by learning disentangled visual
representations. Concretely, we define a view-common variable and multiple
view-peculiar variables in the generative model. The prior of view-common
variable obeys approximately discrete Gumbel Softmax distribution, which is
introduced to extract the common cluster factor of multiple views. Meanwhile,
the prior of view-peculiar variable follows continuous Gaussian distribution,
which is used to represent each view's peculiar visual factors. By controlling
the mutual information capacity to disentangle the view-common and
view-peculiar representations, continuous visual information of multiple views
can be separated so that their common discrete cluster information can be
effectively mined. Experimental results demonstrate that Multi-VAE enjoys the
disentangled and explainable visual representations, while obtaining superior
clustering performance compared with state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jie Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yazhou Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Huayi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_X/0/1/0/all/0/1"&gt;Xiaorong Pu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaofeng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1"&gt;Ming Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lifang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anomaly detection and automatic labeling for solar cell quality inspection based on Generative Adversarial Network. (arXiv:2103.03518v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03518</id>
        <link href="http://arxiv.org/abs/2103.03518"/>
        <updated>2021-07-08T01:57:58.583Z</updated>
        <summary type="html"><![CDATA[Quality inspection applications in industry are required to move towards a
zero-defect manufacturing scenario, withnon-destructive inspection and
traceability of 100 % of produced parts. Developing robust fault detection and
classification modelsfrom the start-up of the lines is challenging due to the
difficulty in getting enough representative samples of the faulty patternsand
the need to manually label them. This work presents a methodology to develop a
robust inspection system, targeting thesepeculiarities, in the context of solar
cell manufacturing. The methodology is divided into two phases: In the first
phase, an anomalydetection model based on a Generative Adversarial Network
(GAN) is employed. This model enables the detection and localizationof
anomalous patterns within the solar cells from the beginning, using only
non-defective samples for training and without anymanual labeling involved. In
a second stage, as defective samples arise, the detected anomalies will be used
as automaticallygenerated annotations for the supervised training of a Fully
Convolutional Network that is capable of detecting multiple types offaults. The
experimental results using 1873 EL images of monocrystalline cells show that
(a) the anomaly detection scheme can beused to start detecting features with
very little available data, (b) the anomaly detection may serve as automatic
labeling in order totrain a supervised model, and (c) segmentation and
classification results of supervised models trained with automatic labels
arecomparable to the ones obtained from the models trained with manual labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balzategui_J/0/1/0/all/0/1"&gt;Julen Balzategui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eciolaza_L/0/1/0/all/0/1"&gt;Luka Eciolaza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maestro_Watson_D/0/1/0/all/0/1"&gt;Daniel Maestro-Watson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auxiliary Diagnosing Coronary Stenosis Using Machine Learning. (arXiv:2007.10316v3 [q-bio.TO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.10316</id>
        <link href="http://arxiv.org/abs/2007.10316"/>
        <updated>2021-07-08T01:57:58.575Z</updated>
        <summary type="html"><![CDATA[How to accurately classify and diagnose whether an individual has Coronary
Stenosis (CS) without invasive physical examination? This problem has not been
solved satisfactorily. To this end, the four machine learning (ML) algorithms,
i.e., Boosted Tree (BT), Decision Tree (DT), Logistic Regression (LR) and
Random Forest (RF) are employed in this paper. First, eleven features including
basic information of an individual, symptoms and results of routine physical
examination are selected, as well as one label is specified, indicating whether
an individual suffers from different severity of coronary artery stenosis or
not. On the basis of it, a sample set is constructed. Second, each of these
four ML algorithms learns from the sample set to obtain the corresponding
optimal classified results, respectively. The experimental results show that:
RF performs better than other three algorithms, and the former algorithm
classifies whether an individual has CS with an accuracy of 95.7% (=90/94).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhu_W/0/1/0/all/0/1"&gt;Weijun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+LU_F/0/1/0/all/0/1"&gt;Fengyuan LU&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaoyu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+LI_E/0/1/0/all/0/1"&gt;En LI&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Atmospheric Data and Identifying Dynamics: Temporal Data-Driven Modeling of Air Pollutants. (arXiv:2010.06538v2 [stat.AP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.06538</id>
        <link href="http://arxiv.org/abs/2010.06538"/>
        <updated>2021-07-08T01:57:58.548Z</updated>
        <summary type="html"><![CDATA[Atmospheric modeling has recently experienced a surge with the advent of deep
learning. Most of these models, however, predict concentrations of pollutants
following a data-driven approach in which the physical laws that govern their
behaviors and relationships remain hidden. With the aid of real-world air
quality data collected hourly in different stations throughout Madrid, we
present an empirical approach using data-driven techniques with the following
goals: (1) Find parsimonious systems of ordinary differential equations via
sparse identification of nonlinear dynamics (SINDy) that model the
concentration of pollutants and their changes over time; (2) assess the
performance and limitations of our models using stability analysis; (3)
reconstruct the time series of chemical pollutants not measured in certain
stations using delay coordinate embedding results. Our results show that
Akaike's Information Criterion can work well in conjunction with best subset
regression as to find an equilibrium between sparsity and goodness of fit. We
also find that, due to the complexity of the chemical system under study,
identifying the dynamics of this system over longer periods of time require
higher levels of data filtering and smoothing. Stability analysis for the
reconstructed ordinary differential equations (ODEs) reveals that more than
half of the physically relevant critical points are saddle points, suggesting
that the system is unstable even under the idealized assumption that all
environmental conditions are constant over time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Rubio_Herrero_J/0/1/0/all/0/1"&gt;Javier Rubio-Herrero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Marrero_C/0/1/0/all/0/1"&gt;Carlos Ortiz Marrero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Fan_W/0/1/0/all/0/1"&gt;Wai-Tong Louis Fan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Architecture Pruning for Transfer Learning. (arXiv:2107.03375v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03375</id>
        <link href="http://arxiv.org/abs/2107.03375"/>
        <updated>2021-07-08T01:57:58.541Z</updated>
        <summary type="html"><![CDATA[We propose a new gradient-based approach for extracting sub-architectures
from a given large model. Contrarily to existing pruning methods, which are
unable to disentangle the network architecture and the corresponding weights,
our architecture-pruning scheme produces transferable new structures that can
be successfully retrained to solve different tasks. We focus on a
transfer-learning setup where architectures can be trained on a large data set
but very few data points are available for fine-tuning them on new tasks. We
define a new gradient-based algorithm that trains architectures of arbitrarily
low complexity independently from the attached weights. Given a search space
defined by an existing large neural model, we reformulate the architecture
search task as a complexity-penalized subset-selection problem and solve it
through a two-temperature relaxation scheme. We provide theoretical convergence
guarantees and validate the proposed transfer-learning strategy on real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Colombo_N/0/1/0/all/0/1"&gt;Nicolo Colombo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yang Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Representation Models for Text Classification with AutoML Tools. (arXiv:2106.12798v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12798</id>
        <link href="http://arxiv.org/abs/2106.12798"/>
        <updated>2021-07-08T01:57:58.535Z</updated>
        <summary type="html"><![CDATA[Automated Machine Learning (AutoML) has gained increasing success on tabular
data in recent years. However, processing unstructured data like text is a
challenge and not widely supported by open-source AutoML tools. This work
compares three manually created text representations and text embeddings
automatically created by AutoML tools. Our benchmark includes four popular
open-source AutoML tools and eight datasets for text classification purposes.
The results show that straightforward text representations perform better than
AutoML tools with automatically created text embeddings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brandle_S/0/1/0/all/0/1"&gt;Sebastian Br&amp;#xe4;ndle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanussek_M/0/1/0/all/0/1"&gt;Marc Hanussek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blohm_M/0/1/0/all/0/1"&gt;Matthias Blohm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kintz_M/0/1/0/all/0/1"&gt;Maximilien Kintz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An algorithmic view of $\ell_2$ regularization and some path-following algorithms. (arXiv:2107.03322v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.03322</id>
        <link href="http://arxiv.org/abs/2107.03322"/>
        <updated>2021-07-08T01:57:58.527Z</updated>
        <summary type="html"><![CDATA[We establish an equivalence between the $\ell_2$-regularized solution path
for a convex loss function, and the solution of an ordinary differentiable
equation (ODE). Importantly, this equivalence reveals that the solution path
can be viewed as the flow of a hybrid of gradient descent and Newton method
applying to the empirical loss, which is similar to a widely used optimization
technique called trust region method. This provides an interesting algorithmic
view of $\ell_2$ regularization, and is in contrast to the conventional view
that the $\ell_2$ regularization solution path is similar to the gradient flow
of the empirical loss.New path-following algorithms based on homotopy methods
and numerical ODE solvers are proposed to numerically approximate the solution
path. In particular, we consider respectively Newton method and gradient
descent method as the basis algorithm for the homotopy method, and establish
their approximation error rates over the solution path. Importantly, our theory
suggests novel schemes to choose grid points that guarantee an arbitrarily
small suboptimality for the solution path. In terms of computational cost, we
prove that in order to achieve an $\epsilon$-suboptimality for the entire
solution path, the number of Newton steps required for the Newton method is
$\mathcal O(\epsilon^{-1/2})$, while the number of gradient steps required for
the gradient descent method is $\mathcal O\left(\epsilon^{-1}
\ln(\epsilon^{-1})\right)$. Finally, we use $\ell_2$-regularized logistic
regression as an illustrating example to demonstrate the effectiveness of the
proposed path-following algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yunzhang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Liu_R/0/1/0/all/0/1"&gt;Renxiong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RISAN: Robust Instance Specific Abstention Network. (arXiv:2107.03090v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03090</id>
        <link href="http://arxiv.org/abs/2107.03090"/>
        <updated>2021-07-08T01:57:58.520Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose deep architectures for learning instance specific
abstain (reject option) binary classifiers. The proposed approach uses double
sigmoid loss function as described by Kulin Shah and Naresh Manwani in ("Online
Active Learning of Reject Option Classifiers", AAAI, 2020), as a performance
measure. We show that the double sigmoid loss is classification calibrated. We
also show that the excess risk of 0-d-1 loss is upper bounded by the excess
risk of double sigmoid loss. We derive the generalization error bounds for the
proposed architecture for reject option classifiers. To show the effectiveness
of the proposed approach, we experiment with several real world datasets. We
observe that the proposed approach not only performs comparable to the
state-of-the-art approaches, it is also robust against label noise. We also
provide visualizations to observe the important features learned by the network
corresponding to the abstaining decision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kalra_B/0/1/0/all/0/1"&gt;Bhavya Kalra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_K/0/1/0/all/0/1"&gt;Kulin Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manwani_N/0/1/0/all/0/1"&gt;Naresh Manwani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ICDAR 2021 Competition on Components Segmentation Task of Document Photos. (arXiv:2106.08499v1 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2106.08499</id>
        <link href="http://arxiv.org/abs/2106.08499"/>
        <updated>2021-07-08T01:57:58.513Z</updated>
        <summary type="html"><![CDATA[This paper describes the short-term competition on Components Segmentation
Task of Document Photos that was prepared in the context of the 16th
International Conference on Document Analysis and Recognition (ICDAR 2021).
This competition aims to bring together researchers working on the filed of
identification document image processing and provides them a suitable benchmark
to compare their techniques on the component segmentation task of document
images. Three challenge tasks were proposed entailing different segmentation
assignments to be performed on a provided dataset. The collected data are from
several types of Brazilian ID documents, whose personal information was
conveniently replaced. There were 16 participants whose results obtained for
some or all the three tasks show different rates for the adopted metrics, like
Dice Similarity Coefficient ranging from 0.06 to 0.99. Different Deep Learning
models were applied by the entrants with diverse strategies to achieve the best
results in each of the tasks. Obtained results show that the current applied
methods for solving one of the proposed tasks (document boundary detection) are
already well stablished. However, for the other two challenge tasks (text zone
and handwritten sign detection) research and development of more robust
approaches are still required to achieve acceptable results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Junior_C/0/1/0/all/0/1"&gt;Celso A. M. Lopes Junior&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Junior_R/0/1/0/all/0/1"&gt;Ricardo B. das Neves Junior&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bezerra_B/0/1/0/all/0/1"&gt;Byron L. D. Bezerra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toselli_A/0/1/0/all/0/1"&gt;Alejandro H. Toselli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Impedovo_D/0/1/0/all/0/1"&gt;Donato Impedovo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combined Global and Local Search for Optimization with Gaussian Process Models. (arXiv:2107.03217v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.03217</id>
        <link href="http://arxiv.org/abs/2107.03217"/>
        <updated>2021-07-08T01:57:58.489Z</updated>
        <summary type="html"><![CDATA[Gaussian process (GP) model based optimization is widely applied in
simulation and machine learning. In general, it first estimates a GP model
based on a few observations from the true response and then employs this model
to guide the search, aiming to quickly locate the global optimum. Despite its
successful applications, it has several limitations that may hinder its broader
usage. First, building an accurate GP model can be difficult and
computationally expensive, especially when the response function is multi-modal
or varies significantly over the design space. Second, even with an appropriate
model, the search process can be trapped in suboptimal regions before moving to
the global optimum due to the excessive effort spent around the current best
solution. In this work, we adopt the Additive Global and Local GP (AGLGP) model
in the optimization framework. The model is rooted in the inducing-points-based
GP sparse approximations and is combined with independent local models in
different regions. With these properties, the AGLGP model is suitable for
multi-modal responses with relatively large data sizes. Based on this AGLGP
model, we propose a Combined Global and Local search for Optimization (CGLO)
algorithm. It first divides the whole design space into disjoint local regions
and identifies a promising region with the global model. Next, a local model in
the selected region is fit to guide detailed search within this region. The
algorithm then switches back to the global step when a good local solution is
found. The global and local natures of CGLO enable it to enjoy the benefits of
both global and local search to efficiently locate the global optimum.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Meng_Q/0/1/0/all/0/1"&gt;Qun Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1"&gt;Songhao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ng_S/0/1/0/all/0/1"&gt;Szu Hui Ng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Predict Error for MRI Reconstruction. (arXiv:2002.05582v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.05582</id>
        <link href="http://arxiv.org/abs/2002.05582"/>
        <updated>2021-07-08T01:57:58.483Z</updated>
        <summary type="html"><![CDATA[In healthcare applications, predictive uncertainty has been used to assess
predictive accuracy. In this paper, we demonstrate that predictive uncertainty
estimated by the current methods does not highly correlate with prediction
error by decomposing the latter into random and systematic errors, and showing
that the former is equivalent to the variance of the random error. In addition,
we observe that current methods unnecessarily compromise performance by
modifying the model and training loss to estimate the target and uncertainty
jointly. We show that estimating them separately without modifications improves
performance. Following this, we propose a novel method that estimates the
target labels and magnitude of the prediction error in two steps. We
demonstrate this method on a large-scale MRI reconstruction task, and achieve
significantly better results than the state-of-the-art uncertainty estimation
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shi Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pezzotti_N/0/1/0/all/0/1"&gt;Nicola Pezzotti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1"&gt;Max Welling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Shape Completion via IMLE. (arXiv:2106.16237v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.16237</id>
        <link href="http://arxiv.org/abs/2106.16237"/>
        <updated>2021-07-08T01:57:58.477Z</updated>
        <summary type="html"><![CDATA[Shape completion is the problem of completing partial input shapes such as
partial scans. This problem finds important applications in computer vision and
robotics due to issues such as occlusion or sparsity in real-world data.
However, most of the existing research related to shape completion has been
focused on completing shapes by learning a one-to-one mapping which limits the
diversity and creativity of the produced results. We propose a novel multimodal
shape completion technique that is effectively able to learn a one-to-many
mapping and generates diverse complete shapes. Our approach is based on the
conditional Implicit MaximumLikelihood Estimation (IMLE) technique wherein we
condition our inputs on partial 3D point clouds. We extensively evaluate our
approach by comparing it to various baselines both quantitatively and
qualitatively. We show that our method is superior to alternatives in terms of
completeness and diversity of shapes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arora_H/0/1/0/all/0/1"&gt;Himanshu Arora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Saurabh Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1"&gt;Shichong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Ke Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahdavi_Amiri_A/0/1/0/all/0/1"&gt;Ali Mahdavi-Amiri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ICDAR 2021 Competition on Components Segmentation Task of Document Photos. (arXiv:2106.08499v1 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2106.08499</id>
        <link href="http://arxiv.org/abs/2106.08499"/>
        <updated>2021-07-08T01:57:58.470Z</updated>
        <summary type="html"><![CDATA[This paper describes the short-term competition on Components Segmentation
Task of Document Photos that was prepared in the context of the 16th
International Conference on Document Analysis and Recognition (ICDAR 2021).
This competition aims to bring together researchers working on the filed of
identification document image processing and provides them a suitable benchmark
to compare their techniques on the component segmentation task of document
images. Three challenge tasks were proposed entailing different segmentation
assignments to be performed on a provided dataset. The collected data are from
several types of Brazilian ID documents, whose personal information was
conveniently replaced. There were 16 participants whose results obtained for
some or all the three tasks show different rates for the adopted metrics, like
Dice Similarity Coefficient ranging from 0.06 to 0.99. Different Deep Learning
models were applied by the entrants with diverse strategies to achieve the best
results in each of the tasks. Obtained results show that the current applied
methods for solving one of the proposed tasks (document boundary detection) are
already well stablished. However, for the other two challenge tasks (text zone
and handwritten sign detection) research and development of more robust
approaches are still required to achieve acceptable results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Junior_C/0/1/0/all/0/1"&gt;Celso A. M. Lopes Junior&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Junior_R/0/1/0/all/0/1"&gt;Ricardo B. das Neves Junior&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bezerra_B/0/1/0/all/0/1"&gt;Byron L. D. Bezerra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toselli_A/0/1/0/all/0/1"&gt;Alejandro H. Toselli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Impedovo_D/0/1/0/all/0/1"&gt;Donato Impedovo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Big Data Information and Nowcasting: Consumption and Investment from Bank Transactions in Turkey. (arXiv:2107.03299v1 [econ.EM])]]></title>
        <id>http://arxiv.org/abs/2107.03299</id>
        <link href="http://arxiv.org/abs/2107.03299"/>
        <updated>2021-07-08T01:57:58.464Z</updated>
        <summary type="html"><![CDATA[We use the aggregate information from individual-to-firm and firm-to-firm in
Garanti BBVA Bank transactions to mimic domestic private demand. Particularly,
we replicate the quarterly national accounts aggregate consumption and
investment (gross fixed capital formation) and its bigger components (Machinery
and Equipment and Construction) in real time for the case of Turkey. In order
to validate the usefulness of the information derived from these indicators we
test the nowcasting ability of both indicators to nowcast the Turkish GDP using
different nowcasting models. The results are successful and confirm the
usefulness of Consumption and Investment Banking transactions for nowcasting
purposes. The value of the Big data information is more relevant at the
beginning of the nowcasting process, when the traditional hard data information
is scarce. This makes this information specially relevant for those countries
where statistical release lags are longer like the Emerging Markets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/econ/1/au:+Barlas_A/0/1/0/all/0/1"&gt;Ali B. Barlas&lt;/a&gt; (BBVA Research), &lt;a href="http://arxiv.org/find/econ/1/au:+Mert_S/0/1/0/all/0/1"&gt;Seda Guler Mert&lt;/a&gt; (BBVA Research), &lt;a href="http://arxiv.org/find/econ/1/au:+Isa_B/0/1/0/all/0/1"&gt;Berk Orkun Isa&lt;/a&gt; (BBVA Research) &lt;a href="http://arxiv.org/find/econ/1/au:+Ortiz_A/0/1/0/all/0/1"&gt;Alvaro Ortiz&lt;/a&gt; (BBVA Research), &lt;a href="http://arxiv.org/find/econ/1/au:+Rodrigo_T/0/1/0/all/0/1"&gt;Tomasa Rodrigo&lt;/a&gt; (BBVA Research), &lt;a href="http://arxiv.org/find/econ/1/au:+Soybilgen_B/0/1/0/all/0/1"&gt;Baris Soybilgen&lt;/a&gt; (Bilgi University), &lt;a href="http://arxiv.org/find/econ/1/au:+Yazgan_E/0/1/0/all/0/1"&gt;Ege Yazgan&lt;/a&gt; (Bilgi University)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximate Multi-Agent Fitted Q Iteration. (arXiv:2104.09343v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09343</id>
        <link href="http://arxiv.org/abs/2104.09343"/>
        <updated>2021-07-08T01:57:58.444Z</updated>
        <summary type="html"><![CDATA[We formulate an efficient approximation for multi-agent batch reinforcement
learning, the approximate multi-agent fitted Q iteration (AMAFQI). We present a
detailed derivation of our approach. We propose an iterative policy search and
show that it yields a greedy policy with respect to multiple approximations of
the centralized, standard Q-function. In each iteration and policy evaluation,
AMAFQI requires a number of computations that scales linearly with the number
of agents whereas the analogous number of computations increase exponentially
for the fitted Q iteration (FQI), one of the most commonly used approaches in
batch reinforcement learning. This property of AMAFQI is fundamental for the
design of a tractable multi-agent approach. We evaluate the performance of
AMAFQI and compare it to FQI in numerical simulations. Numerical examples
illustrate the significant computation time reduction when using AMAFQI instead
of FQI in multi-agent problems and corroborate the similar decision-making
performance of both approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lesage_Landry_A/0/1/0/all/0/1"&gt;Antoine Lesage-Landry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Callaway_D/0/1/0/all/0/1"&gt;Duncan S. Callaway&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the Security of Deepfake Detection. (arXiv:2107.02045v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02045</id>
        <link href="http://arxiv.org/abs/2107.02045"/>
        <updated>2021-07-08T01:57:58.438Z</updated>
        <summary type="html"><![CDATA[Deepfakes pose growing challenges to the trust of information on the
Internet. Thus, detecting deepfakes has attracted increasing attentions from
both academia and industry. State-of-the-art deepfake detection methods consist
of two key components, i.e., face extractor and face classifier, which extract
the face region in an image and classify it to be real/fake, respectively.
Existing studies mainly focused on improving the detection performance in
non-adversarial settings, leaving security of deepfake detection in adversarial
settings largely unexplored. In this work, we aim to bridge the gap. In
particular, we perform a systematic measurement study to understand the
security of the state-of-the-art deepfake detection methods in adversarial
settings. We use two large-scale public deepfakes data sources including
FaceForensics++ and Facebook Deepfake Detection Challenge, where the deepfakes
are fake face images; and we train state-of-the-art deepfake detection methods.
These detection methods can achieve 0.94--0.99 accuracies in non-adversarial
settings on these datasets. However, our measurement results uncover multiple
security limitations of the deepfake detection methods in adversarial settings.
First, we find that an attacker can evade a face extractor, i.e., the face
extractor fails to extract the correct face regions, via adding small Gaussian
noise to its deepfake images. Second, we find that a face classifier trained
using deepfakes generated by one method cannot detect deepfakes generated by
another method, i.e., an attacker can evade detection via generating deepfakes
using a new method. Third, we find that an attacker can leverage backdoor
attacks developed by the adversarial machine learning community to evade a face
classifier. Our results highlight that deepfake detection should consider the
adversarial nature of the problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1"&gt;Xiaoyu Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1"&gt;Neil Zhenqiang Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Formal derivation of Mesh Neural Networks with their Forward-Only gradient Propagation. (arXiv:1905.06684v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.06684</id>
        <link href="http://arxiv.org/abs/1905.06684"/>
        <updated>2021-07-08T01:57:58.431Z</updated>
        <summary type="html"><![CDATA[This paper proposes the Mesh Neural Network (MNN), a novel architecture which
allows neurons to be connected in any topology, to efficiently route
information. In MNNs, information is propagated between neurons throughout a
state transition function. State and error gradients are then directly computed
from state updates without backward computation. The MNN architecture and the
error propagation schema is formalized and derived in tensor algebra. The
proposed computational model can fully supply a gradient descent process, and
is potentially suitable for very large scale sparse NNs, due to its
expressivity and training efficiency, with respect to NNs based on
back-propagation and computational graphs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Galatolo_F/0/1/0/all/0/1"&gt;Federico A. Galatolo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cimino_M/0/1/0/all/0/1"&gt;Mario G.C.A. Cimino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vaglini_G/0/1/0/all/0/1"&gt;Gigliola Vaglini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the performance of EEG decoding using anchored-STFT in conjunction with gradient norm adversarial augmentation. (arXiv:2011.14694v3 [q-bio.QM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14694</id>
        <link href="http://arxiv.org/abs/2011.14694"/>
        <updated>2021-07-08T01:57:58.422Z</updated>
        <summary type="html"><![CDATA[Brain-computer interfaces (BCIs) enable direct communication between humans
and machines by translating brain activity into control commands. EEG is one of
the most common sources of neural signals because of its inexpensive and
non-invasive nature. However, interpretation of EEG signals is non-trivial
because EEG signals have a low spatial resolution and are often distorted with
noise and artifacts. Therefore, it is possible that meaningful patterns for
classifying EEG signals are deeply hidden. Nowadays, state-of-the-art
deep-learning algorithms have proven to be quite efficient in learning hidden,
meaningful patterns. However, the performance of the deep learning algorithms
depends upon the quality and the amount of the provided training data. Hence, a
better input formation (feature extraction) technique and a generative model to
produce high-quality data can enable the deep learning algorithms to adapt high
generalization quality. In this study, we proposed a novel input formation
(feature extraction) method in conjunction with a novel deep learning based
generative model to harness new training examples. The feature vectors are
extracted using a modified Short Time Fourier Transform (STFT) called
anchored-STFT. Anchored-STFT, inspired by wavelet transform, tries to minimize
the tradeoff between time and frequency resolution. As a result, it extracts
the inputs (feature vectors) with better time and frequency resolution compared
to the standard STFT. Secondly, we introduced a novel generative adversarial
data augmentation technique called gradient norm adversarial augmentation
(GNAA) for generating more training data. Thirdly, we investigated the
existence and significance of adversarial inputs in EEG data. Our approach
obtained the kappa value of 0.814 for BCI competition II dataset III and 0.755
for BCI competition IV dataset 2b for session-to-session transfer on test data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Ali_O/0/1/0/all/0/1"&gt;Omair Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Saif_ur_Rehman_M/0/1/0/all/0/1"&gt;Muhammad Saif-ur-Rehman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Dyck_S/0/1/0/all/0/1"&gt;Susanne Dyck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Glasmachers_T/0/1/0/all/0/1"&gt;Tobias Glasmachers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Iossifidis_I/0/1/0/all/0/1"&gt;Ioannis Iossifidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Klaes_C/0/1/0/all/0/1"&gt;Christian Klaes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transfer Learning in Information Criteria-based Feature Selection. (arXiv:2107.02847v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.02847</id>
        <link href="http://arxiv.org/abs/2107.02847"/>
        <updated>2021-07-08T01:57:58.414Z</updated>
        <summary type="html"><![CDATA[This paper investigates the effectiveness of transfer learning based on
Mallows' Cp. We propose a procedure that combines transfer learning with
Mallows' Cp (TLCp) and prove that it outperforms the conventional Mallows' Cp
criterion in terms of accuracy and stability. Our theoretical results indicate
that, for any sample size in the target domain, the proposed TLCp estimator
performs better than the Cp estimator by the mean squared error (MSE) metric in
the case of orthogonal predictors, provided that i) the dissimilarity between
the tasks from source domain and target domain is small, and ii) the procedure
parameters (complexity penalties) are tuned according to certain explicit
rules. Moreover, we show that our transfer learning framework can be extended
to other feature selection criteria, such as the Bayesian information
criterion. By analyzing the solution of the orthogonalized Cp, we identify an
estimator that asymptotically approximates the solution of the Cp criterion in
the case of non-orthogonal predictors. Similar results are obtained for the
non-orthogonal TLCp. Finally, simulation studies and applications with real
data demonstrate the usefulness of the TLCp scheme.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shaohan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sahinidis_N/0/1/0/all/0/1"&gt;Nikolaos V. Sahinidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gao_C/0/1/0/all/0/1"&gt;Chuanhou Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solution of Physics-based Bayesian Inverse Problems with Deep Generative Priors. (arXiv:2107.02926v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.02926</id>
        <link href="http://arxiv.org/abs/2107.02926"/>
        <updated>2021-07-08T01:57:58.392Z</updated>
        <summary type="html"><![CDATA[Inverse problems are notoriously difficult to solve because they can have no
solutions, multiple solutions, or have solutions that vary significantly in
response to small perturbations in measurements. Bayesian inference, which
poses an inverse problem as a stochastic inference problem, addresses these
difficulties and provides quantitative estimates of the inferred field and the
associated uncertainty. However, it is difficult to employ when inferring
vectors of large dimensions, and/or when prior information is available through
previously acquired samples. In this paper, we describe how deep generative
adversarial networks can be used to represent the prior distribution in
Bayesian inference and overcome these challenges. We apply these ideas to
inverse problems that are diverse in terms of the governing physical
principles, sources of prior knowledge, type of measurement, and the extent of
available information about measurement noise. In each case we apply the
proposed approach to infer the most likely solution and quantitative estimates
of uncertainty.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Patel_D/0/1/0/all/0/1"&gt;Dhruv V Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ray_D/0/1/0/all/0/1"&gt;Deep Ray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Oberai_A/0/1/0/all/0/1"&gt;Assad A Oberai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interflow: Aggregating Multi-layer Feature Mappings with Attention Mechanism. (arXiv:2106.14073v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14073</id>
        <link href="http://arxiv.org/abs/2106.14073"/>
        <updated>2021-07-08T01:57:58.385Z</updated>
        <summary type="html"><![CDATA[Traditionally, CNN models possess hierarchical structures and utilize the
feature mapping of the last layer to obtain the prediction output. However, it
can be difficulty to settle the optimal network depth and make the middle
layers learn distinguished features. This paper proposes the Interflow
algorithm specially for traditional CNN models. Interflow divides CNNs into
several stages according to the depth and makes predictions by the feature
mappings in each stage. Subsequently, we input these prediction branches into a
well-designed attention module, which learns the weights of these prediction
branches, aggregates them and obtains the final output. Interflow weights and
fuses the features learned in both shallower and deeper layers, making the
feature information at each stage processed reasonably and effectively,
enabling the middle layers to learn more distinguished features, and enhancing
the model representation ability. In addition, Interflow can alleviate gradient
vanishing problem, lower the difficulty of network depth selection, and lighten
possible over-fitting problem by introducing attention mechanism. Besides, it
can avoid network degradation as a byproduct. Compared with the original model,
the CNN model with Interflow achieves higher test accuracy on multiple
benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhicheng Cai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Harnessing Heterogeneity: Learning from Decomposed Feedback in Bayesian Modeling. (arXiv:2107.03003v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03003</id>
        <link href="http://arxiv.org/abs/2107.03003"/>
        <updated>2021-07-08T01:57:58.378Z</updated>
        <summary type="html"><![CDATA[There is significant interest in learning and optimizing a complex system
composed of multiple sub-components, where these components may be agents or
autonomous sensors. Among the rich literature on this topic, agent-based and
domain-specific simulations can capture complex dynamics and subgroup
interaction, but optimizing over such simulations can be computationally and
algorithmically challenging. Bayesian approaches, such as Gaussian processes
(GPs), can be used to learn a computationally tractable approximation to the
underlying dynamics but typically neglect the detailed information about
subgroups in the complicated system. We attempt to find the best of both worlds
by proposing the idea of decomposed feedback, which captures group-based
heterogeneity and dynamics. We introduce a novel decomposed GP regression to
incorporate the subgroup decomposed feedback. Our modified regression has
provably lower variance -- and thus a more accurate posterior -- compared to
previous approaches; it also allows us to introduce a decomposed GP-UCB
optimization algorithm that leverages subgroup feedback. The Bayesian nature of
our method makes the optimization algorithm trackable with a theoretical
guarantee on convergence and no-regret property. To demonstrate the wide
applicability of this work, we execute our algorithm on two disparate social
problems: infectious disease control in a heterogeneous population and
allocation of distributed weather sensors. Experimental results show that our
new method provides significant improvement compared to the state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilder_B/0/1/0/all/0/1"&gt;Bryan Wilder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suen_S/0/1/0/all/0/1"&gt;Sze-chuan Suen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dilkina_B/0/1/0/all/0/1"&gt;Bistra Dilkina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tambe_M/0/1/0/all/0/1"&gt;Milind Tambe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic partition of unity networks: clustering based deep approximation. (arXiv:2107.03066v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03066</id>
        <link href="http://arxiv.org/abs/2107.03066"/>
        <updated>2021-07-08T01:57:58.371Z</updated>
        <summary type="html"><![CDATA[Partition of unity networks (POU-Nets) have been shown capable of realizing
algebraic convergence rates for regression and solution of PDEs, but require
empirical tuning of training parameters. We enrich POU-Nets with a Gaussian
noise model to obtain a probabilistic generalization amenable to gradient-based
minimization of a maximum likelihood loss. The resulting architecture provides
spatial representations of both noiseless and noisy data as Gaussian mixtures
with closed form expressions for variance which provides an estimator of local
error. The training process yields remarkably sharp partitions of input space
based upon correlation of function values. This classification of training
points is amenable to a hierarchical refinement strategy that significantly
improves the localization of the regression, allowing for higher-order
polynomial approximation to be utilized. The framework scales more favorably to
large data sets as compared to Gaussian process regression and allows for
spatially varying uncertainty, leveraging the expressive power of deep neural
networks while bypassing expensive training associated with other probabilistic
deep learning methods. Compared to standard deep neural networks, the framework
demonstrates hp-convergence without the use of regularizers to tune the
localization of partitions. We provide benchmarks quantifying performance in
high/low-dimensions, demonstrating that convergence rates depend only on the
latent dimension of data within high-dimensional space. Finally, we introduce a
new open-source data set of PDE-based simulations of a semiconductor device and
perform unsupervised extraction of a physically interpretable reduced-order
basis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trask_N/0/1/0/all/0/1"&gt;Nat Trask&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gulian_M/0/1/0/all/0/1"&gt;Mamikon Gulian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_A/0/1/0/all/0/1"&gt;Andy Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kookjin Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Machine Learning for Cybersecurity and Computer Vision: Current Developments and Challenges. (arXiv:2107.02894v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.02894</id>
        <link href="http://arxiv.org/abs/2107.02894"/>
        <updated>2021-07-08T01:57:58.357Z</updated>
        <summary type="html"><![CDATA[We provide a comprehensive overview of adversarial machine learning focusing
on two application domains, i.e., cybersecurity and computer vision. Research
in adversarial machine learning addresses a significant threat to the wide
application of machine learning techniques -- they are vulnerable to carefully
crafted attacks from malicious adversaries. For example, deep neural networks
fail to correctly classify adversarial images, which are generated by adding
imperceptible perturbations to clean images.We first discuss three main
categories of attacks against machine learning techniques -- poisoning attacks,
evasion attacks, and privacy attacks. Then the corresponding defense approaches
are introduced along with the weakness and limitations of the existing defense
approaches. We notice adversarial samples in cybersecurity and computer vision
are fundamentally different. While adversarial samples in cybersecurity often
have different properties/distributions compared with training data,
adversarial images in computer vision are created with minor input
perturbations. This further complicates the development of robust learning
techniques, because a robust learning technique must withstand different types
of attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xi_B/0/1/0/all/0/1"&gt;Bowei Xi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling up Continuous-Time Markov Chains Helps Resolve Underspecification. (arXiv:2107.02911v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.02911</id>
        <link href="http://arxiv.org/abs/2107.02911"/>
        <updated>2021-07-08T01:57:58.338Z</updated>
        <summary type="html"><![CDATA[Modeling the time evolution of discrete sets of items (e.g., genetic
mutations) is a fundamental problem in many biomedical applications. We
approach this problem through the lens of continuous-time Markov chains, and
show that the resulting learning task is generally underspecified in the usual
setting of cross-sectional data. We explore a perhaps surprising remedy:
including a number of additional independent items can help determine time
order, and hence resolve underspecification. This is in sharp contrast to the
common practice of limiting the analysis to a small subset of relevant items,
which is followed largely due to poor scaling of existing methods. To put our
theoretical insight into practice, we develop an approximate likelihood
maximization method for learning continuous-time Markov chains, which can scale
to hundreds of items and is orders of magnitude faster than previous methods.
We demonstrate the effectiveness of our approach on synthetic and real cancer
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gotovos_A/0/1/0/all/0/1"&gt;Alkis Gotovos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burkholz_R/0/1/0/all/0/1"&gt;Rebekka Burkholz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quackenbush_J/0/1/0/all/0/1"&gt;John Quackenbush&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jegelka_S/0/1/0/all/0/1"&gt;Stefanie Jegelka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Face Hallucination via Split-Attention in Split-Attention Network. (arXiv:2010.11575v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11575</id>
        <link href="http://arxiv.org/abs/2010.11575"/>
        <updated>2021-07-08T01:57:58.332Z</updated>
        <summary type="html"><![CDATA[Recently, convolutional neural networks (CNNs) have been widely employed to
promote the face hallucination due to the ability to predict high-frequency
details from a large number of samples. However, most of them fail to take into
account the overall facial profile and fine texture details simultaneously,
resulting in reduced naturalness and fidelity of the reconstructed face, and
further impairing the performance of downstream tasks (e.g., face detection,
facial recognition). To tackle this issue, we propose a novel external-internal
split attention group (ESAG), which encompasses two paths responsible for
facial structure information and facial texture details, respectively. By
fusing the features from these two paths, the consistency of facial structure
and the fidelity of facial details are strengthened at the same time. Then, we
propose a split-attention in split-attention network (SISN) to reconstruct
photorealistic high-resolution facial images by cascading several ESAGs.
Experimental results on face hallucination and face recognition unveil that the
proposed method not only significantly improves the clarity of hallucinated
faces, but also encourages the subsequent face recognition performance
substantially. Codes have been released at
https://github.com/mdswyz/SISN-Face-Hallucination.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1"&gt;Tao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuanzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yanduo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhongyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Junjun Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trans4E: Link Prediction on Scholarly Knowledge Graphs. (arXiv:2107.03297v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.03297</id>
        <link href="http://arxiv.org/abs/2107.03297"/>
        <updated>2021-07-08T01:57:58.325Z</updated>
        <summary type="html"><![CDATA[The incompleteness of Knowledge Graphs (KGs) is a crucial issue affecting the
quality of AI-based services. In the scholarly domain, KGs describing research
publications typically lack important information, hindering our ability to
analyse and predict research dynamics. In recent years, link prediction
approaches based on Knowledge Graph Embedding models became the first aid for
this issue. In this work, we present Trans4E, a novel embedding model that is
particularly fit for KGs which include N to M relations with N$\gg$M. This is
typical for KGs that categorize a large number of entities (e.g., research
articles, patents, persons) according to a relatively small set of categories.
Trans4E was applied on two large-scale knowledge graphs, the Academia/Industry
DynAmics (AIDA) and Microsoft Academic Graph (MAG), for completing the
information about Fields of Study (e.g., 'neural networks', 'machine learning',
'artificial intelligence'), and affiliation types (e.g., 'education',
'company', 'government'), improving the scope and accuracy of the resulting
data. We evaluated our approach against alternative solutions on AIDA, MAG, and
four other benchmarks (FB15k, FB15k-237, WN18, and WN18RR). Trans4E outperforms
the other models when using low embedding dimensions and obtains competitive
results in high dimensions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nayyeri_M/0/1/0/all/0/1"&gt;Mojtaba Nayyeri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cil_G/0/1/0/all/0/1"&gt;Gokce Muge Cil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vahdati_S/0/1/0/all/0/1"&gt;Sahar Vahdati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osborne_F/0/1/0/all/0/1"&gt;Francesco Osborne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1"&gt;Mahfuzur Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Angioni_S/0/1/0/all/0/1"&gt;Simone Angioni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salatino_A/0/1/0/all/0/1"&gt;Angelo Salatino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Recupero_D/0/1/0/all/0/1"&gt;Diego Reforgiato Recupero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vassilyeva_N/0/1/0/all/0/1"&gt;Nadezhda Vassilyeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Motta_E/0/1/0/all/0/1"&gt;Enrico Motta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1"&gt;Jens Lehmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSANet: Dynamic Segment Aggregation Network for Video-Level Representation Learning. (arXiv:2105.12085v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12085</id>
        <link href="http://arxiv.org/abs/2105.12085"/>
        <updated>2021-07-08T01:57:58.318Z</updated>
        <summary type="html"><![CDATA[Long-range and short-range temporal modeling are two complementary and
crucial aspects of video recognition. Most of the state-of-the-arts focus on
short-range spatio-temporal modeling and then average multiple snippet-level
predictions to yield the final video-level prediction. Thus, their video-level
prediction does not consider spatio-temporal features of how video evolves
along the temporal dimension. In this paper, we introduce a novel Dynamic
Segment Aggregation (DSA) module to capture relationship among snippets. To be
more specific, we attempt to generate a dynamic kernel for a convolutional
operation to aggregate long-range temporal information among adjacent snippets
adaptively. The DSA module is an efficient plug-and-play module and can be
combined with the off-the-shelf clip-based models (i.e., TSM, I3D) to perform
powerful long-range modeling with minimal overhead. The final video
architecture, coined as DSANet. We conduct extensive experiments on several
video recognition benchmarks (i.e., Mini-Kinetics-200, Kinetics-400,
Something-Something V1 and ActivityNet) to show its superiority. Our proposed
DSA module is shown to benefit various video recognition models significantly.
For example, equipped with DSA modules, the top-1 accuracy of I3D ResNet-50 is
improved from 74.9% to 78.2% on Kinetics-400. Codes are available at
https://github.com/whwu95/DSANet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wenhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yuxiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yanwu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xiao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Dongliang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1"&gt;Zhikang Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jin Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yingying Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_M/0/1/0/all/0/1"&gt;Mingde Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1"&gt;Zichao Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yifeng Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Urban Tree Species Classification Using Aerial Imagery. (arXiv:2107.03182v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03182</id>
        <link href="http://arxiv.org/abs/2107.03182"/>
        <updated>2021-07-08T01:57:58.311Z</updated>
        <summary type="html"><![CDATA[Urban trees help regulate temperature, reduce energy consumption, improve
urban air quality, reduce wind speeds, and mitigating the urban heat island
effect. Urban trees also play a key role in climate change mitigation and
global warming by capturing and storing atmospheric carbon-dioxide which is the
largest contributor to greenhouse gases. Automated tree detection and species
classification using aerial imagery can be a powerful tool for sustainable
forest and urban tree management. Hence, This study first offers a pipeline for
generating labelled dataset of urban trees using Google Map's aerial images and
then investigates how state of the art deep Convolutional Neural Network models
such as VGG and ResNet handle the classification problem of urban tree aerial
images under different parameters. Experimental results show our best model
achieves an average accuracy of 60% over 6 tree species.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Waters_E/0/1/0/all/0/1"&gt;Emily Waters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oghaz_M/0/1/0/all/0/1"&gt;Mahdi Maktabdar Oghaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saheer_L/0/1/0/all/0/1"&gt;Lakshmi Babu Saheer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cognitive Learning-Aided Multi-Antenna Communications. (arXiv:2010.03131v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03131</id>
        <link href="http://arxiv.org/abs/2010.03131"/>
        <updated>2021-07-08T01:57:58.292Z</updated>
        <summary type="html"><![CDATA[Cognitive communications have emerged as a promising solution to enhance,
adapt, and invent new tools and capabilities that transcend conventional
wireless networks. Deep learning (DL) is critical in enabling essential
features of cognitive systems because of its fast prediction performance,
adaptive behavior, and model-free structure. These features are especially
significant for multi-antenna wireless communications systems, which generate
and handle massive data. Multiple antennas may provide multiplexing, diversity,
or antenna gains that, respectively, improve the capacity, bit error rate, or
the signal-to-interference-plus-noise ratio. In practice, multi-antenna
cognitive communications encounter challenges in terms of data complexity and
diversity, hardware complexity, and wireless channel dynamics. The DL-based
solutions tackle these problems at the various stages of communications
processing such as channel estimation, hybrid beamforming, user localization,
and sparse array design. There are research opportunities to address
significant design challenges arising from insufficient data coverage, learning
model complexity, and data transmission overheads. This article provides
synopses of various DL-based methods to impart cognitive behavior to
multi-antenna wireless communications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Elbir_A/0/1/0/all/0/1"&gt;Ahmet M. Elbir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mishra_K/0/1/0/all/0/1"&gt;Kumar Vijay Mishra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ADAPT : Awesome Domain Adaptation Python Toolbox. (arXiv:2107.03049v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03049</id>
        <link href="http://arxiv.org/abs/2107.03049"/>
        <updated>2021-07-08T01:57:58.284Z</updated>
        <summary type="html"><![CDATA[ADAPT is an open-source python library providing the implementation of
several domain adaptation methods. The library is suited for scikit-learn
estimator object (object which implement fit and predict methods) and
tensorflow models. Most of the implemented methods are developed in an
estimator agnostic fashion, offering various possibilities adapted to multiple
usage. The library offers three modules corresponding to the three principal
strategies of domain adaptation: (i) feature-based containing methods
performing feature transformation; (ii) instance-based with the implementation
of reweighting techniques and (iii) parameter-based proposing methods to adapt
pre-trained models to novel observations. A full documentation is proposed
online https://adapt-python.github.io/adapt/ with gallery of examples. Besides,
the library presents an high test coverage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mathelin_A/0/1/0/all/0/1"&gt;Antoine de Mathelin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deheeger_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Deheeger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richard_G/0/1/0/all/0/1"&gt;Guillaume Richard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mougeot_M/0/1/0/all/0/1"&gt;Mathilde Mougeot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vayatis_N/0/1/0/all/0/1"&gt;Nicolas Vayatis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonmyopic Multifidelity Active Search. (arXiv:2106.06356v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06356</id>
        <link href="http://arxiv.org/abs/2106.06356"/>
        <updated>2021-07-08T01:57:58.278Z</updated>
        <summary type="html"><![CDATA[Active search is a learning paradigm where we seek to identify as many
members of a rare, valuable class as possible given a labeling budget. Previous
work on active search has assumed access to a faithful (and expensive) oracle
reporting experimental results. However, some settings offer access to cheaper
surrogates such as computational simulation that may aid in the search. We
propose a model of multifidelity active search, as well as a novel,
computationally efficient policy for this setting that is motivated by
state-of-the-art classical policies. Our policy is nonmyopic and budget aware,
allowing for a dynamic tradeoff between exploration and exploitation. We
evaluate the performance of our solution on real-world datasets and demonstrate
significantly better performance than natural benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1"&gt;Quan Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Modiri_A/0/1/0/all/0/1"&gt;Arghavan Modiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garnett_R/0/1/0/all/0/1"&gt;Roman Garnett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Training Instance Selection for Few-Shot Neural Text Generation. (arXiv:2107.03176v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.03176</id>
        <link href="http://arxiv.org/abs/2107.03176"/>
        <updated>2021-07-08T01:57:58.271Z</updated>
        <summary type="html"><![CDATA[Large-scale pretrained language models have led to dramatic improvements in
text generation. Impressive performance can be achieved by finetuning only on a
small number of instances (few-shot setting). Nonetheless, almost all previous
work simply applies random sampling to select the few-shot training instances.
Little to no attention has been paid to the selection strategies and how they
would affect model performance. In this work, we present a study on training
instance selection in few-shot neural text generation. The selection decision
is made based only on the unlabeled data so as to identify the most worthwhile
data points that should be annotated under some budget of labeling cost. Based
on the intuition that the few-shot training instances should be diverse and
representative of the entire data distribution, we propose a simple selection
strategy with K-means clustering. We show that even with the naive
clustering-based approach, the generation models consistently outperform random
sampling on three text generation tasks: data-to-text generation, document
summarization and question generation. We hope that this work will call for
more attention on this largely unexplored area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1"&gt;Ernie Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1"&gt;Xiaoyu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeh_H/0/1/0/all/0/1"&gt;Hui-Syuan Yeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demberg_V/0/1/0/all/0/1"&gt;Vera Demberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Detection of Botnet Traffic by features selection and Decision Trees. (arXiv:2107.02896v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.02896</id>
        <link href="http://arxiv.org/abs/2107.02896"/>
        <updated>2021-07-08T01:57:58.264Z</updated>
        <summary type="html"><![CDATA[Botnets are one of the online threats with the biggest presence, causing
billionaire losses to global economies. Nowadays, the increasing number of
devices connected to the Internet makes it necessary to analyze large amounts
of network traffic data. In this work, we focus on increasing the performance
on botnet traffic classification by selecting those features that further
increase the detection rate. For this purpose we use two feature selection
techniques, Information Gain and Gini Importance, which led to three
pre-selected subsets of five, six and seven features. Then, we evaluate the
three feature subsets along with three models, Decision Tree, Random Forest and
k-Nearest Neighbors. To test the performance of the three feature vectors and
the three models we generate two datasets based on the CTU-13 dataset, namely
QB-CTU13 and EQB-CTU13. We measure the performance as the macro averaged F1
score over the computational time required to classify a sample. The results
show that the highest performance is achieved by Decision Trees using a five
feature set which obtained a mean F1 score of 85% classifying each sample in an
average time of 0.78 microseconds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Velasco_Mata_J/0/1/0/all/0/1"&gt;Javier Velasco-Mata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Castro_V/0/1/0/all/0/1"&gt;V&amp;#xed;ctor Gonz&amp;#xe1;lez-Castro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fidalgo_E/0/1/0/all/0/1"&gt;Eduardo Fidalgo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alegre_E/0/1/0/all/0/1"&gt;Enrique Alegre&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KaFiStO: A Kalman Filtering Framework for Stochastic Optimization. (arXiv:2107.03331v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03331</id>
        <link href="http://arxiv.org/abs/2107.03331"/>
        <updated>2021-07-08T01:57:58.244Z</updated>
        <summary type="html"><![CDATA[Optimization is often cast as a deterministic problem, where the solution is
found through some iterative procedure such as gradient descent. However, when
training neural networks the loss function changes over (iteration) time due to
the randomized selection of a subset of the samples. This randomization turns
the optimization problem into a stochastic one. We propose to consider the loss
as a noisy observation with respect to some reference optimum. This
interpretation of the loss allows us to adopt Kalman filtering as an optimizer,
as its recursive formulation is designed to estimate unknown parameters from
noisy measurements. Moreover, we show that the Kalman Filter dynamical model
for the evolution of the unknown parameters can be used to capture the gradient
dynamics of advanced methods such as Momentum and Adam. We call this stochastic
optimization method KaFiStO. KaFiStO is an easy to implement, scalable, and
efficient method to train neural networks. We show that it also yields
parameter estimates that are on par with or better than existing optimization
algorithms across several neural network architectures and machine learning
tasks, such as computer vision and language modeling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Davtyan_A/0/1/0/all/0/1"&gt;Aram Davtyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sameni_S/0/1/0/all/0/1"&gt;Sepehr Sameni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cerkezi_L/0/1/0/all/0/1"&gt;Llukman Cerkezi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meishvilli_G/0/1/0/all/0/1"&gt;Givi Meishvilli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bielski_A/0/1/0/all/0/1"&gt;Adam Bielski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Favaro_P/0/1/0/all/0/1"&gt;Paolo Favaro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New Methods and Datasets for Group Anomaly Detection From Fundamental Physics. (arXiv:2107.02821v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.02821</id>
        <link href="http://arxiv.org/abs/2107.02821"/>
        <updated>2021-07-08T01:57:58.237Z</updated>
        <summary type="html"><![CDATA[The identification of anomalous overdensities in data - group or collective
anomaly detection - is a rich problem with a large number of real world
applications. However, it has received relatively little attention in the
broader ML community, as compared to point anomalies or other types of single
instance outliers. One reason for this is the lack of powerful benchmark
datasets. In this paper, we first explain how, after the Nobel-prize winning
discovery of the Higgs boson, unsupervised group anomaly detection has become a
new frontier of fundamental physics (where the motivation is to find new
particles and forces). Then we propose a realistic synthetic benchmark dataset
(LHCO2020) for the development of group anomaly detection algorithms. Finally,
we compare several existing statistically-sound techniques for unsupervised
group anomaly detection, and demonstrate their performance on the LHCO2020
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kasieczka_G/0/1/0/all/0/1"&gt;Gregor Kasieczka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nachman_B/0/1/0/all/0/1"&gt;Benjamin Nachman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shih_D/0/1/0/all/0/1"&gt;David Shih&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Alternative Practice of Tropical Convolution to Traditional Convolutional Neural Networks. (arXiv:2103.02096v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02096</id>
        <link href="http://arxiv.org/abs/2103.02096"/>
        <updated>2021-07-08T01:57:58.230Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) have been used in many machine learning
fields. In practical applications, the computational cost of convolutional
neural networks is often high with the deepening of the network and the growth
of data volume, mostly due to a large amount of multiplication operations of
floating-point numbers in convolution operations. To reduce the amount of
multiplications, we propose a new type of CNNs called Tropical Convolutional
Neural Networks (TCNNs) which are built on tropical convolutions in which the
multiplications and additions in conventional convolutional layers are replaced
by additions and min/max operations respectively. In addition, since tropical
convolution operators are essentially nonlinear operators, we expect TCNNs to
have higher nonlinear fitting ability than conventional CNNs. In the
experiments, we test and analyze several different architectures of TCNNs for
image classification tasks in comparison with similar-sized conventional CNNs.
The results show that TCNN can achieve higher expressive power than ordinary
convolutional layers on the MNIST and CIFAR10 image data set. In different
noise environments, there are wins and losses in the robustness of TCNN and
ordinary CNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1"&gt;Shiqing Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liying_L/0/1/0/all/0/1"&gt;Liu Liying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Ye Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interventional Video Grounding with Dual Contrastive Learning. (arXiv:2106.11013v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11013</id>
        <link href="http://arxiv.org/abs/2106.11013"/>
        <updated>2021-07-08T01:57:58.222Z</updated>
        <summary type="html"><![CDATA[Video grounding aims to localize a moment from an untrimmed video for a given
textual query. Existing approaches focus more on the alignment of visual and
language stimuli with various likelihood-based matching or regression
strategies, i.e., P(Y|X). Consequently, these models may suffer from spurious
correlations between the language and video features due to the selection bias
of the dataset. 1) To uncover the causality behind the model and data, we first
propose a novel paradigm from the perspective of the causal inference, i.e.,
interventional video grounding (IVG) that leverages backdoor adjustment to
deconfound the selection bias based on structured causal model (SCM) and
do-calculus P(Y|do(X)). Then, we present a simple yet effective method to
approximate the unobserved confounder as it cannot be directly sampled from the
dataset. 2) Meanwhile, we introduce a dual contrastive learning approach (DCL)
to better align the text and video by maximizing the mutual information (MI)
between query and video clips, and the MI between start/end frames of a target
moment and the others within a video to learn more informative visual
representations. Experiments on three standard benchmarks show the
effectiveness of our approaches. Our code is available on GitHub:
https://github.com/nanguoshun/IVG.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nan_G/0/1/0/all/0/1"&gt;Guoshun Nan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_R/0/1/0/all/0/1"&gt;Rui Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1"&gt;Yao Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leng_S/0/1/0/all/0/1"&gt;Sicong Leng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wei Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bio-Inspired Adversarial Attack Against Deep Neural Networks. (arXiv:2107.02895v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.02895</id>
        <link href="http://arxiv.org/abs/2107.02895"/>
        <updated>2021-07-08T01:57:58.215Z</updated>
        <summary type="html"><![CDATA[The paper develops a new adversarial attack against deep neural networks
(DNN), based on applying bio-inspired design to moving physical objects. To the
best of our knowledge, this is the first work to introduce physical attacks
with a moving object. Instead of following the dominating attack strategy in
the existing literature, i.e., to introduce minor perturbations to a digital
input or a stationary physical object, we show two new successful attack
strategies in this paper. We show by superimposing several patterns onto one
physical object, a DNN becomes confused and picks one of the patterns to assign
a class label. Our experiment with three flapping wing robots demonstrates the
possibility of developing an adversarial camouflage to cause a targeted mistake
by DNN. We also show certain motion can reduce the dependency among consecutive
frames in a video and make an object detector "blind", i.e., not able to detect
an object exists in the video. Hence in a successful physical attack against
DNN, targeted motion against the system should also be considered.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xi_B/0/1/0/all/0/1"&gt;Bowei Xi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yujie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_F/0/1/0/all/0/1"&gt;Fan Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1"&gt;Zhan Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1"&gt;Xinyan Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Data Balancing for Unlabeled Satellite Imagery. (arXiv:2107.03227v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03227</id>
        <link href="http://arxiv.org/abs/2107.03227"/>
        <updated>2021-07-08T01:57:58.196Z</updated>
        <summary type="html"><![CDATA[Data imbalance is a ubiquitous problem in machine learning. In large scale
collected and annotated datasets, data imbalance is either mitigated manually
by undersampling frequent classes and oversampling rare classes, or planned for
with imputation and augmentation techniques. In both cases balancing data
requires labels. In other words, only annotated data can be balanced.
Collecting fully annotated datasets is challenging, especially for large scale
satellite systems such as the unlabeled NASA's 35 PB Earth Imagery dataset.
Although the NASA Earth Imagery dataset is unlabeled, there are implicit
properties of the data source that we can rely on to hypothesize about its
imbalance, such as distribution of land and water in the case of the Earth's
imagery. We present a new iterative method to balance unlabeled data. Our
method utilizes image embeddings as a proxy for image labels that can be used
to balance data, and ultimately when trained increases overall accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1"&gt;Deep Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_E/0/1/0/all/0/1"&gt;Erin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1"&gt;Anirudh Koul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1"&gt;Siddha Ganju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasam_M/0/1/0/all/0/1"&gt;Meher Anand Kasam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Architecture Pruning for Transfer Learning. (arXiv:2107.03375v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03375</id>
        <link href="http://arxiv.org/abs/2107.03375"/>
        <updated>2021-07-08T01:57:58.189Z</updated>
        <summary type="html"><![CDATA[We propose a new gradient-based approach for extracting sub-architectures
from a given large model. Contrarily to existing pruning methods, which are
unable to disentangle the network architecture and the corresponding weights,
our architecture-pruning scheme produces transferable new structures that can
be successfully retrained to solve different tasks. We focus on a
transfer-learning setup where architectures can be trained on a large data set
but very few data points are available for fine-tuning them on new tasks. We
define a new gradient-based algorithm that trains architectures of arbitrarily
low complexity independently from the attached weights. Given a search space
defined by an existing large neural model, we reformulate the architecture
search task as a complexity-penalized subset-selection problem and solve it
through a two-temperature relaxation scheme. We provide theoretical convergence
guarantees and validate the proposed transfer-learning strategy on real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Colombo_N/0/1/0/all/0/1"&gt;Nicolo Colombo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yang Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GAN-based Data Augmentation for Chest X-ray Classification. (arXiv:2107.02970v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.02970</id>
        <link href="http://arxiv.org/abs/2107.02970"/>
        <updated>2021-07-08T01:57:58.181Z</updated>
        <summary type="html"><![CDATA[A common problem in computer vision -- particularly in medical applications
-- is a lack of sufficiently diverse, large sets of training data. These
datasets often suffer from severe class imbalance. As a result, networks often
overfit and are unable to generalize to novel examples. Generative Adversarial
Networks (GANs) offer a novel method of synthetic data augmentation. In this
work, we evaluate the use of GAN- based data augmentation to artificially
expand the CheXpert dataset of chest radiographs. We compare performance to
traditional augmentation and find that GAN-based augmentation leads to higher
downstream performance for underrepresented classes. Furthermore, we see that
this result is pronounced in low data regimens. This suggests that GAN-based
augmentation a promising area of research to improve network performance when
data collection is prohibitively expensive.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sundaram_S/0/1/0/all/0/1"&gt;Shobhita Sundaram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hulkund_N/0/1/0/all/0/1"&gt;Neha Hulkund&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controlled Caption Generation for Images Through Adversarial Attacks. (arXiv:2107.03050v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03050</id>
        <link href="http://arxiv.org/abs/2107.03050"/>
        <updated>2021-07-08T01:57:58.174Z</updated>
        <summary type="html"><![CDATA[Deep learning is found to be vulnerable to adversarial examples. However, its
adversarial susceptibility in image caption generation is under-explored. We
study adversarial examples for vision and language models, which typically
adopt an encoder-decoder framework consisting of two major components: a
Convolutional Neural Network (i.e., CNN) for image feature extraction and a
Recurrent Neural Network (RNN) for caption generation. In particular, we
investigate attacks on the visual encoder's hidden layer that is fed to the
subsequent recurrent network. The existing methods either attack the
classification layer of the visual encoder or they back-propagate the gradients
from the language model. In contrast, we propose a GAN-based algorithm for
crafting adversarial examples for neural image captioning that mimics the
internal representation of the CNN such that the resulting deep features of the
input image enable a controlled incorrect caption generation through the
recurrent network. Our contribution provides new insights for understanding
adversarial attacks on vision systems with language component. The proposed
method employs two strategies for a comprehensive evaluation. The first
examines if a neural image captioning system can be misled to output targeted
image captions. The second analyzes the possibility of keywords into the
predicted captions. Experiments show that our algorithm can craft effective
adversarial images based on the CNN hidden layers to fool captioning framework.
Moreover, we discover the proposed attack to be highly transferable. Our work
leads to new robustness implications for neural image captioning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aafaq_N/0/1/0/all/0/1"&gt;Nayyer Aafaq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1"&gt;Naveed Akhtar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Mubarak Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1"&gt;Ajmal Mian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RAILS: A Robust Adversarial Immune-inspired Learning System. (arXiv:2107.02840v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2107.02840</id>
        <link href="http://arxiv.org/abs/2107.02840"/>
        <updated>2021-07-08T01:57:58.167Z</updated>
        <summary type="html"><![CDATA[Adversarial attacks against deep neural networks (DNNs) are continuously
evolving, requiring increasingly powerful defense strategies. We develop a
novel adversarial defense framework inspired by the adaptive immune system: the
Robust Adversarial Immune-inspired Learning System (RAILS). Initializing a
population of exemplars that is balanced across classes, RAILS starts from a
uniform label distribution that encourages diversity and debiases a potentially
corrupted initial condition. RAILS implements an evolutionary optimization
process to adjust the label distribution and achieve specificity towards ground
truth. RAILS displays a tradeoff between robustness (diversity) and accuracy
(specificity), providing a new immune-inspired perspective on adversarial
learning. We empirically validate the benefits of RAILS through several
adversarial image classification experiments on MNIST, SVHN, and CIFAR-10
datasets. For the PGD attack, RAILS is found to improve the robustness over
existing methods by >= 5.62%, 12.5% and 10.32%, respectively, without
appreciable loss of standard accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ren Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lindsly_S/0/1/0/all/0/1"&gt;Stephen Lindsly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stansbury_C/0/1/0/all/0/1"&gt;Cooper Stansbury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rehemtulla_A/0/1/0/all/0/1"&gt;Alnawaz Rehemtulla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajapakse_I/0/1/0/all/0/1"&gt;Indika Rajapakse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hero_A/0/1/0/all/0/1"&gt;Alfred Hero&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nested Counterfactual Identification from Arbitrary Surrogate Experiments. (arXiv:2107.03190v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.03190</id>
        <link href="http://arxiv.org/abs/2107.03190"/>
        <updated>2021-07-08T01:57:58.144Z</updated>
        <summary type="html"><![CDATA[The Ladder of Causation describes three qualitatively different types of
activities an agent may be interested in engaging in, namely, seeing
(observational), doing (interventional), and imagining (counterfactual) (Pearl
and Mackenzie, 2018). The inferential challenge imposed by the causal hierarchy
is that data is collected by an agent observing or intervening in a system
(layers 1 and 2), while its goal may be to understand what would have happened
had it taken a different course of action, contrary to what factually ended up
happening (layer 3). While there exists a solid understanding of the conditions
under which cross-layer inferences are allowed from observations to
interventions, the results are somewhat scarcer when targeting counterfactual
quantities. In this paper, we study the identification of nested
counterfactuals from an arbitrary combination of observations and experiments.
Specifically, building on a more explicit definition of nested counterfactuals,
we prove the counterfactual unnesting theorem (CUT), which allows one to map
arbitrary nested counterfactuals to unnested ones. For instance, applications
in mediation and fairness analysis usually evoke notions of direct, indirect,
and spurious effects, which naturally require nesting. Second, we introduce a
sufficient and necessary graphical condition for counterfactual identification
from an arbitrary combination of observational and experimental distributions.
Lastly, we develop an efficient and complete algorithm for identifying nested
counterfactuals; failure of the algorithm returning an expression for a query
implies it is not identifiable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Correa_J/0/1/0/all/0/1"&gt;Juan D Correa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sanghack Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bareinboim_E/0/1/0/all/0/1"&gt;Elias Bareinboim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Contextual Bandits without Regret. (arXiv:2107.03144v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.03144</id>
        <link href="http://arxiv.org/abs/2107.03144"/>
        <updated>2021-07-08T01:57:58.137Z</updated>
        <summary type="html"><![CDATA[Contextual bandits are a rich model for sequential decision making given side
information, with important applications, e.g., in recommender systems. We
propose novel algorithms for contextual bandits harnessing neural networks to
approximate the unknown reward function. We resolve the open problem of proving
sublinear regret bounds in this setting for general context sequences,
considering both fully-connected and convolutional networks. To this end, we
first analyze NTK-UCB, a kernelized bandit optimization algorithm employing the
Neural Tangent Kernel (NTK), and bound its regret in terms of the NTK maximum
information gain $\gamma_T$, a complexity parameter capturing the difficulty of
learning. Our bounds on $\gamma_T$ for the NTK may be of independent interest.
We then introduce our neural network based algorithm NN-UCB, and show that its
regret closely tracks that of NTK-UCB. Under broad non-parametric assumptions
about the reward function, our approach converges to the optimal policy at a
$\tilde{\mathcal{O}}(T^{-1/2d})$ rate, where $d$ is the dimension of the
context.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kassraie_P/0/1/0/all/0/1"&gt;Parnian Kassraie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Krause_A/0/1/0/all/0/1"&gt;Andreas Krause&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bias-Tolerant Fair Classification. (arXiv:2107.03207v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03207</id>
        <link href="http://arxiv.org/abs/2107.03207"/>
        <updated>2021-07-08T01:57:58.104Z</updated>
        <summary type="html"><![CDATA[The label bias and selection bias are acknowledged as two reasons in data
that will hinder the fairness of machine-learning outcomes. The label bias
occurs when the labeling decision is disturbed by sensitive features, while the
selection bias occurs when subjective bias exists during the data sampling.
Even worse, models trained on such data can inherit or even intensify the
discrimination. Most algorithmic fairness approaches perform an empirical risk
minimization with predefined fairness constraints, which tends to trade-off
accuracy for fairness. However, such methods would achieve the desired fairness
level with the sacrifice of the benefits (receive positive outcomes) for
individuals affected by the bias. Therefore, we propose a
Bias-TolerantFAirRegularizedLoss (B-FARL), which tries to regain the benefits
using data affected by label bias and selection bias. B-FARL takes the biased
data as input, calls a model that approximates the one trained with fair but
latent data, and thus prevents discrimination without constraints required. In
addition, we show the effective components by decomposing B-FARL, and we
utilize the meta-learning framework for the B-FARL optimization. The
experimental results on real-world datasets show that our method is empirically
effective in improving fairness towards the direction of true but latent
labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yixuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1"&gt;Feng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhidong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Fang Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Particle Convolution for High Energy Physics. (arXiv:2107.02908v1 [hep-ph])]]></title>
        <id>http://arxiv.org/abs/2107.02908</id>
        <link href="http://arxiv.org/abs/2107.02908"/>
        <updated>2021-07-08T01:57:58.098Z</updated>
        <summary type="html"><![CDATA[We introduce the Particle Convolution Network (PCN), a new type of
equivariant neural network layer suitable for many tasks in jet physics. The
particle convolution layer can be viewed as an extension of Deep Sets and
Energy Flow network architectures, in which the permutation-invariant operator
is promoted to a group convolution. While the PCN can be implemented for
various kinds of symmetries, we consider the specific case of rotation about
the jet axis the $\eta - \phi$ plane. In two standard benchmark tasks, q/g
tagging and top tagging, we show that the rotational PCN (rPCN) achieves
performance comparable to graph networks such as ParticleNet. Moreover, we show
that it is possible to implement an IRC-safe rPCN, which significantly
outperforms existing IRC-safe tagging methods on both tasks. We speculate that
by generalizing the PCN to include additional convolutional symmetries relevant
to jet physics, it may outperform the current state-of-the-art set by graph
networks, while offering a new degree of control over physically-motivated
inductive biases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-ph/1/au:+Shimmin_C/0/1/0/all/0/1"&gt;Chase Shimmin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Immuno-mimetic Deep Neural Networks (Immuno-Net). (arXiv:2107.02842v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2107.02842</id>
        <link href="http://arxiv.org/abs/2107.02842"/>
        <updated>2021-07-08T01:57:58.086Z</updated>
        <summary type="html"><![CDATA[Biomimetics has played a key role in the evolution of artificial neural
networks. Thus far, in silico metaphors have been dominated by concepts from
neuroscience and cognitive psychology. In this paper we introduce a different
type of biomimetic model, one that borrows concepts from the immune system, for
designing robust deep neural networks. This immuno-mimetic model leads to a new
computational biology framework for robustification of deep neural networks
against adversarial attacks. Within this Immuno-Net framework we define a
robust adaptive immune-inspired learning system (Immuno-Net RAILS) that
emulates, in silico, the adaptive biological mechanisms of B-cells that are
used to defend a mammalian host against pathogenic attacks. When applied to
image classification tasks on benchmark datasets, we demonstrate that
Immuno-net RAILS results in improvement of as much as 12.5% in adversarial
accuracy of a baseline method, the DkNN-robustified CNN, without appreciable
loss of accuracy on clean data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ren Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lindsly_S/0/1/0/all/0/1"&gt;Stephen Lindsly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stansbury_C/0/1/0/all/0/1"&gt;Cooper Stansbury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajapakse_I/0/1/0/all/0/1"&gt;Indika Rajapakse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hero_A/0/1/0/all/0/1"&gt;Alfred Hero&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An automatic multi-tissue human fetal brain segmentation benchmark using the Fetal Tissue Annotation Dataset. (arXiv:2010.15526v4 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.15526</id>
        <link href="http://arxiv.org/abs/2010.15526"/>
        <updated>2021-07-08T01:57:58.053Z</updated>
        <summary type="html"><![CDATA[It is critical to quantitatively analyse the developing human fetal brain in
order to fully understand neurodevelopment in both normal fetuses and those
with congenital disorders. To facilitate this analysis, automatic multi-tissue
fetal brain segmentation algorithms are needed, which in turn requires open
databases of segmented fetal brains. Here we introduce a publicly available
database of 50 manually segmented pathological and non-pathological fetal
magnetic resonance brain volume reconstructions across a range of gestational
ages (20 to 33 weeks) into 7 different tissue categories (external
cerebrospinal fluid, grey matter, white matter, ventricles, cerebellum, deep
grey matter, brainstem/spinal cord). In addition, we quantitatively evaluate
the accuracy of several automatic multi-tissue segmentation algorithms of the
developing human fetal brain. Four research groups participated, submitting a
total of 10 algorithms, demonstrating the benefits the database for the
development of automatic algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Payette_K/0/1/0/all/0/1"&gt;Kelly Payette&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dumast_P/0/1/0/all/0/1"&gt;Priscille de Dumast&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kebiri_H/0/1/0/all/0/1"&gt;Hamza Kebiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ezhov_I/0/1/0/all/0/1"&gt;Ivan Ezhov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Paetzold_J/0/1/0/all/0/1"&gt;Johannes C. Paetzold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shit_S/0/1/0/all/0/1"&gt;Suprosanna Shit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Iqbal_A/0/1/0/all/0/1"&gt;Asim Iqbal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khan_R/0/1/0/all/0/1"&gt;Romesa Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kottke_R/0/1/0/all/0/1"&gt;Raimund Kottke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Grehten_P/0/1/0/all/0/1"&gt;Patrice Grehten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ji_H/0/1/0/all/0/1"&gt;Hui Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lanczi_L/0/1/0/all/0/1"&gt;Levente Lanczi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nagy_M/0/1/0/all/0/1"&gt;Marianna Nagy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Beresova_M/0/1/0/all/0/1"&gt;Monika Beresova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thi Dao Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Natalucci_G/0/1/0/all/0/1"&gt;Giancarlo Natalucci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Karayannis_T/0/1/0/all/0/1"&gt;Theofanis Karayannis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Menze_B/0/1/0/all/0/1"&gt;Bjoern Menze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cuadra_M/0/1/0/all/0/1"&gt;Meritxell Bach Cuadra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jakab_A/0/1/0/all/0/1"&gt;Andras Jakab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decentralised Learning from Independent Multi-Domain Labels for Person Re-Identification. (arXiv:2006.04150v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04150</id>
        <link href="http://arxiv.org/abs/2006.04150"/>
        <updated>2021-07-08T01:57:58.031Z</updated>
        <summary type="html"><![CDATA[Deep learning has been successful for many computer vision tasks due to the
availability of shared and centralised large-scale training data. However,
increasing awareness of privacy concerns poses new challenges to deep learning,
especially for human subject related recognition such as person
re-identification (Re-ID). In this work, we solve the Re-ID problem by
decentralised learning from non-shared private training data distributed at
multiple user sites of independent multi-domain label spaces. We propose a
novel paradigm called Federated Person Re-Identification (FedReID) to construct
a generalisable global model (a central server) by simultaneously learning with
multiple privacy-preserved local models (local clients). Specifically, each
local client receives global model updates from the server and trains a local
model using its local data independent from all the other clients. Then, the
central server aggregates transferrable local model updates to construct a
generalisable global feature embedding model without accessing local data so to
preserve local privacy. This client-server collaborative learning process is
iteratively performed under privacy control, enabling FedReID to realise
decentralised learning without sharing distributed data nor collecting any
centralised data. Extensive experiments on ten Re-ID benchmarks show that
FedReID achieves compelling generalisation performance beyond any locally
trained models without using shared training data, whilst inherently protects
the privacy of each local client. This is uniquely advantageous over
contemporary Re-ID methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1"&gt;Guile Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1"&gt;Shaogang Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Partial 3D Object Retrieval using Local Binary QUICCI Descriptors and Dissimilarity Tree Indexing. (arXiv:2107.03368v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03368</id>
        <link href="http://arxiv.org/abs/2107.03368"/>
        <updated>2021-07-08T01:57:58.023Z</updated>
        <summary type="html"><![CDATA[A complete pipeline is presented for accurate and efficient partial 3D object
retrieval based on Quick Intersection Count Change Image (QUICCI) binary local
descriptors and a novel indexing tree. It is shown how a modification to the
QUICCI query descriptor makes it ideal for partial retrieval. An indexing
structure called Dissimilarity Tree is proposed which can significantly
accelerate searching the large space of local descriptors; this is applicable
to QUICCI and other binary descriptors. The index exploits the distribution of
bits within descriptors for efficient retrieval. The retrieval pipeline is
tested on the artificial part of SHREC'16 dataset with near-ideal retrieval
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blokland_B/0/1/0/all/0/1"&gt;Bart Iver van Blokland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theoharis_T/0/1/0/all/0/1"&gt;Theoharis Theoharis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Principles for Evaluation of AI/ML Model Performance and Robustness. (arXiv:2107.02868v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.02868</id>
        <link href="http://arxiv.org/abs/2107.02868"/>
        <updated>2021-07-08T01:57:58.015Z</updated>
        <summary type="html"><![CDATA[The Department of Defense (DoD) has significantly increased its investment in
the design, evaluation, and deployment of Artificial Intelligence and Machine
Learning (AI/ML) capabilities to address national security needs. While there
are numerous AI/ML successes in the academic and commercial sectors, many of
these systems have also been shown to be brittle and nonrobust. In a complex
and ever-changing national security environment, it is vital that the DoD
establish a sound and methodical process to evaluate the performance and
robustness of AI/ML models before these new capabilities are deployed to the
field. This paper reviews the AI/ML development process, highlights common best
practices for AI/ML model evaluation, and makes recommendations to DoD
evaluators to ensure the deployment of robust AI/ML capabilities for national
security needs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brown_O/0/1/0/all/0/1"&gt;Olivia Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Curtis_A/0/1/0/all/0/1"&gt;Andrew Curtis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goodwin_J/0/1/0/all/0/1"&gt;Justin Goodwin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-VAE: Learning Disentangled View-common and View-peculiar Visual Representations for Multi-view Clustering. (arXiv:2106.11232v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11232</id>
        <link href="http://arxiv.org/abs/2106.11232"/>
        <updated>2021-07-08T01:57:58.008Z</updated>
        <summary type="html"><![CDATA[Multi-view clustering, a long-standing and important research problem,
focuses on mining complementary information from diverse views. However,
existing works often fuse multiple views' representations or handle clustering
in a common feature space, which may result in their entanglement especially
for visual representations. To address this issue, we present a novel VAE-based
multi-view clustering framework (Multi-VAE) by learning disentangled visual
representations. Concretely, we define a view-common variable and multiple
view-peculiar variables in the generative model. The prior of view-common
variable obeys approximately discrete Gumbel Softmax distribution, which is
introduced to extract the common cluster factor of multiple views. Meanwhile,
the prior of view-peculiar variable follows continuous Gaussian distribution,
which is used to represent each view's peculiar visual factors. By controlling
the mutual information capacity to disentangle the view-common and
view-peculiar representations, continuous visual information of multiple views
can be separated so that their common discrete cluster information can be
effectively mined. Experimental results demonstrate that Multi-VAE enjoys the
disentangled and explainable visual representations, while obtaining superior
clustering performance compared with state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jie Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yazhou Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Huayi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_X/0/1/0/all/0/1"&gt;Xiaorong Pu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaofeng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1"&gt;Ming Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lifang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed adaptive algorithm based on the asymmetric cost of error functions. (arXiv:2107.03067v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03067</id>
        <link href="http://arxiv.org/abs/2107.03067"/>
        <updated>2021-07-08T01:57:57.991Z</updated>
        <summary type="html"><![CDATA[In this paper, a family of novel diffusion adaptive estimation algorithm is
proposed from the asymmetric cost function perspective by combining diffusion
strategy and the linear-linear cost (LLC), quadratic-quadratic cost (QQC), and
linear-exponential cost (LEC), at all distributed network nodes, and named
diffusion LLCLMS (DLLCLMS), diffusion QQCLMS (DQQCLMS), and diffusion LECLMS
(DLECLMS), respectively. Then the stability of mean estimation error and
computational complexity of those three diffusion algorithms are analyzed
theoretically. Finally, several experiment simulation results are designed to
verify the superiority of those three proposed diffusion algorithms.
Experimental simulation results show that DLLCLMS, DQQCLMS, and DLECLMS
algorithms are more robust to the input signal and impulsive noise than the
DSELMS, DRVSSLMS, and DLLAD algorithms. In brief, theoretical analysis and
experiment results show that those proposed DLLCLMS, DQQCLMS, and DLECLMS
algorithms have superior performance when estimating the unknown linear system
under the changeable impulsive noise environments and different types of input
signals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1"&gt;Sihai Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1"&gt;Qing Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yong Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Teacher Forcing Network for Semi-Supervised Large Scale Data Streams. (arXiv:2107.02943v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2107.02943</id>
        <link href="http://arxiv.org/abs/2107.02943"/>
        <updated>2021-07-08T01:57:57.978Z</updated>
        <summary type="html"><![CDATA[The large-scale data stream problem refers to high-speed information flow
which cannot be processed in scalable manner under a traditional computing
platform. This problem also imposes expensive labelling cost making the
deployment of fully supervised algorithms unfeasible. On the other hand, the
problem of semi-supervised large-scale data streams is little explored in the
literature because most works are designed in the traditional single-node
computing environments while also being fully supervised approaches. This paper
offers Weakly Supervised Scalable Teacher Forcing Network (WeScatterNet) to
cope with the scarcity of labelled samples and the large-scale data streams
simultaneously. WeScatterNet is crafted under distributed computing platform of
Apache Spark with a data-free model fusion strategy for model compression after
parallel computing stage. It features an open network structure to address the
global and local drift problems while integrating a data augmentation,
annotation and auto-correction ($DA^3$) method for handling partially labelled
data streams. The performance of WeScatterNet is numerically evaluated in the
six large-scale data stream problems with only $25\%$ label proportions. It
shows highly competitive performance even if compared with fully supervised
learners with $100\%$ label proportions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pratama_M/0/1/0/all/0/1"&gt;Mahardhika Pratama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zain_C/0/1/0/all/0/1"&gt;Choiru Za&amp;#x27;in&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lughofer_E/0/1/0/all/0/1"&gt;Edwin Lughofer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pardede_E/0/1/0/all/0/1"&gt;Eric Pardede&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahayu_D/0/1/0/all/0/1"&gt;Dwi A. P. Rahayu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured Denoising Diffusion Models in Discrete State-Spaces. (arXiv:2107.03006v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03006</id>
        <link href="http://arxiv.org/abs/2107.03006"/>
        <updated>2021-07-08T01:57:57.929Z</updated>
        <summary type="html"><![CDATA[Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown
impressive results on image and waveform generation in continuous state spaces.
Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs),
diffusion-like generative models for discrete data that generalize the
multinomial diffusion model of Hoogeboom et al. 2021, by going beyond
corruption processes with uniform transition probabilities. This includes
corruption with transition matrices that mimic Gaussian kernels in continuous
space, matrices based on nearest neighbors in embedding space, and matrices
that introduce absorbing states. The third allows us to draw a connection
between diffusion models and autoregressive and mask-based generative models.
We show that the choice of transition matrix is an important design decision
that leads to improved results in image and text domains. We also introduce a
new loss function that combines the variational lower bound with an auxiliary
cross entropy loss. For text, this model class achieves strong results on
character-level text generation while scaling to large vocabularies on LM1B. On
the image dataset CIFAR-10, our models approach the sample quality and exceed
the log-likelihood of the continuous-space DDPM model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1"&gt;Jacob Austin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johnson_D/0/1/0/all/0/1"&gt;Daniel Johnson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1"&gt;Jonathan Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tarlow_D/0/1/0/all/0/1"&gt;Danny Tarlow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_R/0/1/0/all/0/1"&gt;Rianne van den Berg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Deep Neural Network Saliency Visualizations with Gradual Extrapolation. (arXiv:2104.04945v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04945</id>
        <link href="http://arxiv.org/abs/2104.04945"/>
        <updated>2021-07-08T01:57:57.848Z</updated>
        <summary type="html"><![CDATA[In this paper, an enhancement technique for the class activation mapping
methods such as gradient-weighted class activation maps or excitation
backpropagation is proposed to present the visual explanations of decisions
from convolutional neural network-based models. The proposed idea, called
Gradual Extrapolation, can supplement any method that generates a heatmap
picture by sharpening the output. Instead of producing a coarse localization
map that highlights the important predictive regions in the image, the proposed
method outputs the specific shape that most contributes to the model output.
Thus, the proposed method improves the accuracy of saliency maps. The effect
has been achieved by the gradual propagation of the crude map obtained in the
deep layer through all preceding layers with respect to their activations. In
validation tests conducted on a selected set of images, the faithfulness,
interpretability, and applicability of the method are evaluated. The proposed
technique significantly improves the localization detection of the neural
networks attention at low additional computational costs. Furthermore, the
proposed method is applicable to a variety deep neural network models. The code
for the method can be found at
https://github.com/szandala/gradual-extrapolation]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Szandala_T/0/1/0/all/0/1"&gt;Tomasz Szandala&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Monocular Depth Estimation via Listwise Ranking using the Plackett-Luce Model. (arXiv:2010.13118v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.13118</id>
        <link href="http://arxiv.org/abs/2010.13118"/>
        <updated>2021-07-08T01:57:57.840Z</updated>
        <summary type="html"><![CDATA[In many real-world applications, the relative depth of objects in an image is
crucial for scene understanding. Recent approaches mainly tackle the problem of
depth prediction in monocular images by treating the problem as a regression
task. Yet, being interested in an order relation in the first place, ranking
methods suggest themselves as a natural alternative to regression, and indeed,
ranking approaches leveraging pairwise comparisons as training information
("object A is closer to the camera than B") have shown promising performance on
this problem. In this paper, we elaborate on the use of so-called listwise
ranking as a generalization of the pairwise approach. Our method is based on
the Plackett-Luce (PL) model, a probability distribution on rankings, which we
combine with a state-of-the-art neural network architecture and a simple
sampling strategy to reduce training complexity. Moreover, taking advantage of
the representation of PL as a random utility model, the proposed predictor
offers a natural way to recover (shift-invariant) metric depth information from
ranking-only data provided at training time. An empirical evaluation on several
benchmark datasets in a "zero-shot" setting demonstrates the effectiveness of
our approach compared to existing ranking and regression methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lienen_J/0/1/0/all/0/1"&gt;Julian Lienen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1"&gt;Eyke H&amp;#xfc;llermeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1"&gt;Ralph Ewerth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nommensen_N/0/1/0/all/0/1"&gt;Nils Nommensen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reborn Mechanism: Rethinking the Negative Phase Information Flow in Convolutional Neural Network. (arXiv:2106.07026v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07026</id>
        <link href="http://arxiv.org/abs/2106.07026"/>
        <updated>2021-07-08T01:57:57.830Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel nonlinear activation mechanism typically for
convolutional neural network (CNN), named as reborn mechanism. In sharp
contrast to ReLU which cuts off the negative phase value, the reborn mechanism
enjoys the capacity to reborn and reconstruct dead neurons. Compared to other
improved ReLU functions, reborn mechanism introduces a more proper way to
utilize the negative phase information. Extensive experiments validate that
this activation mechanism is able to enhance the model representation ability
more significantly and make the better use of the input data information while
maintaining the advantages of the original ReLU function. Moreover, reborn
mechanism enables a non-symmetry that is hardly achieved by traditional CNNs
and can act as a channel compensation method, offering competitive or even
better performance but with fewer learned parameters than traditional methods.
Reborn mechanism was tested on various benchmark datasets, all obtaining better
performance than previous nonlinear activation functions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhicheng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kaizhu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1"&gt;Chenglei Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection and Mitigation of Rare Subclasses in Deep Neural Network Classifiers. (arXiv:1911.12780v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.12780</id>
        <link href="http://arxiv.org/abs/1911.12780"/>
        <updated>2021-07-08T01:57:57.822Z</updated>
        <summary type="html"><![CDATA[Regions of high-dimensional input spaces that are underrepresented in
training datasets reduce machine-learnt classifier performance, and may lead to
corner cases and unwanted bias for classifiers used in decision making systems.
When these regions belong to otherwise well-represented classes, their presence
and negative impact are very hard to identify. We propose an approach for the
detection and mitigation of such rare subclasses in deep neural network
classifiers. The new approach is underpinned by an easy-to-compute commonality
metric that supports the detection of rare subclasses, and comprises methods
for reducing the impact of these subclasses during both model training and
model exploitation. We demonstrate our approach using two well-known datasets,
MNIST's handwritten digits and Kaggle's cats/dogs, identifying rare subclasses
and producing models which compensate for subclass rarity. In addition we
demonstrate how our run-time approach increases the ability of users to
identify samples likely to be misclassified at run-time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paterson_C/0/1/0/all/0/1"&gt;Colin Paterson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calinescu_R/0/1/0/all/0/1"&gt;Radu Calinescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Picardi_C/0/1/0/all/0/1"&gt;Chiara Picardi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bi-Level Poisoning Attack Model and Countermeasure for Appliance Consumption Data of Smart Homes. (arXiv:2107.02897v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.02897</id>
        <link href="http://arxiv.org/abs/2107.02897"/>
        <updated>2021-07-08T01:57:57.805Z</updated>
        <summary type="html"><![CDATA[Accurate building energy prediction is useful in various applications
starting from building energy automation and management to optimal storage
control. However, vulnerabilities should be considered when designing building
energy prediction models, as intelligent attackers can deliberately influence
the model performance using sophisticated attack models. These may consequently
degrade the prediction accuracy, which may affect the efficiency and
performance of the building energy management systems. In this paper, we
investigate the impact of bi-level poisoning attacks on regression models of
energy usage obtained from household appliances. Furthermore, an effective
countermeasure against the poisoning attacks on the prediction model is
proposed in this paper. Attacks and defenses are evaluated on a benchmark
dataset. Experimental results show that an intelligent cyber-attacker can
poison the prediction model to manipulate the decision. However, our proposed
solution successfully ensures defense against such poisoning attacks
effectively compared to other benchmark techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Billah_M/0/1/0/all/0/1"&gt;Mustain Billah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anwar_A/0/1/0/all/0/1"&gt;Adnan Anwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_Z/0/1/0/all/0/1"&gt;Ziaur Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galib_S/0/1/0/all/0/1"&gt;Syed Md. Galib&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Embedding of Structural and Functional Brain Networks with Graph Neural Networks for Mental Illness Diagnosis. (arXiv:2107.03220v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2107.03220</id>
        <link href="http://arxiv.org/abs/2107.03220"/>
        <updated>2021-07-08T01:57:57.784Z</updated>
        <summary type="html"><![CDATA[Multimodal brain networks characterize complex connectivities among different
brain regions from both structural and functional aspects and provide a new
means for mental disease analysis. Recently, Graph Neural Networks (GNNs) have
become a de facto model for analyzing graph-structured data. However, how to
employ GNNs to extract effective representations from brain networks in
multiple modalities remains rarely explored. Moreover, as brain networks
provide no initial node features, how to design informative node attributes and
leverage edge weights for GNNs to learn is left unsolved. To this end, we
develop a novel multiview GNN for multimodal brain networks. In particular, we
regard each modality as a view for brain networks and employ contrastive
learning for multimodal fusion. Then, we propose a GNN model which takes
advantage of the message passing scheme by propagating messages based on degree
statistics and brain region connectivities. Extensive experiments on two
real-world disease datasets (HIV and Bipolar) demonstrate the effectiveness of
our proposed method over state-of-the-art baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yanqiao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Cui_H/0/1/0/all/0/1"&gt;Hejie Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+He_L/0/1/0/all/0/1"&gt;Lifang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lichao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Yang_C/0/1/0/all/0/1"&gt;Carl Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Complexity Guided Network Compression for Biomedical Image Segmentation. (arXiv:2107.02927v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.02927</id>
        <link href="http://arxiv.org/abs/2107.02927"/>
        <updated>2021-07-08T01:57:57.751Z</updated>
        <summary type="html"><![CDATA[Compression is a standard procedure for making convolutional neural networks
(CNNs) adhere to some specific computing resource constraints. However,
searching for a compressed architecture typically involves a series of
time-consuming training/validation experiments to determine a good compromise
between network size and performance accuracy. To address this, we propose an
image complexity-guided network compression technique for biomedical image
segmentation. Given any resource constraints, our framework utilizes data
complexity and network architecture to quickly estimate a compressed model
which does not require network training. Specifically, we map the dataset
complexity to the target network accuracy degradation caused by compression.
Such mapping enables us to predict the final accuracy for different network
sizes, based on the computed dataset complexity. Thus, one may choose a
solution that meets both the network size and segmentation accuracy
requirements. Finally, the mapping is used to determine the convolutional
layer-wise multiplicative factor for generating a compressed network. We
conduct experiments using 5 datasets, employing 3 commonly-used CNN
architectures for biomedical image segmentation as representative networks. Our
proposed framework is shown to be effective for generating compressed
segmentation networks, retaining up to $\approx 95\%$ of the full-sized network
segmentation accuracy, and at the same time, utilizing $\approx 32x$ fewer
network trainable weights (average reduction) of the full-sized networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Suraj Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_D/0/1/0/all/0/1"&gt;Danny Z. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1"&gt;X. Sharon Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controlled Caption Generation for Images Through Adversarial Attacks. (arXiv:2107.03050v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03050</id>
        <link href="http://arxiv.org/abs/2107.03050"/>
        <updated>2021-07-08T01:57:57.744Z</updated>
        <summary type="html"><![CDATA[Deep learning is found to be vulnerable to adversarial examples. However, its
adversarial susceptibility in image caption generation is under-explored. We
study adversarial examples for vision and language models, which typically
adopt an encoder-decoder framework consisting of two major components: a
Convolutional Neural Network (i.e., CNN) for image feature extraction and a
Recurrent Neural Network (RNN) for caption generation. In particular, we
investigate attacks on the visual encoder's hidden layer that is fed to the
subsequent recurrent network. The existing methods either attack the
classification layer of the visual encoder or they back-propagate the gradients
from the language model. In contrast, we propose a GAN-based algorithm for
crafting adversarial examples for neural image captioning that mimics the
internal representation of the CNN such that the resulting deep features of the
input image enable a controlled incorrect caption generation through the
recurrent network. Our contribution provides new insights for understanding
adversarial attacks on vision systems with language component. The proposed
method employs two strategies for a comprehensive evaluation. The first
examines if a neural image captioning system can be misled to output targeted
image captions. The second analyzes the possibility of keywords into the
predicted captions. Experiments show that our algorithm can craft effective
adversarial images based on the CNN hidden layers to fool captioning framework.
Moreover, we discover the proposed attack to be highly transferable. Our work
leads to new robustness implications for neural image captioning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aafaq_N/0/1/0/all/0/1"&gt;Nayyer Aafaq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1"&gt;Naveed Akhtar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Mubarak Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1"&gt;Ajmal Mian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Immunization of Pruning Attack in DNN Watermarking Using Constant Weight Code. (arXiv:2107.02961v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.02961</id>
        <link href="http://arxiv.org/abs/2107.02961"/>
        <updated>2021-07-08T01:57:57.736Z</updated>
        <summary type="html"><![CDATA[To ensure protection of the intellectual property rights of DNN models,
watermarking techniques have been investigated to insert side-information into
the models without seriously degrading the performance of original task. One of
the threats for the DNN watermarking is the pruning attack such that less
important neurons in the model are pruned to make it faster and more compact as
well as to remove the watermark. In this study, we investigate a channel coding
approach to resist the pruning attack. As the channel model is completely
different from conventional models like digital images, it has been an open
problem what kind of encoding method is suitable for DNN watermarking. A novel
encoding approach by using constant weight codes to immunize the effects of
pruning attacks is presented. To the best of our knowledge, this is the first
study that introduces an encoding technique for DNN watermarking to make it
robust against pruning attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuribayashi_M/0/1/0/all/0/1"&gt;Minoru Kuribayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yasui_T/0/1/0/all/0/1"&gt;Tatsuya Yasui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malik_A/0/1/0/all/0/1"&gt;Asad Malik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Funabiki_N/0/1/0/all/0/1"&gt;Nobuo Funabiki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the Security of Deepfake Detection. (arXiv:2107.02045v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02045</id>
        <link href="http://arxiv.org/abs/2107.02045"/>
        <updated>2021-07-08T01:57:57.728Z</updated>
        <summary type="html"><![CDATA[Deepfakes pose growing challenges to the trust of information on the
Internet. Thus, detecting deepfakes has attracted increasing attentions from
both academia and industry. State-of-the-art deepfake detection methods consist
of two key components, i.e., face extractor and face classifier, which extract
the face region in an image and classify it to be real/fake, respectively.
Existing studies mainly focused on improving the detection performance in
non-adversarial settings, leaving security of deepfake detection in adversarial
settings largely unexplored. In this work, we aim to bridge the gap. In
particular, we perform a systematic measurement study to understand the
security of the state-of-the-art deepfake detection methods in adversarial
settings. We use two large-scale public deepfakes data sources including
FaceForensics++ and Facebook Deepfake Detection Challenge, where the deepfakes
are fake face images; and we train state-of-the-art deepfake detection methods.
These detection methods can achieve 0.94--0.99 accuracies in non-adversarial
settings on these datasets. However, our measurement results uncover multiple
security limitations of the deepfake detection methods in adversarial settings.
First, we find that an attacker can evade a face extractor, i.e., the face
extractor fails to extract the correct face regions, via adding small Gaussian
noise to its deepfake images. Second, we find that a face classifier trained
using deepfakes generated by one method cannot detect deepfakes generated by
another method, i.e., an attacker can evade detection via generating deepfakes
using a new method. Third, we find that an attacker can leverage backdoor
attacks developed by the adversarial machine learning community to evade a face
classifier. Our results highlight that deepfake detection should consider the
adversarial nature of the problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1"&gt;Xiaoyu Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1"&gt;Neil Zhenqiang Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anomaly detection and automatic labeling for solar cell quality inspection based on Generative Adversarial Network. (arXiv:2103.03518v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03518</id>
        <link href="http://arxiv.org/abs/2103.03518"/>
        <updated>2021-07-08T01:57:57.708Z</updated>
        <summary type="html"><![CDATA[Quality inspection applications in industry are required to move towards a
zero-defect manufacturing scenario, withnon-destructive inspection and
traceability of 100 % of produced parts. Developing robust fault detection and
classification modelsfrom the start-up of the lines is challenging due to the
difficulty in getting enough representative samples of the faulty patternsand
the need to manually label them. This work presents a methodology to develop a
robust inspection system, targeting thesepeculiarities, in the context of solar
cell manufacturing. The methodology is divided into two phases: In the first
phase, an anomalydetection model based on a Generative Adversarial Network
(GAN) is employed. This model enables the detection and localizationof
anomalous patterns within the solar cells from the beginning, using only
non-defective samples for training and without anymanual labeling involved. In
a second stage, as defective samples arise, the detected anomalies will be used
as automaticallygenerated annotations for the supervised training of a Fully
Convolutional Network that is capable of detecting multiple types offaults. The
experimental results using 1873 EL images of monocrystalline cells show that
(a) the anomaly detection scheme can beused to start detecting features with
very little available data, (b) the anomaly detection may serve as automatic
labeling in order totrain a supervised model, and (c) segmentation and
classification results of supervised models trained with automatic labels
arecomparable to the ones obtained from the models trained with manual labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balzategui_J/0/1/0/all/0/1"&gt;Julen Balzategui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eciolaza_L/0/1/0/all/0/1"&gt;Luka Eciolaza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maestro_Watson_D/0/1/0/all/0/1"&gt;Daniel Maestro-Watson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating the progress of Deep Reinforcement Learning in the real world: aligning domain-agnostic and domain-specific research. (arXiv:2107.03015v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03015</id>
        <link href="http://arxiv.org/abs/2107.03015"/>
        <updated>2021-07-08T01:57:57.701Z</updated>
        <summary type="html"><![CDATA[Deep Reinforcement Learning (DRL) is considered a potential framework to
improve many real-world autonomous systems; it has attracted the attention of
multiple and diverse fields. Nevertheless, the successful deployment in the
real world is a test most of DRL models still need to pass. In this work we
focus on this issue by reviewing and evaluating the research efforts from both
domain-agnostic and domain-specific communities. On one hand, we offer a
comprehensive summary of DRL challenges and summarize the different proposals
to mitigate them; this helps identifying five gaps of domain-agnostic research.
On the other hand, from the domain-specific perspective, we discuss different
success stories and argue why other models might fail to be deployed. Finally,
we take up on ways to move forward accounting for both perspectives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garau_Luis_J/0/1/0/all/0/1"&gt;Juan Jose Garau-Luis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crawley_E/0/1/0/all/0/1"&gt;Edward Crawley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cameron_B/0/1/0/all/0/1"&gt;Bruce Cameron&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-supervised Left Atrium Segmentation with Mutual Consistency Training. (arXiv:2103.02911v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02911</id>
        <link href="http://arxiv.org/abs/2103.02911"/>
        <updated>2021-07-08T01:57:57.694Z</updated>
        <summary type="html"><![CDATA[Semi-supervised learning has attracted great attention in the field of
machine learning, especially for medical image segmentation tasks, since it
alleviates the heavy burden of collecting abundant densely annotated data for
training. However, most of existing methods underestimate the importance of
challenging regions (e.g. small branches or blurred edges) during training. We
believe that these unlabeled regions may contain more crucial information to
minimize the uncertainty prediction for the model and should be emphasized in
the training process. Therefore, in this paper, we propose a novel Mutual
Consistency Network (MC-Net) for semi-supervised left atrium segmentation from
3D MR images. Particularly, our MC-Net consists of one encoder and two slightly
different decoders, and the prediction discrepancies of two decoders are
transformed as an unsupervised loss by our designed cycled pseudo label scheme
to encourage mutual consistency. Such mutual consistency encourages the two
decoders to have consistent and low-entropy predictions and enables the model
to gradually capture generalized features from these unlabeled challenging
regions. We evaluate our MC-Net on the public Left Atrium (LA) database and it
obtains impressive performance gains by exploiting the unlabeled data
effectively. Our MC-Net outperforms six recent semi-supervised methods for left
atrium segmentation, and sets the new state-of-the-art performance on the LA
database.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yicheng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Minfeng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1"&gt;Zongyuan Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jianfei Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Residual Star Generative Adversarial Network for multi-domain Image Super-Resolution. (arXiv:2107.03145v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.03145</id>
        <link href="http://arxiv.org/abs/2107.03145"/>
        <updated>2021-07-08T01:57:57.686Z</updated>
        <summary type="html"><![CDATA[Recently, most of state-of-the-art single image super-resolution (SISR)
methods have attained impressive performance by using deep convolutional neural
networks (DCNNs). The existing SR methods have limited performance due to a
fixed degradation settings, i.e. usually a bicubic downscaling of
low-resolution (LR) image. However, in real-world settings, the LR degradation
process is unknown which can be bicubic LR, bilinear LR, nearest-neighbor LR,
or real LR. Therefore, most SR methods are ineffective and inefficient in
handling more than one degradation settings within a single network. To handle
the multiple degradation, i.e. refers to multi-domain image super-resolution,
we propose a deep Super-Resolution Residual StarGAN (SR2*GAN), a novel and
scalable approach that super-resolves the LR images for the multiple LR domains
using only a single model. The proposed scheme is trained in a StarGAN like
network topology with a single generator and discriminator networks. We
demonstrate the effectiveness of our proposed approach in quantitative and
qualitative experiments compared to other state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Umer_R/0/1/0/all/0/1"&gt;Rao Muhammad Umer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Munir_A/0/1/0/all/0/1"&gt;Asad Munir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Micheloni_C/0/1/0/all/0/1"&gt;Christian Micheloni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Categorical Relation-Preserving Contrastive Knowledge Distillation for Medical Image Classification. (arXiv:2107.03225v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03225</id>
        <link href="http://arxiv.org/abs/2107.03225"/>
        <updated>2021-07-08T01:57:57.679Z</updated>
        <summary type="html"><![CDATA[The amount of medical images for training deep classification models is
typically very scarce, making these deep models prone to overfit the training
data. Studies showed that knowledge distillation (KD), especially the
mean-teacher framework which is more robust to perturbations, can help mitigate
the over-fitting effect. However, directly transferring KD from computer vision
to medical image classification yields inferior performance as medical images
suffer from higher intra-class variance and class imbalance. To address these
issues, we propose a novel Categorical Relation-preserving Contrastive
Knowledge Distillation (CRCKD) algorithm, which takes the commonly used
mean-teacher model as the supervisor. Specifically, we propose a novel
Class-guided Contrastive Distillation (CCD) module to pull closer positive
image pairs from the same class in the teacher and student models, while
pushing apart negative image pairs from different classes. With this
regularization, the feature distribution of the student model shows higher
intra-class similarity and inter-class variance. Besides, we propose a
Categorical Relation Preserving (CRP) loss to distill the teacher's relational
knowledge in a robust and class-balanced manner. With the contribution of the
CCD and CRP, our CRCKD algorithm can distill the relational knowledge more
comprehensively. Extensive experiments on the HAM10000 and APTOS datasets
demonstrate the superiority of the proposed CRCKD method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1"&gt;Xiaohan Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1"&gt;Yuenan Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1"&gt;Yixuan Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongsheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_M/0/1/0/all/0/1"&gt;Max Q.-H. Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-View Exocentric to Egocentric Video Synthesis. (arXiv:2107.03120v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03120</id>
        <link href="http://arxiv.org/abs/2107.03120"/>
        <updated>2021-07-08T01:57:57.660Z</updated>
        <summary type="html"><![CDATA[Cross-view video synthesis task seeks to generate video sequences of one view
from another dramatically different view. In this paper, we investigate the
exocentric (third-person) view to egocentric (first-person) view video
generation task. This is challenging because egocentric view sometimes is
remarkably different from the exocentric view. Thus, transforming the
appearances across the two different views is a non-trivial task. Particularly,
we propose a novel Bi-directional Spatial Temporal Attention Fusion Generative
Adversarial Network (STA-GAN) to learn both spatial and temporal information to
generate egocentric video sequences from the exocentric view. The proposed
STA-GAN consists of three parts: temporal branch, spatial branch, and attention
fusion. First, the temporal and spatial branches generate a sequence of fake
frames and their corresponding features. The fake frames are generated in both
downstream and upstream directions for both temporal and spatial branches.
Next, the generated four different fake frames and their corresponding features
(spatial and temporal branches in two directions) are fed into a novel
multi-generation attention fusion module to produce the final video sequence.
Meanwhile, we also propose a novel temporal and spatial dual-discriminator for
more robust network optimization. Extensive experiments on the Side2Ego and
Top2Ego datasets show that the proposed STA-GAN significantly outperforms the
existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Gaowen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Hao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Latapie_H/0/1/0/all/0/1"&gt;Hugo Latapie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Corso_J/0/1/0/all/0/1"&gt;Jason Corso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yan Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Zero to The Hero: A Collaborative Market Aware Recommendation System for Crowd Workers. (arXiv:2107.02890v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2107.02890</id>
        <link href="http://arxiv.org/abs/2107.02890"/>
        <updated>2021-07-08T01:57:57.652Z</updated>
        <summary type="html"><![CDATA[The success of software crowdsourcing depends on active and trustworthy pool
of worker supply. The uncertainty of crowd workers' behaviors makes it
challenging to predict workers' success and plan accordingly. In a competitive
crowdsourcing marketplace, competition for success over shared tasks adds
another layer of uncertainty in crowd workers' decision-making process.
Preliminary analysis on software worker behaviors reveals an alarming task
dropping rate of 82.9%. These factors lead to the need for an automated
recommendation system for CSD workers to improve the visibility and
predictability of their success in the competition. To that end, this paper
proposes a collaborative recommendation system for crowd workers. The proposed
recommendation system method uses five input metrics based on workers'
collaboration history in the pool, workers' preferences in taking tasks in
terms of monetary prize and duration, workers' specialty, and workers'
proficiency. The proposed method then recommends the most suitable tasks for a
worker to compete on based on workers' probability of success in the task.
Experimental results on 260 active crowd workers demonstrate that just
following the top three success probabilities of task recommendations, workers
can achieve success up to 86%]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shamszare_H/0/1/0/all/0/1"&gt;Hamid Shamszare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saremi_R/0/1/0/all/0/1"&gt;Razieh Saremi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jena_S/0/1/0/all/0/1"&gt;Sanam Jena&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is 2D Heatmap Representation Even Necessary for Human Pose Estimation?. (arXiv:2107.03332v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03332</id>
        <link href="http://arxiv.org/abs/2107.03332"/>
        <updated>2021-07-08T01:57:57.645Z</updated>
        <summary type="html"><![CDATA[The 2D heatmap representation has dominated human pose estimation for years
due to its high performance. However, heatmap-based approaches have some
drawbacks: 1) The performance drops dramatically in the low-resolution images,
which are frequently encountered in real-world scenarios. 2) To improve the
localization precision, multiple upsample layers may be needed to recover the
feature map resolution from low to high, which are computationally expensive.
3) Extra coordinate refinement is usually necessary to reduce the quantization
error of downscaled heatmaps. To address these issues, we propose a
\textbf{Sim}ple yet promising \textbf{D}isentangled \textbf{R}epresentation for
keypoint coordinate (\emph{SimDR}), reformulating human keypoint localization
as a task of classification. In detail, we propose to disentangle the
representation of horizontal and vertical coordinates for keypoint location,
leading to a more efficient scheme without extra upsampling and refinement.
Comprehensive experiments conducted over COCO dataset show that the proposed
\emph{heatmap-free} methods outperform \emph{heatmap-based} counterparts in all
tested input resolutions, especially in lower resolutions by a large margin.
Code will be made publicly available at \url{https://github.com/leeyegy/SimDR}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yanjie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Sen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shoukui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhicheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wankou Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1"&gt;Shu-Tao Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1"&gt;Erjin Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Long Short-Term Transformer for Online Action Detection. (arXiv:2107.03377v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03377</id>
        <link href="http://arxiv.org/abs/2107.03377"/>
        <updated>2021-07-08T01:57:57.638Z</updated>
        <summary type="html"><![CDATA[In this paper, we present Long Short-term TRansformer (LSTR), a new temporal
modeling algorithm for online action detection, by employing a long- and
short-term memories mechanism that is able to model prolonged sequence data. It
consists of an LSTR encoder that is capable of dynamically exploiting
coarse-scale historical information from an extensively long time window (e.g.,
2048 long-range frames of up to 8 minutes), together with an LSTR decoder that
focuses on a short time window (e.g., 32 short-range frames of 8 seconds) to
model the fine-scale characterization of the ongoing event. Compared to prior
work, LSTR provides an effective and efficient method to model long videos with
less heuristic algorithm design. LSTR achieves significantly improved results
on standard online action detection benchmarks, THUMOS'14, TVSeries, and HACS
Segment, over the existing state-of-the-art approaches. Extensive empirical
analysis validates the setup of the long- and short-term memories and the
design choices of LSTR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mingze Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1"&gt;Yuanjun Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xinyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1"&gt;Wei Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1"&gt;Zhuowen Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latent Space Regularization for Unsupervised Domain Adaptation in Semantic Segmentation. (arXiv:2104.02633v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02633</id>
        <link href="http://arxiv.org/abs/2104.02633"/>
        <updated>2021-07-08T01:57:57.631Z</updated>
        <summary type="html"><![CDATA[Deep convolutional neural networks for semantic segmentation achieve
outstanding accuracy, however they also have a couple of major drawbacks:
first, they do not generalize well to distributions slightly different from the
one of the training data; second, they require a huge amount of labeled data
for their optimization. In this paper, we introduce feature-level space-shaping
regularization strategies to reduce the domain discrepancy in semantic
segmentation. In particular, for this purpose we jointly enforce a clustering
objective, a perpendicularity constraint and a norm alignment goal on the
feature vectors corresponding to source and target samples. Additionally, we
propose a novel measure able to capture the relative efficacy of an adaptation
strategy compared to supervised training. We verify the effectiveness of such
methods in the autonomous driving setting achieving state-of-the-art results in
multiple synthetic-to-real road scenes benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barbato_F/0/1/0/all/0/1"&gt;Francesco Barbato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toldo_M/0/1/0/all/0/1"&gt;Marco Toldo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michieli_U/0/1/0/all/0/1"&gt;Umberto Michieli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zanuttigh_P/0/1/0/all/0/1"&gt;Pietro Zanuttigh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Samplets: A new paradigm for data compression. (arXiv:2107.03337v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2107.03337</id>
        <link href="http://arxiv.org/abs/2107.03337"/>
        <updated>2021-07-08T01:57:57.624Z</updated>
        <summary type="html"><![CDATA[In this article, we introduce the novel concept of samplets by transferring
the construction of Tausch-White wavelets to the realm of data. This way we
obtain a multilevel representation of discrete data which directly enables data
compression, detection of singularities and adaptivity. Applying samplets to
represent kernel matrices, as they arise in kernel based learning or Gaussian
process regression, we end up with quasi-sparse matrices. By thresholding small
entries, these matrices are compressible to O(N log N) relevant entries, where
N is the number of data points. This feature allows for the use of fill-in
reducing reorderings to obtain a sparse factorization of the compressed
matrices. Besides the comprehensive introduction to samplets and their
properties, we present extensive numerical studies to benchmark the approach.
Our results demonstrate that samplets mark a considerable step in the direction
of making large data sets accessible for analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Harbrecht_H/0/1/0/all/0/1"&gt;Helmut Harbrecht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Multerer_M/0/1/0/all/0/1"&gt;Michael Multerer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Medical Transformer: Gated Axial-Attention for Medical Image Segmentation. (arXiv:2102.10662v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10662</id>
        <link href="http://arxiv.org/abs/2102.10662"/>
        <updated>2021-07-08T01:57:57.604Z</updated>
        <summary type="html"><![CDATA[Over the past decade, Deep Convolutional Neural Networks have been widely
adopted for medical image segmentation and shown to achieve adequate
performance. However, due to the inherent inductive biases present in the
convolutional architectures, they lack understanding of long-range dependencies
in the image. Recently proposed Transformer-based architectures that leverage
self-attention mechanism encode long-range dependencies and learn
representations that are highly expressive. This motivates us to explore
Transformer-based solutions and study the feasibility of using
Transformer-based network architectures for medical image segmentation tasks.
Majority of existing Transformer-based network architectures proposed for
vision applications require large-scale datasets to train properly. However,
compared to the datasets for vision applications, for medical imaging the
number of data samples is relatively low, making it difficult to efficiently
train transformers for medical applications. To this end, we propose a Gated
Axial-Attention model which extends the existing architectures by introducing
an additional control mechanism in the self-attention module. Furthermore, to
train the model effectively on medical images, we propose a Local-Global
training strategy (LoGo) which further improves the performance. Specifically,
we operate on the whole image and patches to learn global and local features,
respectively. The proposed Medical Transformer (MedT) is evaluated on three
different medical image segmentation datasets and it is shown that it achieves
better performance than the convolutional and other related transformer-based
architectures. Code: https://github.com/jeya-maria-jose/Medical-Transformer]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Valanarasu_J/0/1/0/all/0/1"&gt;Jeya Maria Jose Valanarasu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oza_P/0/1/0/all/0/1"&gt;Poojan Oza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hacihaliloglu_I/0/1/0/all/0/1"&gt;Ilker Hacihaliloglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1"&gt;Vishal M. Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Egocentric Videoconferencing. (arXiv:2107.03109v1 [cs.GR])]]></title>
        <id>http://arxiv.org/abs/2107.03109</id>
        <link href="http://arxiv.org/abs/2107.03109"/>
        <updated>2021-07-08T01:57:57.596Z</updated>
        <summary type="html"><![CDATA[We introduce a method for egocentric videoconferencing that enables
hands-free video calls, for instance by people wearing smart glasses or other
mixed-reality devices. Videoconferencing portrays valuable non-verbal
communication and face expression cues, but usually requires a front-facing
camera. Using a frontal camera in a hands-free setting when a person is on the
move is impractical. Even holding a mobile phone camera in the front of the
face while sitting for a long duration is not convenient. To overcome these
issues, we propose a low-cost wearable egocentric camera setup that can be
integrated into smart glasses. Our goal is to mimic a classical video call, and
therefore, we transform the egocentric perspective of this camera into a front
facing video. To this end, we employ a conditional generative adversarial
neural network that learns a transition from the highly distorted egocentric
views to frontal views common in videoconferencing. Our approach learns to
transfer expression details directly from the egocentric view without using a
complex intermediate parametric expressions model, as it is used by related
face reenactment methods. We successfully handle subtle expressions, not easily
captured by parametric blendshape-based solutions, e.g., tongue movement, eye
movements, eye blinking, strong expressions and depth varying movements. To get
control over the rigid head movements in the target view, we condition the
generator on synthetic renderings of a moving neutral face. This allows us to
synthesis results at different head poses. Our technique produces temporally
smooth video-realistic renderings in real-time using a video-to-video
translation network in conjunction with a temporal discriminator. We
demonstrate the improved capabilities of our technique by comparing against
related state-of-the art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elgharib_M/0/1/0/all/0/1"&gt;Mohamed Elgharib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mendiratta_M/0/1/0/all/0/1"&gt;Mohit Mendiratta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1"&gt;Justus Thies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1"&gt;Matthias Nie&amp;#xdf;ner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seidel_H/0/1/0/all/0/1"&gt;Hans-Peter Seidel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1"&gt;Ayush Tewari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1"&gt;Vladislav Golyanik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HIDA: Towards Holistic Indoor Understanding for the Visually Impaired via Semantic Instance Segmentation with a Wearable Solid-State LiDAR Sensor. (arXiv:2107.03180v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03180</id>
        <link href="http://arxiv.org/abs/2107.03180"/>
        <updated>2021-07-08T01:57:57.587Z</updated>
        <summary type="html"><![CDATA[Independently exploring unknown spaces or finding objects in an indoor
environment is a daily but challenging task for visually impaired people.
However, common 2D assistive systems lack depth relationships between various
objects, resulting in difficulty to obtain accurate spatial layout and relative
positions of objects. To tackle these issues, we propose HIDA, a lightweight
assistive system based on 3D point cloud instance segmentation with a
solid-state LiDAR sensor, for holistic indoor detection and avoidance. Our
entire system consists of three hardware components, two interactive
functions~(obstacle avoidance and object finding) and a voice user interface.
Based on voice guidance, the point cloud from the most recent state of the
changing indoor environment is captured through an on-site scanning performed
by the user. In addition, we design a point cloud segmentation model with dual
lightweight decoders for semantic and offset predictions, which satisfies the
efficiency of the whole system. After the 3D instance segmentation, we
post-process the segmented point cloud by removing outliers and projecting all
points onto a top-view 2D map representation. The system integrates the
information above and interacts with users intuitively by acoustic feedback.
The proposed 3D instance segmentation model has achieved state-of-the-art
performance on ScanNet v2 dataset. Comprehensive field tests with various tasks
in a user study verify the usability and effectiveness of our system for
assisting visually impaired people in holistic indoor understanding, obstacle
avoidance and object search.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Huayao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Ruiping Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kailun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiaming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1"&gt;Kunyu Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1"&gt;Rainer Stiefelhagen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Semantic Segmentation using Psychometric Learning. (arXiv:2107.03212v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03212</id>
        <link href="http://arxiv.org/abs/2107.03212"/>
        <updated>2021-07-08T01:57:57.579Z</updated>
        <summary type="html"><![CDATA[Assigning meaning to parts of image data is the goal of semantic image
segmentation. Machine learning methods, specifically supervised learning is
commonly used in a variety of tasks formulated as semantic segmentation. One of
the major challenges in the supervised learning approaches is expressing and
collecting the rich knowledge that experts have with respect to the meaning
present in the image data. Towards this, typically a fixed set of labels is
specified and experts are tasked with annotating the pixels, patches or
segments in the images with the given labels. In general, however, the set of
classes does not fully capture the rich semantic information present in the
images. For example, in medical imaging such as histology images, the different
parts of cells could be grouped and sub-grouped based on the expertise of the
pathologist.

To achieve such a precise semantic representation of the concepts in the
image, we need access to the full depth of knowledge of the annotator. In this
work, we develop a novel approach to collect segmentation annotations from
experts based on psychometric testing. Our method consists of the psychometric
testing procedure, active query selection, query enhancement, and a deep metric
learning model to achieve a patch-level image embedding that allows for
semantic segmentation of images. We show the merits of our method with
evaluation on the synthetically generated image, aerial image and histology
image.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1"&gt;Lu Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1"&gt;Vlado Menkovski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shiwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1"&gt;Mykola Pechenizkiy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mitigating Generation Shifts for Generalized Zero-Shot Learning. (arXiv:2107.03163v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03163</id>
        <link href="http://arxiv.org/abs/2107.03163"/>
        <updated>2021-07-08T01:57:57.571Z</updated>
        <summary type="html"><![CDATA[Generalized Zero-Shot Learning (GZSL) is the task of leveraging semantic
information (e.g., attributes) to recognize the seen and unseen samples, where
unseen classes are not observable during training. It is natural to derive
generative models and hallucinate training samples for unseen classes based on
the knowledge learned from the seen samples. However, most of these models
suffer from the `generation shifts', where the synthesized samples may drift
from the real distribution of unseen data. In this paper, we conduct an
in-depth analysis on this issue and propose a novel Generation Shifts
Mitigating Flow (GSMFlow) framework, which is comprised of multiple conditional
affine coupling layers for learning unseen data synthesis efficiently and
effectively. In particular, we identify three potential problems that trigger
the generation shifts, i.e., semantic inconsistency, variance decay, and
structural permutation and address them respectively. First, to reinforce the
correlations between the generated samples and the respective attributes, we
explicitly embed the semantic information into the transformations in each of
the coupling layers. Second, to recover the intrinsic variance of the
synthesized unseen features, we introduce a visual perturbation strategy to
diversify the intra-class variance of generated data and hereby help adjust the
decision boundary of the classifier. Third, to avoid structural permutation in
the semantic space, we propose a relative positioning strategy to manipulate
the attribute embeddings, guiding which to fully preserve the inter-class
geometric structure. Experimental results demonstrate that GSMFlow achieves
state-of-the-art recognition performance in both conventional and generalized
zero-shot settings. Our code is available at:
https://github.com/uqzhichen/GSMFlow]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yadan Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1"&gt;Ruihong Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jingjing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zi Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Action Units Recognition Using Improved Pairwise Deep Architecture. (arXiv:2107.03143v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03143</id>
        <link href="http://arxiv.org/abs/2107.03143"/>
        <updated>2021-07-08T01:57:57.551Z</updated>
        <summary type="html"><![CDATA[Facial Action Units (AUs) represent a set of facial muscular activities and
various combinations of AUs can represent a wide range of emotions. AU
recognition is often used in many applications, including marketing,
healthcare, education, and so forth. Although a lot of studies have developed
various methods to improve recognition accuracy, it still remains a major
challenge for AU recognition. In the Affective Behavior Analysis in-the-wild
(ABAW) 2020 competition, we proposed a new automatic Action Units (AUs)
recognition method using a pairwise deep architecture to derive the
Pseudo-Intensities of each AU and then convert them into predicted intensities.
This year, we introduced a new technique to last year's framework to further
reduce AU recognition errors due to temporary face occlusion such as temporary
face occlusion such as face hiding or large face orientation. We obtained a
score of 0.65 in the validation data set for this year's competition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saito_J/0/1/0/all/0/1"&gt;Junya Saito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mi_X/0/1/0/all/0/1"&gt;Xiaoyu Mi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uchida_A/0/1/0/all/0/1"&gt;Akiyoshi Uchida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Youoku_S/0/1/0/all/0/1"&gt;Sachihiro Youoku&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamamoto_T/0/1/0/all/0/1"&gt;Takahisa Yamamoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murase_K/0/1/0/all/0/1"&gt;Kentaro Murase&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trans4Trans: Efficient Transformer for Transparent Object Segmentation to Help Visually Impaired People Navigate in the Real World. (arXiv:2107.03172v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03172</id>
        <link href="http://arxiv.org/abs/2107.03172"/>
        <updated>2021-07-08T01:57:57.544Z</updated>
        <summary type="html"><![CDATA[Common fully glazed facades and transparent objects present architectural
barriers and impede the mobility of people with low vision or blindness, for
instance, a path detected behind a glass door is inaccessible unless it is
correctly perceived and reacted. However, segmenting these safety-critical
objects is rarely covered by conventional assistive technologies. To tackle
this issue, we construct a wearable system with a novel dual-head Transformer
for Transparency (Trans4Trans) model, which is capable of segmenting general
and transparent objects and performing real-time wayfinding to assist people
walking alone more safely. Especially, both decoders created by our proposed
Transformer Parsing Module (TPM) enable effective joint learning from different
datasets. Besides, the efficient Trans4Trans model composed of symmetric
transformer-based encoder and decoder, requires little computational expenses
and is readily deployed on portable GPUs. Our Trans4Trans model outperforms
state-of-the-art methods on the test sets of Stanford2D3D and Trans10K-v2
datasets and obtains mIoU of 45.13% and 75.14%, respectively. Through various
pre-tests and a user study conducted in indoor and outdoor scenarios, the
usability and reliability of our assistive system have been extensively
verified.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiaming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kailun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Constantinescu_A/0/1/0/all/0/1"&gt;Angela Constantinescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1"&gt;Kunyu Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1"&gt;Karin M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1"&gt;Rainer Stiefelhagen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Logit-based Uncertainty Measure in Classification. (arXiv:2107.02845v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.02845</id>
        <link href="http://arxiv.org/abs/2107.02845"/>
        <updated>2021-07-08T01:57:57.537Z</updated>
        <summary type="html"><![CDATA[We introduce a new, reliable, and agnostic uncertainty measure for
classification tasks called logit uncertainty. It is based on logit outputs of
neural networks. We in particular show that this new uncertainty measure yields
a superior performance compared to existing uncertainty measures on different
tasks, including out of sample detection and finding erroneous predictions. We
analyze theoretical foundations of the measure and explore a relationship with
high density regions. We also demonstrate how to test uncertainty using
intermediate outputs in training of generative adversarial networks. We propose
two potential ways to utilize logit-based uncertainty in real world
applications, and show that the uncertainty measure outperforms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Huiyu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klabjan_D/0/1/0/all/0/1"&gt;Diego Klabjan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Novel Visual Category Discovery with Dual Ranking Statistics and Mutual Knowledge Distillation. (arXiv:2107.03358v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03358</id>
        <link href="http://arxiv.org/abs/2107.03358"/>
        <updated>2021-07-08T01:57:57.530Z</updated>
        <summary type="html"><![CDATA[In this paper, we tackle the problem of novel visual category discovery,
i.e., grouping unlabelled images from new classes into different semantic
partitions by leveraging a labelled dataset that contains images from other
different but relevant categories. This is a more realistic and challenging
setting than conventional semi-supervised learning. We propose a two-branch
learning framework for this problem, with one branch focusing on local
part-level information and the other branch focusing on overall
characteristics. To transfer knowledge from the labelled data to the
unlabelled, we propose using dual ranking statistics on both branches to
generate pseudo labels for training on the unlabelled data. We further
introduce a mutual knowledge distillation method to allow information exchange
and encourage agreement between the two branches for discovering new
categories, allowing our model to enjoy the benefits of global and local
features. We comprehensively evaluate our method on public benchmarks for
generic object classification, as well as the more challenging datasets for
fine-grained visual recognition, achieving state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1"&gt;Bingchen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1"&gt;Kai Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speaker embeddings by modeling channel-wise correlations. (arXiv:2104.02571v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02571</id>
        <link href="http://arxiv.org/abs/2104.02571"/>
        <updated>2021-07-08T01:57:57.522Z</updated>
        <summary type="html"><![CDATA[Speaker embeddings extracted with deep 2D convolutional neural networks are
typically modeled as projections of first and second order statistics of
channel-frequency pairs onto a linear layer, using either average or attentive
pooling along the time axis. In this paper we examine an alternative pooling
method, where pairwise correlations between channels for given frequencies are
used as statistics. The method is inspired by style-transfer methods in
computer vision, where the style of an image, modeled by the matrix of
channel-wise correlations, is transferred to another image, in order to produce
a new image having the style of the first and the content of the second. By
drawing analogies between image style and speaker characteristics, and between
image content and phonetic sequence, we explore the use of such channel-wise
correlations features to train a ResNet architecture in an end-to-end fashion.
Our experiments on VoxCeleb demonstrate the effectiveness of the proposed
pooling method in speaker recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Stafylakis_T/0/1/0/all/0/1"&gt;Themos Stafylakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rohdin_J/0/1/0/all/0/1"&gt;Johan Rohdin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Burget_L/0/1/0/all/0/1"&gt;Lukas Burget&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FBC-GAN: Diverse and Flexible Image Synthesis via Foreground-Background Composition. (arXiv:2107.03166v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03166</id>
        <link href="http://arxiv.org/abs/2107.03166"/>
        <updated>2021-07-08T01:57:57.502Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) have become the de-facto standard in
image synthesis. However, without considering the foreground-background
decomposition, existing GANs tend to capture excessive content correlation
between foreground and background, thus constraining the diversity in image
generation. This paper presents a novel Foreground-Background Composition GAN
(FBC-GAN) that performs image generation by generating foreground objects and
background scenes concurrently and independently, followed by composing them
with style and geometrical consistency. With this explicit design, FBC-GAN can
generate images with foregrounds and backgrounds that are mutually independent
in contents, thus lifting the undesirably learned content correlation
constraint and achieving superior diversity. It also provides excellent
flexibility by allowing the same foreground object with different background
scenes, the same background scene with varying foreground objects, or the same
foreground object and background scene with different object positions, sizes
and poses. It can compose foreground objects and background scenes sampled from
different datasets as well. Extensive experiments over multiple datasets show
that FBC-GAN achieves competitive visual realism and superior diversity as
compared with state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_K/0/1/0/all/0/1"&gt;Kaiwen Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Gongjie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1"&gt;Fangneng Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiaxing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shijian Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting with Confidence on Unseen Distributions. (arXiv:2107.03315v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03315</id>
        <link href="http://arxiv.org/abs/2107.03315"/>
        <updated>2021-07-08T01:57:57.493Z</updated>
        <summary type="html"><![CDATA[Recent work has shown that the performance of machine learning models can
vary substantially when models are evaluated on data drawn from a distribution
that is close to but different from the training distribution. As a result,
predicting model performance on unseen distributions is an important challenge.
Our work connects techniques from domain adaptation and predictive uncertainty
literature, and allows us to predict model accuracy on challenging unseen
distributions without access to labeled data. In the context of distribution
shift, distributional distances are often used to adapt models and improve
their performance on new domains, however accuracy estimation, or other forms
of predictive uncertainty, are often neglected in these investigations. Through
investigating a wide range of established distributional distances, such as
Frechet distance or Maximum Mean Discrepancy, we determine that they fail to
induce reliable estimates of performance under distribution shift. On the other
hand, we find that the difference of confidences (DoC) of a classifier's
predictions successfully estimates the classifier's performance change over a
variety of shifts. We specifically investigate the distinction between
synthetic and natural distribution shifts and observe that despite its
simplicity DoC consistently outperforms other quantifications of distributional
difference. $DoC$ reduces predictive error by almost half ($46\%$) on several
realistic and challenging distribution shifts, e.g., on the ImageNet-Vid-Robust
and ImageNet-Rendition datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guillory_D/0/1/0/all/0/1"&gt;Devin Guillory&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1"&gt;Vaishaal Shankar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ebrahimi_S/0/1/0/all/0/1"&gt;Sayna Ebrahimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1"&gt;Trevor Darrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1"&gt;Ludwig Schmidt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning based Micro-expression Recognition: A Survey. (arXiv:2107.02823v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.02823</id>
        <link href="http://arxiv.org/abs/2107.02823"/>
        <updated>2021-07-08T01:57:57.486Z</updated>
        <summary type="html"><![CDATA[Micro-expressions (MEs) are involuntary facial movements revealing people's
hidden feelings in high-stake situations and have practical importance in
medical treatment, national security, interrogations and many human-computer
interaction systems. Early methods for MER mainly based on traditional
appearance and geometry features. Recently, with the success of deep learning
(DL) in various fields, neural networks have received increasing interests in
MER. Different from macro-expressions, MEs are spontaneous, subtle, and rapid
facial movements, leading to difficult data collection, thus have small-scale
datasets. DL based MER becomes challenging due to above ME characters. To data,
various DL approaches have been proposed to solve the ME issues and improve MER
performance. In this survey, we provide a comprehensive review of deep
micro-expression recognition (MER), including datasets, deep MER pipeline, and
the bench-marking of most influential methods. This survey defines a new
taxonomy for the field, encompassing all aspects of MER based on DL. For each
aspect, the basic approaches and advanced developments are summarized and
discussed. In addition, we conclude the remaining challenges and and potential
directions for the design of robust deep MER systems. To the best of our
knowledge, this is the first survey of deep MER methods, and this survey can
serve as a reference point for future MER research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yante Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jinsheng Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohammadifoumani_S/0/1/0/all/0/1"&gt;Seyednavid Mohammadifoumani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1"&gt;Guoying Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rotation Transformation Network: Learning View-Invariant Point Cloud for Classification and Segmentation. (arXiv:2107.03105v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03105</id>
        <link href="http://arxiv.org/abs/2107.03105"/>
        <updated>2021-07-08T01:57:57.474Z</updated>
        <summary type="html"><![CDATA[Many recent works show that a spatial manipulation module could boost the
performances of deep neural networks (DNNs) for 3D point cloud analysis. In
this paper, we aim to provide an insight into spatial manipulation modules.
Firstly, we find that the smaller the rotational degree of freedom (RDF) of
objects is, the more easily these objects are handled by these DNNs. Then, we
investigate the effect of the popular T-Net module and find that it could not
reduce the RDF of objects. Motivated by the above two issues, we propose a
rotation transformation network for point cloud analysis, called RTN, which
could reduce the RDF of input 3D objects to 0. The RTN could be seamlessly
inserted into many existing DNNs for point cloud analysis. Extensive
experimental results on 3D point cloud classification and segmentation tasks
demonstrate that the proposed RTN could improve the performances of several
state-of-the-art methods significantly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1"&gt;Shuang Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1"&gt;Qiulei Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhanyi Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing the structural bases of typicality effects in deep learning. (arXiv:2107.03279v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03279</id>
        <link href="http://arxiv.org/abs/2107.03279"/>
        <updated>2021-07-08T01:57:57.465Z</updated>
        <summary type="html"><![CDATA[In this paper, we hypothesize that the effects of the degree of typicality in
natural semantic categories can be generated based on the structure of
artificial categories learned with deep learning models. Motivated by the human
approach to representing natural semantic categories and based on the Prototype
Theory foundations, we propose a novel Computational Prototype Model (CPM) to
represent the internal structure of semantic categories. Unlike other prototype
learning approaches, our mathematical framework proposes a first approach to
provide deep neural networks with the ability to model abstract semantic
concepts such as category central semantic meaning, typicality degree of an
object's image, and family resemblance relationship. We proposed several
methodologies based on the typicality's concept to evaluate our CPM-model in
image semantic processing tasks such as image classification, a global semantic
description, and transfer learning. Our experiments on different image
datasets, such as ImageNet and Coco, showed that our approach might be an
admissible proposition in the effort to endow machines with greater power of
abstraction for the semantic representation of objects' categories.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pino_O/0/1/0/all/0/1"&gt;Omar Vidal Pino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nascimento_E/0/1/0/all/0/1"&gt;Erickson Rangel Nascimento&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Campos_M/0/1/0/all/0/1"&gt;Mario Fernando Montenegro Campos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Data Balancing for Unlabeled Satellite Imagery. (arXiv:2107.03227v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03227</id>
        <link href="http://arxiv.org/abs/2107.03227"/>
        <updated>2021-07-08T01:57:57.441Z</updated>
        <summary type="html"><![CDATA[Data imbalance is a ubiquitous problem in machine learning. In large scale
collected and annotated datasets, data imbalance is either mitigated manually
by undersampling frequent classes and oversampling rare classes, or planned for
with imputation and augmentation techniques. In both cases balancing data
requires labels. In other words, only annotated data can be balanced.
Collecting fully annotated datasets is challenging, especially for large scale
satellite systems such as the unlabeled NASA's 35 PB Earth Imagery dataset.
Although the NASA Earth Imagery dataset is unlabeled, there are implicit
properties of the data source that we can rely on to hypothesize about its
imbalance, such as distribution of land and water in the case of the Earth's
imagery. We present a new iterative method to balance unlabeled data. Our
method utilizes image embeddings as a proxy for image labels that can be used
to balance data, and ultimately when trained increases overall accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1"&gt;Deep Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_E/0/1/0/all/0/1"&gt;Erin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1"&gt;Anirudh Koul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1"&gt;Siddha Ganju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasam_M/0/1/0/all/0/1"&gt;Meher Anand Kasam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WeClick: Weakly-Supervised Video Semantic Segmentation with Click Annotations. (arXiv:2107.03088v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03088</id>
        <link href="http://arxiv.org/abs/2107.03088"/>
        <updated>2021-07-08T01:57:57.432Z</updated>
        <summary type="html"><![CDATA[Compared with tedious per-pixel mask annotating, it is much easier to
annotate data by clicks, which costs only several seconds for an image.
However, applying clicks to learn video semantic segmentation model has not
been explored before. In this work, we propose an effective weakly-supervised
video semantic segmentation pipeline with click annotations, called WeClick,
for saving laborious annotating effort by segmenting an instance of the
semantic class with only a single click. Since detailed semantic information is
not captured by clicks, directly training with click labels leads to poor
segmentation predictions. To mitigate this problem, we design a novel memory
flow knowledge distillation strategy to exploit temporal information (named
memory flow) in abundant unlabeled video frames, by distilling the neighboring
predictions to the target frame via estimated motion. Moreover, we adopt
vanilla knowledge distillation for model compression. In this case, WeClick
learns compact video semantic segmentation models with the low-cost click
annotations during the training phase yet achieves real-time and accurate
models during the inference period. Experimental results on Cityscapes and
Camvid show that WeClick outperforms the state-of-the-art methods, increases
performance by 10.24% mIoU than baseline, and achieves real-time execution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Peidong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zibin He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1"&gt;Xiyu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yong Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1"&gt;Shutao Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1"&gt;Feng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1"&gt;Maowei Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Odometry with an Event Camera Using Continuous Ray Warping and Volumetric Contrast Maximization. (arXiv:2107.03011v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03011</id>
        <link href="http://arxiv.org/abs/2107.03011"/>
        <updated>2021-07-08T01:57:57.425Z</updated>
        <summary type="html"><![CDATA[We present a new solution to tracking and mapping with an event camera. The
motion of the camera contains both rotation and translation, and the
displacements happen in an arbitrarily structured environment. As a result, the
image matching may no longer be represented by a low-dimensional homographic
warping, thus complicating an application of the commonly used Image of Warped
Events (IWE). We introduce a new solution to this problem by performing
contrast maximization in 3D. The 3D location of the rays cast for each event is
smoothly varied as a function of a continuous-time motion parametrization, and
the optimal parameters are found by maximizing the contrast in a volumetric ray
density field. Our method thus performs joint optimization over motion and
structure. The practical validity of our approach is supported by an
application to AGV motion estimation and 3D reconstruction with a single
vehicle-mounted event camera. The method approaches the performance obtained
with regular cameras, and eventually outperforms in challenging visual
conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yifu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jiaqi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1"&gt;Xin Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1"&gt;Peng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Ling Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiaben Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kneip_L/0/1/0/all/0/1"&gt;Laurent Kneip&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[XPDNet for MRI Reconstruction: an application to the 2020 fastMRI challenge. (arXiv:2010.07290v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07290</id>
        <link href="http://arxiv.org/abs/2010.07290"/>
        <updated>2021-07-08T01:57:57.418Z</updated>
        <summary type="html"><![CDATA[We present a new neural network, the XPDNet, for MRI reconstruction from
periodically under-sampled multi-coil data. We inform the design of this
network by taking best practices from MRI reconstruction and computer vision.
We show that this network can achieve state-of-the-art reconstruction results,
as shown by its ranking of second in the fastMRI 2020 challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ramzi_Z/0/1/0/all/0/1"&gt;Zaccharie Ramzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ciuciu_P/0/1/0/all/0/1"&gt;Philippe Ciuciu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Starck_J/0/1/0/all/0/1"&gt;Jean-Luc Starck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Invariant Representation with Consistency and Diversity for Semi-supervised Source Hypothesis Transfer. (arXiv:2107.03008v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03008</id>
        <link href="http://arxiv.org/abs/2107.03008"/>
        <updated>2021-07-08T01:57:57.410Z</updated>
        <summary type="html"><![CDATA[Semi-supervised domain adaptation (SSDA) aims to solve tasks in target domain
by utilizing transferable information learned from the available source domain
and a few labeled target data. However, source data is not always accessible in
practical scenarios, which restricts the application of SSDA in real world
circumstances. In this paper, we propose a novel task named Semi-supervised
Source Hypothesis Transfer (SSHT), which performs domain adaptation based on
source trained model, to generalize well in target domain with a few
supervisions. In SSHT, we are facing two challenges: (1) The insufficient
labeled target data may result in target features near the decision boundary,
with the increased risk of mis-classification; (2) The data are usually
imbalanced in source domain, so the model trained with these data is biased.
The biased model is prone to categorize samples of minority categories into
majority ones, resulting in low prediction diversity. To tackle the above
issues, we propose Consistency and Diversity Learning (CDL), a simple but
effective framework for SSHT by facilitating prediction consistency between two
randomly augmented unlabeled data and maintaining the prediction diversity when
adapting model to target domain. Encouraging consistency regularization brings
difficulty to memorize the few labeled target data and thus enhances the
generalization ability of the learned model. We further integrate Batch
Nuclear-norm Maximization into our method to enhance the discriminability and
diversity. Experimental results show that our method outperforms existing SSDA
methods and unsupervised model adaptation methods on DomainNet, Office-Home and
Office-31 datasets. The code is available at
https://github.com/Wang-xd1899/SSHT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaodong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuo_J/0/1/0/all/0/1"&gt;Junbao Zhuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Shuhao Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuhui Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Learning by Integrating Spatial and Frequency Representation. (arXiv:2105.05348v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05348</id>
        <link href="http://arxiv.org/abs/2105.05348"/>
        <updated>2021-07-08T01:57:57.389Z</updated>
        <summary type="html"><![CDATA[Human beings can recognize new objects with only a few labeled examples,
however, few-shot learning remains a challenging problem for machine learning
systems. Most previous algorithms in few-shot learning only utilize spatial
information of the images. In this paper, we propose to integrate the frequency
information into the learning model to boost the discrimination ability of the
system. We employ Discrete Cosine Transformation (DCT) to generate the
frequency representation, then, integrate the features from both the spatial
domain and frequency domain for classification. The proposed strategy and its
effectiveness are validated with different backbones, datasets, and algorithms.
Extensive experiments demonstrate that the frequency information is
complementary to the spatial representations in few-shot classification. The
classification accuracy is boosted significantly by integrating features from
both the spatial and frequency domains in different few-shot learning tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiangyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guanghui Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FasterPose: A Faster Simple Baseline for Human Pose Estimation. (arXiv:2107.03215v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03215</id>
        <link href="http://arxiv.org/abs/2107.03215"/>
        <updated>2021-07-08T01:57:57.382Z</updated>
        <summary type="html"><![CDATA[The performance of human pose estimation depends on the spatial accuracy of
keypoint localization. Most existing methods pursue the spatial accuracy
through learning the high-resolution (HR) representation from input images. By
the experimental analysis, we find that the HR representation leads to a sharp
increase of computational cost, while the accuracy improvement remains marginal
compared with the low-resolution (LR) representation. In this paper, we propose
a design paradigm for cost-effective network with LR representation for
efficient pose estimation, named FasterPose. Whereas the LR design largely
shrinks the model complexity, yet how to effectively train the network with
respect to the spatial accuracy is a concomitant challenge. We study the
training behavior of FasterPose, and formulate a novel regressive cross-entropy
(RCE) loss function for accelerating the convergence and promoting the
accuracy. The RCE loss generalizes the ordinary cross-entropy loss from the
binary supervision to a continuous range, thus the training of pose estimation
network is able to benefit from the sigmoid function. By doing so, the output
heatmap can be inferred from the LR features without loss of spatial accuracy,
while the computational cost and model size has been significantly reduced.
Compared with the previously dominant network of pose estimation, our method
reduces 58% of the FLOPs and simultaneously gains 1.3% improvement of accuracy.
Extensive experiments show that FasterPose yields promising results on the
common benchmarks, i.e., COCO and MPII, consistently validating the
effectiveness and efficiency for practical utilization, especially the
low-latency and low-energy-budget applications in the non-GPU scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1"&gt;Hanbin Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1"&gt;Hailin Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Linfang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yinglu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1"&gt;Tao Mei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MuVAM: A Multi-View Attention-based Model for Medical Visual Question Answering. (arXiv:2107.03216v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03216</id>
        <link href="http://arxiv.org/abs/2107.03216"/>
        <updated>2021-07-08T01:57:57.363Z</updated>
        <summary type="html"><![CDATA[Medical Visual Question Answering (VQA) is a multi-modal challenging task
widely considered by research communities of the computer vision and natural
language processing. Since most current medical VQA models focus on visual
content, ignoring the importance of text, this paper proposes a multi-view
attention-based model(MuVAM) for medical visual question answering which
integrates the high-level semantics of medical images on the basis of text
description. Firstly, different methods are utilized to extract the features of
the image and the question for the two modalities of vision and text. Secondly,
this paper proposes a multi-view attention mechanism that include
Image-to-Question (I2Q) attention and Word-to-Text (W2T) attention. Multi-view
attention can correlate the question with image and word in order to better
analyze the question and get an accurate answer. Thirdly, a composite loss is
presented to predict the answer accurately after multi-modal feature fusion and
improve the similarity between visual and textual cross-modal features. It
consists of classification loss and image-question complementary (IQC) loss.
Finally, for data errors and missing labels in the VQA-RAD dataset, we
collaborate with medical experts to correct and complete this dataset and then
construct an enhanced dataset, VQA-RADPh. The experiments on these two datasets
show that the effectiveness of MuVAM surpasses the state-of-the-art method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1"&gt;Haiwei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shuning He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kejia Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_B/0/1/0/all/0/1"&gt;Bo Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chunling Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1"&gt;Kun Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IntraLoss: Further Margin via Gradient-Enhancing Term for Deep Face Recognition. (arXiv:2107.03352v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03352</id>
        <link href="http://arxiv.org/abs/2107.03352"/>
        <updated>2021-07-08T01:57:57.355Z</updated>
        <summary type="html"><![CDATA[Existing classification-based face recognition methods have achieved
remarkable progress, introducing large margin into hypersphere manifold to
learn discriminative facial representations. However, the feature distribution
is ignored. Poor feature distribution will wipe out the performance improvement
brought about by margin scheme. Recent studies focus on the unbalanced
inter-class distribution and form a equidistributed feature representations by
penalizing the angle between identity and its nearest neighbor. But the problem
is more than that, we also found the anisotropy of intra-class distribution. In
this paper, we propose the `gradient-enhancing term' that concentrates on the
distribution characteristics within the class. This method, named IntraLoss,
explicitly performs gradient enhancement in the anisotropic region so that the
intra-class distribution continues to shrink, resulting in isotropic and more
compact intra-class distribution and further margin between identities. The
experimental results on LFW, YTF and CFP-FP show that our outperforms
state-of-the-art methods by gradient enhancement, demonstrating the superiority
of our method. In addition, our method has intuitive geometric interpretation
and can be easily combined with existing methods to solve the previously
ignored problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1"&gt;Chengzhi Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1"&gt;Yanzhou Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1"&gt;Haiwei Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Haijun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1"&gt;Jian Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangle Your Dense Object Detector. (arXiv:2107.02963v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.02963</id>
        <link href="http://arxiv.org/abs/2107.02963"/>
        <updated>2021-07-08T01:57:57.332Z</updated>
        <summary type="html"><![CDATA[Deep learning-based dense object detectors have achieved great success in the
past few years and have been applied to numerous multimedia applications such
as video understanding. However, the current training pipeline for dense
detectors is compromised to lots of conjunctions that may not hold. In this
paper, we investigate three such important conjunctions: 1) only samples
assigned as positive in classification head are used to train the regression
head; 2) classification and regression share the same input feature and
computational fields defined by the parallel head architecture; and 3) samples
distributed in different feature pyramid layers are treated equally when
computing the loss. We first carry out a series of pilot experiments to show
disentangling such conjunctions can lead to persistent performance improvement.
Then, based on these findings, we propose Disentangled Dense Object Detector
(DDOD), in which simple and effective disentanglement mechanisms are designed
and integrated into the current state-of-the-art dense object detectors.
Extensive experiments on MS COCO benchmark show that our approach can lead to
2.0 mAP, 2.4 mAP and 2.2 mAP absolute improvements on RetinaNet, FCOS, and ATSS
baselines with negligible extra overhead. Notably, our best model reaches 55.0
mAP on the COCO test-dev set and 93.5 AP on the hard subset of WIDER FACE,
achieving new state-of-the-art performance on these two competitive benchmarks.
Code is available at https://github.com/zehuichen123/DDOD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zehui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chenhongyi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qiaofei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1"&gt;Feng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zhengjun Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Feng Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exact Learning Augmented Naive Bayes Classifier. (arXiv:2107.03018v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03018</id>
        <link href="http://arxiv.org/abs/2107.03018"/>
        <updated>2021-07-08T01:57:57.325Z</updated>
        <summary type="html"><![CDATA[Earlier studies have shown that classification accuracies of Bayesian
networks (BNs) obtained by maximizing the conditional log likelihood (CLL) of a
class variable, given the feature variables, were higher than those obtained by
maximizing the marginal likelihood (ML). However, differences between the
performances of the two scores in the earlier studies may be attributed to the
fact that they used approximate learning algorithms, not exact ones. This paper
compares the classification accuracies of BNs with approximate learning using
CLL to those with exact learning using ML. The results demonstrate that the
classification accuracies of BNs obtained by maximizing the ML are higher than
those obtained by maximizing the CLL for large data. However, the results also
demonstrate that the classification accuracies of exact learning BNs using the
ML are much worse than those of other methods when the sample size is small and
the class variable has numerous parents. To resolve the problem, we propose an
exact learning augmented naive Bayes classifier (ANB), which ensures a class
variable with no parents. The proposed method is guaranteed to asymptotically
estimate the identical class posterior to that of the exactly learned BN.
Comparison experiments demonstrated the superior performance of the proposed
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sugahara_S/0/1/0/all/0/1"&gt;Shouta Sugahara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ueno_M/0/1/0/all/0/1"&gt;Maomi Ueno&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DORA: Toward Policy Optimization for Task-oriented Dialogue System with Efficient Context. (arXiv:2107.03286v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.03286</id>
        <link href="http://arxiv.org/abs/2107.03286"/>
        <updated>2021-07-08T01:57:57.316Z</updated>
        <summary type="html"><![CDATA[Recently, reinforcement learning (RL) has been applied to task-oriented
dialogue systems by using latent actions to solve shortcomings of supervised
learning (SL). In this paper, we propose a multi-domain task-oriented dialogue
system, called Dialogue System with Optimizing a Recurrent Action Policy using
Efficient Context (DORA), that uses SL, with subsequently applied RL to
optimize dialogue systems using a recurrent dialogue policy. This dialogue
policy recurrently generates explicit system actions as a both word-level and
high-level policy. As a result, DORA is clearly optimized during both SL and RL
steps by using an explicit system action policy that considers an efficient
context instead of the entire dialogue history. The system actions are both
interpretable and controllable, whereas the latent actions are not. DORA
improved the success rate by 6.6 points on MultiWOZ 2.0 and by 10.9 points on
MultiWOZ 2.1.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1"&gt;Hyunmin Jeon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1"&gt;Gary Geunbae Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AGD-Autoencoder: Attention Gated Deep Convolutional Autoencoder for Brain Tumor Segmentation. (arXiv:2107.03323v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.03323</id>
        <link href="http://arxiv.org/abs/2107.03323"/>
        <updated>2021-07-08T01:57:57.304Z</updated>
        <summary type="html"><![CDATA[Brain tumor segmentation is a challenging problem in medical image analysis.
The endpoint is to generate the salient masks that accurately identify brain
tumor regions in an fMRI screening. In this paper, we propose a novel attention
gate (AG model) for brain tumor segmentation that utilizes both the edge
detecting unit and the attention gated network to highlight and segment the
salient regions from fMRI images. This feature enables us to eliminate the
necessity of having to explicitly point towards the damaged area(external
tissue localization) and classify(classification) as per classical computer
vision techniques. AGs can easily be integrated within the deep convolutional
neural networks(CNNs). Minimal computional overhead is required while the AGs
increase the sensitivity scores significantly. We show that the edge detector
along with an attention gated mechanism provide a sufficient enough method for
brain segmentation reaching an IOU of 0.78]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Cvetko_T/0/1/0/all/0/1"&gt;Tim Cvetko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Blind Image Super-Resolution: A Survey and Beyond. (arXiv:2107.03055v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03055</id>
        <link href="http://arxiv.org/abs/2107.03055"/>
        <updated>2021-07-08T01:57:57.286Z</updated>
        <summary type="html"><![CDATA[Blind image super-resolution (SR), aiming to super-resolve low-resolution
images with unknown degradation, has attracted increasing attention due to its
significance in promoting real-world applications. Many novel and effective
solutions have been proposed recently, especially with the powerful deep
learning techniques. Despite years of efforts, it still remains as a
challenging research problem. This paper serves as a systematic review on
recent progress in blind image SR, and proposes a taxonomy to categorize
existing methods into three different classes according to their ways of
degradation modelling and the data used for solving the SR model. This taxonomy
helps summarize and distinguish among existing methods. We hope to provide
insights into current research states, as well as to reveal novel research
directions worth exploring. In addition, we make a summary on commonly used
datasets and previous competitions related to blind image SR. Last but not
least, a comparison among different methods is provided with detailed analysis
on their merits and demerits using both synthetic and real testing images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Anran Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yihao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jinjin Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1"&gt;Chao Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable Prediction of Text Complexity: The Missing Preliminaries for Text Simplification. (arXiv:2007.15823v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.15823</id>
        <link href="http://arxiv.org/abs/2007.15823"/>
        <updated>2021-07-08T01:57:57.267Z</updated>
        <summary type="html"><![CDATA[Text simplification reduces the language complexity of professional content
for accessibility purposes. End-to-end neural network models have been widely
adopted to directly generate the simplified version of input text, usually
functioning as a blackbox. We show that text simplification can be decomposed
into a compact pipeline of tasks to ensure the transparency and explainability
of the process. The first two steps in this pipeline are often neglected: 1) to
predict whether a given piece of text needs to be simplified, and 2) if yes, to
identify complex parts of the text. The two tasks can be solved separately
using either lexical or deep learning methods, or solved jointly. Simply
applying explainable complexity prediction as a preliminary step, the
out-of-sample text simplification performance of the state-of-the-art,
black-box simplification models can be improved by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garbacea_C/0/1/0/all/0/1"&gt;Cristina Garbacea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Mengtian Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carton_S/0/1/0/all/0/1"&gt;Samuel Carton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_Q/0/1/0/all/0/1"&gt;Qiaozhu Mei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Urban Tree Species Classification Using Aerial Imagery. (arXiv:2107.03182v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03182</id>
        <link href="http://arxiv.org/abs/2107.03182"/>
        <updated>2021-07-08T01:57:57.258Z</updated>
        <summary type="html"><![CDATA[Urban trees help regulate temperature, reduce energy consumption, improve
urban air quality, reduce wind speeds, and mitigating the urban heat island
effect. Urban trees also play a key role in climate change mitigation and
global warming by capturing and storing atmospheric carbon-dioxide which is the
largest contributor to greenhouse gases. Automated tree detection and species
classification using aerial imagery can be a powerful tool for sustainable
forest and urban tree management. Hence, This study first offers a pipeline for
generating labelled dataset of urban trees using Google Map's aerial images and
then investigates how state of the art deep Convolutional Neural Network models
such as VGG and ResNet handle the classification problem of urban tree aerial
images under different parameters. Experimental results show our best model
achieves an average accuracy of 60% over 6 tree species.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Waters_E/0/1/0/all/0/1"&gt;Emily Waters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oghaz_M/0/1/0/all/0/1"&gt;Mahdi Maktabdar Oghaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saheer_L/0/1/0/all/0/1"&gt;Lakshmi Babu Saheer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interventional Video Grounding with Dual Contrastive Learning. (arXiv:2106.11013v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11013</id>
        <link href="http://arxiv.org/abs/2106.11013"/>
        <updated>2021-07-08T01:57:57.249Z</updated>
        <summary type="html"><![CDATA[Video grounding aims to localize a moment from an untrimmed video for a given
textual query. Existing approaches focus more on the alignment of visual and
language stimuli with various likelihood-based matching or regression
strategies, i.e., P(Y|X). Consequently, these models may suffer from spurious
correlations between the language and video features due to the selection bias
of the dataset. 1) To uncover the causality behind the model and data, we first
propose a novel paradigm from the perspective of the causal inference, i.e.,
interventional video grounding (IVG) that leverages backdoor adjustment to
deconfound the selection bias based on structured causal model (SCM) and
do-calculus P(Y|do(X)). Then, we present a simple yet effective method to
approximate the unobserved confounder as it cannot be directly sampled from the
dataset. 2) Meanwhile, we introduce a dual contrastive learning approach (DCL)
to better align the text and video by maximizing the mutual information (MI)
between query and video clips, and the MI between start/end frames of a target
moment and the others within a video to learn more informative visual
representations. Experiments on three standard benchmarks show the
effectiveness of our approaches. Our code is available on GitHub:
https://github.com/nanguoshun/IVG.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nan_G/0/1/0/all/0/1"&gt;Guoshun Nan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_R/0/1/0/all/0/1"&gt;Rui Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1"&gt;Yao Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leng_S/0/1/0/all/0/1"&gt;Sicong Leng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wei Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bi-level Feature Alignment for Versatile Image Translation and Manipulation. (arXiv:2107.03021v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03021</id>
        <link href="http://arxiv.org/abs/2107.03021"/>
        <updated>2021-07-08T01:57:57.233Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) have achieved great success in image
translation and manipulation. However, high-fidelity image generation with
faithful style control remains a grand challenge in computer vision. This paper
presents a versatile image translation and manipulation framework that achieves
accurate semantic and style guidance in image generation by explicitly building
a correspondence. To handle the quadratic complexity incurred by building the
dense correspondences, we introduce a bi-level feature alignment strategy that
adopts a top-$k$ operation to rank block-wise features followed by dense
attention between block features which reduces memory cost substantially. As
the top-$k$ operation involves index swapping which precludes the gradient
propagation, we propose to approximate the non-differentiable top-$k$ operation
with a regularized earth mover's problem so that its gradient can be
effectively back-propagated. In addition, we design a novel semantic position
encoding mechanism that builds up coordinate for each individual semantic
region to preserve texture structures while building correspondences. Further,
we design a novel confidence feature injection module which mitigates mismatch
problem by fusing features adaptively according to the reliability of built
correspondences. Extensive experiments show that our method achieves superior
performance qualitatively and quantitatively as compared with the
state-of-the-art. The code is available at
\href{https://github.com/fnzhan/RABIT}{https://github.com/fnzhan/RABIT}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1"&gt;Fangneng Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yingchen Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1"&gt;Rongliang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_K/0/1/0/all/0/1"&gt;Kaiwen Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1"&gt;Aoran Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shijian Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bone Surface Reconstruction and Clinical Features Estimation from Sparse Landmarks and Statistical Shape Models: A feasibility study on the femur. (arXiv:2107.03292v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.03292</id>
        <link href="http://arxiv.org/abs/2107.03292"/>
        <updated>2021-07-08T01:57:57.226Z</updated>
        <summary type="html"><![CDATA[In this study, we investigated a method allowing the determination of the
femur bone surface as well as its mechanical axis from some easy-to-identify
bony landmarks. The reconstruction of the whole femur is therefore performed
from these landmarks using a Statistical Shape Model (SSM). The aim of this
research is therefore to assess the impact of the number, the position, and the
accuracy of the landmarks for the reconstruction of the femur and the
determination of its related mechanical axis, an important clinical parameter
to consider for the lower limb analysis. Two statistical femur models were
created from our in-house dataset and a publicly available dataset. Both were
evaluated in terms of average point-to-point surface distance error and through
the mechanical axis of the femur. Furthermore, the clinical impact of using
landmarks on the skin in replacement of bony landmarks is investigated. The
predicted proximal femurs from bony landmarks were more accurate compared to
on-skin landmarks while both had less than 3.5 degrees mechanical axis angle
deviation error. The results regarding the non-invasive determination of the
mechanical axis are very encouraging and could open very interesting clinical
perspectives for the analysis of the lower limb either for orthopedics or
functional rehabilitation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Asvadi_A/0/1/0/all/0/1"&gt;Alireza Asvadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dardenne_G/0/1/0/all/0/1"&gt;Guillaume Dardenne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Troccaz_J/0/1/0/all/0/1"&gt;Jocelyne Troccaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Burdin_V/0/1/0/all/0/1"&gt;Valerie Burdin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Mesh Prior: Unsupervised Mesh Restoration using Graph Convolutional Networks. (arXiv:2107.02909v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.02909</id>
        <link href="http://arxiv.org/abs/2107.02909"/>
        <updated>2021-07-08T01:57:57.205Z</updated>
        <summary type="html"><![CDATA[This paper addresses mesh restoration problems, i.e., denoising and
completion, by learning self-similarity in an unsupervised manner. For this
purpose, the proposed method, which we refer to as Deep Mesh Prior, uses a
graph convolutional network on meshes to learn the self-similarity. The network
takes a single incomplete mesh as input data and directly outputs the
reconstructed mesh without being trained using large-scale datasets. Our method
does not use any intermediate representations such as an implicit field because
the whole process works on a mesh. We demonstrate that our unsupervised method
performs equally well or even better than the state-of-the-art methods using
large-scale datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hattori_S/0/1/0/all/0/1"&gt;Shota Hattori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yatagawa_T/0/1/0/all/0/1"&gt;Tatsuya Yatagawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ohtake_Y/0/1/0/all/0/1"&gt;Yutaka Ohtake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suzuki_H/0/1/0/all/0/1"&gt;Hiromasa Suzuki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RAM-VO: Less is more in Visual Odometry. (arXiv:2107.02974v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.02974</id>
        <link href="http://arxiv.org/abs/2107.02974"/>
        <updated>2021-07-08T01:57:57.166Z</updated>
        <summary type="html"><![CDATA[Building vehicles capable of operating without human supervision requires the
determination of the agent's pose. Visual Odometry (VO) algorithms estimate the
egomotion using only visual changes from the input images. The most recent VO
methods implement deep-learning techniques using convolutional neural networks
(CNN) extensively, which add a substantial cost when dealing with
high-resolution images. Furthermore, in VO tasks, more input data does not mean
a better prediction; on the contrary, the architecture may filter out useless
information. Therefore, the implementation of computationally efficient and
lightweight architectures is essential. In this work, we propose the RAM-VO, an
extension of the Recurrent Attention Model (RAM) for visual odometry tasks.
RAM-VO improves the visual and temporal representation of information and
implements the Proximal Policy Optimization (PPO) algorithm to learn robust
policies. The results indicate that RAM-VO can perform regressions with six
degrees of freedom from monocular input images using approximately 3 million
parameters. In addition, experiments on the KITTI dataset demonstrate that
RAM-VO achieves competitive results using only 5.7% of the available visual
information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cleveston_I/0/1/0/all/0/1"&gt;Iury Cleveston&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colombini_E/0/1/0/all/0/1"&gt;Esther L. Colombini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Stixel-based Instance Segmentation. (arXiv:2107.03070v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03070</id>
        <link href="http://arxiv.org/abs/2107.03070"/>
        <updated>2021-07-08T01:57:56.949Z</updated>
        <summary type="html"><![CDATA[Stixels have been successfully applied to a wide range of vision tasks in
autonomous driving, recently including instance segmentation. However, due to
their sparse occurrence in the image, until now Stixels seldomly served as
input for Deep Learning algorithms, restricting their utility for such
approaches. In this work we present StixelPointNet, a novel method to perform
fast instance segmentation directly on Stixels. By regarding the Stixel
representation as unstructured data similar to point clouds, architectures like
PointNet are able to learn features from Stixels. We use a bounding box
detector to propose candidate instances, for which the relevant Stixels are
extracted from the input image. On these Stixels, a PointNet models learns
binary segmentations, which we then unify throughout the whole image in a final
selection step. StixelPointNet achieves state-of-the-art performance on
Stixel-level, is considerably faster than pixel-based segmentation methods, and
shows that with our approach the Stixel domain can be introduced to many new 3D
Deep Learning tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Santarossa_M/0/1/0/all/0/1"&gt;Monty Santarossa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schneider_L/0/1/0/all/0/1"&gt;Lukas Schneider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zelenka_C/0/1/0/all/0/1"&gt;Claudius Zelenka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmarje_L/0/1/0/all/0/1"&gt;Lars Schmarje&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koch_R/0/1/0/all/0/1"&gt;Reinhard Koch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franke_U/0/1/0/all/0/1"&gt;Uwe Franke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer Network for Significant Stenosis Detection in CCTA of Coronary Arteries. (arXiv:2107.03035v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.03035</id>
        <link href="http://arxiv.org/abs/2107.03035"/>
        <updated>2021-07-08T01:57:56.931Z</updated>
        <summary type="html"><![CDATA[Coronary artery disease (CAD) has posed a leading threat to the lives of
cardiovascular disease patients worldwide for a long time. Therefore, automated
diagnosis of CAD has indispensable significance in clinical medicine. However,
the complexity of coronary artery plaques that cause CAD makes the automatic
detection of coronary artery stenosis in Coronary CT angiography (CCTA) a
difficult task. In this paper, we propose a Transformer network (TR-Net) for
the automatic detection of significant stenosis (i.e. luminal narrowing > 50%)
while practically completing the computer-assisted diagnosis of CAD. The
proposed TR-Net introduces a novel Transformer, and tightly combines
convolutional layers and Transformer encoders, allowing their advantages to be
demonstrated in the task. By analyzing semantic information sequences, TR-Net
can fully understand the relationship between image information in each
position of a multiplanar reformatted (MPR) image, and accurately detect
significant stenosis based on both local and global information. We evaluate
our TR-Net on a dataset of 76 patients from different patients annotated by
experienced radiologists. Experimental results illustrate that our TR-Net has
achieved better results in ACC (0.92), Spec (0.96), PPV (0.84), F1 (0.79) and
MCC (0.74) indicators compared with the state-of-the-art methods. The source
code is publicly available from the link (https://github.com/XinghuaMa/TR-Net).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xinghua Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Luo_G/0/1/0/all/0/1"&gt;Gongning Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kuanquan Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Simultaneous Learning of Single-particle Orientation and 3D Map Reconstruction from Cryo-electron Microscopy Data. (arXiv:2107.02958v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.02958</id>
        <link href="http://arxiv.org/abs/2107.02958"/>
        <updated>2021-07-08T01:57:56.911Z</updated>
        <summary type="html"><![CDATA[Cryogenic electron microscopy (cryo-EM) provides images from different copies
of the same biomolecule in arbitrary orientations. Here, we present an
end-to-end unsupervised approach that learns individual particle orientations
from cryo-EM data while reconstructing the average 3D map of the biomolecule,
starting from a random initialization. The approach relies on an auto-encoder
architecture where the latent space is explicitly interpreted as orientations
used by the decoder to form an image according to the linear projection model.
We evaluate our method on simulated data and show that it is able to
reconstruct 3D particle maps from noisy- and CTF-corrupted 2D projection images
of unknown particle orientations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nashed_Y/0/1/0/all/0/1"&gt;Youssef S. G. Nashed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Poitevin_F/0/1/0/all/0/1"&gt;Frederic Poitevin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gupta_H/0/1/0/all/0/1"&gt;Harshit Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Woollard_G/0/1/0/all/0/1"&gt;Geoffrey Woollard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kagan_M/0/1/0/all/0/1"&gt;Michael Kagan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yoon_C/0/1/0/all/0/1"&gt;Chuck Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ratner_D/0/1/0/all/0/1"&gt;Daniel Ratner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Plot2Spectra: an Automatic Spectra Extraction Tool. (arXiv:2107.02827v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.02827</id>
        <link href="http://arxiv.org/abs/2107.02827"/>
        <updated>2021-07-08T01:57:56.904Z</updated>
        <summary type="html"><![CDATA[Different types of spectroscopies, such as X-ray absorption near edge
structure (XANES) and Raman spectroscopy, play a very important role in
analyzing the characteristics of different materials. In scientific literature,
XANES/Raman data are usually plotted in line graphs which is a visually
appropriate way to represent the information when the end-user is a human
reader. However, such graphs are not conducive to direct programmatic analysis
due to the lack of automatic tools. In this paper, we develop a plot digitizer,
named Plot2Spectra, to extract data points from spectroscopy graph images in an
automatic fashion, which makes it possible for large scale data acquisition and
analysis. Specifically, the plot digitizer is a two-stage framework. In the
first axis alignment stage, we adopt an anchor-free detector to detect the plot
region and then refine the detected bounding boxes with an edge-based
constraint to locate the position of two axes. We also apply scene text
detector to extract and interpret all tick information below the x-axis. In the
second plot data extraction stage, we first employ semantic segmentation to
separate pixels belonging to plot lines from the background, and from there,
incorporate optical flow constraints to the plot line pixels to assign them to
the appropriate line (data instance) they encode. Extensive experiments are
conducted to validate the effectiveness of the proposed plot digitizer, which
shows that such a tool could help accelerate the discovery and machine learning
of materials properties.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1"&gt;Weixin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwenker_E/0/1/0/all/0/1"&gt;Eric Schwenker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spreadbury_T/0/1/0/all/0/1"&gt;Trevor Spreadbury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Kai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_M/0/1/0/all/0/1"&gt;Maria K.Y. Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cossairt_O/0/1/0/all/0/1"&gt;Oliver Cossairt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A convolutional neural network for teeth margin detection on 3-dimensional dental meshes. (arXiv:2107.03030v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03030</id>
        <link href="http://arxiv.org/abs/2107.03030"/>
        <updated>2021-07-08T01:57:56.895Z</updated>
        <summary type="html"><![CDATA[We proposed a convolutional neural network for vertex classification on
3-dimensional dental meshes, and used it to detect teeth margins. An expanding
layer was constructed to collect statistic values of neighbor vertex features
and compute new features for each vertex with convolutional neural networks. An
end-to-end neural network was proposed to take vertex features, including
coordinates, curvatures and distance, as input and output each vertex
classification label. Several network structures with different parameters of
expanding layers and a base line network without expanding layers were designed
and trained by 1156 dental meshes. The accuracy, recall and precision were
validated on 145 dental meshes to rate the best network structures, which were
finally tested on another 144 dental meshes. All networks with our expanding
layers performed better than baseline, and the best one achieved an accuracy of
0.877 both on validation dataset and test dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1"&gt;Bifu Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1"&gt;Kenan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yuchun Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Outdoor Scene Relighting. (arXiv:2107.03106v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03106</id>
        <link href="http://arxiv.org/abs/2107.03106"/>
        <updated>2021-07-08T01:57:56.888Z</updated>
        <summary type="html"><![CDATA[Outdoor scene relighting is a challenging problem that requires good
understanding of the scene geometry, illumination and albedo. Current
techniques are completely supervised, requiring high quality synthetic
renderings to train a solution. Such renderings are synthesized using priors
learned from limited data. In contrast, we propose a self-supervised approach
for relighting. Our approach is trained only on corpora of images collected
from the internet without any user-supervision. This virtually endless source
of training data allows training a general relighting solution. Our approach
first decomposes an image into its albedo, geometry and illumination. A novel
relighting is then produced by modifying the illumination parameters. Our
solution capture shadow using a dedicated shadow prediction map, and does not
rely on accurate geometry estimation. We evaluate our technique subjectively
and objectively using a new dataset with ground-truth relighting. Results show
the ability of our technique to produce photo-realistic and physically
plausible results, that generalizes to unseen scenes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Ye Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meka_A/0/1/0/all/0/1"&gt;Abhimitra Meka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elgharib_M/0/1/0/all/0/1"&gt;Mohamed Elgharib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seidel_H/0/1/0/all/0/1"&gt;Hans-Peter Seidel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_W/0/1/0/all/0/1"&gt;William A. P. Smith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Convolutional Correlation Iterative Particle Filter for Visual Tracking. (arXiv:2107.02984v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.02984</id>
        <link href="http://arxiv.org/abs/2107.02984"/>
        <updated>2021-07-08T01:57:56.877Z</updated>
        <summary type="html"><![CDATA[This work proposes a novel framework for visual tracking based on the
integration of an iterative particle filter, a deep convolutional neural
network, and a correlation filter. The iterative particle filter enables the
particles to correct themselves and converge to the correct target position. We
employ a novel strategy to assess the likelihood of the particles after the
iterations by applying K-means clustering. Our approach ensures a consistent
support for the posterior distribution. Thus, we do not need to perform
resampling at every video frame, improving the utilization of prior
distribution information. Experimental results on two different benchmark
datasets show that our tracker performs favorably against state-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mozhdehi_R/0/1/0/all/0/1"&gt;Reza Jalil Mozhdehi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Medeiros_H/0/1/0/all/0/1"&gt;Henry Medeiros&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trans4E: Link Prediction on Scholarly Knowledge Graphs. (arXiv:2107.03297v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.03297</id>
        <link href="http://arxiv.org/abs/2107.03297"/>
        <updated>2021-07-08T01:57:56.870Z</updated>
        <summary type="html"><![CDATA[The incompleteness of Knowledge Graphs (KGs) is a crucial issue affecting the
quality of AI-based services. In the scholarly domain, KGs describing research
publications typically lack important information, hindering our ability to
analyse and predict research dynamics. In recent years, link prediction
approaches based on Knowledge Graph Embedding models became the first aid for
this issue. In this work, we present Trans4E, a novel embedding model that is
particularly fit for KGs which include N to M relations with N$\gg$M. This is
typical for KGs that categorize a large number of entities (e.g., research
articles, patents, persons) according to a relatively small set of categories.
Trans4E was applied on two large-scale knowledge graphs, the Academia/Industry
DynAmics (AIDA) and Microsoft Academic Graph (MAG), for completing the
information about Fields of Study (e.g., 'neural networks', 'machine learning',
'artificial intelligence'), and affiliation types (e.g., 'education',
'company', 'government'), improving the scope and accuracy of the resulting
data. We evaluated our approach against alternative solutions on AIDA, MAG, and
four other benchmarks (FB15k, FB15k-237, WN18, and WN18RR). Trans4E outperforms
the other models when using low embedding dimensions and obtains competitive
results in high dimensions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nayyeri_M/0/1/0/all/0/1"&gt;Mojtaba Nayyeri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cil_G/0/1/0/all/0/1"&gt;Gokce Muge Cil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vahdati_S/0/1/0/all/0/1"&gt;Sahar Vahdati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osborne_F/0/1/0/all/0/1"&gt;Francesco Osborne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1"&gt;Mahfuzur Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Angioni_S/0/1/0/all/0/1"&gt;Simone Angioni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salatino_A/0/1/0/all/0/1"&gt;Angelo Salatino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Recupero_D/0/1/0/all/0/1"&gt;Diego Reforgiato Recupero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vassilyeva_N/0/1/0/all/0/1"&gt;Nadezhda Vassilyeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Motta_E/0/1/0/all/0/1"&gt;Enrico Motta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1"&gt;Jens Lehmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Poly-NL: Linear Complexity Non-local Layers with Polynomials. (arXiv:2107.02859v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.02859</id>
        <link href="http://arxiv.org/abs/2107.02859"/>
        <updated>2021-07-08T01:57:56.849Z</updated>
        <summary type="html"><![CDATA[Spatial self-attention layers, in the form of Non-Local blocks, introduce
long-range dependencies in Convolutional Neural Networks by computing pairwise
similarities among all possible positions. Such pairwise functions underpin the
effectiveness of non-local layers, but also determine a complexity that scales
quadratically with respect to the input size both in space and time. This is a
severely limiting factor that practically hinders the applicability of
non-local blocks to even moderately sized inputs. Previous works focused on
reducing the complexity by modifying the underlying matrix operations, however
in this work we aim to retain full expressiveness of non-local layers while
keeping complexity linear. We overcome the efficiency limitation of non-local
blocks by framing them as special cases of 3rd order polynomial functions. This
fact enables us to formulate novel fast Non-Local blocks, capable of reducing
the complexity from quadratic to linear with no loss in performance, by
replacing any direct computation of pairwise similarities with element-wise
multiplications. The proposed method, which we dub as "Poly-NL", is competitive
with state-of-the-art performance across image recognition, instance
segmentation, and face detection tasks, while having considerably less
computational overhead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Babiloni_F/0/1/0/all/0/1"&gt;Francesca Babiloni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marras_I/0/1/0/all/0/1"&gt;Ioannis Marras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kokkinos_F/0/1/0/all/0/1"&gt;Filippos Kokkinos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Jiankang Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chrysos_G/0/1/0/all/0/1"&gt;Grigorios Chrysos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1"&gt;Stefanos Zafeiriou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Greedy Offset-Guided Keypoint Grouping for Human Pose Estimation. (arXiv:2107.03098v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03098</id>
        <link href="http://arxiv.org/abs/2107.03098"/>
        <updated>2021-07-08T01:57:56.842Z</updated>
        <summary type="html"><![CDATA[We propose a simple yet reliable bottom-up approach with a good trade-off
between accuracy and efficiency for the problem of multi-person pose
estimation. Given an image, we employ an Hourglass Network to infer all the
keypoints from different persons indiscriminately as well as the guiding
offsets connecting the adjacent keypoints belonging to the same persons. Then,
we greedily group the candidate keypoints into multiple human poses (if any),
utilizing the predicted guiding offsets. And we refer to this process as greedy
offset-guided keypoint grouping (GOG). Moreover, we revisit the
encoding-decoding method for the multi-person keypoint coordinates and reveal
some important facts affecting accuracy. Experiments have demonstrated the
obvious performance improvements brought by the introduced components. Our
approach is comparable to the state of the art on the challenging COCO dataset
under fair conditions. The source code and our pre-trained model are publicly
available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1"&gt;Linhua Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiwei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zengfu Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text2App: A Framework for Creating Android Apps from Text Descriptions. (arXiv:2104.08301v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08301</id>
        <link href="http://arxiv.org/abs/2104.08301"/>
        <updated>2021-07-08T01:57:56.835Z</updated>
        <summary type="html"><![CDATA[We present Text2App -- a framework that allows users to create functional
Android applications from natural language specifications. The conventional
method of source code generation tries to generate source code directly, which
is impractical for creating complex software. We overcome this limitation by
transforming natural language into an abstract intermediate formal language
representing an application with a substantially smaller number of tokens. The
intermediate formal representation is then compiled into target source codes.
This abstraction of programming details allows seq2seq networks to learn
complex application structures with less overhead. In order to train sequence
models, we introduce a data synthesis method grounded in a human survey. We
demonstrate that Text2App generalizes well to unseen combination of app
components and it is capable of handling noisy natural language instructions.
We explore the possibility of creating applications from highly abstract
instructions by coupling our system with GPT-3 -- a large pretrained language
model. We perform an extensive human evaluation and identify the capabilities
and limitations of our system. The source code, a ready-to-run demo notebook,
and a demo video are publicly available at
\url{https://github.com/text2app/Text2App}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1"&gt;Masum Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehrab_K/0/1/0/all/0/1"&gt;Kazi Sajeed Mehrab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1"&gt;Wasi Uddin Ahmad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahriyar_R/0/1/0/all/0/1"&gt;Rifat Shahriyar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GA-NET: Global Attention Network for Point Cloud Semantic Segmentation. (arXiv:2107.03101v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03101</id>
        <link href="http://arxiv.org/abs/2107.03101"/>
        <updated>2021-07-08T01:57:56.815Z</updated>
        <summary type="html"><![CDATA[How to learn long-range dependencies from 3D point clouds is a challenging
problem in 3D point cloud analysis. Addressing this problem, we propose a
global attention network for point cloud semantic segmentation, named as
GA-Net, consisting of a point-independent global attention module and a
point-dependent global attention module for obtaining contextual information of
3D point clouds in this paper. The point-independent global attention module
simply shares a global attention map for all 3D points. In the point-dependent
global attention module, for each point, a novel random cross attention block
using only two randomly sampled subsets is exploited to learn the contextual
information of all the points. Additionally, we design a novel point-adaptive
aggregation block to replace linear skip connection for aggregating more
discriminate features. Extensive experimental results on three 3D public
datasets demonstrate that our method outperforms state-of-the-art methods in
most cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1"&gt;Shuang Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1"&gt;Qiulei Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-modal Affect Analysis using standardized data within subjects in the Wild. (arXiv:2107.03009v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03009</id>
        <link href="http://arxiv.org/abs/2107.03009"/>
        <updated>2021-07-08T01:57:56.808Z</updated>
        <summary type="html"><![CDATA[Human affective recognition is an important factor in human-computer
interaction. However, the method development with in-the-wild data is not yet
accurate enough for practical usage. In this paper, we introduce the affective
recognition method focusing on facial expression (EXP) and valence-arousal
calculation that was submitted to the Affective Behavior Analysis in-the-wild
(ABAW) 2021 Contest.

When annotating facial expressions from a video, we thought that it would be
judged not only from the features common to all people, but also from the
relative changes in the time series of individuals. Therefore, after learning
the common features for each frame, we constructed a facial expression
estimation model and valence-arousal model using time-series data after
combining the common features and the standardized features for each video.
Furthermore, the above features were learned using multi-modal data such as
image features, AU, Head pose, and Gaze. In the validation set, our model
achieved a facial expression score of 0.546. These verification results reveal
that our proposed framework can improve estimation accuracy and robustness
effectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Youoku_S/0/1/0/all/0/1"&gt;Sachihiro Youoku&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamamoto_T/0/1/0/all/0/1"&gt;Takahisa Yamamoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saito_J/0/1/0/all/0/1"&gt;Junya Saito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uchida_A/0/1/0/all/0/1"&gt;Akiyoshi Uchida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mi_X/0/1/0/all/0/1"&gt;Xiaoyu Mi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1"&gt;Ziqiang Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Liu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhongling Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Over a Decade of Social Opinion Mining: A Systematic Review. (arXiv:2012.03091v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03091</id>
        <link href="http://arxiv.org/abs/2012.03091"/>
        <updated>2021-07-08T01:57:56.799Z</updated>
        <summary type="html"><![CDATA[Social media popularity and importance is on the increase due to people using
it for various types of social interaction across multiple channels. This
systematic review focuses on the evolving research area of Social Opinion
Mining, tasked with the identification of multiple opinion dimensions, such as
subjectivity, sentiment polarity, emotion, affect, sarcasm and irony, from
user-generated content represented across multiple social media platforms and
in various media formats, like text, image, video and audio. Through Social
Opinion Mining, natural language can be understood in terms of the different
opinion dimensions, as expressed by humans. This contributes towards the
evolution of Artificial Intelligence which in turn helps the advancement of
several real-world use cases, such as customer service and decision making. A
thorough systematic review was carried out on Social Opinion Mining research
which totals 485 published studies and spans a period of twelve years between
2007 and 2018. The in-depth analysis focuses on the social media platforms,
techniques, social datasets, language, modality, tools and technologies, and
other aspects derived. Social Opinion Mining can be utilised in many
application areas, ranging from marketing, advertising and sales for
product/service management, and in multiple domains and industries, such as
politics, technology, finance, healthcare, sports and government. The latest
developments in Social Opinion Mining beyond 2018 are also presented together
with future research directions, with the aim of leaving a wider academic and
societal impact in several real-world applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cortis_K/0/1/0/all/0/1"&gt;Keith Cortis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_B/0/1/0/all/0/1"&gt;Brian Davis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured Denoising Diffusion Models in Discrete State-Spaces. (arXiv:2107.03006v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03006</id>
        <link href="http://arxiv.org/abs/2107.03006"/>
        <updated>2021-07-08T01:57:56.791Z</updated>
        <summary type="html"><![CDATA[Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown
impressive results on image and waveform generation in continuous state spaces.
Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs),
diffusion-like generative models for discrete data that generalize the
multinomial diffusion model of Hoogeboom et al. 2021, by going beyond
corruption processes with uniform transition probabilities. This includes
corruption with transition matrices that mimic Gaussian kernels in continuous
space, matrices based on nearest neighbors in embedding space, and matrices
that introduce absorbing states. The third allows us to draw a connection
between diffusion models and autoregressive and mask-based generative models.
We show that the choice of transition matrix is an important design decision
that leads to improved results in image and text domains. We also introduce a
new loss function that combines the variational lower bound with an auxiliary
cross entropy loss. For text, this model class achieves strong results on
character-level text generation while scaling to large vocabularies on LM1B. On
the image dataset CIFAR-10, our models approach the sample quality and exceed
the log-likelihood of the continuous-space DDPM model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1"&gt;Jacob Austin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johnson_D/0/1/0/all/0/1"&gt;Daniel Johnson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1"&gt;Jonathan Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tarlow_D/0/1/0/all/0/1"&gt;Danny Tarlow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_R/0/1/0/all/0/1"&gt;Rianne van den Berg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maintaining a Reliable World Model using Action-aware Perceptual Anchoring. (arXiv:2107.03038v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.03038</id>
        <link href="http://arxiv.org/abs/2107.03038"/>
        <updated>2021-07-08T01:57:56.784Z</updated>
        <summary type="html"><![CDATA[Reliable perception is essential for robots that interact with the world. But
sensors alone are often insufficient to provide this capability, and they are
prone to errors due to various conditions in the environment. Furthermore,
there is a need for robots to maintain a model of its surroundings even when
objects go out of view and are no longer visible. This requires anchoring
perceptual information onto symbols that represent the objects in the
environment. In this paper, we present a model for action-aware perceptual
anchoring that enables robots to track objects in a persistent manner. Our
rule-based approach considers inductive biases to perform high-level reasoning
over the results from low-level object detection, and it improves the robot's
perceptual capability for complex tasks. We evaluate our model against existing
baseline models for object permanence and show that it outperforms these on a
snitch localisation task using a dataset of 1,371 videos. We also integrate our
action-aware perceptual anchoring in the context of a cognitive architecture
and demonstrate its benefits in a realistic gearbox assembly task on a
Universal Robot.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Ying Siu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1"&gt;Dongkyu Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwok_K/0/1/0/all/0/1"&gt;Kenneth Kwok&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video-Based Camera Localization Using Anchor View Detection and Recursive 3D Reconstruction. (arXiv:2107.03068v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03068</id>
        <link href="http://arxiv.org/abs/2107.03068"/>
        <updated>2021-07-08T01:57:56.777Z</updated>
        <summary type="html"><![CDATA[In this paper we introduce a new camera localization strategy designed for
image sequences captured in challenging industrial situations such as
industrial parts inspection. To deal with peculiar appearances that hurt
standard 3D reconstruction pipeline, we exploit pre-knowledge of the scene by
selecting key frames in the sequence (called as anchors) which are roughly
connected to a certain location. Our method then seek the location of each
frame in time-order, while recursively updating an augmented 3D model which can
provide current camera location and surrounding 3D structure. In an experiment
on a practical industrial situation, our method can localize over 99% frames in
the input sequence, whereas standard localization methods fail to reconstruct a
complete camera trajectory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Taira_H/0/1/0/all/0/1"&gt;Hajime Taira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Onbe_K/0/1/0/all/0/1"&gt;Koki Onbe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miyashita_N/0/1/0/all/0/1"&gt;Naoyuki Miyashita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Okutomi_M/0/1/0/all/0/1"&gt;Masatoshi Okutomi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Vision Transformer with Squeeze and Excitation for Facial Expression Recognition. (arXiv:2107.03107v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03107</id>
        <link href="http://arxiv.org/abs/2107.03107"/>
        <updated>2021-07-08T01:57:56.770Z</updated>
        <summary type="html"><![CDATA[As various databases of facial expressions have been made accessible over the
last few decades, the Facial Expression Recognition (FER) task has gotten a lot
of interest. The multiple sources of the available databases raised several
challenges for facial recognition task. These challenges are usually addressed
by Convolution Neural Network (CNN) architectures. Different from CNN models, a
Transformer model based on attention mechanism has been presented recently to
address vision tasks. One of the major issue with Transformers is the need of a
large data for training, while most FER databases are limited compared to other
vision applications. Therefore, we propose in this paper to learn a vision
Transformer jointly with a Squeeze and Excitation (SE) block for FER task. The
proposed method is evaluated on different publicly available FER databases
including CK+, JAFFE,RAF-DB and SFEW. Experiments demonstrate that our model
outperforms state-of-the-art methods on CK+ and SFEW and achieves competitive
results on JAFFE and RAF-DB.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aouayeb_M/0/1/0/all/0/1"&gt;Mouath Aouayeb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1"&gt;Wassim Hamidouche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soladie_C/0/1/0/all/0/1"&gt;Catherine Soladie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kpalma_K/0/1/0/all/0/1"&gt;Kidiyo Kpalma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seguier_R/0/1/0/all/0/1"&gt;Renaud Seguier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VIN: Voxel-based Implicit Network for Joint 3D Object Detection and Segmentation for Lidars. (arXiv:2107.02980v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.02980</id>
        <link href="http://arxiv.org/abs/2107.02980"/>
        <updated>2021-07-08T01:57:56.760Z</updated>
        <summary type="html"><![CDATA[A unified neural network structure is presented for joint 3D object detection
and point cloud segmentation in this paper. We leverage rich supervision from
both detection and segmentation labels rather than using just one of them. In
addition, an extension based on single-stage object detectors is proposed based
on the implicit function widely used in 3D scene and object understanding. The
extension branch takes the final feature map from the object detection module
as input, and produces an implicit function that generates semantic
distribution for each point for its corresponding voxel center. We demonstrated
the performance of our structure on nuScenes-lidarseg, a large-scale outdoor
dataset. Our solution achieves competitive results against state-of-the-art
methods in both 3D object detection and point cloud segmentation with little
additional computation load compared with object detection solutions. The
capability of efficient weakly supervision semantic segmentation of the
proposed method is also validated by experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1"&gt;Yuanxin Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Minghan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Huei Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[U2++: Unified Two-pass Bidirectional End-to-end Model for Speech Recognition. (arXiv:2106.05642v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05642</id>
        <link href="http://arxiv.org/abs/2106.05642"/>
        <updated>2021-07-08T01:57:56.753Z</updated>
        <summary type="html"><![CDATA[The unified streaming and non-streaming two-pass (U2) end-to-end model for
speech recognition has shown great performance in terms of streaming
capability, accuracy, real-time factor (RTF), and latency. In this paper, we
present U2++, an enhanced version of U2 to further improve the accuracy. The
core idea of U2++ is to use the forward and the backward information of the
labeling sequences at the same time at training to learn richer information,
and combine the forward and backward prediction at decoding to give more
accurate recognition results. We also proposed a new data augmentation method
called SpecSub to help the U2++ model to be more accurate and robust. Our
experiments show that, compared with U2, U2++ shows faster convergence at
training, better robustness to the decoding method, as well as consistent 5\% -
8\% word error rate reduction gain over U2. On the experiment of AISHELL-1, we
achieve a 4.63\% character error rate (CER) with a non-streaming setup and
5.05\% with a streaming setup with 320ms latency by U2++. To the best of our
knowledge, 5.05\% is the best-published streaming result on the AISHELL-1 test
set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Di Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Binbin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1"&gt;Zhendong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1"&gt;Wenjing Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaoyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1"&gt;Xin Lei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Representation Models for Text Classification with AutoML Tools. (arXiv:2106.12798v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12798</id>
        <link href="http://arxiv.org/abs/2106.12798"/>
        <updated>2021-07-08T01:57:56.723Z</updated>
        <summary type="html"><![CDATA[Automated Machine Learning (AutoML) has gained increasing success on tabular
data in recent years. However, processing unstructured data like text is a
challenge and not widely supported by open-source AutoML tools. This work
compares three manually created text representations and text embeddings
automatically created by AutoML tools. Our benchmark includes four popular
open-source AutoML tools and eight datasets for text classification purposes.
The results show that straightforward text representations perform better than
AutoML tools with automatically created text embeddings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brandle_S/0/1/0/all/0/1"&gt;Sebastian Br&amp;#xe4;ndle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanussek_M/0/1/0/all/0/1"&gt;Marc Hanussek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blohm_M/0/1/0/all/0/1"&gt;Matthias Blohm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kintz_M/0/1/0/all/0/1"&gt;Maximilien Kintz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Assignment of Radiology Examination Protocols Using Pre-trained Language Models with Knowledge Distillation. (arXiv:2009.00694v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.00694</id>
        <link href="http://arxiv.org/abs/2009.00694"/>
        <updated>2021-07-08T01:57:56.707Z</updated>
        <summary type="html"><![CDATA[Selecting radiology examination protocol is a repetitive, and time-consuming
process. In this paper, we present a deep learning approach to automatically
assign protocols to computer tomography examinations, by pre-training a
domain-specific BERT model ($BERT_{rad}$). To handle the high data imbalance
across exam protocols, we used a knowledge distillation approach that
up-sampled the minority classes through data augmentation. We compared
classification performance of the described approach with the statistical
n-gram models using Support Vector Machine (SVM), Gradient Boosting Machine
(GBM), and Random Forest (RF) classifiers, as well as the Google's
$BERT_{base}$ model. SVM, GBM and RF achieved macro-averaged F1 scores of 0.45,
0.45, and 0.6 while $BERT_{base}$ and $BERT_{rad}$ achieved 0.61 and 0.63.
Knowledge distillation improved overall performance on the minority classes,
achieving a F1 score of 0.66.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lau_W/0/1/0/all/0/1"&gt;Wilson Lau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aaltonen_L/0/1/0/all/0/1"&gt;Laura Aaltonen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunn_M/0/1/0/all/0/1"&gt;Martin Gunn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yetisgen_M/0/1/0/all/0/1"&gt;Meliha Yetisgen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpectralFormer: Rethinking Hyperspectral Image Classification with Transformers. (arXiv:2107.02988v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.02988</id>
        <link href="http://arxiv.org/abs/2107.02988"/>
        <updated>2021-07-08T01:57:56.687Z</updated>
        <summary type="html"><![CDATA[Hyperspectral (HS) images are characterized by approximately contiguous
spectral information, enabling the fine identification of materials by
capturing subtle spectral discrepancies. Owing to their excellent locally
contextual modeling ability, convolutional neural networks (CNNs) have been
proven to be a powerful feature extractor in HS image classification. However,
CNNs fail to mine and represent the sequence attributes of spectral signatures
well due to the limitations of their inherent network backbone. To solve this
issue, we rethink HS image classification from a sequential perspective with
transformers, and propose a novel backbone network called \ul{SpectralFormer}.
Beyond band-wise representations in classic transformers, SpectralFormer is
capable of learning spectrally local sequence information from neighboring
bands of HS images, yielding group-wise spectral embeddings. More
significantly, to reduce the possibility of losing valuable information in the
layer-wise propagation process, we devise a cross-layer skip connection to
convey memory-like components from shallow to deep layers by adaptively
learning to fuse "soft" residuals across layers. It is worth noting that the
proposed SpectralFormer is a highly flexible backbone network, which can be
applicable to both pixel- and patch-wise inputs. We evaluate the classification
performance of the proposed SpectralFormer on three HS datasets by conducting
extensive experiments, showing the superiority over classic transformers and
achieving a significant improvement in comparison with state-of-the-art
backbone networks. The codes of this work will be available at
\url{https://sites.google.com/view/danfeng-hong} for the sake of
reproducibility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1"&gt;Danfeng Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1"&gt;Zhu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1"&gt;Jing Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Lianru Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plaza_A/0/1/0/all/0/1"&gt;Antonio Plaza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1"&gt;Jocelyn Chanussot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustifying Multi-hop QA through Pseudo-Evidentiality Training. (arXiv:2107.03242v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.03242</id>
        <link href="http://arxiv.org/abs/2107.03242"/>
        <updated>2021-07-08T01:57:56.668Z</updated>
        <summary type="html"><![CDATA[This paper studies the bias problem of multi-hop question answering models,
of answering correctly without correct reasoning. One way to robustify these
models is by supervising to not only answer right, but also with right
reasoning chains. An existing direction is to annotate reasoning chains to
train models, requiring expensive additional annotations. In contrast, we
propose a new approach to learn evidentiality, deciding whether the answer
prediction is supported by correct evidences, without such annotations.
Instead, we compare counterfactual changes in answer confidence with and
without evidence sentences, to generate "pseudo-evidentiality" annotations. We
validate our proposed model on an original set and challenge set in HotpotQA,
showing that our method is accurate and robust in multi-hop reasoning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kyungjae Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Seung-won Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Sang-eun Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dohyeon Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learn to Learn Metric Space for Few-Shot Segmentation of 3D Shapes. (arXiv:2107.02972v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.02972</id>
        <link href="http://arxiv.org/abs/2107.02972"/>
        <updated>2021-07-08T01:57:56.661Z</updated>
        <summary type="html"><![CDATA[Recent research has seen numerous supervised learning-based methods for 3D
shape segmentation and remarkable performance has been achieved on various
benchmark datasets. These supervised methods require a large amount of
annotated data to train deep neural networks to ensure the generalization
ability on the unseen test set. In this paper, we introduce a
meta-learning-based method for few-shot 3D shape segmentation where only a few
labeled samples are provided for the unseen classes. To achieve this, we treat
the shape segmentation as a point labeling problem in the metric space.
Specifically, we first design a meta-metric learner to transform input shapes
into embedding space and our model learns to learn a proper metric space for
each object class based on point embeddings. Then, for each class, we design a
metric learner to extract part-specific prototype representations from a few
support shapes and our model performs per-point segmentation over the query
shapes by matching each point to its nearest prototype in the learned metric
space. A metric-based loss function is used to dynamically modify distances
between point embeddings thus maximizes in-part similarity while minimizing
inter-part similarity. A dual segmentation branch is adopted to make full use
of the support information and implicitly encourages consistency between the
support and query prototypes. We demonstrate the superior performance of our
proposed on the ShapeNet part dataset under the few-shot scenario, compared
with well-established baseline and state-of-the-art semi-supervised methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lingjing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yi Fang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Plot and Rework: Modeling Storylines for Visual Storytelling. (arXiv:2105.06950v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06950</id>
        <link href="http://arxiv.org/abs/2105.06950"/>
        <updated>2021-07-08T01:57:56.654Z</updated>
        <summary type="html"><![CDATA[Writing a coherent and engaging story is not easy. Creative writers use their
knowledge and worldview to put disjointed elements together to form a coherent
storyline, and work and rework iteratively toward perfection. Automated visual
storytelling (VIST) models, however, make poor use of external knowledge and
iterative generation when attempting to create stories. This paper introduces
PR-VIST, a framework that represents the input image sequence as a story graph
in which it finds the best path to form a storyline. PR-VIST then takes this
path and learns to generate the final story via an iterative training process.
This framework produces stories that are superior in terms of diversity,
coherence, and humanness, per both automatic and human evaluations. An ablation
study shows that both plotting and reworking contribute to the model's
superiority.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1"&gt;Chi-Yang Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_Y/0/1/0/all/0/1"&gt;Yun-Wei Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1"&gt;Ting-Hao &amp;#x27;Kenneth&amp;#x27; Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ku_L/0/1/0/all/0/1"&gt;Lun-Wei Ku&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Edge-aware Bidirectional Diffusion for Dense Depth Estimation from Light Fields. (arXiv:2107.02967v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.02967</id>
        <link href="http://arxiv.org/abs/2107.02967"/>
        <updated>2021-07-08T01:57:56.646Z</updated>
        <summary type="html"><![CDATA[We present an algorithm to estimate fast and accurate depth maps from light
fields via a sparse set of depth edges and gradients. Our proposed approach is
based around the idea that true depth edges are more sensitive than texture
edges to local constraints, and so they can be reliably disambiguated through a
bidirectional diffusion process. First, we use epipolar-plane images to
estimate sub-pixel disparity at a sparse set of pixels. To find sparse points
efficiently, we propose an entropy-based refinement approach to a line estimate
from a limited set of oriented filter banks. Next, to estimate the diffusion
direction away from sparse points, we optimize constraints at these points via
our bidirectional diffusion method. This resolves the ambiguity of which
surface the edge belongs to and reliably separates depth from texture edges,
allowing us to diffuse the sparse set in a depth-edge and occlusion-aware
manner to obtain accurate dense depth maps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1"&gt;Numair Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Min H. Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tompkin_J/0/1/0/all/0/1"&gt;James Tompkin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fibrational Initial Algebra-Final Coalgebra Coincidence over Initial Algebras: Turning Verification Witnesses Upside Down. (arXiv:2105.04817v2 [cs.LO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04817</id>
        <link href="http://arxiv.org/abs/2105.04817"/>
        <updated>2021-07-08T01:57:56.639Z</updated>
        <summary type="html"><![CDATA[The coincidence between initial algebras (IAs) and final coalgebras (FCs) is
a phenomenon that underpins various important results in theoretical computer
science. In this paper, we identify a general fibrational condition for the
IA-FC coincidence, namely in the fiber over an initial algebra in the base
category. Identifying (co)algebras in a fiber as (co)inductive predicates, our
fibrational IA-FC coincidence allows one to use coinductive witnesses (such as
invariants) for verifying inductive properties (such as liveness). Our general
fibrational theory features the technical condition of stability of chain
colimits; we extend the framework to the presence of a monadic effect, too,
restricting to fibrations of complete lattice-valued predicates. Practical
benefits of our categorical theory are exemplified by new "upside-down" witness
notions for three verification problems: probabilistic liveness, and acceptance
and model-checking with respect to bottom-up tree automata.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kori_M/0/1/0/all/0/1"&gt;Mayuko Kori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasuo_I/0/1/0/all/0/1"&gt;Ichiro Hasuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katsumata_S/0/1/0/all/0/1"&gt;Shin-ya Katsumata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GAN-based Data Augmentation for Chest X-ray Classification. (arXiv:2107.02970v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.02970</id>
        <link href="http://arxiv.org/abs/2107.02970"/>
        <updated>2021-07-08T01:57:56.619Z</updated>
        <summary type="html"><![CDATA[A common problem in computer vision -- particularly in medical applications
-- is a lack of sufficiently diverse, large sets of training data. These
datasets often suffer from severe class imbalance. As a result, networks often
overfit and are unable to generalize to novel examples. Generative Adversarial
Networks (GANs) offer a novel method of synthetic data augmentation. In this
work, we evaluate the use of GAN- based data augmentation to artificially
expand the CheXpert dataset of chest radiographs. We compare performance to
traditional augmentation and find that GAN-based augmentation leads to higher
downstream performance for underrepresented classes. Furthermore, we see that
this result is pronounced in low data regimens. This suggests that GAN-based
augmentation a promising area of research to improve network performance when
data collection is prohibitively expensive.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sundaram_S/0/1/0/all/0/1"&gt;Shobhita Sundaram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hulkund_N/0/1/0/all/0/1"&gt;Neha Hulkund&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PoseRN: A 2D pose refinement network for bias-free multi-view 3D human pose estimation. (arXiv:2107.03000v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03000</id>
        <link href="http://arxiv.org/abs/2107.03000"/>
        <updated>2021-07-08T01:57:56.611Z</updated>
        <summary type="html"><![CDATA[We propose a new 2D pose refinement network that learns to predict the human
bias in the estimated 2D pose. There are biases in 2D pose estimations that are
due to differences between annotations of 2D joint locations based on
annotators' perception and those defined by motion capture (MoCap) systems.
These biases are crafted into publicly available 2D pose datasets and cannot be
removed with existing error reduction approaches. Our proposed pose refinement
network allows us to efficiently remove the human bias in the estimated 2D
poses and achieve highly accurate multi-view 3D human pose estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sayo_A/0/1/0/all/0/1"&gt;Akihiko Sayo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thomas_D/0/1/0/all/0/1"&gt;Diego Thomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawasaki_H/0/1/0/all/0/1"&gt;Hiroshi Kawasaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakashima_Y/0/1/0/all/0/1"&gt;Yuta Nakashima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ikeuchi_K/0/1/0/all/0/1"&gt;Katsushi Ikeuchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GLiT: Neural Architecture Search for Global and Local Image Transformer. (arXiv:2107.02960v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.02960</id>
        <link href="http://arxiv.org/abs/2107.02960"/>
        <updated>2021-07-08T01:57:56.603Z</updated>
        <summary type="html"><![CDATA[We introduce the first Neural Architecture Search (NAS) method to find a
better transformer architecture for image recognition. Recently, transformers
without CNN-based backbones are found to achieve impressive performance for
image recognition. However, the transformer is designed for NLP tasks and thus
could be sub-optimal when directly used for image recognition. In order to
improve the visual representation ability for transformers, we propose a new
search space and searching algorithm. Specifically, we introduce a locality
module that models the local correlations in images explicitly with fewer
computational cost. With the locality module, our search space is defined to
let the search algorithm freely trade off between global and local information
as well as optimizing the low-level design choice in each module. To tackle the
problem caused by huge search space, a hierarchical neural architecture search
method is proposed to search the optimal vision transformer from two levels
separately with the evolutionary algorithm. Extensive experiments on the
ImageNet dataset demonstrate that our method can find more discriminative and
efficient transformer variants than the ResNet family (e.g., ResNet101) and the
baseline ViT for image classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Boyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peixia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chuming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Baopu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1"&gt;Lei Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Ming Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+yan_J/0/1/0/all/0/1"&gt;Junjie yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1"&gt;Wanli Ouyang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Group Sampling for Unsupervised Person Re-identification. (arXiv:2107.03024v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03024</id>
        <link href="http://arxiv.org/abs/2107.03024"/>
        <updated>2021-07-08T01:57:56.595Z</updated>
        <summary type="html"><![CDATA[Unsupervised person re-identification (re-ID) remains a challenging task,
where the classifier and feature representation could be easily misled by the
noisy pseudo labels towards deteriorated over-fitting. In this paper, we
propose a simple yet effective approach, termed Group Sampling, to alleviate
the negative impact of noisy pseudo labels within unsupervised person re-ID
models. The idea behind Group Sampling is that it can gather a group of samples
from the same class in the same mini-batch, such that the model is trained upon
group normalized samples while alleviating the effect of a single sample. Group
sampling updates the pipeline of pseudo label generation by guaranteeing the
samples to be better divided into the correct classes. Group Sampling
regularizes classifier training and representation learning, leading to the
statistical stability of feature representation in a progressive fashion.
Qualitative and quantitative experiments on Market-1501, DukeMTMC-reID, and
MSMT17 show that Grouping Sampling improves the state-of-the-arts by up to
2.2%~6.1%. Code is available at https://github.com/wavinflaghxm/GroupSampling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xumeng Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xuehui Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1"&gt;Nan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guorong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jian Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qixiang Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1"&gt;Zhenjun Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[E-PixelHop: An Enhanced PixelHop Method for Object Classification. (arXiv:2107.02966v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.02966</id>
        <link href="http://arxiv.org/abs/2107.02966"/>
        <updated>2021-07-08T01:57:56.586Z</updated>
        <summary type="html"><![CDATA[Based on PixelHop and PixelHop++, which are recently developed using the
successive subspace learning (SSL) framework, we propose an enhanced solution
for object classification, called E-PixelHop, in this work. E-PixelHop consists
of the following steps. First, to decouple the color channels for a color
image, we apply principle component analysis and project RGB three color
channels onto two principle subspaces which are processed separately for
classification. Second, to address the importance of multi-scale features, we
conduct pixel-level classification at each hop with various receptive fields.
Third, to further improve pixel-level classification accuracy, we develop a
supervised label smoothing (SLS) scheme to ensure prediction consistency.
Forth, pixel-level decisions from each hop and from each color subspace are
fused together for image-level decision. Fifth, to resolve confusing classes
for further performance boosting, we formulate E-PixelHop as a two-stage
pipeline. In the first stage, multi-class classification is performed to get a
soft decision for each class, where the top 2 classes with the highest
probabilities are called confusing classes. Then,we conduct a binary
classification in the second stage. The main contributions lie in Steps 1, 3
and 5.We use the classification of the CIFAR-10 dataset as an example to
demonstrate the effectiveness of the above-mentioned key components of
E-PixelHop.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yijing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Magoulianitis_V/0/1/0/all/0/1"&gt;Vasileios Magoulianitis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1"&gt;C.-C. Jay Kuo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linear-time calculation of the expected sum of edge lengths in random projective linearizations of trees. (arXiv:2107.03277v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.03277</id>
        <link href="http://arxiv.org/abs/2107.03277"/>
        <updated>2021-07-08T01:57:56.565Z</updated>
        <summary type="html"><![CDATA[The syntactic structure of a sentence is often represented using syntactic
dependency trees. The sum of the distances between syntactically related words
has been in the limelight for the past decades. Research on dependency
distances led to the formulation of the principle of dependency distance
minimization whereby words in sentences are ordered so as to minimize that sum.
Numerous random baselines have been defined to carry out related quantitative
studies on languages. The simplest random baseline is the expected value of the
sum in unconstrained random permutations of the words in the sentence, namely
when all the shufflings of the words of a sentence are allowed and equally
likely. Here we focus on a popular baseline: random projective permutations of
the words of the sentence, that is, permutations where the syntactic dependency
structure is projective, a formal constraint that sentences satisfy often in
languages. Thus far, the expectation of the sum of dependency distances in
random projective shufflings of a sentence has been estimated approximately
with a Monte Carlo procedure whose cost is of the order of $Zn$, where $n$ is
the number of words of the sentence and $Z$ is the number of samples; the
larger $Z$, the lower the error of the estimation but the larger the time cost.
Here we present formulae to compute that expectation without error in time of
the order of $n$. Furthermore, we show that star trees maximize it, and devise
a dynamic programming algorithm to retrieve the trees that minimize it.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alemany_Puig_L/0/1/0/all/0/1"&gt;Llu&amp;#xed;s Alemany-Puig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1"&gt;Ramon Ferrer-i-Cancho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vocabulary Learning via Optimal Transport for Machine Translation. (arXiv:2012.15671v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15671</id>
        <link href="http://arxiv.org/abs/2012.15671"/>
        <updated>2021-07-08T01:57:56.556Z</updated>
        <summary type="html"><![CDATA[The choice of token vocabulary affects the performance of machine
translation. This paper aims to figure out what is a good vocabulary and
whether one can find the optimal vocabulary without trial training. To answer
these questions, we first provide an alternative understanding of the role of
vocabulary from the perspective of information theory. Motivated by this, we
formulate the quest of vocabularization -- finding the best token dictionary
with a proper size -- as an optimal transport (OT) problem. We propose VOLT, a
simple and efficient solution without trial training. Empirical results show
that VOLT outperforms widely-used vocabularies in diverse scenarios, including
WMT-14 English-German and TED's 52 translation directions. For example, VOLT
achieves almost 70% vocabulary size reduction and 0.5 BLEU gain on
English-German translation. Also, compared to BPE-search, VOLT reduces the
search time from 384 GPU hours to 30 GPU hours on English-German translation.
Codes are available at https://github.com/Jingjing-NLP/VOLT .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jingjing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1"&gt;Chun Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zaixiang Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Training Instance Selection for Few-Shot Neural Text Generation. (arXiv:2107.03176v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.03176</id>
        <link href="http://arxiv.org/abs/2107.03176"/>
        <updated>2021-07-08T01:57:56.484Z</updated>
        <summary type="html"><![CDATA[Large-scale pretrained language models have led to dramatic improvements in
text generation. Impressive performance can be achieved by finetuning only on a
small number of instances (few-shot setting). Nonetheless, almost all previous
work simply applies random sampling to select the few-shot training instances.
Little to no attention has been paid to the selection strategies and how they
would affect model performance. In this work, we present a study on training
instance selection in few-shot neural text generation. The selection decision
is made based only on the unlabeled data so as to identify the most worthwhile
data points that should be annotated under some budget of labeling cost. Based
on the intuition that the few-shot training instances should be diverse and
representative of the entire data distribution, we propose a simple selection
strategy with K-means clustering. We show that even with the naive
clustering-based approach, the generation models consistently outperform random
sampling on three text generation tasks: data-to-text generation, document
summarization and question generation. We hope that this work will call for
more attention on this largely unexplored area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1"&gt;Ernie Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1"&gt;Xiaoyu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeh_H/0/1/0/all/0/1"&gt;Hui-Syuan Yeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demberg_V/0/1/0/all/0/1"&gt;Vera Demberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lemmatization of Historical Old Literary Finnish Texts in Modern Orthography. (arXiv:2107.03266v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.03266</id>
        <link href="http://arxiv.org/abs/2107.03266"/>
        <updated>2021-07-08T01:57:56.456Z</updated>
        <summary type="html"><![CDATA[Texts written in Old Literary Finnish represent the first literary work ever
written in Finnish starting from the 16th century. There have been several
projects in Finland that have digitized old publications and made them
available for research use. However, using modern NLP methods in such data
poses great challenges. In this paper we propose an approach for simultaneously
normalizing and lemmatizing Old Literary Finnish into modern spelling. Our best
model reaches to 96.3\% accuracy in texts written by Agricola and 87.7\%
accuracy in other contemporary out-of-domain text. Our method has been made
freely available on Zenodo and Github.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hamalainen_M/0/1/0/all/0/1"&gt;Mika H&amp;#xe4;m&amp;#xe4;l&amp;#xe4;inen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Partanen_N/0/1/0/all/0/1"&gt;Niko Partanen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alnajjar_K/0/1/0/all/0/1"&gt;Khalid Alnajjar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MACCIF-TDNN: Multi aspect aggregation of channel and context interdependence features in TDNN-based speaker verification. (arXiv:2107.03104v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.03104</id>
        <link href="http://arxiv.org/abs/2107.03104"/>
        <updated>2021-07-08T01:57:56.177Z</updated>
        <summary type="html"><![CDATA[Most of the recent state-of-the-art results for speaker verification are
achieved by X-vector and its subsequent variants. In this paper, we propose a
new network architecture which aggregates the channel and context
interdependence features from multi aspect based on Time Delay Neural Network
(TDNN). Firstly, we use the SE-Res2Blocks as in ECAPA-TDNN to explicitly model
the channel interdependence to realize adaptive calibration of channel
features, and process local context features in a multi-scale way at a more
granular level compared with conventional TDNN-based methods. Secondly, we
explore to use the encoder structure of Transformer to model the global context
interdependence features at an utterance level which can capture better long
term temporal characteristics. Before the pooling layer, we aggregate the
outputs of SE-Res2Blocks and Transformer encoder to leverage the complementary
channel and context interdependence features learned by themself respectively.
Finally, instead of performing a single attentive statistics pooling, we also
find it beneficial to extend the pooling method in a multi-head way which can
discriminate features from multiple aspect. The proposed MACCIF-TDNN
architecture can outperform most of the state-of-the-art TDNN-based systems on
VoxCeleb1 test sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fangyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1"&gt;Zhigang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Hongchen Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1"&gt;Bo Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Extrapolation for Attribute-Enhanced Generation. (arXiv:2107.02968v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.02968</id>
        <link href="http://arxiv.org/abs/2107.02968"/>
        <updated>2021-07-08T01:57:56.168Z</updated>
        <summary type="html"><![CDATA[Attribute extrapolation in sample generation is challenging for deep neural
networks operating beyond the training distribution. We formulate a new task
for extrapolation in sequence generation, focusing on natural language and
proteins, and propose GENhance, a generative framework that enhances attributes
through a learned latent space. Trained on movie reviews and a computed protein
stability dataset, GENhance can generate strongly-positive text reviews and
highly stable protein sequences without being exposed to similar data during
training. We release our benchmark tasks and models to contribute to the study
of generative modeling extrapolation and data-driven design in biology and
chemistry.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1"&gt;Alvin Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madani_A/0/1/0/all/0/1"&gt;Ali Madani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_B/0/1/0/all/0/1"&gt;Ben Krause&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naik_N/0/1/0/all/0/1"&gt;Nikhil Naik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Text Classification of Urdu News using Deep Neural Network. (arXiv:2107.03141v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.03141</id>
        <link href="http://arxiv.org/abs/2107.03141"/>
        <updated>2021-07-08T01:57:56.156Z</updated>
        <summary type="html"><![CDATA[Digital text is increasing day by day on the internet. It is very challenging
to classify a large and heterogeneous collection of data, which require
improved information processing methods to organize text. To classify large
size of corpus, one common approach is to use hierarchical text classification,
which aims to classify textual data in a hierarchical structure. Several
approaches have been proposed to tackle classification of text but most of the
research has been done on English language. This paper proposes a deep learning
model for hierarchical text classification of news in Urdu language -
consisting of 51,325 sentences from 8 online news websites belonging to the
following genres: Sports; Technology; and Entertainment. The objectives of this
paper are twofold: (1) to develop a large human-annotated dataset of news in
Urdu language for hierarchical text classification; and (2) to classify Urdu
news hierarchically using our proposed model based on LSTM mechanism named as
Hierarchical Multi-layer LSTMs (HMLSTM). Our model consists of two modules:
Text Representing Layer, for obtaining text representation in which we use
Word2vec embedding to transform the words to vector and Urdu Hierarchical LSTM
Layer (UHLSTML) an end-to-end fully connected deep LSTMs network to perform
automatic feature learning, we train one LSTM layer for each level of the class
hierarchy. We have performed extensive experiments on our self created dataset
named as Urdu News Dataset for Hierarchical Text Classification (UNDHTC). The
result shows that our proposed method is very effective for hierarchical text
classification and it outperforms baseline methods significantly and also
achieved good results as compare to deep neural model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Javed_T/0/1/0/all/0/1"&gt;Taimoor Ahmed Javed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahzad_W/0/1/0/all/0/1"&gt;Waseem Shahzad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arshad_U/0/1/0/all/0/1"&gt;Umair Arshad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Data Augmentation for Text Classification. (arXiv:2107.03158v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.03158</id>
        <link href="http://arxiv.org/abs/2107.03158"/>
        <updated>2021-07-08T01:57:56.147Z</updated>
        <summary type="html"><![CDATA[Data augmentation, the artificial creation of training data for machine
learning by transformations, is a widely studied research field across machine
learning disciplines. While it is useful for increasing the generalization
capabilities of a model, it can also address many other challenges and
problems, from overcoming a limited amount of training data over regularizing
the objective to limiting the amount data used to protect privacy. Based on a
precise description of the goals and applications of data augmentation (C1) and
a taxonomy for existing works (C2), this survey is concerned with data
augmentation methods for textual classification and aims to achieve a concise
and comprehensive overview for researchers and practitioners (C3). Derived from
the taxonomy, we divided more than 100 methods into 12 different groupings and
provide state-of-the-art references expounding which methods are highly
promising (C4). Finally, research perspectives that may constitute a building
block for future work are given (C5).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bayer_M/0/1/0/all/0/1"&gt;Markus Bayer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaufhold_M/0/1/0/all/0/1"&gt;Marc-Andr&amp;#xe9; Kaufhold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reuter_C/0/1/0/all/0/1"&gt;Christian Reuter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SinSpell: A Comprehensive Spelling Checker for Sinhala. (arXiv:2107.02983v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.02983</id>
        <link href="http://arxiv.org/abs/2107.02983"/>
        <updated>2021-07-08T01:57:56.138Z</updated>
        <summary type="html"><![CDATA[We have built SinSpell, a comprehensive spelling checker for the Sinhala
language which is spoken by over 16 million people, mainly in Sri Lanka.
However, until recently, Sinhala had no spelling checker with acceptable
coverage. Sinspell is still the only open source Sinhala spelling checker.
SinSpell identifies possible spelling errors and suggests corrections. It also
contains a module which auto-corrects evident errors. To maintain accuracy,
SinSpell was designed as a rule-based system based on Hunspell. A set of words
was compiled from several sources and verified. These were divided into
morphological classes, and the valid roots, suffixes and prefixes for each
class were identified, together with lists of irregular words and exceptions.
The errors in a corpus of Sinhala documents were analysed and commonly
misspelled words and types of common errors were identified. We found that the
most common errors were in vowel length and similar sounding letters. Errors
due to incorrect typing and encoding were also found. This analysis was used to
develop the suggestion generator and auto-corrector.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liyanapathirana_U/0/1/0/all/0/1"&gt;Upuli Liyanapathirana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunasinghe_K/0/1/0/all/0/1"&gt;Kaumini Gunasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dias_G/0/1/0/all/0/1"&gt;Gihan Dias&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Advancing CTC-CRF Based End-to-End Speech Recognition with Wordpieces and Conformers. (arXiv:2107.03007v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.03007</id>
        <link href="http://arxiv.org/abs/2107.03007"/>
        <updated>2021-07-08T01:57:56.113Z</updated>
        <summary type="html"><![CDATA[Automatic speech recognition systems have been largely improved in the past
few decades and current systems are mainly hybrid-based and end-to-end-based.
The recently proposed CTC-CRF framework inherits the data-efficiency of the
hybrid approach and the simplicity of the end-to-end approach. In this paper,
we further advance CTC-CRF based ASR technique with explorations on modeling
units and neural architectures. Specifically, we investigate techniques to
enable the recently developed wordpiece modeling units and Conformer neural
networks to be succesfully applied in CTC-CRFs. Experiments are conducted on
two English datasets (Switchboard, Librispeech) and a German dataset from
CommonVoice. Experimental results suggest that (i) Conformer can improve the
recognition performance significantly; (ii) Wordpiece-based systems perform
slightly worse compared with phone-based systems for the target language with a
low degree of grapheme-phoneme correspondence (e.g. English), while the two
systems can perform equally strong when such degree of correspondence is high
for the target language (e.g. German).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Huahuan Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Peng_W/0/1/0/all/0/1"&gt;Wenjie Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ou_Z/0/1/0/all/0/1"&gt;Zhijian Ou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinsong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Answering Chinese Elementary School Social Study Multiple Choice Questions. (arXiv:2107.02893v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.02893</id>
        <link href="http://arxiv.org/abs/2107.02893"/>
        <updated>2021-07-08T01:57:56.100Z</updated>
        <summary type="html"><![CDATA[We present a novel approach to answer the Chinese elementary school Social
Study Multiple Choice questions. Although BERT has demonstrated excellent
performance on Reading Comprehension tasks, it is found not good at handling
some specific types of questions, such as Negation, All-of-the-above, and
None-of-the-above. We thus propose a novel framework to cascade BERT with a
Pre-Processor and an Answer-Selector modules to tackle the above challenges.
Experimental results show the proposed approach effectively improves the
performance of BERT, and thus demonstrate the feasibility of supplementing BERT
with additional modules.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Daniel Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1"&gt;Chao-Chun Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_K/0/1/0/all/0/1"&gt;Keh-Yih Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Transformer for Direct Speech Translation. (arXiv:2107.03069v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.03069</id>
        <link href="http://arxiv.org/abs/2107.03069"/>
        <updated>2021-07-08T01:57:56.089Z</updated>
        <summary type="html"><![CDATA[The advent of Transformer-based models has surpassed the barriers of text.
When working with speech, we must face a problem: the sequence length of an
audio input is not suitable for the Transformer. To bypass this problem, a
usual approach is adding strided convolutional layers, to reduce the sequence
length before using the Transformer. In this paper, we propose a new approach
for direct Speech Translation, where thanks to an efficient Transformer we can
work with a spectrogram without having to use convolutional layers before the
Transformer. This allows the encoder to learn directly from the spectrogram and
no information is lost. We have created an encoder-decoder model, where the
encoder is an efficient Transformer -- the Longformer -- and the decoder is a
traditional Transformer decoder. Our results, which are close to the ones
obtained with the standard approach, show that this is a promising research
direction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alastruey_B/0/1/0/all/0/1"&gt;Belen Alastruey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1"&gt;Gerard I. G&amp;#xe1;llego&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1"&gt;Marta R. Costa-juss&amp;#xe0;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MedGPT: Medical Concept Prediction from Clinical Narratives. (arXiv:2107.03134v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.03134</id>
        <link href="http://arxiv.org/abs/2107.03134"/>
        <updated>2021-07-08T01:57:56.080Z</updated>
        <summary type="html"><![CDATA[The data available in Electronic Health Records (EHRs) provides the
opportunity to transform care, and the best way to provide better care for one
patient is through learning from the data available on all other patients.
Temporal modelling of a patient's medical history, which takes into account the
sequence of past events, can be used to predict future events such as a
diagnosis of a new disorder or complication of a previous or existing disorder.
While most prediction approaches use mostly the structured data in EHRs or a
subset of single-domain predictions and outcomes, we present MedGPT a novel
transformer-based pipeline that uses Named Entity Recognition and Linking tools
(i.e. MedCAT) to structure and organize the free text portion of EHRs and
anticipate a range of future medical events (initially disorders). Since a
large portion of EHR data is in text form, such an approach benefits from a
granular and detailed view of a patient while introducing modest additional
noise. MedGPT effectively deals with the noise and the added granularity, and
achieves a precision of 0.344, 0.552 and 0.640 (vs LSTM 0.329, 0.538 and 0.633)
when predicting the top 1, 3 and 5 candidate future disorders on real world
hospital data from King's College Hospital, London, UK (\textasciitilde600k
patients). We also show that our model captures medical knowledge by testing it
on an experimental medical multiple choice question answering task, and by
examining the attentional focus of the model using gradient-based saliency
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kraljevic_Z/0/1/0/all/0/1"&gt;Zeljko Kraljevic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shek_A/0/1/0/all/0/1"&gt;Anthony Shek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bean_D/0/1/0/all/0/1"&gt;Daniel Bean&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bendayan_R/0/1/0/all/0/1"&gt;Rebecca Bendayan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teo_J/0/1/0/all/0/1"&gt;James Teo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dobson_R/0/1/0/all/0/1"&gt;Richard Dobson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Natural Language Processing for Unstructured Data in Electronic Health Records: a Review. (arXiv:2107.02975v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.02975</id>
        <link href="http://arxiv.org/abs/2107.02975"/>
        <updated>2021-07-08T01:57:56.071Z</updated>
        <summary type="html"><![CDATA[Electronic health records (EHRs), digital collections of patient healthcare
events and observations, are ubiquitous in medicine and critical to healthcare
delivery, operations, and research. Despite this central role, EHRs are
notoriously difficult to process automatically. Well over half of the
information stored within EHRs is in the form of unstructured text (e.g.
provider notes, operation reports) and remains largely untapped for secondary
use. Recently, however, newer neural network and deep learning approaches to
Natural Language Processing (NLP) have made considerable advances,
outperforming traditional statistical and rule-based systems on a variety of
tasks. In this survey paper, we summarize current neural NLP methods for EHR
applications. We focus on a broad scope of tasks, namely, classification and
prediction, word embeddings, extraction, generation, and other topics such as
question answering, phenotyping, knowledge graphs, medical dialogue,
multilinguality, interpretability, etc.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1"&gt;Irene Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1"&gt;Jessica Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldwasser_J/0/1/0/all/0/1"&gt;Jeremy Goldwasser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_N/0/1/0/all/0/1"&gt;Neha Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_W/0/1/0/all/0/1"&gt;Wai Pan Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nuzumlali_M/0/1/0/all/0/1"&gt;Muhammed Yavuz Nuzumlal&amp;#x131;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosand_B/0/1/0/all/0/1"&gt;Benjamin Rosand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yixin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Matthew Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1"&gt;David Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taylor_R/0/1/0/all/0/1"&gt;R. Andrew Taylor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krumholz_H/0/1/0/all/0/1"&gt;Harlan M. Krumholz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1"&gt;Dragomir Radev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured Denoising Diffusion Models in Discrete State-Spaces. (arXiv:2107.03006v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.03006</id>
        <link href="http://arxiv.org/abs/2107.03006"/>
        <updated>2021-07-08T01:57:56.044Z</updated>
        <summary type="html"><![CDATA[Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown
impressive results on image and waveform generation in continuous state spaces.
Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs),
diffusion-like generative models for discrete data that generalize the
multinomial diffusion model of Hoogeboom et al. 2021, by going beyond
corruption processes with uniform transition probabilities. This includes
corruption with transition matrices that mimic Gaussian kernels in continuous
space, matrices based on nearest neighbors in embedding space, and matrices
that introduce absorbing states. The third allows us to draw a connection
between diffusion models and autoregressive and mask-based generative models.
We show that the choice of transition matrix is an important design decision
that leads to improved results in image and text domains. We also introduce a
new loss function that combines the variational lower bound with an auxiliary
cross entropy loss. For text, this model class achieves strong results on
character-level text generation while scaling to large vocabularies on LM1B. On
the image dataset CIFAR-10, our models approach the sample quality and exceed
the log-likelihood of the continuous-space DDPM model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1"&gt;Jacob Austin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johnson_D/0/1/0/all/0/1"&gt;Daniel Johnson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1"&gt;Jonathan Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tarlow_D/0/1/0/all/0/1"&gt;Danny Tarlow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_R/0/1/0/all/0/1"&gt;Rianne van den Berg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Time-Aware Ancient Chinese Text Translation and Inference. (arXiv:2107.03179v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.03179</id>
        <link href="http://arxiv.org/abs/2107.03179"/>
        <updated>2021-07-08T01:57:56.035Z</updated>
        <summary type="html"><![CDATA[In this paper, we aim to address the challenges surrounding the translation
of ancient Chinese text: (1) The linguistic gap due to the difference in eras
results in translations that are poor in quality, and (2) most translations are
missing the contextual information that is often very crucial to understanding
the text. To this end, we improve upon past translation techniques by proposing
the following: We reframe the task as a multi-label prediction task where the
model predicts both the translation and its particular era. We observe that
this helps to bridge the linguistic gap as chronological context is also used
as auxiliary information. % As a natural step of generalization, we pivot on
the modern Chinese translations to generate multilingual outputs. %We show
experimentally the efficacy of our framework in producing quality translation
outputs and also validate our framework on a collected task-specific parallel
corpus. We validate our framework on a parallel corpus annotated with
chronology information and show experimentally its efficacy in producing
quality translation outputs. We release both the code and the data
https://github.com/orina1123/time-aware-ancient-text-translation for future
research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1"&gt;Ernie Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shiue_Y/0/1/0/all/0/1"&gt;Yow-Ting Shiue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeh_H/0/1/0/all/0/1"&gt;Hui-Syuan Yeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demberg_V/0/1/0/all/0/1"&gt;Vera Demberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Dialogue Summarization: Recent Advances and New Frontiers. (arXiv:2107.03175v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.03175</id>
        <link href="http://arxiv.org/abs/2107.03175"/>
        <updated>2021-07-08T01:57:56.021Z</updated>
        <summary type="html"><![CDATA[With the development of dialogue systems and natural language generation
techniques, the resurgence of dialogue summarization has attracted significant
research attentions, which aims to condense the original dialogue into a
shorter version covering salient information. However, there remains a lack of
comprehensive survey for this task. To this end, we take the first step and
present a thorough review of this research field. In detail, we provide an
overview of publicly available research datasets, summarize existing works
according to the domain of input dialogue as well as organize leaderboards
under unified metrics. Furthermore, we discuss some future directions and give
our thoughts. We hope that this first survey of dialogue summarization can
provide the community with a quick access and a general picture to this task
and motivate future researches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1"&gt;Xiachong Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1"&gt;Xiaocheng Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1"&gt;Bing Qin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Android Security using NLP Techniques: A Review. (arXiv:2107.03072v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.03072</id>
        <link href="http://arxiv.org/abs/2107.03072"/>
        <updated>2021-07-08T01:57:56.012Z</updated>
        <summary type="html"><![CDATA[Android is among the most targeted platform by attackers. While attackers are
improving their techniques, traditional solutions based on static and dynamic
analysis have been also evolving. In addition to the application code, Android
applications have some metadata that could be useful for security analysis of
applications. Unlike traditional application distribution mechanisms, Android
applications are distributed centrally in mobile markets. Therefore, beside
application packages, such markets contain app information provided by app
developers and app users. The availability of such useful textual data together
with the advancement in Natural Language Processing (NLP) that is used to
process and understand textual data has encouraged researchers to investigate
the use of NLP techniques in Android security. Especially, security solutions
based on NLP have accelerated in the last 5 years and proven to be useful. This
study reviews these proposals and aim to explore possible research directions
for future studies by presenting state-of-the-art in this domain. We mainly
focus on NLP-based solutions under four categories: description-to-behaviour
fidelity, description generation, privacy and malware detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sen_S/0/1/0/all/0/1"&gt;Sevil Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Can_B/0/1/0/all/0/1"&gt;Burcu Can&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EchoEA: Echo Information between Entities and Relations for Entity Alignment. (arXiv:2107.03054v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.03054</id>
        <link href="http://arxiv.org/abs/2107.03054"/>
        <updated>2021-07-08T01:57:56.004Z</updated>
        <summary type="html"><![CDATA[Entity alignment (EA) is to discover entities referring to the same object in
the real world from different knowledge graphs (KGs). It plays an important
role in automatically integrating KGs from multiple sources.

Existing knowledge graph embedding (KGE) methods based on Graph Neural
Networks (GNNs) have achieved promising results, which enhance entity
representation with relation information unidirectionally. Besides, more and
more methods introduce semi-supervision to ask for more labeled training data.

However, two challenges still exist in these methods: (1) Insufficient
interaction: The interaction between entities and relations is insufficiently
utilized. (2) Low-quality bootstrapping: The generated semi-supervised data is
of low quality.

In this paper, we propose a novel framework, Echo Entity Alignment (EchoEA),
which leverages self-attention mechanism to spread entity information to
relations and echo back to entities. The relation representation is dynamically
computed from entity representation. Symmetrically, the next entity
representation is dynamically calculated from relation representation, which
shows sufficient interaction.

Furthermore, we propose attribute-combined bi-directional global-filtered
strategy (ABGS) to improve bootstrapping, reduce false samples and generate
high-quality training data.

The experimental results on three real-world cross-lingual datasets are
stable at around 96\% at hits@1 on average, showing that our approach not only
significantly outperforms the state-of-the-art methods, but also is universal
and transferable for existing KGE methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xueyuan Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+E_H/0/1/0/all/0/1"&gt;Haihong E&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1"&gt;Wenyu Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Haoran Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Question Answering over Knowledge Graphs with Neural Machine Translation and Entity Linking. (arXiv:2107.02865v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.02865</id>
        <link href="http://arxiv.org/abs/2107.02865"/>
        <updated>2021-07-08T01:57:55.977Z</updated>
        <summary type="html"><![CDATA[The goal of Question Answering over Knowledge Graphs (KGQA) is to find
answers for natural language questions over a knowledge graph. Recent KGQA
approaches adopt a neural machine translation (NMT) approach, where the natural
language question is translated into a structured query language. However, NMT
suffers from the out-of-vocabulary problem, where terms in a question may not
have been seen during training, impeding their translation. This issue is
particularly problematic for the millions of entities that large knowledge
graphs describe. We rather propose a KGQA approach that delegates the
processing of entities to entity linking (EL) systems. NMT is then used to
create a query template with placeholders that are filled by entities
identified in an EL phase. Slot filling is used to decide which entity fills
which placeholder. Experiments for QA over Wikidata show that our approach
outperforms pure NMT: while there remains a strong dependence on having seen
similar query templates during training, errors relating to entities are
greatly reduced.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diomedi_D/0/1/0/all/0/1"&gt;Daniel Diomedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hogan_A/0/1/0/all/0/1"&gt;Aidan Hogan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSANet: Dynamic Segment Aggregation Network for Video-Level Representation Learning. (arXiv:2105.12085v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12085</id>
        <link href="http://arxiv.org/abs/2105.12085"/>
        <updated>2021-07-08T01:57:55.966Z</updated>
        <summary type="html"><![CDATA[Long-range and short-range temporal modeling are two complementary and
crucial aspects of video recognition. Most of the state-of-the-arts focus on
short-range spatio-temporal modeling and then average multiple snippet-level
predictions to yield the final video-level prediction. Thus, their video-level
prediction does not consider spatio-temporal features of how video evolves
along the temporal dimension. In this paper, we introduce a novel Dynamic
Segment Aggregation (DSA) module to capture relationship among snippets. To be
more specific, we attempt to generate a dynamic kernel for a convolutional
operation to aggregate long-range temporal information among adjacent snippets
adaptively. The DSA module is an efficient plug-and-play module and can be
combined with the off-the-shelf clip-based models (i.e., TSM, I3D) to perform
powerful long-range modeling with minimal overhead. The final video
architecture, coined as DSANet. We conduct extensive experiments on several
video recognition benchmarks (i.e., Mini-Kinetics-200, Kinetics-400,
Something-Something V1 and ActivityNet) to show its superiority. Our proposed
DSA module is shown to benefit various video recognition models significantly.
For example, equipped with DSA modules, the top-1 accuracy of I3D ResNet-50 is
improved from 74.9% to 78.2% on Kinetics-400. Codes are available at
https://github.com/whwu95/DSANet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wenhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yuxiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yanwu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xiao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Dongliang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1"&gt;Zhikang Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jin Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yingying Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_M/0/1/0/all/0/1"&gt;Mingde Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1"&gt;Zichao Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yifeng Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kosp2e: Korean Speech to English Translation Corpus. (arXiv:2107.02875v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.02875</id>
        <link href="http://arxiv.org/abs/2107.02875"/>
        <updated>2021-07-08T01:57:55.956Z</updated>
        <summary type="html"><![CDATA[Most speech-to-text (S2T) translation studies use English speech as a source,
which makes it difficult for non-English speakers to take advantage of the S2T
technologies. For some languages, this problem was tackled through corpus
construction, but the farther linguistically from English or the more
under-resourced, this deficiency and underrepresentedness becomes more
significant. In this paper, we introduce kosp2e (read as `kospi'), a corpus
that allows Korean speech to be translated into English text in an end-to-end
manner. We adopt open license speech recognition corpus, translation corpus,
and spoken language corpora to make our dataset freely available to the public,
and check the performance through the pipeline and training-based approaches.
Using pipeline and various end-to-end schemes, we obtain the highest BLEU of
21.3 and 18.0 for each based on the English hypothesis, validating the
feasibility of our data. We plan to supplement annotations for other target
languages through community contributions in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cho_W/0/1/0/all/0/1"&gt;Won Ik Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seok Min Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1"&gt;Hyunchang Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1"&gt;Nam Soo Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topic Modeling in the Voynich Manuscript. (arXiv:2107.02858v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.02858</id>
        <link href="http://arxiv.org/abs/2107.02858"/>
        <updated>2021-07-08T01:57:55.946Z</updated>
        <summary type="html"><![CDATA[This article presents the results of investigations using topic modeling of
the Voynich Manuscript (Beinecke MS408). Topic modeling is a set of
computational methods which are used to identify clusters of subjects within
text. We use latent dirichlet allocation, latent semantic analysis, and
nonnegative matrix factorization to cluster Voynich pages into `topics'. We
then compare the topics derived from the computational models to clusters
derived from the Voynich illustrations and from paleographic analysis. We find
that computationally derived clusters match closely to a conjunction of scribe
and subject matter (as per the illustrations), providing further evidence that
the Voynich Manuscript contains meaningful text.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sterneck_R/0/1/0/all/0/1"&gt;Rachel Sterneck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polish_A/0/1/0/all/0/1"&gt;Annie Polish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bowern_C/0/1/0/all/0/1"&gt;Claire Bowern&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VAENAR-TTS: Variational Auto-Encoder based Non-AutoRegressive Text-to-Speech Synthesis. (arXiv:2107.03298v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.03298</id>
        <link href="http://arxiv.org/abs/2107.03298"/>
        <updated>2021-07-08T01:57:55.934Z</updated>
        <summary type="html"><![CDATA[This paper describes a variational auto-encoder based non-autoregressive
text-to-speech (VAENAR-TTS) model. The autoregressive TTS (AR-TTS) models based
on the sequence-to-sequence architecture can generate high-quality speech, but
their sequential decoding process can be time-consuming. Recently,
non-autoregressive TTS (NAR-TTS) models have been shown to be more efficient
with the parallel decoding process. However, these NAR-TTS models rely on
phoneme-level durations to generate a hard alignment between the text and the
spectrogram. Obtaining duration labels, either through forced alignment or
knowledge distillation, is cumbersome. Furthermore, hard alignment based on
phoneme expansion can degrade the naturalness of the synthesized speech. In
contrast, the proposed model of VAENAR-TTS is an end-to-end approach that does
not require phoneme-level durations. The VAENAR-TTS model does not contain
recurrent structures and is completely non-autoregressive in both the training
and inference phases. Based on the VAE architecture, the alignment information
is encoded in the latent variable, and attention-based soft alignment between
the text and the latent variable is used in the decoder to reconstruct the
spectrogram. Experiments show that VAENAR-TTS achieves state-of-the-art
synthesis quality, while the synthesis speed is comparable with other NAR-TTS
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Hui Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhiyong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xixin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1"&gt;Shiyin Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xunying Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1"&gt;Helen Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SelfCF: A Simple Framework for Self-supervised Collaborative Filtering. (arXiv:2107.03019v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.03019</id>
        <link href="http://arxiv.org/abs/2107.03019"/>
        <updated>2021-07-08T01:57:55.907Z</updated>
        <summary type="html"><![CDATA[Collaborative filtering (CF) is widely used to learn an informative latent
representation of a user or item from observed interactions. Existing CF-based
methods commonly adopt negative sampling to discriminate different items. That
is, observed user-item pairs are treated as positive instances; unobserved
pairs are considered as negative instances and are sampled under a defined
distribution for training. Training with negative sampling on large datasets is
computationally expensive. Further, negative items should be carefully sampled
under the defined distribution, in order to avoid selecting an observed
positive item in the training dataset. Unavoidably, some negative items sampled
from the training dataset could be positive in the test set. Recently,
self-supervised learning (SSL) has emerged as a powerful tool to learn a model
without negative samples. In this paper, we propose a self-supervised
collaborative filtering framework (SelfCF), that is specially designed for
recommender scenario with implicit feedback. The main idea of SelfCF is to
augment the output embeddings generated by backbone networks, because it is
infeasible to augment raw input of user/item ids. We propose and study three
output perturbation techniques that can be applied to different types of
backbone networks including both traditional CF models and graph-based models.
By encapsulating two popular recommendation models into the framework, our
experiments on three datasets show that the best performance of our framework
is comparable or better than the supervised counterpart. We also show that
SelfCF can boost up the performance by up to 8.93\% on average, compared with
another self-supervised framework as the baseline. Source codes are available
at: https://github.com/enoche/SelfCF.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1"&gt;Aixin Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1"&gt;Chunyan Miao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Are you sure?": Preliminary Insights from Scaling Product Comparisons to Multiple Shops. (arXiv:2107.03256v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.03256</id>
        <link href="http://arxiv.org/abs/2107.03256"/>
        <updated>2021-07-08T01:57:55.877Z</updated>
        <summary type="html"><![CDATA[Large eCommerce players introduced comparison tables as a new type of
recommendations. However, building comparisons at scale without pre-existing
training/taxonomy data remains an open challenge, especially within the
operational constraints of shops in the long tail. We present preliminary
results from building a comparison pipeline designed to scale in a multi-shop
scenario: we describe our design choices and run extensive benchmarks on
multiple shops to stress-test it. Finally, we run a small user study on
property selection and conclude by discussing potential improvements and
highlighting the questions that remain to be addressed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chia_P/0/1/0/all/0/1"&gt;Patrick John Chia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1"&gt;Bingqing Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tagliabue_J/0/1/0/all/0/1"&gt;Jacopo Tagliabue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comparative Study of Modular and Joint Approaches for Speaker-Attributed ASR on Monaural Long-Form Audio. (arXiv:2107.02852v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.02852</id>
        <link href="http://arxiv.org/abs/2107.02852"/>
        <updated>2021-07-08T01:57:55.850Z</updated>
        <summary type="html"><![CDATA[Speaker-attributed automatic speech recognition (SA-ASR) is a task to
recognize "who spoke what" from multi-talker recordings. An SA-ASR system
usually consists of multiple modules such as speech separation, speaker
diarization and ASR. On the other hand, considering the joint optimization, an
end-to-end (E2E) SA-ASR model has recently been proposed with promising results
on simulation data. In this paper, we present our recent study on the
comparison of such modular and joint approaches towards SA-ASR on real monaural
recordings. We develop state-of-the-art SA-ASR systems for both modular and
joint approaches by leveraging large-scale training data, including 75 thousand
hours of ASR training data and the VoxCeleb corpus for speaker representation
learning. We also propose a new pipeline that performs the E2E SA-ASR model
after speaker clustering. Our evaluation on the AMI meeting corpus reveals that
after fine-tuning with a small real data, the joint system performs 9.2--29.4%
better in accuracy compared to the best modular system while the modular system
performs better before such fine-tuning. We also conduct various error analyses
to show the remaining issues for the monaural SA-ASR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1"&gt;Naoyuki Kanda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xiao_X/0/1/0/all/0/1"&gt;Xiong Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_T/0/1/0/all/0/1"&gt;Tianyan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gaur_Y/0/1/0/all/0/1"&gt;Yashesh Gaur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaofei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1"&gt;Zhong Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yoshioka_T/0/1/0/all/0/1"&gt;Takuya Yoshioka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-View Exocentric to Egocentric Video Synthesis. (arXiv:2107.03120v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03120</id>
        <link href="http://arxiv.org/abs/2107.03120"/>
        <updated>2021-07-08T01:57:55.834Z</updated>
        <summary type="html"><![CDATA[Cross-view video synthesis task seeks to generate video sequences of one view
from another dramatically different view. In this paper, we investigate the
exocentric (third-person) view to egocentric (first-person) view video
generation task. This is challenging because egocentric view sometimes is
remarkably different from the exocentric view. Thus, transforming the
appearances across the two different views is a non-trivial task. Particularly,
we propose a novel Bi-directional Spatial Temporal Attention Fusion Generative
Adversarial Network (STA-GAN) to learn both spatial and temporal information to
generate egocentric video sequences from the exocentric view. The proposed
STA-GAN consists of three parts: temporal branch, spatial branch, and attention
fusion. First, the temporal and spatial branches generate a sequence of fake
frames and their corresponding features. The fake frames are generated in both
downstream and upstream directions for both temporal and spatial branches.
Next, the generated four different fake frames and their corresponding features
(spatial and temporal branches in two directions) are fed into a novel
multi-generation attention fusion module to produce the final video sequence.
Meanwhile, we also propose a novel temporal and spatial dual-discriminator for
more robust network optimization. Extensive experiments on the Side2Ego and
Top2Ego datasets show that the proposed STA-GAN significantly outperforms the
existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Gaowen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Hao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Latapie_H/0/1/0/all/0/1"&gt;Hugo Latapie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Corso_J/0/1/0/all/0/1"&gt;Jason Corso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yan Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graphing else matters: exploiting aspect opinions and ratings in explainable graph-based recommendations. (arXiv:2107.03226v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.03226</id>
        <link href="http://arxiv.org/abs/2107.03226"/>
        <updated>2021-07-08T01:57:55.820Z</updated>
        <summary type="html"><![CDATA[The success of neural network embeddings has entailed a renewed interest in
using knowledge graphs for a wide variety of machine learning and information
retrieval tasks. In particular, current recommendation methods based on graph
embeddings have shown state-of-the-art performance. These methods commonly
encode latent rating patterns and content features. Different from previous
work, in this paper, we propose to exploit embeddings extracted from graphs
that combine information from ratings and aspect-based opinions expressed in
textual reviews. We then adapt and evaluate state-of-the-art graph embedding
techniques over graphs generated from Amazon and Yelp reviews on six domains,
outperforming baseline recommenders. Our approach has the advantage of
providing explanations which leverage aspect-based opinions given by users
about recommended items. Furthermore, we also provide examples of the
applicability of recommendations utilizing aspect opinions as explanations in a
visualization dashboard, which allows obtaining information about the most and
least liked aspects of similar users obtained from the embeddings of an input
graph.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cantador_I/0/1/0/all/0/1"&gt;Iv&amp;#xe1;n Cantador&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carvallo_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9;s Carvallo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diez_F/0/1/0/all/0/1"&gt;Fernando Diez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parra_D/0/1/0/all/0/1"&gt;Denis Parra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoDebias: Learning to Debias for Recommendation. (arXiv:2105.04170v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04170</id>
        <link href="http://arxiv.org/abs/2105.04170"/>
        <updated>2021-07-08T01:57:55.795Z</updated>
        <summary type="html"><![CDATA[Recommender systems rely on user behavior data like ratings and clicks to
build personalization model. However, the collected data is observational
rather than experimental, causing various biases in the data which
significantly affect the learned model. Most existing work for recommendation
debiasing, such as the inverse propensity scoring and imputation approaches,
focuses on one or two specific biases, lacking the universal capacity that can
account for mixed or even unknown biases in the data.

Towards this research gap, we first analyze the origin of biases from the
perspective of \textit{risk discrepancy} that represents the difference between
the expectation empirical risk and the true risk. Remarkably, we derive a
general learning framework that well summarizes most existing debiasing
strategies by specifying some parameters of the general framework. This
provides a valuable opportunity to develop a universal solution for debiasing,
e.g., by learning the debiasing parameters from data. However, the training
data lacks important signal of how the data is biased and what the unbiased
data looks like. To move this idea forward, we propose \textit{AotoDebias} that
leverages another (small) set of uniform data to optimize the debiasing
parameters by solving the bi-level optimization problem with meta-learning.
Through theoretical analyses, we derive the generalization bound for AutoDebias
and prove its ability to acquire the appropriate debiasing strategy. Extensive
experiments on two real datasets and a simulated dataset demonstrated
effectiveness of AutoDebias. The code is available at
\url{https://github.com/DongHande/AutoDebias}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiawei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Hande Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1"&gt;Yang Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangnan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1"&gt;Xin Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1"&gt;Guli Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Keping Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WeClick: Weakly-Supervised Video Semantic Segmentation with Click Annotations. (arXiv:2107.03088v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.03088</id>
        <link href="http://arxiv.org/abs/2107.03088"/>
        <updated>2021-07-08T01:57:55.772Z</updated>
        <summary type="html"><![CDATA[Compared with tedious per-pixel mask annotating, it is much easier to
annotate data by clicks, which costs only several seconds for an image.
However, applying clicks to learn video semantic segmentation model has not
been explored before. In this work, we propose an effective weakly-supervised
video semantic segmentation pipeline with click annotations, called WeClick,
for saving laborious annotating effort by segmenting an instance of the
semantic class with only a single click. Since detailed semantic information is
not captured by clicks, directly training with click labels leads to poor
segmentation predictions. To mitigate this problem, we design a novel memory
flow knowledge distillation strategy to exploit temporal information (named
memory flow) in abundant unlabeled video frames, by distilling the neighboring
predictions to the target frame via estimated motion. Moreover, we adopt
vanilla knowledge distillation for model compression. In this case, WeClick
learns compact video semantic segmentation models with the low-cost click
annotations during the training phase yet achieves real-time and accurate
models during the inference period. Experimental results on Cityscapes and
Camvid show that WeClick outperforms the state-of-the-art methods, increases
performance by 10.24% mIoU than baseline, and achieves real-time execution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Peidong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zibin He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1"&gt;Xiyu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yong Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1"&gt;Shutao Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1"&gt;Feng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1"&gt;Maowei Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computationally Efficient Optimization of Plackett-Luce Ranking Models for Relevance and Fairness. (arXiv:2105.00855v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00855</id>
        <link href="http://arxiv.org/abs/2105.00855"/>
        <updated>2021-07-08T01:57:55.749Z</updated>
        <summary type="html"><![CDATA[Recent work has proposed stochastic Plackett-Luce (PL) ranking models as a
robust choice for optimizing relevance and fairness metrics. Unlike their
deterministic counterparts that require heuristic optimization algorithms, PL
models are fully differentiable. Theoretically, they can be used to optimize
ranking metrics via stochastic gradient descent. However, in practice, the
computation of the gradient is infeasible because it requires one to iterate
over all possible permutations of items. Consequently, actual applications rely
on approximating the gradient via sampling techniques. In this paper, we
introduce a novel algorithm: PL-Rank, that estimates the gradient of a PL
ranking model w.r.t. both relevance and fairness metrics. Unlike existing
approaches that are based on policy gradients, PL-Rank makes use of the
specific structure of PL models and ranking metrics. Our experimental analysis
shows that PL-Rank has a greater sample-efficiency and is computationally less
costly than existing policy gradients, resulting in faster convergence at
higher performance. PL-Rank further enables the industry to apply PL models for
more relevant and fairer real-world ranking systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oosterhuis_H/0/1/0/all/0/1"&gt;Harrie Oosterhuis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Big Are Peoples' Computer Files? File Size Distributions Among User-managed Collections. (arXiv:2107.03272v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2107.03272</id>
        <link href="http://arxiv.org/abs/2107.03272"/>
        <updated>2021-07-08T01:57:55.715Z</updated>
        <summary type="html"><![CDATA[Improving file management interfaces and optimising system performance
requires current data about users' digital collections and particularly about
the file size distributions of such collections. However, prior works have
examined only the sizes of system files and users' work files in varied
contexts, and there has been no such study since 2013; it therefore remains
unclear how today's file sizes are distributed, particularly personal files,
and further if distributions differ among the major operating systems or common
occupations. Here we examine such differences among 49 million files in 348
user collections. We find that the average file size has grown more than
ten-fold since the mid-2000s, though most files are still under 8 MB, and that
there are demographic and technological influences in the size distributions.
We discuss the implications for user interfaces, system optimisation, and PIM
research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dinneen_J/0/1/0/all/0/1"&gt;Jesse David Dinneen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1"&gt;Ba Xuan Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
</feed>