<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://looperxx.github.io/ArxivDaily/index.html</id>
    <title>ArxivDaily</title>
    <updated>2021-05-24T05:08:43.249Z</updated>
    <generator>osmosfeed 1.8.1</generator>
    <link rel="alternate" href="https://looperxx.github.io/ArxivDaily/index.html"/>
    <link rel="self" href="https://looperxx.github.io/ArxivDaily/feed.atom"/>
    <entry>
        <title type="html"><![CDATA[Unsupervised MRI Reconstruction via Zero-Shot Learned Adversarial Transformers. (arXiv:2105.08059v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08059</id>
        <link href="http://arxiv.org/abs/2105.08059"/>
        <updated>2021-05-24T05:08:43.206Z</updated>
        <summary type="html"><![CDATA[Supervised deep learning has swiftly become a workhorse for accelerated MRI
in recent years, offering state-of-the-art performance in image reconstruction
from undersampled acquisitions. Training deep supervised models requires large
datasets of undersampled and fully-sampled acquisitions typically from a
matching set of subjects. Given scarce access to large medical datasets, this
limitation has sparked interest in unsupervised methods that reduce reliance on
fully-sampled ground-truth data. A common framework is based on the deep image
prior, where network-driven regularization is enforced directly during
inference on undersampled acquisitions. Yet, canonical convolutional
architectures are suboptimal in capturing long-range relationships, and
randomly initialized networks may hamper convergence. To address these
limitations, here we introduce a novel unsupervised MRI reconstruction method
based on zero-Shot Learned Adversarial TransformERs (SLATER). SLATER embodies a
deep adversarial network with cross-attention transformer blocks to map noise
and latent variables onto MR images. This unconditional network learns a
high-quality MRI prior in a self-supervised encoding task. A zero-shot
reconstruction is performed on undersampled test data, where inference is
performed by optimizing network parameters, latent and noise variables to
ensure maximal consistency to multi-coil MRI data. Comprehensive experiments on
brain MRI datasets clearly demonstrate the superior performance of SLATER
against several state-of-the-art unsupervised methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Korkmaz_Y/0/1/0/all/0/1"&gt;Yilmaz Korkmaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dar_S/0/1/0/all/0/1"&gt;Salman UH Dar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yurt_M/0/1/0/all/0/1"&gt;Mahmut Yurt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ozbey_M/0/1/0/all/0/1"&gt;Muzaffer &amp;#xd6;zbey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cukur_T/0/1/0/all/0/1"&gt;Tolga &amp;#xc7;ukur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-guided Chained Context Aggregation for Semantic Segmentation. (arXiv:2002.12041v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.12041</id>
        <link href="http://arxiv.org/abs/2002.12041"/>
        <updated>2021-05-24T05:08:43.184Z</updated>
        <summary type="html"><![CDATA[The way features propagate in Fully Convolutional Networks is of momentous
importance to capture multi-scale contexts for obtaining precise segmentation
masks. This paper proposes a novel series-parallel hybrid paradigm called the
Chained Context Aggregation Module (CAM) to diversify feature propagation. CAM
gains features of various spatial scales through chain-connected ladder-style
information flows and fuses them in a two-stage process, namely pre-fusion and
re-fusion. The serial flow continuously increases receptive fields of output
neurons and those in parallel encode different region-based contexts. Each
information flow is a shallow encoder-decoder with appropriate down-sampling
scales to sufficiently capture contextual information. We further adopt an
attention model in CAM to guide feature re-fusion. Based on these developments,
we construct the Chained Context Aggregation Network (CANet), which employs an
asymmetric decoder to recover precise spatial details of prediction maps. We
conduct extensive experiments on six challenging datasets, including Pascal VOC
2012, Pascal Context, Cityscapes, CamVid, SUN-RGBD and GATECH. Results evidence
that CANet achieves state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1"&gt;Quan Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fagui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jun Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CARRADA Dataset: Camera and Automotive Radar with Range-Angle-Doppler Annotations. (arXiv:2005.01456v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.01456</id>
        <link href="http://arxiv.org/abs/2005.01456"/>
        <updated>2021-05-24T05:08:43.164Z</updated>
        <summary type="html"><![CDATA[High quality perception is essential for autonomous driving (AD) systems. To
reach the accuracy and robustness that are required by such systems, several
types of sensors must be combined. Currently, mostly cameras and laser scanners
(lidar) are deployed to build a representation of the world around the vehicle.
While radar sensors have been used for a long time in the automotive industry,
they are still under-used for AD despite their appealing characteristics
(notably, their ability to measure the relative speed of obstacles and to
operate even in adverse weather conditions). To a large extent, this situation
is due to the relative lack of automotive datasets with real radar signals that
are both raw and annotated. In this work, we introduce CARRADA, a dataset of
synchronized camera and radar recordings with range-angle-Doppler annotations.
We also present a semi-automatic annotation approach, which was used to
annotate the dataset, and a radar semantic segmentation baseline, which we
evaluate on several metrics. Both our code and dataset are available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ouaknine_A/0/1/0/all/0/1"&gt;A. Ouaknine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Newson_A/0/1/0/all/0/1"&gt;A. Newson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rebut_J/0/1/0/all/0/1"&gt;J. Rebut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tupin_F/0/1/0/all/0/1"&gt;F. Tupin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1"&gt;P. P&amp;#xe9;rez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identity-Free Facial Expression Recognition using conditional Generative Adversarial Network. (arXiv:1903.08051v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1903.08051</id>
        <link href="http://arxiv.org/abs/1903.08051"/>
        <updated>2021-05-24T05:08:43.156Z</updated>
        <summary type="html"><![CDATA[A novel Identity-Free conditional Generative Adversarial Network (IF-GAN) was
proposed for Facial Expression Recognition (FER) to explicitly reduce high
inter-subject variations caused by identity-related facial attributes, e.g.,
age, race, and gender. As part of an end-to-end system, a cGAN was designed to
transform a given input facial expression image to an "average" identity face
with the same expression as the input. Then, identity-free FER is possible
since the generated images have the same synthetic "average" identity and
differ only in their displayed expressions. Experiments on four facial
expression datasets, one with spontaneous expressions, show that IF-GAN
outperforms the baseline CNN and achieves state-of-the-art performance for FER.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jie Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1"&gt;Zibo Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1"&gt;Ahmed Shehab Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhiyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OReilly_J/0/1/0/all/0/1"&gt;James O&amp;#x27;Reilly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Shizhong Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1"&gt;Yan Tong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The SpaceNet Multi-Temporal Urban Development Challenge. (arXiv:2102.11958v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11958</id>
        <link href="http://arxiv.org/abs/2102.11958"/>
        <updated>2021-05-24T05:08:43.135Z</updated>
        <summary type="html"><![CDATA[Building footprints provide a useful proxy for a great many humanitarian
applications. For example, building footprints are useful for high fidelity
population estimates, and quantifying population statistics is fundamental to
~1/4 of the United Nations Sustainable Development Goals Indicators. In this
paper we (the SpaceNet Partners) discuss efforts to develop techniques for
precise building footprint localization, tracking, and change detection via the
SpaceNet Multi-Temporal Urban Development Challenge (also known as SpaceNet 7).
In this NeurIPS 2020 competition, participants were asked identify and track
buildings in satellite imagery time series collected over rapidly urbanizing
areas. The competition centered around a brand new open source dataset of
Planet Labs satellite imagery mosaics at 4m resolution, which includes 24
images (one per month) covering ~100 unique geographies. Tracking individual
buildings at this resolution is quite challenging, yet the winning participants
demonstrated impressive performance with the newly developed SpaceNet Change
and Object Tracking (SCOT) metric. This paper details the top-5 winning
approaches, as well as analysis of results that yielded a handful of
interesting anecdotes such as decreasing performance with latitude.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Etten_A/0/1/0/all/0/1"&gt;Adam Van Etten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hogan_D/0/1/0/all/0/1"&gt;Daniel Hogan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VisualSparta: An Embarrassingly Simple Approach to Large-scale Text-to-Image Search with Weighted Bag-of-words. (arXiv:2101.00265v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00265</id>
        <link href="http://arxiv.org/abs/2101.00265"/>
        <updated>2021-05-24T05:08:43.123Z</updated>
        <summary type="html"><![CDATA[Text-to-image retrieval is an essential task in cross-modal information
retrieval, i.e., retrieving relevant images from a large and unlabelled dataset
given textual queries. In this paper, we propose VisualSparta, a novel
(Visual-text Sparse Transformer Matching) model that shows significant
improvement in terms of both accuracy and efficiency. VisualSparta is capable
of outperforming previous state-of-the-art scalable methods in MSCOCO and
Flickr30K. We also show that it achieves substantial retrieving speed
advantages, i.e., for a 1 million image index, VisualSparta using CPU gets
~391X speedup compared to CPU vector search and ~5.4X speedup compared to
vector search with GPU acceleration. Experiments show that this speed advantage
even gets bigger for larger datasets because VisualSparta can be efficiently
implemented as an inverted index. To the best of our knowledge, VisualSparta is
the first transformer-based text-to-image retrieval model that can achieve
real-time searching for large-scale datasets, with significant accuracy
improvement compared to previous state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaopeng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tiancheng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kyusong Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking Domain Randomisation for Visual Sim-to-Real Transfer. (arXiv:2011.07112v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.07112</id>
        <link href="http://arxiv.org/abs/2011.07112"/>
        <updated>2021-05-24T05:08:43.117Z</updated>
        <summary type="html"><![CDATA[Domain randomisation is a very popular method for visual sim-to-real transfer
in robotics, due to its simplicity and ability to achieve transfer without any
real-world images at all. Nonetheless, a number of design choices must be made
to achieve optimal transfer. In this paper, we perform a comprehensive
benchmarking study on these different choices, with two key experiments
evaluated on a real-world object pose estimation task. First, we study the
rendering quality, and find that a small number of high-quality images is
superior to a large number of low-quality images. Second, we study the type of
randomisation, and find that both distractors and textures are important for
generalisation to novel environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alghonaim_R/0/1/0/all/0/1"&gt;Raghad Alghonaim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1"&gt;Edward Johns&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Frequency Domain Loss Function for Deep Exposure Correction of Dark Images. (arXiv:2104.10856v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10856</id>
        <link href="http://arxiv.org/abs/2104.10856"/>
        <updated>2021-05-24T05:08:42.990Z</updated>
        <summary type="html"><![CDATA[We address the problem of exposure correction of dark, blurry and noisy
images captured in low-light conditions in the wild. Classical image-denoising
filters work well in the frequency space but are constrained by several factors
such as the correct choice of thresholds, frequency estimates etc. On the other
hand, traditional deep networks are trained end-to-end in the RGB space by
formulating this task as an image-translation problem. However, that is done
without any explicit constraints on the inherent noise of the dark images and
thus produce noisy and blurry outputs. To this end we propose a DCT/FFT based
multi-scale loss function, which when combined with traditional losses, trains
a network to translate the important features for visually pleasing output. Our
loss function is end-to-end differentiable, scale-agnostic, and generic; i.e.,
it can be applied to both RAW and JPEG images in most existing frameworks
without additional overhead. Using this loss function, we report significant
improvements over the state-of-the-art using quantitative metrics and
subjective tests.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yadav_O/0/1/0/all/0/1"&gt;Ojasvi Yadav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ghosal_K/0/1/0/all/0/1"&gt;Koustav Ghosal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lutz_S/0/1/0/all/0/1"&gt;Sebastian Lutz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Smolic_A/0/1/0/all/0/1"&gt;Aljosa Smolic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Going Deeper through the Gleason Scoring Scale: An Automatic end-to-end System for Histology Prostate Grading and Cribriform Pattern Detection. (arXiv:2105.10490v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10490</id>
        <link href="http://arxiv.org/abs/2105.10490"/>
        <updated>2021-05-24T05:08:42.984Z</updated>
        <summary type="html"><![CDATA[The Gleason scoring system is the primary diagnostic and prognostic tool for
prostate cancer. In recent years, with the development of digitisation devices,
the use of computer vision techniques for the analysis of biopsies has
increased. However, to the best of the authors' knowledge, the development of
algorithms to automatically detect individual cribriform patterns belonging to
Gleason grade 4 has not yet been studied in the literature. The objective of
the work presented in this paper is to develop a deep-learning-based system
able to support pathologists in the daily analysis of prostate biopsies. The
methodological core of this work is a patch-wise predictive model based on
convolutional neural networks able to determine the presence of cancerous
patterns. In particular, we train from scratch a simple self-design
architecture. The cribriform pattern is detected by retraining the set of
filters of the last convolutional layer in the network. From the reconstructed
prediction map, we compute the percentage of each Gleason grade in the tissue
to feed a multi-layer perceptron which provides a biopsy-level score.mIn our
SICAPv2 database, composed of 182 annotated whole slide images, we obtained a
Cohen's quadratic kappa of 0.77 in the test set for the patch-level Gleason
grading with the proposed architecture trained from scratch. Our results
outperform previous ones reported in the literature. Furthermore, this model
reaches the level of fine-tuned state-of-the-art architectures in a
patient-based four groups cross validation. In the cribriform pattern detection
task, we obtained an area under ROC curve of 0.82. Regarding the biopsy Gleason
scoring, we achieved a quadratic Cohen's Kappa of 0.81 in the test subset.
Shallow CNN architectures trained from scratch outperform current
state-of-the-art methods for Gleason grades classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Silva_Rodriguez_J/0/1/0/all/0/1"&gt;Julio Silva-Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Colomer_A/0/1/0/all/0/1"&gt;Adri&amp;#xe1;n Colomer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sales_M/0/1/0/all/0/1"&gt;Mar&amp;#xed;a A. Sales&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Molina_R/0/1/0/all/0/1"&gt;Rafael Molina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1"&gt;Valery Naranjo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distinguishing artefacts: evaluating the saturation point of convolutional neural networks. (arXiv:2105.10448v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10448</id>
        <link href="http://arxiv.org/abs/2105.10448"/>
        <updated>2021-05-24T05:08:42.966Z</updated>
        <summary type="html"><![CDATA[Prior work has shown Convolutional Neural Networks (CNNs) trained on
surrogate Computer Aided Design (CAD) models are able to detect and classify
real-world artefacts from photographs. The applications of which support
twinning of digital and physical assets in design, including rapid extraction
of part geometry from model repositories, information search \& retrieval and
identifying components in the field for maintenance, repair, and recording. The
performance of CNNs in classification tasks have been shown dependent on
training data set size and number of classes. Where prior works have used
relatively small surrogate model data sets ($<100$ models), the question
remains as to the ability of a CNN to differentiate between models in
increasingly large model repositories. This paper presents a method for
generating synthetic image data sets from online CAD model repositories, and
further investigates the capacity of an off-the-shelf CNN architecture trained
on synthetic data to classify models as class size increases. 1,000 CAD models
were curated and processed to generate large scale surrogate data sets,
featuring model coverage at steps of 10$^{\circ}$, 30$^{\circ}$, 60$^{\circ}$,
and 120$^{\circ}$ degrees. The findings demonstrate the capability of computer
vision algorithms to classify artefacts in model repositories of up to 200,
beyond this point the CNN's performance is observed to deteriorate
significantly, limiting its present ability for automated twinning of physical
to digital artefacts. Although, a match is more often found in the top-5
results showing potential for information search and retrieval on large
repositories of surrogate models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Real_R/0/1/0/all/0/1"&gt;Ric Real&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gopsill_J/0/1/0/all/0/1"&gt;James Gopsill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jones_D/0/1/0/all/0/1"&gt;David Jones&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snider_C/0/1/0/all/0/1"&gt;Chris Snider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hicks_B/0/1/0/all/0/1"&gt;Ben Hicks&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-learning for weakly supervised Gleason grading of local patterns. (arXiv:2105.10420v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10420</id>
        <link href="http://arxiv.org/abs/2105.10420"/>
        <updated>2021-05-24T05:08:42.960Z</updated>
        <summary type="html"><![CDATA[Prostate cancer is one of the main diseases affecting men worldwide. The gold
standard for diagnosis and prognosis is the Gleason grading system. In this
process, pathologists manually analyze prostate histology slides under
microscope, in a high time-consuming and subjective task. In the last years,
computer-aided-diagnosis (CAD) systems have emerged as a promising tool that
could support pathologists in the daily clinical practice. Nevertheless, these
systems are usually trained using tedious and prone-to-error pixel-level
annotations of Gleason grades in the tissue. To alleviate the need of manual
pixel-wise labeling, just a handful of works have been presented in the
literature. Motivated by this, we propose a novel weakly-supervised
deep-learning model, based on self-learning CNNs, that leverages only the
global Gleason score of gigapixel whole slide images during training to
accurately perform both, grading of patch-level patterns and biopsy-level
scoring. To evaluate the performance of the proposed method, we perform
extensive experiments on three different external datasets for the patch-level
Gleason grading, and on two different test sets for global Grade Group
prediction. We empirically demonstrate that our approach outperforms its
supervised counterpart on patch-level Gleason grading by a large margin, as
well as state-of-the-art methods on global biopsy-level scoring. Particularly,
the proposed model brings an average improvement on the Cohen's quadratic kappa
(k) score of nearly 18% compared to full-supervision for the patch-level
Gleason grading task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Silva_Rodriguez_J/0/1/0/all/0/1"&gt;Julio Silva-Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Colomer_A/0/1/0/all/0/1"&gt;Adri&amp;#xe1;n Colomer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dolz_J/0/1/0/all/0/1"&gt;Jose Dolz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1"&gt;Valery Naranjo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ICON: Learning Regular Maps Through Inverse Consistency. (arXiv:2105.04459v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04459</id>
        <link href="http://arxiv.org/abs/2105.04459"/>
        <updated>2021-05-24T05:08:42.954Z</updated>
        <summary type="html"><![CDATA[Learning maps between data samples is fundamental. Applications range from
representation learning, image translation and generative modeling, to the
estimation of spatial deformations. Such maps relate feature vectors, or map
between feature spaces. Well-behaved maps should be regular, which can be
imposed explicitly or may emanate from the data itself. We explore what induces
regularity for spatial transformations, e.g., when computing image
registrations. Classical optimization-based models compute maps between pairs
of samples and rely on an appropriate regularizer for well-posedness. Recent
deep learning approaches have attempted to avoid using such regularizers
altogether by relying on the sample population instead. We explore if it is
possible to obtain spatial regularity using an inverse consistency loss only
and elucidate what explains map regularity in such a context. We find that deep
networks combined with an inverse consistency loss and randomized off-grid
interpolation yield well behaved, approximately diffeomorphic, spatial
transformations. Despite the simplicity of this approach, our experiments
present compelling evidence, on both synthetic and real data, that regular maps
can be obtained without carefully tuned explicit regularizers, while achieving
competitive registration performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Greer_H/0/1/0/all/0/1"&gt;Hastings Greer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwitt_R/0/1/0/all/0/1"&gt;Roland Kwitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vialard_F/0/1/0/all/0/1"&gt;Francois-Xavier Vialard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niethammer_M/0/1/0/all/0/1"&gt;Marc Niethammer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Framework for Efficient Deep Learning Using Metasurfaces Optics. (arXiv:2011.11728v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11728</id>
        <link href="http://arxiv.org/abs/2011.11728"/>
        <updated>2021-05-24T05:08:42.948Z</updated>
        <summary type="html"><![CDATA[Deep learning using Convolutional Neural Networks (CNNs) has been shown to
significantly out-performed many conventional vision algorithms. Despite
efforts to increase the CNN efficiency both algorithmically and with
specialized hardware, deep learning remains difficult to deploy in
resource-constrained environments. In this paper, we propose an end-to-end
framework to explore optically compute the CNNs in free-space, much like a
computational camera. Compared to existing free-space optics-based approaches
which are limited to processing single-channel (i.e., grayscale) inputs, we
propose the first general approach, based on nanoscale meta-surface optics,
that can process RGB data directly from the natural scenes. Our system achieves
up to an order of magnitude energy saving, simplifies the sensor design, all
the while sacrificing little network accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Burgos_C/0/1/0/all/0/1"&gt;Carlos Mauricio Villegas Burgos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1"&gt;Tianqi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vamivakas_N/0/1/0/all/0/1"&gt;Nick Vamivakas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuhao Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying Uncertainty from Different Sources in Deep Neural Networks for Image Classification. (arXiv:2011.08712v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08712</id>
        <link href="http://arxiv.org/abs/2011.08712"/>
        <updated>2021-05-24T05:08:42.941Z</updated>
        <summary type="html"><![CDATA[Quantifying uncertainty in a model's predictions is important as it enables
the safety of an AI system to be increased by acting on the model's output in
an informed manner. This is crucial for applications where the cost of an error
is high, such as in autonomous vehicle control, medical image analysis,
financial estimations or legal fields. Deep Neural Networks are powerful
predictors that have recently achieved state-of-the-art performance on a wide
spectrum of tasks. Quantifying predictive uncertainty in DNNs is a challenging
and yet on-going problem. In this paper we propose a complete framework to
capture and quantify all of these three types of uncertainties in DNNs for
image classification. This framework includes an ensemble of CNNs for model
uncertainty, a supervised reconstruction auto-encoder to capture distributional
uncertainty and using the output of activation functions in the last layer of
the network, to capture data uncertainty. Finally we demonstrate the efficiency
of our method on popular image datasets for classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khoshsirat_A/0/1/0/all/0/1"&gt;Aria Khoshsirat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Modal Answer Validation for Knowledge-Based VQA. (arXiv:2103.12248v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12248</id>
        <link href="http://arxiv.org/abs/2103.12248"/>
        <updated>2021-05-24T05:08:42.923Z</updated>
        <summary type="html"><![CDATA[The problem of knowledge-based visual question answering involves answering
questions that require external knowledge in addition to the content of the
image. Such knowledge typically comes in a variety of forms, including visual,
textual, and commonsense knowledge. The use of more knowledge sources, however,
also increases the chance of retrieving more irrelevant or noisy facts, making
it difficult to comprehend the facts and find the answer. To address this
challenge, we propose Multi-modal Answer Validation using External knowledge
(MAVEx), where the idea is to validate a set of promising answer candidates
based on answer-specific knowledge retrieval. This is in contrast to existing
approaches that search for the answer in a vast collection of often irrelevant
facts. Our approach aims to learn which knowledge source should be trusted for
each answer candidate and how to validate the candidate using that source. We
consider a multi-modal setting, relying on both textual and visual knowledge
resources, including images searched using Google, sentences from Wikipedia
articles, and concepts from ConceptNet. Our experiments with OK-VQA, a
challenging knowledge-based VQA dataset, demonstrate that MAVEx achieves new
state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jialin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiasen Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1"&gt;Ashish Sabharwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1"&gt;Roozbeh Mottaghi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Halluci-Net: Scene Completion by Exploiting Object Co-occurrence Relationships. (arXiv:2004.08614v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.08614</id>
        <link href="http://arxiv.org/abs/2004.08614"/>
        <updated>2021-05-24T05:08:42.917Z</updated>
        <summary type="html"><![CDATA[Recently, there has been substantial progress in image synthesis from
semantic labelmaps. However, methods used for this task assume the availability
of complete and unambiguous labelmaps, with instance boundaries of objects, and
class labels for each pixel. This reliance on heavily annotated inputs
restricts the application of image synthesis techniques to real-world
applications, especially under uncertainty due to weather, occlusion, or noise.
On the other hand, algorithms that can synthesize images from sparse labelmaps
or sketches are highly desirable as tools that can guide content creators and
artists to quickly generate scenes by simply specifying locations of a few
objects. In this paper, we address the problem of complex scene completion from
sparse labelmaps. Under this setting, very few details about the scene (30\% of
object instances) are available as input for image synthesis. We propose a
two-stage deep network based method, called `Halluci-Net', that learns
co-occurence relationships between objects in scenes, and then exploits these
relationships to produce a dense and complete labelmap. The generated dense
labelmap can then be used as input by state-of-the-art image synthesis
techniques like pix2pixHD to obtain the final image. The proposed method is
evaluated on the Cityscapes dataset and it outperforms two baselines methods on
performance metrics like Fr\'echet Inception Distance (FID), semantic
segmentation accuracy, and similarity in object co-occurrences. We also show
qualitative results on a subset of ADE20K dataset that contains bedroom images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_K/0/1/0/all/0/1"&gt;Kuldeep Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1"&gt;Tejas Gokhale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rajhans Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turaga_P/0/1/0/all/0/1"&gt;Pavan Turaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sankaranarayanan_A/0/1/0/all/0/1"&gt;Aswin Sankaranarayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Impressions2Font: Generating Fonts by Specifying Impressions. (arXiv:2103.10036v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10036</id>
        <link href="http://arxiv.org/abs/2103.10036"/>
        <updated>2021-05-24T05:08:42.910Z</updated>
        <summary type="html"><![CDATA[Various fonts give us various impressions, which are often represented by
words. This paper proposes Impressions2Font (Imp2Font) that generates font
images with specific impressions. Imp2Font is an extended version of
conditional generative adversarial networks (GANs). More precisely, Imp2Font
accepts an arbitrary number of impression words as the condition to generate
the font images. These impression words are converted into a soft-constraint
vector by an impression embedding module built on a word embedding technique.
Qualitative and quantitative evaluations prove that Imp2Font generates font
images with higher quality than comparative methods by providing multiple
impression words or even unlearned words.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Matsuda_S/0/1/0/all/0/1"&gt;Seiya Matsuda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kimura_A/0/1/0/all/0/1"&gt;Akisato Kimura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1"&gt;Seiichi Uchida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Psychophysical Oriented Saliency Map Prediction Model. (arXiv:2011.04076v8 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04076</id>
        <link href="http://arxiv.org/abs/2011.04076"/>
        <updated>2021-05-24T05:08:42.904Z</updated>
        <summary type="html"><![CDATA[Visual attention is one of the most significant characteristics for selecting
and understanding the outside redundancy world. The nature of complex scenes
includes enormous redundancy. The human vision system can not process all
information simultaneously because of visual information bottleneck. The human
visual system mainly focuses on dominant parts of the scenes to reduce the
input visual redundancy information. It is commonly known as visual attention
prediction or visual saliency map. This paper proposes a new psychophysical
saliency prediction architecture, WECSF, inspired by human low-level visual
cortex function. The model consists of opponent color channels, wavelet
transform, wavelet energy map, and contrast sensitivity function for extracting
low-level image features and maximum approximation to the human visual system.
The proposed model is evaluated several datasets, including MIT1003, MIT300,
TORONTO, SID4VAM and UCF Sports dataset to explain its efficiency. We also
quantitatively and qualitatively compared the performance of saliency
prediction with other state-of-the-art models. Our model achieved very stable
and good performance. Second, we also confirmed that Fourier and
spectral-inspired saliency prediction models achieved outperformance compared
to other start-of-the-art non-neural networks and even deep neural network
models on psychophysical synthesis images. Finally, the proposed model also can
be applied to spatial-temporal saliency prediction and got better performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qiang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compressing Deep CNNs using Basis Representation and Spectral Fine-tuning. (arXiv:2105.10436v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10436</id>
        <link href="http://arxiv.org/abs/2105.10436"/>
        <updated>2021-05-24T05:08:42.897Z</updated>
        <summary type="html"><![CDATA[We propose an efficient and straightforward method for compressing deep
convolutional neural networks (CNNs) that uses basis filters to represent the
convolutional layers, and optimizes the performance of the compressed network
directly in the basis space. Specifically, any spatial convolution layer of the
CNN can be replaced by two successive convolution layers: the first is a set of
three-dimensional orthonormal basis filters, followed by a layer of
one-dimensional filters that represents the original spatial filters in the
basis space. We jointly fine-tune both the basis and the filter representation
to directly mitigate any performance loss due to the truncation. Generality of
the proposed approach is demonstrated by applying it to several well known deep
CNN architectures and data sets for image classification and object detection.
We also present the execution time and power usage at different compression
levels on the Xavier Jetson AGX processor.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tayyab_M/0/1/0/all/0/1"&gt;Muhammad Tayyab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1"&gt;Fahad Ahmad Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahalanobis_A/0/1/0/all/0/1"&gt;Abhijit Mahalanobis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Realization of Augmented Intelligence in Dermatology: Advances and Future Directions. (arXiv:2105.10477v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10477</id>
        <link href="http://arxiv.org/abs/2105.10477"/>
        <updated>2021-05-24T05:08:42.891Z</updated>
        <summary type="html"><![CDATA[Artificial intelligence (AI) algorithms using deep learning have advanced the
classification of skin disease images; however these algorithms have been
mostly applied "in silico" and not validated clinically. Most dermatology AI
algorithms perform binary classification tasks (e.g. malignancy versus benign
lesions), but this task is not representative of dermatologists' diagnostic
range. The American Academy of Dermatology Task Force on Augmented Intelligence
published a position statement emphasizing the importance of clinical
validation to create human-computer synergy, termed augmented intelligence
(AuI). Liu et al's recent paper, "A deep learning system for differential
diagnosis of skin diseases" represents a significant advancement of AI in
dermatology, bringing it closer to clinical impact. However, significant issues
must be addressed before this algorithm can be integrated into clinical
workflow. These issues include accurate and equitable model development,
defining and assessing appropriate clinical outcomes, and real-world
integration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Daneshjou_R/0/1/0/all/0/1"&gt;Roxana Daneshjou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kovarik_C/0/1/0/all/0/1"&gt;Carrie Kovarik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1"&gt;Justin M Ko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Visual Learning by Variable Playback Speeds Prediction of a Video. (arXiv:2003.02692v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.02692</id>
        <link href="http://arxiv.org/abs/2003.02692"/>
        <updated>2021-05-24T05:08:42.873Z</updated>
        <summary type="html"><![CDATA[We propose a self-supervised visual learning method by predicting the
variable playback speeds of a video. Without semantic labels, we learn the
spatio-temporal visual representation of the video by leveraging the variations
in the visual appearance according to different playback speeds under the
assumption of temporal coherence. To learn the spatio-temporal visual
variations in the entire video, we have not only predicted a single playback
speed but also generated clips of various playback speeds and directions with
randomized starting points. Hence the visual representation can be successfully
learned from the meta information (playback speeds and directions) of the
video. We also propose a new layer dependable temporal group normalization
method that can be applied to 3D convolutional networks to improve the
representation learning performance where we divide the temporal features into
several groups and normalize each one using the different corresponding
parameters. We validate the effectiveness of our method by fine-tuning it to
the action recognition and video retrieval tasks on UCF-101 and HMDB-51.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1"&gt;Hyeon Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1"&gt;Taehoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Hyung Jin Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1"&gt;Wonjun Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalisable and distinctive 3D local deep descriptors for point cloud registration. (arXiv:2105.10382v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10382</id>
        <link href="http://arxiv.org/abs/2105.10382"/>
        <updated>2021-05-24T05:08:42.867Z</updated>
        <summary type="html"><![CDATA[An effective 3D descriptor should be invariant to different geometric
transformations, such as scale and rotation, repeatable in the case of
occlusions and clutter, and generalisable in different contexts when data is
captured with different sensors. We present a simple but yet effective method
to learn generalisable and distinctive 3D local descriptors that can be used to
register point clouds captured in different contexts with different sensors.
Point cloud patches are extracted, canonicalised with respect to their local
reference frame, and encoded into scale and rotation-invariant compact
descriptors by a point permutation-invariant deep neural network. Our
descriptors can effectively generalise across sensor modalities from locally
and randomly sampled points. We evaluate and compare our descriptors with
alternative handcrafted and deep learning-based descriptors on several indoor
and outdoor datasets reconstructed using both RGBD sensors and laser scanners.
Our descriptors outperform most recent descriptors by a large margin in terms
of generalisation, and become the state of the art also in benchmarks where
training and testing are performed in the same scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Poiesi_F/0/1/0/all/0/1"&gt;Fabio Poiesi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boscaini_D/0/1/0/all/0/1"&gt;Davide Boscaini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Consistency Regularized Mean Teacher for Semi-supervised 3D Left Atrium Segmentation. (arXiv:2105.10369v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10369</id>
        <link href="http://arxiv.org/abs/2105.10369"/>
        <updated>2021-05-24T05:08:42.861Z</updated>
        <summary type="html"><![CDATA[Deep learning has achieved promising segmentation performance on 3D left
atrium MR images. However, annotations for segmentation tasks are expensive,
costly and difficult to obtain. In this paper, we introduce a novel
hierarchical consistency regularized mean teacher framework for 3D left atrium
segmentation. In each iteration, the student model is optimized by multi-scale
deep supervision and hierarchical consistency regularization, concurrently.
Extensive experiments have shown that our method achieves competitive
performance as compared with full annotation, outperforming other
stateof-the-art semi-supervised segmentation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shumeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Ziyuan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kaixin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1"&gt;Zeng Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1"&gt;Cuntai Guan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-driven Weight Initialization with Sylvester Solvers. (arXiv:2105.10335v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2105.10335</id>
        <link href="http://arxiv.org/abs/2105.10335"/>
        <updated>2021-05-24T05:08:42.855Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a data-driven scheme to initialize the parameters of
a deep neural network. This is in contrast to traditional approaches which
randomly initialize parameters by sampling from transformed standard
distributions. Such methods do not use the training data to produce a more
informed initialization. Our method uses a sequential layer-wise approach where
each layer is initialized using its input activations. The initialization is
cast as an optimization problem where we minimize a combination of encoding and
decoding losses of the input activations, which is further constrained by a
user-defined latent code. The optimization problem is then restructured into
the well-known Sylvester equation, which has fast and efficient gradient-free
solutions. Our data-driven method achieves a boost in performance compared to
random initialization methods, both before start of training and after training
is over. We show that our proposed method is especially effective in few-shot
and fine-tuning settings. We conclude this paper with analyses on time
complexity and the effect of different latent codes on the recognition
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1"&gt;Debasmit Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhalgat_Y/0/1/0/all/0/1"&gt;Yash Bhalgat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1"&gt;Fatih Porikli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MedMNIST Classification Decathlon: A Lightweight AutoML Benchmark for Medical Image Analysis. (arXiv:2010.14925v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14925</id>
        <link href="http://arxiv.org/abs/2010.14925"/>
        <updated>2021-05-24T05:08:42.839Z</updated>
        <summary type="html"><![CDATA[We present MedMNIST, a collection of 10 pre-processed medical open datasets.
MedMNIST is standardized to perform classification tasks on lightweight 28x28
images, which requires no background knowledge. Covering the primary data
modalities in medical image analysis, it is diverse on data scale (from 100 to
100,000) and tasks (binary/multi-class, ordinal regression and multi-label).
MedMNIST could be used for educational purpose, rapid prototyping, multi-modal
machine learning or AutoML in medical image analysis. Moreover, MedMNIST
Classification Decathlon is designed to benchmark AutoML algorithms on all 10
datasets; We have compared several baseline methods, including open-source or
commercial AutoML tools. The datasets, evaluation code and baseline methods for
MedMNIST are publicly available at https://medmnist.github.io/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jiancheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1"&gt;Rui Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1"&gt;Bingbing Ni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Fairness of Generative Adversarial Networks (GANs). (arXiv:2103.00950v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00950</id>
        <link href="http://arxiv.org/abs/2103.00950"/>
        <updated>2021-05-24T05:08:42.833Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) are one of the greatest advances in AI
in recent years. With their ability to directly learn the probability
distribution of data, and then sample synthetic realistic data. Many
applications have emerged, using GANs to solve classical problems in machine
learning, such as data augmentation, class unbalance problems, and fair
representation learning. In this paper, we analyze and highlight fairness
concerns of GANs model. In this regard, we show empirically that GANs models
may inherently prefer certain groups during the training process and therefore
they're not able to homogeneously generate data from different groups during
the testing phase. Furthermore, we propose solutions to solve this issue by
conditioning the GAN model towards samples' group or using ensemble method
(boosting) to allow the GAN model to leverage distributed structure of data
during the training phase and generate groups at equal rate during the testing
phase.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kenfack_P/0/1/0/all/0/1"&gt;Patrik Joslin Kenfack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arapov_D/0/1/0/all/0/1"&gt;Daniil Dmitrievich Arapov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hussain_R/0/1/0/all/0/1"&gt;Rasheed Hussain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kazmi_S/0/1/0/all/0/1"&gt;S.M. Ahsan Kazmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1"&gt;Adil Mehmood Khan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeLF: Practical Novel View Synthesis with Neural Light Field. (arXiv:2105.07112v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07112</id>
        <link href="http://arxiv.org/abs/2105.07112"/>
        <updated>2021-05-24T05:08:42.820Z</updated>
        <summary type="html"><![CDATA[In this paper, we present an efficient and robust deep learning solution for
novel view synthesis of complex scenes. In our approach, a 3D scene is
represented as a light field, i.e., a set of rays, each of which has a
corresponding color when reaching the image plane. For efficient novel view
rendering, we adopt a 4D parameterization of the light field, where each ray is
characterized by a 4D parameter. We then formulate the light field as a 4D
function that maps 4D coordinates to corresponding color values. We train a
deep fully connected network to optimize this implicit function and memorize
the 3D scene. Then, the scene-specific model is used to synthesize novel views.
Different from previous light field approaches which require dense view
sampling to reliably render novel views, our method can render novel views by
sampling rays and querying the color for each ray from the network directly,
thus enabling high-quality light field rendering with a sparser set of training
images. Our method achieves state-of-the-art novel view synthesis results while
maintaining an interactive frame rate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Celong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Junsong Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yi Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis of voxel-based 3D object detection methods efficiency for real-time embedded systems. (arXiv:2105.10316v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10316</id>
        <link href="http://arxiv.org/abs/2105.10316"/>
        <updated>2021-05-24T05:08:42.764Z</updated>
        <summary type="html"><![CDATA[Real-time detection of objects in the 3D scene is one of the tasks an
autonomous agent needs to perform for understanding its surroundings. While
recent Deep Learning-based solutions achieve satisfactory performance, their
high computational cost renders their application in real-life settings in
which computations need to be performed on embedded platforms intractable. In
this paper, we analyze the efficiency of two popular voxel-based 3D object
detection methods providing a good compromise between high performance and
speed based on two aspects, their ability to detect objects located at large
distances from the agent and their ability to operate in real time on embedded
platforms equipped with high-performance GPUs. Our experiments show that these
methods mostly fail to detect distant small objects due to the sparsity of the
input point clouds at large distances. Moreover, models trained on near objects
achieve similar or better performance compared to those trained on all objects
in the scene. This means that the models learn object appearance
representations mostly from near objects. Our findings suggest that a
considerable part of the computations of existing methods is focused on
locations of the scene that do not contribute with successful detection. This
means that the methods can achieve a speed-up of $40$-$60\%$ by restricting
operation to near objects while not sacrificing much in performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oleksiienko_I/0/1/0/all/0/1"&gt;Illia Oleksiienko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1"&gt;Alexandros Iosifidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rotation invariant CNN using scattering transform for image classification. (arXiv:2105.10175v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10175</id>
        <link href="http://arxiv.org/abs/2105.10175"/>
        <updated>2021-05-24T05:08:42.723Z</updated>
        <summary type="html"><![CDATA[Deep convolutional neural networks accuracy is heavily impacted by rotations
of the input data. In this paper, we propose a convolutional predictor that is
invariant to rotations in the input. This architecture is capable of predicting
the angular orientation without angle-annotated data. Furthermore, the
predictor maps continuously the random rotation of the input to a circular
space of the prediction. For this purpose, we use the roto-translation
properties existing in the Scattering Transform Networks with a series of 3D
Convolutions. We validate the results by training with upright and randomly
rotated samples. This allows further applications of this work on fields like
automatic re-orientation of randomly oriented datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Salas_R/0/1/0/all/0/1"&gt;Rosemberg Rodriguez Salas&lt;/a&gt; (LIGM), &lt;a href="http://arxiv.org/find/cs/1/au:+Dokladalova_E/0/1/0/all/0/1"&gt;Eva Dokladalova&lt;/a&gt; (LIGM), &lt;a href="http://arxiv.org/find/cs/1/au:+Dokladal_P/0/1/0/all/0/1"&gt;Petr Dokl&amp;#xe1;dal&lt;/a&gt; (CMM)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anomaly Detection By Autoencoder Based On Weighted Frequency Domain Loss. (arXiv:2105.10214v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10214</id>
        <link href="http://arxiv.org/abs/2105.10214"/>
        <updated>2021-05-24T05:08:42.711Z</updated>
        <summary type="html"><![CDATA[In image anomaly detection, Autoencoders are the popular methods that
reconstruct the input image that might contain anomalies and output a clean
image with no abnormalities. These Autoencoder-based methods usually calculate
the anomaly score from the reconstruction error, the difference between the
input image and the reconstructed image. On the other hand, the accuracy of the
reconstruction is insufficient in many of these methods, so it leads to
degraded accuracy of anomaly detection. To improve the accuracy of the
reconstruction, we consider defining loss function in the frequency domain. In
general, we know that natural images contain many low-frequency components and
few high-frequency components. Hence, to improve the accuracy of the
reconstruction of high-frequency components, we introduce a new loss function
named weighted frequency domain loss(WFDL). WFDL provides a sharper
reconstructed image, which contributes to improving the accuracy of anomaly
detection. In this paper, we show our method's superiority over the
conventional Autoencoder methods by comparing it with AUROC on the MVTec AD
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nakanishi_M/0/1/0/all/0/1"&gt;Masaki Nakanishi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sato_K/0/1/0/all/0/1"&gt;Kazuki Sato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Terada_H/0/1/0/all/0/1"&gt;Hideo Terada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ViPNAS: Efficient Video Pose Estimation via Neural Architecture Search. (arXiv:2105.10154v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10154</id>
        <link href="http://arxiv.org/abs/2105.10154"/>
        <updated>2021-05-24T05:08:42.704Z</updated>
        <summary type="html"><![CDATA[Human pose estimation has achieved significant progress in recent years.
However, most of the recent methods focus on improving accuracy using
complicated models and ignoring real-time efficiency. To achieve a better
trade-off between accuracy and efficiency, we propose a novel neural
architecture search (NAS) method, termed ViPNAS, to search networks in both
spatial and temporal levels for fast online video pose estimation. In the
spatial level, we carefully design the search space with five different
dimensions including network depth, width, kernel size, group number, and
attentions. In the temporal level, we search from a series of temporal feature
fusions to optimize the total accuracy and speed across multiple video frames.
To the best of our knowledge, we are the first to search for the temporal
feature fusion and automatic computation allocation in videos. Extensive
experiments demonstrate the effectiveness of our approach on the challenging
COCO2017 and PoseTrack2018 datasets. Our discovered model family, S-ViPNAS and
T-ViPNAS, achieve significantly higher inference speed (CPU real-time) without
sacrificing the accuracy compared to the previous state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lumin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1"&gt;Yingda Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1"&gt;Sheng Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wentao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1"&gt;Chen Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1"&gt;Ping Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1"&gt;Wanli Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intriguing Properties of Vision Transformers. (arXiv:2105.10497v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10497</id>
        <link href="http://arxiv.org/abs/2105.10497"/>
        <updated>2021-05-24T05:08:42.674Z</updated>
        <summary type="html"><![CDATA[Vision transformers (ViT) have demonstrated impressive performance across
various machine vision problems. These models are based on multi-head
self-attention mechanisms that can flexibly attend to a sequence of image
patches to encode contextual cues. An important question is how such
flexibility in attending image-wide context conditioned on a given patch can
facilitate handling nuisances in natural images e.g., severe occlusions, domain
shifts, spatial permutations, adversarial and natural perturbations. We
systematically study this question via an extensive set of experiments
encompassing three ViT families and comparisons with a high-performing
convolutional neural network (CNN). We show and analyze the following
intriguing properties of ViT: (a) Transformers are highly robust to severe
occlusions, perturbations and domain shifts, e.g., retain as high as 60% top-1
accuracy on ImageNet even after randomly occluding 80% of the image content.
(b) The robust performance to occlusions is not due to a bias towards local
textures, and ViTs are significantly less biased towards textures compared to
CNNs. When properly trained to encode shape-based features, ViTs demonstrate
shape recognition capability comparable to that of human visual system,
previously unmatched in the literature. (c) Using ViTs to encode shape
representation leads to an interesting consequence of accurate semantic
segmentation without pixel-level supervision. (d) Off-the-shelf features from a
single ViT model can be combined to create a feature ensemble, leading to high
accuracy rates across a range of classification datasets in both traditional
and few-shot learning paradigms. We show effective features of ViTs are due to
flexible and dynamic receptive fields possible via the self-attention
mechanism.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1"&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranasinghe_K/0/1/0/all/0/1"&gt;Kanchana Ranasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1"&gt;Salman Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1"&gt;Munawar Hayat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1"&gt;Fahad Shahbaz Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Ming-Hsuan Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DAVOS: Semi-Supervised Video Object Segmentation via Adversarial Domain Adaptation. (arXiv:2105.10201v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10201</id>
        <link href="http://arxiv.org/abs/2105.10201"/>
        <updated>2021-05-24T05:08:42.646Z</updated>
        <summary type="html"><![CDATA[Domain shift has always been one of the primary issues in video object
segmentation (VOS), for which models suffer from degeneration when tested on
unfamiliar datasets. Recently, many online methods have emerged to narrow the
performance gap between training data (source domain) and test data (target
domain) by fine-tuning on annotations of test data which are usually in
shortage. In this paper, we propose a novel method to tackle domain shift by
first introducing adversarial domain adaptation to the VOS task, with
supervised training on the source domain and unsupervised training on the
target domain. By fusing appearance and motion features with a convolution
layer, and by adding supervision onto the motion branch, our model achieves
state-of-the-art performance on DAVIS2016 with 82.6% mean IoU score after
supervised training. Meanwhile, our adversarial domain adaptation strategy
significantly raises the performance of the trained model when applied on
FBMS59 and Youtube-Object, without exploiting extra annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinshuo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhicheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Songyan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1"&gt;Gang Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WeGleNet: A Weakly-Supervised Convolutional Neural Network for the Semantic Segmentation of Gleason Grades in Prostate Histology Images. (arXiv:2105.10445v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10445</id>
        <link href="http://arxiv.org/abs/2105.10445"/>
        <updated>2021-05-24T05:08:42.636Z</updated>
        <summary type="html"><![CDATA[Prostate cancer is one of the main diseases affecting men worldwide. The
Gleason scoring system is the primary diagnostic tool for prostate cancer. This
is obtained via the visual analysis of cancerous patterns in prostate biopsies
performed by expert pathologists, and the aggregation of the main Gleason
grades in a combined score. Computer-aided diagnosis systems allow to reduce
the workload of pathologists and increase the objectivity. Recently, efforts
have been made in the literature to develop algorithms aiming the direct
estimation of the global Gleason score at biopsy/core level with global labels.
However, these algorithms do not cover the accurate localization of the Gleason
patterns into the tissue. In this work, we propose a deep-learning-based system
able to detect local cancerous patterns in the prostate tissue using only the
global-level Gleason score during training. The methodological core of this
work is the proposed weakly-supervised-trained convolutional neural network,
WeGleNet, based on a multi-class segmentation layer after the feature
extraction module, a global-aggregation, and the slicing of the background
class for the model loss estimation during training. We obtained a Cohen's
quadratic kappa (k) of 0.67 for the pixel-level prediction of cancerous
patterns in the validation cohort. We compared the model performance for
semantic segmentation of Gleason grades with supervised state-of-the-art
architectures in the test cohort. We obtained a pixel-level k of 0.61 and a
macro-averaged f1-score of 0.58, at the same level as fully-supervised methods.
Regarding the estimation of the core-level Gleason score, we obtained a k of
0.76 and 0.67 between the model and two different pathologists. WeGleNet is
capable of performing the semantic segmentation of Gleason grades similarly to
fully-supervised methods without requiring pixel-level annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Silva_Rodriguez_J/0/1/0/all/0/1"&gt;Julio Silva-Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Colomer_A/0/1/0/all/0/1"&gt;Adri&amp;#xe1;n Colomer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1"&gt;Valery Naranjo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Direct Simultaneous Multi-Image Registration. (arXiv:2105.10087v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10087</id>
        <link href="http://arxiv.org/abs/2105.10087"/>
        <updated>2021-05-24T05:08:42.626Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel algorithm that registers a collection of
mono-modal 3D images in a simultaneous fashion, named as Direct Simultaneous
Registration (DSR). The algorithm optimizes global poses of local frames
directly based on the intensities of images (without extracting features from
the images). To obtain the optimal result, we start with formulating a Direct
Bundle Adjustment (DBA) problem which jointly optimizes pose parameters of
local frames and intensities of panoramic image. By proving the independence of
the pose from panoramic image in the iterative process, DSR is proposed and
proved to be able to generate the same optimal poses as DBA, but without
optimizing the intensities of the panoramic image. The proposed DSR method is
particularly suitable in mono-modal registration and in the scenarios where
distinct features are not available, such as Transesophageal Echocardiography
(TEE) images. The proposed method is validated via simulated and in-vivo 3D TEE
images. It is shown that the proposed method outperforms conventional
sequential registration method in terms of accuracy and the obtained results
can produce good alignment in in-vivo images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1"&gt;Zhehua Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Liang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shoudong Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1"&gt;Yiting Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1"&gt;Alex Pui-Wai Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AC-CovidNet: Attention Guided Contrastive CNN for Recognition of Covid-19 in Chest X-Ray Images. (arXiv:2105.10239v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10239</id>
        <link href="http://arxiv.org/abs/2105.10239"/>
        <updated>2021-05-24T05:08:42.619Z</updated>
        <summary type="html"><![CDATA[Covid-19 global pandemic continues to devastate health care systems across
the world. In many countries, the 2nd wave is very severe. Economical and rapid
testing, as well as diagnosis, is urgently needed to control the pandemic. At
present, the Covid-19 testing is costly and time-consuming. Chest X-Ray (CXR)
testing can be the fastest, scalable, and non-invasive method. The existing
methods suffer due to the limited CXR samples available from Covid-19. Thus,
inspired by the limitations of the open-source work in this field, we propose
attention guided contrastive CNN architecture (AC-CovidNet) for Covid-19
detection in CXR images. The proposed method learns the robust and
discriminative features with the help of contrastive loss. Moreover, the
proposed method gives more importance to the infected regions as guided by the
attention mechanism. We compute the sensitivity of the proposed method over the
publicly available Covid-19 dataset. It is observed that the proposed
AC-CovidNet exhibits very promising performance as compared to the existing
methods even with limited training data. It can tackle the bottleneck of CXR
Covid-19 datasets being faced by the researchers. The code used in this paper
is released publicly at \url{https://github.com/shivram1987/AC-CovidNet/}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ambati_A/0/1/0/all/0/1"&gt;Anirudh Ambati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dubey_S/0/1/0/all/0/1"&gt;Shiv Ram Dubey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Guidance and Teaching Network for Video Salient Object Detection. (arXiv:2105.10110v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10110</id>
        <link href="http://arxiv.org/abs/2105.10110"/>
        <updated>2021-05-24T05:08:42.612Z</updated>
        <summary type="html"><![CDATA[Owing to the difficulties of mining spatial-temporal cues, the existing
approaches for video salient object detection (VSOD) are limited in
understanding complex and noisy scenarios, and often fail in inferring
prominent objects. To alleviate such shortcomings, we propose a simple yet
efficient architecture, termed Guidance and Teaching Network (GTNet), to
independently distil effective spatial and temporal cues with implicit guidance
and explicit teaching at feature- and decision-level, respectively. To be
specific, we (a) introduce a temporal modulator to implicitly bridge features
from motion into the appearance branch, which is capable of fusing cross-modal
features collaboratively, and (b) utilise motion-guided mask to propagate the
explicit cues during the feature aggregation. This novel learning strategy
achieves satisfactory results via decoupling the complex spatial-temporal cues
and mapping informative cues across different modalities. Extensive experiments
on three challenging benchmarks show that the proposed method can run at ~28
fps on a single TITAN Xp GPU and perform competitively against 14 cutting-edge
baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1"&gt;Ge-Peng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1"&gt;Yu-Cheng Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yuming Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shouyuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1"&gt;Rong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1"&gt;Ge Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Error Resilient Collaborative Intelligence via Low-Rank Tensor Completion. (arXiv:2105.10341v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10341</id>
        <link href="http://arxiv.org/abs/2105.10341"/>
        <updated>2021-05-24T05:08:42.595Z</updated>
        <summary type="html"><![CDATA[In the race to bring Artificial Intelligence (AI) to the edge, collaborative
intelligence has emerged as a promising way to lighten the computation load on
edge devices that run applications based on Deep Neural Networks (DNNs).
Typically, a deep model is split at a certain layer into edge and cloud
sub-models. The deep feature tensor produced by the edge sub-model is
transmitted to the cloud, where the remaining computationally intensive
workload is performed by the cloud sub-model. The communication channel between
the edge and cloud is imperfect, which will result in missing data in the deep
feature tensor received at the cloud side. In this study, we examine the
effectiveness of four low-rank tensor completion methods in recovering missing
data in the deep feature tensor. We consider both sparse tensors, such as those
produced by the VGG16 model, as well as non-sparse tensors, such as those
produced by ResNet34 model. We study tensor completion effectiveness in both
conplexity-constrained and unconstrained scenario.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bragilevsky_L/0/1/0/all/0/1"&gt;Lior Bragilevsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bajic_I/0/1/0/all/0/1"&gt;Ivan V. Baji&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction. (arXiv:2105.10446v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10446</id>
        <link href="http://arxiv.org/abs/2105.10446"/>
        <updated>2021-05-24T05:08:42.589Z</updated>
        <summary type="html"><![CDATA[This work attempts to provide a plausible theoretical framework that aims to
interpret modern deep (convolutional) networks from the principles of data
compression and discriminative representation. We show that for
high-dimensional multi-class data, the optimal linear discriminative
representation maximizes the coding rate difference between the whole dataset
and the average of all the subsets. We show that the basic iterative gradient
ascent scheme for optimizing the rate reduction objective naturally leads to a
multi-layer deep network, named ReduNet, that shares common characteristics of
modern deep networks. The deep layered architectures, linear and nonlinear
operators, and even parameters of the network are all explicitly constructed
layer-by-layer via forward propagation, instead of learned via back
propagation. All components of so-obtained "white-box" network have precise
optimization, statistical, and geometric interpretation. Moreover, all linear
operators of the so-derived network naturally become multi-channel convolutions
when we enforce classification to be rigorously shift-invariant. The derivation
also indicates that such a deep convolution network is significantly more
efficient to construct and learn in the spectral domain. Our preliminary
simulations and experiments clearly verify the effectiveness of both the rate
reduction objective and the associated ReduNet. All code and data are available
at https://github.com/Ma-Lab-Berkeley.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1"&gt;Kwan Ho Ryan Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yaodong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chong You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1"&gt;Haozhi Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wright_J/0/1/0/all/0/1"&gt;John Wright&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yi Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Endmember-Guided Unmixing Network (EGU-Net): A General Deep Learning Framework for Self-Supervised Hyperspectral Unmixing. (arXiv:2105.10194v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10194</id>
        <link href="http://arxiv.org/abs/2105.10194"/>
        <updated>2021-05-24T05:08:42.582Z</updated>
        <summary type="html"><![CDATA[Over the past decades, enormous efforts have been made to improve the
performance of linear or nonlinear mixing models for hyperspectral unmixing,
yet their ability to simultaneously generalize various spectral variabilities
and extract physically meaningful endmembers still remains limited due to the
poor ability in data fitting and reconstruction and the sensitivity to various
spectral variabilities. Inspired by the powerful learning ability of deep
learning, we attempt to develop a general deep learning approach for
hyperspectral unmixing, by fully considering the properties of endmembers
extracted from the hyperspectral imagery, called endmember-guided unmixing
network (EGU-Net). Beyond the alone autoencoder-like architecture, EGU-Net is a
two-stream Siamese deep network, which learns an additional network from the
pure or nearly-pure endmembers to correct the weights of another unmixing
network by sharing network parameters and adding spectrally meaningful
constraints (e.g., non-negativity and sum-to-one) towards a more accurate and
interpretable unmixing solution. Furthermore, the resulting general framework
is not only limited to pixel-wise spectral unmixing but also applicable to
spatial information modeling with convolutional operators for spatial-spectral
unmixing. Experimental results conducted on three different datasets with the
ground-truth of abundance maps corresponding to each material demonstrate the
effectiveness and superiority of the EGU-Net over state-of-the-art unmixing
algorithms. The codes will be available from the website:
https://github.com/danfenghong/IEEE_TNNLS_EGU-Net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hong_D/0/1/0/all/0/1"&gt;Danfeng Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gao_L/0/1/0/all/0/1"&gt;Lianru Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yao_J/0/1/0/all/0/1"&gt;Jing Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yokoya_N/0/1/0/all/0/1"&gt;Naoto Yokoya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chanussot_J/0/1/0/all/0/1"&gt;Jocelyn Chanussot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Heiden_U/0/1/0/all/0/1"&gt;Uta Heiden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bing Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aligning Visual Prototypes with BERT Embeddings for Few-Shot Learning. (arXiv:2105.10195v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10195</id>
        <link href="http://arxiv.org/abs/2105.10195"/>
        <updated>2021-05-24T05:08:42.562Z</updated>
        <summary type="html"><![CDATA[Few-shot learning (FSL) is the task of learning to recognize previously
unseen categories of images from a small number of training examples. This is a
challenging task, as the available examples may not be enough to unambiguously
determine which visual features are most characteristic of the considered
categories. To alleviate this issue, we propose a method that additionally
takes into account the names of the image classes. While the use of class names
has already been explored in previous work, our approach differs in two key
aspects. First, while previous work has aimed to directly predict visual
prototypes from word embeddings, we found that better results can be obtained
by treating visual and text-based prototypes separately. Second, we propose a
simple strategy for learning class name embeddings using the BERT language
model, which we found to substantially outperform the GloVe vectors that were
used in previous work. We furthermore propose a strategy for dealing with the
high dimensionality of these vectors, inspired by models for aligning
cross-lingual word embeddings. We provide experiments on miniImageNet, CUB and
tieredImageNet, showing that our approach consistently improves the
state-of-the-art in metric-based FSL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1"&gt;Kun Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouraoui_Z/0/1/0/all/0/1"&gt;Zied Bouraoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Ping Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jameel_S/0/1/0/all/0/1"&gt;Shoaib Jameel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schockaert_S/0/1/0/all/0/1"&gt;Steven Schockaert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-color balance for color constancy. (arXiv:2105.10228v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10228</id>
        <link href="http://arxiv.org/abs/2105.10228"/>
        <updated>2021-05-24T05:08:42.552Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel multi-color balance adjustment for color
constancy. The proposed method, called "n-color balancing," allows us not only
to perfectly correct n target colors on the basis of corresponding ground truth
colors but also to correct colors other than the n colors. In contrast,
although white-balancing can perfectly adjust white, colors other than white
are not considered in the framework of white-balancing in general. In an
experiment, the proposed multi-color balancing is demonstrated to outperform
both conventional white and multi-color balance adjustments including
Bradford's model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akazawa_T/0/1/0/all/0/1"&gt;Teruaki Akazawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kinoshita_Y/0/1/0/all/0/1"&gt;Yuma Kinoshita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1"&gt;Hitoshi Kiya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compositional Fine-Grained Low-Shot Learning. (arXiv:2105.10438v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10438</id>
        <link href="http://arxiv.org/abs/2105.10438"/>
        <updated>2021-05-24T05:08:42.522Z</updated>
        <summary type="html"><![CDATA[We develop a novel compositional generative model for zero- and few-shot
learning to recognize fine-grained classes with a few or no training samples.
Our key observation is that generating holistic features for fine-grained
classes fails to capture small attribute differences between classes.
Therefore, we propose a feature composition framework that learns to extract
attribute features from training samples and combines them to construct
fine-grained features for rare and unseen classes. Feature composition allows
us to not only selectively compose features of every class from only relevant
training samples, but also obtain diversity among composed features via
changing samples used for the composition. In addition, instead of building
holistic features for classes, we use our attribute features to form dense
representations capable of capturing fine-grained attribute details of classes.
We propose a training scheme that uses a discriminative model to construct
features that are subsequently used to train the model itself. Therefore, we
directly train the discriminative model on the composed features without
learning a separate generative model. We conduct experiments on four popular
datasets of DeepFashion, AWA2, CUB, and SUN, showing the effectiveness of our
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huynh_D/0/1/0/all/0/1"&gt;Dat Huynh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elhamifar_E/0/1/0/all/0/1"&gt;Ehsan Elhamifar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sharing Pain: Using Domain Transfer Between Pain Types for Recognition of Sparse Pain Expressions in Horses. (arXiv:2105.10313v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10313</id>
        <link href="http://arxiv.org/abs/2105.10313"/>
        <updated>2021-05-24T05:08:42.516Z</updated>
        <summary type="html"><![CDATA[Orthopedic disorders are a common cause for euthanasia among horses, which
often could have been avoided with earlier detection. These conditions often
create varying degrees of subtle but long-term pain. It is challenging to train
a visual pain recognition method with video data depicting such pain, since the
resulting pain behavior also is subtle, sparsely appearing, and varying, making
it challenging for even an expert human labeler to provide accurate
ground-truth for the data. We show that transferring features from a dataset of
horses with acute nociceptive pain (where labeling is less ambiguous) can aid
the learning to recognize more complex orthopedic pain. Moreover, we present a
human expert baseline for the problem, as well as an extensive empirical study
of various domain transfer methods and of what is detected by the pain
recognition method trained on acute pain in the orthopedic dataset. Finally,
this is accompanied with a discussion around the challenges posed by real-world
animal behavior datasets and how best practices can be established for similar
fine-grained action recognition tasks. Our code is available at
https://github.com/sofiabroome/painface-recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Broome_S/0/1/0/all/0/1"&gt;Sofia Broom&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ask_K/0/1/0/all/0/1"&gt;Katrina Ask&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rashid_M/0/1/0/all/0/1"&gt;Maheen Rashid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andersen_P/0/1/0/all/0/1"&gt;Pia Haubro Andersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kjellstrom_H/0/1/0/all/0/1"&gt;Hedvig Kjellstr&amp;#xf6;m&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Interpretable Approach to Automated Severity Scoring in Pelvic Trauma. (arXiv:2105.10238v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10238</id>
        <link href="http://arxiv.org/abs/2105.10238"/>
        <updated>2021-05-24T05:08:42.510Z</updated>
        <summary type="html"><![CDATA[Pelvic ring disruptions result from blunt injury mechanisms and are often
found in patients with multi-system trauma. To grade pelvic fracture severity
in trauma victims based on whole-body CT, the Tile AO/OTA classification is
frequently used. Due to the high volume of whole-body trauma CTs generated in
busy trauma centers, an automated approach to Tile classification would provide
substantial value, e.,g., to prioritize the reading queue of the attending
trauma radiologist. In such scenario, an automated method should perform
grading based on a transparent process and based on interpretable features to
enable interaction with human readers and lower their workload by offering
insights from a first automated read of the scan. This paper introduces an
automated yet interpretable pelvic trauma decision support system to assist
radiologists in fracture detection and Tile grade classification. The method
operates similarly to human interpretation of CT scans and first detects
distinct pelvic fractures on CT with high specificity using a Faster-RCNN model
that are then interpreted using a structural causal model based on clinical
best practices to infer an initial Tile grade. The Bayesian causal model and
finally, the object detector are then queried for likely co-occurring fractures
that may have been rejected initially due to the highly specific operating
point of the detector, resulting in an updated list of detected fractures and
corresponding final Tile grade. Our method is transparent in that it provides
finding location and type using the object detector, as well as information on
important counterfactuals that would invalidate the system's recommendation and
achieves an AUC of 83.3%/85.1% for translational/rotational instability.
Despite being designed for human-machine teaming, our approach does not
compromise on performance compared to previous black-box approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zapaishchykova_A/0/1/0/all/0/1"&gt;Anna Zapaishchykova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dreizin_D/0/1/0/all/0/1"&gt;David Dreizin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhaoshuo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jie Ying Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Roohi_S/0/1/0/all/0/1"&gt;Shahrooz Faghih Roohi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Unberath_M/0/1/0/all/0/1"&gt;Mathias Unberath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Driving-Signal Aware Full-Body Avatars. (arXiv:2105.10441v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10441</id>
        <link href="http://arxiv.org/abs/2105.10441"/>
        <updated>2021-05-24T05:08:42.501Z</updated>
        <summary type="html"><![CDATA[We present a learning-based method for building driving-signal aware
full-body avatars. Our model is a conditional variational autoencoder that can
be animated with incomplete driving signals, such as human pose and facial
keypoints, and produces a high-quality representation of human geometry and
view-dependent appearance. The core intuition behind our method is that better
drivability and generalization can be achieved by disentangling the driving
signals and remaining generative factors, which are not available during
animation. To this end, we explicitly account for information deficiency in the
driving signal by introducing a latent space that exclusively captures the
remaining information, thus enabling the imputation of the missing factors
required during full-body animation, while remaining faithful to the driving
signal. We also propose a learnable localized compression for the driving
signal which promotes better generalization, and helps minimize the influence
of global chance-correlations often found in real datasets. For a given driving
signal, the resulting variational model produces a compact space of uncertainty
for missing factors that allows for an imputation strategy best suited to a
particular application. We demonstrate the efficacy of our approach on the
challenging problem of full-body animation for virtual telepresence with
driving signals acquired from minimal sensors placed in the environment and
mounted on a VR-headset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1"&gt;Timur Bagautdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chenglei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simon_T/0/1/0/all/0/1"&gt;Tomas Simon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prada_F/0/1/0/all/0/1"&gt;Fabian Prada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shiratori_T/0/1/0/all/0/1"&gt;Takaaki Shiratori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1"&gt;Shih-En Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Weipeng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheikh_Y/0/1/0/all/0/1"&gt;Yaser Sheikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saragih_J/0/1/0/all/0/1"&gt;Jason Saragih&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IDEAL: Independent Domain Embedding Augmentation Learning. (arXiv:2105.10112v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10112</id>
        <link href="http://arxiv.org/abs/2105.10112"/>
        <updated>2021-05-24T05:08:42.495Z</updated>
        <summary type="html"><![CDATA[Many efforts have been devoted to designing sampling, mining, and weighting
strategies in high-level deep metric learning (DML) loss objectives. However,
little attention has been paid to low-level but essential data transformation.
In this paper, we develop a novel mechanism, the independent domain embedding
augmentation learning ({IDEAL}) method. It can simultaneously learn multiple
independent embedding spaces for multiple domains generated by predefined data
transformations. Our IDEAL is orthogonal to existing DML techniques and can be
seamlessly combined with prior DML approaches for enhanced performance.
Empirical results on visual retrieval tasks demonstrate the superiority of the
proposed method. For example, the IDEAL improves the performance of MS loss by
a large margin, 84.5\% $\rightarrow$ 87.1\% on Cars-196, and 65.8\%
$\rightarrow$ 69.5\% on CUB-200 at Recall$@1$. Our IDEAL with MS loss also
achieves the new state-of-the-art performance on three image retrieval
benchmarks, \ie, \emph{Cars-196}, \emph{CUB-200}, and \emph{SOP}. It
outperforms the most recent DML approaches, such as Circle loss and XBM,
significantly. The source code and pre-trained models of our method will be
available at\emph{\url{https://github.com/emdata-ailab/IDEAL}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhiyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_G/0/1/0/all/0/1"&gt;Guang Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1"&gt;Wennan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sheaves as a Framework for Understanding and Interpreting Model Fit. (arXiv:2105.10414v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10414</id>
        <link href="http://arxiv.org/abs/2105.10414"/>
        <updated>2021-05-24T05:08:42.478Z</updated>
        <summary type="html"><![CDATA[As data grows in size and complexity, finding frameworks which aid in
interpretation and analysis has become critical. This is particularly true when
data comes from complex systems where extensive structure is available, but
must be drawn from peripheral sources. In this paper we argue that in such
situations, sheaves can provide a natural framework to analyze how well a
statistical model fits at the local level (that is, on subsets of related
datapoints) vs the global level (on all the data). The sheaf-based approach
that we propose is suitably general enough to be useful in a range of
applications, from analyzing sensor networks to understanding the feature space
of a deep learning model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1"&gt;Henry Kvinge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jefferson_B/0/1/0/all/0/1"&gt;Brett Jefferson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joslyn_C/0/1/0/all/0/1"&gt;Cliff Joslyn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Purvine_E/0/1/0/all/0/1"&gt;Emilie Purvine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Act Like a Radiologist: Towards Reliable Multi-view Correspondence Reasoning for Mammogram Mass Detection. (arXiv:2105.10160v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10160</id>
        <link href="http://arxiv.org/abs/2105.10160"/>
        <updated>2021-05-24T05:08:42.472Z</updated>
        <summary type="html"><![CDATA[Mammogram mass detection is crucial for diagnosing and preventing the breast
cancers in clinical practice. The complementary effect of multi-view mammogram
images provides valuable information about the breast anatomical prior
structure and is of great significance in digital mammography interpretation.
However, unlike radiologists who can utilize the natural reasoning ability to
identify masses based on multiple mammographic views, how to endow the existing
object detection models with the capability of multi-view reasoning is vital
for decision-making in clinical diagnosis but remains the boundary to explore.
In this paper, we propose an Anatomy-aware Graph convolutional Network (AGN),
which is tailored for mammogram mass detection and endows existing detection
methods with multi-view reasoning ability. The proposed AGN consists of three
steps. Firstly, we introduce a Bipartite Graph convolutional Network (BGN) to
model the intrinsic geometric and semantic relations of ipsilateral views.
Secondly, considering that the visual asymmetry of bilateral views is widely
adopted in clinical practice to assist the diagnosis of breast lesions, we
propose an Inception Graph convolutional Network (IGN) to model the structural
similarities of bilateral views. Finally, based on the constructed graphs, the
multi-view information is propagated through nodes methodically, which equips
the features learned from the examined view with multi-view reasoning ability.
Experiments on two standard benchmarks reveal that AGN significantly exceeds
the state-of-the-art performance. Visualization results show that AGN provides
interpretable visual cues for clinical diagnosis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuhang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fandong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chaoqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Siwen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yizhou Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yizhou Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Human Pose Regression using Graph Convolutional Network. (arXiv:2105.10379v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10379</id>
        <link href="http://arxiv.org/abs/2105.10379"/>
        <updated>2021-05-24T05:08:42.465Z</updated>
        <summary type="html"><![CDATA[3D human pose estimation is a difficult task, due to challenges such as
occluded body parts and ambiguous poses. Graph convolutional networks encode
the structural information of the human skeleton in the form of an adjacency
matrix, which is beneficial for better pose prediction. We propose one such
graph convolutional network named PoseGraphNet for 3D human pose regression
from 2D poses. Our network uses an adaptive adjacency matrix and kernels
specific to neighbor groups. We evaluate our model on the Human3.6M dataset
which is a standard dataset for 3D pose estimation. Our model's performance is
close to the state-of-the-art, but with much fewer parameters. The model learns
interesting adjacency relations between joints that have no physical
connections, but are behaviorally similar.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Banik_S/0/1/0/all/0/1"&gt;Soubarna Banik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gracia_A/0/1/0/all/0/1"&gt;Alejandro Mendoza Gracia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1"&gt;Alois Knoll&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Omni-supervised Point Cloud Segmentation via Gradual Receptive Field Component Reasoning. (arXiv:2105.10203v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10203</id>
        <link href="http://arxiv.org/abs/2105.10203"/>
        <updated>2021-05-24T05:08:42.459Z</updated>
        <summary type="html"><![CDATA[Hidden features in neural network usually fail to learn informative
representation for 3D segmentation as supervisions are only given on output
prediction, while this can be solved by omni-scale supervision on intermediate
layers. In this paper, we bring the first omni-scale supervision method to
point cloud segmentation via the proposed gradual Receptive Field Component
Reasoning (RFCR), where target Receptive Field Component Codes (RFCCs) are
designed to record categories within receptive fields for hidden units in the
encoder. Then, target RFCCs will supervise the decoder to gradually infer the
RFCCs in a coarse-to-fine categories reasoning manner, and finally obtain the
semantic labels. Because many hidden features are inactive with tiny magnitude
and make minor contributions to RFCC prediction, we propose a Feature
Densification with a centrifugal potential to obtain more unambiguous features,
and it is in effect equivalent to entropy regularization over features. More
active features can further unleash the potential of our omni-supervision
method. We embed our method into four prevailing backbones and test on three
challenging benchmarks. Our method can significantly improve the backbones in
all three datasets. Specifically, our method brings new state-of-the-art
performances for S3DIS as well as Semantic3D and ranks the 1st in the ScanNet
benchmark among all the point-based methods. Code will be publicly available at
https://github.com/azuki-miho/RFCR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gong_J/0/1/0/all/0/1"&gt;Jingyu Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jiachen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xin Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1"&gt;Haichuan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1"&gt;Yanyun Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yuan Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lizhuang Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combining Transformer Generators with Convolutional Discriminators. (arXiv:2105.10189v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10189</id>
        <link href="http://arxiv.org/abs/2105.10189"/>
        <updated>2021-05-24T05:08:42.452Z</updated>
        <summary type="html"><![CDATA[Transformer models have recently attracted much interest from computer vision
researchers and have since been successfully employed for several problems
traditionally addressed with convolutional neural networks. At the same time,
image synthesis using generative adversarial networks (GANs) has drastically
improved over the last few years. The recently proposed TransGAN is the first
GAN using only transformer-based architectures and achieves competitive results
when compared to convolutional GANs. However, since transformers are
data-hungry architectures, TransGAN requires data augmentation, an auxiliary
super-resolution task during training, and a masking prior to guide the
self-attention mechanism. In this paper, we study the combination of a
transformer-based generator and convolutional discriminator and successfully
remove the need of the aforementioned required design choices. We evaluate our
approach by conducting a benchmark of well-known CNN discriminators, ablate the
size of the transformer-based generator, and show that combining both
architectural elements into a hybrid model leads to better results.
Furthermore, we investigate the frequency spectrum properties of generated
images and observe that our model retains the benefits of an attention based
generator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Durall_R/0/1/0/all/0/1"&gt;Ricard Durall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frolov_S/0/1/0/all/0/1"&gt;Stanislav Frolov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1"&gt;Andreas Dengel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1"&gt;Janis Keuper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Correlated Input-Dependent Label Noise in Large-Scale Image Classification. (arXiv:2105.10305v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10305</id>
        <link href="http://arxiv.org/abs/2105.10305"/>
        <updated>2021-05-24T05:08:42.430Z</updated>
        <summary type="html"><![CDATA[Large scale image classification datasets often contain noisy labels. We take
a principled probabilistic approach to modelling input-dependent, also known as
heteroscedastic, label noise in these datasets. We place a multivariate Normal
distributed latent variable on the final hidden layer of a neural network
classifier. The covariance matrix of this latent variable, models the aleatoric
uncertainty due to label noise. We demonstrate that the learned covariance
structure captures known sources of label noise between semantically similar
and co-occurring classes. Compared to standard neural network training and
other baselines, we show significantly improved accuracy on Imagenet ILSVRC
2012 79.3% (+2.6%), Imagenet-21k 47.0% (+1.1%) and JFT 64.7% (+1.6%). We set a
new state-of-the-art result on WebVision 1.0 with 76.6% top-1 accuracy. These
datasets range from over 1M to over 300M training examples and from 1k classes
to more than 21k classes. Our method is simple to use, and we provide an
implementation that is a drop-in replacement for the final fully-connected
layer in a deep classifier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Collier_M/0/1/0/all/0/1"&gt;Mark Collier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1"&gt;Basil Mustafa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kokiopoulou_E/0/1/0/all/0/1"&gt;Efi Kokiopoulou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jenatton_R/0/1/0/all/0/1"&gt;Rodolphe Jenatton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berent_J/0/1/0/all/0/1"&gt;Jesse Berent&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Helsinki Deblur Challenge 2021: description of photographic data. (arXiv:2105.10233v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10233</id>
        <link href="http://arxiv.org/abs/2105.10233"/>
        <updated>2021-05-24T05:08:42.415Z</updated>
        <summary type="html"><![CDATA[The photographic dataset collected for the Helsinki Deblur Challenge 2021
(HDC2021) contains pairs of images taken by two identical cameras of the same
target but with different conditions. One camera is always in focus and
produces sharp and low-noise images the other camera produces blurred and noisy
images as it is gradually more and more out of focus and has a higher ISO
setting. Even though the dataset was designed and captured with the HDC2021 in
mind it can be used for any testing and benchmarking of image deblurring
algorithms. The data is available here: https://doi.org/10.5281/zenodo.477228]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Juvonen_M/0/1/0/all/0/1"&gt;Markus Juvonen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Siltanen_S/0/1/0/all/0/1"&gt;Samuli Siltanen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moura_F/0/1/0/all/0/1"&gt;Fernando Silva de Moura&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient Training Approach for Very Large Scale Face Recognition. (arXiv:2105.10375v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10375</id>
        <link href="http://arxiv.org/abs/2105.10375"/>
        <updated>2021-05-24T05:08:42.409Z</updated>
        <summary type="html"><![CDATA[Face recognition has achieved significant progress in deep-learning era due
to the ultra-large-scale and well-labeled datasets. However, training on
ultra-large-scale datasets is time-consuming and takes up a lot of hardware
resource. Therefore, how to design an appropriate training approach is very
crucial and indispensable. The computational and hardware cost of training
ultra-large-scale datasets mainly focuses on the Fully-Connected (FC) layer
rather than convolutional layers. To this end, we propose a novel training
approach for ultra-large-scale face datasets, termed Faster Face Classification
(F$^2$C). In F$^2$C, we first define a Gallery Net and a Probe Net that are
used to generate identities' centers and extract faces' features for face
recognition, respectively. Gallery Net has the same structure as Probe Net and
inherits the parameters from Probe Net with a moving average paradigm. After
that, to reduce the training time and hardware resource occupancy of the FC
layer, we propose the Dynamic Class Pool that stores the features from Gallery
Net and calculates the inner product (logits) with positive samples (its
identities appear in Dynamic Class Pool) in each mini-batch. Dynamic Class Pool
can be regarded as a substitute for the FC layer and its size is much smaller
than FC, which is the reason why Dynamic Class Pool can largely reduce the time
and resource cost. For negative samples (its identities are not appear in the
Dynamic Class Pool), we minimize the cosine similarities between negative
samples and Dynamic Class Pool. Then, to improve the update efficiency and
speed of Dynamic Class Pool's parameters, we design the Dual Loaders including
Identity-based and Instance-based Loaders. Dual Loaders load images from given
dataset by instances and identities to generate batches for training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhipeng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaobo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1"&gt;Xiaojiang Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1"&gt;Baigui Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1"&gt;Yang You&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extremely Lightweight Quantization Robust Real-Time Single-Image Super Resolution for Mobile Devices. (arXiv:2105.10288v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10288</id>
        <link href="http://arxiv.org/abs/2105.10288"/>
        <updated>2021-05-24T05:08:42.383Z</updated>
        <summary type="html"><![CDATA[Single-Image Super Resolution (SISR) is a classical computer vision problem
and it has been studied for over decades. With the recent success of deep
learning methods, recent work on SISR focuses solutions with deep learning
methodologies and achieves state-of-the-art results. However most of the
state-of-the-art SISR methods contain millions of parameters and layers, which
limits their practical applications. In this paper, we propose a hardware
(Synaptics Dolphin NPU) limitation aware, extremely lightweight quantization
robust real-time super resolution network (XLSR). The proposed model's building
block is inspired from root modules for Image classification. We successfully
applied root modules to SISR problem, further more to make the model uint8
quantization robust we used Clipped ReLU at the last layer of the network and
achieved great balance between reconstruction quality and runtime. Furthermore,
although the proposed network contains 30x fewer parameters than VDSR its
performance surpasses it on Div2K validation set. The network proved itself by
winning Mobile AI 2021 Real-Time Single Image Super Resolution Challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ayazoglu_M/0/1/0/all/0/1"&gt;Mustafa Ayazoglu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Task, Multi-Domain Deep Segmentation with Shared Representations and Contrastive Regularization for Sparse Pediatric Datasets. (arXiv:2105.10310v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10310</id>
        <link href="http://arxiv.org/abs/2105.10310"/>
        <updated>2021-05-24T05:08:42.370Z</updated>
        <summary type="html"><![CDATA[Automatic segmentation of magnetic resonance (MR) images is crucial for
morphological evaluation of the pediatric musculoskeletal system in clinical
practice. However, the accuracy and generalization performance of individual
segmentation models are limited due to the restricted amount of annotated
pediatric data. Hence, we propose to train a segmentation model on multiple
datasets, arising from different parts of the anatomy, in a multi-task and
multi-domain learning framework. This approach allows to overcome the inherent
scarcity of pediatric data while benefiting from a more robust shared
representation. The proposed segmentation network comprises shared
convolutional filters, domain-specific batch normalization parameters that
compute the respective dataset statistics and a domain-specific segmentation
layer. Furthermore, a supervised contrastive regularization is integrated to
further improve generalization capabilities, by promoting intra-domain
similarity and impose inter-domain margins in embedded space. We evaluate our
contributions on two pediatric imaging datasets of the ankle and shoulder
joints for bone segmentation. Results demonstrate that the proposed model
outperforms state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boutillon_A/0/1/0/all/0/1"&gt;Arnaud Boutillon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Conze_P/0/1/0/all/0/1"&gt;Pierre-Henri Conze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pons_C/0/1/0/all/0/1"&gt;Christelle Pons&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burdin_V/0/1/0/all/0/1"&gt;Val&amp;#xe9;rie Burdin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borotikar_B/0/1/0/all/0/1"&gt;Bhushan Borotikar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Visible Connectivity Dynamics for Cloth Smoothing. (arXiv:2105.10389v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.10389</id>
        <link href="http://arxiv.org/abs/2105.10389"/>
        <updated>2021-05-24T05:08:42.352Z</updated>
        <summary type="html"><![CDATA[Robotic manipulation of cloth remains challenging for robotics due to the
complex dynamics of the cloth, lack of a low-dimensional state representation,
and self-occlusions. In contrast to previous model-based approaches that learn
a pixel-based dynamics model or a compressed latent vector dynamics, we propose
to learn a particle-based dynamics model from a partial point cloud
observation. To overcome the challenges of partial observability, we infer
which visible points are connected on the underlying cloth mesh. We then learn
a dynamics model over this visible connectivity graph. Compared to previous
learning-based approaches, our model poses strong inductive bias with its
particle based representation for learning the underlying cloth physics; it is
invariant to visual features; and the predictions can be more easily
visualized. We show that our method greatly outperforms previous
state-of-the-art model-based and model-free reinforcement learning methods in
simulation. Furthermore, we demonstrate zero-shot sim-to-real transfer where we
deploy the model trained in simulation on a Franka arm and show that the model
can successfully smooth different types of cloth from crumpled configurations.
Videos can be found on our project website.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xingyu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yufei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1"&gt;David Held&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anomaly Detection of Test-Time Evasion Attacks using Class-conditional Generative Adversarial Networks. (arXiv:2105.10101v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10101</id>
        <link href="http://arxiv.org/abs/2105.10101"/>
        <updated>2021-05-24T05:08:42.346Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) have been shown vulnerable to adversarial
(Test-Time Evasion (TTE)) attacks which, by making small changes to the input,
alter the DNN's decision. We propose an attack detector based on
class-conditional Generative Adversarial Networks (GANs). We model the
distribution of clean data conditioned on the predicted class label by an
Auxiliary Classifier GAN (ACGAN). Given a test sample and its predicted class,
three detection statistics are calculated using the ACGAN Generator and
Discriminator. Experiments on image classification datasets under different TTE
attack methods show that our method outperforms state-of-the-art detection
methods. We also investigate the effectiveness of anomaly detection using
different DNN layers (input features or internal-layer features) and
demonstrate that anomalies are harder to detect using features closer to the
DNN's output layer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miller_D/0/1/0/all/0/1"&gt;David J. Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kesidis_G/0/1/0/all/0/1"&gt;George Kesidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Memory Implementations of Ridge Solutions for Broad Learning System with Incremental Learning. (arXiv:2105.10424v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10424</id>
        <link href="http://arxiv.org/abs/2105.10424"/>
        <updated>2021-05-24T05:08:42.339Z</updated>
        <summary type="html"><![CDATA[The existing low-memory BLS implementation proposed recently avoids the need
for storing and inverting large matrices, to achieve efficient usage of
memories. However, the existing low-memory BLS implementation sacrifices the
testing accuracy as a price for efficient usage of memories, since it can no
longer obtain the generalized inverse or ridge solution for the output weights
during incremental learning, and it cannot work under the very small ridge
parameter that is utilized in the original BLS. Accordingly, it is required to
develop the low-memory BLS implementations, which can work under very small
ridge parameters and compute the generalized inverse or ridge solution for the
output weights in the process of incremental learning. In this paper, firstly
we propose the low-memory implementations for the recently proposed recursive
and square-root BLS algorithms on added inputs and the recently proposed
squareroot BLS algorithm on added nodes, by simply processing a batch of inputs
or nodes in each recursion. Since the recursive BLS implementation includes the
recursive updates of the inverse matrix that may introduce numerical
instabilities after a large number of iterations, and needs the extra
computational load to decompose the inverse matrix into the Cholesky factor
when cooperating with the proposed low-memory implementation of the square-root
BLS algorithm on added nodes, we only improve the low-memory implementations of
the square-root BLS algorithms on added inputs and nodes, to propose the full
lowmemory implementation of the square-root BLS algorithm. All the proposed
low-memory BLS implementations compute the ridge solution for the output
weights in the process of incremental learning, and most of them can work under
very small ridge parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hufei Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latent Gaussian Model Boosting. (arXiv:2105.08966v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08966</id>
        <link href="http://arxiv.org/abs/2105.08966"/>
        <updated>2021-05-24T05:08:42.333Z</updated>
        <summary type="html"><![CDATA[Latent Gaussian models and boosting are widely used techniques in statistics
and machine learning. Tree-boosting shows excellent predictive accuracy on many
data sets, but potential drawbacks are that it assumes conditional independence
of samples, produces discontinuous predictions for, e.g., spatial data, and it
can have difficulty with high-cardinality categorical variables. Latent
Gaussian models, such as Gaussian process and grouped random effects models,
are flexible prior models that allow for making probabilistic predictions.
However, existing latent Gaussian models usually assume either a zero or a
linear prior mean function which can be an unrealistic assumption. This article
introduces a novel approach that combines boosting and latent Gaussian models
in order to remedy the above-mentioned drawbacks and to leverage the advantages
of both techniques. We obtain increased predictive accuracy compared to
existing approaches in both simulated and real-world data experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sigrist_F/0/1/0/all/0/1"&gt;Fabio Sigrist&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond permutation equivariance in graph networks. (arXiv:2103.14066v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14066</id>
        <link href="http://arxiv.org/abs/2103.14066"/>
        <updated>2021-05-24T05:08:42.327Z</updated>
        <summary type="html"><![CDATA[In this draft paper, we introduce a novel architecture for graph networks
which is equivariant to the Euclidean group in $n$-dimensions. The model is
designed to work with graph networks in their general form and can be shown to
include particular variants as special cases. Thanks to its equivariance
properties, we expect the proposed model to be more data efficient with respect
to classical graph architectures and also intrinsically equipped with a better
inductive bias. We defer investigating this matter to future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Slade_E/0/1/0/all/0/1"&gt;Emma Slade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farina_F/0/1/0/all/0/1"&gt;Francesco Farina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VisualSparta: An Embarrassingly Simple Approach to Large-scale Text-to-Image Search with Weighted Bag-of-words. (arXiv:2101.00265v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00265</id>
        <link href="http://arxiv.org/abs/2101.00265"/>
        <updated>2021-05-24T05:08:42.310Z</updated>
        <summary type="html"><![CDATA[Text-to-image retrieval is an essential task in cross-modal information
retrieval, i.e., retrieving relevant images from a large and unlabelled dataset
given textual queries. In this paper, we propose VisualSparta, a novel
(Visual-text Sparse Transformer Matching) model that shows significant
improvement in terms of both accuracy and efficiency. VisualSparta is capable
of outperforming previous state-of-the-art scalable methods in MSCOCO and
Flickr30K. We also show that it achieves substantial retrieving speed
advantages, i.e., for a 1 million image index, VisualSparta using CPU gets
~391X speedup compared to CPU vector search and ~5.4X speedup compared to
vector search with GPU acceleration. Experiments show that this speed advantage
even gets bigger for larger datasets because VisualSparta can be efficiently
implemented as an inverted index. To the best of our knowledge, VisualSparta is
the first transformer-based text-to-image retrieval model that can achieve
real-time searching for large-scale datasets, with significant accuracy
improvement compared to previous state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaopeng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tiancheng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kyusong Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BOTD: Bold Outline Text Detector. (arXiv:2011.14714v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14714</id>
        <link href="http://arxiv.org/abs/2011.14714"/>
        <updated>2021-05-24T05:08:42.278Z</updated>
        <summary type="html"><![CDATA[Recently, text detection has attracted sufficient attention in the field of
computer vision and artificial intelligence. Among the existing approaches,
regression-based models are limited to handle the texts with arbitrary shapes,
while segmentation-based algorithms have high computational costs and suffer
from the text adhesion problem. In this paper, we propose a new one-stage text
detector, termed as Bold Outline Text Detector (BOTD), which is able to process
the arbitrary-shaped text with low model complexity. Different from previous
works, BOTD utilizes the Polar Minimum Distance (PMD) to encode the shortest
distance between the center point and the contour of the text instance, and
generates a Center Mask (CM) for each text instance. After learning the PMD
heat map and CM map, the final results can be obtained with a simple Text
Reconstruction Module (TRM). Since the CM resides within the text box exactly,
the text adhesion problem is avoided naturally. Meanwhile, all the points on
the text contour share the same PMD, so the complexity of BOTD is much lower
than existing segmentation-based methods. Experimental results on three
real-world benchmarks show the state-of-the-art performance of BOTD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chuang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1"&gt;Zhitong Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mulin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuelong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[e-ACJ: Accurate Junction Extraction For Event Cameras. (arXiv:2101.11251v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11251</id>
        <link href="http://arxiv.org/abs/2101.11251"/>
        <updated>2021-05-24T05:08:42.266Z</updated>
        <summary type="html"><![CDATA[Junctions reflect the important geometrical structure information of the
image, and are of primary significance to applications such as image matching
and motion analysis. Previous event-based feature extraction methods are mainly
focused on corners, which mainly find their locations, however, ignoring the
geometrical structure information like orientations and scales of edges. This
paper adapts the frame-based a-contrario junction detector(ACJ) to event data,
proposing the event-based a-contrario junction detector(e-ACJ), which yields
junctions' locations while giving the scales and orientations of their
branches. The proposed method relies on an a-contrario model and can operate on
asynchronous events directly without generating synthesized event frames. We
evaluate the performance on public event datasets. The result shows our method
successfully finds the orientations and scales of branches, while maintaining
high accuracy in junction's location.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhihao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yuqian Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High Fidelity Fingerprint Generation: Quality, Uniqueness, and Privacy. (arXiv:2105.10403v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10403</id>
        <link href="http://arxiv.org/abs/2105.10403"/>
        <updated>2021-05-24T05:08:42.259Z</updated>
        <summary type="html"><![CDATA[In this work, we utilize progressive growth-based Generative Adversarial
Networks (GANs) to develop the Clarkson Fingerprint Generator (CFG). We
demonstrate that the CFG is capable of generating realistic, high fidelity,
$512\times512$ pixels, full, plain impression fingerprints. Our results suggest
that the fingerprints generated by the CFG are unique, diverse, and resemble
the training dataset in terms of minutiae configuration and quality, while not
revealing the underlying identities of the training data. We make the
pre-trained CFG model and the synthetically generated dataset publicly
available at https://github.com/keivanB/Clarkson_Finger_Gen]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bahmani_K/0/1/0/all/0/1"&gt;Keivan Bahmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plesh_R/0/1/0/all/0/1"&gt;Richard Plesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johnson_P/0/1/0/all/0/1"&gt;Peter Johnson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schuckers_S/0/1/0/all/0/1"&gt;Stephanie Schuckers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swyka_T/0/1/0/all/0/1"&gt;Timothy Swyka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Remote Sensing Benchmark Datasets for Land Cover Classification with A Shared and Specific Feature Learning Model. (arXiv:2105.10196v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10196</id>
        <link href="http://arxiv.org/abs/2105.10196"/>
        <updated>2021-05-24T05:08:42.253Z</updated>
        <summary type="html"><![CDATA[As remote sensing (RS) data obtained from different sensors become available
largely and openly, multimodal data processing and analysis techniques have
been garnering increasing interest in the RS and geoscience community. However,
due to the gap between different modalities in terms of imaging sensors,
resolutions, and contents, embedding their complementary information into a
consistent, compact, accurate, and discriminative representation, to a great
extent, remains challenging. To this end, we propose a shared and specific
feature learning (S2FL) model. S2FL is capable of decomposing multimodal RS
data into modality-shared and modality-specific components, enabling the
information blending of multi-modalities more effectively, particularly for
heterogeneous data sources. Moreover, to better assess multimodal baselines and
the newly-proposed S2FL model, three multimodal RS benchmark datasets, i.e.,
Houston2013 -- hyperspectral and multispectral data, Berlin -- hyperspectral
and synthetic aperture radar (SAR) data, Augsburg -- hyperspectral, SAR, and
digital surface model (DSM) data, are released and used for land cover
classification. Extensive experiments conducted on the three datasets
demonstrate the superiority and advancement of our S2FL model in the task of
land cover classification in comparison with previously-proposed
state-of-the-art baselines. Furthermore, the baseline codes and datasets used
in this paper will be made available freely at
https://github.com/danfenghong/ISPRS_S2FL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1"&gt;Danfeng Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jingliang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1"&gt;Jing Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1"&gt;Jocelyn Chanussot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiao Xiang Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Swimmer Stroke Rate Estimation From Overhead Race Video. (arXiv:2104.12056v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12056</id>
        <link href="http://arxiv.org/abs/2104.12056"/>
        <updated>2021-05-24T05:08:42.130Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a swimming analytics system for automatically
determining swimmer stroke rates from overhead race video (ORV). General ORV is
defined as any footage of swimmers in competition, taken for the purposes of
viewing or analysis. Examples of this are footage from live streams,
broadcasts, or specialized camera equipment, with or without camera motion.
These are the most typical forms of swimming competition footage. We detail how
to create a system that will automatically collect swimmer stroke rates in any
competition, given the video of the competition of interest. With this
information, better systems can be created and additions to our analytics
system can be proposed to automatically extract other swimming metrics of
interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Woinoski_T/0/1/0/all/0/1"&gt;Timothy Woinoski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bajic_I/0/1/0/all/0/1"&gt;Ivan V. Baji&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pyramid Fusion Dark Channel Prior for Single Image Dehazing. (arXiv:2105.10192v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10192</id>
        <link href="http://arxiv.org/abs/2105.10192"/>
        <updated>2021-05-24T05:08:42.112Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose the pyramid fusion dark channel prior (PF-DCP) for
single image dehazing. Based on the well-known Dark Channel Prior (DCP), we
introduce an easy yet effective approach PF-DCP by employing the DCP algorithm
at a pyramid of multi-scale images to alleviate the problem of patch size
selection. In this case, we obtain the final transmission map by fusing
transmission maps at each level to recover a high-quality haze-free image.
Experiments on RESIDE SOTS show that PF-DCP not only outperforms the
traditional prior-based methods with a large margin but also achieves
comparable or even better results of state-of-art deep learning approaches.
Furthermore, the visual quality is also greatly improved with much fewer color
distortions and halo artifacts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Q/0/1/0/all/0/1"&gt;Qiyuan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1"&gt;Bin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1"&gt;Chong-Wah Ngo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Convolutional Networks in Feature Space for Image Deblurring and Super-resolution. (arXiv:2105.10465v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10465</id>
        <link href="http://arxiv.org/abs/2105.10465"/>
        <updated>2021-05-24T05:08:42.089Z</updated>
        <summary type="html"><![CDATA[Graph convolutional networks (GCNs) have achieved great success in dealing
with data of non-Euclidean structures. Their success directly attributes to
fitting graph structures effectively to data such as in social media and
knowledge databases. For image processing applications, the use of graph
structures and GCNs have not been fully explored. In this paper, we propose a
novel encoder-decoder network with added graph convolutions by converting
feature maps to vertexes of a pre-generated graph to synthetically construct
graph-structured data. By doing this, we inexplicitly apply graph Laplacian
regularization to the feature maps, making them more structured. The
experiments show that it significantly boosts performance for image restoration
tasks, including deblurring and super-resolution. We believe it opens up
opportunities for GCN-based approaches in more applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1"&gt;Boyan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Hujun Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey of Data Augmentation Approaches for NLP. (arXiv:2105.03075v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03075</id>
        <link href="http://arxiv.org/abs/2105.03075"/>
        <updated>2021-05-24T05:08:42.075Z</updated>
        <summary type="html"><![CDATA[Data augmentation has recently seen increased interest in NLP due to more
work in low-resource domains, new tasks, and the popularity of large-scale
neural networks that require large amounts of training data. Despite this
recent upsurge, this area is still relatively underexplored, perhaps due to the
challenges posed by the discrete nature of language data. In this paper, we
present a comprehensive and unifying survey of data augmentation for NLP by
summarizing the literature in a structured manner. We first introduce and
motivate data augmentation for NLP, and then discuss major methodologically
representative approaches. Next, we highlight techniques that are used for
popular NLP applications and tasks. We conclude by outlining current challenges
and directions for future research. Overall, our paper aims to clarify the
landscape of existing literature in data augmentation for NLP and motivate
additional work in this area. We also present a GitHub repository with a paper
list that will be continuously updated at
https://github.com/styfeng/DataAug4NLP]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1"&gt;Steven Y. Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1"&gt;Varun Gangal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jason Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1"&gt;Sarath Chandar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1"&gt;Soroush Vosoughi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1"&gt;Teruko Mitamura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1"&gt;Eduard Hovy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elliptical Ordinal Embedding. (arXiv:2105.10457v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10457</id>
        <link href="http://arxiv.org/abs/2105.10457"/>
        <updated>2021-05-24T05:08:42.060Z</updated>
        <summary type="html"><![CDATA[Ordinal embedding aims at finding a low dimensional representation of objects
from a set of constraints of the form "item $j$ is closer to item $i$ than item
$k$". Typically, each object is mapped onto a point vector in a low dimensional
metric space. We argue that mapping to a density instead of a point vector
provides some interesting advantages, including an inherent reflection of the
uncertainty about the representation itself and its relative location in the
space. Indeed, in this paper, we propose to embed each object as a Gaussian
distribution. We investigate the ability of these embeddings to capture the
underlying structure of the data while satisfying the constraints, and explore
properties of the representation. Experiments on synthetic and real-world
datasets showcase the advantages of our approach. In addition, we illustrate
the merit of modelling uncertainty, which enriches the visual perception of the
mapped objects in the space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diallo_A/0/1/0/all/0/1"&gt;A&amp;#xef;ssatou Diallo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Furnkranz_J/0/1/0/all/0/1"&gt;Johannes F&amp;#xfc;rnkranz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Analysis Of Protected Health Information Leakage In Deep-Learning Based De-Identification Algorithms. (arXiv:2101.12099v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.12099</id>
        <link href="http://arxiv.org/abs/2101.12099"/>
        <updated>2021-05-24T05:08:42.050Z</updated>
        <summary type="html"><![CDATA[The increasing complexity of algorithms for analyzing medical data, including
de-identification tasks, raises the possibility that complex algorithms are
learning not just the general representation of the problem, but specifics of
given individuals within the data. Modern legal frameworks specifically
prohibit the intentional or accidental distribution of patient data, but have
not addressed this potential avenue for leakage of such protected health
information. Modern deep learning algorithms have the highest potential of such
leakage due to complexity of the models. Recent research in the field has
highlighted such issues in non-medical data, but all analysis is likely to be
data and algorithm specific. We, therefore, chose to analyze a state-of-the-art
free-text de-identification algorithm based on LSTM (Long Short-Term Memory)
and its potential in encoding any individual in the training set. Using the
i2b2 Challenge Data, we trained, then analyzed the model to assess whether the
output of the LSTM, before the compression layer of the classifier, could be
used to estimate the membership of the training data. Furthermore, we used
different attacks including membership inference attack method to attack the
model. Results indicate that the attacks could not identify whether members of
the training data were distinguishable from non-members based on the model
output. This indicates that the model does not provide any strong evidence into
the identification of the individuals in the training data set and there is not
yet empirical evidence it is unsafe to distribute the model for general use.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seyedi_S/0/1/0/all/0/1"&gt;Salman Seyedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1"&gt;Li Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nemati_S/0/1/0/all/0/1"&gt;Shamim Nemati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clifford_G/0/1/0/all/0/1"&gt;Gari D. Clifford&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Model Distillation with Noise-Free Differential Privacy. (arXiv:2009.05537v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.05537</id>
        <link href="http://arxiv.org/abs/2009.05537"/>
        <updated>2021-05-24T05:08:42.031Z</updated>
        <summary type="html"><![CDATA[Conventional federated learning directly averages model weights, which is
only possible for collaboration between models with homogeneous architectures.
Sharing prediction instead of weight removes this obstacle and eliminates the
risk of white-box inference attacks in conventional federated learning.
However, the predictions from local models are sensitive and would leak
training data privacy to the public. To address this issue, one naive approach
is adding the differentially private random noise to the predictions, which
however brings a substantial trade-off between privacy budget and model
performance. In this paper, we propose a novel framework called FEDMD-NFDP,
which applies a Noise-Free Differential Privacy (NFDP) mechanism into a
federated model distillation framework. Our extensive experimental results on
various datasets validate that FEDMD-NFDP can deliver not only comparable
utility and communication efficiency but also provide a noise-free differential
privacy guarantee. We also demonstrate the feasibility of our FEDMD-NFDP by
considering both IID and non-IID setting, heterogeneous model architectures,
and unlabelled public datasets from a different distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lichao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1"&gt;Lingjuan Lyu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Backdoor Attacks on Self-Supervised Learning. (arXiv:2105.10123v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10123</id>
        <link href="http://arxiv.org/abs/2105.10123"/>
        <updated>2021-05-24T05:08:42.024Z</updated>
        <summary type="html"><![CDATA[Large-scale unlabeled data has allowed recent progress in self-supervised
learning methods that learn rich visual representations. State-of-the-art
self-supervised methods for learning representations from images (MoCo and
BYOL) use an inductive bias that different augmentations (e.g. random crops) of
an image should produce similar embeddings. We show that such methods are
vulnerable to backdoor attacks where an attacker poisons a part of the
unlabeled data by adding a small trigger (known to the attacker) to the images.
The model performance is good on clean test images but the attacker can
manipulate the decision of the model by showing the trigger at test time.
Backdoor attacks have been studied extensively in supervised learning and to
the best of our knowledge, we are the first to study them for self-supervised
learning. Backdoor attacks are more practical in self-supervised learning since
the unlabeled data is large and as a result, an inspection of the data to avoid
the presence of poisoned data is prohibitive. We show that in our targeted
attack, the attacker can produce many false positives for the target category
by using the trigger at test time. We also propose a knowledge distillation
based defense algorithm that succeeds in neutralizing the attack. Our code is
available here: https://github.com/UMBCvision/SSL-Backdoor .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1"&gt;Aniruddha Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tejankar_A/0/1/0/all/0/1"&gt;Ajinkya Tejankar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koohpayegani_S/0/1/0/all/0/1"&gt;Soroush Abbasi Koohpayegani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1"&gt;Hamed Pirsiavash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A parallel-network continuous quantitative trading model with GARCH and PPO. (arXiv:2105.03625v2 [q-fin.TR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03625</id>
        <link href="http://arxiv.org/abs/2105.03625"/>
        <updated>2021-05-24T05:08:42.019Z</updated>
        <summary type="html"><![CDATA[It is a difficult task for both professional investors and individual traders
continuously making profit in stock market. With the development of computer
science and deep reinforcement learning, Buy\&Hold (B\&H) has been oversteped
by many artificial intelligence trading algorithms. However, the information
and process are not enough, which limit the performance of reinforcement
learning algorithms. Thus, we propose a parallel-network continuous
quantitative trading model with GARCH and PPO to enrich the basical deep
reinforcement learning model, where the deep learning parallel network layers
deal with 3 different frequencies data (including GARCH information) and
proximal policy optimization (PPO) algorithm interacts actions and rewards with
stock trading environment. Experiments in 5 stocks from Chinese stock market
show our method achieves more extra profit comparing with basical reinforcement
learning methods and bench models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhishun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wei Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kaixin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianhao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zixi Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pseudo Pixel-level Labeling for Images with Evolving Content. (arXiv:2105.09975v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09975</id>
        <link href="http://arxiv.org/abs/2105.09975"/>
        <updated>2021-05-24T05:08:42.013Z</updated>
        <summary type="html"><![CDATA[Annotating images for semantic segmentation requires intense manual labor and
is a time-consuming and expensive task especially for domains with a scarcity
of experts, such as Forensic Anthropology. We leverage the evolving nature of
images depicting the decay process in human decomposition data to design a
simple yet effective pseudo-pixel-level label generation technique to reduce
the amount of effort for manual annotation of such images. We first identify
sequences of images with a minimum variation that are most suitable to share
the same or similar annotation using an unsupervised approach. Given one
user-annotated image in each sequence, we propagate the annotation to the
remaining images in the sequence by merging it with annotations produced by a
state-of-the-art CAM-based pseudo label generation technique. To evaluate the
quality of our pseudo-pixel-level labels, we train two semantic segmentation
models with VGG and ResNet backbones on images labeled using our pseudo
labeling method and those of a state-of-the-art method. The results indicate
that using our pseudo-labels instead of those generated using the
state-of-the-art method in the training process improves the mean-IoU and the
frequency-weighted-IoU of the VGG and ResNet-based semantic segmentation models
by 3.36%, 2.58%, 10.39%, and 12.91% respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mousavi_S/0/1/0/all/0/1"&gt;Sara Mousavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhenning Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cross_K/0/1/0/all/0/1"&gt;Kelley Cross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steadman_D/0/1/0/all/0/1"&gt;Dawnie Steadman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mockus_A/0/1/0/all/0/1"&gt;Audris Mockus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Filters in Graph Convolutional Neural Networks. (arXiv:2105.10377v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10377</id>
        <link href="http://arxiv.org/abs/2105.10377"/>
        <updated>2021-05-24T05:08:42.006Z</updated>
        <summary type="html"><![CDATA[Over the last few years, we have seen increasing data generated from
non-Euclidean domains, which are usually represented as graphs with complex
relationships, and Graph Neural Networks (GNN) have gained a high interest
because of their potential in processing graph-structured data. In particular,
there is a strong interest in exploring the possibilities in performing
convolution on graphs using an extension of the GNN architecture, generally
referred to as Graph Convolutional Neural Networks (GCNN). Convolution on
graphs has been achieved mainly in two forms: spectral and spatial
convolutions. Due to the higher flexibility in exploring and exploiting the
graph structure of data, recently, there is an increasing interest in
investigating the possibilities that the spatial approach can offer. The idea
of finding a way to adapt the network behaviour to the inputs they process to
maximize the total performances has aroused much interest in the neural
networks literature over the years. This paper presents a novel method to adapt
the behaviour of a GCNN to the input proposing two ways to perform spatial
convolution on graphs using input-based filters which are dynamically
generated. Our model also investigates the problem of discovering and refining
relations among nodes. The experimental assessment confirms the capabilities of
the proposed approach, which achieves satisfying results using simple
architectures with a low number of filters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Apicella_A/0/1/0/all/0/1"&gt;Andrea Apicella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Isgro_F/0/1/0/all/0/1"&gt;Francesco Isgr&amp;#xf2;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pollastro_A/0/1/0/all/0/1"&gt;Andrea Pollastro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prevete_R/0/1/0/all/0/1"&gt;Roberto Prevete&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Optical physics inspired CNN approach for intrinsic image decomposition. (arXiv:2105.10076v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10076</id>
        <link href="http://arxiv.org/abs/2105.10076"/>
        <updated>2021-05-24T05:08:42.000Z</updated>
        <summary type="html"><![CDATA[Intrinsic Image Decomposition is an open problem of generating the
constituents of an image. Generating reflectance and shading from a single
image is a challenging task specifically when there is no ground truth. There
is a lack of unsupervised learning approaches for decomposing an image into
reflectance and shading using a single image. We propose a neural network
architecture capable of this decomposition using physics-based parameters
derived from the image. Through experimental results, we show that (a) the
proposed methodology outperforms the existing deep learning-based IID
techniques and (b) the derived parameters improve the efficacy significantly.
We conclude with a closer analysis of the results (numerical and example
images) showing several avenues for improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weligampola_H/0/1/0/all/0/1"&gt;Harshana Weligampola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jayatilaka_G/0/1/0/all/0/1"&gt;Gihan Jayatilaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sritharan_S/0/1/0/all/0/1"&gt;Suren Sritharan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ekanayake_P/0/1/0/all/0/1"&gt;Parakrama Ekanayake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ragel_R/0/1/0/all/0/1"&gt;Roshan Ragel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herath_V/0/1/0/all/0/1"&gt;Vijitha Herath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Godaliyadda_R/0/1/0/all/0/1"&gt;Roshan Godaliyadda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding. (arXiv:2105.09996v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09996</id>
        <link href="http://arxiv.org/abs/2105.09996"/>
        <updated>2021-05-24T05:08:41.986Z</updated>
        <summary type="html"><![CDATA[We present a simplified, task-agnostic multi-modal pre-training approach that
can accept either video or text input, or both for a variety of end tasks.
Existing pre-training are task-specific by adopting either a single cross-modal
encoder that requires both modalities, limiting their use for retrieval-style
end tasks or more complex multitask learning with two unimodal encoders,
limiting early cross-modal fusion. We instead introduce new pretraining masking
schemes that better mix across modalities (e.g. by forcing masks for text to
predict the closest video embeddings) while also maintaining separability (e.g.
unimodal predictions are sometimes required, without using all the input).
Experimental results show strong performance across a wider range of tasks than
any previous methods, often outperforming task-specific pre-training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1"&gt;Gargi Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1"&gt;Po-Yao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arora_P/0/1/0/all/0/1"&gt;Prahal Arora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aminzadeh_M/0/1/0/all/0/1"&gt;Masoumeh Aminzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1"&gt;Christoph Feichtenhofer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1"&gt;Florian Metze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1"&gt;Luke Zettlemoyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised MRI Reconstruction via Zero-Shot Learned Adversarial Transformers. (arXiv:2105.08059v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08059</id>
        <link href="http://arxiv.org/abs/2105.08059"/>
        <updated>2021-05-24T05:08:41.964Z</updated>
        <summary type="html"><![CDATA[Supervised deep learning has swiftly become a workhorse for accelerated MRI
in recent years, offering state-of-the-art performance in image reconstruction
from undersampled acquisitions. Training deep supervised models requires large
datasets of undersampled and fully-sampled acquisitions typically from a
matching set of subjects. Given scarce access to large medical datasets, this
limitation has sparked interest in unsupervised methods that reduce reliance on
fully-sampled ground-truth data. A common framework is based on the deep image
prior, where network-driven regularization is enforced directly during
inference on undersampled acquisitions. Yet, canonical convolutional
architectures are suboptimal in capturing long-range relationships, and
randomly initialized networks may hamper convergence. To address these
limitations, here we introduce a novel unsupervised MRI reconstruction method
based on zero-Shot Learned Adversarial TransformERs (SLATER). SLATER embodies a
deep adversarial network with cross-attention transformer blocks to map noise
and latent variables onto MR images. This unconditional network learns a
high-quality MRI prior in a self-supervised encoding task. A zero-shot
reconstruction is performed on undersampled test data, where inference is
performed by optimizing network parameters, latent and noise variables to
ensure maximal consistency to multi-coil MRI data. Comprehensive experiments on
brain MRI datasets clearly demonstrate the superior performance of SLATER
against several state-of-the-art unsupervised methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Korkmaz_Y/0/1/0/all/0/1"&gt;Yilmaz Korkmaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dar_S/0/1/0/all/0/1"&gt;Salman UH Dar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yurt_M/0/1/0/all/0/1"&gt;Mahmut Yurt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ozbey_M/0/1/0/all/0/1"&gt;Muzaffer &amp;#xd6;zbey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cukur_T/0/1/0/all/0/1"&gt;Tolga &amp;#xc7;ukur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning-based Implicit CSI Feedback in Massive MIMO. (arXiv:2105.10100v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2105.10100</id>
        <link href="http://arxiv.org/abs/2105.10100"/>
        <updated>2021-05-24T05:08:41.945Z</updated>
        <summary type="html"><![CDATA[Massive multiple-input multiple-output can obtain more performance gain by
exploiting the downlink channel state information (CSI) at the base station
(BS). Therefore, studying CSI feedback with limited communication resources in
frequency-division duplexing systems is of great importance. Recently, deep
learning (DL)-based CSI feedback has shown considerable potential. However, the
existing DL-based explicit feedback schemes are difficult to deploy because
current fifth-generation mobile communication protocols and systems are
designed based on an implicit feedback mechanism. In this paper, we propose a
DL-based implicit feedback architecture to inherit the low-overhead
characteristic, which uses neural networks (NNs) to replace the precoding
matrix indicator (PMI) encoding and decoding modules. By using environment
information, the NNs can achieve a more refined mapping between the precoding
matrix and the PMI compared with codebooks. The correlation between subbands is
also used to further improve the feedback performance. Simulation results show
that, for a single resource block (RB), the proposed architecture can save
25.0% and 40.0% of overhead compared with Type I codebook under two antenna
configurations, respectively. For a wideband system with 52 RBs, overhead can
be saved by 30.7% and 48.0% compared with Type II codebook when ignoring and
considering extracting subband correlation, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_M/0/1/0/all/0/1"&gt;Muhan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jiajia Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wen_C/0/1/0/all/0/1"&gt;Chao-Kai Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jin_S/0/1/0/all/0/1"&gt;Shi Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1"&gt;Geoffrey Ye Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_A/0/1/0/all/0/1"&gt;Ang Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Uniform OPE and Model-based Offline Reinforcement Learning in Time-Homogeneous, Reward-Free and Task-Agnostic Settings. (arXiv:2105.06029v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06029</id>
        <link href="http://arxiv.org/abs/2105.06029"/>
        <updated>2021-05-24T05:08:41.938Z</updated>
        <summary type="html"><![CDATA[This work studies the statistical limits of uniform convergence for offline
policy evaluation (OPE) problems with model-based methods (for finite horizon
MDP) and provides a unified view towards optimal learning for several
well-motivated offline tasks. Uniform OPE
$\sup_\Pi|Q^\pi-\hat{Q}^\pi|<\epsilon$ (initiated by \citet{yin2021near}) is a
stronger measure than the point-wise (fixed policy) OPE and ensures offline
policy learning when $\Pi$ contains all policies (global policy class). In this
paper, we establish an $\Omega(H^2 S/d_m\epsilon^2)$ lower bound (over
model-based family) for the global uniform OPE, where $d_m$ is the minimal
state-action probability induced by the behavior policy. Next, our main result
establishes an episode complexity of $\tilde{O}(H^2/d_m\epsilon^2)$ for
\emph{local} uniform convergence that applies to all \emph{near-empirically
optimal} policies for the MDPs with \emph{stationary} transition. This result
implies the optimal sample complexity for offline learning and separates the
local uniform OPE from the global case due to the extra $S$ factor.
Paramountly, the model-based method combining with our new analysis technique
(singleton absorbing MDP) can be adapted to the new settings: offline
task-agnostic and the offline reward-free with optimal complexity
$\tilde{O}(H^2\log(K)/d_m\epsilon^2)$ ($K$ is the number of tasks) and
$\tilde{O}(H^2S/d_m\epsilon^2)$ respectively, which provides a unified
framework for simultaneously solving different offline RL problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1"&gt;Ming Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu-Xiang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MedMNIST Classification Decathlon: A Lightweight AutoML Benchmark for Medical Image Analysis. (arXiv:2010.14925v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14925</id>
        <link href="http://arxiv.org/abs/2010.14925"/>
        <updated>2021-05-24T05:08:41.932Z</updated>
        <summary type="html"><![CDATA[We present MedMNIST, a collection of 10 pre-processed medical open datasets.
MedMNIST is standardized to perform classification tasks on lightweight 28x28
images, which requires no background knowledge. Covering the primary data
modalities in medical image analysis, it is diverse on data scale (from 100 to
100,000) and tasks (binary/multi-class, ordinal regression and multi-label).
MedMNIST could be used for educational purpose, rapid prototyping, multi-modal
machine learning or AutoML in medical image analysis. Moreover, MedMNIST
Classification Decathlon is designed to benchmark AutoML algorithms on all 10
datasets; We have compared several baseline methods, including open-source or
commercial AutoML tools. The datasets, evaluation code and baseline methods for
MedMNIST are publicly available at https://medmnist.github.io/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jiancheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1"&gt;Rui Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1"&gt;Bingbing Ni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel Methods for Unobserved Confounding: Negative Controls, Proxies, and Instruments. (arXiv:2012.10315v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10315</id>
        <link href="http://arxiv.org/abs/2012.10315"/>
        <updated>2021-05-24T05:08:41.925Z</updated>
        <summary type="html"><![CDATA[Negative control is a strategy for learning the causal relationship between
treatment and outcome in the presence of unmeasured confounding. The treatment
effect can nonetheless be identified if two auxiliary variables are available:
a negative control treatment (which has no effect on the actual outcome), and a
negative control outcome (which is not affected by the actual treatment). These
auxiliary variables can also be viewed as proxies for a traditional set of
control variables, and they bear resemblance to instrumental variables. I
propose a family of algorithms based on kernel ridge regression for learning
nonparametric treatment effects with negative controls. Examples include dose
response curves, dose response curves with distribution shift, and
heterogeneous treatment effects. Data may be discrete or continuous, and low,
high, or infinite dimensional. I prove uniform consistency and provide finite
sample rates of convergence. I estimate the dose response curve of cigarette
smoking on infant birth weight adjusting for unobserved confounding due to
household income, using a data set of singleton births in the state of
Pennsylvania between 1989 and 1991.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rahul Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning in EEG: Advance of the Last Ten-Year Critical Period. (arXiv:2011.11128v3 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11128</id>
        <link href="http://arxiv.org/abs/2011.11128"/>
        <updated>2021-05-24T05:08:41.918Z</updated>
        <summary type="html"><![CDATA[Deep learning has achieved excellent performance in a wide range of domains,
especially in speech recognition and computer vision. Relatively less work has
been done for EEG, but there is still significant progress attained in the last
decade. Due to the lack of a comprehensive and topic widely covered survey for
deep learning in EEG, we attempt to summarize recent progress to provide an
overview, as well as perspectives for future developments. We first briefly
mention the artifacts removal for EEG signal and then introduce deep learning
models that have been utilized in EEG processing and classification.
Subsequently, the applications of deep learning in EEG are reviewed by
categorizing them into groups such as brain-computer interface, disease
detection, and emotion recognition. They are followed by the discussion, in
which the pros and cons of deep learning are presented and future directions
and challenges for deep learning in EEG are proposed. We hope that this paper
could serve as a summary of past work for deep learning in EEG and the
beginning of further developments and achievements of EEG studies based on deep
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gong_S/0/1/0/all/0/1"&gt;Shu Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xing_K/0/1/0/all/0/1"&gt;Kaibo Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cichocki_A/0/1/0/all/0/1"&gt;Andrzej Cichocki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Junhua Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards a Universal NLG for Dialogue Systems and Simulators with Future Bridging. (arXiv:2105.10267v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10267</id>
        <link href="http://arxiv.org/abs/2105.10267"/>
        <updated>2021-05-24T05:08:41.900Z</updated>
        <summary type="html"><![CDATA[In a dialogue system pipeline, a natural language generation (NLG) unit
converts the dialogue direction and content to a corresponding natural language
realization. A recent trend for dialogue systems is to first pre-train on large
datasets and then fine-tune in a supervised manner using datasets annotated
with application-specific features. Though novel behaviours can be learned from
custom annotation, the required effort severely bounds the quantity of the
training set, and the application-specific nature limits the reuse. In light of
the recent success of data-driven approaches, we propose the novel future
bridging NLG (FBNLG) concept for dialogue systems and simulators. The critical
step is for an FBNLG to accept a future user or system utterance to bridge the
present context towards. Future bridging enables self supervised training over
annotation-free datasets, decoupled the training of NLG from the rest of the
system. An FBNLG, pre-trained with massive datasets, is expected to apply in
classical or new dialogue scenarios with minimal adaptation effort. We evaluate
a prototype FBNLG to show that future bridging can be a viable approach to a
universal few-shot NLG for task-oriented and chit-chat dialogues.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ennen_P/0/1/0/all/0/1"&gt;Philipp Ennen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yen-Ting Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozbay_A/0/1/0/all/0/1"&gt;Ali Girayhan Ozbay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Insalata_F/0/1/0/all/0/1"&gt;Ferdinando Insalata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Ye Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jalali_S/0/1/0/all/0/1"&gt;Sepehr Jalali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shiu_D/0/1/0/all/0/1"&gt;Da-shan Shiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relational Algorithms for k-means Clustering. (arXiv:2008.00358v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.00358</id>
        <link href="http://arxiv.org/abs/2008.00358"/>
        <updated>2021-05-24T05:08:41.893Z</updated>
        <summary type="html"><![CDATA[This paper gives a k-means approximation algorithm that is efficient in the
relational algorithms model. This is an algorithm that operates directly on a
relational database without performing a join to convert it to a matrix whose
rows represent the data points. The running time is potentially exponentially
smaller than $N$, the number of data points to be clustered that the relational
database represents.

Few relational algorithms are known and this paper offers techniques for
designing relational algorithms as well as characterizing their limitations. We
show that given two data points as cluster centers, if we cluster points
according to their closest centers, it is NP-Hard to approximate the number of
points in the clusters on a general relational input. This is trivial for
conventional data inputs and this result exemplifies that standard algorithmic
techniques may not be directly applied when designing an efficient relational
algorithm. This paper then introduces a new method that leverages rejection
sampling and the $k$-means++ algorithm to construct an O(1)-approximate k-means
solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moseley_B/0/1/0/all/0/1"&gt;Benjamin Moseley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pruhs_K/0/1/0/all/0/1"&gt;Kirk Pruhs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samadian_A/0/1/0/all/0/1"&gt;Alireza Samadian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuyan Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dense Reconstruction of Transparent Objects by Altering Incident Light Paths Through Refraction. (arXiv:2105.09993v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.09993</id>
        <link href="http://arxiv.org/abs/2105.09993"/>
        <updated>2021-05-24T05:08:41.887Z</updated>
        <summary type="html"><![CDATA[This paper addresses the problem of reconstructing the surface shape of
transparent objects. The difficulty of this problem originates from the
viewpoint dependent appearance of a transparent object, which quickly makes
reconstruction methods tailored for diffuse surfaces fail disgracefully. In
this paper, we introduce a fixed viewpoint approach to dense surface
reconstruction of transparent objects based on refraction of light. We present
a simple setup that allows us to alter the incident light paths before light
rays enter the object by immersing the object partially in a liquid, and
develop a method for recovering the object surface through reconstructing and
triangulating such incident light paths. Our proposed approach does not need to
model the complex interactions of light as it travels through the object,
neither does it assume any parametric form for the object shape nor the exact
number of refractions and reflections taken place along the light paths. It can
therefore handle transparent objects with a relatively complex shape and
structure, with unknown and inhomogeneous refractive index. We also show that
for thin transparent objects, our proposed acquisition setup can be further
simplified by adopting a single refraction approximation. Experimental results
on both synthetic and real data demonstrate the feasibility and accuracy of our
proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Han_K/0/1/0/all/0/1"&gt;Kai Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wong_K/0/1/0/all/0/1"&gt;Kwan-Yee K. Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_M/0/1/0/all/0/1"&gt;Miaomiao Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Unsupervised Multi-Object Tracking in Noisy Environments. (arXiv:2105.10005v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10005</id>
        <link href="http://arxiv.org/abs/2105.10005"/>
        <updated>2021-05-24T05:08:41.880Z</updated>
        <summary type="html"><![CDATA[Camera movement and unpredictable environmental conditions like dust and wind
induce noise into video feeds. We observe that popular unsupervised MOT methods
are dependent on noise-free conditions. We show that the addition of a small
amount of artificial random noise causes a sharp degradation in model
performance on benchmark metrics. We resolve this problem by introducing a
robust unsupervised multi-object tracking (MOT) model: AttU-Net. The proposed
single-head attention model helps limit the negative impact of noise by
learning visual representations at different segment scales. AttU-Net shows
better unsupervised MOT tracking performance over variational inference-based
state-of-the-art baselines. We evaluate our method in the MNIST and the Atari
game video benchmark. We also provide two extended video datasets consisting of
complex visual patterns that include Kuzushiji characters and fashion images to
validate the effectiveness of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;C.-H. Huck Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chhabra_M/0/1/0/all/0/1"&gt;Mohit Chhabra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Y.-C. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1"&gt;Quan Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoshinaga_T/0/1/0/all/0/1"&gt;Tomoaki Yoshinaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murakam_T/0/1/0/all/0/1"&gt;Tomokazu Murakam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Opening Deep Neural Networks with Generative Models. (arXiv:2105.10013v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10013</id>
        <link href="http://arxiv.org/abs/2105.10013"/>
        <updated>2021-05-24T05:08:41.872Z</updated>
        <summary type="html"><![CDATA[Image classification methods are usually trained to perform predictions
taking into account a predefined group of known classes. Real-world problems,
however, may not allow for a full knowledge of the input and label spaces,
making failures in recognition a hazard to deep visual learning. Open set
recognition methods are characterized by the ability to correctly identifying
inputs of known and unknown classes. In this context, we propose GeMOS: simple
and plug-and-play open set recognition modules that can be attached to
pretrained Deep Neural Networks for visual recognition. The GeMOS framework
pairs pre-trained Convolutional Neural Networks with generative models for open
set recognition to extract open set scores for each sample, allowing for
failure recognition in object recognition tasks. We conduct a thorough
evaluation of the proposed method in comparison with state-of-the-art open set
algorithms, finding that GeMOS either outperforms or is statistically
indistinguishable from more complex and costly models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vendramini_M/0/1/0/all/0/1"&gt;Marcos Vendramini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1"&gt;Hugo Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Machado_A/0/1/0/all/0/1"&gt;Alexei Machado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1"&gt;Jefersson A. dos Santos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficiency-boosting Client Selection Scheme for Federated Learning with Fairness Guarantee. (arXiv:2011.01783v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.01783</id>
        <link href="http://arxiv.org/abs/2011.01783"/>
        <updated>2021-05-24T05:08:41.866Z</updated>
        <summary type="html"><![CDATA[The issue of potential privacy leakage during centralized AI's model training
has drawn intensive concern from the public. A Parallel and Distributed
Computing (or PDC) scheme, termed Federated Learning (FL), has emerged as a new
paradigm to cope with the privacy issue by allowing clients to perform model
training locally, without the necessity to upload their personal sensitive
data. In FL, the number of clients could be sufficiently large, but the
bandwidth available for model distribution and re-upload is quite limited,
making it sensible to only involve part of the volunteers to participate in the
training process. The client selection policy is critical to an FL process in
terms of training efficiency, the final model's quality as well as fairness. In
this paper, we will model the fairness guaranteed client selection as a
Lyapunov optimization problem and then a C2MAB-based method is proposed for
estimation of the model exchange time between each client and the server, based
on which we design a fairness guaranteed algorithm termed RBCS-F for
problem-solving. The regret of RBCS-F is strictly bounded by a finite constant,
justifying its theoretical feasibility. Barring the theoretical results, more
empirical data can be derived from our real training experiments on public
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1"&gt;Tiansheng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Weiwei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wentai Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Ligang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Keqin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zomaya_A/0/1/0/all/0/1"&gt;Albert Y.Zomaya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Disentangled Representations for Time Series. (arXiv:2105.08179v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08179</id>
        <link href="http://arxiv.org/abs/2105.08179"/>
        <updated>2021-05-24T05:08:41.847Z</updated>
        <summary type="html"><![CDATA[Time-series representation learning is a fundamental task for time-series
analysis. While significant progress has been made to achieve accurate
representations for downstream applications, the learned representations often
lack interpretability and do not expose semantic meanings. Different from
previous efforts on the entangled feature space, we aim to extract the
semantic-rich temporal correlations in the latent interpretable factorized
representation of the data. Motivated by the success of disentangled
representation learning in computer vision, we study the possibility of
learning semantic-rich time-series representations, which remains unexplored
due to three main challenges: 1) sequential data structure introduces complex
temporal correlations and makes the latent representations hard to interpret,
2) sequential models suffer from KL vanishing problem, and 3) interpretable
semantic concepts for time-series often rely on multiple factors instead of
individuals. To bridge the gap, we propose Disentangle Time Series (DTS), a
novel disentanglement enhancement framework for sequential data. Specifically,
to generate hierarchical semantic concepts as the interpretable and
disentangled representation of time-series, DTS introduces multi-level
disentanglement strategies by covering both individual latent factors and group
semantic segments. We further theoretically show how to alleviate the KL
vanishing problem: DTS introduces a mutual information maximization term, while
preserving a heavier penalty on the total correlation and the dimension-wise KL
to keep the disentanglement property. Experimental results on various
real-world benchmark datasets demonstrate that the representations learned by
DTS achieve superior performance in downstream applications, with high
interpretability of semantic concepts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuening Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhengzhang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1"&gt;Daochen Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1"&gt;Mengnan Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Denghui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haifeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xia Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LAPAR: Linearly-Assembled Pixel-Adaptive Regression Network for Single Image Super-Resolution and Beyond. (arXiv:2105.10422v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10422</id>
        <link href="http://arxiv.org/abs/2105.10422"/>
        <updated>2021-05-24T05:08:41.840Z</updated>
        <summary type="html"><![CDATA[Single image super-resolution (SISR) deals with a fundamental problem of
upsampling a low-resolution (LR) image to its high-resolution (HR) version.
Last few years have witnessed impressive progress propelled by deep learning
methods. However, one critical challenge faced by existing methods is to strike
a sweet spot of deep model complexity and resulting SISR quality. This paper
addresses this pain point by proposing a linearly-assembled pixel-adaptive
regression network (LAPAR), which casts the direct LR to HR mapping learning
into a linear coefficient regression task over a dictionary of multiple
predefined filter bases. Such a parametric representation renders our model
highly lightweight and easy to optimize while achieving state-of-the-art
results on SISR benchmarks. Moreover, based on the same idea, LAPAR is extended
to tackle other restoration tasks, e.g., image denoising and JPEG image
deblocking, and again, yields strong performance. The code is available at
https://github.com/dvlab-research/Simple-SR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenbo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1"&gt;Kun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1"&gt;Lu Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1"&gt;Nianjuan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiangbo Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1"&gt;Jiaya Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intriguing Properties of Vision Transformers. (arXiv:2105.10497v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10497</id>
        <link href="http://arxiv.org/abs/2105.10497"/>
        <updated>2021-05-24T05:08:41.834Z</updated>
        <summary type="html"><![CDATA[Vision transformers (ViT) have demonstrated impressive performance across
various machine vision problems. These models are based on multi-head
self-attention mechanisms that can flexibly attend to a sequence of image
patches to encode contextual cues. An important question is how such
flexibility in attending image-wide context conditioned on a given patch can
facilitate handling nuisances in natural images e.g., severe occlusions, domain
shifts, spatial permutations, adversarial and natural perturbations. We
systematically study this question via an extensive set of experiments
encompassing three ViT families and comparisons with a high-performing
convolutional neural network (CNN). We show and analyze the following
intriguing properties of ViT: (a) Transformers are highly robust to severe
occlusions, perturbations and domain shifts, e.g., retain as high as 60% top-1
accuracy on ImageNet even after randomly occluding 80% of the image content.
(b) The robust performance to occlusions is not due to a bias towards local
textures, and ViTs are significantly less biased towards textures compared to
CNNs. When properly trained to encode shape-based features, ViTs demonstrate
shape recognition capability comparable to that of human visual system,
previously unmatched in the literature. (c) Using ViTs to encode shape
representation leads to an interesting consequence of accurate semantic
segmentation without pixel-level supervision. (d) Off-the-shelf features from a
single ViT model can be combined to create a feature ensemble, leading to high
accuracy rates across a range of classification datasets in both traditional
and few-shot learning paradigms. We show effective features of ViTs are due to
flexible and dynamic receptive fields possible via the self-attention
mechanism.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1"&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranasinghe_K/0/1/0/all/0/1"&gt;Kanchana Ranasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1"&gt;Salman Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1"&gt;Munawar Hayat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1"&gt;Fahad Shahbaz Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Ming-Hsuan Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Recursive Markov Boundary-Based Approach to Causal Structure Learning. (arXiv:2010.04992v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04992</id>
        <link href="http://arxiv.org/abs/2010.04992"/>
        <updated>2021-05-24T05:08:41.827Z</updated>
        <summary type="html"><![CDATA[Constraint-based methods are one of the main approaches for causal structure
learning that are particularly valued as they are asymptotically guaranteed to
find a structure that is Markov equivalent to the causal graph of the system.
On the other hand, they may require an exponentially large number of
conditional independence (CI) tests in the number of variables of the system.
In this paper, we propose a novel recursive constraint-based method for causal
structure learning that significantly reduces the required number of CI tests
compared to the existing literature. The idea of the proposed approach is to
use Markov boundary information to identify a specific variable that can be
removed from the set of variables without affecting the statistical
dependencies among the other variables. Having identified such a variable, we
discover its neighborhood, remove that variable from the set of variables, and
recursively learn the causal structure over the remaining variables. We further
provide a lower bound on the number of CI tests required by any
constraint-based method. Comparing this lower bound to our achievable bound
demonstrates the efficiency of the proposed approach. Our experimental results
show that the proposed algorithm outperforms state-of-the-art both on synthetic
and real-world structures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mokhtarian_E/0/1/0/all/0/1"&gt;Ehsan Mokhtarian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akbari_S/0/1/0/all/0/1"&gt;Sina Akbari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghassami_A/0/1/0/all/0/1"&gt;AmirEmad Ghassami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiyavash_N/0/1/0/all/0/1"&gt;Negar Kiyavash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations. (arXiv:2006.03659v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03659</id>
        <link href="http://arxiv.org/abs/2006.03659"/>
        <updated>2021-05-24T05:08:41.820Z</updated>
        <summary type="html"><![CDATA[Sentence embeddings are an important component of many natural language
processing (NLP) systems. Like word embeddings, sentence embeddings are
typically learned on large text corpora and then transferred to various
downstream tasks, such as clustering and retrieval. Unlike word embeddings, the
highest performing solutions for learning sentence embeddings require labelled
data, limiting their usefulness to languages and domains where labelled data is
abundant. In this paper, we present DeCLUTR: Deep Contrastive Learning for
Unsupervised Textual Representations. Inspired by recent advances in deep
metric learning (DML), we carefully design a self-supervised objective for
learning universal sentence embeddings that does not require labelled training
data. When used to extend the pretraining of transformer-based language models,
our approach closes the performance gap between unsupervised and supervised
pretraining for universal sentence encoders. Importantly, our experiments
suggest that the quality of the learned embeddings scale with both the number
of trainable parameters and the amount of unlabelled training data, making
further improvements straightforward. Our code and pretrained models are
publicly available and can be easily adapted to new domains or used to embed
unseen text.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Giorgi_J/0/1/0/all/0/1"&gt;John Giorgi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nitski_O/0/1/0/all/0/1"&gt;Osvald Nitski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bader_G/0/1/0/all/0/1"&gt;Gary Bader&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Robustness over High Level Driving Instruction for Autonomous Driving. (arXiv:2105.10014v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10014</id>
        <link href="http://arxiv.org/abs/2105.10014"/>
        <updated>2021-05-24T05:08:41.803Z</updated>
        <summary type="html"><![CDATA[In recent years, we have witnessed increasingly high performance in the field
of autonomous end-to-end driving. In particular, more and more research is
being done on driving in urban environments, where the car has to follow high
level commands to navigate. However, few evaluations are made on the ability of
these agents to react in an unexpected situation. Specifically, no evaluations
are conducted on the robustness of driving agents in the event of a bad
high-level command. We propose here an evaluation method, namely a benchmark
that allows to assess the robustness of an agent, and to appreciate its
understanding of the environment through its ability to keep a safe behavior,
regardless of the instruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carton_F/0/1/0/all/0/1"&gt;Florence Carton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filliat_D/0/1/0/all/0/1"&gt;David Filliat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabarisoa_J/0/1/0/all/0/1"&gt;Jaonary Rabarisoa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1"&gt;Quoc Cuong Pham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel 3D-UNet Deep Learning Framework Based on High-Dimensional Bilateral Grid for Edge Consistent Single Image Depth Estimation. (arXiv:2105.10129v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10129</id>
        <link href="http://arxiv.org/abs/2105.10129"/>
        <updated>2021-05-24T05:08:41.797Z</updated>
        <summary type="html"><![CDATA[The task of predicting smooth and edge-consistent depth maps is notoriously
difficult for single image depth estimation. This paper proposes a novel
Bilateral Grid based 3D convolutional neural network, dubbed as 3DBG-UNet, that
parameterizes high dimensional feature space by encoding compact 3D bilateral
grids with UNets and infers sharp geometric layout of the scene. Further,
another novel 3DBGES-UNet model is introduced that integrate 3DBG-UNet for
inferring an accurate depth map given a single color view. The 3DBGES-UNet
concatenates 3DBG-UNet geometry map with the inception network edge
accentuation map and a spatial object's boundary map obtained by leveraging
semantic segmentation and train the UNet model with ResNet backbone. Both
models are designed with a particular attention to explicitly account for edges
or minute details. Preserving sharp discontinuities at depth edges is critical
for many applications such as realistic integration of virtual objects in AR
video or occlusion-aware view synthesis for 3D display applications.The
proposed depth prediction network achieves state-of-the-art performance in both
qualitative and quantitative evaluations on the challenging NYUv2-Depth data.
The code and corresponding pre-trained weights will be made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1"&gt;Mansi Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Abheesht Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tushar_K/0/1/0/all/0/1"&gt;Kadvekar Rohit Tushar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panneer_A/0/1/0/all/0/1"&gt;Avinash Panneer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Generation and Evaluation of Visual Stories via Semantic Consistency. (arXiv:2105.10026v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10026</id>
        <link href="http://arxiv.org/abs/2105.10026"/>
        <updated>2021-05-24T05:08:41.790Z</updated>
        <summary type="html"><![CDATA[Story visualization is an under-explored task that falls at the intersection
of many important research directions in both computer vision and natural
language processing. In this task, given a series of natural language captions
which compose a story, an agent must generate a sequence of images that
correspond to the captions. Prior work has introduced recurrent generative
models which outperform text-to-image synthesis models on this task. However,
there is room for improvement of generated images in terms of visual quality,
coherence and relevance. We present a number of improvements to prior modeling
approaches, including (1) the addition of a dual learning framework that
utilizes video captioning to reinforce the semantic alignment between the story
and generated images, (2) a copy-transform mechanism for
sequentially-consistent story visualization, and (3) MART-based transformers to
model complex interactions between frames. We present ablation studies to
demonstrate the effect of each of these techniques on the generative power of
the model for both individual images as well as the entire narrative.
Furthermore, due to the complexity and generative nature of the task, standard
evaluation metrics do not accurately reflect performance. Therefore, we also
provide an exploration of evaluation metrics for the model, focused on aspects
of the generated frames such as the presence/quality of generated characters,
the relevance to captions, and the diversity of the generated images. We also
present correlation experiments of our proposed automated metrics with human
evaluations. Code and data available at:
https://github.com/adymaharana/StoryViz]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maharana_A/0/1/0/all/0/1"&gt;Adyasha Maharana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hannan_D/0/1/0/all/0/1"&gt;Darryl Hannan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An interpretable object detection based model for the diagnosis of neonatal lung diseases using Ultrasound images. (arXiv:2105.10081v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10081</id>
        <link href="http://arxiv.org/abs/2105.10081"/>
        <updated>2021-05-24T05:08:41.783Z</updated>
        <summary type="html"><![CDATA[Over the last few decades, Lung Ultrasound (LUS) has been increasingly used
to diagnose and monitor different lung diseases in neonates. It is a non
invasive tool that allows a fast bedside examination while minimally handling
the neonate. Acquiring a LUS scan is easy, but understanding the artifacts
concerned with each respiratory disease is challenging. Mixed artifact patterns
found in different respiratory diseases may limit LUS readability by the
operator. While machine learning (ML), especially deep learning can assist in
automated analysis, simply feeding the ultrasound images to an ML model for
diagnosis is not enough to earn the trust of medical professionals. The
algorithm should output LUS features that are familiar to the operator instead.
Therefore, in this paper we present a unique approach for extracting seven
meaningful LUS features that can be easily associated with a specific
pathological lung condition: Normal pleura, irregular pleura, thick pleura,
Alines, Coalescent B-lines, Separate B-lines and Consolidations. These
artifacts can lead to early prediction of infants developing later respiratory
distress symptoms. A single multi-class region proposal-based object detection
model faster-RCNN (fRCNN) was trained on lower posterior lung ultrasound videos
to detect these LUS features which are further linked to four common neonatal
diseases. Our results show that fRCNN surpasses single stage models such as
RetinaNet and can successfully detect the aforementioned LUS features with a
mean average precision of 86.4%. Instead of a fully automatic diagnosis from
images without any interpretability, detection of such LUS features leave the
ultimate control of diagnosis to the clinician, which can result in a more
trustworthy intelligent system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bassiouny_R/0/1/0/all/0/1"&gt;Rodina Bassiouny&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1"&gt;Adel Mohamed&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Umapathy_K/0/1/0/all/0/1"&gt;Karthi Umapathy&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1"&gt;Naimul Khan&lt;/a&gt; (1) ((1) Ryerson University, Toronto, Canada, (2) Mount Sinai Hospital, University of Toronto, Toronto, Canada)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sheaves as a Framework for Understanding and Interpreting Model Fit. (arXiv:2105.10414v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10414</id>
        <link href="http://arxiv.org/abs/2105.10414"/>
        <updated>2021-05-24T05:08:41.687Z</updated>
        <summary type="html"><![CDATA[As data grows in size and complexity, finding frameworks which aid in
interpretation and analysis has become critical. This is particularly true when
data comes from complex systems where extensive structure is available, but
must be drawn from peripheral sources. In this paper we argue that in such
situations, sheaves can provide a natural framework to analyze how well a
statistical model fits at the local level (that is, on subsets of related
datapoints) vs the global level (on all the data). The sheaf-based approach
that we propose is suitably general enough to be useful in a range of
applications, from analyzing sensor networks to understanding the feature space
of a deep learning model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1"&gt;Henry Kvinge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jefferson_B/0/1/0/all/0/1"&gt;Brett Jefferson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joslyn_C/0/1/0/all/0/1"&gt;Cliff Joslyn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Purvine_E/0/1/0/all/0/1"&gt;Emilie Purvine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the Performance of Knowledge Graph Embeddings in Drug Discovery. (arXiv:2105.10488v1 [q-bio.BM])]]></title>
        <id>http://arxiv.org/abs/2105.10488</id>
        <link href="http://arxiv.org/abs/2105.10488"/>
        <updated>2021-05-24T05:08:41.669Z</updated>
        <summary type="html"><![CDATA[Knowledge Graphs (KG) and associated Knowledge Graph Embedding (KGE) models
have recently begun to be explored in the context of drug discovery and have
the potential to assist in key challenges such as target identification. In the
drug discovery domain, KGs can be employed as part of a process which can
result in lab-based experiments being performed, or impact on other decisions,
incurring significant time and financial costs and most importantly, ultimately
influencing patient healthcare. For KGE models to have impact in this domain, a
better understanding of not only of performance, but also the various factors
which determine it, is required.

In this study we investigate, over the course of many thousands of
experiments, the predictive performance of five KGE models on two public drug
discovery-oriented KGs. Our goal is not to focus on the best overall model or
configuration, instead we take a deeper look at how performance can be affected
by changes in the training setup, choice of hyperparameters, model parameter
initialisation seed and different splits of the datasets. Our results highlight
that these factors have significant impact on performance and can even affect
the ranking of models. Indeed these factors should be reported along with model
architectures to ensure complete reproducibility and fair comparisons of future
work, and we argue this is critical for the acceptance of use, and impact of
KGEs in a biomedical setting. To aid reproducibility of our own work, we
release all experimentation code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Bonner_S/0/1/0/all/0/1"&gt;Stephen Bonner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Barrett_I/0/1/0/all/0/1"&gt;Ian P Barrett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ye_C/0/1/0/all/0/1"&gt;Cheng Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Swiers_R/0/1/0/all/0/1"&gt;Rowan Swiers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Engkvist_O/0/1/0/all/0/1"&gt;Ola Engkvist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hamilton_W/0/1/0/all/0/1"&gt;William L Hamilton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Knowledge Expansion. (arXiv:2103.14431v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14431</id>
        <link href="http://arxiv.org/abs/2103.14431"/>
        <updated>2021-05-24T05:08:41.629Z</updated>
        <summary type="html"><![CDATA[The popularity of multimodal sensors and the accessibility of the Internet
have brought us a massive amount of unlabeled multimodal data. Since existing
datasets and well-trained models are primarily unimodal, the modality gap
between a unimodal network and unlabeled multimodal data poses an interesting
problem: how to transfer a pre-trained unimodal network to perform the same
task on unlabeled multimodal data? In this work, we propose multimodal
knowledge expansion (MKE), a knowledge distillation-based framework to
effectively utilize multimodal data without requiring labels. Opposite to
traditional knowledge distillation, where the student is designed to be
lightweight and inferior to the teacher, we observe that a multimodal student
model consistently denoises pseudo labels and generalizes better than its
teacher. Extensive experiments on four tasks and different modalities verify
this finding. Furthermore, we connect the mechanism of MKE to semi-supervised
learning and offer both empirical and theoretical explanations to understand
the denoising capability of a multimodal student.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1"&gt;Zihui Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Sucheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhengqi Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hang Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temp-Frustum Net: 3D Object Detection with Temporal Fusion. (arXiv:2104.12106v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12106</id>
        <link href="http://arxiv.org/abs/2104.12106"/>
        <updated>2021-05-24T05:08:41.622Z</updated>
        <summary type="html"><![CDATA[3D object detection is a core component of automated driving systems.
State-of-the-art methods fuse RGB imagery and LiDAR point cloud data
frame-by-frame for 3D bounding box regression. However, frame-by-frame 3D
object detection suffers from noise, field-of-view obstruction, and sparsity.
We propose a novel Temporal Fusion Module (TFM) to use information from
previous time-steps to mitigate these problems. First, a state-of-the-art
frustum network extracts point cloud features from raw RGB and LiDAR point
cloud data frame-by-frame. Then, our TFM module fuses these features with a
recurrent neural network. As a result, 3D object detection becomes robust
against single frame failures and transient occlusions. Experiments on the
KITTI object tracking dataset show the efficiency of the proposed TFM, where we
obtain ~6%, ~4%, and ~6% improvements on Car, Pedestrian, and Cyclist classes,
respectively, compared to frame-by-frame baselines. Furthermore, ablation
studies reinforce that the subject of improvement is temporal fusion and show
the effects of different placements of TFM in the object detection pipeline.
Our code is open-source and available at
https://github.com/emecercelik/Temp-Frustum-Net.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ercelik_E/0/1/0/all/0/1"&gt;Eme&amp;#xe7; Er&amp;#xe7;elik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yurtsever_E/0/1/0/all/0/1"&gt;Ekim Yurtsever&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1"&gt;Alois Knoll&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Triplet Autoencoder for Histopathological Colon Cancer Nuclei Retrieval. (arXiv:2105.10262v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10262</id>
        <link href="http://arxiv.org/abs/2105.10262"/>
        <updated>2021-05-24T05:08:41.616Z</updated>
        <summary type="html"><![CDATA[Deep learning has shown a great improvement in the performance of visual
tasks. Image retrieval is the task of extracting the visually similar images
from a database for a query image. The feature matching is performed to rank
the images. Various hand-designed features have been derived in past to
represent the images. Nowadays, the power of deep learning is being utilized
for automatic feature learning from data in the field of biomedical image
analysis. Autoencoder and Siamese networks are two deep learning models to
learn the latent space (i.e., features or embedding). Autoencoder works based
on the reconstruction of the image from latent space. Siamese network utilizes
the triplets to learn the intra-class similarity and inter-class dissimilarity.
Moreover, Autoencoder is unsupervised, whereas Siamese network is supervised.
We propose a Joint Triplet Autoencoder Network (JTANet) by facilitating the
triplet learning in autoencoder framework. A joint supervised learning for
Siamese network and unsupervised learning for Autoencoder is performed.
Moreover, the Encoder network of Autoencoder is shared with Siamese network and
referred as the Siamcoder network. The features are extracted by using the
trained Siamcoder network for retrieval purpose. The experiments are performed
over Histopathological Routine Colon Cancer dataset. We have observed the
promising performance using the proposed JTANet model against the Autoencoder
and Siamese models for colon cancer nuclei retrieval in histopathological
images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Satya Rajendra Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1"&gt;Shiv Ram Dubey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+MS_S/0/1/0/all/0/1"&gt;Shruthi MS&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ventrapragada_S/0/1/0/all/0/1"&gt;Sairathan Ventrapragada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dasharatha_S/0/1/0/all/0/1"&gt;Saivamshi Salla Dasharatha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clustered Sampling: Low-Variance and Improved Representativity for Clients Selection in Federated Learning. (arXiv:2105.05883v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05883</id>
        <link href="http://arxiv.org/abs/2105.05883"/>
        <updated>2021-05-24T05:08:41.595Z</updated>
        <summary type="html"><![CDATA[This work addresses the problem of optimizing communications between server
and clients in federated learning (FL). Current sampling approaches in FL are
either biased, or non optimal in terms of server-clients communications and
training stability. To overcome this issue, we introduce \textit{clustered
sampling} for clients selection. We prove that clustered sampling leads to
better clients representatitivity and to reduced variance of the clients
stochastic aggregation weights in FL. Compatibly with our theory, we provide
two different clustering approaches enabling clients aggregation based on 1)
sample size, and 2) models similarity. Through a series of experiments in
non-iid and unbalanced scenarios, we demonstrate that model aggregation through
clustered sampling consistently leads to better training convergence and
variability when compared to standard sampling approaches. Our approach does
not require any additional operation on the clients side, and can be seamlessly
integrated in standard FL implementations. Finally, clustered sampling is
compatible with existing methods and technologies for privacy enhancement, and
for communication reduction through model compression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fraboni_Y/0/1/0/all/0/1"&gt;Yann Fraboni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1"&gt;Richard Vidal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kameni_L/0/1/0/all/0/1"&gt;Laetitia Kameni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lorenzi_M/0/1/0/all/0/1"&gt;Marco Lorenzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uma implementa\c{c}\~ao do jogo Pedra, Papel e Tesoura utilizando Visao Computacional. (arXiv:2105.10063v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10063</id>
        <link href="http://arxiv.org/abs/2105.10063"/>
        <updated>2021-05-24T05:08:41.575Z</updated>
        <summary type="html"><![CDATA[This paper presents a game, controlled by computer vision, in identification
of hand gestures (hand-tracking). The proposed work is based on image
segmentation and construction of a convex hull with Jarvis Algorithm , and
determination of the pattern based on the extraction of area characteristics in
the convex hull.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Santos_E/0/1/0/all/0/1"&gt;Ezequiel Fran&amp;#xe7;a dos Santos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fontenelle_G/0/1/0/all/0/1"&gt;Gabriel Fontenelle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Context for improving recognition of Online Handwritten Mathematical Expressions. (arXiv:2105.10156v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10156</id>
        <link href="http://arxiv.org/abs/2105.10156"/>
        <updated>2021-05-24T05:08:41.549Z</updated>
        <summary type="html"><![CDATA[This paper presents a temporal classification method for all three subtasks
of symbol segmentation, symbol recognition and relation classification in
online handwritten mathematical expressions (HMEs). The classification model is
trained by multiple paths of symbols and spatial relations derived from the
Symbol Relation Tree (SRT) representation of HMEs. The method benefits from
global context of a deep bidirectional Long Short-term Memory network, which
learns the temporal classification directly from online handwriting by the
Connectionist Temporal Classification loss. To recognize an online HME, a
symbol-level parse tree with Context-Free Grammar is constructed, where symbols
and spatial relations are obtained from the temporal classification results. We
show the effectiveness of the proposed method on the two latest CROHME
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1"&gt;Cuong Tuan Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1"&gt;Thanh-Nghia Truong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Hung Tuan Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakagawa_M/0/1/0/all/0/1"&gt;Masaki Nakagawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GSSF: A Generative Sequence Similarity Function based on a Seq2Seq model for clustering online handwritten mathematical answers. (arXiv:2105.10159v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10159</id>
        <link href="http://arxiv.org/abs/2105.10159"/>
        <updated>2021-05-24T05:08:41.541Z</updated>
        <summary type="html"><![CDATA[Toward a computer-assisted marking for descriptive math questions,this paper
presents clustering of online handwritten mathematical expressions (OnHMEs) to
help human markers to mark them efficiently and reliably. We propose a
generative sequence similarity function for computing a similarity score of two
OnHMEs based on a sequence-to-sequence OnHME recognizer. Each OnHME is
represented by a similarity-based representation (SbR) vector. The SbR matrix
is inputted to the k-means algorithm for clustering OnHMEs. Experiments are
conducted on an answer dataset (Dset_Mix) of 200 OnHMEs mixed of real patterns
and synthesized patterns for each of 10 questions and a real online handwritten
mathematical answer dataset of 122 student answers at most for each of 15
questions (NIER_CBT). The best clustering results achieved around 0.916 and
0.915 for purity, and around 0.556 and 0.702 for the marking cost on Dset_Mix
and NIER_CBT, respectively. Our method currently outperforms the previous
methods for clustering HMEs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ung_H/0/1/0/all/0/1"&gt;Huy Quang Ung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1"&gt;Cuong Tuan Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Hung Tuan Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakagawa_M/0/1/0/all/0/1"&gt;Masaki Nakagawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Network and Embedding Usage: New Tricks of Node Classification with Graph Convolutional Networks. (arXiv:2105.08330v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08330</id>
        <link href="http://arxiv.org/abs/2105.08330"/>
        <updated>2021-05-24T05:08:41.530Z</updated>
        <summary type="html"><![CDATA[Graph Convolutional Networks (GCNs) and subsequent variants have been
proposed to solve tasks on graphs, especially node classification tasks. In the
literature, however, most tricks or techniques are either briefly mentioned as
implementation details or only visible in source code. In this paper, we first
summarize some existing effective tricks used in GCNs mini-batch training.
Based on this, two novel tricks named GCN_res Framework and Embedding Usage are
proposed by leveraging residual network and pre-trained embedding to improve
baseline's test accuracy in different datasets. Experiments on Open Graph
Benchmark (OGB) show that, by combining these techniques, the test accuracy of
various GCNs increases by 1.21%~2.84%. We open source our implementation at
https://github.com/ytchx1999/PyG-OGB-Tricks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chi_H/0/1/0/all/0/1"&gt;Huixuan Chi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuying Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_Q/0/1/0/all/0/1"&gt;Qinfen Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1"&gt;Hong Xia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Behind the leaves -- Estimation of occluded grapevine berries with conditional generative adversarial networks. (arXiv:2105.10325v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10325</id>
        <link href="http://arxiv.org/abs/2105.10325"/>
        <updated>2021-05-24T05:08:41.520Z</updated>
        <summary type="html"><![CDATA[The need for accurate yield estimates for viticulture is becoming more
important due to increasing competition in the wine market worldwide. One of
the most promising methods to estimate the harvest is berry counting, as it can
be approached non-destructively, and its process can be automated. In this
article, we present a method that addresses the challenge of occluded berries
with leaves to obtain a more accurate estimate of the number of berries that
will enable a better estimate of the harvest. We use generative adversarial
networks, a deep learning-based approach that generates a likely scenario
behind the leaves exploiting learned patterns from images with non-occluded
berries. Our experiments show that the estimate of the number of berries after
applying our method is closer to the manually counted reference. In contrast to
applying a factor to the berry count, our approach better adapts to local
conditions by directly involving the appearance of the visible berries.
Furthermore, we show that our approach can identify which areas in the image
should be changed by adding new berries without explicitly requiring
information about hidden areas.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kierdorf_J/0/1/0/all/0/1"&gt;Jana Kierdorf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weber_I/0/1/0/all/0/1"&gt;Immanuel Weber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kicherer_A/0/1/0/all/0/1"&gt;Anna Kicherer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zabawa_L/0/1/0/all/0/1"&gt;Laura Zabawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drees_L/0/1/0/all/0/1"&gt;Lukas Drees&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roscher_R/0/1/0/all/0/1"&gt;Ribana Roscher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random Hash Code Generation for Cancelable Fingerprint Templates using Vector Permutation and Shift-order Process. (arXiv:2105.10227v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2105.10227</id>
        <link href="http://arxiv.org/abs/2105.10227"/>
        <updated>2021-05-24T05:08:41.502Z</updated>
        <summary type="html"><![CDATA[Cancelable biometric techniques have been used to prevent the compromise of
biometric data by generating and using their corresponding cancelable templates
for user authentication. However, the non-invertible distance preserving
transformation methods employed in various schemes are often vulnerable to
information leakage since matching is performed in the transformed domain. In
this paper, we propose a non-invertible distance preserving scheme based on
vector permutation and shift-order process. First, the dimension of feature
vectors is reduced using kernelized principle component analysis (KPCA) prior
to randomly permuting the extracted vector features. A shift-order process is
then applied to the generated features in order to achieve non-invertibility
and combat similarity-based attacks. The generated hash codes are resilient to
different security and privacy attacks whilst fulfilling the major revocability
and unlinkability requirements. Experimental evaluation conducted on 6 datasets
of FVC2002 and FVC2004 reveals a high-performance accuracy of the proposed
scheme better than other existing state-of-the-art schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdullahi_S/0/1/0/all/0/1"&gt;Sani M. Abdullahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shuifa_S/0/1/0/all/0/1"&gt;Sun Shuifa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EMface: Detecting Hard Faces by Exploring Receptive Field Pyraminds. (arXiv:2105.10104v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10104</id>
        <link href="http://arxiv.org/abs/2105.10104"/>
        <updated>2021-05-24T05:08:41.496Z</updated>
        <summary type="html"><![CDATA[Scale variation is one of the most challenging problems in face detection.
Modern face detectors employ feature pyramids to deal with scale variation.
However, it might break the feature consistency across different scales of
faces. In this paper, we propose a simple yet effective method named the
receptive field pyramids (RFP) method to enhance the representation ability of
feature pyramids. It can learn different receptive fields in each feature map
adaptively based on the varying scales of detected faces. Empirical results on
two face detection benchmark datasets, i.e., WIDER FACE and UFDD, demonstrate
that our proposed method can accelerate the inference rate significantly while
achieving state-of-the-art performance. The source code of our method is
available at \url{https://github.com/emdata-ailab/EMface}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1"&gt;Leilei Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1"&gt;Yao Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual representation of negation: Real world data analysis on comic image design. (arXiv:2105.10131v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10131</id>
        <link href="http://arxiv.org/abs/2105.10131"/>
        <updated>2021-05-24T05:08:41.489Z</updated>
        <summary type="html"><![CDATA[There has been a widely held view that visual representations (e.g.,
photographs and illustrations) do not depict negation, for example, one that
can be expressed by a sentence "the train is not coming". This view is
empirically challenged by analyzing the real-world visual representations of
comic (manga) illustrations. In the experiment using image captioning tasks, we
gave people comic illustrations and asked them to explain what they could read
from them. The collected data showed that some comic illustrations could depict
negation without any aid of sequences (multiple panels) or conventional devices
(special symbols). This type of comic illustrations was subjected to further
experiments, classifying images into those containing negation and those not
containing negation. While this image classification was easy for humans, it
was difficult for data-driven machines, i.e., deep learning models (CNN), to
achieve the same high performance. Given the findings, we argue that some comic
illustrations evoke background knowledge and thus can depict negation with
purely visual elements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1"&gt;Yuri Sato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mineshima_K/0/1/0/all/0/1"&gt;Koji Mineshima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ueda_K/0/1/0/all/0/1"&gt;Kazuhiro Ueda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From parcel to continental scale -- A first European crop type map based on Sentinel-1 and LUCAS Copernicus in-situ observations. (arXiv:2105.09261v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09261</id>
        <link href="http://arxiv.org/abs/2105.09261"/>
        <updated>2021-05-24T05:08:41.483Z</updated>
        <summary type="html"><![CDATA[Detailed parcel-level crop type mapping for the whole European Union (EU) is
necessary for the evaluation of agricultural policies. The Copernicus program,
and Sentinel-1 (S1) in particular, offers the opportunity to monitor
agricultural land at a continental scale and in a timely manner. However, so
far the potential of S1 has not been explored at such a scale. Capitalizing on
the unique LUCAS 2018 Copernicus in-situ survey, we present the first
continental crop type map at 10-m spatial resolution for the EU based on S1A
and S1B Synthetic Aperture Radar observations for the year 2018. Random forest
classification algorithms are tuned to detect 19 different crop types. We
assess the accuracy of this EU crop map with three approaches. First, the
accuracy is assessed with independent LUCAS core in-situ observations over the
continent. Second, an accuracy assessment is done specifically for main crop
types from farmers declarations from 6 EU member countries or regions totaling
>3M parcels and 8.21 Mha. Finally, the crop areas derived by classification are
compared to the subnational (NUTS 2) area statistics reported by Eurostat. The
overall accuracy for the map is reported as 80.3% when grouping main crop
classes and 76% when considering all 19 crop type classes separately. Highest
accuracies are obtained for rape and turnip rape with user and produced
accuracies higher than 96%. The correlation between the remotely sensed
estimated and Eurostat reported crop area ranges from 0.93 (potatoes) to 0.99
(rape and turnip rape). Finally, we discuss how the framework presented here
can underpin the operational delivery of in-season high-resolution based crop
mapping.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+dAndrimont_R/0/1/0/all/0/1"&gt;Rapha&amp;#xeb;l d&amp;#x27;Andrimont&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Verhegghen_A/0/1/0/all/0/1"&gt;Astrid Verhegghen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lemoine_G/0/1/0/all/0/1"&gt;Guido Lemoine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kempeneers_P/0/1/0/all/0/1"&gt;Pieter Kempeneers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Meroni_M/0/1/0/all/0/1"&gt;Michele Meroni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Velde_M/0/1/0/all/0/1"&gt;Marijn van der Velde&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safety Metrics for Semantic Segmentation in Autonomous Driving. (arXiv:2105.10142v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10142</id>
        <link href="http://arxiv.org/abs/2105.10142"/>
        <updated>2021-05-24T05:08:41.476Z</updated>
        <summary type="html"><![CDATA[Within the context of autonomous driving, safety-related metrics for deep
neural networks have been widely studied for image classification and object
detection. In this paper, we further consider safety-aware correctness and
robustness metrics specialized for semantic segmentation. The novelty of our
proposal is to move beyond pixel-level metrics: Given two images with each
having N pixels being class-flipped, the designed metrics should, depending on
the clustering of pixels being class-flipped or the location of occurrence,
reflect a different level of safety criticality. The result evaluated on an
autonomous driving dataset demonstrates the validity and practicality of our
proposed methodology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1"&gt;Chih-Hong Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1"&gt;Alois Knoll&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1"&gt;Hsuan-Cheng Liao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Context-Aware Translation Models Pay the Right Attention?. (arXiv:2105.06977v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06977</id>
        <link href="http://arxiv.org/abs/2105.06977"/>
        <updated>2021-05-24T05:08:41.459Z</updated>
        <summary type="html"><![CDATA[Context-aware machine translation models are designed to leverage contextual
information, but often fail to do so. As a result, they inaccurately
disambiguate pronouns and polysemous words that require context for resolution.
In this paper, we ask several questions: What contexts do human translators use
to resolve ambiguous words? Are models paying large amounts of attention to the
same context? What if we explicitly train them to do so? To answer these
questions, we introduce SCAT (Supporting Context for Ambiguous Translations), a
new English-French dataset comprising supporting context words for 14K
translations that professional translators found useful for pronoun
disambiguation. Using SCAT, we perform an in-depth analysis of the context used
to disambiguate, examining positional and lexical characteristics of the
supporting words. Furthermore, we measure the degree of alignment between the
model's attention scores and the supporting context from SCAT, and apply a
guided attention strategy to encourage agreement between the two.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1"&gt;Kayo Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1"&gt;Patrick Fernandes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pruthi_D/0/1/0/all/0/1"&gt;Danish Pruthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1"&gt;Aditi Chaudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; F. T. Martins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1"&gt;Graham Neubig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Happy Dance, Slow Clap: Using Reaction GIFs to Predict Induced Affect on Twitter. (arXiv:2105.09967v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09967</id>
        <link href="http://arxiv.org/abs/2105.09967"/>
        <updated>2021-05-24T05:08:41.422Z</updated>
        <summary type="html"><![CDATA[Datasets with induced emotion labels are scarce but of utmost importance for
many NLP tasks. We present a new, automated method for collecting texts along
with their induced reaction labels. The method exploits the online use of
reaction GIFs, which capture complex affective states. We show how to augment
the data with induced emotion and induced sentiment labels. We use our method
to create and publish ReactionGIF, a first-of-its-kind affective dataset of 30K
tweets. We provide baselines for three new tasks, including induced sentiment
prediction and multilabel classification of induced emotions. Our method and
dataset open new research opportunities in emotion detection and affective
computing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shmueli_B/0/1/0/all/0/1"&gt;Boaz Shmueli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ray_S/0/1/0/all/0/1"&gt;Soumya Ray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ku_L/0/1/0/all/0/1"&gt;Lun-Wei Ku&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Fairness of Generative Adversarial Networks (GANs). (arXiv:2103.00950v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00950</id>
        <link href="http://arxiv.org/abs/2103.00950"/>
        <updated>2021-05-24T05:08:41.413Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) are one of the greatest advances in AI
in recent years. With their ability to directly learn the probability
distribution of data, and then sample synthetic realistic data. Many
applications have emerged, using GANs to solve classical problems in machine
learning, such as data augmentation, class unbalance problems, and fair
representation learning. In this paper, we analyze and highlight fairness
concerns of GANs model. In this regard, we show empirically that GANs models
may inherently prefer certain groups during the training process and therefore
they're not able to homogeneously generate data from different groups during
the testing phase. Furthermore, we propose solutions to solve this issue by
conditioning the GAN model towards samples' group or using ensemble method
(boosting) to allow the GAN model to leverage distributed structure of data
during the training phase and generate groups at equal rate during the testing
phase.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kenfack_P/0/1/0/all/0/1"&gt;Patrik Joslin Kenfack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arapov_D/0/1/0/all/0/1"&gt;Daniil Dmitrievich Arapov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hussain_R/0/1/0/all/0/1"&gt;Rasheed Hussain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kazmi_S/0/1/0/all/0/1"&gt;S.M. Ahsan Kazmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1"&gt;Adil Mehmood Khan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Stroke Rehabilitation Assessment using Wearable Accelerometers in Free-Living Environments. (arXiv:2009.08798v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08798</id>
        <link href="http://arxiv.org/abs/2009.08798"/>
        <updated>2021-05-24T05:08:41.403Z</updated>
        <summary type="html"><![CDATA[Stroke is known as a major global health problem, and for stroke survivors it
is key to monitor the recovery levels. However, traditional stroke
rehabilitation assessment methods (such as the popular clinical assessment) can
be subjective and expensive, and it is also less convenient for patients to
visit clinics in a high frequency. To address this issue, in this work based on
wearable sensing and machine learning techniques, we developed an automated
system that can predict the assessment score in an objective manner. With
wrist-worn sensors, accelerometer data was collected from 59 stroke survivors
in free-living environments for a duration of 8 weeks, and we aim to map the
week-wise accelerometer data (3 days per week) to the assessment score by
developing signal processing and predictive model pipeline. To achieve this, we
proposed two types of new features, which can encode the rehabilitation
information from both paralysed/non-paralysed sides while suppressing the
high-level noises such as irrelevant daily activities. Based on the proposed
features, we further developed the longitudinal mixed-effects model with
Gaussian process prior (LMGP), which can model the random effects caused by
different subjects and time slots (during the 8 weeks). Comprehensive
experiments were conducted to evaluate our system on both acute and chronic
patients, and the results suggested its effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Guan_Y/0/1/0/all/0/1"&gt;Yu Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jian-Qing Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Du_X/0/1/0/all/0/1"&gt;Xiu-Li Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Eyre_J/0/1/0/all/0/1"&gt;Janet Eyre&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedProf: Efficient Federated Learning with Data Representation Profiling. (arXiv:2102.01733v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01733</id>
        <link href="http://arxiv.org/abs/2102.01733"/>
        <updated>2021-05-24T05:08:41.397Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) has shown great potential as a privacy-preserving
solution to learning from decentralized data which are only accessible locally
on end devices (i.e., clients). In many scenarios, however, a large proportion
of the clients are probably in possession of low-quality data that are biased,
noisy or even irrelevant. As a result, they could significantly slow down the
convergence of the global model we aim to build and also compromise its
quality. In light of this, we propose FedProf, a novel protocol for optimizing
FL under such circumstances without breaching data privacy. The key of our
approach is using the global model to dynamically profile the latent
representations of data (termed representation footprints) on the clients. By
matching local footprints on clients against a baseline footprint on the
server, we adaptively score each client and adjust its probability of being
selected each round so as to mitigate the impact of the clients with
low-quality data on the training process. We have conducted extensive
experiments on public data sets using various FL settings. The results show
that FedProf effectively reduces the number of communication rounds and overall
time (providing up to 4.5x speedup) for the global model to converge while
improving the accuracy of the final global model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wentai Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Ligang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Weiwei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_R/0/1/0/all/0/1"&gt;Rui Mao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LoopNet: Musical Loop Synthesis Conditioned On Intuitive Musical Parameters. (arXiv:2105.10371v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2105.10371</id>
        <link href="http://arxiv.org/abs/2105.10371"/>
        <updated>2021-05-24T05:08:41.378Z</updated>
        <summary type="html"><![CDATA[Loops, seamlessly repeatable musical segments, are a cornerstone of modern
music production. Contemporary artists often mix and match various sampled or
pre-recorded loops based on musical criteria such as rhythm, harmony and
timbral texture to create compositions. Taking such criteria into account, we
present LoopNet, a feed-forward generative model for creating loops conditioned
on intuitive parameters. We leverage Music Information Retrieval (MIR) models
as well as a large collection of public loop samples in our study and use the
Wave-U-Net architecture to map control parameters to audio. We also evaluate
the quality of the generated audio and propose intuitive controls for composers
to map the ideas in their minds to an audio loop.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chandna_P/0/1/0/all/0/1"&gt;Pritish Chandna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramires_A/0/1/0/all/0/1"&gt;Ant&amp;#xf3;nio Ramires&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Serra_X/0/1/0/all/0/1"&gt;Xavier Serra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_E/0/1/0/all/0/1"&gt;Emilia G&amp;#xf3;mez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying Uncertainty from Different Sources in Deep Neural Networks for Image Classification. (arXiv:2011.08712v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08712</id>
        <link href="http://arxiv.org/abs/2011.08712"/>
        <updated>2021-05-24T05:08:41.367Z</updated>
        <summary type="html"><![CDATA[Quantifying uncertainty in a model's predictions is important as it enables
the safety of an AI system to be increased by acting on the model's output in
an informed manner. This is crucial for applications where the cost of an error
is high, such as in autonomous vehicle control, medical image analysis,
financial estimations or legal fields. Deep Neural Networks are powerful
predictors that have recently achieved state-of-the-art performance on a wide
spectrum of tasks. Quantifying predictive uncertainty in DNNs is a challenging
and yet on-going problem. In this paper we propose a complete framework to
capture and quantify all of these three types of uncertainties in DNNs for
image classification. This framework includes an ensemble of CNNs for model
uncertainty, a supervised reconstruction auto-encoder to capture distributional
uncertainty and using the output of activation functions in the last layer of
the network, to capture data uncertainty. Finally we demonstrate the efficiency
of our method on popular image datasets for classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khoshsirat_A/0/1/0/all/0/1"&gt;Aria Khoshsirat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elliptical Ordinal Embedding. (arXiv:2105.10457v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10457</id>
        <link href="http://arxiv.org/abs/2105.10457"/>
        <updated>2021-05-24T05:08:41.329Z</updated>
        <summary type="html"><![CDATA[Ordinal embedding aims at finding a low dimensional representation of objects
from a set of constraints of the form "item $j$ is closer to item $i$ than item
$k$". Typically, each object is mapped onto a point vector in a low dimensional
metric space. We argue that mapping to a density instead of a point vector
provides some interesting advantages, including an inherent reflection of the
uncertainty about the representation itself and its relative location in the
space. Indeed, in this paper, we propose to embed each object as a Gaussian
distribution. We investigate the ability of these embeddings to capture the
underlying structure of the data while satisfying the constraints, and explore
properties of the representation. Experiments on synthetic and real-world
datasets showcase the advantages of our approach. In addition, we illustrate
the merit of modelling uncertainty, which enriches the visual perception of the
mapped objects in the space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diallo_A/0/1/0/all/0/1"&gt;A&amp;#xef;ssatou Diallo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Furnkranz_J/0/1/0/all/0/1"&gt;Johannes F&amp;#xfc;rnkranz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Explainable Classification Model for Chronic Kidney Disease Patients. (arXiv:2105.10368v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10368</id>
        <link href="http://arxiv.org/abs/2105.10368"/>
        <updated>2021-05-24T05:08:41.320Z</updated>
        <summary type="html"><![CDATA[Currently, Chronic Kidney Disease (CKD) is experiencing a globally increasing
incidence and high cost to health systems. A delayed recognition leads to
premature mortality due to progressive loss of kidney function. The employment
of data mining to discover subtle patterns in CKD indicators would contribute
to an early diagnosis. This work develops a classifier model that would support
healthcare professionals in the early diagnosis of CKD patients. Through a data
pipeline, an exhaustive search is performed to find the best data mining
classifier with different parameters of the data preparation's sub-stages like
data missing or feature selection. Therefore, Extra Trees is selected as the
best classifier with a 100% and 99% of accuracy with, respectively,
cross-validation technique and with new unseen data. Moreover, the 8 features
selected are employed to assess the explainability of the model's results
denoting which features are more relevant in the model's output.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moreno_Sanchez_P/0/1/0/all/0/1"&gt;Pedro A. Moreno-Sanchez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Explaining Random Forests with SAT. (arXiv:2105.10278v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10278</id>
        <link href="http://arxiv.org/abs/2105.10278"/>
        <updated>2021-05-24T05:08:41.312Z</updated>
        <summary type="html"><![CDATA[Random Forest (RFs) are among the most widely used Machine Learning (ML)
classifiers. Even though RFs are not interpretable, there are no dedicated
non-heuristic approaches for computing explanations of RFs. Moreover, there is
recent work on polynomial algorithms for explaining ML models, including naive
Bayes classifiers. Hence, one question is whether finding explanations of RFs
can be solved in polynomial time. This paper answers this question negatively,
by proving that computing one PI-explanation of an RF is D^P-complete.
Furthermore, the paper proposes a propositional encoding for computing
explanations of RFs, thus enabling finding PI-explanations with a SAT solver.
This contrasts with earlier work on explaining boosted trees (BTs) and neural
networks (NNs), which requires encodings based on SMT/MILP. Experimental
results, obtained on a wide range of publicly available datasets, demontrate
that the proposed SAT-based approach scales to RFs of sizes common in practical
applications. Perhaps more importantly, the experimental results demonstrate
that, for the vast majority of examples considered, the SAT-based approach
proposed in this paper significantly outperforms existing heuristic approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Izza_Y/0/1/0/all/0/1"&gt;Yacine Izza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marques_Silva_J/0/1/0/all/0/1"&gt;Joao Marques-Silva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Error Resilient Collaborative Intelligence via Low-Rank Tensor Completion. (arXiv:2105.10341v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10341</id>
        <link href="http://arxiv.org/abs/2105.10341"/>
        <updated>2021-05-24T05:08:41.305Z</updated>
        <summary type="html"><![CDATA[In the race to bring Artificial Intelligence (AI) to the edge, collaborative
intelligence has emerged as a promising way to lighten the computation load on
edge devices that run applications based on Deep Neural Networks (DNNs).
Typically, a deep model is split at a certain layer into edge and cloud
sub-models. The deep feature tensor produced by the edge sub-model is
transmitted to the cloud, where the remaining computationally intensive
workload is performed by the cloud sub-model. The communication channel between
the edge and cloud is imperfect, which will result in missing data in the deep
feature tensor received at the cloud side. In this study, we examine the
effectiveness of four low-rank tensor completion methods in recovering missing
data in the deep feature tensor. We consider both sparse tensors, such as those
produced by the VGG16 model, as well as non-sparse tensors, such as those
produced by ResNet34 model. We study tensor completion effectiveness in both
conplexity-constrained and unconstrained scenario.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bragilevsky_L/0/1/0/all/0/1"&gt;Lior Bragilevsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bajic_I/0/1/0/all/0/1"&gt;Ivan V. Baji&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring Model Fairness under Noisy Covariates: A Theoretical Perspective. (arXiv:2105.09985v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09985</id>
        <link href="http://arxiv.org/abs/2105.09985"/>
        <updated>2021-05-24T05:08:41.287Z</updated>
        <summary type="html"><![CDATA[In this work we study the problem of measuring the fairness of a machine
learning model under noisy information. Focusing on group fairness metrics, we
investigate the particular but common situation when the evaluation requires
controlling for the confounding effect of covariate variables. In a practical
setting, we might not be able to jointly observe the covariate and group
information, and a standard workaround is to then use proxies for one or more
of these variables. Prior works have demonstrated the challenges with using a
proxy for sensitive attributes, and strong independence assumptions are needed
to provide guarantees on the accuracy of the noisy estimates. In contrast, in
this work we study using a proxy for the covariate variable and present a
theoretical analysis that aims to characterize weaker conditions under which
accurate fairness evaluation is possible.

Furthermore, our theory identifies potential sources of errors and decouples
them into two interpretable parts $\gamma$ and $\epsilon$. The first part
$\gamma$ depends solely on the performance of the proxy such as precision and
recall, whereas the second part $\epsilon$ captures correlations between all
the variables of interest. We show that in many scenarios the error in the
estimates is dominated by $\gamma$ via a linear dependence, whereas the
dependence on the correlations $\epsilon$ only constitutes a lower order term.
As a result we expand the understanding of scenarios where measuring model
fairness via proxies can be an effective approach. Finally, we compare, via
simulations, the theoretical upper-bounds to the distribution of simulated
estimation errors and show that assuming some structure on the data, even weak,
is key to significantly improve both theoretical guarantees and empirical
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Prost_F/0/1/0/all/0/1"&gt;Flavien Prost&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awasthi_P/0/1/0/all/0/1"&gt;Pranjal Awasthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blumm_N/0/1/0/all/0/1"&gt;Nick Blumm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumthekar_A/0/1/0/all/0/1"&gt;Aditee Kumthekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potter_T/0/1/0/all/0/1"&gt;Trevor Potter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1"&gt;Li Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xuezhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1"&gt;Ed H. Chi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jilin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beutel_A/0/1/0/all/0/1"&gt;Alex Beutel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extremely Lightweight Quantization Robust Real-Time Single-Image Super Resolution for Mobile Devices. (arXiv:2105.10288v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10288</id>
        <link href="http://arxiv.org/abs/2105.10288"/>
        <updated>2021-05-24T05:08:41.280Z</updated>
        <summary type="html"><![CDATA[Single-Image Super Resolution (SISR) is a classical computer vision problem
and it has been studied for over decades. With the recent success of deep
learning methods, recent work on SISR focuses solutions with deep learning
methodologies and achieves state-of-the-art results. However most of the
state-of-the-art SISR methods contain millions of parameters and layers, which
limits their practical applications. In this paper, we propose a hardware
(Synaptics Dolphin NPU) limitation aware, extremely lightweight quantization
robust real-time super resolution network (XLSR). The proposed model's building
block is inspired from root modules for Image classification. We successfully
applied root modules to SISR problem, further more to make the model uint8
quantization robust we used Clipped ReLU at the last layer of the network and
achieved great balance between reconstruction quality and runtime. Furthermore,
although the proposed network contains 30x fewer parameters than VDSR its
performance surpasses it on Div2K validation set. The network proved itself by
winning Mobile AI 2021 Real-Time Single Image Super Resolution Challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ayazoglu_M/0/1/0/all/0/1"&gt;Mustafa Ayazoglu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep-Learned Event Variables for Collider Phenomenology. (arXiv:2105.10126v1 [hep-ph])]]></title>
        <id>http://arxiv.org/abs/2105.10126</id>
        <link href="http://arxiv.org/abs/2105.10126"/>
        <updated>2021-05-24T05:08:41.274Z</updated>
        <summary type="html"><![CDATA[The choice of optimal event variables is crucial for achieving the maximal
sensitivity of experimental analyses. Over time, physicists have derived
suitable kinematic variables for many typical event topologies in collider
physics. Here we introduce a deep learning technique to design good event
variables, which are sensitive over a wide range of values for the unknown
model parameters. We demonstrate that the neural networks trained with our
technique on some simple event topologies are able to reproduce standard event
variables like invariant mass, transverse mass, and stransverse mass. The
method is automatable, completely general, and can be used to derive sensitive,
previously unknown, event variables for other, more complex event topologies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-ph/1/au:+Kim_D/0/1/0/all/0/1"&gt;Doojin Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Kong_K/0/1/0/all/0/1"&gt;Kyoungchul Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Matchev_K/0/1/0/all/0/1"&gt;Konstantin T. Matchev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Park_M/0/1/0/all/0/1"&gt;Myeonghun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Shyamsundar_P/0/1/0/all/0/1"&gt;Prasanth Shyamsundar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Automatic Sizing for PPE with a Point Cloud Based Variational Autoencoder. (arXiv:2105.10067v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10067</id>
        <link href="http://arxiv.org/abs/2105.10067"/>
        <updated>2021-05-24T05:08:41.258Z</updated>
        <summary type="html"><![CDATA[Sizing and fitting of Personal Protective Equipment (PPE) is a critical part
of the product creation process; however, traditional methods to do this type
of work can be labor intensive and based on limited or non-representative
anthropomorphic data. In the case of PPE, a poor fit can jeopardize an
individual's health and safety. In this paper we present an unsupervised
machine learning algorithm that can identify a representative set of exemplars,
individuals that can be utilized by designers as idealized sizing models. The
algorithm is based around a Variational Autoencoder (VAE) with a Point-Net
inspired encoder and decoder architecture trained on Human point-cloud data
obtained from the CEASAR dataset. The learned latent space is then clustered to
identify a specified number of sizing groups. We demonstrate this technique on
scans of human faces to provide designers of masks and facial coverings a
reference set of individuals to test existing mask styles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Searcy_J/0/1/0/all/0/1"&gt;Jacob A. Searcy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sokolowski_S/0/1/0/all/0/1"&gt;Susan L. Sokolowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model Compression. (arXiv:2105.10059v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10059</id>
        <link href="http://arxiv.org/abs/2105.10059"/>
        <updated>2021-05-24T05:08:41.222Z</updated>
        <summary type="html"><![CDATA[With time, machine learning models have increased in their scope,
functionality and size. Consequently, the increased functionality and size of
such models requires high-end hardware to both train and provide inference
after the fact. This paper aims to explore the possibilities within the domain
of model compression and discuss the efficiency of each of the possible
approaches while comparing model size and performance with respect to pre- and
post-compression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ishtiaq_A/0/1/0/all/0/1"&gt;Arhum Ishtiaq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_S/0/1/0/all/0/1"&gt;Sara Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anees_M/0/1/0/all/0/1"&gt;Maheen Anees&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mumtaz_N/0/1/0/all/0/1"&gt;Neha Mumtaz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Happy Dance, Slow Clap: Using Reaction GIFs to Predict Induced Affect on Twitter. (arXiv:2105.09967v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09967</id>
        <link href="http://arxiv.org/abs/2105.09967"/>
        <updated>2021-05-24T05:08:41.216Z</updated>
        <summary type="html"><![CDATA[Datasets with induced emotion labels are scarce but of utmost importance for
many NLP tasks. We present a new, automated method for collecting texts along
with their induced reaction labels. The method exploits the online use of
reaction GIFs, which capture complex affective states. We show how to augment
the data with induced emotion and induced sentiment labels. We use our method
to create and publish ReactionGIF, a first-of-its-kind affective dataset of 30K
tweets. We provide baselines for three new tasks, including induced sentiment
prediction and multilabel classification of induced emotions. Our method and
dataset open new research opportunities in emotion detection and affective
computing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shmueli_B/0/1/0/all/0/1"&gt;Boaz Shmueli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ray_S/0/1/0/all/0/1"&gt;Soumya Ray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ku_L/0/1/0/all/0/1"&gt;Lun-Wei Ku&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CapillaryNet: An Automated System to Analyze Microcirculation Videos from Handheld Vital Microscopy. (arXiv:2104.11574v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11574</id>
        <link href="http://arxiv.org/abs/2104.11574"/>
        <updated>2021-05-24T05:08:41.208Z</updated>
        <summary type="html"><![CDATA[Capillaries are the smallest vessels in the body responsible for the delivery
of oxygen and nutrients to the surrounding cells. Various diseases have been
shown to alter the density of nutritive capillaries and the flow velocity of
erythrocytes. In previous studies, capillary density and flow velocity have
been assessed manually by trained specialists. Manual analysis of a 20-second
long microvascular video takes on average 20 minutes and requires extensive
training. Several studies have reported that manual analysis hinders the
application of microvascular microscopy in a clinical setting. In this paper,
we present a fully automated system, called CapillaryNet, that can automate
microvascular microscopy analysis so it can be used as a clinical application.
Moreover, CapillaryNet measures several microvascular parameters that
researchers were previously unable to quantify, i.e. capillary hematocrit and
intra-capillary flow velocity heterogeneity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Helmy_M/0/1/0/all/0/1"&gt;Maged Helmy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dykyy_A/0/1/0/all/0/1"&gt;Anastasiya Dykyy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1"&gt;Tuyen Trung Truong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferreira_P/0/1/0/all/0/1"&gt;Paulo Ferreira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jul_E/0/1/0/all/0/1"&gt;Eric Jul&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic Sufficient Explanations. (arXiv:2105.10118v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10118</id>
        <link href="http://arxiv.org/abs/2105.10118"/>
        <updated>2021-05-24T05:08:41.199Z</updated>
        <summary type="html"><![CDATA[Understanding the behavior of learned classifiers is an important task, and
various black-box explanations, logical reasoning approaches, and
model-specific methods have been proposed. In this paper, we introduce
probabilistic sufficient explanations, which formulate explaining an instance
of classification as choosing the "simplest" subset of features such that only
observing those features is "sufficient" to explain the classification. That
is, sufficient to give us strong probabilistic guarantees that the model will
behave similarly when all features are observed under the data distribution. In
addition, we leverage tractable probabilistic reasoning tools such as
probabilistic circuits and expected predictions to design a scalable algorithm
for finding the desired explanations while keeping the guarantees intact. Our
experiments demonstrate the effectiveness of our algorithm in finding
sufficient explanations, and showcase its advantages compared to Anchors and
logical explanations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1"&gt;Eric Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khosravi_P/0/1/0/all/0/1"&gt;Pasha Khosravi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1"&gt;Guy Van den Broeck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Clustering and Representation Learning with Geometric Structure Preservation. (arXiv:2009.09590v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.09590</id>
        <link href="http://arxiv.org/abs/2009.09590"/>
        <updated>2021-05-24T05:08:41.162Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel framework for Deep Clustering and
multi-manifold Representation Learning (DCRL) that preserves the geometric
structure of data. In the proposed framework, manifold clustering is done in
the latent space guided by a clustering loss. To overcome the problem that
clustering-oriented losses may deteriorate the geometric structure of
embeddings in the latent space, an isometric loss is proposed for preserving
intra-manifold structure locally and a ranking loss for inter-manifold
structure globally. Experimental results on various datasets show that DCRL
leads to performances comparable to current state-of-the-art deep clustering
algorithms, yet exhibits superior performance for manifold representation. Our
results also demonstrate the importance and effectiveness of the proposed
losses in preserving geometric structure in terms of visualization and
performance metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lirong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zicheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zang_Z/0/1/0/all/0/1"&gt;Zelin Zang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1"&gt;Jun Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Siyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Stan. Z Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RL-IoT: Reinforcement Learning to Interact with IoT Devices. (arXiv:2105.00884v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00884</id>
        <link href="http://arxiv.org/abs/2105.00884"/>
        <updated>2021-05-24T05:08:41.150Z</updated>
        <summary type="html"><![CDATA[Our life is getting filled by Internet of Things (IoT) devices. These devices
often rely on closed or poorly documented protocols, with unknown formats and
semantics. Learning how to interact with such devices in an autonomous manner
is the key for interoperability and automatic verification of their
capabilities. In this paper, we propose RL-IoT, a system that explores how to
automatically interact with possibly unknown IoT devices. We leverage
reinforcement learning (RL) to recover the semantics of protocol messages and
to take control of the device to reach a given goal, while minimizing the
number of interactions. We assume to know only a database of possible IoT
protocol messages, whose semantics are however unknown. RL-IoT exchanges
messages with the target IoT device, learning those commands that are useful to
reach the given goal. Our results show that RL-IoT is able to solve both simple
and complex tasks. With properly tuned parameters, RL-IoT learns how to perform
actions with the target device, a Yeelight smart bulb in our case study,
completing non-trivial patterns with as few as 400 interactions. RL-IoT paves
the road for automatic interactions with poorly documented IoT protocols, thus
enabling interoperable systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Milan_G/0/1/0/all/0/1"&gt;Giulia Milan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vassio_L/0/1/0/all/0/1"&gt;Luca Vassio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drago_I/0/1/0/all/0/1"&gt;Idilio Drago&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mellia_M/0/1/0/all/0/1"&gt;Marco Mellia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian hierarchical stacking: Some models are (somewhere) useful. (arXiv:2101.08954v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08954</id>
        <link href="http://arxiv.org/abs/2101.08954"/>
        <updated>2021-05-24T05:08:41.136Z</updated>
        <summary type="html"><![CDATA[Stacking is a widely used model averaging technique that asymptotically
yields optimal predictions among linear averages. We show that stacking is most
effective when model predictive performance is heterogeneous in inputs, and we
can further improve the stacked mixture with a hierarchical model. We
generalize stacking to Bayesian hierarchical stacking. The model weights are
varying as a function of data, partially-pooled, and inferred using Bayesian
inference. We further incorporate discrete and continuous inputs, other
structured priors, and time series and longitudinal data. To verify the
performance gain of the proposed method, we derive theory bounds, and
demonstrate on several applied problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yuling Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pirs_G/0/1/0/all/0/1"&gt;Gregor Pir&amp;#x161;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vehtari_A/0/1/0/all/0/1"&gt;Aki Vehtari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gelman_A/0/1/0/all/0/1"&gt;Andrew Gelman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Removing the mini-batching error in Bayesian inference using Adaptive Langevin dynamics. (arXiv:2105.10347v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.10347</id>
        <link href="http://arxiv.org/abs/2105.10347"/>
        <updated>2021-05-24T05:08:41.096Z</updated>
        <summary type="html"><![CDATA[The computational cost of usual Monte Carlo methods for sampling a posteriori
laws in Bayesian inference scales linearly with the number of data points. One
option to reduce it to a fraction of this cost is to resort to mini-batching in
conjunction with unadjusted discretizations of Langevin dynamics, in which case
only a random fraction of the data is used to estimate the gradient. However,
this leads to an additional noise in the dynamics and hence a bias on the
invariant measure which is sampled by the Markov chain. We advocate using the
so-called Adaptive Langevin dynamics, which is a modification of standard
inertial Langevin dynamics with a dynamical friction which automatically
corrects for the increased noise arising from mini-batching. We investigate the
practical relevance of the assumptions underpinning Adaptive Langevin (constant
covariance for the estimation of the gradient), which are not satisfied in
typical models of Bayesian inference; and show how to extend the approach to
more general situations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sekkat_I/0/1/0/all/0/1"&gt;Inass Sekkat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Stoltz_G/0/1/0/all/0/1"&gt;Gabriel Stoltz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inverse Constrained Reinforcement Learning. (arXiv:2011.09999v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.09999</id>
        <link href="http://arxiv.org/abs/2011.09999"/>
        <updated>2021-05-24T05:08:41.089Z</updated>
        <summary type="html"><![CDATA[In real world settings, numerous constraints are present which are hard to
specify mathematically. However, for the real world deployment of reinforcement
learning (RL), it is critical that RL agents are aware of these constraints, so
that they can act safely. In this work, we consider the problem of learning
constraints from demonstrations of a constraint-abiding agent's behavior. We
experimentally validate our approach and show that our framework can
successfully learn the most likely constraints that the agent respects. We
further show that these learned constraints are \textit{transferable} to new
agents that may have different morphologies and/or reward functions. Previous
works in this regard have either mainly been restricted to tabular (discrete)
settings, specific types of constraints or assume the environment's transition
dynamics. In contrast, our framework is able to learn arbitrary
\textit{Markovian} constraints in high-dimensions in a completely model-free
setting. The code can be found it:
\url{https://github.com/shehryar-malik/icrl}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anwar_U/0/1/0/all/0/1"&gt;Usman Anwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malik_S/0/1/0/all/0/1"&gt;Shehryar Malik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aghasi_A/0/1/0/all/0/1"&gt;Alireza Aghasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1"&gt;Ali Ahmed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Knowledge Expansion. (arXiv:2103.14431v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14431</id>
        <link href="http://arxiv.org/abs/2103.14431"/>
        <updated>2021-05-24T05:08:41.068Z</updated>
        <summary type="html"><![CDATA[The popularity of multimodal sensors and the accessibility of the Internet
have brought us a massive amount of unlabeled multimodal data. Since existing
datasets and well-trained models are primarily unimodal, the modality gap
between a unimodal network and unlabeled multimodal data poses an interesting
problem: how to transfer a pre-trained unimodal network to perform the same
task on unlabeled multimodal data? In this work, we propose multimodal
knowledge expansion (MKE), a knowledge distillation-based framework to
effectively utilize multimodal data without requiring labels. Opposite to
traditional knowledge distillation, where the student is designed to be
lightweight and inferior to the teacher, we observe that a multimodal student
model consistently denoises pseudo labels and generalizes better than its
teacher. Extensive experiments on four tasks and different modalities verify
this finding. Furthermore, we connect the mechanism of MKE to semi-supervised
learning and offer both empirical and theoretical explanations to understand
the denoising capability of a multimodal student.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1"&gt;Zihui Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Sucheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhengqi Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hang Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Particle gradient descent model for point process generation. (arXiv:2010.14928v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14928</id>
        <link href="http://arxiv.org/abs/2010.14928"/>
        <updated>2021-05-24T05:08:41.039Z</updated>
        <summary type="html"><![CDATA[This paper introduces a generative model for planar point processes in a
square window, built upon a single realization of a stationary, ergodic point
process observed in this window. Inspired by recent advances in gradient
descent methods for maximum entropy models, we propose a method to generate
similar point patterns by jointly moving particles of an initial Poisson
configuration towards a target counting measure. The target measure is
generated via a deterministic gradient descent algorithm, so as to match a set
of statistics of the given, observed realization. Our statistics are estimators
of the multi-scale wavelet phase harmonic covariance, recently proposed in
image modeling. They allow one to capture geometric structures through
multi-scale interactions between wavelet coefficients. Both our statistics and
the gradient descent algorithm scale better with the number of observed points
than the classical k-nearest neighbour distances previously used in generative
models for point processes, based on the rejection sampling or
simulated-annealing. The overall quality of our model is evaluated on point
processes with various geometric structures through spectral and topological
data analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Brochard_A/0/1/0/all/0/1"&gt;Antoine Brochard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Blaszczyszyn_B/0/1/0/all/0/1"&gt;Bart&amp;#x142;omiej B&amp;#x142;aszczyszyn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mallat_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phane Mallat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Sixin Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CapillaryNet: An Automated System to Analyze Microcirculation Videos from Handheld Vital Microscopy. (arXiv:2104.11574v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11574</id>
        <link href="http://arxiv.org/abs/2104.11574"/>
        <updated>2021-05-24T05:08:41.032Z</updated>
        <summary type="html"><![CDATA[Capillaries are the smallest vessels in the body responsible for the delivery
of oxygen and nutrients to the surrounding cells. Various diseases have been
shown to alter the density of nutritive capillaries and the flow velocity of
erythrocytes. In previous studies, capillary density and flow velocity have
been assessed manually by trained specialists. Manual analysis of a 20-second
long microvascular video takes on average 20 minutes and requires extensive
training. Several studies have reported that manual analysis hinders the
application of microvascular microscopy in a clinical setting. In this paper,
we present a fully automated system, called CapillaryNet, that can automate
microvascular microscopy analysis so it can be used as a clinical application.
Moreover, CapillaryNet measures several microvascular parameters that
researchers were previously unable to quantify, i.e. capillary hematocrit and
intra-capillary flow velocity heterogeneity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Helmy_M/0/1/0/all/0/1"&gt;Maged Helmy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dykyy_A/0/1/0/all/0/1"&gt;Anastasiya Dykyy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1"&gt;Tuyen Trung Truong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferreira_P/0/1/0/all/0/1"&gt;Paulo Ferreira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jul_E/0/1/0/all/0/1"&gt;Eric Jul&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Scalable Modeling of Biology in Event-B. (arXiv:2105.10344v1 [q-bio.MN])]]></title>
        <id>http://arxiv.org/abs/2105.10344</id>
        <link href="http://arxiv.org/abs/2105.10344"/>
        <updated>2021-05-24T05:08:41.025Z</updated>
        <summary type="html"><![CDATA[Biology offers many examples of large-scale, complex, concurrent systems:
many processes take place in parallel, compete on resources and influence each
other's behavior. The scalable modeling of biological systems continues to be a
very active field of research. In this paper we introduce a new approach based
on Event-B, a state-based formal method with refinement as its central
ingredient, allowing us to check for model consistency step-by-step in an
automated way. Our approach based on functions leads to an elegant and concise
modeling method. We demonstrate this approach by constructing what is, to our
knowledge, the largest ever built Event-B model, describing the ErbB signaling
pathway, a key evolutionary pathway with a significant role in development and
in many types of cancer. The Event-B model for the ErbB pathway describes 1320
molecular reactions through 242 events.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Sanwal_U/0/1/0/all/0/1"&gt;Usman Sanwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hoang_T/0/1/0/all/0/1"&gt;Thai Son Hoang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Petre_L/0/1/0/all/0/1"&gt;Luigia Petre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Petre_I/0/1/0/all/0/1"&gt;Ion Petre&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient PAC Reinforcement Learning in Regular Decision Processes. (arXiv:2105.06784v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06784</id>
        <link href="http://arxiv.org/abs/2105.06784"/>
        <updated>2021-05-24T05:08:41.018Z</updated>
        <summary type="html"><![CDATA[Recently regular decision processes have been proposed as a well-behaved form
of non-Markov decision process. Regular decision processes are characterised by
a transition function and a reward function that depend on the whole history,
though regularly (as in regular languages). In practice both the transition and
the reward functions can be seen as finite transducers. We study reinforcement
learning in regular decision processes. Our main contribution is to show that a
near-optimal policy can be PAC-learned in polynomial time in a set of
parameters that describe the underlying decision process. We argue that the
identified set of parameters is minimal and it reasonably captures the
difficulty of a regular decision process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ronca_A/0/1/0/all/0/1"&gt;Alessandro Ronca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giacomo_G/0/1/0/all/0/1"&gt;Giuseppe De Giacomo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Covariance-Free Sparse Bayesian Learning. (arXiv:2105.10439v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2105.10439</id>
        <link href="http://arxiv.org/abs/2105.10439"/>
        <updated>2021-05-24T05:08:41.001Z</updated>
        <summary type="html"><![CDATA[Sparse Bayesian learning (SBL) is a powerful framework for tackling the
sparse coding problem while also providing uncertainty quantification. However,
the most popular inference algorithms for SBL become too expensive for
high-dimensional problems due to the need to maintain a large covariance
matrix. To resolve this issue, we introduce a new SBL inference algorithm that
avoids explicit computation of the covariance matrix, thereby saving
significant time and space. Instead of performing costly matrix inversions, our
covariance-free method solves multiple linear systems to obtain provably
unbiased estimates of the posterior statistics needed by SBL. These systems can
be solved in parallel, enabling further acceleration of the algorithm via
graphics processing units. In practice, our method can be up to thousands of
times faster than existing baselines, reducing hours of computation time to
seconds. We showcase how our new algorithm enables SBL to tractably tackle
high-dimensional signal recovery problems, such as deconvolution of calcium
imaging data and multi-contrast reconstruction of magnetic resonance images.
Finally, we open-source a toolbox containing all of our implementations to
drive future research in SBL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lin_A/0/1/0/all/0/1"&gt;Alexander Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Song_A/0/1/0/all/0/1"&gt;Andrew H. Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bilgic_B/0/1/0/all/0/1"&gt;Berkin Bilgic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ba_D/0/1/0/all/0/1"&gt;Demba Ba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning for Sparse-Reward Object-Interaction Tasks in a First-person Simulated 3D Environment. (arXiv:2010.15195v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.15195</id>
        <link href="http://arxiv.org/abs/2010.15195"/>
        <updated>2021-05-24T05:08:40.993Z</updated>
        <summary type="html"><![CDATA[First-person object-interaction tasks in high-fidelity, 3D, simulated
environments such as the AI2Thor virtual home-environment pose significant
sample-efficiency challenges for reinforcement learning (RL) agents learning
from sparse task rewards. To alleviate these challenges, prior work has
provided extensive supervision via a combination of reward-shaping,
ground-truth object-information, and expert demonstrations. In this work, we
show that one can learn object-interaction tasks from scratch without
supervision by learning an attentive object-model as an auxiliary task during
task learning with an object-centric relational RL agent. Our key insight is
that learning an object-model that incorporates object-attention into forward
prediction provides a dense learning signal for unsupervised representation
learning of both objects and their relationships. This, in turn, enables faster
policy learning for an object-centric relational RL agent. We demonstrate our
agent by introducing a set of challenging object-interaction tasks in the
AI2Thor environment where learning with our attentive object-model is key to
strong performance. Specifically, we compare our agent and relational RL agents
with alternative auxiliary tasks to a relational RL agent equipped with
ground-truth object-information, and show that learning with our object-model
best closes the performance gap in terms of both learning speed and maximum
success rate. Additionally, we find that incorporating object-attention into an
object-model's forward predictions is key to learning representations which
capture object-category and object-state.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carvalho_W/0/1/0/all/0/1"&gt;Wilka Carvalho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_A/0/1/0/all/0/1"&gt;Anthony Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kimin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1"&gt;Sungryull Sohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Honglak Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_R/0/1/0/all/0/1"&gt;Richard L. Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Satinder Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Error Bounds of the Invariant Statistics in Machine Learning of Ergodic It\^o Diffusions. (arXiv:2105.10102v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10102</id>
        <link href="http://arxiv.org/abs/2105.10102"/>
        <updated>2021-05-24T05:08:40.984Z</updated>
        <summary type="html"><![CDATA[This paper studies the theoretical underpinnings of machine learning of
ergodic It\^o diffusions. The objective is to understand the convergence
properties of the invariant statistics when the underlying system of stochastic
differential equations (SDEs) is empirically estimated with a supervised
regression framework. Using the perturbation theory of ergodic Markov chains
and the linear response theory, we deduce a linear dependence of the errors of
one-point and two-point invariant statistics on the error in the learning of
the drift and diffusion coefficients. More importantly, our study shows that
the usual $L^2$-norm characterization of the learning generalization error is
insufficient for achieving this linear dependence result. We find that
sufficient conditions for such a linear dependence result are through learning
algorithms that produce a uniformly Lipschitz and consistent estimator in the
hypothesis space that retains certain characteristics of the drift
coefficients, such as the usual linear growth condition that guarantees the
existence of solutions of the underlying SDEs. We examine these conditions on
two well-understood learning algorithms: the kernel-based spectral regression
method and the shallow random neural networks with the ReLU activation
function.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;He Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harlim_J/0/1/0/all/0/1"&gt;John Harlim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiantao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Halluci-Net: Scene Completion by Exploiting Object Co-occurrence Relationships. (arXiv:2004.08614v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.08614</id>
        <link href="http://arxiv.org/abs/2004.08614"/>
        <updated>2021-05-24T05:08:40.959Z</updated>
        <summary type="html"><![CDATA[Recently, there has been substantial progress in image synthesis from
semantic labelmaps. However, methods used for this task assume the availability
of complete and unambiguous labelmaps, with instance boundaries of objects, and
class labels for each pixel. This reliance on heavily annotated inputs
restricts the application of image synthesis techniques to real-world
applications, especially under uncertainty due to weather, occlusion, or noise.
On the other hand, algorithms that can synthesize images from sparse labelmaps
or sketches are highly desirable as tools that can guide content creators and
artists to quickly generate scenes by simply specifying locations of a few
objects. In this paper, we address the problem of complex scene completion from
sparse labelmaps. Under this setting, very few details about the scene (30\% of
object instances) are available as input for image synthesis. We propose a
two-stage deep network based method, called `Halluci-Net', that learns
co-occurence relationships between objects in scenes, and then exploits these
relationships to produce a dense and complete labelmap. The generated dense
labelmap can then be used as input by state-of-the-art image synthesis
techniques like pix2pixHD to obtain the final image. The proposed method is
evaluated on the Cityscapes dataset and it outperforms two baselines methods on
performance metrics like Fr\'echet Inception Distance (FID), semantic
segmentation accuracy, and similarity in object co-occurrences. We also show
qualitative results on a subset of ADE20K dataset that contains bedroom images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_K/0/1/0/all/0/1"&gt;Kuldeep Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1"&gt;Tejas Gokhale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rajhans Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turaga_P/0/1/0/all/0/1"&gt;Pavan Turaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sankaranarayanan_A/0/1/0/all/0/1"&gt;Aswin Sankaranarayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Recommender Systems at Scale: Communication-Efficient Model and Data Parallelism. (arXiv:2010.08899v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08899</id>
        <link href="http://arxiv.org/abs/2010.08899"/>
        <updated>2021-05-24T05:08:40.952Z</updated>
        <summary type="html"><![CDATA[In this paper, we consider hybrid parallelism -- a paradigm that employs both
Data Parallelism (DP) and Model Parallelism (MP) -- to scale distributed
training of large recommendation models. We propose a compression framework
called Dynamic Communication Thresholding (DCT) for communication-efficient
hybrid training. DCT filters the entities to be communicated across the network
through a simple hard-thresholding function, allowing only the most relevant
information to pass through. For communication efficient DP, DCT compresses the
parameter gradients sent to the parameter server during model synchronization.
The threshold is updated only once every few thousand iterations to reduce the
computational overhead of compression. For communication efficient MP, DCT
incorporates a novel technique to compress the activations and gradients sent
across the network during the forward and backward propagation, respectively.
This is done by identifying and updating only the most relevant neurons of the
neural network for each training sample in the data. We evaluate DCT on
publicly available natural language processing and recommender models and
datasets, as well as recommendation systems used in production at Facebook. DCT
reduces communication by at least $100\times$ and $20\times$ during DP and MP,
respectively. The algorithm has been deployed in production, and it improves
end-to-end training time for a state-of-the-art industrial recommender model by
37\%, without any loss in performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1"&gt;Vipul Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhary_D/0/1/0/all/0/1"&gt;Dhruv Choudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_P/0/1/0/all/0/1"&gt;Ping Tak Peter Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xiaohan Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yuzhen Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kejariwal_A/0/1/0/all/0/1"&gt;Arun Kejariwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramchandran_K/0/1/0/all/0/1"&gt;Kannan Ramchandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1"&gt;Michael W. Mahoney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoolMomentum: A Method for Stochastic Optimization by Langevin Dynamics with Simulated Annealing. (arXiv:2005.14605v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.14605</id>
        <link href="http://arxiv.org/abs/2005.14605"/>
        <updated>2021-05-24T05:08:40.934Z</updated>
        <summary type="html"><![CDATA[Deep learning applications require global optimization of non-convex
objective functions, which have multiple local minima. The same problem is
often found in physical simulations and may be resolved by the methods of
Langevin dynamics with Simulated Annealing, which is a well-established
approach for minimization of many-particle potentials. This analogy provides
useful insights for non-convex stochastic optimization in machine learning.
Here we find that integration of the discretized Langevin equation gives a
coordinate updating rule equivalent to the famous Momentum optimization
algorithm. As a main result, we show that a gradual decrease of the momentum
coefficient from the initial value close to unity until zero is equivalent to
application of Simulated Annealing or slow cooling, in physical terms. Making
use of this novel approach, we propose CoolMomentum -- a new stochastic
optimization method. Applying Coolmomentum to optimization of Resnet-20 on
Cifar-10 dataset and Efficientnet-B0 on Imagenet, we demonstrate that it is
able to achieve high accuracies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Borysenko_O/0/1/0/all/0/1"&gt;Oleksandr Borysenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Byshkin_M/0/1/0/all/0/1"&gt;Maksym Byshkin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Horizon Forecasting for Limit Order Books: Novel Deep Learning Approaches and Hardware Acceleration using Intelligent Processing Units. (arXiv:2105.10430v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10430</id>
        <link href="http://arxiv.org/abs/2105.10430"/>
        <updated>2021-05-24T05:08:40.927Z</updated>
        <summary type="html"><![CDATA[We design multi-horizon forecasting models for limit order book (LOB) data by
using deep learning techniques. Unlike standard structures where a single
prediction is made, we adopt encoder-decoder models with sequence-to-sequence
and Attention mechanisms, to generate a forecasting path. Our methods achieve
comparable performance to state-of-art algorithms at short prediction horizons.
Importantly, they outperform when generating predictions over long horizons by
leveraging the multi-horizon setup. Given that encoder-decoder models rely on
recurrent neural layers, they generally suffer from a slow training process. To
remedy this, we experiment with utilising novel hardware, so-called Intelligent
Processing Units (IPUs) produced by Graphcore. IPUs are specifically designed
for machine intelligence workload with the aim to speed up the computation
process. We show that in our setup this leads to significantly faster training
times when compared to training models with GPUs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zihao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zohren_S/0/1/0/all/0/1"&gt;Stefan Zohren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online DR-Submodular Maximization with Stochastic Cumulative Constraints. (arXiv:2005.14708v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.14708</id>
        <link href="http://arxiv.org/abs/2005.14708"/>
        <updated>2021-05-24T05:08:40.916Z</updated>
        <summary type="html"><![CDATA[In this paper, we consider online continuous DR-submodular maximization with
linear stochastic long-term constraints. Compared to the prior work on online
submodular maximization, our setting introduces the extra complication of
stochastic linear constraint functions that are i.i.d. generated at each round.
To be precise, at step $t\in\{1,\dots,T\}$, a DR-submodular utility function
$f_t(\cdot)$ and a constraint vector $p_t$, i.i.d. generated from an unknown
distribution with mean $p$, are revealed after committing to an action $x_t$
and we aim to maximize the overall utility while the expected cumulative
resource consumption $\sum_{t=1}^T \langle p,x_t\rangle$ is below a fixed
budget $B_T$. Stochastic long-term constraints arise naturally in applications
where there is a limited budget or resource available and resource consumption
at each step is governed by stochastically time-varying environments. We
propose the Online Lagrangian Frank-Wolfe (OLFW) algorithm to solve this class
of online problems. We analyze the performance of the OLFW algorithm and we
obtain sub-linear regret bounds as well as sub-linear cumulative constraint
violation bounds, both in expectation and with high probability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Raut_P/0/1/0/all/0/1"&gt;Prasanna Sanjay Raut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sadeghi_O/0/1/0/all/0/1"&gt;Omid Sadeghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Fazel_M/0/1/0/all/0/1"&gt;Maryam Fazel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Effects of Linguistic Properties. (arXiv:2010.12919v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12919</id>
        <link href="http://arxiv.org/abs/2010.12919"/>
        <updated>2021-05-24T05:08:40.908Z</updated>
        <summary type="html"><![CDATA[We consider the problem of using observational data to estimate the causal
effects of linguistic properties. For example, does writing a complaint
politely lead to a faster response time? How much will a positive product
review increase sales? This paper addresses two technical challenges related to
the problem before developing a practical method. First, we formalize the
causal quantity of interest as the effect of a writer's intent, and establish
the assumptions necessary to identify this from observational data. Second, in
practice, we only have access to noisy proxies for the linguistic properties of
interest -- e.g., predictions from classifiers and lexicons. We propose an
estimator for this setting and prove that its bias is bounded when we perform
an adjustment for the text. Based on these results, we introduce TextCause, an
algorithm for estimating causal effects of linguistic properties. The method
leverages (1) distant supervision to improve the quality of noisy proxies, and
(2) a pre-trained language model (BERT) to adjust for the text. We show that
the proposed method outperforms related approaches when estimating the effect
of Amazon review sentiment on semi-simulated sales figures. Finally, we present
an applied case study investigating the effects of complaint politeness on
bureaucratic response times.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pryzant_R/0/1/0/all/0/1"&gt;Reid Pryzant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Card_D/0/1/0/all/0/1"&gt;Dallas Card&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1"&gt;Dan Jurafsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veitch_V/0/1/0/all/0/1"&gt;Victor Veitch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sridhar_D/0/1/0/all/0/1"&gt;Dhanya Sridhar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-driven Weight Initialization with Sylvester Solvers. (arXiv:2105.10335v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2105.10335</id>
        <link href="http://arxiv.org/abs/2105.10335"/>
        <updated>2021-05-24T05:08:40.901Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a data-driven scheme to initialize the parameters of
a deep neural network. This is in contrast to traditional approaches which
randomly initialize parameters by sampling from transformed standard
distributions. Such methods do not use the training data to produce a more
informed initialization. Our method uses a sequential layer-wise approach where
each layer is initialized using its input activations. The initialization is
cast as an optimization problem where we minimize a combination of encoding and
decoding losses of the input activations, which is further constrained by a
user-defined latent code. The optimization problem is then restructured into
the well-known Sylvester equation, which has fast and efficient gradient-free
solutions. Our data-driven method achieves a boost in performance compared to
random initialization methods, both before start of training and after training
is over. We show that our proposed method is especially effective in few-shot
and fine-tuning settings. We conclude this paper with analyses on time
complexity and the effect of different latent codes on the recognition
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1"&gt;Debasmit Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhalgat_Y/0/1/0/all/0/1"&gt;Yash Bhalgat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1"&gt;Fatih Porikli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beware the Black-Box: on the Robustness of Recent Defenses to Adversarial Examples. (arXiv:2006.10876v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.10876</id>
        <link href="http://arxiv.org/abs/2006.10876"/>
        <updated>2021-05-24T05:08:40.883Z</updated>
        <summary type="html"><![CDATA[Many defenses have recently been proposed at venues like NIPS, ICML, ICLR and
CVPR. These defenses are mainly focused on mitigating white-box attacks. They
do not properly examine black-box attacks. In this paper, we expand upon the
analysis of these defenses to include adaptive black-box adversaries. Our
evaluation is done on nine defenses including Barrage of Random Transforms,
ComDefend, Ensemble Diversity, Feature Distillation, The Odds are Odd, Error
Correcting Codes, Distribution Classifier Defense, K-Winner Take All and Buffer
Zones. Our investigation is done using two black-box adversarial models and six
widely studied adversarial attacks for CIFAR-10 and Fashion-MNIST datasets. Our
analyses show most recent defenses (7 out of 9) provide only marginal
improvements in security ($<25\%$), as compared to undefended networks. For
every defense, we also show the relationship between the amount of data the
adversary has at their disposal, and the effectiveness of adaptive black-box
attacks. Overall, our results paint a clear picture: defenses need both
thorough white-box and black-box analyses to be considered secure. We provide
this large scale study and analyses to motivate the field to move towards the
development of more robust black-box defenses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_K/0/1/0/all/0/1"&gt;Kaleel Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurevin_D/0/1/0/all/0/1"&gt;Deniz Gurevin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dijk_M/0/1/0/all/0/1"&gt;Marten van Dijk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1"&gt;Phuong Ha Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Correlated Input-Dependent Label Noise in Large-Scale Image Classification. (arXiv:2105.10305v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10305</id>
        <link href="http://arxiv.org/abs/2105.10305"/>
        <updated>2021-05-24T05:08:40.876Z</updated>
        <summary type="html"><![CDATA[Large scale image classification datasets often contain noisy labels. We take
a principled probabilistic approach to modelling input-dependent, also known as
heteroscedastic, label noise in these datasets. We place a multivariate Normal
distributed latent variable on the final hidden layer of a neural network
classifier. The covariance matrix of this latent variable, models the aleatoric
uncertainty due to label noise. We demonstrate that the learned covariance
structure captures known sources of label noise between semantically similar
and co-occurring classes. Compared to standard neural network training and
other baselines, we show significantly improved accuracy on Imagenet ILSVRC
2012 79.3% (+2.6%), Imagenet-21k 47.0% (+1.1%) and JFT 64.7% (+1.6%). We set a
new state-of-the-art result on WebVision 1.0 with 76.6% top-1 accuracy. These
datasets range from over 1M to over 300M training examples and from 1k classes
to more than 21k classes. Our method is simple to use, and we provide an
implementation that is a drop-in replacement for the final fully-connected
layer in a deep classifier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Collier_M/0/1/0/all/0/1"&gt;Mark Collier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1"&gt;Basil Mustafa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kokiopoulou_E/0/1/0/all/0/1"&gt;Efi Kokiopoulou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jenatton_R/0/1/0/all/0/1"&gt;Rodolphe Jenatton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berent_J/0/1/0/all/0/1"&gt;Jesse Berent&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evening the Score: Targeting SARS-CoV-2 Protease Inhibition in Graph Generative Models for Therapeutic Candidates. (arXiv:2105.10489v1 [q-bio.BM])]]></title>
        <id>http://arxiv.org/abs/2105.10489</id>
        <link href="http://arxiv.org/abs/2105.10489"/>
        <updated>2021-05-24T05:08:40.870Z</updated>
        <summary type="html"><![CDATA[We examine a pair of graph generative models for the therapeutic design of
novel drug candidates targeting SARS-CoV-2 viral proteins. Due to a sense of
urgency, we chose well-validated models with unique strengths: an autoencoder
that generates molecules with similar structures to a dataset of drugs with
anti-SARS activity and a reinforcement learning algorithm that generates highly
novel molecules. During generation, we explore optimization toward several
design targets to balance druglikeness, synthetic accessability, and anti-SARS
activity based on \icfifty. This generative
framework\footnote{https://github.com/exalearn/covid-drug-design} will
accelerate drug discovery in future pandemics through the high-throughput
generation of targeted therapeutic candidates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Bilbrey_J/0/1/0/all/0/1"&gt;Jenna Bilbrey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ward_L/0/1/0/all/0/1"&gt;Logan Ward&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Choudhury_S/0/1/0/all/0/1"&gt;Sutanay Choudhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kumar_N/0/1/0/all/0/1"&gt;Neeraj Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Sivaraman_G/0/1/0/all/0/1"&gt;Ganesh Sivaraman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Detection of Abnormal EEGs in Epilepsy With a Compact and Efficient CNN Model. (arXiv:2105.10358v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2105.10358</id>
        <link href="http://arxiv.org/abs/2105.10358"/>
        <updated>2021-05-24T05:08:40.863Z</updated>
        <summary type="html"><![CDATA[Electroencephalography (EEG) is essential for the diagnosis of epilepsy, but
it requires expertise and experience to identify abnormalities. It is thus
crucial to develop automated models for the detection of abnormal EEGs related
to epilepsy. This paper describes the development of a novel class of compact
and efficient convolutional neural networks (CNNs) for detecting abnormal time
intervals and electrodes in EEGs for epilepsy. The designed model is inspired
by a CNN developed for brain-computer interfacing called multichannel EEGNet
(mEEGNet). Unlike the EEGNet, the proposed model, mEEGNet, has the same number
of electrode inputs and outputs to detect abnormalities. The mEEGNet was
evaluated with a clinical dataset consisting of 29 cases of juvenile and
childhood absence epilepsy labeled by a clinical expert. The labels were given
to paroxysmal discharges visually observed in both ictal (seizure) and
interictal (nonseizure) intervals. Results showed that the mEEGNet detected
abnormal EEGs with the area under the curve, F1-values, and sensitivity
equivalent to or higher than those of existing CNNs. Moreover, the number of
parameters is much smaller than other CNN models. To our knowledge, the dataset
of absence epilepsy validated with machine learning through this research is
the largest in the literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shoji_T/0/1/0/all/0/1"&gt;Taku Shoji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yoshida_N/0/1/0/all/0/1"&gt;Noboru Yoshida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tanaka_T/0/1/0/all/0/1"&gt;Toshihisa Tanaka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Understanding for Field and Service Robots in a Priori Unknown Environments. (arXiv:2105.10396v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.10396</id>
        <link href="http://arxiv.org/abs/2105.10396"/>
        <updated>2021-05-24T05:08:40.831Z</updated>
        <summary type="html"><![CDATA[Contemporary approaches to perception, planning, estimation, and control have
allowed robots to operate robustly as our remote surrogates in uncertain,
unstructured environments. There is now an opportunity for robots to operate
not only in isolation, but also with and alongside humans in our complex
environments. Natural language provides an efficient and flexible medium
through which humans can communicate with collaborative robots. Through
significant progress in statistical methods for natural language understanding,
robots are now able to interpret a diverse array of free-form navigation,
manipulation, and mobile manipulation commands. However, most contemporary
approaches require a detailed prior spatial-semantic map of the robot's
environment that models the space of possible referents of the utterance.
Consequently, these methods fail when robots are deployed in new, previously
unknown, or partially observed environments, particularly when mental models of
the environment differ between the human operator and the robot. This paper
provides a comprehensive description of a novel learning framework that allows
field and service robots to interpret and correctly execute natural language
instructions in a priori unknown, unstructured environments. Integral to our
approach is its use of language as a "sensor" -- inferring spatial,
topological, and semantic information implicit in natural language utterances
and then exploiting this information to learn a distribution over a latent
environment model. We incorporate this distribution in a probabilistic language
grounding model and infer a distribution over a symbolic representation of the
robot's action space. We use imitation learning to identify a belief space
policy that reasons over the environment and behavior distributions. We
evaluate our framework through a variety of different navigation and mobile
manipulation experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Walter_M/0/1/0/all/0/1"&gt;Matthew R. Walter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patki_S/0/1/0/all/0/1"&gt;Siddharth Patki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daniele_A/0/1/0/all/0/1"&gt;Andrea F. Daniele&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fahnestock_E/0/1/0/all/0/1"&gt;Ethan Fahnestock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duvallet_F/0/1/0/all/0/1"&gt;Felix Duvallet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hemachandra_S/0/1/0/all/0/1"&gt;Sachithra Hemachandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1"&gt;Jean Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stentz_A/0/1/0/all/0/1"&gt;Anthony Stentz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_N/0/1/0/all/0/1"&gt;Nicholas Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Howard_T/0/1/0/all/0/1"&gt;Thomas M. Howard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LDP-FL: Practical Private Aggregation in Federated Learning with Local Differential Privacy. (arXiv:2007.15789v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.15789</id>
        <link href="http://arxiv.org/abs/2007.15789"/>
        <updated>2021-05-24T05:08:40.812Z</updated>
        <summary type="html"><![CDATA[Train machine learning models on sensitive user data has raised increasing
privacy concerns in many areas. Federated learning is a popular approach for
privacy protection that collects the local gradient information instead of real
data. One way to achieve a strict privacy guarantee is to apply local
differential privacy into federated learning. However, previous works do not
give a practical solution due to three issues. First, the noisy data is close
to its original value with high probability, increasing the risk of information
exposure. Second, a large variance is introduced to the estimated average,
causing poor accuracy. Last, the privacy budget explodes due to the high
dimensionality of weights in deep learning models. In this paper, we proposed a
novel design of local differential privacy mechanism for federated learning to
address the abovementioned issues. It is capable of making the data more
distinct from its original value and introducing lower variance. Moreover, the
proposed mechanism bypasses the curse of dimensionality by splitting and
shuffling model updates. A series of empirical evaluations on three commonly
used datasets, MNIST, Fashion-MNIST and CIFAR-10, demonstrate that our solution
can not only achieve superior deep learning performance but also provide a
strong privacy guarantee at the same time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lichao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1"&gt;Jianwei Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xun Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maximum and Leaky Maximum Propagation. (arXiv:2105.10277v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10277</id>
        <link href="http://arxiv.org/abs/2105.10277"/>
        <updated>2021-05-24T05:08:40.800Z</updated>
        <summary type="html"><![CDATA[In this work, we present an alternative to conventional residual connections,
which is inspired by maxout nets. This means that instead of the addition in
residual connections, our approach only propagates the maximum value or, in the
leaky formulation, propagates a percentage of both. In our evaluation, we show
on different public data sets that the presented approaches are comparable to
the residual connections and have other interesting properties, such as better
generalization with a constant batch normalization, faster learning, and also
the possibility to generalize without additional activation functions. In
addition, the proposed approaches work very well if ensembles together with
residual networks are formed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fuhl_W/0/1/0/all/0/1"&gt;Wolfgang Fuhl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Federated Learning in Phishing Email Detection. (arXiv:2007.13300v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.13300</id>
        <link href="http://arxiv.org/abs/2007.13300"/>
        <updated>2021-05-24T05:08:40.795Z</updated>
        <summary type="html"><![CDATA[The use of Artificial Intelligence (AI) to detect phishing emails is
primarily dependent on large-scale centralized datasets, which opens it up to a
myriad of privacy, trust, and legal issues. Moreover, organizations are loathed
to share emails, given the risk of leakage of commercially sensitive
information. So, it is uncommon to obtain sufficient emails to train a global
AI model efficiently. Accordingly, privacy-preserving distributed and
collaborative machine learning, particularly Federated Learning (FL), is a
desideratum. Already prevalent in the healthcare sector, questions remain
regarding the effectiveness and efficacy of FL-based phishing detection within
the context of multi-organization collaborations. To the best of our knowledge,
the work herein is the first to investigate the use of FL in email
anti-phishing. This paper builds upon a deep neural network model, particularly
RNN and BERT for phishing email detection. It analyzes the FL-entangled
learning performance under various settings, including balanced and
asymmetrical data distribution. Our results corroborate comparable performance
statistics of FL in phishing email detection to centralized learning for
balanced datasets, and low organization counts. Moreover, we observe a
variation in performance when increasing organizational counts. For a fixed
total email dataset, the global RNN based model suffers by a 1.8% accuracy drop
when increasing organizational counts from 2 to 10. In contrast, BERT accuracy
rises by 0.6% when going from 2 to 5 organizations. However, if we allow
increasing the overall email dataset with the introduction of new organizations
in the FL framework, the organizational level performance is improved by
achieving a faster convergence speed. Besides, FL suffers in its overall global
model performance due to highly unstable outputs if the email dataset
distribution is highly asymmetric.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thapa_C/0/1/0/all/0/1"&gt;Chandra Thapa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jun Wen Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abuadbba_A/0/1/0/all/0/1"&gt;Alsharif Abuadbba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yansong Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camtepe_S/0/1/0/all/0/1"&gt;Seyit Camtepe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nepal_S/0/1/0/all/0/1"&gt;Surya Nepal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Almashor_M/0/1/0/all/0/1"&gt;Mahathir Almashor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yifeng Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Generative Adversarial Networks via stochastic Nash games. (arXiv:2010.10013v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10013</id>
        <link href="http://arxiv.org/abs/2010.10013"/>
        <updated>2021-05-24T05:08:40.782Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) are a class of generative models with
two antagonistic neural networks: a generator and a discriminator. These two
neural networks compete against each other through an adversarial process that
can be modeled as a stochastic Nash equilibrium problem. Since the associated
training process is challenging, it is fundamental to design reliable
algorithms to compute an equilibrium. In this paper, we propose a stochastic
relaxed forward-backward (SRFB) algorithm for GANs and we show convergence to
an exact solution when an increasing number of data is available. We also show
convergence of an averaged variant of the SRFB algorithm to a neighborhood of
the solution when only few samples are available. In both cases, convergence is
guaranteed when the pseudogradient mapping of the game is monotone. This
assumption is among the weakest known in the literature. Moreover, we apply our
algorithm to the image generation problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Franci_B/0/1/0/all/0/1"&gt;Barbara Franci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grammatico_S/0/1/0/all/0/1"&gt;Sergio Grammatico&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Multilingual Sentence Embeddings for Parallel Corpus Mining. (arXiv:2105.10419v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10419</id>
        <link href="http://arxiv.org/abs/2105.10419"/>
        <updated>2021-05-24T05:08:40.774Z</updated>
        <summary type="html"><![CDATA[Existing models of multilingual sentence embeddings require large parallel
data resources which are not available for low-resource languages. We propose a
novel unsupervised method to derive multilingual sentence embeddings relying
only on monolingual data. We first produce a synthetic parallel corpus using
unsupervised machine translation, and use it to fine-tune a pretrained
cross-lingual masked language model (XLM) to derive the multilingual sentence
representations. The quality of the representations is evaluated on two
parallel corpus mining tasks with improvements of up to 22 F1 points over
vanilla XLM. In addition, we observe that a single synthetic bilingual corpus
is able to improve results for other language pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kvapilikova_I/0/1/0/all/0/1"&gt;Ivana Kvapil&amp;#x131;kova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1"&gt;Mikel Artetxe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labaka_G/0/1/0/all/0/1"&gt;Gorka Labaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1"&gt;Eneko Agirre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1"&gt;Ond&amp;#x159;ej Bojar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Knowledge Expansion. (arXiv:2103.14431v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14431</id>
        <link href="http://arxiv.org/abs/2103.14431"/>
        <updated>2021-05-24T05:08:40.756Z</updated>
        <summary type="html"><![CDATA[The popularity of multimodal sensors and the accessibility of the Internet
have brought us a massive amount of unlabeled multimodal data. Since existing
datasets and well-trained models are primarily unimodal, the modality gap
between a unimodal network and unlabeled multimodal data poses an interesting
problem: how to transfer a pre-trained unimodal network to perform the same
task on unlabeled multimodal data? In this work, we propose multimodal
knowledge expansion (MKE), a knowledge distillation-based framework to
effectively utilize multimodal data without requiring labels. Opposite to
traditional knowledge distillation, where the student is designed to be
lightweight and inferior to the teacher, we observe that a multimodal student
model consistently denoises pseudo labels and generalizes better than its
teacher. Extensive experiments on four tasks and different modalities verify
this finding. Furthermore, we connect the mechanism of MKE to semi-supervised
learning and offer both empirical and theoretical explanations to understand
the denoising capability of a multimodal student.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1"&gt;Zihui Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Sucheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhengqi Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hang Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Don't Do What Doesn't Matter: Intrinsic Motivation with Action Usefulness. (arXiv:2105.09992v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09992</id>
        <link href="http://arxiv.org/abs/2105.09992"/>
        <updated>2021-05-24T05:08:40.748Z</updated>
        <summary type="html"><![CDATA[Sparse rewards are double-edged training signals in reinforcement learning:
easy to design but hard to optimize. Intrinsic motivation guidances have thus
been developed toward alleviating the resulting exploration problem. They
usually incentivize agents to look for new states through novelty signals. Yet,
such methods encourage exhaustive exploration of the state space rather than
focusing on the environment's salient interaction opportunities. We propose a
new exploration method, called Don't Do What Doesn't Matter (DoWhaM), shifting
the emphasis from state novelty to state with relevant actions. While most
actions consistently change the state when used, \textit{e.g.} moving the
agent, some actions are only effective in specific states, \textit{e.g.},
\emph{opening} a door, \emph{grabbing} an object. DoWhaM detects and rewards
actions that seldom affect the environment. We evaluate DoWhaM on the
procedurally-generated environment MiniGrid, against state-of-the-art methods
and show that DoWhaM greatly reduces sample complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seurin_M/0/1/0/all/0/1"&gt;Mathieu Seurin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strub_F/0/1/0/all/0/1"&gt;Florian Strub&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Preux_P/0/1/0/all/0/1"&gt;Philippe Preux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pietquin_O/0/1/0/all/0/1"&gt;Olivier Pietquin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Unsupervised Multi-Object Tracking in Noisy Environments. (arXiv:2105.10005v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10005</id>
        <link href="http://arxiv.org/abs/2105.10005"/>
        <updated>2021-05-24T05:08:40.726Z</updated>
        <summary type="html"><![CDATA[Camera movement and unpredictable environmental conditions like dust and wind
induce noise into video feeds. We observe that popular unsupervised MOT methods
are dependent on noise-free conditions. We show that the addition of a small
amount of artificial random noise causes a sharp degradation in model
performance on benchmark metrics. We resolve this problem by introducing a
robust unsupervised multi-object tracking (MOT) model: AttU-Net. The proposed
single-head attention model helps limit the negative impact of noise by
learning visual representations at different segment scales. AttU-Net shows
better unsupervised MOT tracking performance over variational inference-based
state-of-the-art baselines. We evaluate our method in the MNIST and the Atari
game video benchmark. We also provide two extended video datasets consisting of
complex visual patterns that include Kuzushiji characters and fashion images to
validate the effectiveness of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;C.-H. Huck Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chhabra_M/0/1/0/all/0/1"&gt;Mohit Chhabra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Y.-C. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1"&gt;Quan Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoshinaga_T/0/1/0/all/0/1"&gt;Tomoaki Yoshinaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murakam_T/0/1/0/all/0/1"&gt;Tomokazu Murakam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction. (arXiv:2105.10446v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10446</id>
        <link href="http://arxiv.org/abs/2105.10446"/>
        <updated>2021-05-24T05:08:40.710Z</updated>
        <summary type="html"><![CDATA[This work attempts to provide a plausible theoretical framework that aims to
interpret modern deep (convolutional) networks from the principles of data
compression and discriminative representation. We show that for
high-dimensional multi-class data, the optimal linear discriminative
representation maximizes the coding rate difference between the whole dataset
and the average of all the subsets. We show that the basic iterative gradient
ascent scheme for optimizing the rate reduction objective naturally leads to a
multi-layer deep network, named ReduNet, that shares common characteristics of
modern deep networks. The deep layered architectures, linear and nonlinear
operators, and even parameters of the network are all explicitly constructed
layer-by-layer via forward propagation, instead of learned via back
propagation. All components of so-obtained "white-box" network have precise
optimization, statistical, and geometric interpretation. Moreover, all linear
operators of the so-derived network naturally become multi-channel convolutions
when we enforce classification to be rigorously shift-invariant. The derivation
also indicates that such a deep convolution network is significantly more
efficient to construct and learn in the spectral domain. Our preliminary
simulations and experiments clearly verify the effectiveness of both the rate
reduction objective and the associated ReduNet. All code and data are available
at https://github.com/Ma-Lab-Berkeley.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1"&gt;Kwan Ho Ryan Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yaodong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chong You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1"&gt;Haozhi Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wright_J/0/1/0/all/0/1"&gt;John Wright&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yi Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatial-Temporal Conv-sequence Learning with Accident Encoding for Traffic Flow Prediction. (arXiv:2105.10478v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10478</id>
        <link href="http://arxiv.org/abs/2105.10478"/>
        <updated>2021-05-24T05:08:40.703Z</updated>
        <summary type="html"><![CDATA[In intelligent transportation system, the key problem of traffic forecasting
is how to extract the periodic temporal dependencies and complex spatial
correlation. Current state-of-the-art methods for traffic flow prediction are
based on graph architectures and sequence learning models, but they do not
fully exploit spatial-temporal dynamic information in traffic system.
Specifically, the temporal dependence of short-range is diluted by recurrent
neural networks, and existing sequence model ignores local spatial information
because the convolution operation uses global average pooling. Besides, there
will be some traffic accidents during the transitions of objects causing
congestion in the real world that trigger increased prediction deviation. To
overcome these challenges, we propose the Spatial-Temporal Conv-sequence
Learning (STCL), in which a focused temporal block uses unidirectional
convolution to effectively capture short-term periodic temporal dependence, and
a spatial-temporal fusion module is able to extract the dependencies of both
interactions and decrease the feature dimensions. Moreover, the accidents
features impact on local traffic congestion and position encoding is employed
to detect anomalies in complex traffic situations. We conduct extensive
experiments on large-scale real-world tasks and verify the effectiveness of our
proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zichuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Hongbo Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NAIS-Net: Stable Deep Networks from Non-Autonomous Differential Equations. (arXiv:1804.07209v4 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1804.07209</id>
        <link href="http://arxiv.org/abs/1804.07209"/>
        <updated>2021-05-24T05:08:40.619Z</updated>
        <summary type="html"><![CDATA[This paper introduces Non-Autonomous Input-Output Stable Network(NAIS-Net), a
very deep architecture where each stacked processing block is derived from a
time-invariant non-autonomous dynamical system. Non-autonomy is implemented by
skip connections from the block input to each of the unrolled processing stages
and allows stability to be enforced so that blocks can be unrolled adaptively
to a pattern-dependent processing depth. NAIS-Net induces non-trivial,
Lipschitz input-output maps, even for an infinite unroll length. We prove that
the network is globally asymptotically stable so that for every initial
condition there is exactly one input-dependent equilibrium assuming $tanh$
units, and incrementally stable for ReL units. An efficient implementation that
enforces the stability under derived conditions for both fully-connected and
convolutional layers is also presented. Experimental results show how NAIS-Net
exhibits stability in practice, yielding a significant reduction in
generalization gap compared to ResNets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ciccone_M/0/1/0/all/0/1"&gt;Marco Ciccone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gallieri_M/0/1/0/all/0/1"&gt;Marco Gallieri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masci_J/0/1/0/all/0/1"&gt;Jonathan Masci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osendorfer_C/0/1/0/all/0/1"&gt;Christian Osendorfer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_F/0/1/0/all/0/1"&gt;Faustino Gomez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Statistical Inference for Parameters Estimation with Linear-Equality Constraints. (arXiv:2105.10315v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.10315</id>
        <link href="http://arxiv.org/abs/2105.10315"/>
        <updated>2021-05-24T05:08:40.607Z</updated>
        <summary type="html"><![CDATA[Stochastic gradient descent (SGD) and projected stochastic gradient descent
(PSGD) are scalable algorithms to compute model parameters in unconstrained and
constrained optimization problems. In comparison with stochastic gradient
descent (SGD), PSGD forces its iterative values into the constrained parameter
space via projection. The convergence rate of PSGD-type estimates has been
exhaustedly studied, while statistical properties such as asymptotic
distribution remain less explored. From a purely statistical point of view,
this paper studies the limiting distribution of PSGD-based estimate when the
true parameters satisfying some linear-equality constraints. Our theoretical
findings reveal the role of projection played in the uncertainty of the PSGD
estimate. As a byproduct, we propose an online hypothesis testing procedure to
test the linear-equality constraints. Simulation studies on synthetic data and
an application to a real-world dataset confirm our theory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Liu_R/0/1/0/all/0/1"&gt;Ruiqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yuan_M/0/1/0/all/0/1"&gt;Mingao Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shang_Z/0/1/0/all/0/1"&gt;Zuofeng Shang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable Machine Learning with Prior Knowledge: An Overview. (arXiv:2105.10172v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10172</id>
        <link href="http://arxiv.org/abs/2105.10172"/>
        <updated>2021-05-24T05:08:40.595Z</updated>
        <summary type="html"><![CDATA[This survey presents an overview of integrating prior knowledge into machine
learning systems in order to improve explainability. The complexity of machine
learning models has elicited research to make them more explainable. However,
most explainability methods cannot provide insight beyond the given data,
requiring additional information about the context. We propose to harness prior
knowledge to improve upon the explanation capabilities of machine learning
models. In this paper, we present a categorization of current research into
three main categories which either integrate knowledge into the machine
learning pipeline, into the explainability method or derive knowledge from
explanations. To classify the papers, we build upon the existing taxonomy of
informed machine learning and extend it from the perspective of explainability.
We conclude with open challenges and research directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Beckh_K/0/1/0/all/0/1"&gt;Katharina Beckh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_S/0/1/0/all/0/1"&gt;Sebastian M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jakobs_M/0/1/0/all/0/1"&gt;Matthias Jakobs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toborek_V/0/1/0/all/0/1"&gt;Vanessa Toborek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1"&gt;Hanxiao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fischer_R/0/1/0/all/0/1"&gt;Raphael Fischer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welke_P/0/1/0/all/0/1"&gt;Pascal Welke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Houben_S/0/1/0/all/0/1"&gt;Sebastian Houben&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rueden_L/0/1/0/all/0/1"&gt;Laura von Rueden&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BELT: Blockwise Missing Embedding Learning Transfomer. (arXiv:2105.10360v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.10360</id>
        <link href="http://arxiv.org/abs/2105.10360"/>
        <updated>2021-05-24T05:08:40.586Z</updated>
        <summary type="html"><![CDATA[Matrix completion has attracted a lot of attention in many fields including
statistics, applied mathematics and electrical engineering. Most of works focus
on the independent sampling models under which the individual observed entries
are sampled independently. Motivated by applications in the integration of
multiple (point-wise mutual information) PMI matrices, we propose the model
{\bf B}lockwise missing {\bf E}mbedding {\bf L}earning {\bf T}ransformer (BELT)
to treat row-wise/column-wise missingness. Specifically, our proposed method
aims at efficient matrix recovery when every pair of matrices from multiple
sources has an overlap. We provide theoretical justification for the proposed
BELT method. Simulation studies show that the method performs well in finite
sample under a variety of configurations. The method is applied to integrate
several PMI matrices built by EHR data and Chinese medical text data, which
enables us to construct a comprehensive embedding set for CUI and Chinese with
high quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Doudou Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cai_T/0/1/0/all/0/1"&gt;Tianxi Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lu_J/0/1/0/all/0/1"&gt;Junwei Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unit Test Case Generation with Transformers and Focal Context. (arXiv:2009.05617v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.05617</id>
        <link href="http://arxiv.org/abs/2009.05617"/>
        <updated>2021-05-24T05:08:40.578Z</updated>
        <summary type="html"><![CDATA[Automated unit test case generation tools facilitate test-driven development
and support developers by suggesting tests intended to identify flaws in their
code. Existing approaches are usually guided by the test coverage criteria,
generating synthetic test cases that are often difficult for developers to read
or understand. In this paper we propose AthenaTest, an approach that aims to
generate unit test cases by learning from real-world focal methods and
developer-written testcases. We formulate unit test case generation as a
sequence-to-sequence learning task, adopting a two-step training procedure
consisting of denoising pretraining on a large unsupervised Java corpus, and
supervised finetuning for a downstream translation task of generating unit
tests. We investigate the impact of natural language and source code
pretraining, as well as the focal context information surrounding the focal
method. Both techniques provide improvements in terms of validation loss, with
pretraining yielding 25% relative improvement and focal context providing
additional 11.1% improvement. We also introduce Methods2Test, the largest
publicly available supervised parallel corpus of unit test case methods and
corresponding focal methods in Java, which comprises 780K test cases mined from
91K open-source repositories from GitHub. We evaluate AthenaTest on five
defects4j projects, generating 25K passing test cases covering 43.7% of the
focal methods with only 30 attempts. We execute the test cases, collect test
coverage information, and compare them with test cases generated by EvoSuite
and GPT-3, finding that our approach outperforms GPT-3 and has comparable
coverage w.r.t. EvoSuite. Finally, we survey professional developers on their
preference in terms of readability, understandability, and testing
effectiveness of the generated tests, showing overwhelmingly preference towards
AthenaTest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tufano_M/0/1/0/all/0/1"&gt;Michele Tufano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1"&gt;Dawn Drain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Svyatkovskiy_A/0/1/0/all/0/1"&gt;Alexey Svyatkovskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1"&gt;Shao Kun Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1"&gt;Neel Sundaresan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Entropy-based Discovery of Summary Causal Graphs in Time Series. (arXiv:2105.10381v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2105.10381</id>
        <link href="http://arxiv.org/abs/2105.10381"/>
        <updated>2021-05-24T05:08:40.560Z</updated>
        <summary type="html"><![CDATA[We address in this study the problem of learning a summary causal graph on
time series with potentially different sampling rates. To do so, we first
propose a new temporal mutual information measure defined on a window-based
representation of time series. We then show how this measure relates to an
entropy reduction principle that can be seen as a special case of the
Probabilistic Raising Principle. We finally combine these two ingredients in a
PC-like algorithm to construct the summary causal graph. This algorithm is
evaluated on several datasets that shows both its efficacy and efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Assaad_K/0/1/0/all/0/1"&gt;Karim Assaad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Devijver_E/0/1/0/all/0/1"&gt;Emilie Devijver&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaussier_E/0/1/0/all/0/1"&gt;Eric Gaussier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ait_Bachir_A/0/1/0/all/0/1"&gt;Ali Ait-Bachir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NUQSGD: Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization. (arXiv:1908.06077v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.06077</id>
        <link href="http://arxiv.org/abs/1908.06077"/>
        <updated>2021-05-24T05:08:40.555Z</updated>
        <summary type="html"><![CDATA[As the size and complexity of models and datasets grow, so does the need for
communication-efficient variants of stochastic gradient descent that can be
deployed to perform parallel model training. One popular
communication-compression method for data-parallel SGD is QSGD (Alistarh et
al., 2017), which quantizes and encodes gradients to reduce communication
costs. The baseline variant of QSGD provides strong theoretical guarantees,
however, for practical purposes, the authors proposed a heuristic variant which
we call QSGDinf, which demonstrated impressive empirical gains for distributed
training of large neural networks. In this paper, we build on this work to
propose a new gradient quantization scheme, and show that it has both stronger
theoretical guarantees than QSGD, and matches and exceeds the empirical
performance of the QSGDinf heuristic and of other compression methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ramezani_Kebrya_A/0/1/0/all/0/1"&gt;Ali Ramezani-Kebrya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faghri_F/0/1/0/all/0/1"&gt;Fartash Faghri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markov_I/0/1/0/all/0/1"&gt;Ilya Markov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aksenov_V/0/1/0/all/0/1"&gt;Vitalii Aksenov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1"&gt;Dan Alistarh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1"&gt;Daniel M. Roy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Precise Performance Analysis of Support Vector Regression. (arXiv:2105.10373v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10373</id>
        <link href="http://arxiv.org/abs/2105.10373"/>
        <updated>2021-05-24T05:08:40.533Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the hard and soft support vector regression
techniques applied to a set of $n$ linear measurements of the form
$y_i=\boldsymbol{\beta}_\star^{T}{\bf x}_i +n_i$ where
$\boldsymbol{\beta}_\star$ is an unknown vector, $\left\{{\bf
x}_i\right\}_{i=1}^n$ are the feature vectors and
$\left\{{n}_i\right\}_{i=1}^n$ model the noise. Particularly, under some
plausible assumptions on the statistical distribution of the data, we
characterize the feasibility condition for the hard support vector regression
in the regime of high dimensions and, when feasible, derive an asymptotic
approximation for its risk. Similarly, we study the test risk for the soft
support vector regression as a function of its parameters. Our results are then
used to optimally tune the parameters intervening in the design of hard and
soft support vector regression algorithms. Based on our analysis, we illustrate
that adding more samples may be harmful to the test performance of support
vector regression, while it is always beneficial when the parameters are
optimally selected. Such a result reminds a similar phenomenon observed in
modern learning architectures according to which optimally tuned architectures
present a decreasing test performance curve with respect to the number of
samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sifaou_H/0/1/0/all/0/1"&gt;Houssem Sifaou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+kammoun_A/0/1/0/all/0/1"&gt;Abla kammoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alouini_M/0/1/0/all/0/1"&gt;Mohamed-Slim Alouini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalization Error Bound for Hyperbolic Ordinal Embedding. (arXiv:2105.10475v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10475</id>
        <link href="http://arxiv.org/abs/2105.10475"/>
        <updated>2021-05-24T05:08:40.494Z</updated>
        <summary type="html"><![CDATA[Hyperbolic ordinal embedding (HOE) represents entities as points in
hyperbolic space so that they agree as well as possible with given constraints
in the form of entity i is more similar to entity j than to entity k. It has
been experimentally shown that HOE can obtain representations of hierarchical
data such as a knowledge base and a citation network effectively, owing to
hyperbolic space's exponential growth property. However, its theoretical
analysis has been limited to ideal noiseless settings, and its generalization
error in compensation for hyperbolic space's exponential representation ability
has not been guaranteed. The difficulty is that existing generalization error
bound derivations for ordinal embedding based on the Gramian matrix do not work
in HOE, since hyperbolic space is not inner-product space. In this paper,
through our novel characterization of HOE with decomposed Lorentz Gramian
matrices, we provide a generalization error bound of HOE for the first time,
which is at most exponential with respect to the embedding space's radius. Our
comparison between the bounds of HOE and Euclidean ordinal embedding shows that
HOE's generalization error is reasonable as a cost for its exponential
representation ability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suzuki_A/0/1/0/all/0/1"&gt;Atsushi Suzuki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nitanda_A/0/1/0/all/0/1"&gt;Atsushi Nitanda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Linchuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cavazza_M/0/1/0/all/0/1"&gt;Marc Cavazza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamanishi_K/0/1/0/all/0/1"&gt;Kenji Yamanishi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stance Detection with BERT Embeddings for Credibility Analysis of Information on Social Media. (arXiv:2105.10272v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2105.10272</id>
        <link href="http://arxiv.org/abs/2105.10272"/>
        <updated>2021-05-24T05:08:40.476Z</updated>
        <summary type="html"><![CDATA[The evolution of electronic media is a mixed blessing. Due to the easy
access, low cost, and faster reach of the information, people search out and
devour news from online social networks. In contrast, the increasing acceptance
of social media reporting leads to the spread of fake news. This is a minacious
problem that causes disputes and endangers societal stability and harmony. Fake
news spread has gained attention from researchers due to its vicious nature.
proliferation of misinformation in all media, from the internet to cable news,
paid advertising and local news outlets, has made it essential for people to
identify the misinformation and sort through the facts. Researchers are trying
to analyze the credibility of information and curtail false information on such
platforms. Credibility is the believability of the piece of information at
hand. Analyzing the credibility of fake news is challenging due to the intent
of its creation and the polychromatic nature of the news. In this work, we
propose a model for detecting fake news. Our method investigates the content of
the news at the early stage i.e. when the news is published but is yet to be
disseminated through social media. Our work interprets the content with
automatic feature extraction and the relevance of the text pieces. In summary,
we introduce stance as one of the features along with the content of the
article and employ the pre-trained contextualized word embeddings BERT to
obtain the state-of-art results for fake news detection. The experiment
conducted on the real-world dataset indicates that our model outperforms the
previous work and enables fake news detection with an accuracy of 95.32%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karande_H/0/1/0/all/0/1"&gt;Hema Karande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walambe_R/0/1/0/all/0/1"&gt;Rahee Walambe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benjamin_V/0/1/0/all/0/1"&gt;Victor Benjamin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kotecha_K/0/1/0/all/0/1"&gt;Ketan Kotecha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raghu_T/0/1/0/all/0/1"&gt;T. S. Raghu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Quantum Classifiers Through the Lens of the Hessian. (arXiv:2105.10162v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2105.10162</id>
        <link href="http://arxiv.org/abs/2105.10162"/>
        <updated>2021-05-24T05:08:40.469Z</updated>
        <summary type="html"><![CDATA[In quantum computing, the variational quantum algorithms (VQAs) are well
suited for finding optimal combinations of things in specific applications
ranging from chemistry all the way to finance. The training of VQAs with
gradient descent optimization algorithm has shown a good convergence. At an
early stage, the simulation of variational quantum circuits on noisy
intermediate-scale quantum (NISQ) devices suffers from noisy outputs. Just like
classical deep learning, it also suffers from vanishing gradient problems. It
is a realistic goal to study the topology of loss landscape, to visualize the
curvature information and trainability of these circuits in the existence of
vanishing gradients. In this paper, we calculated the Hessian and visualized
the loss landscape of variational quantum classifiers at different points in
parameter space. The curvature information of variational quantum classifiers
(VQC) is interpreted and the loss function's convergence is shown. It helps us
better understand the behavior of variational quantum circuits to tackle
optimization problems efficiently. We investigated the variational quantum
classifiers via Hessian on quantum computers, started with a simple 4-bit
parity problem to gain insight into the practical behavior of Hessian, then
thoroughly analyzed the behavior of Hessian's eigenvalues on training the
variational quantum classifier for the Diabetes dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Sen_P/0/1/0/all/0/1"&gt;Pinaki Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Bhatia_A/0/1/0/all/0/1"&gt;Amandeep Singh Bhatia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble Quantile Networks: Uncertainty-Aware Reinforcement Learning with Applications in Autonomous Driving. (arXiv:2105.10266v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.10266</id>
        <link href="http://arxiv.org/abs/2105.10266"/>
        <updated>2021-05-24T05:08:40.462Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning (RL) can be used to create a decision-making agent for
autonomous driving. However, previous approaches provide only black-box
solutions, which do not offer information on how confident the agent is about
its decisions. An estimate of both the aleatoric and epistemic uncertainty of
the agent's decisions is fundamental for real-world applications of autonomous
driving. Therefore, this paper introduces the Ensemble Quantile Networks (EQN)
method, which combines distributional RL with an ensemble approach, to obtain a
complete uncertainty estimate. The distribution over returns is estimated by
learning its quantile function implicitly, which gives the aleatoric
uncertainty, whereas an ensemble of agents is trained on bootstrapped data to
provide a Bayesian estimation of the epistemic uncertainty. A criterion for
classifying which decisions that have an unacceptable uncertainty is also
introduced. The results show that the EQN method can balance risk and time
efficiency in different occluded intersection scenarios, by considering the
estimated aleatoric uncertainty. Furthermore, it is shown that the trained
agent can use the epistemic uncertainty information to identify situations that
the agent has not been trained for and thereby avoid making unfounded,
potentially dangerous, decisions outside of the training distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hoel_C/0/1/0/all/0/1"&gt;Carl-Johan Hoel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolff_K/0/1/0/all/0/1"&gt;Krister Wolff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laine_L/0/1/0/all/0/1"&gt;Leo Laine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Behind the leaves -- Estimation of occluded grapevine berries with conditional generative adversarial networks. (arXiv:2105.10325v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10325</id>
        <link href="http://arxiv.org/abs/2105.10325"/>
        <updated>2021-05-24T05:08:40.438Z</updated>
        <summary type="html"><![CDATA[The need for accurate yield estimates for viticulture is becoming more
important due to increasing competition in the wine market worldwide. One of
the most promising methods to estimate the harvest is berry counting, as it can
be approached non-destructively, and its process can be automated. In this
article, we present a method that addresses the challenge of occluded berries
with leaves to obtain a more accurate estimate of the number of berries that
will enable a better estimate of the harvest. We use generative adversarial
networks, a deep learning-based approach that generates a likely scenario
behind the leaves exploiting learned patterns from images with non-occluded
berries. Our experiments show that the estimate of the number of berries after
applying our method is closer to the manually counted reference. In contrast to
applying a factor to the berry count, our approach better adapts to local
conditions by directly involving the appearance of the visible berries.
Furthermore, we show that our approach can identify which areas in the image
should be changed by adding new berries without explicitly requiring
information about hidden areas.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kierdorf_J/0/1/0/all/0/1"&gt;Jana Kierdorf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weber_I/0/1/0/all/0/1"&gt;Immanuel Weber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kicherer_A/0/1/0/all/0/1"&gt;Anna Kicherer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zabawa_L/0/1/0/all/0/1"&gt;Laura Zabawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drees_L/0/1/0/all/0/1"&gt;Lukas Drees&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roscher_R/0/1/0/all/0/1"&gt;Ribana Roscher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Word-level Text Highlighting of Medical Texts forTelehealth Services. (arXiv:2105.10400v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10400</id>
        <link href="http://arxiv.org/abs/2105.10400"/>
        <updated>2021-05-24T05:08:40.432Z</updated>
        <summary type="html"><![CDATA[The medical domain is often subject to information overload. The digitization
of healthcare, constant updates to online medical repositories, and increasing
availability of biomedical datasets make it challenging to effectively analyze
the data. This creates additional work for medical professionals who are
heavily dependent on medical data to complete their research and consult their
patients. This paper aims to show how different text highlighting techniques
can capture relevant medical context. This would reduce the doctors' cognitive
load and response time to patients by facilitating them in making faster
decisions, thus improving the overall quality of online medical services. Three
different word-level text highlighting methodologies are implemented and
evaluated. The first method uses TF-IDF scores directly to highlight important
parts of the text. The second method is a combination of TF-IDF scores and the
application of Local Interpretable Model-Agnostic Explanations to
classification models. The third method uses neural networks directly to make
predictions on whether or not a word should be highlighted. The results of our
experiments show that the neural network approach is successful in highlighting
medically-relevant terms and its performance is improved as the size of the
input segment increases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozyegen_O/0/1/0/all/0/1"&gt;Ozan Ozyegen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kabe_D/0/1/0/all/0/1"&gt;Devika Kabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cevik_M/0/1/0/all/0/1"&gt;Mucahit Cevik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Definite Non-Ancestral Relations and Structure Learning. (arXiv:2105.10350v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10350</id>
        <link href="http://arxiv.org/abs/2105.10350"/>
        <updated>2021-05-24T05:08:40.424Z</updated>
        <summary type="html"><![CDATA[In causal graphical models based on directed acyclic graphs (DAGs), directed
paths represent causal pathways between the corresponding variables. The
variable at the beginning of such a path is referred to as an ancestor of the
variable at the end of the path. Ancestral relations between variables play an
important role in causal modeling. In existing literature on structure
learning, these relations are usually deduced from learned structures and used
for orienting edges or formulating constraints of the space of possible DAGs.
However, they are usually not posed as immediate target of inference. In this
work we investigate the graphical characterization of ancestral relations via
CPDAGs and d-separation relations. We propose a framework that can learn
definite non-ancestral relations without first learning the skeleton. This
frame-work yields structural information that can be used in both score- and
constraint-based algorithms to learn causal DAGs more efficiently.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wenyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drton_M/0/1/0/all/0/1"&gt;Mathias Drton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shojaie_A/0/1/0/all/0/1"&gt;Ali Shojaie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distinguishing artefacts: evaluating the saturation point of convolutional neural networks. (arXiv:2105.10448v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10448</id>
        <link href="http://arxiv.org/abs/2105.10448"/>
        <updated>2021-05-24T05:08:40.405Z</updated>
        <summary type="html"><![CDATA[Prior work has shown Convolutional Neural Networks (CNNs) trained on
surrogate Computer Aided Design (CAD) models are able to detect and classify
real-world artefacts from photographs. The applications of which support
twinning of digital and physical assets in design, including rapid extraction
of part geometry from model repositories, information search \& retrieval and
identifying components in the field for maintenance, repair, and recording. The
performance of CNNs in classification tasks have been shown dependent on
training data set size and number of classes. Where prior works have used
relatively small surrogate model data sets ($<100$ models), the question
remains as to the ability of a CNN to differentiate between models in
increasingly large model repositories. This paper presents a method for
generating synthetic image data sets from online CAD model repositories, and
further investigates the capacity of an off-the-shelf CNN architecture trained
on synthetic data to classify models as class size increases. 1,000 CAD models
were curated and processed to generate large scale surrogate data sets,
featuring model coverage at steps of 10$^{\circ}$, 30$^{\circ}$, 60$^{\circ}$,
and 120$^{\circ}$ degrees. The findings demonstrate the capability of computer
vision algorithms to classify artefacts in model repositories of up to 200,
beyond this point the CNN's performance is observed to deteriorate
significantly, limiting its present ability for automated twinning of physical
to digital artefacts. Although, a match is more often found in the top-5
results showing potential for information search and retrieval on large
repositories of surrogate models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Real_R/0/1/0/all/0/1"&gt;Ric Real&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gopsill_J/0/1/0/all/0/1"&gt;James Gopsill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jones_D/0/1/0/all/0/1"&gt;David Jones&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snider_C/0/1/0/all/0/1"&gt;Chris Snider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hicks_B/0/1/0/all/0/1"&gt;Ben Hicks&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Instrumental Variable Regression for Deep Offline Policy Evaluation. (arXiv:2105.10148v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10148</id>
        <link href="http://arxiv.org/abs/2105.10148"/>
        <updated>2021-05-24T05:08:40.397Z</updated>
        <summary type="html"><![CDATA[We show that the popular reinforcement learning (RL) strategy of estimating
the state-action value (Q-function) by minimizing the mean squared Bellman
error leads to a regression problem with confounding, the inputs and output
noise being correlated. Hence, direct minimization of the Bellman error can
result in significantly biased Q-function estimates. We explain why fixing the
target Q-network in Deep Q-Networks and Fitted Q Evaluation provides a way of
overcoming this confounding, thus shedding new light on this popular but not
well understood trick in the deep RL literature. An alternative approach to
address confounding is to leverage techniques developed in the causality
literature, notably instrumental variables (IV). We bring together here the
literature on IV and RL by investigating whether IV approaches can lead to
improved Q-function estimates. This paper analyzes and compares a wide range of
recent IV methods in the context of offline policy evaluation (OPE), where the
goal is to estimate the value of a policy using logged data only. By applying
different IV techniques to OPE, we are not only able to recover previously
proposed OPE methods such as model-based techniques but also to obtain
competitive new techniques. We find empirically that state-of-the-art OPE
methods are closely matched in performance by some IV methods such as AGMM,
which were not developed for OPE. We open-source all our code and datasets at
https://github.com/liyuan9988/IVOPEwithACME.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yutian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Liyuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gulcehre_C/0/1/0/all/0/1"&gt;Caglar Gulcehre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paine_T/0/1/0/all/0/1"&gt;Tom Le Paine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gretton_A/0/1/0/all/0/1"&gt;Arthur Gretton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freitas_N/0/1/0/all/0/1"&gt;Nando de Freitas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1"&gt;Arnaud Doucet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AC-CovidNet: Attention Guided Contrastive CNN for Recognition of Covid-19 in Chest X-Ray Images. (arXiv:2105.10239v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10239</id>
        <link href="http://arxiv.org/abs/2105.10239"/>
        <updated>2021-05-24T05:08:40.383Z</updated>
        <summary type="html"><![CDATA[Covid-19 global pandemic continues to devastate health care systems across
the world. In many countries, the 2nd wave is very severe. Economical and rapid
testing, as well as diagnosis, is urgently needed to control the pandemic. At
present, the Covid-19 testing is costly and time-consuming. Chest X-Ray (CXR)
testing can be the fastest, scalable, and non-invasive method. The existing
methods suffer due to the limited CXR samples available from Covid-19. Thus,
inspired by the limitations of the open-source work in this field, we propose
attention guided contrastive CNN architecture (AC-CovidNet) for Covid-19
detection in CXR images. The proposed method learns the robust and
discriminative features with the help of contrastive loss. Moreover, the
proposed method gives more importance to the infected regions as guided by the
attention mechanism. We compute the sensitivity of the proposed method over the
publicly available Covid-19 dataset. It is observed that the proposed
AC-CovidNet exhibits very promising performance as compared to the existing
methods even with limited training data. It can tackle the bottleneck of CXR
Covid-19 datasets being faced by the researchers. The code used in this paper
is released publicly at \url{https://github.com/shivram1987/AC-CovidNet/}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ambati_A/0/1/0/all/0/1"&gt;Anirudh Ambati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dubey_S/0/1/0/all/0/1"&gt;Shiv Ram Dubey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Gaussian equivalence of generative models for learning with shallow neural networks. (arXiv:2006.14709v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.14709</id>
        <link href="http://arxiv.org/abs/2006.14709"/>
        <updated>2021-05-24T05:08:40.355Z</updated>
        <summary type="html"><![CDATA[Understanding the impact of data structure on the computational tractability
of learning is a key challenge for the theory of neural networks. Many
theoretical works do not explicitly model training data, or assume that inputs
are drawn component-wise independently from some simple probability
distribution. Here, we go beyond this simple paradigm by studying the
performance of neural networks trained on data drawn from pre-trained
generative models. This is possible due to a Gaussian equivalence stating that
the key metrics of interest, such as the training and test errors, can be fully
captured by an appropriately chosen Gaussian model. We provide three strands of
rigorous, analytical and numerical evidence corroborating this equivalence.
First, we establish rigorous conditions for the Gaussian equivalence to hold in
the case of single-layer generative models, as well as deterministic rates for
convergence in distribution. Second, we leverage this equivalence to derive a
closed set of equations describing the generalisation performance of two widely
studied machine learning problems: two-layer neural networks trained using
one-pass stochastic gradient descent, and full-batch pre-learned features or
kernel methods. Finally, we perform experiments demonstrating how our theory
applies to deep, pre-trained generative models. These results open a viable
path to the theoretical study of machine learning models with realistic data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Goldt_S/0/1/0/all/0/1"&gt;Sebastian Goldt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Loureiro_B/0/1/0/all/0/1"&gt;Bruno Loureiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Reeves_G/0/1/0/all/0/1"&gt;Galen Reeves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Krzakala_F/0/1/0/all/0/1"&gt;Florent Krzakala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mezard_M/0/1/0/all/0/1"&gt;Marc M&amp;#xe9;zard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zdeborova_L/0/1/0/all/0/1"&gt;Lenka Zdeborov&amp;#xe1;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Non-Linear Structural Probe. (arXiv:2105.10185v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10185</id>
        <link href="http://arxiv.org/abs/2105.10185"/>
        <updated>2021-05-24T05:08:40.348Z</updated>
        <summary type="html"><![CDATA[Probes are models devised to investigate the encoding of knowledge -- e.g.
syntactic structure -- in contextual representations. Probes are often designed
for simplicity, which has led to restrictions on probe design that may not
allow for the full exploitation of the structure of encoded information; one
such restriction is linearity. We examine the case of a structural probe
(Hewitt and Manning, 2019), which aims to investigate the encoding of syntactic
structure in contextual representations through learning only linear
transformations. By observing that the structural probe learns a metric, we are
able to kernelize it and develop a novel non-linear variant with an identical
number of parameters. We test on 6 languages and find that the radial-basis
function (RBF) kernel, in conjunction with regularization, achieves a
statistically significant improvement over the baseline in all languages --
implying that at least part of the syntactic knowledge is encoded non-linearly.
We conclude by discussing how the RBF kernel resembles BERT's self-attention
layers and speculate that this resemblance leads to the RBF-based probe's
stronger performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+White_J/0/1/0/all/0/1"&gt;Jennifer C. White&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1"&gt;Tiago Pimentel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1"&gt;Naomi Saphra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anomaly Mining -- Past, Present and Future. (arXiv:2105.10077v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10077</id>
        <link href="http://arxiv.org/abs/2105.10077"/>
        <updated>2021-05-24T05:08:40.331Z</updated>
        <summary type="html"><![CDATA[Anomaly mining is an important problem that finds numerous applications in
various real world domains such as environmental monitoring, cybersecurity,
finance, healthcare and medicine, to name a few. In this article, I focus on
two areas, (1) point-cloud and (2) graph-based anomaly mining. I aim to present
a broad view of each area, and discuss classes of main research problems,
recent trends and future directions. I conclude with key take-aways and
overarching open problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akoglu_L/0/1/0/all/0/1"&gt;Leman Akoglu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AngularGrad: A New Optimization Technique for Angular Convergence of Convolutional Neural Networks. (arXiv:2105.10190v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10190</id>
        <link href="http://arxiv.org/abs/2105.10190"/>
        <updated>2021-05-24T05:08:40.312Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) are trained using stochastic gradient
descent (SGD)-based optimizers. Recently, the adaptive moment estimation (Adam)
optimizer has become very popular due to its adaptive momentum, which tackles
the dying gradient problem of SGD. Nevertheless, existing optimizers are still
unable to exploit the optimization curvature information efficiently. This
paper proposes a new AngularGrad optimizer that considers the behavior of the
direction/angle of consecutive gradients. This is the first attempt in the
literature to exploit the gradient angular information apart from its
magnitude. The proposed AngularGrad generates a score to control the step size
based on the gradient angular information of previous iterations. Thus, the
optimization steps become smoother as a more accurate step size of immediate
past gradients is captured through the angular information. Two variants of
AngularGrad are developed based on the use of Tangent or Cosine functions for
computing the gradient angular information. Theoretically, AngularGrad exhibits
the same regret bound as Adam for convergence purposes. Nevertheless, extensive
experiments conducted on benchmark data sets against state-of-the-art methods
reveal a superior performance of AngularGrad. The source code will be made
publicly available at: https://github.com/mhaut/AngularGrad.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1"&gt;S.K. Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paoletti_M/0/1/0/all/0/1"&gt;M.E. Paoletti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haut_J/0/1/0/all/0/1"&gt;J.M. Haut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1"&gt;S.R. Dubey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1"&gt;P. Kar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plaza_A/0/1/0/all/0/1"&gt;A. Plaza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_B/0/1/0/all/0/1"&gt;B.B. Chaudhuri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Robust Misclassifications of Neural Networks to Enhance Adversarial Attacks. (arXiv:2105.10304v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10304</id>
        <link href="http://arxiv.org/abs/2105.10304"/>
        <updated>2021-05-24T05:08:40.277Z</updated>
        <summary type="html"><![CDATA[Progress in making neural networks more robust against adversarial attacks is
mostly marginal, despite the great efforts of the research community. Moreover,
the robustness evaluation is often imprecise, making it difficult to identify
promising approaches. We analyze the classification decisions of 19 different
state-of-the-art neural networks trained to be robust against adversarial
attacks. Our findings suggest that current untargeted adversarial attacks
induce misclassification towards only a limited amount of different classes.
Additionally, we observe that both over- and under-confidence in model
predictions result in an inaccurate assessment of model robustness. Based on
these observations, we propose a novel loss function for adversarial attacks
that consistently improves attack success rate compared to prior loss functions
for 19 out of 19 analyzed models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schwinn_L/0/1/0/all/0/1"&gt;Leo Schwinn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raab_R/0/1/0/all/0/1"&gt;Ren&amp;#xe9; Raab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;An Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zanca_D/0/1/0/all/0/1"&gt;Dario Zanca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eskofier_B/0/1/0/all/0/1"&gt;Bjoern Eskofier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Have you tried Neural Topic Models? Comparative Analysis of Neural and Non-Neural Topic Models with Application to COVID-19 Twitter Data. (arXiv:2105.10165v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10165</id>
        <link href="http://arxiv.org/abs/2105.10165"/>
        <updated>2021-05-24T05:08:40.269Z</updated>
        <summary type="html"><![CDATA[Topic models are widely used in studying social phenomena. We conduct a
comparative study examining state-of-the-art neural versus non-neural topic
models, performing a rigorous quantitative and qualitative assessment on a
dataset of tweets about the COVID-19 pandemic. Our results show that not only
do neural topic models outperform their classical counterparts on standard
evaluation metrics, but they also produce more coherent topics, which are of
great benefit when studying complex social problems. We also propose a novel
regularization term for neural topic models, which is designed to address the
well-documented problem of mode collapse, and demonstrate its effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bennett_A/0/1/0/all/0/1"&gt;Andrew Bennett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1"&gt;Dipendra Misra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Than_N/0/1/0/all/0/1"&gt;Nga Than&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rule Augmented Unsupervised Constituency Parsing. (arXiv:2105.10193v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10193</id>
        <link href="http://arxiv.org/abs/2105.10193"/>
        <updated>2021-05-24T05:08:40.263Z</updated>
        <summary type="html"><![CDATA[Recently, unsupervised parsing of syntactic trees has gained considerable
attention. A prototypical approach to such unsupervised parsing employs
reinforcement learning and auto-encoders. However, no mechanism ensures that
the learnt model leverages the well-understood language grammar. We propose an
approach that utilizes very generic linguistic knowledge of the language
present in the form of syntactic rules, thus inducing better syntactic
structures. We introduce a novel formulation that takes advantage of the
syntactic grammar rules and is independent of the base system. We achieve new
state-of-the-art results on two benchmarks datasets, MNLI and WSJ. The source
code of the paper is available at https://github.com/anshuln/Diora_with_rules.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sahay_A/0/1/0/all/0/1"&gt;Atul Sahay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasery_A/0/1/0/all/0/1"&gt;Anshul Nasery&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maheshwari_A/0/1/0/all/0/1"&gt;Ayush Maheshwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1"&gt;Ganesh Ramakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1"&gt;Rishabh Iyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GAN pretraining for deep convolutional autoencoders applied to Software-based Fingerprint Presentation Attack Detection. (arXiv:2105.10213v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10213</id>
        <link href="http://arxiv.org/abs/2105.10213"/>
        <updated>2021-05-24T05:08:40.241Z</updated>
        <summary type="html"><![CDATA[The need for reliable systems to determine fingerprint presentation attacks
grows with the rising use of the fingerprint for authentication. This work
presents a new approach to single-class classification for software-based
fingerprint presentation attach detection. The described method utilizes a
Wasserstein GAN to apply transfer learning to a deep convolutional autoencoder.
By doing so, the autoencoder could be pretrained and finetuned on the
LivDet2021 Dermalog sensor dataset with only 1122 bona fide training samples.
Without making use of any presentation attack samples, the model could archive
an average classification error rate of 16.79%. The Wasserstein GAN implemented
to pretrain the autoencoders weights can further be used to generate
realistic-looking artificial fingerprint patches. Extensive testing of
different autoencoder architectures and hyperparameters led to coarse
architectural guidelines as well as multiple implementations which can be
utilized for future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rohrer_T/0/1/0/all/0/1"&gt;Tobias Rohrer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolberg_J/0/1/0/all/0/1"&gt;Jascha Kolberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trimming Feature Extraction and Inference for MCU-based Edge NILM: a Systematic Approach. (arXiv:2105.10302v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10302</id>
        <link href="http://arxiv.org/abs/2105.10302"/>
        <updated>2021-05-24T05:08:40.168Z</updated>
        <summary type="html"><![CDATA[Non-Intrusive Load Monitoring (NILM) enables the disaggregation of the global
power consumption of multiple loads, taken from a single smart electrical
meter, into appliance-level details. State-of-the-Art approaches are based on
Machine Learning methods and exploit the fusion of time- and frequency-domain
features from current and voltage sensors. Unfortunately, these methods are
compute-demanding and memory-intensive. Therefore, running low-latency NILM on
low-cost, resource-constrained MCU-based meters is currently an open challenge.
This paper addresses the optimization of the feature spaces as well as the
computational and storage cost reduction needed for executing State-of-the-Art
(SoA) NILM algorithms on memory- and compute-limited MCUs. We compare four
supervised learning techniques on different classification scenarios and
characterize the overall NILM pipeline's implementation on a MCU-based Smart
Measurement Node. Experimental results demonstrate that optimizing the feature
space enables edge MCU-based NILM with 95.15% accuracy, resulting in a small
drop compared to the most-accurate feature vector deployment (96.19%) while
achieving up to 5.45x speed-up and 80.56% storage reduction. Furthermore, we
show that low-latency NILM relying only on current measurements reaches almost
80% accuracy, allowing a major cost reduction by removing voltage sensors from
the hardware design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tabanelli_E/0/1/0/all/0/1"&gt;Enrico Tabanelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brunelli_D/0/1/0/all/0/1"&gt;Davide Brunelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Acquaviva_A/0/1/0/all/0/1"&gt;Andrea Acquaviva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1"&gt;Luca Benini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Interpretable Approach to Automated Severity Scoring in Pelvic Trauma. (arXiv:2105.10238v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.10238</id>
        <link href="http://arxiv.org/abs/2105.10238"/>
        <updated>2021-05-24T05:08:40.152Z</updated>
        <summary type="html"><![CDATA[Pelvic ring disruptions result from blunt injury mechanisms and are often
found in patients with multi-system trauma. To grade pelvic fracture severity
in trauma victims based on whole-body CT, the Tile AO/OTA classification is
frequently used. Due to the high volume of whole-body trauma CTs generated in
busy trauma centers, an automated approach to Tile classification would provide
substantial value, e.,g., to prioritize the reading queue of the attending
trauma radiologist. In such scenario, an automated method should perform
grading based on a transparent process and based on interpretable features to
enable interaction with human readers and lower their workload by offering
insights from a first automated read of the scan. This paper introduces an
automated yet interpretable pelvic trauma decision support system to assist
radiologists in fracture detection and Tile grade classification. The method
operates similarly to human interpretation of CT scans and first detects
distinct pelvic fractures on CT with high specificity using a Faster-RCNN model
that are then interpreted using a structural causal model based on clinical
best practices to infer an initial Tile grade. The Bayesian causal model and
finally, the object detector are then queried for likely co-occurring fractures
that may have been rejected initially due to the highly specific operating
point of the detector, resulting in an updated list of detected fractures and
corresponding final Tile grade. Our method is transparent in that it provides
finding location and type using the object detector, as well as information on
important counterfactuals that would invalidate the system's recommendation and
achieves an AUC of 83.3%/85.1% for translational/rotational instability.
Despite being designed for human-machine teaming, our approach does not
compromise on performance compared to previous black-box approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zapaishchykova_A/0/1/0/all/0/1"&gt;Anna Zapaishchykova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dreizin_D/0/1/0/all/0/1"&gt;David Dreizin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhaoshuo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jie Ying Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Roohi_S/0/1/0/all/0/1"&gt;Shahrooz Faghih Roohi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Unberath_M/0/1/0/all/0/1"&gt;Mathias Unberath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Yes We Care! -- Certification for Machine Learning Methods through the Care Label Framework. (arXiv:2105.10197v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10197</id>
        <link href="http://arxiv.org/abs/2105.10197"/>
        <updated>2021-05-24T05:08:40.117Z</updated>
        <summary type="html"><![CDATA[Machine learning applications have become ubiquitous. Their applications from
machine embedded control in production over process optimization in diverse
areas (e.g., traffic, finance, sciences) to direct user interactions like
advertising and recommendations. This has led to an increased effort of making
machine learning trustworthy. Explainable and fair AI have already matured.
They address knowledgeable users and application engineers. However, there are
users that want to deploy a learned model in a similar way as their washing
machine. These stakeholders do not want to spend time understanding the model.
Instead, they want to rely on guaranteed properties. What are the relevant
properties? How can they be expressed to stakeholders without presupposing
machine learning knowledge? How can they be guaranteed for a certain
implementation of a model? These questions move far beyond the current
state-of-the-art and we want to address them here. We propose a unified
framework that certifies learning methods via care labels. They are easy to
understand and draw inspiration from well-known certificates like textile
labels or property cards of electronic devices. Our framework considers both,
the machine learning theory and a given implementation. We test the
implementation's compliance with theoretical properties and bounds. In this
paper, we illustrate care labels by a prototype implementation of a
certification suite for a selection of probabilistic graphical models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Morik_K/0/1/0/all/0/1"&gt;Katharina Morik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kotthaus_H/0/1/0/all/0/1"&gt;Helena Kotthaus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heppe_L/0/1/0/all/0/1"&gt;Lukas Heppe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heinrich_D/0/1/0/all/0/1"&gt;Danny Heinrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fischer_R/0/1/0/all/0/1"&gt;Raphael Fischer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mucke_S/0/1/0/all/0/1"&gt;Sascha M&amp;#xfc;cke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pauly_A/0/1/0/all/0/1"&gt;Andreas Pauly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jakobs_M/0/1/0/all/0/1"&gt;Matthias Jakobs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piatkowski_N/0/1/0/all/0/1"&gt;Nico Piatkowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Certification of Iterative Predictions in Bayesian Neural Networks. (arXiv:2105.10134v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10134</id>
        <link href="http://arxiv.org/abs/2105.10134"/>
        <updated>2021-05-24T05:08:40.074Z</updated>
        <summary type="html"><![CDATA[We consider the problem of computing reach-avoid probabilities for iterative
predictions made with Bayesian neural network (BNN) models. Specifically, we
leverage bound propagation techniques and backward recursion to compute lower
bounds for the probability that trajectories of the BNN model reach a given set
of states while avoiding a set of unsafe states. We use the lower bounds in the
context of control and reinforcement learning to provide safety certification
for given control policies, as well as to synthesize control policies that
improve the certification bounds. On a set of benchmarks, we demonstrate that
our framework can be employed to certify policies over BNNs predictions for
problems of more than $10$ dimensions, and to effectively synthesize policies
that significantly increase the lower bound on the satisfaction probability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wicker_M/0/1/0/all/0/1"&gt;Matthew Wicker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laurenti_L/0/1/0/all/0/1"&gt;Luca Laurenti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patane_A/0/1/0/all/0/1"&gt;Andrea Patane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paoletti_N/0/1/0/all/0/1"&gt;Nicola Paoletti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abate_A/0/1/0/all/0/1"&gt;Alessandro Abate&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwiatkowska_M/0/1/0/all/0/1"&gt;Marta Kwiatkowska&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A GAN-Like Approach for Physics-Based Imitation Learning and Interactive Character Control. (arXiv:2105.10066v1 [cs.GR])]]></title>
        <id>http://arxiv.org/abs/2105.10066</id>
        <link href="http://arxiv.org/abs/2105.10066"/>
        <updated>2021-05-24T05:08:40.063Z</updated>
        <summary type="html"><![CDATA[We present a simple and intuitive approach for interactive control of
physically simulated characters. Our work builds upon generative adversarial
networks (GAN) and reinforcement learning, and introduces an imitation learning
framework where an ensemble of classifiers and an imitation policy are trained
in tandem given pre-processed reference clips. The classifiers are trained to
discriminate the reference motion from the motion generated by the imitation
policy, while the policy is rewarded for fooling the discriminators. Using our
GAN-based approach, multiple motor control policies can be trained separately
to imitate different behaviors. In runtime, our system can respond to external
control signal provided by the user and interactively switch between different
policies. Compared to existing methods, our proposed approach has the following
attractive properties: 1) achieves state-of-the-art imitation performance
without manually designing and fine tuning a reward function; 2) directly
controls the character without having to track any target reference pose
explicitly or implicitly through a phase state; and 3) supports interactive
policy switching without requiring any motion generation or motion matching
mechanism. We highlight the applicability of our approach in a range of
imitation and interactive control tasks, while also demonstrating its ability
to withstand external perturbations as well as to recover balance. Overall, our
approach generates high-fidelity motion, has low runtime cost, and can be
easily integrated into interactive applications and games.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1"&gt;Pei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karamouzas_I/0/1/0/all/0/1"&gt;Ioannis Karamouzas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safety Metrics for Semantic Segmentation in Autonomous Driving. (arXiv:2105.10142v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10142</id>
        <link href="http://arxiv.org/abs/2105.10142"/>
        <updated>2021-05-24T05:08:40.037Z</updated>
        <summary type="html"><![CDATA[Within the context of autonomous driving, safety-related metrics for deep
neural networks have been widely studied for image classification and object
detection. In this paper, we further consider safety-aware correctness and
robustness metrics specialized for semantic segmentation. The novelty of our
proposal is to move beyond pixel-level metrics: Given two images with each
having N pixels being class-flipped, the designed metrics should, depending on
the clustering of pixels being class-flipped or the location of occurrence,
reflect a different level of safety criticality. The result evaluated on an
autonomous driving dataset demonstrates the validity and practicality of our
proposed methodology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1"&gt;Chih-Hong Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1"&gt;Alois Knoll&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1"&gt;Hsuan-Cheng Liao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Cross-Sectional Currency Strategies by Ranking Refinement with Transformer-based Architectures. (arXiv:2105.10019v1 [q-fin.PM])]]></title>
        <id>http://arxiv.org/abs/2105.10019</id>
        <link href="http://arxiv.org/abs/2105.10019"/>
        <updated>2021-05-24T05:08:40.020Z</updated>
        <summary type="html"><![CDATA[The performance of a cross-sectional currency strategy depends crucially on
accurately ranking instruments prior to portfolio construction. While this
ranking step is traditionally performed using heuristics, or by sorting outputs
produced by pointwise regression or classification models, Learning to Rank
algorithms have recently presented themselves as competitive and viable
alternatives. Despite improving ranking accuracy on average however, these
techniques do not account for the possibility that assets positioned at the
extreme ends of the ranked list -- which are ultimately used to construct the
long/short portfolios -- can assume different distributions in the input space,
and thus lead to sub-optimal strategy performance. Drawing from research in
Information Retrieval that demonstrates the utility of contextual information
embedded within top-ranked documents to learn the query's characteristics to
improve ranking, we propose an analogous approach: exploiting the features of
both out- and under-performing instruments to learn a model for refining the
original ranked list. Under a re-ranking framework, we adapt the Transformer
architecture to encode the features of extreme assets for refining our
selection of long/short instruments obtained with an initial retrieval.
Backtesting on a set of 31 currencies, our proposed methodology significantly
boosts Sharpe ratios -- by approximately 20% over the original LTR algorithms
and double that of traditional baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Poh_D/0/1/0/all/0/1"&gt;Daniel Poh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Lim_B/0/1/0/all/0/1"&gt;Bryan Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Zohren_S/0/1/0/all/0/1"&gt;Stefan Zohren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Roberts_S/0/1/0/all/0/1"&gt;Stephen Roberts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel Stein Discrepancy Descent. (arXiv:2105.09994v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.09994</id>
        <link href="http://arxiv.org/abs/2105.09994"/>
        <updated>2021-05-24T05:08:40.003Z</updated>
        <summary type="html"><![CDATA[Among dissimilarities between probability distributions, the Kernel Stein
Discrepancy (KSD) has received much interest recently. We investigate the
properties of its Wasserstein gradient flow to approximate a target probability
distribution $\pi$ on $\mathbb{R}^d$, known up to a normalization constant.
This leads to a straightforwardly implementable, deterministic score-based
method to sample from $\pi$, named KSD Descent, which uses a set of particles
to approximate $\pi$. Remarkably, owing to a tractable loss function, KSD
Descent can leverage robust parameter-free optimization schemes such as L-BFGS;
this contrasts with other popular particle-based schemes such as the Stein
Variational Gradient Descent algorithm. We study the convergence properties of
KSD Descent and demonstrate its practical relevance. However, we also highlight
failure cases by showing that the algorithm can get stuck in spurious local
minima.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Korba_A/0/1/0/all/0/1"&gt;Anna Korba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Aubin_Frankowski_P/0/1/0/all/0/1"&gt;Pierre-Cyril Aubin-Frankowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Majewski_S/0/1/0/all/0/1"&gt;Szymon Majewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ablin_P/0/1/0/all/0/1"&gt;Pierre Ablin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TestRank: Bringing Order into Unlabeled Test Instances for Deep Learning Tasks. (arXiv:2105.10113v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10113</id>
        <link href="http://arxiv.org/abs/2105.10113"/>
        <updated>2021-05-24T05:08:39.980Z</updated>
        <summary type="html"><![CDATA[Deep learning (DL) has achieved unprecedented success in a variety of tasks.
However, DL systems are notoriously difficult to test and debug due to the lack
of explainability of DL models and the huge test input space to cover.
Generally speaking, it is relatively easy to collect a massive amount of test
data, but the labeling cost can be quite high. Consequently, it is essential to
conduct test selection and label only those selected "high quality"
bug-revealing test inputs for test cost reduction.

In this paper, we propose a novel test prioritization technique that brings
order into the unlabeled test instances according to their bug-revealing
capabilities, namely TestRank. Different from existing solutions, TestRank
leverages both intrinsic attributes and contextual attributes of test instances
when prioritizing them. To be specific, we first build a similarity graph on
test instances and training samples, and we conduct graph-based semi-supervised
learning to extract contextual features. Then, for a particular test instance,
the contextual features extracted from the graph neural network (GNN) and the
intrinsic features obtained with the DL model itself are combined to predict
its bug-revealing probability. Finally, TestRank prioritizes unlabeled test
instances in descending order of the above probability value. We evaluate the
performance of TestRank on a variety of image classification datasets.
Experimental results show that the debugging efficiency of our method
significantly outperforms existing test prioritization techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Min Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_Q/0/1/0/all/0/1"&gt;Qiuxia Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yannan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1"&gt;Qiang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comment on Stochastic Polyak Step-Size: Performance of ALI-G. (arXiv:2105.10011v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10011</id>
        <link href="http://arxiv.org/abs/2105.10011"/>
        <updated>2021-05-24T05:08:39.973Z</updated>
        <summary type="html"><![CDATA[This is a short note on the performance of the ALI-G algorithm (Berrada et
al., 2020) as reported in (Loizou et al., 2021). ALI-G (Berrada et al., 2020)
and SPS (Loizou et al., 2021) are both adaptations of the Polyak step-size to
optimize machine learning models that can interpolate the training data. The
main algorithmic differences are that (1) SPS employs a multiplicative constant
in the denominator of the learning-rate while ALI-G uses an additive constant,
and (2) SPS uses an iteration-dependent maximal learning-rate while ALI-G uses
a constant one. There are also differences in the analysis provided by the two
works, with less restrictive assumptions proposed in (Loizou et al., 2021). In
their experiments, (Loizou et al., 2021) did not use momentum for ALI-G (which
is a standard part of the algorithm) or standard hyper-parameter tuning (for
e.g. learning-rate and regularization). Hence this note as a reference for the
improved performance that ALI-G can obtain with well-chosen hyper-parameters.
In particular, we show that when training a ResNet-34 on CIFAR-10 and
CIFAR-100, the performance of ALI-G can reach respectively 93.5% (+6%) and 76%
(+8%) with a very small amount of tuning. Thus ALI-G remains a very competitive
method for training interpolating neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Berrada_L/0/1/0/all/0/1"&gt;Leonard Berrada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1"&gt;Andrew Zisserman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1"&gt;M. Pawan Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Escaping Saddle Points with Compressed SGD. (arXiv:2105.10090v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10090</id>
        <link href="http://arxiv.org/abs/2105.10090"/>
        <updated>2021-05-24T05:08:39.954Z</updated>
        <summary type="html"><![CDATA[Stochastic gradient descent (SGD) is a prevalent optimization technique for
large-scale distributed machine learning. While SGD computation can be
efficiently divided between multiple machines, communication typically becomes
a bottleneck in the distributed setting. Gradient compression methods can be
used to alleviate this problem, and a recent line of work shows that SGD
augmented with gradient compression converges to an $\varepsilon$-first-order
stationary point. In this paper we extend these results to convergence to an
$\varepsilon$-second-order stationary point ($\varepsilon$-SOSP), which is to
the best of our knowledge the first result of this type. In addition, we show
that, when the stochastic gradient is not Lipschitz, compressed SGD with
RandomK compressor converges to an $\varepsilon$-SOSP with the same number of
iterations as uncompressed SGD [Jin et al.,2021] (JACM), while improving the
total communication by a factor of $\tilde \Theta(\sqrt{d}
\varepsilon^{-3/4})$, where $d$ is the dimension of the optimization problem.
We present additional results for the cases when the compressor is arbitrary
and when the stochastic gradient is Lipschitz.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Avdiukhin_D/0/1/0/all/0/1"&gt;Dmitrii Avdiukhin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yaroslavtsev_G/0/1/0/all/0/1"&gt;Grigory Yaroslavtsev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[XGBoost energy consumption prediction based on multi-system data HVAC. (arXiv:2105.09945v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09945</id>
        <link href="http://arxiv.org/abs/2105.09945"/>
        <updated>2021-05-24T05:08:39.944Z</updated>
        <summary type="html"><![CDATA[The energy consumption of the HVAC system accounts for a significant portion
of the energy consumption of the public building system, and using an efficient
energy consumption prediction model can assist it in carrying out effective
energy-saving transformation. Unlike the traditional energy consumption
prediction model, this paper extracts features from large data sets using
XGBoost, trains them separately to obtain multiple models, then fuses them with
LightGBM's independent prediction results using MAE, infers energy consumption
related variables, and successfully applies this model to the self-developed
Internet of Things platform.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yunlong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1"&gt;Yiming Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dengzheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mai_Y/0/1/0/all/0/1"&gt;Yingan Mai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruan_Z/0/1/0/all/0/1"&gt;Zhengrong Ruan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-Free Knowledge Distillation for Heterogeneous Federated Learning. (arXiv:2105.10056v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10056</id>
        <link href="http://arxiv.org/abs/2105.10056"/>
        <updated>2021-05-24T05:08:39.924Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) is a decentralized machine-learning paradigm, in
which a global server iteratively averages the model parameters of local users
without accessing their data. User heterogeneity has imposed significant
challenges to FL, which can incur drifted global models that are slow to
converge. Knowledge Distillation has recently emerged to tackle this issue, by
refining the server model using aggregated knowledge from heterogeneous users,
other than directly averaging their model parameters. This approach, however,
depends on a proxy dataset, making it impractical unless such a prerequisite is
satisfied. Moreover, the ensemble knowledge is not fully utilized to guide
local model learning, which may in turn affect the quality of the aggregated
model. Inspired by the prior art, we propose a data-free knowledge
distillation} approach to address heterogeneous FL, where the server learns a
lightweight generator to ensemble user information in a data-free manner, which
is then broadcasted to users, regulating local training using the learned
knowledge as an inductive bias. Empirical studies powered by theoretical
implications show that, our approach facilitates FL with better generalization
performance using fewer communication rounds, compared with the
state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhuangdi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1"&gt;Junyuan Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jiayu Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Probabilistic Approach to Neural Network Pruning. (arXiv:2105.10065v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10065</id>
        <link href="http://arxiv.org/abs/2105.10065"/>
        <updated>2021-05-24T05:08:39.917Z</updated>
        <summary type="html"><![CDATA[Neural network pruning techniques reduce the number of parameters without
compromising predicting ability of a network. Many algorithms have been
developed for pruning both over-parameterized fully-connected networks (FCNs)
and convolutional neural networks (CNNs), but analytical studies of
capabilities and compression ratios of such pruned sub-networks are lacking. We
theoretically study the performance of two pruning techniques (random and
magnitude-based) on FCNs and CNNs. Given a target network {whose weights are
independently sampled from appropriate distributions}, we provide a universal
approach to bound the gap between a pruned and the target network in a
probabilistic sense. The results establish that there exist pruned networks
with expressive power within any specified bound from the target network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1"&gt;Xin Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klabjan_D/0/1/0/all/0/1"&gt;Diego Klabjan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Robustness over High Level Driving Instruction for Autonomous Driving. (arXiv:2105.10014v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10014</id>
        <link href="http://arxiv.org/abs/2105.10014"/>
        <updated>2021-05-24T05:08:39.892Z</updated>
        <summary type="html"><![CDATA[In recent years, we have witnessed increasingly high performance in the field
of autonomous end-to-end driving. In particular, more and more research is
being done on driving in urban environments, where the car has to follow high
level commands to navigate. However, few evaluations are made on the ability of
these agents to react in an unexpected situation. Specifically, no evaluations
are conducted on the robustness of driving agents in the event of a bad
high-level command. We propose here an evaluation method, namely a benchmark
that allows to assess the robustness of an agent, and to appreciate its
understanding of the environment through its ability to keep a safe behavior,
regardless of the instruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carton_F/0/1/0/all/0/1"&gt;Florence Carton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filliat_D/0/1/0/all/0/1"&gt;David Filliat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabarisoa_J/0/1/0/all/0/1"&gt;Jaonary Rabarisoa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1"&gt;Quoc Cuong Pham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Modular Robot Control Policies. (arXiv:2105.10049v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.10049</id>
        <link href="http://arxiv.org/abs/2105.10049"/>
        <updated>2021-05-24T05:08:39.886Z</updated>
        <summary type="html"><![CDATA[To make a modular robotic system both capable and scalable, the controller
must be equally as modular as the mechanism. Given the large number of designs
that can be generated from even a small set of modules, it becomes impractical
to create a new system-wide controller for each design. Instead, we construct a
modular control policy that handles a broad class of designs. We take the view
that a module is both form and function, i.e. both mechanism and controller. As
the modules are physically re-configured, the policy automatically
re-configures to match the kinematic structure. This novel policy is trained
with a new model-based reinforcement learning algorithm, which interleaves
model learning and trajectory optimization to guide policy learning for
multiple designs simultaneously. Training the policy on a varied set of designs
teaches it how to adapt its behavior to the design. We show that the policy can
then generalize to a larger set of designs not seen during training. We
demonstrate one policy controlling many designs with different combinations of
legs and wheels to locomote both in simulation and on real robots.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Whitman_J/0/1/0/all/0/1"&gt;Julian Whitman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Travers_M/0/1/0/all/0/1"&gt;Matthew Travers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choset_H/0/1/0/all/0/1"&gt;Howie Choset&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Opening Deep Neural Networks with Generative Models. (arXiv:2105.10013v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10013</id>
        <link href="http://arxiv.org/abs/2105.10013"/>
        <updated>2021-05-24T05:08:39.877Z</updated>
        <summary type="html"><![CDATA[Image classification methods are usually trained to perform predictions
taking into account a predefined group of known classes. Real-world problems,
however, may not allow for a full knowledge of the input and label spaces,
making failures in recognition a hazard to deep visual learning. Open set
recognition methods are characterized by the ability to correctly identifying
inputs of known and unknown classes. In this context, we propose GeMOS: simple
and plug-and-play open set recognition modules that can be attached to
pretrained Deep Neural Networks for visual recognition. The GeMOS framework
pairs pre-trained Convolutional Neural Networks with generative models for open
set recognition to extract open set scores for each sample, allowing for
failure recognition in object recognition tasks. We conduct a thorough
evaluation of the proposed method in comparison with state-of-the-art open set
algorithms, finding that GeMOS either outperforms or is statistically
indistinguishable from more complex and costly models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vendramini_M/0/1/0/all/0/1"&gt;Marcos Vendramini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1"&gt;Hugo Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Machado_A/0/1/0/all/0/1"&gt;Alexei Machado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1"&gt;Jefersson A. dos Santos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wide & Deep neural network model for patch aggregation in CNN-based prostate cancer detection systems. (arXiv:2105.09974v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09974</id>
        <link href="http://arxiv.org/abs/2105.09974"/>
        <updated>2021-05-24T05:08:39.858Z</updated>
        <summary type="html"><![CDATA[Prostate cancer (PCa) is one of the most commonly diagnosed cancer and one of
the leading causes of death among men, with almost 1.41 million new cases and
around 375,000 deaths in 2020. Artificial Intelligence algorithms have had a
huge impact in medical image analysis, including digital histopathology, where
Convolutional Neural Networks (CNNs) are used to provide a fast and accurate
diagnosis, supporting experts in this task. To perform an automatic diagnosis,
prostate tissue samples are first digitized into gigapixel-resolution
whole-slide images. Due to the size of these images, neural networks cannot use
them as input and, therefore, small subimages called patches are extracted and
predicted, obtaining a patch-level classification. In this work, a novel patch
aggregation method based on a custom Wide & Deep neural network model is
presented, which performs a slide-level classification using the patch-level
classes obtained from a CNN. The malignant tissue ratio, a 10-bin malignant
probability histogram, the least squares regression line of the histogram, and
the number of malignant connected components are used by the proposed model to
perform the classification. An accuracy of 94.24% and a sensitivity of 98.87%
were achieved, proving that the proposed system could aid pathologists by
speeding up the screening process and, thus, contribute to the fight against
PCa.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duran_Lopez_L/0/1/0/all/0/1"&gt;Lourdes Duran-Lopez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dominguez_Morales_J/0/1/0/all/0/1"&gt;Juan P. Dominguez-Morales&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gutierrez_Galan_D/0/1/0/all/0/1"&gt;Daniel Gutierrez-Galan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rios_Navarro_A/0/1/0/all/0/1"&gt;Antonio Rios-Navarro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jimenez_Fernandez_A/0/1/0/all/0/1"&gt;Angel Jimenez-Fernandez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vicente_Diaz_S/0/1/0/all/0/1"&gt;Saturnino Vicente-Diaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Linares_Barranco_A/0/1/0/all/0/1"&gt;Alejandro Linares-Barranco&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-domain Imitation from Observations. (arXiv:2105.10037v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.10037</id>
        <link href="http://arxiv.org/abs/2105.10037"/>
        <updated>2021-05-24T05:08:39.851Z</updated>
        <summary type="html"><![CDATA[Imitation learning seeks to circumvent the difficulty in designing proper
reward functions for training agents by utilizing expert behavior. With
environments modeled as Markov Decision Processes (MDP), most of the existing
imitation algorithms are contingent on the availability of expert
demonstrations in the same MDP as the one in which a new imitation policy is to
be learned. In this paper, we study the problem of how to imitate tasks when
there exist discrepancies between the expert and agent MDP. These discrepancies
across domains could include differing dynamics, viewpoint, or morphology; we
present a novel framework to learn correspondences across such domains.
Importantly, in contrast to prior works, we use unpaired and unaligned
trajectories containing only states in the expert domain, to learn this
correspondence. We utilize a cycle-consistency constraint on both the state
space and a domain agnostic latent space to do this. In addition, we enforce
consistency on the temporal position of states via a normalized position
estimator function, to align the trajectories across the two domains. Once this
correspondence is found, we can directly transfer the demonstrations on one
domain to the other and use it for imitation. Experiments across a wide variety
of challenging domains demonstrate the efficacy of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raychaudhuri_D/0/1/0/all/0/1"&gt;Dripta S. Raychaudhuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1"&gt;Sujoy Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baar_J/0/1/0/all/0/1"&gt;Jeroen van Baar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1"&gt;Amit K. Roy-Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Multi-Robot System for Non-myopic Spatial Sampling. (arXiv:2105.10018v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.10018</id>
        <link href="http://arxiv.org/abs/2105.10018"/>
        <updated>2021-05-24T05:08:39.795Z</updated>
        <summary type="html"><![CDATA[This paper presents a distributed scalable multi-robot planning algorithm for
non-uniform sampling of quasi-static spatial fields. We address the problem of
efficient data collection using multiple autonomous vehicles. In this paper, we
are interested in analyzing the effect of communication between multiple
robots, acting independently, on the overall sampling performance of the team.
Our focus is on distributed sampling problem where the robots are operating
independent of their teammates, but have the ability to communicate their
states to other neighbors with a constraint on the communication range. We
design and apply an informed non-myopic path planning technique on multiple
robotic platforms to efficiently collect measurements from a spatial field. Our
proposed approach is highly adaptive to challenging environments, growing team
size, and runs in real-time, which are the key features for any real-world
scenario. The results show that our distributed sampling approach is able to
achieve efficient sampling with minimal communication between the robots. We
evaluate our approach in simulation over multiple distributions commonly
occurring in nature and on the real-world data collected during a field trial.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Manjanna_S/0/1/0/all/0/1"&gt;Sandeep Manjanna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_A/0/1/0/all/0/1"&gt;Ani Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1"&gt;Gregory Dudek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal prediction of oxygen uptake dynamics from wearable sensors during low-, moderate-, and heavy-intensity exercise. (arXiv:2105.09987v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09987</id>
        <link href="http://arxiv.org/abs/2105.09987"/>
        <updated>2021-05-24T05:08:39.767Z</updated>
        <summary type="html"><![CDATA[Oxygen consumption (VO$_2$) provides established clinical and physiological
indicators of cardiorespiratory function and exercise capacity. However, VO$_2$
monitoring is largely limited to specialized laboratory settings, making its
widespread monitoring elusive. Here, we investigate temporal prediction of
VO$_2$ from wearable sensors during cycle ergometer exercise using a temporal
convolutional network (TCN). Cardiorespiratory signals were acquired from a
smart shirt with integrated textile sensors alongside ground-truth VO$_2$ from
a metabolic system on twenty-two young healthy adults. Participants performed
one ramp-incremental and three pseudorandom binary sequence exercise protocols
to assess a range of VO$_2$ dynamics. A TCN model was developed using causal
convolutions across an effective history length to model the time-dependent
nature of VO$_2$. Optimal history length was determined through minimum
validation loss across hyperparameter values. The best performing model encoded
218 s history length (TCN-VO$_2$ A), with 187 s, 97 s, and 76 s yielding less
than 3% deviation from the optimal validation loss. TCN-VO$_2$ A showed strong
prediction accuracy (mean, 95% CI) across all exercise intensities (-22
ml.min$^{-1}$, [-262, 218]), spanning transitions from low-moderate (-23
ml.min$^{-1}$, [-250, 204]), low-heavy (14 ml.min$^{-1}$, [-252, 280]),
ventilatory threshold-heavy (-49 ml.min$^{-1}$, [-274, 176]), and maximal (-32
ml.min$^{-1}$, [-261, 197]) exercise. Second-by-second classification of
physical activity across 16090 s of predicted VO$_2$ was able to discern
between vigorous, moderate, and light activity with high accuracy (94.1%). This
system enables quantitative aerobic activity monitoring in non-laboratory
settings across a range of exercise intensities using wearable sensors for
monitoring exercise prescription adherence and personal fitness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Amelard_R/0/1/0/all/0/1"&gt;Robert Amelard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hedge_E/0/1/0/all/0/1"&gt;Eric T Hedge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hughson_R/0/1/0/all/0/1"&gt;Richard L Hughson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards a Universal NLG for Dialogue Systems and Simulators with Future Bridging. (arXiv:2105.10267v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10267</id>
        <link href="http://arxiv.org/abs/2105.10267"/>
        <updated>2021-05-24T05:08:39.757Z</updated>
        <summary type="html"><![CDATA[In a dialogue system pipeline, a natural language generation (NLG) unit
converts the dialogue direction and content to a corresponding natural language
realization. A recent trend for dialogue systems is to first pre-train on large
datasets and then fine-tune in a supervised manner using datasets annotated
with application-specific features. Though novel behaviours can be learned from
custom annotation, the required effort severely bounds the quantity of the
training set, and the application-specific nature limits the reuse. In light of
the recent success of data-driven approaches, we propose the novel future
bridging NLG (FBNLG) concept for dialogue systems and simulators. The critical
step is for an FBNLG to accept a future user or system utterance to bridge the
present context towards. Future bridging enables self supervised training over
annotation-free datasets, decoupled the training of NLG from the rest of the
system. An FBNLG, pre-trained with massive datasets, is expected to apply in
classical or new dialogue scenarios with minimal adaptation effort. We evaluate
a prototype FBNLG to show that future bridging can be a viable approach to a
universal few-shot NLG for task-oriented and chit-chat dialogues.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ennen_P/0/1/0/all/0/1"&gt;Philipp Ennen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yen-Ting Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozbay_A/0/1/0/all/0/1"&gt;Ali Girayhan Ozbay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Insalata_F/0/1/0/all/0/1"&gt;Ferdinando Insalata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Ye Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jalali_S/0/1/0/all/0/1"&gt;Sepehr Jalali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shiu_D/0/1/0/all/0/1"&gt;Da-shan Shiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-group Agnostic PAC Learnability. (arXiv:2105.09989v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09989</id>
        <link href="http://arxiv.org/abs/2105.09989"/>
        <updated>2021-05-24T05:08:39.739Z</updated>
        <summary type="html"><![CDATA[An agnostic PAC learning algorithm finds a predictor that is competitive with
the best predictor in a benchmark hypothesis class, where competitiveness is
measured with respect to a given loss function. However, its predictions might
be quite sub-optimal for structured subgroups of individuals, such as protected
demographic groups. Motivated by such fairness concerns, we study "multi-group
agnostic PAC learnability": fixing a measure of loss, a benchmark class $\H$
and a (potentially) rich collection of subgroups $\G$, the objective is to
learn a single predictor such that the loss experienced by every group $g \in
\G$ is not much larger than the best possible loss for this group within $\H$.
Under natural conditions, we provide a characterization of the loss functions
for which such a predictor is guaranteed to exist. For any such loss function
we construct a learning algorithm whose sample complexity is logarithmic in the
size of the collection $\G$. Our results unify and extend previous positive and
negative results from the multi-group fairness literature, which applied for
specific loss functions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rothblum_G/0/1/0/all/0/1"&gt;Guy N Rothblum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yona_G/0/1/0/all/0/1"&gt;Gal Yona&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Modal Answer Validation for Knowledge-Based VQA. (arXiv:2103.12248v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12248</id>
        <link href="http://arxiv.org/abs/2103.12248"/>
        <updated>2021-05-24T05:08:39.733Z</updated>
        <summary type="html"><![CDATA[The problem of knowledge-based visual question answering involves answering
questions that require external knowledge in addition to the content of the
image. Such knowledge typically comes in a variety of forms, including visual,
textual, and commonsense knowledge. The use of more knowledge sources, however,
also increases the chance of retrieving more irrelevant or noisy facts, making
it difficult to comprehend the facts and find the answer. To address this
challenge, we propose Multi-modal Answer Validation using External knowledge
(MAVEx), where the idea is to validate a set of promising answer candidates
based on answer-specific knowledge retrieval. This is in contrast to existing
approaches that search for the answer in a vast collection of often irrelevant
facts. Our approach aims to learn which knowledge source should be trusted for
each answer candidate and how to validate the candidate using that source. We
consider a multi-modal setting, relying on both textual and visual knowledge
resources, including images searched using Google, sentences from Wikipedia
articles, and concepts from ConceptNet. Our experiments with OK-VQA, a
challenging knowledge-based VQA dataset, demonstrate that MAVEx achieves new
state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jialin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiasen Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1"&gt;Ashish Sabharwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1"&gt;Roozbeh Mottaghi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Representation for Dialogue Modeling. (arXiv:2105.10188v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10188</id>
        <link href="http://arxiv.org/abs/2105.10188"/>
        <updated>2021-05-24T05:08:39.722Z</updated>
        <summary type="html"><![CDATA[Although neural models have achieved competitive results in dialogue systems,
they have shown limited ability in representing core semantics, such as
ignoring important entities. To this end, we exploit Abstract Meaning
Representation (AMR) to help dialogue modeling. Compared with the textual
input, AMR explicitly provides core semantic knowledge and reduces data
sparsity. We develop an algorithm to construct dialogue-level AMR graphs from
sentence-level AMRs and explore two ways to incorporate AMRs into dialogue
systems. Experimental results on both dialogue understanding and response
generation tasks show the superiority of our model. To our knowledge, we are
the first to leverage a formal semantic representation into neural dialogue
modeling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1"&gt;Xuefeng Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yulong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"&gt;Linfeng Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yue Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Neural Network Weights using Nature-Inspired Algorithms. (arXiv:2105.09983v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09983</id>
        <link href="http://arxiv.org/abs/2105.09983"/>
        <updated>2021-05-24T05:08:39.696Z</updated>
        <summary type="html"><![CDATA[This study aims to optimize Deep Feedforward Neural Networks (DFNNs) training
using nature-inspired optimization algorithms, such as PSO, MTO, and its
variant called MTOCL. We show how these algorithms efficiently update the
weights of DFNNs when learning from data. We evaluate the performance of DFNN
fused with optimization algorithms using three Wisconsin breast cancer
datasets, Original, Diagnostic, and Prognosis, under different experimental
scenarios. The empirical analysis demonstrates that MTOCL is the most
performing in most scenarios across the three datasets. Also, MTOCL is
comparable to past weight optimization algorithms for the original dataset, and
superior for the other datasets, especially for the challenging Prognostic
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Korani_W/0/1/0/all/0/1"&gt;Wael Korani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mouhoub_M/0/1/0/all/0/1"&gt;Malek Mouhoub&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadaoui_S/0/1/0/all/0/1"&gt;Samira Sadaoui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On planetary systems as ordered sequences. (arXiv:2105.09966v1 [astro-ph.EP])]]></title>
        <id>http://arxiv.org/abs/2105.09966</id>
        <link href="http://arxiv.org/abs/2105.09966"/>
        <updated>2021-05-24T05:08:39.628Z</updated>
        <summary type="html"><![CDATA[A planetary system consists of a host star and one or more planets, arranged
into a particular configuration. Here, we consider what information belongs to
the configuration, or ordering, of 4286 Kepler planets in their 3277 planetary
systems. First, we train a neural network model to predict the radius and
period of a planet based on the properties of its host star and the radii and
period of its neighbors. The mean absolute error of the predictions of the
trained model is a factor of 2.1 better than the MAE of the predictions of a
naive model which draws randomly from dynamically allowable periods and radii.
Second, we adapt a model used for unsupervised part-of-speech tagging in
computational linguistics to investigate whether planets or planetary systems
fall into natural categories with physically interpretable "grammatical rules."
The model identifies two robust groups of planetary systems: (1) compact
multi-planet systems and (2) systems around giant stars ($\log{g} \lesssim
4.0$), although the latter group is strongly sculpted by the selection bias of
the transit method. These results reinforce the idea that planetary systems are
not random sequences -- instead, as a population, they contain predictable
patterns that can provide insight into the formation and evolution of planetary
systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Sandford_E/0/1/0/all/0/1"&gt;Emily Sandford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Kipping_D/0/1/0/all/0/1"&gt;David Kipping&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Collins_M/0/1/0/all/0/1"&gt;Michael Collins&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VisualSparta: An Embarrassingly Simple Approach to Large-scale Text-to-Image Search with Weighted Bag-of-words. (arXiv:2101.00265v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00265</id>
        <link href="http://arxiv.org/abs/2101.00265"/>
        <updated>2021-05-24T05:08:39.544Z</updated>
        <summary type="html"><![CDATA[Text-to-image retrieval is an essential task in cross-modal information
retrieval, i.e., retrieving relevant images from a large and unlabelled dataset
given textual queries. In this paper, we propose VisualSparta, a novel
(Visual-text Sparse Transformer Matching) model that shows significant
improvement in terms of both accuracy and efficiency. VisualSparta is capable
of outperforming previous state-of-the-art scalable methods in MSCOCO and
Flickr30K. We also show that it achieves substantial retrieving speed
advantages, i.e., for a 1 million image index, VisualSparta using CPU gets
~391X speedup compared to CPU vector search and ~5.4X speedup compared to
vector search with GPU acceleration. Experiments show that this speed advantage
even gets bigger for larger datasets because VisualSparta can be efficiently
implemented as an inverted index. To the best of our knowledge, VisualSparta is
the first transformer-based text-to-image retrieval model that can achieve
real-time searching for large-scale datasets, with significant accuracy
improvement compared to previous state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaopeng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tiancheng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kyusong Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-driven discovery of interpretable causal relations for deep learning material laws with uncertainty propagation. (arXiv:2105.09980v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09980</id>
        <link href="http://arxiv.org/abs/2105.09980"/>
        <updated>2021-05-24T05:08:39.522Z</updated>
        <summary type="html"><![CDATA[This paper presents a computational framework that generates ensemble
predictive mechanics models with uncertainty quantification (UQ). We first
develop a causal discovery algorithm to infer causal relations among
time-history data measured during each representative volume element (RVE)
simulation through a directed acyclic graph (DAG). With multiple plausible sets
of causal relationships estimated from multiple RVE simulations, the
predictions are propagated in the derived causal graph while using a deep
neural network equipped with dropout layers as a Bayesian approximation for
uncertainty quantification. We select two representative numerical examples
(traction-separation laws for frictional interfaces, elastoplasticity models
for granular assembles) to examine the accuracy and robustness of the proposed
causal discovery method for the common material law predictions in civil
engineering applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xiao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bahmani_B/0/1/0/all/0/1"&gt;Bahador Bahmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vlassis_N/0/1/0/all/0/1"&gt;Nikolaos N. Vlassis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;WaiChing Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yanxun Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-color balancing for correctly adjusting the intensity of target colors. (arXiv:2102.01893v2 [cs.MM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01893</id>
        <link href="http://arxiv.org/abs/2102.01893"/>
        <updated>2021-05-24T05:08:39.515Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel multi-color balance method for reducing
color distortions caused by lighting effects. The proposed method allows us to
adjust three target-colors chosen by a user in an input image so that each
target color is the same as the corresponding destination (benchmark) one. In
contrast, white balancing is a typical technique for reducing the color
distortions, however, they cannot remove lighting effects on colors other than
white. In an experiment, the proposed method is demonstrated to be able to
remove lighting effects on selected three colors, and is compared with existing
white balance adjustments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akazawa_T/0/1/0/all/0/1"&gt;Teruaki Akazawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kinoshita_Y/0/1/0/all/0/1"&gt;Yuma Kinoshita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1"&gt;Hitoshi Kiya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations. (arXiv:2006.03659v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03659</id>
        <link href="http://arxiv.org/abs/2006.03659"/>
        <updated>2021-05-24T05:08:39.454Z</updated>
        <summary type="html"><![CDATA[Sentence embeddings are an important component of many natural language
processing (NLP) systems. Like word embeddings, sentence embeddings are
typically learned on large text corpora and then transferred to various
downstream tasks, such as clustering and retrieval. Unlike word embeddings, the
highest performing solutions for learning sentence embeddings require labelled
data, limiting their usefulness to languages and domains where labelled data is
abundant. In this paper, we present DeCLUTR: Deep Contrastive Learning for
Unsupervised Textual Representations. Inspired by recent advances in deep
metric learning (DML), we carefully design a self-supervised objective for
learning universal sentence embeddings that does not require labelled training
data. When used to extend the pretraining of transformer-based language models,
our approach closes the performance gap between unsupervised and supervised
pretraining for universal sentence encoders. Importantly, our experiments
suggest that the quality of the learned embeddings scale with both the number
of trainable parameters and the amount of unlabelled training data, making
further improvements straightforward. Our code and pretrained models are
publicly available and can be easily adapted to new domains or used to embed
unseen text.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Giorgi_J/0/1/0/all/0/1"&gt;John Giorgi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nitski_O/0/1/0/all/0/1"&gt;Osvald Nitski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bader_G/0/1/0/all/0/1"&gt;Gary Bader&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring the impact of spammers on e-mail and Twitter networks. (arXiv:2105.10256v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2105.10256</id>
        <link href="http://arxiv.org/abs/2105.10256"/>
        <updated>2021-05-24T05:08:39.442Z</updated>
        <summary type="html"><![CDATA[This paper investigates the research question if senders of large amounts of
irrelevant or unsolicited information - commonly called "spammers" - distort
the network structure of social networks. Two large social networks are
analyzed, the first extracted from the Twitter discourse about a big
telecommunication company, and the second obtained from three years of email
communication of 200 managers working for a large multinational company. This
work compares network robustness and the stability of centrality and
interaction metrics, as well as the use of language, after removing spammers
and the most and least connected nodes. The results show that spammers do not
significantly alter the structure of the information-carrying network, for most
of the social indicators. The authors additionally investigate the correlation
between e-mail subject line and content by tracking language sentiment,
emotionality, and complexity, addressing the cases where collecting email
bodies is not permitted for privacy reasons. The findings extend the research
about robustness and stability of social networks metrics, after the
application of graph simplification strategies. The results have practical
implication for network analysts and for those company managers who rely on
network analytics (applied to company emails and social media data) to support
their decision-making processes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1"&gt;A. Fronzetti Colladon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gloor_P/0/1/0/all/0/1"&gt;P. A. Gloor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting the Negative Data of Distantly Supervised Relation Extraction. (arXiv:2105.10158v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10158</id>
        <link href="http://arxiv.org/abs/2105.10158"/>
        <updated>2021-05-24T05:08:39.422Z</updated>
        <summary type="html"><![CDATA[Distantly supervision automatically generates plenty of training samples for
relation extraction. However, it also incurs two major problems: noisy labels
and imbalanced training data. Previous works focus more on reducing wrongly
labeled relations (false positives) while few explore the missing relations
that are caused by incompleteness of knowledge base (false negatives).
Furthermore, the quantity of negative labels overwhelmingly surpasses the
positive ones in previous problem formulations. In this paper, we first provide
a thorough analysis of the above challenges caused by negative data. Next, we
formulate the problem of relation extraction into as a positive unlabeled
learning task to alleviate false negative problem. Thirdly, we propose a
pipeline approach, dubbed \textsc{ReRe}, that performs sentence-level relation
detection then subject/object extraction to achieve sample-efficient training.
Experimental results show that the proposed method consistently outperforms
existing approaches and remains excellent performance even learned with a large
quantity of false positive samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1"&gt;Chenhao Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jiaqing Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingping Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chengsong Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wenhao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1"&gt;Yanghua Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Unsupervised Multi-Object Tracking in Noisy Environments. (arXiv:2105.10005v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.10005</id>
        <link href="http://arxiv.org/abs/2105.10005"/>
        <updated>2021-05-24T05:08:39.414Z</updated>
        <summary type="html"><![CDATA[Camera movement and unpredictable environmental conditions like dust and wind
induce noise into video feeds. We observe that popular unsupervised MOT methods
are dependent on noise-free conditions. We show that the addition of a small
amount of artificial random noise causes a sharp degradation in model
performance on benchmark metrics. We resolve this problem by introducing a
robust unsupervised multi-object tracking (MOT) model: AttU-Net. The proposed
single-head attention model helps limit the negative impact of noise by
learning visual representations at different segment scales. AttU-Net shows
better unsupervised MOT tracking performance over variational inference-based
state-of-the-art baselines. We evaluate our method in the MNIST and the Atari
game video benchmark. We also provide two extended video datasets consisting of
complex visual patterns that include Kuzushiji characters and fashion images to
validate the effectiveness of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;C.-H. Huck Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chhabra_M/0/1/0/all/0/1"&gt;Mohit Chhabra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Y.-C. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1"&gt;Quan Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoshinaga_T/0/1/0/all/0/1"&gt;Tomoaki Yoshinaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murakam_T/0/1/0/all/0/1"&gt;Tomokazu Murakam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey of Data Augmentation Approaches for NLP. (arXiv:2105.03075v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03075</id>
        <link href="http://arxiv.org/abs/2105.03075"/>
        <updated>2021-05-24T05:08:39.407Z</updated>
        <summary type="html"><![CDATA[Data augmentation has recently seen increased interest in NLP due to more
work in low-resource domains, new tasks, and the popularity of large-scale
neural networks that require large amounts of training data. Despite this
recent upsurge, this area is still relatively underexplored, perhaps due to the
challenges posed by the discrete nature of language data. In this paper, we
present a comprehensive and unifying survey of data augmentation for NLP by
summarizing the literature in a structured manner. We first introduce and
motivate data augmentation for NLP, and then discuss major methodologically
representative approaches. Next, we highlight techniques that are used for
popular NLP applications and tasks. We conclude by outlining current challenges
and directions for future research. Overall, our paper aims to clarify the
landscape of existing literature in data augmentation for NLP and motivate
additional work in this area. We also present a GitHub repository with a paper
list that will be continuously updated at
https://github.com/styfeng/DataAug4NLP]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1"&gt;Steven Y. Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1"&gt;Varun Gangal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jason Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1"&gt;Sarath Chandar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1"&gt;Soroush Vosoughi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1"&gt;Teruko Mitamura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1"&gt;Eduard Hovy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of Emotions in Hindi-English Code Mixed Text Data. (arXiv:2105.09226v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09226</id>
        <link href="http://arxiv.org/abs/2105.09226"/>
        <updated>2021-05-24T05:08:39.399Z</updated>
        <summary type="html"><![CDATA[In recent times, we have seen an increased use of text chat for communication
on social networks and smartphones. This particularly involves the use of
Hindi-English code-mixed text which contains words which are not recognized in
English vocabulary. We have worked on detecting emotions in these mixed data
and classify the sentences in human emotions which are angry, fear, happy or
sad. We have used state of the art natural language processing models and
compared their performance on the dataset comprising sentences in this mixed
data. The dataset was collected and annotated from sources and then used to
train the models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_D/0/1/0/all/0/1"&gt;Divyansh Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SF-QA: Simple and Fair Evaluation Library for Open-domain Question Answering. (arXiv:2101.01910v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.01910</id>
        <link href="http://arxiv.org/abs/2101.01910"/>
        <updated>2021-05-24T05:08:39.389Z</updated>
        <summary type="html"><![CDATA[Although open-domain question answering (QA) draws great attention in recent
years, it requires large amounts of resources for building the full system and
is often difficult to reproduce previous results due to complex configurations.
In this paper, we introduce SF-QA: simple and fair evaluation framework for
open-domain QA. SF-QA framework modularizes the pipeline open-domain QA system,
which makes the task itself easily accessible and reproducible to research
groups without enough computing resources. The proposed evaluation framework is
publicly available and anyone can contribute to the code and evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaopeng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kyusong Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tiancheng Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Block Design for Learned Fractional Downsampling. (arXiv:2105.09999v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.09999</id>
        <link href="http://arxiv.org/abs/2105.09999"/>
        <updated>2021-05-24T05:08:39.334Z</updated>
        <summary type="html"><![CDATA[The layers of convolutional neural networks (CNNs) can be used to alter the
resolution of their inputs, but the scaling factors are limited to integer
values. However, in many image and video processing applications, the ability
to resize by a fractional factor would be advantageous. One example is
conversion between resolutions standardized for video compression, such as from
1080p to 720p. To solve this problem, we propose an alternative building block,
formulated as a conventional convolutional layer followed by a differentiable
resizer. More concretely, the convolutional layer preserves the resolution of
the input, while the resizing operation is fully handled by the resizer. In
this way, any CNN architecture can be adapted for non-integer resizing. As an
application, we replace the resizing convolutional layer of a modern deep
downsampling model by the proposed building block, and apply it to an adaptive
bitrate video streaming scenario. Our experimental results show that an
improvement in coding efficiency over the conventional Lanczos algorithm is
attained, in terms of PSNR, SSIM, and VMAF on test videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1"&gt;Li-Heng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bampis_C/0/1/0/all/0/1"&gt;Christos G. Bampis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bovik_A/0/1/0/all/0/1"&gt;Alan C. Bovik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from My Friends: Few-Shot Personalized Conversation Systems via Social Networks. (arXiv:2105.10323v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10323</id>
        <link href="http://arxiv.org/abs/2105.10323"/>
        <updated>2021-05-24T05:08:39.325Z</updated>
        <summary type="html"><![CDATA[Personalized conversation models (PCMs) generate responses according to
speaker preferences. Existing personalized conversation tasks typically require
models to extract speaker preferences from user descriptions or their
conversation histories, which are scarce for newcomers and inactive users. In
this paper, we propose a few-shot personalized conversation task with an
auxiliary social network. The task requires models to generate personalized
responses for a speaker given a few conversations from the speaker and a social
network. Existing methods are mainly designed to incorporate descriptions or
conversation histories. Those methods can hardly model speakers with so few
conversations or connections between speakers. To better cater for newcomers
with few resources, we propose a personalized conversation model (PCM) that
learns to adapt to new speakers as well as enabling new speakers to learn from
resource-rich speakers. Particularly, based on a meta-learning based PCM, we
propose a task aggregator (TA) to collect other speakers' information from the
social network. The TA provides prior knowledge of the new speaker in its
meta-learning. Experimental results show our methods outperform all baselines
in appropriateness, diversity, and consistency with speakers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1"&gt;Zhiliang Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_W/0/1/0/all/0/1"&gt;Wei Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zihan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dongkyu Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yiping Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Nevin L. Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Partner Matters! An Empirical Study on Fusing Personas for Personalized Response Selection in Retrieval-Based Chatbots. (arXiv:2105.09050v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09050</id>
        <link href="http://arxiv.org/abs/2105.09050"/>
        <updated>2021-05-24T05:08:39.305Z</updated>
        <summary type="html"><![CDATA[Persona can function as the prior knowledge for maintaining the consistency
of dialogue systems. Most of previous studies adopted the self persona in
dialogue whose response was about to be selected from a set of candidates or
directly generated, but few have noticed the role of partner in dialogue. This
paper makes an attempt to thoroughly explore the impact of utilizing personas
that describe either self or partner speakers on the task of response selection
in retrieval-based chatbots. Four persona fusion strategies are designed, which
assume personas interact with contexts or responses in different ways. These
strategies are implemented into three representative models for response
selection, which are based on the Hierarchical Recurrent Encoder (HRE),
Interactive Matching Network (IMN) and Bidirectional Encoder Representations
from Transformers (BERT) respectively. Empirical studies on the Persona-Chat
dataset show that the partner personas neglected in previous studies can
improve the accuracy of response selection in the IMN- and BERT-based models.
Besides, our BERT-based model implemented with the context-response-aware
persona fusion strategy outperforms previous methods by margins larger than
2.7% on original personas and 4.6% on revised personas in terms of hits@1
(top-1 accuracy), achieving a new state-of-the-art performance on the
Persona-Chat dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jia-Chen Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1"&gt;Zhen-Hua Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Quan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhigang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaodan Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Functionals in the Clouds: An abstract architecture of serverless Cloud-Native Apps. (arXiv:2105.10362v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10362</id>
        <link href="http://arxiv.org/abs/2105.10362"/>
        <updated>2021-05-24T05:08:39.287Z</updated>
        <summary type="html"><![CDATA[Cloud Native Application CNApp (as a distributed system) is a collection of
independent components (micro-services) interacting via communication
protocols. This gives rise to present an abstract architecture of CNApp as
dynamically re-configurable acyclic directed multi graph where vertices are
microservices, and edges are the protocols. Generic mechanisms for such
reconfigurations evidently correspond to higher-level functions (functionals).
This implies also internal abstract architecture of microservice as a
collection of event-triggered serverless functions (including functions
implementing the protocols) that are dynamically composed into event-dependent
data-flow graphs. Again, generic mechanisms for such compositions correspond to
calculus of functionals and relations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ambroszkiewicz_S/0/1/0/all/0/1"&gt;Stanislaw Ambroszkiewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bartyna_W/0/1/0/all/0/1"&gt;Waldemar Bartyna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bylka_S/0/1/0/all/0/1"&gt;Stanislaw Bylka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pretrained Language Models for Text Generation: A Survey. (arXiv:2105.10311v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10311</id>
        <link href="http://arxiv.org/abs/2105.10311"/>
        <updated>2021-05-24T05:08:39.249Z</updated>
        <summary type="html"><![CDATA[Text generation has become one of the most important yet challenging tasks in
natural language processing (NLP). The resurgence of deep learning has greatly
advanced this field by neural generation models, especially the paradigm of
pretrained language models (PLMs). In this paper, we present an overview of the
major advances achieved in the topic of PLMs for text generation. As the
preliminaries, we present the general task definition and briefly describe the
mainstream architectures of PLMs for text generation. As the core content, we
discuss how to adapt existing PLMs to model different input data and satisfy
special properties in the generated text. We further summarize several
important fine-tuning strategies for text generation. Finally, we present
several future directions and conclude this paper. Our survey aims to provide
text generation researchers a synthesis and pointer to related research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Junyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1"&gt;Tianyi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wayne Xin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Ji-Rong Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unit Test Case Generation with Transformers and Focal Context. (arXiv:2009.05617v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.05617</id>
        <link href="http://arxiv.org/abs/2009.05617"/>
        <updated>2021-05-24T05:08:39.225Z</updated>
        <summary type="html"><![CDATA[Automated unit test case generation tools facilitate test-driven development
and support developers by suggesting tests intended to identify flaws in their
code. Existing approaches are usually guided by the test coverage criteria,
generating synthetic test cases that are often difficult for developers to read
or understand. In this paper we propose AthenaTest, an approach that aims to
generate unit test cases by learning from real-world focal methods and
developer-written testcases. We formulate unit test case generation as a
sequence-to-sequence learning task, adopting a two-step training procedure
consisting of denoising pretraining on a large unsupervised Java corpus, and
supervised finetuning for a downstream translation task of generating unit
tests. We investigate the impact of natural language and source code
pretraining, as well as the focal context information surrounding the focal
method. Both techniques provide improvements in terms of validation loss, with
pretraining yielding 25% relative improvement and focal context providing
additional 11.1% improvement. We also introduce Methods2Test, the largest
publicly available supervised parallel corpus of unit test case methods and
corresponding focal methods in Java, which comprises 780K test cases mined from
91K open-source repositories from GitHub. We evaluate AthenaTest on five
defects4j projects, generating 25K passing test cases covering 43.7% of the
focal methods with only 30 attempts. We execute the test cases, collect test
coverage information, and compare them with test cases generated by EvoSuite
and GPT-3, finding that our approach outperforms GPT-3 and has comparable
coverage w.r.t. EvoSuite. Finally, we survey professional developers on their
preference in terms of readability, understandability, and testing
effectiveness of the generated tests, showing overwhelmingly preference towards
AthenaTest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tufano_M/0/1/0/all/0/1"&gt;Michele Tufano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1"&gt;Dawn Drain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Svyatkovskiy_A/0/1/0/all/0/1"&gt;Alexey Svyatkovskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1"&gt;Shao Kun Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1"&gt;Neel Sundaresan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fact-driven Logical Reasoning. (arXiv:2105.10334v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10334</id>
        <link href="http://arxiv.org/abs/2105.10334"/>
        <updated>2021-05-24T05:08:39.208Z</updated>
        <summary type="html"><![CDATA[Logical reasoning, which is closely related to human cognition, is of vital
importance in human's understanding of texts. Recent years have witnessed
increasing attentions on machine's logical reasoning abilities. However,
previous studies commonly apply ad-hoc methods to model pre-defined relation
patterns, such as linking named entities, which only considers global knowledge
components that are related to commonsense, without local perception of
complete facts or events. Such methodology is obviously insufficient to deal
with complicated logical structures. Therefore, we argue that the natural logic
units would be the group of backbone constituents of the sentence such as the
subject-verb-object formed "facts", covering both global and local knowledge
pieces that are necessary as the basis for logical reasoning. Beyond building
the ad-hoc graphs, we propose a more general and convenient fact-driven
approach to construct a supergraph on top of our newly defined fact units, and
enhance the supergraph with further explicit guidance of local question and
option interactions. Experiments on two challenging logical reasoning benchmark
datasets, ReClor and LogiQA, show that our proposed model, \textsc{Focal
Reasoner}, outperforms the baseline models dramatically. It can also be
smoothly applied to other downstream tasks such as MuTual, a dialogue reasoning
dataset, achieving competitive results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1"&gt;Siru Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhuosheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hai Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Context-Aware Translation Models Pay the Right Attention?. (arXiv:2105.06977v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06977</id>
        <link href="http://arxiv.org/abs/2105.06977"/>
        <updated>2021-05-24T05:08:39.198Z</updated>
        <summary type="html"><![CDATA[Context-aware machine translation models are designed to leverage contextual
information, but often fail to do so. As a result, they inaccurately
disambiguate pronouns and polysemous words that require context for resolution.
In this paper, we ask several questions: What contexts do human translators use
to resolve ambiguous words? Are models paying large amounts of attention to the
same context? What if we explicitly train them to do so? To answer these
questions, we introduce SCAT (Supporting Context for Ambiguous Translations), a
new English-French dataset comprising supporting context words for 14K
translations that professional translators found useful for pronoun
disambiguation. Using SCAT, we perform an in-depth analysis of the context used
to disambiguate, examining positional and lexical characteristics of the
supporting words. Furthermore, we measure the degree of alignment between the
model's attention scores and the supporting context from SCAT, and apply a
guided attention strategy to encourage agreement between the two.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1"&gt;Kayo Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1"&gt;Patrick Fernandes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pruthi_D/0/1/0/all/0/1"&gt;Danish Pruthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1"&gt;Aditi Chaudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; F. T. Martins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1"&gt;Graham Neubig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding. (arXiv:2105.09996v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09996</id>
        <link href="http://arxiv.org/abs/2105.09996"/>
        <updated>2021-05-24T05:08:39.151Z</updated>
        <summary type="html"><![CDATA[We present a simplified, task-agnostic multi-modal pre-training approach that
can accept either video or text input, or both for a variety of end tasks.
Existing pre-training are task-specific by adopting either a single cross-modal
encoder that requires both modalities, limiting their use for retrieval-style
end tasks or more complex multitask learning with two unimodal encoders,
limiting early cross-modal fusion. We instead introduce new pretraining masking
schemes that better mix across modalities (e.g. by forcing masks for text to
predict the closest video embeddings) while also maintaining separability (e.g.
unimodal predictions are sometimes required, without using all the input).
Experimental results show strong performance across a wider range of tasks than
any previous methods, often outperforming task-specific pre-training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1"&gt;Gargi Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1"&gt;Po-Yao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arora_P/0/1/0/all/0/1"&gt;Prahal Arora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aminzadeh_M/0/1/0/all/0/1"&gt;Masoumeh Aminzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1"&gt;Christoph Feichtenhofer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1"&gt;Florian Metze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1"&gt;Luke Zettlemoyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Have you tried Neural Topic Models? Comparative Analysis of Neural and Non-Neural Topic Models with Application to COVID-19 Twitter Data. (arXiv:2105.10165v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10165</id>
        <link href="http://arxiv.org/abs/2105.10165"/>
        <updated>2021-05-24T05:08:39.143Z</updated>
        <summary type="html"><![CDATA[Topic models are widely used in studying social phenomena. We conduct a
comparative study examining state-of-the-art neural versus non-neural topic
models, performing a rigorous quantitative and qualitative assessment on a
dataset of tweets about the COVID-19 pandemic. Our results show that not only
do neural topic models outperform their classical counterparts on standard
evaluation metrics, but they also produce more coherent topics, which are of
great benefit when studying complex social problems. We also propose a novel
regularization term for neural topic models, which is designed to address the
well-documented problem of mode collapse, and demonstrate its effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bennett_A/0/1/0/all/0/1"&gt;Andrew Bennett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1"&gt;Dipendra Misra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Than_N/0/1/0/all/0/1"&gt;Nga Than&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rule Augmented Unsupervised Constituency Parsing. (arXiv:2105.10193v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10193</id>
        <link href="http://arxiv.org/abs/2105.10193"/>
        <updated>2021-05-24T05:08:39.000Z</updated>
        <summary type="html"><![CDATA[Recently, unsupervised parsing of syntactic trees has gained considerable
attention. A prototypical approach to such unsupervised parsing employs
reinforcement learning and auto-encoders. However, no mechanism ensures that
the learnt model leverages the well-understood language grammar. We propose an
approach that utilizes very generic linguistic knowledge of the language
present in the form of syntactic rules, thus inducing better syntactic
structures. We introduce a novel formulation that takes advantage of the
syntactic grammar rules and is independent of the base system. We achieve new
state-of-the-art results on two benchmarks datasets, MNLI and WSJ. The source
code of the paper is available at https://github.com/anshuln/Diora_with_rules.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sahay_A/0/1/0/all/0/1"&gt;Atul Sahay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasery_A/0/1/0/all/0/1"&gt;Anshul Nasery&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maheshwari_A/0/1/0/all/0/1"&gt;Ayush Maheshwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1"&gt;Ganesh Ramakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1"&gt;Rishabh Iyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conversational Machine Reading Comprehension for Vietnamese Healthcare Texts. (arXiv:2105.01542v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01542</id>
        <link href="http://arxiv.org/abs/2105.01542"/>
        <updated>2021-05-24T05:08:38.979Z</updated>
        <summary type="html"><![CDATA[Machine reading comprehension (MRC) is a sub-field in natural language
processing that aims to help computers understand unstructured texts and then
answer questions related to them. In practice, conversation is an essential way
to communicate and transfer information. To help machines understand
conversation texts, we present UIT-ViCoQA - a new corpus for conversational
machine reading comprehension in the Vietnamese language. This corpus consists
of 10,000 questions with answers to over 2,000 conversations about health news
articles. Then, we evaluate several baseline approaches for conversational
machine comprehension on the UIT-ViCoQA corpus. The best model obtains an F1
score of 45.27%, which is 30.91 points behind human performance (76.18%),
indicating that there is ample room for improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1"&gt;Son T. Luu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_M/0/1/0/all/0/1"&gt;Mao Nguyen Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1"&gt;Loi Duc Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1"&gt;Khiem Vinh Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Kiet Van Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1"&gt;Ngan Luu-Thuy Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Generation and Evaluation of Visual Stories via Semantic Consistency. (arXiv:2105.10026v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10026</id>
        <link href="http://arxiv.org/abs/2105.10026"/>
        <updated>2021-05-24T05:08:38.915Z</updated>
        <summary type="html"><![CDATA[Story visualization is an under-explored task that falls at the intersection
of many important research directions in both computer vision and natural
language processing. In this task, given a series of natural language captions
which compose a story, an agent must generate a sequence of images that
correspond to the captions. Prior work has introduced recurrent generative
models which outperform text-to-image synthesis models on this task. However,
there is room for improvement of generated images in terms of visual quality,
coherence and relevance. We present a number of improvements to prior modeling
approaches, including (1) the addition of a dual learning framework that
utilizes video captioning to reinforce the semantic alignment between the story
and generated images, (2) a copy-transform mechanism for
sequentially-consistent story visualization, and (3) MART-based transformers to
model complex interactions between frames. We present ablation studies to
demonstrate the effect of each of these techniques on the generative power of
the model for both individual images as well as the entire narrative.
Furthermore, due to the complexity and generative nature of the task, standard
evaluation metrics do not accurately reflect performance. Therefore, we also
provide an exploration of evaluation metrics for the model, focused on aspects
of the generated frames such as the presence/quality of generated characters,
the relevance to captions, and the diversity of the generated images. We also
present correlation experiments of our proposed automated metrics with human
evaluations. Code and data available at:
https://github.com/adymaharana/StoryViz]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maharana_A/0/1/0/all/0/1"&gt;Adyasha Maharana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hannan_D/0/1/0/all/0/1"&gt;Darryl Hannan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Bi-Encoders for Word Sense Disambiguation. (arXiv:2105.10146v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10146</id>
        <link href="http://arxiv.org/abs/2105.10146"/>
        <updated>2021-05-24T05:08:38.868Z</updated>
        <summary type="html"><![CDATA[Modern transformer-based neural architectures yield impressive results in
nearly every NLP task and Word Sense Disambiguation, the problem of discerning
the correct sense of a word in a given context, is no exception.
State-of-the-art approaches in WSD today leverage lexical information along
with pre-trained embeddings from these models to achieve results comparable to
human inter-annotator agreement on standard evaluation benchmarks. In the same
vein, we experiment with several strategies to optimize bi-encoders for this
specific task and propose alternative methods of presenting lexical information
to our model. Through our multi-stage pre-training and fine-tuning pipeline we
further the state of the art in Word Sense Disambiguation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kohli_H/0/1/0/all/0/1"&gt;Harsh Kohli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Non-Linear Structural Probe. (arXiv:2105.10185v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10185</id>
        <link href="http://arxiv.org/abs/2105.10185"/>
        <updated>2021-05-24T05:08:38.860Z</updated>
        <summary type="html"><![CDATA[Probes are models devised to investigate the encoding of knowledge -- e.g.
syntactic structure -- in contextual representations. Probes are often designed
for simplicity, which has led to restrictions on probe design that may not
allow for the full exploitation of the structure of encoded information; one
such restriction is linearity. We examine the case of a structural probe
(Hewitt and Manning, 2019), which aims to investigate the encoding of syntactic
structure in contextual representations through learning only linear
transformations. By observing that the structural probe learns a metric, we are
able to kernelize it and develop a novel non-linear variant with an identical
number of parameters. We test on 6 languages and find that the radial-basis
function (RBF) kernel, in conjunction with regularization, achieves a
statistically significant improvement over the baseline in all languages --
implying that at least part of the syntactic knowledge is encoded non-linearly.
We conclude by discussing how the RBF kernel resembles BERT's self-attention
layers and speculate that this resemblance leads to the RBF-based probe's
stronger performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+White_J/0/1/0/all/0/1"&gt;Jennifer C. White&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1"&gt;Tiago Pimentel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1"&gt;Naomi Saphra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Automatic Comparison of Data Privacy Documents: A Preliminary Experiment on GDPR-like Laws. (arXiv:2105.10117v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10117</id>
        <link href="http://arxiv.org/abs/2105.10117"/>
        <updated>2021-05-24T05:08:38.850Z</updated>
        <summary type="html"><![CDATA[General Data Protection Regulation (GDPR) becomes a standard law for data
protection in many countries. Currently, twelve countries adopt the regulation
and establish their GDPR-like regulation. However, to evaluate the differences
and similarities of these GDPR-like regulations is time-consuming and needs a
lot of manual effort from legal experts. Moreover, GDPR-like regulations from
different countries are written in their languages leading to a more difficult
task since legal experts who know both languages are essential. In this paper,
we investigate a simple natural language processing (NLP) approach to tackle
the problem. We first extract chunks of information from GDPR-like documents
and form structured data from natural language. Next, we use NLP methods to
compare documents to measure their similarity. Finally, we manually label a
small set of data to evaluate our approach. The empirical result shows that the
BERT model with cosine similarity outperforms other baselines. Our data and
code are publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kawintiranon_K/0/1/0/all/0/1"&gt;Kornraphop Kawintiranon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yaguang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Streaming End-to-End Framework For Spoken Language Understanding. (arXiv:2105.10042v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10042</id>
        <link href="http://arxiv.org/abs/2105.10042"/>
        <updated>2021-05-24T05:08:38.840Z</updated>
        <summary type="html"><![CDATA[End-to-end spoken language understanding (SLU) has recently attracted
increasing interest. Compared to the conventional tandem-based approach that
combines speech recognition and language understanding as separate modules, the
new approach extracts users' intentions directly from the speech signals,
resulting in joint optimization and low latency. Such an approach, however, is
typically designed to process one intention at a time, which leads users to
take multiple rounds to fulfill their requirements while interacting with a
dialogue system. In this paper, we propose a streaming end-to-end framework
that can process multiple intentions in an online and incremental way. The
backbone of our framework is a unidirectional RNN trained with the
connectionist temporal classification (CTC) criterion. By this design, an
intention can be identified when sufficient evidence has been accumulated, and
multiple intentions can be identified sequentially. We evaluate our solution on
the Fluent Speech Commands (FSC) dataset and the intent detection accuracy is
about 97 % on all multi-intent settings. This result is comparable to the
performance of the state-of-the-art non-streaming models, but is achieved in an
online and incremental way. We also employ our model to a keyword spotting task
using the Google Speech Commands dataset and the results are also highly
promising.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Potdar_N/0/1/0/all/0/1"&gt;Nihal Potdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avila_A/0/1/0/all/0/1"&gt;Anderson R. Avila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1"&gt;Chao Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yiran Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiao Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Happy Dance, Slow Clap: Using Reaction GIFs to Predict Induced Affect on Twitter. (arXiv:2105.09967v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09967</id>
        <link href="http://arxiv.org/abs/2105.09967"/>
        <updated>2021-05-24T05:08:38.828Z</updated>
        <summary type="html"><![CDATA[Datasets with induced emotion labels are scarce but of utmost importance for
many NLP tasks. We present a new, automated method for collecting texts along
with their induced reaction labels. The method exploits the online use of
reaction GIFs, which capture complex affective states. We show how to augment
the data with induced emotion and induced sentiment labels. We use our method
to create and publish ReactionGIF, a first-of-its-kind affective dataset of 30K
tweets. We provide baselines for three new tasks, including induced sentiment
prediction and multilabel classification of induced emotions. Our method and
dataset open new research opportunities in emotion detection and affective
computing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shmueli_B/0/1/0/all/0/1"&gt;Boaz Shmueli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ray_S/0/1/0/all/0/1"&gt;Soumya Ray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ku_L/0/1/0/all/0/1"&gt;Lun-Wei Ku&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SF-QA: Simple and Fair Evaluation Library for Open-domain Question Answering. (arXiv:2101.01910v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.01910</id>
        <link href="http://arxiv.org/abs/2101.01910"/>
        <updated>2021-05-24T05:08:38.800Z</updated>
        <summary type="html"><![CDATA[Although open-domain question answering (QA) draws great attention in recent
years, it requires large amounts of resources for building the full system and
is often difficult to reproduce previous results due to complex configurations.
In this paper, we introduce SF-QA: simple and fair evaluation framework for
open-domain QA. SF-QA framework modularizes the pipeline open-domain QA system,
which makes the task itself easily accessible and reproducible to research
groups without enough computing resources. The proposed evaluation framework is
publicly available and anyone can contribute to the code and evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaopeng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kyusong Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tiancheng Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty-Aware Abstractive Summarization. (arXiv:2105.10155v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10155</id>
        <link href="http://arxiv.org/abs/2105.10155"/>
        <updated>2021-05-24T05:08:38.791Z</updated>
        <summary type="html"><![CDATA[We propose a novel approach to summarization based on Bayesian deep learning.
We approximate Bayesian summary generation by first extending state-of-the-art
summarization models with Monte Carlo dropout and then using them to perform
multiple stochastic forward passes. This method allows us to improve
summarization performance by simply using the median of multiple stochastic
summaries. We show that our variational equivalents of BART and PEGASUS can
outperform their deterministic counterparts on multiple benchmark datasets. In
addition, we rely on Bayesian inference to measure the uncertainty of the model
when generating summaries. Having a reliable uncertainty measure, we can
improve the experience of the end user by filtering out generated summaries of
high uncertainty. Furthermore, our proposed metric could be used as a criterion
for selecting samples for annotation, and can be paired nicely with active
learning and human-in-the-loop approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gidiotis_A/0/1/0/all/0/1"&gt;Alexios Gidiotis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1"&gt;Grigorios Tsoumakas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting Span-based Joint Entity and Relation Extraction via Squence Tagging Mechanism. (arXiv:2105.10080v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10080</id>
        <link href="http://arxiv.org/abs/2105.10080"/>
        <updated>2021-05-24T05:08:38.783Z</updated>
        <summary type="html"><![CDATA[Span-based joint extraction simultaneously conducts named entity recognition
(NER) and relation extraction (RE) in text span form. Recent studies have shown
that token labels can convey crucial task-specific information and enrich token
semantics. However, as far as we know, due to completely abstain from sequence
tagging mechanism, all prior span-based work fails to use token label
in-formation. To solve this problem, we pro-pose Sequence Tagging enhanced
Span-based Network (STSN), a span-based joint extrac-tion network that is
enhanced by token BIO label information derived from sequence tag-ging based
NER. By stacking multiple atten-tion layers in depth, we design a deep neu-ral
architecture to build STSN, and each atten-tion layer consists of three basic
attention units. The deep neural architecture first learns seman-tic
representations for token labels and span-based joint extraction, and then
constructs in-formation interactions between them, which also realizes
bidirectional information interac-tions between span-based NER and RE.
Fur-thermore, we extend the BIO tagging scheme to make STSN can extract
overlapping en-tity. Experiments on three benchmark datasets show that our
model consistently outperforms previous optimal models by a large margin,
creating new state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_B/0/1/0/all/0/1"&gt;Bin Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shasha Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jie Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jun Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Huijun Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A General Method For Automatic Discovery of Powerful Interactions In Click-Through Rate Prediction. (arXiv:2105.10484v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.10484</id>
        <link href="http://arxiv.org/abs/2105.10484"/>
        <updated>2021-05-24T05:08:38.775Z</updated>
        <summary type="html"><![CDATA[Modeling powerful interactions is a critical challenge in Click-through rate
(CTR) prediction, which is one of the most typical machine learning tasks in
personalized advertising and recommender systems. Although developing
hand-crafted interactions is effective for a small number of datasets, it
generally requires laborious and tedious architecture engineering for extensive
scenarios. In recent years, several neural architecture search (NAS) methods
have been proposed for designing interactions automatically. However, existing
methods only explore limited types and connections of operators for interaction
generation, leading to low generalization ability. To address these problems,
we propose a more general automated method for building powerful interactions
named AutoPI. The main contributions of this paper are as follows: AutoPI
adopts a more general search space in which the computational graph is
generalized from existing network connections, and the interactive operators in
the edges of the graph are extracted from representative hand-crafted works. It
allows searching for various powerful feature interactions to produce higher
AUC and lower Logloss in a wide variety of applications. Besides, AutoPI
utilizes a gradient-based search strategy for exploration with a significantly
low computational cost. Experimentally, we evaluate AutoPI on a diverse suite
of benchmark datasets, demonstrating the generalizability and efficiency of
AutoPI over hand-crafted architectures and state-of-the-art NAS algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1"&gt;Ze Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinnian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yumeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiancheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1"&gt;Tanchao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lifeng Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-modal Sarcasm Detection and Humor Classification in Code-mixed Conversations. (arXiv:2105.09984v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09984</id>
        <link href="http://arxiv.org/abs/2105.09984"/>
        <updated>2021-05-24T05:08:38.766Z</updated>
        <summary type="html"><![CDATA[Sarcasm detection and humor classification are inherently subtle problems,
primarily due to their dependence on the contextual and non-verbal information.
Furthermore, existing studies in these two topics are usually constrained in
non-English languages such as Hindi, due to the unavailability of qualitative
annotated datasets. In this work, we make two major contributions considering
the above limitations: (1) we develop a Hindi-English code-mixed dataset,
MaSaC, for the multi-modal sarcasm detection and humor classification in
conversational dialog, which to our knowledge is the first dataset of its kind;
(2) we propose MSH-COMICS, a novel attention-rich neural architecture for the
utterance classification. We learn efficient utterance representation utilizing
a hierarchical attention mechanism that attends to a small portion of the input
sentence at a time. Further, we incorporate dialog-level contextual attention
mechanism to leverage the dialog history for the multi-modal classification. We
perform extensive experiments for both the tasks by varying multi-modal inputs
and various submodules of MSH-COMICS. We also conduct comparative analysis
against existing approaches. We observe that MSH-COMICS attains superior
performance over the existing models by > 1 F1-score point for the sarcasm
detection and 10 F1-score points in humor classification. We diagnose our model
and perform thorough analysis of the results to understand the superiority and
pitfalls.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bedi_M/0/1/0/all/0/1"&gt;Manjot Bedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Shivani Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1"&gt;Md Shad Akhtar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1"&gt;Tanmoy Chakraborty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ASQ: Automatically Generating Question-Answer Pairs using AMRs. (arXiv:2105.10023v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10023</id>
        <link href="http://arxiv.org/abs/2105.10023"/>
        <updated>2021-05-24T05:08:38.727Z</updated>
        <summary type="html"><![CDATA[In this work, we introduce ASQ, a tool to automatically mine questions and
answers from a sentence, using its Abstract Meaning Representation (AMR).
Previous work has made a case for using question-answer pairs to specify
predicate-argument structure of a sentence using natural language, which does
not require linguistic expertise or training. This has resulted in the creation
of datasets such as QA-SRL and QAMR, for both of which, the question-answer
pair annotations were crowdsourced. Our approach has the same end-goal, but is
automatic, making it faster and cost-effective, without compromising on the
quality and validity of the question-answer pairs thus obtained. A qualitative
evaluation of the output generated by ASQ from the AMR 2.0 data shows that the
question-answer pairs are natural and valid, and demonstrate good coverage of
the content. We run ASQ on the sentences from the QAMR dataset, to observe that
the semantic roles in QAMR are also captured by ASQ.We intend to make this tool
and the results publicly available for others to use and build upon.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rakshit_G/0/1/0/all/0/1"&gt;Geetanjali Rakshit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flanigan_J/0/1/0/all/0/1"&gt;Jeffrey Flanigan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Non-sequential Approach to Deep User Interest Model for CTR Prediction. (arXiv:2104.06312v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06312</id>
        <link href="http://arxiv.org/abs/2104.06312"/>
        <updated>2021-05-24T05:08:38.657Z</updated>
        <summary type="html"><![CDATA[Click-Through Rate (CTR) prediction plays an important role in many
industrial applications, and recently a lot of attention is paid to the deep
interest models which use attention mechanism to capture user interests from
historical behaviors. However, most current models are based on sequential
models which truncate the behavior sequences by a fixed length, thus have
difficulties in handling very long behavior sequences. Another big problem is
that sequences with the same length can be quite different in terms of time,
carrying completely different meanings. In this paper, we propose a
non-sequential approach to tackle the above problems. Specifically, we first
represent the behavior data in a sparse key-vector format, where the vector
contains rich behavior info such as time, count and category. Next, we enhance
the Deep Interest Network to take such rich information into account by a novel
attention network. The sparse representation makes it practical to handle large
scale long behavior sequences. Finally, we introduce a multidimensional
partition framework to mine behavior interactions. The framework can partition
data into custom designed time buckets to capture the interactions among
information aggregated in different time buckets. Similarly, it can also
partition the data into different categories and capture the interactions among
them. Experiments are conducted on two public datasets: one is an advertising
dataset and the other is a production recommender dataset. Our models
outperform other state-of-the-art models on both datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1"&gt;Keke Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xing Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1"&gt;Qi Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mo_L/0/1/0/all/0/1"&gt;Linjian Mo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RFID-based Article-to-Fixture Predictions in Real-World Fashion Stores. (arXiv:2105.10216v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.10216</id>
        <link href="http://arxiv.org/abs/2105.10216"/>
        <updated>2021-05-24T05:08:38.622Z</updated>
        <summary type="html"><![CDATA[In recent years, Radio Frequency Identification (RFID) technology has been
applied to improve numerous processes, such as inventory management in retail
stores. However, automatic localization of RFID-tagged goods in stores is still
a challenging problem. To address this issue, we equip fixtures (e.g., shelves)
with reference tags and use data we collect during RFID-based stocktakes to map
articles to fixtures. Knowing the location of goods enables the implementation
of several practical applications, such as automated Money Mapping (i.e., a
heat map of sales across fixtures). Specifically, we conduct controlled lab
experiments and a case-study in two fashion retail stores to evaluate our
article-to-fixture prediction approaches. The approaches are based on
calculating distances between read event time series using DTW, and clustering
of read events using DBSCAN. We find that, read events collected during
RFID-based stocktakes can be used to assign articles to fixtures with an
accuracy of more than 90%. Additionally, we conduct a pilot to investigate the
challenges related to the integration of such a localization system in the
day-to-day business of retail stores. Hence, in this paper we present an
exploratory venture into novel and practical RFID-based applications in fashion
retails stores, beyond the scope of stock management.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wolbitsch_M/0/1/0/all/0/1"&gt;Matthias W&amp;#xf6;lbitsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasler_T/0/1/0/all/0/1"&gt;Thomas Hasler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasper_P/0/1/0/all/0/1"&gt;Patrick Kasper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Helic_D/0/1/0/all/0/1"&gt;Denis Helic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walk_S/0/1/0/all/0/1"&gt;Simon Walk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diversifying Multi-aspect Search Results Using Simpson's Diversity Index. (arXiv:2105.10075v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.10075</id>
        <link href="http://arxiv.org/abs/2105.10075"/>
        <updated>2021-05-24T05:08:38.573Z</updated>
        <summary type="html"><![CDATA[In search and recommendation, diversifying the multi-aspect search results
could help with reducing redundancy, and promoting results that might not be
shown otherwise. Many previous methods have been proposed for this task.
However, previous methods do not explicitly consider the uniformity of the
number of the items' classes, or evenness, which could degrade the search and
recommendation quality. To address this problem, we introduce a novel method by
adapting the Simpson's Diversity Index from biology, which enables a more
effective and efficient quadratic search result diversification algorithm. We
also extend the method to balance the diversity between multiple aspects
through weighted factors and further improve computational complexity by
developing a fast approximation algorithm. We demonstrate the feasibility of
the proposed method using the openly available Kaggle shoes competition
dataset. Our experimental results show that our approach outperforms previous
state of the art diversification methods, while reducing computational
complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jianghong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agichtein_E/0/1/0/all/0/1"&gt;Eugene Agichtein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kallumadi_S/0/1/0/all/0/1"&gt;Surya Kallumadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Cross-Sectional Currency Strategies by Ranking Refinement with Transformer-based Architectures. (arXiv:2105.10019v1 [q-fin.PM])]]></title>
        <id>http://arxiv.org/abs/2105.10019</id>
        <link href="http://arxiv.org/abs/2105.10019"/>
        <updated>2021-05-24T05:08:38.317Z</updated>
        <summary type="html"><![CDATA[The performance of a cross-sectional currency strategy depends crucially on
accurately ranking instruments prior to portfolio construction. While this
ranking step is traditionally performed using heuristics, or by sorting outputs
produced by pointwise regression or classification models, Learning to Rank
algorithms have recently presented themselves as competitive and viable
alternatives. Despite improving ranking accuracy on average however, these
techniques do not account for the possibility that assets positioned at the
extreme ends of the ranked list -- which are ultimately used to construct the
long/short portfolios -- can assume different distributions in the input space,
and thus lead to sub-optimal strategy performance. Drawing from research in
Information Retrieval that demonstrates the utility of contextual information
embedded within top-ranked documents to learn the query's characteristics to
improve ranking, we propose an analogous approach: exploiting the features of
both out- and under-performing instruments to learn a model for refining the
original ranked list. Under a re-ranking framework, we adapt the Transformer
architecture to encode the features of extreme assets for refining our
selection of long/short instruments obtained with an initial retrieval.
Backtesting on a set of 31 currencies, our proposed methodology significantly
boosts Sharpe ratios -- by approximately 20% over the original LTR algorithms
and double that of traditional baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Poh_D/0/1/0/all/0/1"&gt;Daniel Poh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Lim_B/0/1/0/all/0/1"&gt;Bryan Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Zohren_S/0/1/0/all/0/1"&gt;Stefan Zohren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Roberts_S/0/1/0/all/0/1"&gt;Stephen Roberts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RLIRank: Learning to Rank with Reinforcement Learning for Dynamic Search. (arXiv:2105.10124v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.10124</id>
        <link href="http://arxiv.org/abs/2105.10124"/>
        <updated>2021-05-24T05:08:38.302Z</updated>
        <summary type="html"><![CDATA[To support complex search tasks, where the initial information requirements
are complex or may change during the search, a search engine must adapt the
information delivery as the user's information requirements evolve. To support
this dynamic ranking paradigm effectively, search result ranking must
incorporate both the user feedback received, and the information displayed so
far. To address this problem, we introduce a novel reinforcement learning-based
approach, RLIrank. We first build an adapted reinforcement learning framework
to integrate the key components of the dynamic search. Then, we implement a new
Learning to Rank (LTR) model for each iteration of the dynamic search, using a
recurrent Long Short Term Memory neural network (LSTM), which estimates the
gain for each next result, learning from each previously ranked document. To
incorporate the user's feedback, we develop a word-embedding variation of the
classic Rocchio Algorithm, to help guide the ranking towards the high-value
documents. Those innovations enable RLIrank to outperform the previously
reported methods from the TREC Dynamic Domain Tracks 2017 and exceed all the
methods in 2016 TREC Dynamic Domain after multiple search iterations, advancing
the state of the art for dynamic search.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jianghong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agichtein_E/0/1/0/all/0/1"&gt;Eugene Agichtein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring the impact of spammers on e-mail and Twitter networks. (arXiv:2105.10256v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2105.10256</id>
        <link href="http://arxiv.org/abs/2105.10256"/>
        <updated>2021-05-24T05:08:38.286Z</updated>
        <summary type="html"><![CDATA[This paper investigates the research question if senders of large amounts of
irrelevant or unsolicited information - commonly called "spammers" - distort
the network structure of social networks. Two large social networks are
analyzed, the first extracted from the Twitter discourse about a big
telecommunication company, and the second obtained from three years of email
communication of 200 managers working for a large multinational company. This
work compares network robustness and the stability of centrality and
interaction metrics, as well as the use of language, after removing spammers
and the most and least connected nodes. The results show that spammers do not
significantly alter the structure of the information-carrying network, for most
of the social indicators. The authors additionally investigate the correlation
between e-mail subject line and content by tracking language sentiment,
emotionality, and complexity, addressing the cases where collecting email
bodies is not permitted for privacy reasons. The findings extend the research
about robustness and stability of social networks metrics, after the
application of graph simplification strategies. The results have practical
implication for network analysts and for those company managers who rely on
network analytics (applied to company emails and social media data) to support
their decision-making processes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1"&gt;A. Fronzetti Colladon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gloor_P/0/1/0/all/0/1"&gt;P. A. Gloor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Automatic Comparison of Data Privacy Documents: A Preliminary Experiment on GDPR-like Laws. (arXiv:2105.10117v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10117</id>
        <link href="http://arxiv.org/abs/2105.10117"/>
        <updated>2021-05-24T05:08:38.223Z</updated>
        <summary type="html"><![CDATA[General Data Protection Regulation (GDPR) becomes a standard law for data
protection in many countries. Currently, twelve countries adopt the regulation
and establish their GDPR-like regulation. However, to evaluate the differences
and similarities of these GDPR-like regulations is time-consuming and needs a
lot of manual effort from legal experts. Moreover, GDPR-like regulations from
different countries are written in their languages leading to a more difficult
task since legal experts who know both languages are essential. In this paper,
we investigate a simple natural language processing (NLP) approach to tackle
the problem. We first extract chunks of information from GDPR-like documents
and form structured data from natural language. Next, we use NLP methods to
compare documents to measure their similarity. Finally, we manually label a
small set of data to evaluate our approach. The empirical result shows that the
BERT model with cosine similarity outperforms other baselines. Our data and
code are publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kawintiranon_K/0/1/0/all/0/1"&gt;Kornraphop Kawintiranon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yaguang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[De-Biased Modelling of Search Click Behavior with Reinforcement Learning. (arXiv:2105.10072v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.10072</id>
        <link href="http://arxiv.org/abs/2105.10072"/>
        <updated>2021-05-24T05:08:38.210Z</updated>
        <summary type="html"><![CDATA[Users' clicks on Web search results are one of the key signals for evaluating
and improving web search quality and have been widely used as part of current
state-of-the-art Learning-To-Rank(LTR) models. With a large volume of search
logs available for major search engines, effective models of searcher click
behavior have emerged to evaluate and train LTR models. However, when modeling
the users' click behavior, considering the bias of the behavior is imperative.
In particular, when a search result is not clicked, it is not necessarily
chosen as not relevant by the user, but instead could have been simply missed,
especially for lower-ranked results. These kinds of biases in the click log
data can be incorporated into the click models, propagating the errors to the
resulting LTR ranking models or evaluation metrics. In this paper, we propose
the De-biased Reinforcement Learning Click model (DRLC). The DRLC model relaxes
previously made assumptions about the users' examination behavior and resulting
latent states. To implement the DRLC model, convolutional neural networks are
used as the value networks for reinforcement learning, trained to learn a
policy to reduce bias in the click logs. To demonstrate the effectiveness of
the DRLC model, we first compare performance with the previous state-of-art
approaches using established click prediction metrics, including log-likelihood
and perplexity. We further show that DRLC also leads to improvements in ranking
performance. Our experiments demonstrate the effectiveness of the DRLC model in
learning to reduce bias in click logs, leading to improved modeling performance
and showing the potential for using DRLC for improving Web search quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jianghong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zahiri_S/0/1/0/all/0/1"&gt;Sayyed M. Zahiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hughes_S/0/1/0/all/0/1"&gt;Simon Hughes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jadda_K/0/1/0/all/0/1"&gt;Khalifeh Al Jadda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kallumadi_S/0/1/0/all/0/1"&gt;Surya Kallumadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agichtein_E/0/1/0/all/0/1"&gt;Eugene Agichtein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Load Balanced Recommendation Approach. (arXiv:2105.09981v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.09981</id>
        <link href="http://arxiv.org/abs/2105.09981"/>
        <updated>2021-05-24T05:08:38.183Z</updated>
        <summary type="html"><![CDATA[Recommender systems (RSs) are software tools and algorithms developed to
alleviate the problem of information overload, which makes it difficult for a
user to make right decisions. Two main paradigms toward the recommendation
problem are collaborative filtering and content-based filtering, which try to
recommend the best items using ratings and content available. These methods
typically face infamous problems including cold-start, diversity, scalability,
and great computational expense. We argue that the uptake of deep learning and
reinforcement learning methods is also questionable due to their computational
complexities and uninterpretability. In this paper, we approach the
recommendation problem from a new prospective. We borrow ideas from cluster
head selection algorithms in wireless sensor networks and adapt them to the
recommendation problem. In particular, we propose Load Balanced Recommender
System (LBRS), which uses a probabilistic scheme for item recommendation.
Furthermore, we factor in the importance of items in the recommendation
process, which significantly improves the recommendation accuracy. We also
introduce a method that considers a heterogeneity among items, in order to
balance the similarity and diversity trade-off. Finally, we propose a new
metric for diversity, which emphasizes the importance of diversity not only
from an intra-list perspective, but also from a between-list point of view.
With experiments in a simulation study performed on RecSim, we show that LBRS
is effective and can outperform baseline methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Afsar_M/0/1/0/all/0/1"&gt;Mehdi Afsar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crump_T/0/1/0/all/0/1"&gt;Trafford Crump&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Far_B/0/1/0/all/0/1"&gt;Behrouz Far&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Mixed-Objective Pointing Decoders for Block-Level Optimization in Search Recommendation. (arXiv:2105.10152v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.10152</id>
        <link href="http://arxiv.org/abs/2105.10152"/>
        <updated>2021-05-24T05:08:38.166Z</updated>
        <summary type="html"><![CDATA[Related or ideal follow-up suggestions to a web query in search engines are
often optimized based on several different parameters -- relevance to the
original query, diversity, click probability etc. One or many rankers may be
trained to score each suggestion from a candidate pool based on these factors.
These scorers are usually pairwise classification tasks where each training
example consists of a user query and a single suggestion from the list of
candidates. We propose an architecture that takes all candidate suggestions
associated with a given query and outputs a suggestion block. We discuss the
benefits of such an architecture over traditional approaches and experiment
with further enforcing each individual metric through mixed-objective training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kohli_H/0/1/0/all/0/1"&gt;Harsh Kohli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Have you tried Neural Topic Models? Comparative Analysis of Neural and Non-Neural Topic Models with Application to COVID-19 Twitter Data. (arXiv:2105.10165v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.10165</id>
        <link href="http://arxiv.org/abs/2105.10165"/>
        <updated>2021-05-24T05:08:38.132Z</updated>
        <summary type="html"><![CDATA[Topic models are widely used in studying social phenomena. We conduct a
comparative study examining state-of-the-art neural versus non-neural topic
models, performing a rigorous quantitative and qualitative assessment on a
dataset of tweets about the COVID-19 pandemic. Our results show that not only
do neural topic models outperform their classical counterparts on standard
evaluation metrics, but they also produce more coherent topics, which are of
great benefit when studying complex social problems. We also propose a novel
regularization term for neural topic models, which is designed to address the
well-documented problem of mode collapse, and demonstrate its effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bennett_A/0/1/0/all/0/1"&gt;Andrew Bennett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1"&gt;Dipendra Misra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Than_N/0/1/0/all/0/1"&gt;Nga Than&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The MAMe Dataset: On the relevance of High Resolution and Variable Shape image properties. (arXiv:2007.13693v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.13693</id>
        <link href="http://arxiv.org/abs/2007.13693"/>
        <updated>2021-05-23T06:10:40.919Z</updated>
        <summary type="html"><![CDATA[In the image classification task, the most common approach is to resize all
images in a dataset to a unique shape, while reducing their precision to a size
which facilitates experimentation at scale. This practice has benefits from a
computational perspective, but it entails negative side-effects on performance
due to loss of information and image deformation. In this work we introduce the
MAMe dataset, an image classification dataset with remarkable high resolution
and variable shape properties. The goal of MAMe is to provide a tool for
studying the impact of such properties in image classification, while
motivating research in the field. The MAMe dataset contains thousands of
artworks from three different museums, and proposes a classification task
consisting on differentiating between 29 mediums (i.e. materials and
techniques) supervised by art experts. After reviewing the singularity of MAMe
in the context of current image classification tasks, a thorough description of
the task is provided, together with dataset statistics. Experiments are
conducted to evaluate the impact of using high resolution images, variable
shape inputs and both properties at the same time. Results illustrate the
positive impact in performance when using high resolution images, while
highlighting the lack of solutions to exploit variable shapes. An additional
experiment exposes the distinctiveness between the MAMe dataset and the
prototypical ImageNet dataset. Finally, the baselines are inspected using
explainability methods and expert knowledge, to gain insights on the challenges
that remain ahead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pares_F/0/1/0/all/0/1"&gt;Ferran Par&amp;#xe9;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arias_Duart_A/0/1/0/all/0/1"&gt;Anna Arias-Duart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_Gasulla_D/0/1/0/all/0/1"&gt;Dario Garcia-Gasulla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Campo_Frances_G/0/1/0/all/0/1"&gt;Gema Campo-Franc&amp;#xe9;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viladrich_N/0/1/0/all/0/1"&gt;Nina Viladrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayguade_E/0/1/0/all/0/1"&gt;Eduard Ayguad&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labarta_J/0/1/0/all/0/1"&gt;Jes&amp;#xfa;s Labarta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EiGLasso for Scalable Sparse Kronecker-Sum Inverse Covariance Estimation. (arXiv:2105.09872v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.09872</id>
        <link href="http://arxiv.org/abs/2105.09872"/>
        <updated>2021-05-23T06:10:40.912Z</updated>
        <summary type="html"><![CDATA[In many real-world problems, complex dependencies are present both among
samples and among features. The Kronecker sum or the Cartesian product of two
graphs, each modeling dependencies across features and across samples, has been
used as an inverse covariance matrix for a matrix-variate Gaussian
distribution, as an alternative to a Kronecker-product inverse covariance
matrix, due to its more intuitive sparse structure. However, the existing
methods for sparse Kronecker-sum inverse covariance estimation are limited in
that they do not scale to more than a few hundred features and samples and that
the unidentifiable parameters pose challenges in estimation. In this paper, we
introduce EiGLasso, a highly scalable method for sparse Kronecker-sum inverse
covariance estimation, based on Newton's method combined with
eigendecomposition of the two graphs for exploiting the structure of Kronecker
sum. EiGLasso further reduces computation time by approximating the Hessian
based on the eigendecomposition of the sample and feature graphs. EiGLasso
achieves quadratic convergence with the exact Hessian and linear convergence
with the approximate Hessian. We describe a simple new approach to estimating
the unidentifiable parameters that generalizes the existing methods. On
simulated and real-world data, we demonstrate that EiGLasso achieves two to
three orders-of-magnitude speed-up compared to the existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yoon_J/0/1/0/all/0/1"&gt;Jun Ho Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seyoung Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning and Information in Stochastic Networks and Queues. (arXiv:2105.08769v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08769</id>
        <link href="http://arxiv.org/abs/2105.08769"/>
        <updated>2021-05-23T06:10:40.895Z</updated>
        <summary type="html"><![CDATA[We review the role of information and learning in the stability and
optimization of queueing systems. In recent years, techniques from supervised
learning, bandit learning and reinforcement learning have been applied to
queueing systems supported by increasing role of information in decision
making. We present observations and new results that help rationalize the
application of these areas to queueing systems.

We prove that the MaxWeight and BackPressure policies are an application of
Blackwell's Approachability Theorem. This connects queueing theoretic results
with adversarial learning. We then discuss the requirements of statistical
learning for service parameter estimation. As an example, we show how queue
size regret can be bounded when applying a perceptron algorithm to classify
service. Next, we discuss the role of state information in improved decision
making. Here we contrast the roles of epistemic information (information on
uncertain parameters) and aleatoric information (information on an uncertain
state). Finally we review recent advances in the theory of reinforcement
learning and queueing, as well as, provide discussion on current research
challenges.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Walton_N/0/1/0/all/0/1"&gt;Neil Walton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kuang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Graph Neural Networks without explicit negative sampling. (arXiv:2103.14958v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14958</id>
        <link href="http://arxiv.org/abs/2103.14958"/>
        <updated>2021-05-23T06:10:40.871Z</updated>
        <summary type="html"><![CDATA[Real world data is mostly unlabeled or only few instances are labeled.
Manually labeling data is a very expensive and daunting task. This calls for
unsupervised learning techniques that are powerful enough to achieve comparable
results as semi-supervised/supervised techniques. Contrastive self-supervised
learning has emerged as a powerful direction, in some cases outperforming
supervised techniques. In this study, we propose, SelfGNN, a novel contrastive
self-supervised graph neural network (GNN) without relying on explicit
contrastive terms. We leverage Batch Normalization, which introduces implicit
contrastive terms, without sacrificing performance. Furthermore, as data
augmentation is key in contrastive learning, we introduce four feature
augmentation (FA) techniques for graphs. Though graph topological augmentation
(TA) is commonly used, our empirical findings show that FA perform as good as
TA. Moreover, FA incurs no computational overhead, unlike TA, which often has
O(N^3) time complexity, N-number of nodes. Our empirical evaluation on seven
publicly available real-world data shows that, SelfGNN is powerful and leads to
a performance comparable with SOTA supervised GNNs and always better than SOTA
semi-supervised and unsupervised GNNs. The source code is available at
https://github.com/zekarias-tilahun/SelfGNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kefato_Z/0/1/0/all/0/1"&gt;Zekarias T. Kefato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Girdzijauskas_S/0/1/0/all/0/1"&gt;Sarunas Girdzijauskas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilingual Offensive Language Identification for Low-resource Languages. (arXiv:2105.05996v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05996</id>
        <link href="http://arxiv.org/abs/2105.05996"/>
        <updated>2021-05-23T06:10:40.860Z</updated>
        <summary type="html"><![CDATA[Offensive content is pervasive in social media and a reason for concern to
companies and government organizations. Several studies have been recently
published investigating methods to detect the various forms of such content
(e.g. hate speech, cyberbullying, and cyberaggression). The clear majority of
these studies deal with English partially because most annotated datasets
available contain English data. In this paper, we take advantage of available
English datasets by applying cross-lingual contextual word embeddings and
transfer learning to make predictions in low-resource languages. We project
predictions on comparable data in Arabic, Bengali, Danish, Greek, Hindi,
Spanish, and Turkish. We report results of 0.8415 F1 macro for Bengali in
TRAC-2 shared task, 0.8532 F1 macro for Danish and 0.8701 F1 macro for Greek in
OffensEval 2020, 0.8568 F1 macro for Hindi in HASOC 2019 shared task and 0.7513
F1 macro for Spanish in in SemEval-2019 Task 5 (HatEval) showing that our
approach compares favourably to the best systems submitted to recent shared
tasks on these three languages. Additionally, we report competitive performance
on Arabic, and Turkish using the training and development sets of OffensEval
2020 shared task. The results for all languages confirm the robustness of
cross-lingual contextual embeddings and transfer learning for this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1"&gt;Tharindu Ranasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1"&gt;Marcos Zampieri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practical One-Shot Federated Learning for Cross-Silo Setting. (arXiv:2010.01017v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.01017</id>
        <link href="http://arxiv.org/abs/2010.01017"/>
        <updated>2021-05-23T06:10:40.852Z</updated>
        <summary type="html"><![CDATA[Federated learning enables multiple parties to collaboratively learn a model
without exchanging their data. While most existing federated learning
algorithms need many rounds to converge, one-shot federated learning (i.e.,
federated learning with a single communication round) is a promising approach
to make federated learning applicable in cross-silo setting in practice.
However, existing one-shot algorithms only support specific models and do not
provide any privacy guarantees, which significantly limit the applications in
practice. In this paper, we propose a practical one-shot federated learning
algorithm named FedKT. By utilizing the knowledge transfer technique, FedKT can
be applied to any classification models and can flexibly achieve differential
privacy guarantees. Our experiments on various tasks show that FedKT can
significantly outperform the other state-of-the-art federated learning
algorithms with a single communication round.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qinbin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1"&gt;Bingsheng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1"&gt;Dawn Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auto-Tuned Sim-to-Real Transfer. (arXiv:2104.07662v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07662</id>
        <link href="http://arxiv.org/abs/2104.07662"/>
        <updated>2021-05-23T06:10:40.837Z</updated>
        <summary type="html"><![CDATA[Policies trained in simulation often fail when transferred to the real world
due to the `reality gap' where the simulator is unable to accurately capture
the dynamics and visual properties of the real world. Current approaches to
tackle this problem, such as domain randomization, require prior knowledge and
engineering to determine how much to randomize system parameters in order to
learn a policy that is robust to sim-to-real transfer while also not being too
conservative. We propose a method for automatically tuning simulator system
parameters to match the real world using only raw RGB images of the real world
without the need to define rewards or estimate state. Our key insight is to
reframe the auto-tuning of parameters as a search problem where we iteratively
shift the simulation system parameters to approach the real-world system
parameters. We propose a Search Param Model (SPM) that, given a sequence of
observations and actions and a set of system parameters, predicts whether the
given parameters are higher or lower than the true parameters used to generate
the observations. We evaluate our method on multiple robotic control tasks in
both sim-to-sim and sim-to-real transfer, demonstrating significant improvement
over naive domain randomization. Project videos and code at
https://yuqingd.github.io/autotuned-sim2real/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yuqing Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watkins_O/0/1/0/all/0/1"&gt;Olivia Watkins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1"&gt;Trevor Darrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1"&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1"&gt;Deepak Pathak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variability of Artificial Neural Networks. (arXiv:2105.08911v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08911</id>
        <link href="http://arxiv.org/abs/2105.08911"/>
        <updated>2021-05-23T06:10:40.822Z</updated>
        <summary type="html"><![CDATA[What makes an artificial neural network easier to train and more likely to
produce desirable solutions than other comparable networks? In this paper, we
provide a new angle to study such issues under the setting of a fixed number of
model parameters which in general is the most dominant cost factor. We
introduce a notion of variability and show that it correlates positively to the
activation ratio and negatively to a phenomenon called {Collapse to Constants}
(or C2C), which is closely related but not identical to the phenomenon commonly
known as vanishing gradient. Experiments on a styled model problem empirically
verify that variability is indeed a key performance indicator for fully
connected neural networks. The insights gained from this variability study will
help the design of new and effective neural network architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yueyao Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple Support Recovery Using Very Few Measurements Per Sample. (arXiv:2105.09855v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2105.09855</id>
        <link href="http://arxiv.org/abs/2105.09855"/>
        <updated>2021-05-23T06:10:40.816Z</updated>
        <summary type="html"><![CDATA[In the problem of multiple support recovery, we are given access to linear
measurements of multiple sparse samples in $\mathbb{R}^{d}$. These samples can
be partitioned into $\ell$ groups, with samples having the same support
belonging to the same group. For a given budget of $m$ measurements per sample,
the goal is to recover the $\ell$ underlying supports, in the absence of the
knowledge of group labels. We study this problem with a focus on the
measurement-constrained regime where $m$ is smaller than the support size $k$
of each sample. We design a two-step procedure that estimates the union of the
underlying supports first, and then uses a spectral algorithm to estimate the
individual supports. Our proposed estimator can recover the supports with $m<k$
measurements per sample, from $\tilde{O}(k^{4}\ell^{4}/m^{4})$ samples. Our
guarantees hold for a general, generative model assumption on the samples and
measurement matrices. We also provide results from experiments conducted on
synthetic data and on the MNIST dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ramesh_L/0/1/0/all/0/1"&gt;Lekshmi Ramesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murthy_C/0/1/0/all/0/1"&gt;Chandra R. Murthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tyagi_H/0/1/0/all/0/1"&gt;Himanshu Tyagi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-19 Detection in Computed Tomography Images with 2D and 3D Approaches. (arXiv:2105.08506v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08506</id>
        <link href="http://arxiv.org/abs/2105.08506"/>
        <updated>2021-05-23T06:10:40.796Z</updated>
        <summary type="html"><![CDATA[Detecting COVID-19 in computed tomography (CT) or radiography images has been
proposed as a supplement to the definitive RT-PCR test. We present a deep
learning ensemble for detecting COVID-19 infection, combining slice-based (2D)
and volume-based (3D) approaches. The 2D system detects the infection on each
CT slice independently, combining them to obtain the patient-level decision via
different methods (averaging and long-short term memory networks). The 3D
system takes the whole CT volume to arrive to the patient-level decision in one
step. A new high resolution chest CT scan dataset, called the IST-C dataset, is
also collected in this work. The proposed ensemble, called IST-CovNet, obtains
90.80% accuracy and 0.95 AUC score overall on the IST-C dataset in detecting
COVID-19 among normal controls and other types of lung pathologies; and 93.69%
accuracy and 0.99 AUC score on the publicly available MosMed dataset that
consists of COVID-19 scans and normal controls only. The system is deployed at
Istanbul University Cerrahpasa School of Medicine.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ahmed_S/0/1/0/all/0/1"&gt;Sara Atito Ali Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yavuz_M/0/1/0/all/0/1"&gt;Mehmet Can Yavuz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sen_M/0/1/0/all/0/1"&gt;Mehmet Umut Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gulsen_F/0/1/0/all/0/1"&gt;Fatih Gulsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tutar_O/0/1/0/all/0/1"&gt;Onur Tutar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Korkmazer_B/0/1/0/all/0/1"&gt;Bora Korkmazer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Samanci_C/0/1/0/all/0/1"&gt;Cesur Samanci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sirolu_S/0/1/0/all/0/1"&gt;Sabri Sirolu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hamid_R/0/1/0/all/0/1"&gt;Rauf Hamid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Eryurekli_A/0/1/0/all/0/1"&gt;Ali Ergun Eryurekli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mammadov_T/0/1/0/all/0/1"&gt;Toghrul Mammadov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yanikoglu_B/0/1/0/all/0/1"&gt;Berrin Yanikoglu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pruning of Deep Spiking Neural Networks through Gradient Rewiring. (arXiv:2105.04916v2 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04916</id>
        <link href="http://arxiv.org/abs/2105.04916"/>
        <updated>2021-05-23T06:10:40.788Z</updated>
        <summary type="html"><![CDATA[Spiking Neural Networks (SNNs) have been attached great importance due to
their biological plausibility and high energy-efficiency on neuromorphic chips.
As these chips are usually resource-constrained, the compression of SNNs is
thus crucial along the road of practical use of SNNs. Most existing methods
directly apply pruning approaches in artificial neural networks (ANNs) to SNNs,
which ignore the difference between ANNs and SNNs, thus limiting the
performance of the pruned SNNs. Besides, these methods are only suitable for
shallow SNNs. In this paper, inspired by synaptogenesis and synapse elimination
in the neural system, we propose gradient rewiring (Grad R), a joint learning
algorithm of connectivity and weight for SNNs, that enables us to seamlessly
optimize network structure without retrain. Our key innovation is to redefine
the gradient to a new synaptic parameter, allowing better exploration of
network structures by taking full advantage of the competition between pruning
and regrowth of connections. The experimental results show that the proposed
method achieves minimal loss of SNNs' performance on MNIST and CIFAR-10 dataset
so far. Moreover, it reaches a $\sim$3.5% accuracy loss under unprecedented
0.73% connectivity, which reveals remarkable structure refining capability in
SNNs. Our work suggests that there exists extremely high redundancy in deep
SNNs. Our codes are available at
https://github.com/Yanqi-Chen/Gradient-Rewiring .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yanqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhaofei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1"&gt;Wei Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1"&gt;Tiejun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Value Function is All You Need: A Unified Learning Framework for Ride Hailing Platforms. (arXiv:2105.08791v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08791</id>
        <link href="http://arxiv.org/abs/2105.08791"/>
        <updated>2021-05-23T06:10:40.781Z</updated>
        <summary type="html"><![CDATA[Large ride-hailing platforms, such as DiDi, Uber and Lyft, connect tens of
thousands of vehicles in a city to millions of ride demands throughout the day,
providing great promises for improving transportation efficiency through the
tasks of order dispatching and vehicle repositioning. Existing studies,
however, usually consider the two tasks in simplified settings that hardly
address the complex interactions between the two, the real-time fluctuations
between supply and demand, and the necessary coordinations due to the
large-scale nature of the problem. In this paper we propose a unified
value-based dynamic learning framework (V1D3) for tackling both tasks. At the
center of the framework is a globally shared value function that is updated
continuously using online experiences generated from real-time platform
transactions. To improve the sample-efficiency and the robustness, we further
propose a novel periodic ensemble method combining the fast online learning
with a large-scale offline training scheme that leverages the abundant
historical driver trajectory data. This allows the proposed framework to adapt
quickly to the highly dynamic environment, to generalize robustly to recurrent
patterns and to drive implicit coordinations among the population of managed
vehicles. Extensive experiments based on real-world datasets show considerably
improvements over other recently proposed methods on both tasks. Particularly,
V1D3 outperforms the first prize winners of both dispatching and repositioning
tracks in the KDD Cup 2020 RL competition, achieving state-of-the-art results
on improving both total driver income and user experience related metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xiaocheng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1"&gt;Zhiwei Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yansheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1"&gt;Dingyuan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1"&gt;Bingchen Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1"&gt;Yongxin Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hongtu Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jieping Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-19 Lung Lesion Segmentation Using a Sparsely Supervised Mask R-CNN on Chest X-rays Automatically Computed from Volumetric CTs. (arXiv:2105.08147v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08147</id>
        <link href="http://arxiv.org/abs/2105.08147"/>
        <updated>2021-05-23T06:10:40.774Z</updated>
        <summary type="html"><![CDATA[Chest X-rays of coronavirus disease 2019 (COVID-19) patients are frequently
obtained to determine the extent of lung disease and are a valuable source of
data for creating artificial intelligence models. Most work to date assessing
disease severity on chest imaging has focused on segmenting computed tomography
(CT) images; however, given that CTs are performed much less frequently than
chest X-rays for COVID-19 patients, automated lung lesion segmentation on chest
X-rays could be clinically valuable. There currently exists a universal
shortage of chest X-rays with ground truth COVID-19 lung lesion annotations,
and manually contouring lung opacities is a tedious, labor-intensive task. To
accelerate severity detection and augment the amount of publicly available
chest X-ray training data for supervised deep learning (DL) models, we leverage
existing annotated CT images to generate frontal projection "chest X-ray"
images for training COVID-19 chest X-ray models. In this paper, we propose an
automated pipeline for segmentation of COVID-19 lung lesions on chest X-rays
comprised of a Mask R-CNN trained on a mixed dataset of open-source chest
X-rays and coronal X-ray projections computed from annotated volumetric CTs. On
a test set containing 40 chest X-rays of COVID-19 positive patients, our model
achieved IoU scores of 0.81 $\pm$ 0.03 and 0.79 $\pm$ 0.03 when trained on a
dataset of 60 chest X-rays and on a mixed dataset of 10 chest X-rays and 50
projections from CTs, respectively. Our model far outperforms current baselines
with limited supervised training and may assist in automated COVID-19 severity
quantification on chest X-rays.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ramesh_V/0/1/0/all/0/1"&gt;Vignav Ramesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rister_B/0/1/0/all/0/1"&gt;Blaine Rister&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rubin_D/0/1/0/all/0/1"&gt;Daniel L. Rubin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring Coding Challenge Competence With APPS. (arXiv:2105.09938v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2105.09938</id>
        <link href="http://arxiv.org/abs/2105.09938"/>
        <updated>2021-05-23T06:10:40.767Z</updated>
        <summary type="html"><![CDATA[While programming is one of the most broadly applicable skills in modern
society, modern machine learning models still cannot code solutions to basic
problems. It can be difficult to accurately assess code generation performance,
and there has been surprisingly little work on evaluating code generation in a
way that is both flexible and rigorous. To meet this challenge, we introduce
APPS, a benchmark for code generation. Unlike prior work in more restricted
settings, our benchmark measures the ability of models to take an arbitrary
natural language specification and generate Python code fulfilling this
specification. Similar to how companies assess candidate software developers,
we then evaluate models by checking their generated code on test cases. Our
benchmark includes 10,000 problems, which range from having simple one-line
solutions to being substantial algorithmic challenges. We fine-tune large
language models on both GitHub and our training set, and we find that the
prevalence of syntax errors is decreasing exponentially. Recent models such as
GPT-Neo can pass approximately 15% of the test cases of introductory problems,
so we find that machine learning models are beginning to learn how to code. As
the social significance of automatic code generation increases over the coming
years, our benchmark can provide an important measure for tracking
advancements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1"&gt;Dan Hendrycks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1"&gt;Steven Basart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadavath_S/0/1/0/all/0/1"&gt;Saurav Kadavath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazeika_M/0/1/0/all/0/1"&gt;Mantas Mazeika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1"&gt;Akul Arora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_E/0/1/0/all/0/1"&gt;Ethan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burns_C/0/1/0/all/0/1"&gt;Collin Burns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Puranik_S/0/1/0/all/0/1"&gt;Samir Puranik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Horace He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1"&gt;Dawn Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1"&gt;Jacob Steinhardt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Be Causal: De-biasing Social Network Confounding in Recommendation. (arXiv:2105.07775v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07775</id>
        <link href="http://arxiv.org/abs/2105.07775"/>
        <updated>2021-05-23T06:10:40.748Z</updated>
        <summary type="html"><![CDATA[In recommendation systems, the existence of the missing-not-at-random (MNAR)
problem results in the selection bias issue, degrading the recommendation
performance ultimately. A common practice to address MNAR is to treat missing
entries from the so-called "exposure" perspective, i.e., modeling how an item
is exposed (provided) to a user. Most of the existing approaches use heuristic
models or re-weighting strategy on observed ratings to mimic the
missing-at-random setting. However, little research has been done to reveal how
the ratings are missing from a causal perspective. To bridge the gap, we
propose an unbiased and robust method called DENC (De-bias Network Confounding
in Recommendation) inspired by confounder analysis in causal inference. In
general, DENC provides a causal analysis on MNAR from both the inherent factors
(e.g., latent user or item factors) and auxiliary network's perspective.
Particularly, the proposed exposure model in DENC can control the social
network confounder meanwhile preserves the observed exposure information. We
also develop a deconfounding model through the balanced representation learning
to retain the primary user and item features, which enables DENC generalize
well on the rating prediction. Extensive experiments on three datasets validate
that our proposed model outperforms the state-of-the-art baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiangmeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Guandong Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fundamental limits and algorithms for sparse linear regression with sublinear sparsity. (arXiv:2101.11156v3 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11156</id>
        <link href="http://arxiv.org/abs/2101.11156"/>
        <updated>2021-05-23T06:10:40.742Z</updated>
        <summary type="html"><![CDATA[We establish exact asymptotic expressions for the normalized mutual
information and minimum mean-square-error (MMSE) of sparse linear regression in
the sub-linear sparsity regime. Our result is achieved by a generalization of
the adaptive interpolation method in Bayesian inference for linear regimes to
sub-linear ones. A modification of the well-known approximate message passing
algorithm to approach the MMSE fundamental limit is also proposed, and its
state evolution is rigorously analysed. Our results show that the traditional
linear assumption between the signal dimension and number of observations in
the replica and adaptive interpolation methods is not necessary for sparse
signals. They also show how to modify the existing well-known AMP algorithms
for linear regimes to sub-linear ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Truong_L/0/1/0/all/0/1"&gt;Lan V. Truong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mondegreen: A Post-Processing Solution to Speech Recognition Error Correction for Voice Search Queries. (arXiv:2105.09930v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2105.09930</id>
        <link href="http://arxiv.org/abs/2105.09930"/>
        <updated>2021-05-23T06:10:40.735Z</updated>
        <summary type="html"><![CDATA[As more and more online search queries come from voice, automatic speech
recognition becomes a key component to deliver relevant search results. Errors
introduced by automatic speech recognition (ASR) lead to irrelevant search
results returned to the user, thus causing user dissatisfaction. In this paper,
we introduce an approach, Mondegreen, to correct voice queries in text space
without depending on audio signals, which may not always be available due to
system constraints or privacy or bandwidth (for example, some ASR systems run
on-device) considerations. We focus on voice queries transcribed via several
proprietary commercial ASR systems. These queries come from users making
internet, or online service search queries. We first present an analysis
showing how different the language distribution coming from user voice queries
is from that in traditional text corpora used to train off-the-shelf ASR
systems. We then demonstrate that Mondegreen can achieve significant
improvements in increased user interaction by correcting user voice queries in
one of the largest search systems in Google. Finally, we see Mondegreen as
complementing existing highly-optimized production ASR systems, which may not
be frequently retrained and thus lag behind due to vocabulary drifts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sodhi_S/0/1/0/all/0/1"&gt;Sukhdeep S. Sodhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chio_E/0/1/0/all/0/1"&gt;Ellie Ka-In Chio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jash_A/0/1/0/all/0/1"&gt;Ambarish Jash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1"&gt;Santiago Onta&amp;#xf1;&amp;#xf3;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Apte_A/0/1/0/all/0/1"&gt;Ajit Apte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Ankit Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeje_A/0/1/0/all/0/1"&gt;Ayooluwakunmi Jeje&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuzmin_D/0/1/0/all/0/1"&gt;Dima Kuzmin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fung_H/0/1/0/all/0/1"&gt;Harry Fung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Heng-Tze Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Effrat_J/0/1/0/all/0/1"&gt;Jon Effrat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bali_T/0/1/0/all/0/1"&gt;Tarush Bali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jindal_N/0/1/0/all/0/1"&gt;Nitin Jindal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_P/0/1/0/all/0/1"&gt;Pei Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sarvjeet Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Senqiang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_T/0/1/0/all/0/1"&gt;Tameen Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wankhede_A/0/1/0/all/0/1"&gt;Amol Wankhede&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alzantot_M/0/1/0/all/0/1"&gt;Moustafa Alzantot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1"&gt;Allen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_T/0/1/0/all/0/1"&gt;Tushar Chandra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ADASYN-Random Forest Based Intrusion Detection Model. (arXiv:2105.04301v3 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04301</id>
        <link href="http://arxiv.org/abs/2105.04301"/>
        <updated>2021-05-23T06:10:40.728Z</updated>
        <summary type="html"><![CDATA[Intrusion detection has been a key topic in the field of cyber security, and
the common network threats nowadays have the characteristics of varieties and
variation. Considering the serious imbalance of intrusion detection datasets
will result in low classification performance on attack behaviors of small
sample size and difficulty to detect network attacks accurately and
efficiently, using Adaptive Synthetic Sampling (ADASYN) method to balance
datasets was proposed in this paper. In addition, Random Forest algorithm was
used to train intrusion detection classifiers. Through the comparative
experiment of Intrusion detection on CICIDS 2017 dataset, it is found that
ADASYN with Random Forest performs better. Based on the experimental results,
the improvement of precision, recall, F1 scores and AUC values after ADASYN is
then analyzed. Experiments show that the proposed method can be applied to
intrusion detection with large data, and can effectively improve the
classification accuracy of network attack behaviors. Compared with traditional
machine learning models, it has better performance, generalization ability and
robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhewei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Linyue Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1"&gt;Wenwen Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incentivized Bandit Learning with Self-Reinforcing User Preferences. (arXiv:2105.08869v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08869</id>
        <link href="http://arxiv.org/abs/2105.08869"/>
        <updated>2021-05-23T06:10:40.722Z</updated>
        <summary type="html"><![CDATA[In this paper, we investigate a new multi-armed bandit (MAB) online learning
model that considers real-world phenomena in many recommender systems: (i) the
learning agent cannot pull the arms by itself and thus has to offer rewards to
users to incentivize arm-pulling indirectly; and (ii) if users with specific
arm preferences are well rewarded, they induce a "self-reinforcing" effect in
the sense that they will attract more users of similar arm preferences. Besides
addressing the tradeoff of exploration and exploitation, another key feature of
this new MAB model is to balance reward and incentivizing payment. The goal of
the agent is to maximize the total reward over a fixed time horizon $T$ with a
low total payment. Our contributions in this paper are two-fold: (i) We propose
a new MAB model with random arm selection that considers the relationship of
users' self-reinforcing preferences and incentives; and (ii) We leverage the
properties of a multi-color Polya urn with nonlinear feedback model to propose
two MAB policies termed "At-Least-$n$ Explore-Then-Commit" and "UCB-List". We
prove that both policies achieve $O(log T)$ expected regret with $O(log T)$
expected payment over a time horizon $T$. We conduct numerical simulations to
demonstrate and verify the performances of these two policies and study their
robustness under various settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1"&gt;Tianchen Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jia Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1"&gt;Chaosheng Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Jingyuan Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Capsule GAN for Prostate MRI Super-Resolution. (arXiv:2105.07495v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07495</id>
        <link href="http://arxiv.org/abs/2105.07495"/>
        <updated>2021-05-23T06:10:40.703Z</updated>
        <summary type="html"><![CDATA[Prostate cancer is a very common disease among adult men. One in seven
Canadian men is diagnosed with this cancer in their lifetime. Super-Resolution
(SR) can facilitate early diagnosis and potentially save many lives. In this
paper, a robust and accurate model is proposed for prostate MRI SR. The model
is trained on the Prostate-Diagnosis and PROSTATEx datasets. The proposed model
outperformed the state-of-the-art prostate SR model in all similarity metrics
with notable margins. A new task-specific similarity assessment is introduced
as well. A classifier is trained for severe cancer detection and the drop in
the accuracy of this model when dealing with super-resolved images is used for
evaluating the ability of medical detail reconstruction of the SR models. The
proposed SR model is a step towards an efficient and accurate general medical
SR platform.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Majdabadi_M/0/1/0/all/0/1"&gt;Mahdiyar Molahasani Majdabadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Younhee Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deivalakshmi_S/0/1/0/all/0/1"&gt;S. Deivalakshmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_S/0/1/0/all/0/1"&gt;Seokbum Ko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CURE: Code-Aware Neural Machine Translation for Automatic Program Repair. (arXiv:2103.00073v3 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00073</id>
        <link href="http://arxiv.org/abs/2103.00073"/>
        <updated>2021-05-23T06:10:40.696Z</updated>
        <summary type="html"><![CDATA[Automatic program repair (APR) is crucial to improve software reliability.
Recently, neural machine translation (NMT) techniques have been used to fix
software bugs automatically. While promising, these approaches have two major
limitations. Their search space often does not contain the correct fix, and
their search strategy ignores software knowledge such as strict code syntax.
Due to these limitations, existing NMT-based techniques underperform the best
template-based approaches.

We propose CURE, a new NMT-based APR technique with three major novelties.
First, CURE pre-trains a programming language (PL) model on a large software
codebase to learn developer-like source code before the APR task. Second, CURE
designs a new code-aware search strategy that finds more correct fixes by
focusing on compilable patches and patches that are close in length to the
buggy code. Finally, CURE uses a subword tokenization technique to generate a
smaller search space that contains more correct fixes.

Our evaluation on two widely-used benchmarks shows that CURE correctly fixes
57 Defects4J bugs and 26 QuixBugs bugs, outperforming all existing APR
techniques on both benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1"&gt;Nan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lutellier_T/0/1/0/all/0/1"&gt;Thibaud Lutellier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1"&gt;Lin Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noise Estimation Is Not Optimal: How to Use Kalman Filter the Right Way. (arXiv:2104.02372v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02372</id>
        <link href="http://arxiv.org/abs/2104.02372"/>
        <updated>2021-05-23T06:10:40.667Z</updated>
        <summary type="html"><![CDATA[Determining the noise parameters of a Kalman Filter (KF) has been studied for
decades. A huge body of research focuses on the task of estimation of the noise
under various conditions, since precise noise estimation is considered
equivalent to minimization of the filtering errors. However, we show that even
a small violation of the KF assumptions can significantly modify the effective
noise, breaking the equivalence between the tasks and making noise estimation
an inferior strategy. We show that such violations are very common, and are
often not trivial to handle or even notice. Consequentially, we argue that a
robust solution is needed - rather than choosing a dedicated model per problem.
To that end, we apply gradient-based optimization to the filtering errors
directly, with relation to a simple and efficient parameterization of the
symmetric and positive-definite parameters of KF. In radar tracking and video
tracking, we show that the optimization improves both the accuracy of KF and
its robustness to design decisions. In addition, we demonstrate how an
optimized neural network model can seem to reduce the errors significantly
compared to a KF - and how this reduction vanishes once the KF is optimized
similarly. This indicates how complicated models can be wrongly identified as
superior to KF, while in fact they were merely more optimized.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Greenberg_I/0/1/0/all/0/1"&gt;Ido Greenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yannay_N/0/1/0/all/0/1"&gt;Netanel Yannay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1"&gt;Shie Mannor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A data-driven approach to the forecasting of ground-level ozone concentration. (arXiv:2012.00685v3 [physics.ao-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00685</id>
        <link href="http://arxiv.org/abs/2012.00685"/>
        <updated>2021-05-23T06:10:40.661Z</updated>
        <summary type="html"><![CDATA[The ability to forecast the concentration of air pollutants in an urban
region is crucial for decision-makers wishing to reduce the impact of pollution
on public health through active measures (e.g. temporary traffic closures). In
this study, we present a machine learning approach applied to the forecast of
the day-ahead maximum value of the ozone concentration for several geographical
locations in southern Switzerland. Due to the low density of measurement
stations and to the complex orography of the use case terrain, we adopted
feature selection methods instead of explicitly restricting relevant features
to a neighbourhood of the prediction sites, as common in spatio-temporal
forecasting methods. We then used Shapley values to assess the explainability
of the learned models in terms of feature importance and feature interactions
in relation to ozone predictions; our analysis suggests that the trained models
effectively learned explanatory cross-dependencies among atmospheric variables.
Finally, we show how weighting observations helps in increasing the accuracy of
the forecasts for specific ranges of ozone's daily peak values.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Marvin_D/0/1/0/all/0/1"&gt;Dario Marvin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Nespoli_L/0/1/0/all/0/1"&gt;Lorenzo Nespoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Strepparava_D/0/1/0/all/0/1"&gt;Davide Strepparava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Medici_V/0/1/0/all/0/1"&gt;Vasco Medici&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SAFIN: Arbitrary Style Transfer With Self-Attentive Factorized Instance Normalization. (arXiv:2105.06129v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06129</id>
        <link href="http://arxiv.org/abs/2105.06129"/>
        <updated>2021-05-23T06:10:40.654Z</updated>
        <summary type="html"><![CDATA[Artistic style transfer aims to transfer the style characteristics of one
image onto another image while retaining its content. Existing approaches
commonly leverage various normalization techniques, although these face
limitations in adequately transferring diverse textures to different spatial
locations. Self-Attention-based approaches have tackled this issue with partial
success but suffer from unwanted artifacts. Motivated by these observations,
this paper aims to combine the best of both worlds: self-attention and
normalization. That yields a new plug-and-play module that we name
Self-Attentive Factorized Instance Normalization (SAFIN). SAFIN is essentially
a spatially adaptive normalization module whose parameters are inferred through
attention on the content and style image. We demonstrate that plugging SAFIN
into the base network of another state-of-the-art method results in enhanced
stylization. We also develop a novel base network composed of Wavelet Transform
for multi-scale style transfer, which when combined with SAFIN, produces
visually appealing results with lesser unwanted textures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Aaditya Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hingane_S/0/1/0/all/0/1"&gt;Shreeshail Hingane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1"&gt;Xinyu Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Unknown from Correlations: Graph Neural Network for Inter-novel-protein Interaction Prediction. (arXiv:2105.06709v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06709</id>
        <link href="http://arxiv.org/abs/2105.06709"/>
        <updated>2021-05-23T06:10:40.635Z</updated>
        <summary type="html"><![CDATA[The study of multi-type Protein-Protein Interaction (PPI) is fundamental for
understanding biological processes from a systematic perspective and revealing
disease mechanisms. Existing methods suffer from significant performance
degradation when tested in unseen dataset. In this paper, we investigate the
problem and find that it is mainly attributed to the poor performance for
inter-novel-protein interaction prediction. However, current evaluations
overlook the inter-novel-protein interactions, and thus fail to give an
instructive assessment. As a result, we propose to address the problem from
both the evaluation and the methodology. Firstly, we design a new evaluation
framework that fully respects the inter-novel-protein interactions and gives
consistent assessment across datasets. Secondly, we argue that correlations
between proteins must provide useful information for analysis of novel
proteins, and based on this, we propose a graph neural network based method
(GNN-PPI) for better inter-novel-protein interaction prediction. Experimental
results on real-world datasets of different scales demonstrate that GNN-PPI
significantly outperforms state-of-the-art PPI prediction methods, especially
for the inter-novel-protein interaction prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lv_G/0/1/0/all/0/1"&gt;Guofeng Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhiqiang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_Y/0/1/0/all/0/1"&gt;Yanguang Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shaoting Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Encoding Explanatory Knowledge for Zero-shot Science Question Answering. (arXiv:2105.05737v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05737</id>
        <link href="http://arxiv.org/abs/2105.05737"/>
        <updated>2021-05-23T06:10:40.486Z</updated>
        <summary type="html"><![CDATA[This paper describes N-XKT (Neural encoding based on eXplanatory Knowledge
Transfer), a novel method for the automatic transfer of explanatory knowledge
through neural encoding mechanisms. We demonstrate that N-XKT is able to
improve accuracy and generalization on science Question Answering (QA).
Specifically, by leveraging facts from background explanatory knowledge
corpora, the N-XKT model shows a clear improvement on zero-shot QA.
Furthermore, we show that N-XKT can be fine-tuned on a target QA dataset,
enabling faster convergence and more accurate results. A systematic analysis is
conducted to quantitatively analyze the performance of the N-XKT model and the
impact of different categories of knowledge on the zero-shot generalization
task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zili Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1"&gt;Marco Valentino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Landers_D/0/1/0/all/0/1"&gt;Donal Landers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1"&gt;Andre Freitas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physically-Consistent Generative Adversarial Networks for Coastal Flood Visualization. (arXiv:2104.04785v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04785</id>
        <link href="http://arxiv.org/abs/2104.04785"/>
        <updated>2021-05-23T06:10:40.479Z</updated>
        <summary type="html"><![CDATA[As climate change increases the intensity of natural disasters, society needs
better tools for adaptation. Floods, for example, are the most frequent natural
disaster, and better tools for flood risk communication could increase the
support for flood-resilient infrastructure development. Our work aims to enable
more visual communication of large-scale climate impacts via visualizing the
output of coastal flood models as satellite imagery. We propose the first deep
learning pipeline to ensure physical-consistency in synthetic visual satellite
imagery. We advanced a state-of-the-art GAN called pix2pixHD, such that it
produces imagery that is physically-consistent with the output of an
expert-validated storm surge model (NOAA SLOSH). By evaluating the imagery
relative to physics-based flood maps, we find that our proposed framework
outperforms baseline models in both physical-consistency and photorealism. We
envision our work to be the first step towards a global visualization of how
climate change shapes our landscape. Continuing on this path, we show that the
proposed pipeline generalizes to visualize arctic sea ice melt. We also publish
a dataset of over 25k labelled image-pairs to study image-to-image translation
in Earth observation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn L&amp;#xfc;tjens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leshchinskiy_B/0/1/0/all/0/1"&gt;Brandon Leshchinskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Requena_Mesa_C/0/1/0/all/0/1"&gt;Christian Requena-Mesa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chishtie_F/0/1/0/all/0/1"&gt;Farrukh Chishtie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_Rodriguez_N/0/1/0/all/0/1"&gt;Natalia D&amp;#xed;az-Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boulais_O/0/1/0/all/0/1"&gt;Oc&amp;#xe9;ane Boulais&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sankaranarayanan_A/0/1/0/all/0/1"&gt;Aruna Sankaranarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pina_A/0/1/0/all/0/1"&gt;Aaron Pi&amp;#xf1;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raissi_C/0/1/0/all/0/1"&gt;Chedy Ra&amp;#xef;ssi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lavin_A/0/1/0/all/0/1"&gt;Alexander Lavin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Newman_D/0/1/0/all/0/1"&gt;Dava Newman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proximal Learning for Individualized Treatment Regimes Under Unmeasured Confounding. (arXiv:2105.01187v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01187</id>
        <link href="http://arxiv.org/abs/2105.01187"/>
        <updated>2021-05-23T06:10:40.465Z</updated>
        <summary type="html"><![CDATA[Data-driven individualized decision making has recently received increasing
research interests. Most existing methods rely on the assumption of no
unmeasured confounding, which unfortunately cannot be ensured in practice
especially in observational studies. Motivated by the recent proposed proximal
causal inference, we develop several proximal learning approaches to estimating
optimal individualized treatment regimes (ITRs) in the presence of unmeasured
confounding. In particular, we establish several identification results for
different classes of ITRs, exhibiting the trade-off between the risk of making
untestable assumptions and the value function improvement in decision making.
Based on these results, we propose several classification-based approaches to
finding a variety of restricted in-class optimal ITRs and develop their
theoretical properties. The appealing numerical performance of our proposed
methods is demonstrated via an extensive simulation study and one real data
application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Qi_Z/0/1/0/all/0/1"&gt;Zhengling Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Miao_R/0/1/0/all/0/1"&gt;Rui Miao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaoke Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FastCorrect: Fast Error Correction with Edit Alignment for Automatic Speech Recognition. (arXiv:2105.03842v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03842</id>
        <link href="http://arxiv.org/abs/2105.03842"/>
        <updated>2021-05-23T06:10:40.453Z</updated>
        <summary type="html"><![CDATA[Error correction techniques have been used to refine the output sentences
from automatic speech recognition (ASR) models and achieve a lower word error
rate (WER) than original ASR outputs. Previous works usually use a
sequence-to-sequence model to correct an ASR output sentence autoregressively,
which causes large latency and cannot be deployed in online ASR services. A
straightforward solution to reduce latency, inspired by non-autoregressive
(NAR) neural machine translation, is to use an NAR sequence generation model
for ASR error correction, which, however, comes at the cost of significantly
increased ASR error rate. In this paper, observing distinctive error patterns
and correction operations (i.e., insertion, deletion, and substitution) in ASR,
we propose FastCorrect, a novel NAR error correction model based on edit
alignment. In training, FastCorrect aligns each source token from an ASR output
sentence to the target tokens from the corresponding ground-truth sentence
based on the edit distance between the source and target sentences, and
extracts the number of target tokens corresponding to each source token during
edition/correction, which is then used to train a length predictor and to
adjust the source tokens to match the length of the target sentence for
parallel generation. In inference, the token number predicted by the length
predictor is used to adjust the source tokens for target sequence generation.
Experiments on the public AISHELL-1 dataset and an internal industrial-scale
ASR dataset show the effectiveness of FastCorrect for ASR error correction: 1)
it speeds up the inference by 6-9 times and maintains the accuracy (8-14% WER
reduction) compared with the autoregressive correction model; and 2) it
outperforms the accuracy of popular NAR models adopted in neural machine
translation by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1"&gt;Yichong Leng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xu Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Linchen Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1"&gt;Renqian Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Linquan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang-Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1"&gt;Ed Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable artificial intelligence for mechanics: physics-informing neural networks for constitutive models. (arXiv:2104.10683v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10683</id>
        <link href="http://arxiv.org/abs/2104.10683"/>
        <updated>2021-05-23T06:10:40.434Z</updated>
        <summary type="html"><![CDATA[(Artificial) neural networks have become increasingly popular in mechanics as
means to accelerate computations with model order reduction techniques and as
universal models for a wide variety of materials. However, the major
disadvantage of neural networks remains: their numerous parameters are
challenging to interpret and explain. Thus, neural networks are often labeled
as black boxes, and their results often elude human interpretation. In
mechanics, the new and active field of physics-informed neural networks
attempts to mitigate this disadvantage by designing deep neural networks on the
basis of mechanical knowledge. By using this a priori knowledge, deeper and
more complex neural networks became feasible, since the mechanical assumptions
could be explained. However, the internal reasoning and explanation of neural
network parameters remain mysterious.

Complementary to the physics-informed approach, we propose a first step
towards a physics-informing approach, which explains neural networks trained on
mechanical data a posteriori. This novel explainable artificial intelligence
approach aims at elucidating the black box of neural networks and their
high-dimensional representations. Therein, the principal component analysis
decorrelates the distributed representations in cell states of RNNs and allows
the comparison to known and fundamental functions. The novel approach is
supported by a systematic hyperparameter search strategy that identifies the
best neural network architectures and training parameters. The findings of
three case studies on fundamental constitutive models (hyperelasticity,
elastoplasticity, and viscoelasticity) imply that the proposed strategy can
help identify numerical and analytical closed-form solutions to characterize
new materials.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koeppe_A/0/1/0/all/0/1"&gt;Arnd Koeppe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bamer_F/0/1/0/all/0/1"&gt;Franz Bamer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Selzer_M/0/1/0/all/0/1"&gt;Michael Selzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nestler_B/0/1/0/all/0/1"&gt;Britta Nestler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markert_B/0/1/0/all/0/1"&gt;Bernd Markert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Temporally Consistent Image-based Sun Tracking Algorithm for Solar Energy Forecasting Applications. (arXiv:2012.01059v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01059</id>
        <link href="http://arxiv.org/abs/2012.01059"/>
        <updated>2021-05-23T06:10:40.406Z</updated>
        <summary type="html"><![CDATA[Improving irradiance forecasting is critical to further increase the share of
solar in the energy mix. On a short time scale, fish-eye cameras on the ground
are used to capture cloud displacements causing the local variability of the
electricity production. As most of the solar radiation comes directly from the
Sun, current forecasting approaches use its position in the image as a
reference to interpret the cloud cover dynamics. However, existing Sun tracking
methods rely on external data and a calibration of the camera, which requires
access to the device. To address these limitations, this study introduces an
image-based Sun tracking algorithm to localise the Sun in the image when it is
visible and interpolate its daily trajectory from past observations. We
validate the method on a set of sky images collected over a year at SIRTA's
lab. Experimental results show that the proposed method provides robust smooth
Sun trajectories with a mean absolute error below 1% of the image size.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paletta_Q/0/1/0/all/0/1"&gt;Quentin Paletta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1"&gt;Joan Lasenby&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Abductive Knowledge Induction From Raw Data. (arXiv:2010.03514v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03514</id>
        <link href="http://arxiv.org/abs/2010.03514"/>
        <updated>2021-05-23T06:10:40.399Z</updated>
        <summary type="html"><![CDATA[For many reasoning-heavy tasks involving raw inputs, it is challenging to
design an appropriate end-to-end learning pipeline. Neuro-Symbolic Learning,
divide the process into sub-symbolic perception and symbolic reasoning, trying
to utilise data-driven machine learning and knowledge-driven reasoning
simultaneously. However, they suffer from the exponential computational
complexity within the interface between these two components, where the
sub-symbolic learning model lacks direct supervision, and the symbolic model
lacks accurate input facts. Hence, most of them assume the existence of a
strong symbolic knowledge base and only learn the perception model while
avoiding a crucial problem: where does the knowledge come from? In this paper,
we present Abductive Meta-Interpretive Learning ($Meta_{Abd}$) that unites
abduction and induction to learn neural networks and induce logic theories
jointly from raw data. Experimental results demonstrate that $Meta_{Abd}$ not
only outperforms the compared systems in predictive accuracy and data
efficiency but also induces logic programs that can be re-used as background
knowledge in subsequent learning tasks. To the best of our knowledge,
$Meta_{Abd}$ is the first system that can jointly learn neural networks from
scratch and induce recursive first-order logic theories with predicate
invention.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1"&gt;Wang-Zhou Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muggleton_S/0/1/0/all/0/1"&gt;Stephen H. Muggleton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Machine Learning on Graphs: A Survey. (arXiv:2103.00742v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00742</id>
        <link href="http://arxiv.org/abs/2103.00742"/>
        <updated>2021-05-23T06:10:40.390Z</updated>
        <summary type="html"><![CDATA[Machine learning on graphs has been extensively studied in both academic and
industry. However, as the literature on graph learning booms with a vast number
of emerging methods and techniques, it becomes increasingly difficult to
manually design the optimal machine learning algorithm for different
graph-related tasks. To solve this critical challenge, automated machine
learning (AutoML) on graphs which combines the strength of graph machine
learning and AutoML together, is gaining attention from the research community.
Therefore, we comprehensively survey AutoML on graphs in this paper, primarily
focusing on hyper-parameter optimization (HPO) and neural architecture search
(NAS) for graph machine learning. We further overview libraries related to
automated graph machine learning and in-depth discuss AutoGL, the first
dedicated open-source library for AutoML on graphs. In the end, we share our
insights on future research directions for automated graph machine learning.
This paper is the first systematic and comprehensive review of automated
machine learning on graphs to the best of our knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1"&gt;Wenwu Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedMood: Federated Learning on Mobile Health Data for Mood Detection. (arXiv:2102.09342v6 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09342</id>
        <link href="http://arxiv.org/abs/2102.09342"/>
        <updated>2021-05-23T06:10:40.383Z</updated>
        <summary type="html"><![CDATA[Depression is one of the most common mental illness problems, and the
symptoms shown by patients are not consistent, making it difficult to diagnose
in the process of clinical practice and pathological research. Although
researchers hope that artificial intelligence can contribute to the diagnosis
and treatment of depression, the traditional centralized machine learning needs
to aggregate patient data, and the data privacy of patients with mental illness
needs to be strictly confidential, which hinders machine learning algorithms
clinical application. To solve the problem of privacy of the medical history of
patients with depression, we implement federated learning to analyze and
diagnose depression. First, we propose a general multi-view federated learning
framework using multi-source data, which can extend any traditional machine
learning model to support federated learning across different institutions or
parties. Secondly, we adopt late fusion methods to solve the problem of
inconsistent time series of multi-view data. Finally, we compare the federated
framework with other cooperative learning frameworks in performance and discuss
the related results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiaohang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Hao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lichao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhuiyan_M/0/1/0/all/0/1"&gt;Md Zakirul Alam Bhuiyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lianzhong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lifang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Human Trajectories by Learning and Matching Patterns. (arXiv:2104.10241v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10241</id>
        <link href="http://arxiv.org/abs/2104.10241"/>
        <updated>2021-05-23T06:10:40.362Z</updated>
        <summary type="html"><![CDATA[Thesis document of the degree of Master of Science in Robotics of Carnegie
Mellon University School of Computer Science.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Dapeng Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoML to Date and Beyond: Challenges and Opportunities. (arXiv:2010.10777v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10777</id>
        <link href="http://arxiv.org/abs/2010.10777"/>
        <updated>2021-05-23T06:10:40.354Z</updated>
        <summary type="html"><![CDATA[As big data becomes ubiquitous across domains, and more and more stakeholders
aspire to make the most of their data, demand for machine learning tools has
spurred researchers to explore the possibilities of automated machine learning
(AutoML). AutoML tools aim to make machine learning accessible for non-machine
learning experts (domain experts), to improve the efficiency of machine
learning, and to accelerate machine learning research. But although automation
and efficiency are among AutoML's main selling points, the process still
requires human involvement at a number of vital steps, including understanding
the attributes of domain-specific data, defining prediction problems, creating
a suitable training data set, and selecting a promising machine learning
technique. These steps often require a prolonged back-and-forth that makes this
process inefficient for domain experts and data scientists alike, and keeps
so-called AutoML systems from being truly automatic. In this review article, we
introduce a new classification system for AutoML systems, using a seven-tiered
schematic to distinguish these systems based on their level of autonomy. We
begin by describing what an end-to-end machine learning pipeline actually looks
like, and which subtasks of the machine learning pipeline have been automated
so far. We highlight those subtasks which are still done manually - generally
by a data scientist - and explain how this limits domain experts' access to
machine learning. Next, we introduce our novel level-based taxonomy for AutoML
systems and define each level according to the scope of automation support
provided. Finally, we lay out a roadmap for the future, pinpointing the
research required to further automate the end-to-end machine learning pipeline
and discussing important challenges that stand in the way of this ambitious
goal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Santu_S/0/1/0/all/0/1"&gt;Shubhra Kanti Karmaker Santu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassan_M/0/1/0/all/0/1"&gt;Md. Mahadi Hassan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_M/0/1/0/all/0/1"&gt;Micah J. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1"&gt;ChengXiang Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veeramachaneni_K/0/1/0/all/0/1"&gt;Kalyan Veeramachaneni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reproducibility Report: La-MAML: Look-ahead Meta Learning for Continual Learning. (arXiv:2102.05824v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05824</id>
        <link href="http://arxiv.org/abs/2102.05824"/>
        <updated>2021-05-23T06:10:40.345Z</updated>
        <summary type="html"><![CDATA[The Continual Learning (CL) problem involves performing well on a sequence of
tasks under limited compute. Current algorithms in the domain are either slow,
offline or sensitive to hyper-parameters. La-MAML, an optimization-based
meta-learning algorithm claims to be better than other replay-based,
prior-based and meta-learning based approaches. According to the MER paper [1],
metrics to measure performance in the continual learning arena are Retained
Accuracy (RA) and Backward Transfer-Interference (BTI). La-MAML claims to
perform better in these values when compared to the SOTA in the domain. This is
the main claim of the paper, which we shall be verifying in this report.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Joseph_J/0/1/0/all/0/1"&gt;Joel Joseph&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_A/0/1/0/all/0/1"&gt;Alex Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Approximation Rates and Metric Entropy of ReLU$^k$ and Cosine Networks. (arXiv:2101.12365v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.12365</id>
        <link href="http://arxiv.org/abs/2101.12365"/>
        <updated>2021-05-23T06:10:40.334Z</updated>
        <summary type="html"><![CDATA[This article addresses several fundamental issues associated with the
approximation theory of neural networks, including the characterization of
approximation spaces, the determination of the metric entropy of these spaces,
and approximation rates of neural networks. For any activation function
$\sigma$, we show that the largest Banach space of functions which can be
efficiently approximated by the corresponding shallow neural networks is the
space whose norm is given by the gauge of the closed convex hull of the set
$\{\pm\sigma(\omega\cdot x + b)\}$. We characterize this space for the ReLU$^k$
and cosine activation functions and, in particular, show that the resulting
gauge space is equivalent to the spectral Barron space if $\sigma=\cos$ and is
equivalent to the Barron space when $\sigma={\rm ReLU}$. Our main result
establishes the precise asymptotics of the $L^2$-metric entropy of the unit
ball of these guage spaces and, as a consequence, the optimal approximation
rates for shallow ReLU$^k$ networks. The sharpest previous results hold only in
the special case that $k=0$ and $d=2$, where the metric entropy has been
determined up to logarithmic factors. When $k > 0$ or $d > 2$, there is a
significant gap between the previous best upper and lower bounds. We close all
of these gaps and determine the precise asymptotics of the metric entropy for
all $k \geq 0$ and $d\geq 2$, including removing the logarithmic factors
previously mentioned. Finally, we use these results to quantify how much is
lost by Barron's spectral condition relative to the convex hull of
$\{\pm\sigma(\omega\cdot x + b)\}$ when $\sigma={\rm ReLU}^k$. Finally, we also
show that the orthogonal greedy algorithm can algorithmically realize the
improved approximation rates which have been derived.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Siegel_J/0/1/0/all/0/1"&gt;Jonathan W. Siegel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jinchao Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[tFold-TR: Combining Deep Learning Enhanced Hybrid Potential Energy for Template-Based Modelling Structure Refinement. (arXiv:2105.04350v2 [physics.bio-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04350</id>
        <link href="http://arxiv.org/abs/2105.04350"/>
        <updated>2021-05-23T06:10:40.318Z</updated>
        <summary type="html"><![CDATA[Proteins structure prediction has long been a grand challenge over the past
50 years, owing to its board scientific and application interests. There are
two major types of modelling algorithm, template-free modelling and
template-based modelling, which is suitable for easy prediction tasks, and is
widely adopted in computer aided drug discoveries for drug design and
screening. Although it has been several decades since its first edition, the
current template-based modeling approach suffers from two important problems:
1) there are many missing regions in the template-query sequence alignment, and
2) the accuracy of the distance pairs from different regions of the template
varies, and this information is not well introduced into the modeling. To solve
the two problems, we propose a structural optimization process based on
template modelling, introducing two neural network models predict the distance
information of the missing regions and the accuracy of the distance pairs of
different regions in the template modeling structure. The predicted distances
and residue pairwise specific accuracy information are incorporated into the
potential energy function for structural optimization, which significantly
improves the qualities of the original template modelling decoys.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Liangzhen Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Lan_H/0/1/0/all/0/1"&gt;Haidong Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Shen_T/0/1/0/all/0/1"&gt;Tao Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiaxiang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Huang_J/0/1/0/all/0/1"&gt;Junzhou Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-driven Thermal Anomaly Detection for Batteries using Unsupervised Shape Clustering. (arXiv:2103.08796v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08796</id>
        <link href="http://arxiv.org/abs/2103.08796"/>
        <updated>2021-05-23T06:10:40.311Z</updated>
        <summary type="html"><![CDATA[For electric vehicles (EV) and energy storage (ES) batteries, thermal runaway
is a critical issue as it can lead to uncontrollable fires or even explosions.
Thermal anomaly detection can identify problematic battery packs that may
eventually undergo thermal runaway. However, there are common challenges like
data unavailability, environment and configuration variations, and battery
aging. We propose a data-driven method to detect battery thermal anomaly based
on comparing shape-similarity between thermal measurements. Based on their
shapes, the measurements are continuously being grouped into different
clusters. Anomaly is detected by monitoring deviations within the clusters.
Unlike model-based or other data-driven methods, the proposed method is robust
to data loss and requires minimal reference data for different pack
configurations. As the initial experimental results show, the method not only
can be more accurate than the onboard BMS and but also can detect unforeseen
anomalies at the early stage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaojun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianwei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Abdollahi_A/0/1/0/all/0/1"&gt;Ali Abdollahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jones_T/0/1/0/all/0/1"&gt;Trevor Jones&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The World as a Graph: Improving El Ni\~no Forecasts with Graph Neural Networks. (arXiv:2104.05089v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05089</id>
        <link href="http://arxiv.org/abs/2104.05089"/>
        <updated>2021-05-23T06:10:40.304Z</updated>
        <summary type="html"><![CDATA[Deep learning-based models have recently outperformed state-of-the-art
seasonal forecasting models, such as for predicting El Ni\~no-Southern
Oscillation (ENSO). However, current deep learning models are based on
convolutional neural networks which are difficult to interpret and can fail to
model large-scale atmospheric patterns. In comparison, graph neural networks
(GNNs) are capable of modeling large-scale spatial dependencies and are more
interpretable due to the explicit modeling of information flow through edge
connections. We propose the first application of graph neural networks to
seasonal forecasting. We design a novel graph connectivity learning module that
enables our GNN model to learn large-scale spatial interactions jointly with
the actual ENSO forecasting task. Our model, \graphino, outperforms
state-of-the-art deep learning-based models for forecasts up to six months
ahead. Additionally, we show that our model is more interpretable as it learns
sensible connectivity structures that correlate with the ENSO anomaly pattern.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cachay_S/0/1/0/all/0/1"&gt;Salva R&amp;#xfc;hling Cachay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erickson_E/0/1/0/all/0/1"&gt;Emma Erickson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bucker_A/0/1/0/all/0/1"&gt;Arthur Fender C. Bucker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pokropek_E/0/1/0/all/0/1"&gt;Ernest Pokropek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potosnak_W/0/1/0/all/0/1"&gt;Willa Potosnak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bire_S/0/1/0/all/0/1"&gt;Suyash Bire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osei_S/0/1/0/all/0/1"&gt;Salomey Osei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn L&amp;#xfc;tjens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does Standard Backpropagation Forget Less Catastrophically Than Adam?. (arXiv:2102.07686v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07686</id>
        <link href="http://arxiv.org/abs/2102.07686"/>
        <updated>2021-05-23T06:10:40.297Z</updated>
        <summary type="html"><![CDATA[Catastrophic forgetting remains a severe hindrance to the broad application
of artificial neural networks (ANNs), however, it continues to be a poorly
understood phenomenon. Despite the extensive amount of work on catastrophic
forgetting, we argue that it is still unclear how exactly the phenomenon should
be quantified, and, moreover, to what degree all of the choices we make when
designing learning systems affect the amount of catastrophic forgetting. We use
various testbeds from the reinforcement learning and supervised learning
literature to (1) provide evidence that the choice of which modern
gradient-based optimization algorithm is used to train an ANN has a significant
impact on the amount of catastrophic forgetting and show that-surprisingly-in
many instances classical algorithms such as vanilla SGD experience less
catastrophic forgetting than the more modern algorithms such as Adam. We
empirically compare four different existing metrics for quantifying
catastrophic forgetting and (2) show that the degree to which the learning
systems experience catastrophic forgetting is sufficiently sensitive to the
metric used that a change from one principled metric to another is enough to
change the conclusions of a study dramatically. Our results suggest that a much
more rigorous experimental methodology is required when looking at catastrophic
forgetting. Based on our results, we recommend inter-task forgetting in
supervised learning must be measured with both retention and relearning metrics
concurrently, and intra-task forgetting in reinforcement learning must-at the
very least-be measured with pairwise interference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ashley_D/0/1/0/all/0/1"&gt;Dylan R. Ashley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghiassian_S/0/1/0/all/0/1"&gt;Sina Ghiassian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1"&gt;Richard S. Sutton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Invertible DenseNets with Concatenated LipSwish. (arXiv:2102.02694v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02694</id>
        <link href="http://arxiv.org/abs/2102.02694"/>
        <updated>2021-05-23T06:10:40.286Z</updated>
        <summary type="html"><![CDATA[We introduce Invertible Dense Networks (i-DenseNets), a more parameter
efficient extension of Residual Flows. The method relies on an analysis of the
Lipschitz continuity of the concatenation in DenseNets, where we enforce
invertibility of the network by satisfying the Lipschitz constant. Furthermore,
we propose a learnable weighted concatenation, which not only improves the
model performance but also indicates the importance of the concatenated
weighted representation. Additionally, we introduce the Concatenated LipSwish
as activation function, for which we show how to enforce the Lipschitz
condition and which boosts performance. The new architecture, i-DenseNet,
out-performs Residual Flow and other flow-based models on density estimation
evaluated in bits per dimension, where we utilize an equal parameter budget.
Moreover, we show that the proposed model out-performs Residual Flows when
trained as a hybrid model where the model is both a generative and a
discriminative model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Perugachi_Diaz_Y/0/1/0/all/0/1"&gt;Yura Perugachi-Diaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tomczak_J/0/1/0/all/0/1"&gt;Jakub M. Tomczak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bhulai_S/0/1/0/all/0/1"&gt;Sandjai Bhulai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Data Assimilation with a Learned Inverse Observation Operator. (arXiv:2102.11192v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11192</id>
        <link href="http://arxiv.org/abs/2102.11192"/>
        <updated>2021-05-23T06:10:40.278Z</updated>
        <summary type="html"><![CDATA[Variational data assimilation optimizes for an initial state of a dynamical
system such that its evolution fits observational data. The physical model can
subsequently be evolved into the future to make predictions. This principle is
a cornerstone of large scale forecasting applications such as numerical weather
prediction. As such, it is implemented in current operational systems of
weather forecasting agencies across the globe. However, finding a good initial
state poses a difficult optimization problem in part due to the non-invertible
relationship between physical states and their corresponding observations. We
learn a mapping from observational data to physical states and show how it can
be used to improve optimizability. We employ this mapping in two ways: to
better initialize the non-convex optimization problem, and to reformulate the
objective function in better behaved physics space instead of observation
space. Our experimental results for the Lorenz96 model and a two-dimensional
turbulent fluid flow demonstrate that this procedure significantly improves
forecast quality for chaotic systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Frerix_T/0/1/0/all/0/1"&gt;Thomas Frerix&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kochkov_D/0/1/0/all/0/1"&gt;Dmitrii Kochkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1"&gt;Jamie A. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1"&gt;Daniel Cremers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brenner_M/0/1/0/all/0/1"&gt;Michael P. Brenner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoyer_S/0/1/0/all/0/1"&gt;Stephan Hoyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Rank and Sparse Enhanced Tucker Decomposition for Tensor Completion. (arXiv:2010.00359v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.00359</id>
        <link href="http://arxiv.org/abs/2010.00359"/>
        <updated>2021-05-23T06:10:40.269Z</updated>
        <summary type="html"><![CDATA[Tensor completion refers to the task of estimating the missing data from an
incomplete measurement or observation, which is a core problem frequently
arising from the areas of big data analysis, computer vision, and network
engineering. Due to the multidimensional nature of high-order tensors, the
matrix approaches, e.g., matrix factorization and direct matricization of
tensors, are often not ideal for tensor completion and recovery. In this paper,
we introduce a unified low-rank and sparse enhanced Tucker decomposition model
for tensor completion. Our model possesses a sparse regularization term to
promote a sparse core tensor of the Tucker decomposition, which is beneficial
for tensor data compression. Moreover, we enforce low-rank regularization terms
on factor matrices of the Tucker decomposition for inducing the low-rankness of
the tensor with a cheap computational cost. Numerically, we propose a
customized ADMM with enough easy subproblems to solve the underlying model. It
is remarkable that our model is able to deal with different types of real-world
data sets, since it exploits the potential periodicity and inherent correlation
properties appeared in tensors. A series of computational experiments on
real-world data sets, including internet traffic data sets, color images, and
face recognition, demonstrate that our model performs better than many existing
state-of-the-art matricization and tensorization approaches in terms of
achieving higher recovery accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1"&gt;Chenjian Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_C/0/1/0/all/0/1"&gt;Chen Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Hongjin He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1"&gt;Liqun Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yanwei Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-Efficient Reinforcement Learning with Self-Predictive Representations. (arXiv:2007.05929v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.05929</id>
        <link href="http://arxiv.org/abs/2007.05929"/>
        <updated>2021-05-23T06:10:40.251Z</updated>
        <summary type="html"><![CDATA[While deep reinforcement learning excels at solving tasks where large amounts
of data can be collected through virtually unlimited interaction with the
environment, learning from limited interaction remains a key challenge. We
posit that an agent can learn more efficiently if we augment reward
maximization with self-supervised objectives based on structure in its visual
input and sequential interaction with the environment. Our method,
Self-Predictive Representations(SPR), trains an agent to predict its own latent
state representations multiple steps into the future. We compute target
representations for future states using an encoder which is an exponential
moving average of the agent's parameters and we make predictions using a
learned transition model. On its own, this future prediction objective
outperforms prior methods for sample-efficient deep RL from pixels. We further
improve performance by adding data augmentation to the future prediction loss,
which forces the agent's representations to be consistent across multiple views
of an observation. Our full self-supervised objective, which combines future
prediction and data augmentation, achieves a median human-normalized score of
0.415 on Atari in a setting limited to 100k steps of environment interaction,
which represents a 55% relative improvement over the previous state-of-the-art.
Notably, even in this limited data regime, SPR exceeds expert human scores on 7
out of 26 games. The code associated with this work is available at
https://github.com/mila-iqia/spr]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schwarzer_M/0/1/0/all/0/1"&gt;Max Schwarzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1"&gt;Ankesh Anand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goel_R/0/1/0/all/0/1"&gt;Rishab Goel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hjelm_R/0/1/0/all/0/1"&gt;R Devon Hjelm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1"&gt;Aaron Courville&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bachman_P/0/1/0/all/0/1"&gt;Philip Bachman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review on Modern Computational Optimal Transport Methods with Applications in Biomedical Research. (arXiv:2008.02995v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.02995</id>
        <link href="http://arxiv.org/abs/2008.02995"/>
        <updated>2021-05-23T06:10:40.243Z</updated>
        <summary type="html"><![CDATA[Optimal transport has been one of the most exciting subjects in mathematics,
starting from the 18th century. As a powerful tool to transport between two
probability measures, optimal transport methods have been reinvigorated
nowadays in a remarkable proliferation of modern data science applications. To
meet the big data challenges, various computational tools have been developed
in the recent decade to accelerate the computation for optimal transport
methods. In this review, we present some cutting-edge computational optimal
transport methods with a focus on the regularization-based methods and the
projection-based methods. We discuss their real-world applications in
biomedical research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhong_W/0/1/0/all/0/1"&gt;Wenxuan Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ma_P/0/1/0/all/0/1"&gt;Ping Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning-based attacks in Cyber-Physical Systems: Exploration, Detection, and Control Cost trade-offs. (arXiv:2011.10718v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10718</id>
        <link href="http://arxiv.org/abs/2011.10718"/>
        <updated>2021-05-23T06:10:40.226Z</updated>
        <summary type="html"><![CDATA[We study the problem of learning-based attacks in linear systems, where the
communication channel between the controller and the plant can be hijacked by a
malicious attacker. We assume the attacker learns the dynamics of the system
from observations, then overrides the controller's actuation signal, while
mimicking legitimate operation by providing fictitious sensor readings to the
controller. On the other hand, the controller is on a lookout to detect the
presence of the attacker and tries to enhance the detection performance by
carefully crafting its control signals. We study the trade-offs between the
information acquired by the attacker from observations, the detection
capabilities of the controller, and the control cost. Specifically, we provide
tight upper and lower bounds on the expected $\epsilon$-deception time, namely
the time required by the controller to make a decision regarding the presence
of an attacker with confidence at least $(1-\epsilon\log(1/\epsilon))$. We then
show a probabilistic lower bound on the time that must be spent by the attacker
learning the system, in order for the controller to have a given expected
$\epsilon$-deception time. We show that this bound is also order optimal, in
the sense that if the attacker satisfies it, then there exists a learning
algorithm with the given order expected deception time. Finally, we show a
lower bound on the expected energy expenditure required to guarantee detection
with confidence at least $1-\epsilon \log(1/\epsilon)$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Rangi_A/0/1/0/all/0/1"&gt;Anshuka Rangi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khojasteh_M/0/1/0/all/0/1"&gt;Mohammad Javad Khojasteh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Franceschetti_M/0/1/0/all/0/1"&gt;Massimo Franceschetti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UmlsBERT: Clinical Domain Knowledge Augmentation of Contextual Embeddings Using the Unified Medical Language System Metathesaurus. (arXiv:2010.10391v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10391</id>
        <link href="http://arxiv.org/abs/2010.10391"/>
        <updated>2021-05-23T06:10:40.219Z</updated>
        <summary type="html"><![CDATA[Contextual word embedding models, such as BioBERT and Bio_ClinicalBERT, have
achieved state-of-the-art results in biomedical natural language processing
tasks by focusing their pre-training process on domain-specific corpora.
However, such models do not take into consideration expert domain knowledge.

In this work, we introduced UmlsBERT, a contextual embedding model that
integrates domain knowledge during the pre-training process via a novel
knowledge augmentation strategy. More specifically, the augmentation on
UmlsBERT with the Unified Medical Language System (UMLS) Metathesaurus was
performed in two ways: i) connecting words that have the same underlying
`concept' in UMLS, and ii) leveraging semantic group knowledge in UMLS to
create clinically meaningful input embeddings. By applying these two
strategies, UmlsBERT can encode clinical domain knowledge into word embeddings
and outperform existing domain-specific models on common named-entity
recognition (NER) and clinical natural language inference clinical NLP tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Michalopoulos_G/0/1/0/all/0/1"&gt;George Michalopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuanxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaka_H/0/1/0/all/0/1"&gt;Hussam Kaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Helen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alexander Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MIN2Net: End-to-End Multi-Task Learning for Subject-Independent Motor Imagery EEG Classification. (arXiv:2102.03814v3 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03814</id>
        <link href="http://arxiv.org/abs/2102.03814"/>
        <updated>2021-05-23T06:10:40.212Z</updated>
        <summary type="html"><![CDATA[Advances in the motor imagery (MI)-based brain-computer interfaces (BCIs)
allow control of several applications by decoding neurophysiological phenomena,
which are usually recorded by electroencephalography (EEG) using a non-invasive
technique. Despite great advances in MI-based BCI, EEG rhythms are specific to
a subject and various changes over time. These issues point to significant
challenges to enhance the classification performance, especially in a
subject-independent manner. To overcome these challenges, we propose MIN2Net, a
novel end-to-end multi-task learning to tackle this task. We integrate deep
metric learning into a multi-task autoencoder to learn a compact and
discriminative latent representation from EEG and perform classification
simultaneously. This approach reduces the complexity in pre-processing, results
in significant performance improvement on EEG classification. Experimental
results in a subject-independent manner show that MIN2Net outperforms the
state-of-the-art techniques, achieving an F1-score improvement of 6.72%, and
2.23% on the SMR-BCI, and OpenBMI datasets, respectively. We demonstrate that
MIN2Net improves discriminative information in the latent representation. This
study indicates the possibility and practicality of using this model to develop
MI-based BCI applications for new users without the need for calibration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Autthasan_P/0/1/0/all/0/1"&gt;Phairot Autthasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chaisaen_R/0/1/0/all/0/1"&gt;Rattanaphon Chaisaen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sudhawiyangkul_T/0/1/0/all/0/1"&gt;Thapanun Sudhawiyangkul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rangpong_P/0/1/0/all/0/1"&gt;Phurin Rangpong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kiatthaveephong_S/0/1/0/all/0/1"&gt;Suktipol Kiatthaveephong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dilokthanakul_N/0/1/0/all/0/1"&gt;Nat Dilokthanakul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bhakdisongkhram_G/0/1/0/all/0/1"&gt;Gun Bhakdisongkhram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Phan_H/0/1/0/all/0/1"&gt;Huy Phan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Guan_C/0/1/0/all/0/1"&gt;Cuntai Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wilaiprasitporn_T/0/1/0/all/0/1"&gt;Theerawit Wilaiprasitporn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MoDL-QSM: Model-based Deep Learning for Quantitative Susceptibility Mapping. (arXiv:2101.08413v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08413</id>
        <link href="http://arxiv.org/abs/2101.08413"/>
        <updated>2021-05-23T06:10:40.206Z</updated>
        <summary type="html"><![CDATA[Quantitative susceptibility mapping (QSM) has demonstrated great potential in
quantifying tissue susceptibility in various brain diseases. However, the
intrinsic ill-posed inverse problem relating the tissue phase to the underlying
susceptibility distribution affects the accuracy for quantifying tissue
susceptibility. Recently, deep learning has shown promising results to improve
accuracy by reducing the streaking artifacts. However, there exists a mismatch
between the observed phase and the theoretical forward phase estimated by the
susceptibility label. In this study, we proposed a model-based deep learning
architecture that followed the STI (susceptibility tensor imaging) physical
model, referred to as MoDL-QSM. Specifically, MoDL-QSM accounts for the
relationship between STI-derived phase contrast induced by the susceptibility
tensor terms (ki13,ki23,ki33) and the acquired single-orientation phase. The
convolution neural networks are embedded into the physical model to learn a
regularization term containing prior information. ki33 and phase induced by
ki13 and ki23 terms were used as the labels for network training. Quantitative
evaluation metrics (RSME, SSIM, and HFEN) were compared with recently developed
deep learning QSM methods. The results showed that MoDL-QSM achieved superior
performance, demonstrating its potential for future applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1"&gt;Ruimin Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jiayi Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;He Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Baofeng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jie Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yuting Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Ming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chunlei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuyao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1"&gt;Jie Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1"&gt;Hongjiang Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to send a real number using a single bit (and some shared randomness). (arXiv:2010.02331v4 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.02331</id>
        <link href="http://arxiv.org/abs/2010.02331"/>
        <updated>2021-05-23T06:10:40.199Z</updated>
        <summary type="html"><![CDATA[We consider the fundamental problem of communicating an estimate of a real
number $x\in[0,1]$ using a single bit. A sender that knows $x$ chooses a value
$X\in\set{0,1}$ to transmit. In turn, a receiver estimates $x$ based on the
value of $X$. We consider both the biased and unbiased estimation problems and
aim to minimize the cost. For the biased case, the cost is the worst-case (over
the choice of $x$) expected squared error, which coincides with the variance if
the algorithm is required to be unbiased.

We first overview common biased and unbiased estimation approaches and prove
their optimality when no shared randomness is allowed. We then show how a small
amount of shared randomness, which can be as low as a single bit, reduces the
cost in both cases. Specifically, we derive lower bounds on the cost attainable
by any algorithm with unrestricted use of shared randomness and propose
near-optimal solutions that use a small number of shared random bits. Finally,
we discuss open problems and future directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Basat_R/0/1/0/all/0/1"&gt;Ran Ben-Basat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitzenmacher_M/0/1/0/all/0/1"&gt;Michael Mitzenmacher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vargaftik_S/0/1/0/all/0/1"&gt;Shay Vargaftik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Neural Networks Jamming on the Beat. (arXiv:2007.06284v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.06284</id>
        <link href="http://arxiv.org/abs/2007.06284"/>
        <updated>2021-05-23T06:10:40.190Z</updated>
        <summary type="html"><![CDATA[This paper addresses the issue of long-scale correlations that is
characteristic for symbolic music and is a challenge for modern generative
algorithms. It suggests a very simple workaround for this challenge, namely,
generation of a drum pattern that could be further used as a foundation for
melody generation. The paper presents a large dataset of drum patterns
alongside with corresponding melodies. It explores two possible methods for
drum pattern generation. Exploring a latent space of drum patterns one could
generate new drum patterns with a given music style. Finally, the paper
demonstrates that a simple artificial neural network could be trained to
generate melodies corresponding with these drum patters used as inputs.
Resulting system could be used for end-to-end generation of symbolic music with
song-like structure and higher long-scale correlations between the notes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tikhonov_A/0/1/0/all/0/1"&gt;Alexey Tikhonov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yamshchikov_I/0/1/0/all/0/1"&gt;Ivan P. Yamshchikov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraphReach: Position-Aware Graph Neural Network using Reachability Estimations. (arXiv:2008.09657v3 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.09657</id>
        <link href="http://arxiv.org/abs/2008.09657"/>
        <updated>2021-05-23T06:10:40.181Z</updated>
        <summary type="html"><![CDATA[Majority of the existing graph neural networks (GNN) learn node embeddings
that encode their local neighborhoods but not their positions. Consequently,
two nodes that are vastly distant but located in similar local neighborhoods
map to similar embeddings in those networks. This limitation prevents accurate
performance in predictive tasks that rely on position information. In this
paper,we develop GraphReach, a position-aware inductive GNN that captures the
global positions of nodes through reachability estimations with respect to a
set of anchor nodes. The anchors are strategically selected so that
reachability estimations across all the nodes are maximized. We show that this
combinatorial anchor selection problem is NP-hard and, consequently, develop a
greedy (1-1/e) approximation heuristic. Empirical evaluation against
state-of-the-art GNN architectures reveal that GraphReach provides up to 40%
relative improvement in accuracy. In addition, it is more robust to adversarial
attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nishad_S/0/1/0/all/0/1"&gt;Sunil Nishad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Shubhangi Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1"&gt;Arnab Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranu_S/0/1/0/all/0/1"&gt;Sayan Ranu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling the Field Value Variations and Field Interactions Simultaneously for Fraud Detection. (arXiv:2008.05600v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.05600</id>
        <link href="http://arxiv.org/abs/2008.05600"/>
        <updated>2021-05-23T06:10:40.174Z</updated>
        <summary type="html"><![CDATA[With the explosive growth of e-commerce, online transaction fraud has become
one of the biggest challenges for e-commerce platforms. The historical
behaviors of users provide rich information for digging into the users' fraud
risk. While considerable efforts have been made in this direction, a
long-standing challenge is how to effectively exploit internal user information
and provide explainable prediction results. In fact, the value variations of
same field from different events and the interactions of different fields
inside one event have proven to be strong indicators for fraudulent behaviors.
In this paper, we propose the Dual Importance-aware Factorization Machines
(DIFM), which exploits the internal field information among users' behavior
sequence from dual perspectives, i.e., field value variations and field
interactions simultaneously for fraud detection. The proposed model is deployed
in the risk management system of one of the world's largest e-commerce
platforms, which utilize it to provide real-time transaction fraud detection.
Experimental results on real industrial data from different regions in the
platform clearly demonstrate that our model achieves significant improvements
compared with various state-of-the-art baseline models. Moreover, the DIFM
could also give an insight into the explanation of the prediction results from
dual perspectives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xi_D/0/1/0/all/0/1"&gt;Dongbo Xi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1"&gt;Bowen Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1"&gt;Fuzhen Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yongchun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shuai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1"&gt;Yuan Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1"&gt;Qing He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Head Attention: Collaborate Instead of Concatenate. (arXiv:2006.16362v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.16362</id>
        <link href="http://arxiv.org/abs/2006.16362"/>
        <updated>2021-05-23T06:10:40.021Z</updated>
        <summary type="html"><![CDATA[Attention layers are widely used in natural language processing (NLP) and are
beginning to influence computer vision architectures. Training very large
transformer models allowed significant improvement in both fields, but once
trained, these networks show symptoms of over-parameterization. For instance,
it is known that many attention heads can be pruned without impacting accuracy.
This work aims to enhance current understanding on how multiple heads interact.
Motivated by the observation that attention heads learn redundant key/query
projections, we propose a collaborative multi-head attention layer that enables
heads to learn shared projections. Our scheme decreases the number of
parameters in an attention layer and can be used as a drop-in replacement in
any transformer architecture. Our experiments confirm that sharing key/query
dimensions can be exploited in language understanding, machine translation and
vision. We also show that it is possible to re-parametrize a pre-trained
multi-head attention layer into our collaborative attention layer.
Collaborative multi-head attention reduces the size of the key and query
projections by 4 for same accuracy and speed. Our code is public.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cordonnier_J/0/1/0/all/0/1"&gt;Jean-Baptiste Cordonnier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loukas_A/0/1/0/all/0/1"&gt;Andreas Loukas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge Distillation: A Survey. (arXiv:2006.05525v7 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05525</id>
        <link href="http://arxiv.org/abs/2006.05525"/>
        <updated>2021-05-23T06:10:39.966Z</updated>
        <summary type="html"><![CDATA[In recent years, deep neural networks have been successful in both industry
and academia, especially for computer vision tasks. The great success of deep
learning is mainly due to its scalability to encode large-scale data and to
maneuver billions of model parameters. However, it is a challenge to deploy
these cumbersome deep models on devices with limited resources, e.g., mobile
phones and embedded devices, not only because of the high computational
complexity but also the large storage requirements. To this end, a variety of
model compression and acceleration techniques have been developed. As a
representative type of model compression and acceleration, knowledge
distillation effectively learns a small student model from a large teacher
model. It has received rapid increasing attention from the community. This
paper provides a comprehensive survey of knowledge distillation from the
perspectives of knowledge categories, training schemes, teacher-student
architecture, distillation algorithms, performance comparison and applications.
Furthermore, challenges in knowledge distillation are briefly reviewed and
comments on future research are discussed and forwarded.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gou_J/0/1/0/all/0/1"&gt;Jianping Gou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1"&gt;Baosheng Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maybank_S/0/1/0/all/0/1"&gt;Stephen John Maybank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Surrogate gradients for analog neuromorphic computing. (arXiv:2006.07239v3 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07239</id>
        <link href="http://arxiv.org/abs/2006.07239"/>
        <updated>2021-05-23T06:10:39.959Z</updated>
        <summary type="html"><![CDATA[To rapidly process temporal information at a low metabolic cost, biological
neurons integrate inputs as an analog sum but communicate with spikes, binary
events in time. Analog neuromorphic hardware uses the same principles to
emulate spiking neural networks with exceptional energy-efficiency. However,
instantiating high-performing spiking networks on such hardware remains a
significant challenge due to device mismatch and the lack of efficient training
algorithms. Here, we introduce a general in-the-loop learning framework based
on surrogate gradients that resolves these issues. Using the BrainScaleS-2
neuromorphic system, we show that learning self-corrects for device mismatch
resulting in competitive spiking network performance on both vision and speech
benchmarks. Our networks display sparse spiking activity with, on average, far
less than one spike per hidden neuron and input, perform inference at rates of
up to 85 k frames/second, and consume less than 200 mW. In summary, our work
sets several new benchmarks for low-energy spiking network processing on analog
neuromorphic hardware and paves the way for future on-chip learning algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cramer_B/0/1/0/all/0/1"&gt;Benjamin Cramer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Billaudelle_S/0/1/0/all/0/1"&gt;Sebastian Billaudelle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanya_S/0/1/0/all/0/1"&gt;Simeon Kanya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leibfried_A/0/1/0/all/0/1"&gt;Aron Leibfried&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grubl_A/0/1/0/all/0/1"&gt;Andreas Gr&amp;#xfc;bl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karasenko_V/0/1/0/all/0/1"&gt;Vitali Karasenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pehle_C/0/1/0/all/0/1"&gt;Christian Pehle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schreiber_K/0/1/0/all/0/1"&gt;Korbinian Schreiber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stradmann_Y/0/1/0/all/0/1"&gt;Yannik Stradmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weis_J/0/1/0/all/0/1"&gt;Johannes Weis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schemmel_J/0/1/0/all/0/1"&gt;Johannes Schemmel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zenke_F/0/1/0/all/0/1"&gt;Friedemann Zenke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural networks with superexpressive activations and integer weights. (arXiv:2105.09917v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.09917</id>
        <link href="http://arxiv.org/abs/2105.09917"/>
        <updated>2021-05-23T06:10:39.952Z</updated>
        <summary type="html"><![CDATA[An example of an activation function $\sigma$ is given such that networks
with activations $\{\sigma, \lfloor\cdot\rfloor\}$, integer weights and a fixed
architecture depending on $d$ approximate continuous functions on $[0,1]^d$.
The range of integer weights required for $\varepsilon$-approximation of
H\"older continuous functions is derived, which leads to a convergence rate of
order $n^{\frac{-2\beta}{2\beta+d}}\log_2n$ for neural network regression
estimation of unknown $\beta$-H\"older continuous function with given $n$
samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Beknazaryan_A/0/1/0/all/0/1"&gt;Aleksandr Beknazaryan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning high-dimensional probability distributions using tree tensor networks. (arXiv:1912.07913v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.07913</id>
        <link href="http://arxiv.org/abs/1912.07913"/>
        <updated>2021-05-23T06:10:39.944Z</updated>
        <summary type="html"><![CDATA[We consider the problem of the estimation of a high-dimensional probability
distribution from i.i.d. samples of the distribution using model classes of
functions in tree-based tensor formats, a particular case of tensor networks
associated with a dimension partition tree. The distribution is assumed to
admit a density with respect to a product measure, possibly discrete for
handling the case of discrete random variables.

After discussing the representation of classical model classes in tree-based
tensor formats, we present learning algorithms based on empirical risk
minimization using a $L^2$ contrast.

These algorithms exploit the multilinear parametrization of the formats to
recast the nonlinear minimization problem into a sequence of empirical risk
minimization problems with linear models. A suitable parametrization of the
tensor in tree-based tensor format allows to obtain a linear model with
orthogonal bases, so that each problem admits an explicit expression of the
solution and cross-validation risk estimates. These estimations of the risk
enable the model selection, for instance when exploiting sparsity in the
coefficients of the representation.

A strategy for the adaptation of the tensor format (dimension tree and
tree-based ranks) is provided, which allows to discover and exploit some
specific structures of high-dimensional probability distributions such as
independence or conditional independence.

We illustrate the performances of the proposed algorithms for the
approximation of classical probabilistic models (such as Gaussian distribution,
graphical models, Markov chain).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Grelier_E/0/1/0/all/0/1"&gt;Erwan Grelier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nouy_A/0/1/0/all/0/1"&gt;Anthony Nouy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lebrun_R/0/1/0/all/0/1"&gt;R&amp;#xe9;gis Lebrun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fully Convolutional Networks for Automatically Generating Image Masks to Train Mask R-CNN. (arXiv:2003.01383v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.01383</id>
        <link href="http://arxiv.org/abs/2003.01383"/>
        <updated>2021-05-23T06:10:39.936Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel automatically generating image masks method for
the state-of-the-art Mask R-CNN deep learning method. The Mask R-CNN method
achieves the best results in object detection until now, however, it is very
time-consuming and laborious to get the object Masks for training, the proposed
method is composed by a two-stage design, to automatically generating image
masks, the first stage implements a fully convolutional networks (FCN) based
segmentation network, the second stage network, a Mask R-CNN based object
detection network, which is trained on the object image masks from FCN output,
the original input image, and additional label information. Through
experimentation, our proposed method can obtain the image masks automatically
to train Mask R-CNN, and it can achieve very high classification accuracy with
an over 90% mean of average precision (mAP) for segmentation]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siebert_J/0/1/0/all/0/1"&gt;Jan Paul Siebert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiangrong Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Pruning at Initialization. (arXiv:2002.08797v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.08797</id>
        <link href="http://arxiv.org/abs/2002.08797"/>
        <updated>2021-05-23T06:10:39.916Z</updated>
        <summary type="html"><![CDATA[Overparameterized Neural Networks (NN) display state-of-the-art performance.
However, there is a growing need for smaller, energy-efficient, neural networks
tobe able to use machine learning applications on devices with limited
computational resources. A popular approach consists of using pruning
techniques. While these techniques have traditionally focused on pruning
pre-trained NN (LeCun et al.,1990; Hassibi et al., 1993), recent work by Lee et
al. (2018) has shown promising results when pruning at initialization. However,
for Deep NNs, such procedures remain unsatisfactory as the resulting pruned
networks can be difficult to train and, for instance, they do not prevent one
layer from being fully pruned. In this paper, we provide a comprehensive
theoretical analysis of Magnitude and Gradient based pruning at initialization
and training of sparse architectures. This allows us to propose novel
principled approaches which we validate experimentally on a variety of NN
architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hayou_S/0/1/0/all/0/1"&gt;Soufiane Hayou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ton_J/0/1/0/all/0/1"&gt;Jean-Francois Ton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1"&gt;Arnaud Doucet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Teh_Y/0/1/0/all/0/1"&gt;Yee Whye Teh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computer Users Have Unique Yet Temporally Inconsistent Computer Usage Profiles. (arXiv:2105.09900v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09900</id>
        <link href="http://arxiv.org/abs/2105.09900"/>
        <updated>2021-05-23T06:10:39.909Z</updated>
        <summary type="html"><![CDATA[This paper investigates whether computer usage profiles comprised of
process-, network-, mouse- and keystroke-related events are unique and
temporally consistent in a naturalistic setting, discussing challenges and
opportunities of using such profiles in applications of continuous
authentication. We collected ecologically-valid computer usage profiles from 28
MS Windows 10 computer users over 8 weeks and submitted this data to
comprehensive machine learning analysis involving a diverse set of online and
offline classifiers. We found that (i) computer usage profiles have the
potential to uniquely characterize computer users (with a maximum F-score of
99.94%); (ii) network-related events were the most useful features to properly
recognize profiles (95.14% of the top features distinguishing users being
network-related); (iii) user profiles were mostly inconsistent over the 8-week
data collection period, with 92.86% of users exhibiting drifts in terms of time
and usage habits; and (iv) online models are better suited to handle computer
usage profiles compared to offline models (maximum F-score for each approach
was 95.99% and 99.94%, respectively).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Giovanini_L/0/1/0/all/0/1"&gt;Luiz Giovanini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ceschin_F/0/1/0/all/0/1"&gt;Fabr&amp;#xed;cio Ceschin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_M/0/1/0/all/0/1"&gt;Mirela Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1"&gt;Aokun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_R/0/1/0/all/0/1"&gt;Ramchandra Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banda_S/0/1/0/all/0/1"&gt;Sanjay Banda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lysaght_M/0/1/0/all/0/1"&gt;Madison Lysaght&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_H/0/1/0/all/0/1"&gt;Heng Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sapountzis_N/0/1/0/all/0/1"&gt;Nikolaos Sapountzis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1"&gt;Ruimin Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matthews_B/0/1/0/all/0/1"&gt;Brandon Matthews&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dapeng Oliver Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gregio_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Gr&amp;#xe9;gio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1"&gt;Daniela Oliveira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Graph-Based Behavior-Aware Recommendation for Interactive News. (arXiv:1812.00002v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1812.00002</id>
        <link href="http://arxiv.org/abs/1812.00002"/>
        <updated>2021-05-23T06:10:39.898Z</updated>
        <summary type="html"><![CDATA[Interactive news recommendation has been launched and attracted much
attention recently. In this scenario, user's behavior evolves from single click
behavior to multiple behaviors including like, comment, share etc. However,
most of the existing methods still use single click behavior as the unique
criterion of judging user's preferences. Further, although heterogeneous graphs
have been applied in different areas, a proper way to construct a heterogeneous
graph for interactive news data with an appropriate learning mechanism on it is
still desired. To address the above concerns, we propose a graph-based
behavior-aware network, which simultaneously considers six different types of
behaviors as well as user's demand on the news diversity. We have three main
steps. First, we build an interaction behavior graph for multi-level and
multi-category data. Second, we apply DeepWalk on the behavior graph to obtain
entity semantics, then build a graph-based convolutional neural network called
G-CNN to learn news representations, and an attention-based LSTM to learn
behavior sequence representations. Third, we introduce core and coritivity
features for the behavior graph, which measure the concentration degree of
user's interests. These features affect the trade-off between accuracy and
diversity of our personalized recommendation system. Taking these features into
account, our system finally achieves recommending news to different users at
their different levels of concentration degrees.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1"&gt;Mingyuan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Na_S/0/1/0/all/0/1"&gt;Sen Na&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Congzhou Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classification of Urban Morphology with Deep Learning: Application on Urban Vitality. (arXiv:2105.09908v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09908</id>
        <link href="http://arxiv.org/abs/2105.09908"/>
        <updated>2021-05-23T06:10:39.891Z</updated>
        <summary type="html"><![CDATA[There is a prevailing trend to study urban morphology quantitatively thanks
to the growing accessibility to various forms of spatial big data, increasing
computing power, and use cases benefiting from such information. The methods
developed up to now measure urban morphology with numerical indices describing
density, proportion, and mixture, but they do not directly represent
morphological features from human's visual and intuitive perspective. We take
the first step to bridge the gap by proposing a deep learning-based technique
to automatically classify road networks into four classes on a visual basis.
The method is implemented by generating an image of the street network (Colored
Road Hierarchy Diagram), which we introduce in this paper, and classifying it
using a deep convolutional neural network (ResNet-34). The model achieves an
overall classification accuracy of 0.875. Nine cities around the world are
selected as the study areas and their road networks are acquired from
OpenStreetMap. Latent subgroups among the cities are uncovered through a
clustering on the percentage of each road network category. In the subsequent
part of the paper, we focus on the usability of such classification: the
effectiveness of our human perception augmentation is examined by a case study
of urban vitality prediction. An advanced tree-based regression model is for
the first time designated to establish the relationship between morphological
indices and vitality indicators. A positive effect of human perception
augmentation is detected in the comparative experiment of baseline model and
augmented model. This work expands the toolkit of quantitative urban morphology
study with new techniques, supporting further studies in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wangyang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1"&gt;Abraham Noah Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biljecki_F/0/1/0/all/0/1"&gt;Filip Biljecki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Perspective Anomaly Detection. (arXiv:2105.09903v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09903</id>
        <link href="http://arxiv.org/abs/2105.09903"/>
        <updated>2021-05-23T06:10:39.885Z</updated>
        <summary type="html"><![CDATA[Multi-view classification is inspired by the behavior of humans, especially
when fine-grained features or in our case rarely occurring anomalies are to be
detected. Current contributions point to the problem of how high-dimensional
data can be fused. In this work, we build upon the deep support vector data
description algorithm and address multi-perspective anomaly detection using
three different fusion techniques i.e. early fusion, late fusion, and late
fusion with multiple decoders. We employ different augmentation techniques with
a denoising process to deal with scarce one-class data, which further improves
the performance (ROC AUC = 80\%). Furthermore, we introduce the dices dataset
that consists of over 2000 grayscale images of falling dices from multiple
perspectives, with 5\% of the images containing rare anomalies (e.g. drill
holes, sawing, or scratches). We evaluate our approach on the new dices dataset
using images from two different perspectives and also benchmark on the standard
MNIST dataset. Extensive experiments demonstrate that our proposed approach
exceeds the state-of-the-art on both the MNIST and dices datasets. To the best
of our knowledge, this is the first work that focuses on addressing
multi-perspective anomaly detection in images by jointly using different
perspectives together with one single objective function for anomaly detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Madan_M/0/1/0/all/0/1"&gt;Manav Madan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jakob_P/0/1/0/all/0/1"&gt;Peter Jakob&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmid_Schirling_T/0/1/0/all/0/1"&gt;Tobias Schmid-Schirling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1"&gt;Abhinav Valada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying the Complexity of Standard Benchmarking Datasets for Long-Term Human Trajectory Prediction. (arXiv:2005.13934v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.13934</id>
        <link href="http://arxiv.org/abs/2005.13934"/>
        <updated>2021-05-23T06:10:39.867Z</updated>
        <summary type="html"><![CDATA[Methods to quantify the complexity of trajectory datasets are still a missing
piece in benchmarking human trajectory prediction models. In order to gain a
better understanding of the complexity of trajectory prediction tasks and
following the intuition, that more complex datasets contain more information,
an approach for quantifying the amount of information contained in a dataset
from a prototype-based dataset representation is proposed. The dataset
representation is obtained by first employing a non-trivial spatial sequence
alignment, which enables a subsequent learning vector quantization (LVQ) stage.
A large-scale complexity analysis is conducted on several human trajectory
prediction benchmarking datasets, followed by a brief discussion on indications
for human trajectory prediction and benchmarking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hug_R/0/1/0/all/0/1"&gt;Ronny Hug&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Becker_S/0/1/0/all/0/1"&gt;Stefan Becker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hubner_W/0/1/0/all/0/1"&gt;Wolfgang H&amp;#xfc;bner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arens_M/0/1/0/all/0/1"&gt;Michael Arens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flexible Compositional Learning of Structured Visual Concepts. (arXiv:2105.09848v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09848</id>
        <link href="http://arxiv.org/abs/2105.09848"/>
        <updated>2021-05-23T06:10:39.860Z</updated>
        <summary type="html"><![CDATA[Humans are highly efficient learners, with the ability to grasp the meaning
of a new concept from just a few examples. Unlike popular computer vision
systems, humans can flexibly leverage the compositional structure of the visual
world, understanding new concepts as combinations of existing concepts. In the
current paper, we study how people learn different types of visual
compositions, using abstract visual forms with rich relational structure. We
find that people can make meaningful compositional generalizations from just a
few examples in a variety of scenarios, and we develop a Bayesian program
induction model that provides a close fit to the behavioral data. Unlike past
work examining special cases of compositionality, our work shows how a single
computational approach can account for many distinct types of compositional
generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yanli Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lake_B/0/1/0/all/0/1"&gt;Brenden M. Lake&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bidirectional LSTM-CRF Attention-based Model for Chinese Word Segmentation. (arXiv:2105.09681v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09681</id>
        <link href="http://arxiv.org/abs/2105.09681"/>
        <updated>2021-05-23T06:10:39.853Z</updated>
        <summary type="html"><![CDATA[Chinese word segmentation (CWS) is the basic of Chinese natural language
processing (NLP). The quality of word segmentation will directly affect the
rest of NLP tasks. Recently, with the artificial intelligence tide rising
again, Long Short-Term Memory (LSTM) neural network, as one of easily modeling
in sequence, has been widely utilized in various kinds of NLP tasks, and
functions well. Attention mechanism is an ingenious method to solve the memory
compression problem on LSTM. Furthermore, inspired by the powerful abilities of
bidirectional LSTM models for modeling sequence and CRF model for decoding, we
propose a Bidirectional LSTM-CRF Attention-based Model in this paper.
Experiments on PKU and MSRA benchmark datasets show that our model performs
better than the baseline methods modeling by other neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1"&gt;Chen Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1"&gt;Zhuangwei Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Weihua Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yanbu Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Latency Real-Time Non-Parallel Voice Conversion based on Cyclic Variational Autoencoder and Multiband WaveRNN with Data-Driven Linear Prediction. (arXiv:2105.09858v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2105.09858</id>
        <link href="http://arxiv.org/abs/2105.09858"/>
        <updated>2021-05-23T06:10:39.847Z</updated>
        <summary type="html"><![CDATA[This paper presents a low-latency real-time (LLRT) non-parallel voice
conversion (VC) framework based on cyclic variational autoencoder (CycleVAE)
and multiband WaveRNN with data-driven linear prediction (MWDLP). CycleVAE is a
robust non-parallel multispeaker spectral model, which utilizes a
speaker-independent latent space and a speaker-dependent code to generate
reconstructed/converted spectral features given the spectral features of an
input speaker. On the other hand, MWDLP is an efficient and a high-quality
neural vocoder that can handle multispeaker data and generate speech waveform
for LLRT applications with CPU. To accommodate LLRT constraint with CPU, we
propose a novel CycleVAE framework that utilizes mel-spectrogram as spectral
features and is built with a sparse network architecture. Further, to improve
the modeling performance, we also propose a novel fine-tuning procedure that
refines the frame-rate CycleVAE network by utilizing the waveform loss from the
MWDLP network. The experimental results demonstrate that the proposed framework
achieves high-performance VC, while allowing for LLRT usage with a single-core
of $2.1$--$2.7$~GHz CPU on a real-time factor of $0.87$--$0.95$, including
input/output, feature extraction, on a frame shift of $10$ ms, a window length
of $27.5$ ms, and $2$ lookup frames.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tobing_P/0/1/0/all/0/1"&gt;Patrick Lumban Tobing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1"&gt;Tomoki Toda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Personalized Fairness based on Causal Notion. (arXiv:2105.09829v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.09829</id>
        <link href="http://arxiv.org/abs/2105.09829"/>
        <updated>2021-05-23T06:10:39.840Z</updated>
        <summary type="html"><![CDATA[Recommender systems are gaining increasing and critical impacts on human and
society since a growing number of users use them for information seeking and
decision making. Therefore, it is crucial to address the potential unfairness
problems in recommendations. Just like users have personalized preferences on
items, users' demands for fairness are also personalized in many scenarios.
Therefore, it is important to provide personalized fair recommendations for
users to satisfy their personalized fairness demands. Besides, previous works
on fair recommendation mainly focus on association-based fairness. However, it
is important to advance from associative fairness notions to causal fairness
notions for assessing fairness more properly in recommender systems. Based on
the above considerations, this paper focuses on achieving personalized
counterfactual fairness for users in recommender systems. To this end, we
introduce a framework for achieving counterfactually fair recommendations
through adversary learning by generating feature-independent user embeddings
for recommendation. The framework allows recommender systems to achieve
personalized fairness for users while also covering non-personalized
situations. Experiments on two real-world datasets with shallow and deep
recommendation algorithms show that our method can generate fairer
recommendations for users with a desirable recommendation performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yunqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hanxiong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shuyuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1"&gt;Yingqiang Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongfeng Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning in physics: a study of dielectric quasi-cubic particles in a uniform electric field. (arXiv:2105.09866v1 [physics.class-ph])]]></title>
        <id>http://arxiv.org/abs/2105.09866</id>
        <link href="http://arxiv.org/abs/2105.09866"/>
        <updated>2021-05-23T06:10:39.834Z</updated>
        <summary type="html"><![CDATA[Solving physics problems for which we know the equations, boundary conditions
and symmetries can be done by deep learning. The constraints can be either
imposed as terms in a loss function or used to formulate a neural ansatz. In
the present case study, we calculate the induced field inside and outside a
dielectric cube placed in a uniform electric field, wherein the dielectric
mismatch at edges and corners of the cube makes accurate calculations
numerically challenging. The electric potential is expressed as an ansatz
incorporating neural networks with known leading order behaviors and symmetries
and the Laplace's equation is then solved with boundary conditions at the
dielectric interface by minimizing a loss function. The loss function ensures
that both Laplace's equation and boundary conditions are satisfied everywhere
inside a large solution domain. We study how the electric potential inside and
outside a quasi-cubic particle evolves through a sequence of shapes from a
sphere to a cube. The neural network being differentiable, it is
straightforward to calculate the electric field over the whole domain, the
induced surface charge distribution and the polarizability. The neural network
being retentive, one can efficiently follow how the field changes upon
particle's shape or dielectric constant by iterating from any previously
converged solution. The present work's objective is two-fold, first to show how
an a priori knowledge can be incorporated into neural networks to achieve
efficient learning and second to apply the method and study how the induced
field and polarizability change when a dielectric particle progressively
changes its shape from a sphere to a cube.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Guet_C/0/1/0/all/0/1"&gt;Claude Guet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Fidelity and Low-Latency Universal Neural Vocoder based on Multiband WaveRNN with Data-Driven Linear Prediction for Discrete Waveform Modeling. (arXiv:2105.09856v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2105.09856</id>
        <link href="http://arxiv.org/abs/2105.09856"/>
        <updated>2021-05-23T06:10:39.814Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel high-fidelity and low-latency universal neural
vocoder framework based on multiband WaveRNN with data-driven linear prediction
for discrete waveform modeling (MWDLP). MWDLP employs a coarse-fine bit WaveRNN
architecture for 10-bit mu-law waveform modeling. A sparse gated recurrent unit
with a relatively large size of hidden units is utilized, while the multiband
modeling is deployed to achieve real-time low-latency usage. A novel technique
for data-driven linear prediction (LP) with discrete waveform modeling is
proposed, where the LP coefficients are estimated in a data-driven manner.
Moreover, a novel loss function using short-time Fourier transform (STFT) for
discrete waveform modeling with Gumbel approximation is also proposed. The
experimental results demonstrate that the proposed MWDLP framework generates
high-fidelity synthetic speech for seen and unseen speakers and/or language on
300 speakers training data including clean and noisy/reverberant conditions,
where the number of training utterances is limited to 60 per speaker, while
allowing for real-time low-latency processing using a single core of $\sim\!$
2.1--2.7~GHz CPU with $\sim\!$ 0.57--0.64 real-time factor including
input/output and feature extraction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tobing_P/0/1/0/all/0/1"&gt;Patrick Lumban Tobing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1"&gt;Tomoki Toda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DEHB: Evolutionary Hyberband for Scalable, Robust and Efficient Hyperparameter Optimization. (arXiv:2105.09821v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09821</id>
        <link href="http://arxiv.org/abs/2105.09821"/>
        <updated>2021-05-23T06:10:39.793Z</updated>
        <summary type="html"><![CDATA[Modern machine learning algorithms crucially rely on several design decisions
to achieve strong performance, making the problem of Hyperparameter
Optimization (HPO) more important than ever. Here, we combine the advantages of
the popular bandit-based HPO method Hyperband (HB) and the evolutionary search
approach of Differential Evolution (DE) to yield a new HPO method which we call
DEHB. Comprehensive results on a very broad range of HPO problems, as well as a
wide range of tabular benchmarks from neural architecture search, demonstrate
that DEHB achieves strong performance far more robustly than all previous HPO
methods we are aware of, especially for high-dimensional problems with discrete
input dimensions. For example, DEHB is up to 1000x faster than random search.
It is also efficient in computational time, conceptually simple and easy to
implement, positioning it well to become a new default HPO method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Awad_N/0/1/0/all/0/1"&gt;Noor Awad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mallik_N/0/1/0/all/0/1"&gt;Neeratyoy Mallik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1"&gt;Frank Hutter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Monte Carlo Filtering Objectives: A New Family of Variational Objectives to Learn Generative Model and Neural Adaptive Proposal for Time Series. (arXiv:2105.09801v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09801</id>
        <link href="http://arxiv.org/abs/2105.09801"/>
        <updated>2021-05-23T06:10:39.786Z</updated>
        <summary type="html"><![CDATA[Learning generative models and inferring latent trajectories have shown to be
challenging for time series due to the intractable marginal likelihoods of
flexible generative models. It can be addressed by surrogate objectives for
optimization. We propose Monte Carlo filtering objectives (MCFOs), a family of
variational objectives for jointly learning parametric generative models and
amortized adaptive importance proposals of time series. MCFOs extend the
choices of likelihood estimators beyond Sequential Monte Carlo in
state-of-the-art objectives, possess important properties revealing the factors
for the tightness of objectives, and allow for less biased and variant gradient
estimates. We demonstrate that the proposed MCFOs and gradient estimations lead
to efficient and stable model learning, and learned generative models well
explain data and importance proposals are more sample efficient on various
kinds of time series data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shuangshuang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1"&gt;Sihao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karayiannidis_Y/0/1/0/all/0/1"&gt;Yiannis Karayiannidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bjorkman_M/0/1/0/all/0/1"&gt;M&amp;#xe5;rten Bj&amp;#xf6;rkman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepAVO: Efficient Pose Refining with Feature Distilling for Deep Visual Odometry. (arXiv:2105.09899v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09899</id>
        <link href="http://arxiv.org/abs/2105.09899"/>
        <updated>2021-05-23T06:10:39.777Z</updated>
        <summary type="html"><![CDATA[The technology for Visual Odometry (VO) that estimates the position and
orientation of the moving object through analyzing the image sequences captured
by on-board cameras, has been well investigated with the rising interest in
autonomous driving. This paper studies monocular VO from the perspective of
Deep Learning (DL). Unlike most current learning-based methods, our approach,
called DeepAVO, is established on the intuition that features contribute
discriminately to different motion patterns. Specifically, we present a novel
four-branch network to learn the rotation and translation by leveraging
Convolutional Neural Networks (CNNs) to focus on different quadrants of optical
flow input. To enhance the ability of feature selection, we further introduce
an effective channel-spatial attention mechanism to force each branch to
explicitly distill related information for specific Frame to Frame (F2F) motion
estimation. Experiments on various datasets involving outdoor driving and
indoor walking scenarios show that the proposed DeepAVO outperforms the
state-of-the-art monocular methods by a large margin, demonstrating competitive
performance to the stereo VO algorithm and verifying promising potential for
generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1"&gt;Ran Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Mingkun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1"&gt;Rujun Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1"&gt;Bo Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1"&gt;Zhuoling Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual-side Sparse Tensor Core. (arXiv:2105.09564v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2105.09564</id>
        <link href="http://arxiv.org/abs/2105.09564"/>
        <updated>2021-05-23T06:10:39.770Z</updated>
        <summary type="html"><![CDATA[Leveraging sparsity in deep neural network (DNN) models is promising for
accelerating model inference. Yet existing GPUs can only leverage the sparsity
from weights but not activations, which are dynamic, unpredictable, and hence
challenging to exploit. In this work, we propose a novel architecture to
efficiently harness the dual-side sparsity (i.e., weight and activation
sparsity). We take a systematic approach to understand the (dis)advantages of
previous sparsity-related architectures and propose a novel, unexplored
paradigm that combines outer-product computation primitive and bitmap-based
encoding format. We demonstrate the feasibility of our design with minimal
changes to the existing production-scale inner-product-based Tensor Core. We
propose a set of novel ISA extensions and co-design the matrix-matrix
multiplication and convolution algorithms, which are the two dominant
computation patterns in today's DNN models, to exploit our new dual-side sparse
Tensor Core. Our evaluation shows that our design can fully unleash the
dual-side DNN sparsity and improve the performance by up to one order of
magnitude with \hl{small} hardware overhead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zhiqiang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1"&gt;Cong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yunxin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1"&gt;Jingwen Leng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Covid-19 Detection from Chest X-ray and Patient Metadata using Graph Convolutional Neural Networks. (arXiv:2105.09720v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.09720</id>
        <link href="http://arxiv.org/abs/2105.09720"/>
        <updated>2021-05-23T06:10:39.750Z</updated>
        <summary type="html"><![CDATA[The novel corona virus (Covid-19) has introduced significant challenges due
to its rapid spreading nature through respiratory transmission. As a result,
there is a huge demand for Artificial Intelligence (AI) based quick disease
diagnosis methods as an alternative to high demand tests such as Polymerase
Chain Reaction (PCR). Chest X-ray (CXR) Image analysis is such cost-effective
radiography technique due to resource availability and quick screening. But, a
sufficient and systematic data collection that is required by complex deep
leaning (DL) models is more difficult and hence there are recent efforts that
utilize transfer learning to address this issue. Still these transfer learnt
models suffer from lack of generalization and increased bias to the training
dataset resulting poor performance for unseen data. Limited correlation of the
transferred features from the pre-trained model to a specific medical imaging
domain like X-ray and overfitting on fewer data can be reasons for this
circumstance. In this work, we propose a novel Graph Convolution Neural Network
(GCN) that is capable of identifying bio-markers of Covid-19 pneumonia from CXR
images and meta information about patients. The proposed method exploits
important relational knowledge between data instances and their features using
graph representation and applies convolution to learn the graph data which is
not possible with conventional convolution on Euclidean domain. The results of
extensive experiments of proposed model on binary (Covid vs normal) and three
class (Covid, normal, other pneumonia) classification problems outperform
different benchmark transfer learnt models, hence overcoming the aforementioned
drawbacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mudiyanselage_T/0/1/0/all/0/1"&gt;Thosini Bamunu Mudiyanselage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Senanayake_N/0/1/0/all/0/1"&gt;Nipuna Senanayake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ji_C/0/1/0/all/0/1"&gt;Chunyan Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1"&gt;Yi Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yanqing Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-supervised, Topology-Aware Segmentation of Tubular Structures from Live Imaging 3D Microscopy. (arXiv:2105.09737v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09737</id>
        <link href="http://arxiv.org/abs/2105.09737"/>
        <updated>2021-05-23T06:10:39.743Z</updated>
        <summary type="html"><![CDATA[Motivated by a challenging tubular network segmentation task, this paper
tackles two commonly encountered problems in biomedical imaging: Topological
consistency of the segmentation, and limited annotations. We propose a
topological score which measures both topological and geometric consistency
between the predicted and ground truth segmentations, applied for model
selection and validation. We apply our topological score in three scenarios: i.
a U-net ii. a U-net pretrained on an autoencoder, and iii. a semisupervised
U-net architecture, which offers a straightforward approach to jointly training
the network both as an autoencoder and a segmentation algorithm. This allows us
to utilize un-annotated data for training a representation that generalizes
across test data variability, in spite of our annotated training data having
very limited variation. Our contributions are validated on a challenging
segmentation task, locating tubular structures in the fetal pancreas from noisy
live imaging confocal microscopy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arnavaz_K/0/1/0/all/0/1"&gt;Kasra Arnavaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_O/0/1/0/all/0/1"&gt;Oswin Krause&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krivokapic_J/0/1/0/all/0/1"&gt;Jelena M. Krivokapic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heilmann_S/0/1/0/all/0/1"&gt;Silja Heilmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baerentzen_J/0/1/0/all/0/1"&gt;Jakob Andreas B&amp;#xe6;rentzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nyeng_P/0/1/0/all/0/1"&gt;Pia Nyeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feragen_A/0/1/0/all/0/1"&gt;Aasa Feragen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Quantized Model Parallelism for Graph-Augmented MLPs Based on Gradient-Free ADMM framework. (arXiv:2105.09837v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09837</id>
        <link href="http://arxiv.org/abs/2105.09837"/>
        <updated>2021-05-23T06:10:39.688Z</updated>
        <summary type="html"><![CDATA[The Graph Augmented Multi-layer Perceptron (GA-MLP) model is an attractive
alternative to Graph Neural Networks (GNNs). This is because it is resistant to
the over-smoothing problem, and deeper GA-MLP models yield better performance.
GA-MLP models are traditionally optimized by the Stochastic Gradient Descent
(SGD). However, SGD suffers from the layer dependency problem, which prevents
the gradients of different layers of GA-MLP models from being calculated in
parallel. In this paper, we propose a parallel deep learning Alternating
Direction Method of Multipliers (pdADMM) framework to achieve model
parallelism: parameters in each layer of GA-MLP models can be updated in
parallel. The extended pdADMM-Q algorithm reduces communication cost by
utilizing the quantization technique. Theoretical convergence to a critical
point of the pdADMM algorithm and the pdADMM-Q algorithm is provided with a
sublinear convergence rate $o(1/k)$. Extensive experiments in six benchmark
datasets demonstrate that the pdADMM can lead to high speedup, and outperforms
all the existing state-of-the-art comparison methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Junxiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1"&gt;Zheng Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yongchao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yue Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Liang Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Spatio-temporal Attention-based Model for Infant Movement Assessment from Videos. (arXiv:2105.09783v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09783</id>
        <link href="http://arxiv.org/abs/2105.09783"/>
        <updated>2021-05-23T06:10:39.663Z</updated>
        <summary type="html"><![CDATA[The absence or abnormality of fidgety movements of joints or limbs is
strongly indicative of cerebral palsy in infants. Developing computer-based
methods for assessing infant movements in videos is pivotal for improved
cerebral palsy screening. Most existing methods use appearance-based features
and are thus sensitive to strong but irrelevant signals caused by background
clutter or a moving camera. Moreover, these features are computed over the
whole frame, thus they measure gross whole body movements rather than specific
joint/limb motion.

Addressing these challenges, we develop and validate a new method for fidgety
movement assessment from consumer-grade videos using human poses extracted from
short clips. Human poses capture only relevant motion profiles of joints and
limbs and are thus free from irrelevant appearance artifacts. The dynamics and
coordination between joints are modeled using spatio-temporal graph
convolutional networks. Frames and body parts that contain discriminative
information about fidgety movements are selected through a spatio-temporal
attention mechanism. We validate the proposed model on the cerebral palsy
screening task using a real-life consumer-grade video dataset collected at an
Australian hospital through the Cerebral Palsy Alliance, Australia. Our
experiments show that the proposed method achieves the ROC-AUC score of 81.87%,
significantly outperforming existing competing methods with better
interpretability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_Thai_B/0/1/0/all/0/1"&gt;Binh Nguyen-Thai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1"&gt;Vuong Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morgan_C/0/1/0/all/0/1"&gt;Catherine Morgan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badawi_N/0/1/0/all/0/1"&gt;Nadia Badawi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1"&gt;Truyen Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1"&gt;Svetha Venkatesh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable Activity Recognition for Smart Home Systems. (arXiv:2105.09787v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09787</id>
        <link href="http://arxiv.org/abs/2105.09787"/>
        <updated>2021-05-23T06:10:39.642Z</updated>
        <summary type="html"><![CDATA[Smart home environments are designed to provide services that help improve
the quality of life for the occupant via a variety of sensors and actuators
installed throughout the space. Many automated actions taken by a smart home
are governed by the output of an underlying activity recognition system.
However, activity recognition systems may not be perfectly accurate and
therefore inconsistencies in smart home operations can lead a user to wonder
"why did the smart home do that?" In this work, we build on insights from
Explainable Artificial Intelligence (XAI) techniques to contribute
computational methods for explainable activity recognition. Specifically, we
generate explanations for smart home activity recognition systems that explain
what about an activity led to the given classification. To do so, we introduce
four computational techniques for generating natural language explanations of
smart home data and compare their effectiveness at generating meaningful
explanations. Through a study with everyday users, we evaluate user preferences
towards the four explanation types. Our results show that the leading approach,
SHAP, has a 92% success rate in generating accurate explanations. Moreover, 84%
of sampled scenarios users preferred natural language explanations over a
simple activity label, underscoring the need for explainable activity
recognition systems. Finally, we show that explanations generated by some XAI
methods can lead users to lose confidence in the accuracy of the underlying
activity recognition model, while others lead users to gain confidence. Taking
all studied factors into consideration, we make a recommendation regarding
which existing XAI method leads to the best performance in the domain of smart
home automation, and discuss a range of topics for future work in this area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1"&gt;Devleena Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nishimura_Y/0/1/0/all/0/1"&gt;Yasutaka Nishimura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vivek_R/0/1/0/all/0/1"&gt;Rajan P. Vivek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takeda_N/0/1/0/all/0/1"&gt;Naoto Takeda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fish_S/0/1/0/all/0/1"&gt;Sean T. Fish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ploetz_T/0/1/0/all/0/1"&gt;Thomas Ploetz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chernova_S/0/1/0/all/0/1"&gt;Sonia Chernova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed Adaptive Nearest Neighbor Classifier: Algorithm and Theory. (arXiv:2105.09788v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.09788</id>
        <link href="http://arxiv.org/abs/2105.09788"/>
        <updated>2021-05-23T06:10:39.622Z</updated>
        <summary type="html"><![CDATA[When data is of an extraordinarily large size or physically stored in
different locations, the distributed nearest neighbor (NN) classifier is an
attractive tool for classification. We propose a novel distributed adaptive NN
classifier for which the number of nearest neighbors is a tuning parameter
stochastically chosen by a data-driven criterion. An early stopping rule is
proposed when searching for the optimal tuning parameter, which not only speeds
up the computation but also improves the finite sample performance of the
proposed Algorithm. Convergence rate of excess risk of the distributed adaptive
NN classifier is investigated under various sub-sample size compositions. In
particular, we show that when the sub-sample sizes are sufficiently large, the
proposed classifier achieves the nearly optimal convergence rate. Effectiveness
of the proposed approach is demonstrated through simulation studies as well as
an empirical application to a real-world dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Liu_R/0/1/0/all/0/1"&gt;Ruiqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xu_G/0/1/0/all/0/1"&gt;Ganggang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shang_Z/0/1/0/all/0/1"&gt;Zuofeng Shang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simple Transparent Adversarial Examples. (arXiv:2105.09685v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09685</id>
        <link href="http://arxiv.org/abs/2105.09685"/>
        <updated>2021-05-23T06:10:39.616Z</updated>
        <summary type="html"><![CDATA[There has been a rise in the use of Machine Learning as a Service (MLaaS)
Vision APIs as they offer multiple services including pre-built models and
algorithms, which otherwise take a huge amount of resources if built from
scratch. As these APIs get deployed for high-stakes applications, it's very
important that they are robust to different manipulations. Recent works have
only focused on typical adversarial attacks when evaluating the robustness of
vision APIs. We propose two new aspects of adversarial image generation methods
and evaluate them on the robustness of Google Cloud Vision API's optical
character recognition service and object detection APIs deployed in real-world
settings such as sightengine.com, picpurify.com, Google Cloud Vision API, and
Microsoft Azure's Computer Vision API. Specifically, we go beyond the
conventional small-noise adversarial attacks and introduce secret embedding and
transparent adversarial examples as a simpler way to evaluate robustness. These
methods are so straightforward that even non-specialists can craft such
attacks. As a result, they pose a serious threat where APIs are used for
high-stakes applications. Our transparent adversarial examples successfully
evade state-of-the art object detections APIs such as Azure Cloud Vision
(attack success rate 52%) and Google Cloud Vision (attack success rate 36%).
90% of the images have a secret embedded text that successfully fools the
vision of time-limited humans but is detected by Google Cloud Vision API's
optical character recognition. Complementing to current research, our results
provide simple but unconventional methods on robustness evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Borkar_J/0/1/0/all/0/1"&gt;Jaydeep Borkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Pin-Yu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Stochastic Composite Augmented Lagrangian Method For Reinforcement Learning. (arXiv:2105.09716v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2105.09716</id>
        <link href="http://arxiv.org/abs/2105.09716"/>
        <updated>2021-05-23T06:10:39.608Z</updated>
        <summary type="html"><![CDATA[In this paper, we consider the linear programming (LP) formulation for deep
reinforcement learning. The number of the constraints depends on the size of
state and action spaces, which makes the problem intractable in large or
continuous environments. The general augmented Lagrangian method suffers the
double-sampling obstacle in solving the LP. Namely, the conditional
expectations originated from the constraint functions and the quadratic
penalties in the augmented Lagrangian function impose difficulties in sampling
and evaluation. Motivated from the updates of the multipliers, we overcome the
obstacles in minimizing the augmented Lagrangian function by replacing the
intractable conditional expectations with the multipliers. Therefore, a deep
parameterized augment Lagrangian method is proposed. Furthermore, the
replacement provides a promising breakthrough to integrate the two steps in the
augmented Lagrangian method into a single constrained problem. A general
theoretical analysis shows that the solutions generated from a sequence of the
constrained optimizations converge to the optimal solution of the LP if the
error is controlled properly. A theoretical analysis on the quadratic penalty
algorithm under neural tangent kernel setting shows the residual can be
arbitrarily small if the parameter in network and optimization algorithm is
chosen suitably. Preliminary experiments illustrate that our method is
competitive to other state-of-the-art algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yongfeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhao_M/0/1/0/all/0/1"&gt;Mingming Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weijie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wen_Z/0/1/0/all/0/1"&gt;Zaiwen Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Localization and Control of Magnetic Suture Needles in Cluttered Surgical Site with Blood and Tissue. (arXiv:2105.09481v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.09481</id>
        <link href="http://arxiv.org/abs/2105.09481"/>
        <updated>2021-05-23T06:10:39.583Z</updated>
        <summary type="html"><![CDATA[Real-time visual localization of needles is necessary for various surgical
applications, including surgical automation and visual feedback. In this study
we investigate localization and autonomous robotic control of needles in the
context of our magneto-suturing system. Our system holds the potential for
surgical manipulation with the benefit of minimal invasiveness and reduced
patient side effects. However, the non-linear magnetic fields produce
unintuitive forces and demand delicate position-based control that exceeds the
capabilities of direct human manipulation. This makes automatic needle
localization a necessity. Our localization method combines neural network-based
segmentation and classical techniques, and we are able to consistently locate
our needle with 0.73 mm RMS error in clean environments and 2.72 mm RMS error
in challenging environments with blood and occlusion. The average localization
RMS error is 2.16 mm for all environments we used in the experiments. We
combine this localization method with our closed-loop feedback control system
to demonstrate the further applicability of localization to autonomous control.
Our needle is able to follow a running suture path in (1) no blood, no tissue;
(2) heavy blood, no tissue; (3) no blood, with tissue; and (4) heavy blood,
with tissue environments. The tip position tracking error ranges from 2.6 mm to
3.7 mm RMS, opening the door towards autonomous suturing tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pryor_W/0/1/0/all/0/1"&gt;Will Pryor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barnoy_Y/0/1/0/all/0/1"&gt;Yotam Barnoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raval_S/0/1/0/all/0/1"&gt;Suraj Raval&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaolong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mair_L/0/1/0/all/0/1"&gt;Lamar Mair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lerner_D/0/1/0/all/0/1"&gt;Daniel Lerner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erin_O/0/1/0/all/0/1"&gt;Onder Erin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hager_G/0/1/0/all/0/1"&gt;Gregory D. Hager&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_Mercado_Y/0/1/0/all/0/1"&gt;Yancy Diaz-Mercado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krieger_A/0/1/0/all/0/1"&gt;Axel Krieger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manual Evaluation Matters: Reviewing Test Protocols of Distantly Supervised Relation Extraction. (arXiv:2105.09543v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09543</id>
        <link href="http://arxiv.org/abs/2105.09543"/>
        <updated>2021-05-23T06:10:39.576Z</updated>
        <summary type="html"><![CDATA[Distantly supervised (DS) relation extraction (RE) has attracted much
attention in the past few years as it can utilize large-scale auto-labeled
data. However, its evaluation has long been a problem: previous works either
took costly and inconsistent methods to manually examine a small sample of
model predictions, or directly test models on auto-labeled data -- which, by
our check, produce as much as 53% wrong labels at the entity pair level in the
popular NYT10 dataset. This problem has not only led to inaccurate evaluation,
but also made it hard to understand where we are and what's left to improve in
the research of DS-RE. To evaluate DS-RE models in a more credible way, we
build manually-annotated test sets for two DS-RE datasets, NYT10 and Wiki20,
and thoroughly evaluate several competitive models, especially the latest
pre-trained ones. The experimental results show that the manual evaluation can
indicate very different conclusions from automatic ones, especially some
unexpected observations, e.g., pre-trained models can achieve dominating
performance while being more susceptible to false-positives compared to
previous methods. We hope that both our manual test sets and novel observations
can help advance future DS-RE research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1"&gt;Tianyu Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_K/0/1/0/all/0/1"&gt;Keyue Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yuzhuo Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zhiyu Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yankai Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Neuronal Ensemble Inference with Generative Model and MCMC. (arXiv:2105.09679v1 [cond-mat.dis-nn])]]></title>
        <id>http://arxiv.org/abs/2105.09679</id>
        <link href="http://arxiv.org/abs/2105.09679"/>
        <updated>2021-05-23T06:10:39.539Z</updated>
        <summary type="html"><![CDATA[Neuronal ensemble inference is a significant problem in the study of
biological neural networks. Various methods have been proposed for ensemble
inference from experimental data of neuronal activity. Among them, Bayesian
inference approach with generative model was proposed recently. However, this
method requires large computational cost for appropriate inference. In this
work, we give an improved Bayesian inference algorithm by modifying update rule
in Markov chain Monte Carlo method and introducing the idea of simulated
annealing for hyperparameter control. We compare the performance of ensemble
inference between our algorithm and the original one, and discuss the advantage
of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Kimura_S/0/1/0/all/0/1"&gt;Shun Kimura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Ota_K/0/1/0/all/0/1"&gt;Keisuke Ota&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Takeda_K/0/1/0/all/0/1"&gt;Koujin Takeda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Parameterized Complexity of Polytree Learning. (arXiv:2105.09675v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2105.09675</id>
        <link href="http://arxiv.org/abs/2105.09675"/>
        <updated>2021-05-23T06:10:39.519Z</updated>
        <summary type="html"><![CDATA[A Bayesian network is a directed acyclic graph that represents statistical
dependencies between variables of a joint probability distribution. A
fundamental task in data science is to learn a Bayesian network from observed
data. \textsc{Polytree Learning} is the problem of learning an optimal Bayesian
network that fulfills the additional property that its underlying undirected
graph is a forest. In this work, we revisit the complexity of \textsc{Polytree
Learning}. We show that \textsc{Polytree Learning} can be solved in $3^n \cdot
|I|^{\mathcal{O}(1)}$ time where $n$ is the number of variables and $|I|$ is
the total instance size. Moreover, we consider the influence of the number of
variables $d$ that might receive a nonempty parent set in the final DAG on the
complexity of \textsc{Polytree Learning}. We show that \textsc{Polytree
Learning} has no $f(d)\cdot |I|^{\mathcal{O}(1)}$-time algorithm, unlike
Bayesian network learning which can be solved in $2^d \cdot
|I|^{\mathcal{O}(1)}$ time. We show that, in contrast, if $d$ and the maximum
parent set size are bounded, then we can obtain efficient algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gruttemeier_N/0/1/0/all/0/1"&gt;Niels Gr&amp;#xfc;ttemeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Komusiewicz_C/0/1/0/all/0/1"&gt;Christian Komusiewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morawietz_N/0/1/0/all/0/1"&gt;Nils Morawietz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble machine learning approach for screening of coronary heart disease based on echocardiography and risk factors. (arXiv:2105.09670v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.09670</id>
        <link href="http://arxiv.org/abs/2105.09670"/>
        <updated>2021-05-23T06:10:39.511Z</updated>
        <summary type="html"><![CDATA[Background: Extensive clinical evidence suggests that a preventive screening
of coronary heart disease (CHD) at an earlier stage can greatly reduce the
mortality rate. We use 64 two-dimensional speckle tracking echocardiography
(2D-STE) features and seven clinical features to predict whether one has CHD.
Methods: We develop a machine learning approach that integrates a number of
popular classification methods together by model stacking, and generalize the
traditional stacking method to a two-step stacking method to improve the
diagnostic performance. Results: By borrowing strengths from multiple
classification models through the proposed method, we improve the CHD
classification accuracy from around 70% to 87.7% on the testing set. The
sensitivity of the proposed method is 0.903 and the specificity is 0.843, with
an AUC of 0.904, which is significantly higher than those of the individual
classification models. Conclusions: Our work lays a foundation for the
deployment of speckle tracking echocardiography-based screening tools for
coronary heart disease.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Huolan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yongkai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chenguang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Huimin Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhong_W/0/1/0/all/0/1"&gt;Wenxuan Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TF-IDF vs Word Embeddings for Morbidity Identification in Clinical Notes: An Initial Study. (arXiv:2105.09632v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09632</id>
        <link href="http://arxiv.org/abs/2105.09632"/>
        <updated>2021-05-23T06:10:39.502Z</updated>
        <summary type="html"><![CDATA[Today, we are seeing an ever-increasing number of clinical notes that contain
clinical results, images, and textual descriptions of patient's health state.
All these data can be analyzed and employed to cater novel services that can
help people and domain experts with their common healthcare tasks. However,
many technologies such as Deep Learning and tools like Word Embeddings have
started to be investigated only recently, and many challenges remain open when
it comes to healthcare domain applications. To address these challenges, we
propose the use of Deep Learning and Word Embeddings for identifying sixteen
morbidity types within textual descriptions of clinical records. For this
purpose, we have used a Deep Learning model based on Bidirectional Long-Short
Term Memory (LSTM) layers which can exploit state-of-the-art vector
representations of data such as Word Embeddings. We have employed pre-trained
Word Embeddings namely GloVe and Word2Vec, and our own Word Embeddings trained
on the target domain. Furthermore, we have compared the performances of the
deep learning approaches against the traditional tf-idf using Support Vector
Machine and Multilayer perceptron (our baselines). From the obtained results it
seems that the latter outperforms the combination of Deep Learning approaches
using any word embeddings. Our preliminary results indicate that there are
specific features that make the dataset biased in favour of traditional machine
learning approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dessi_D/0/1/0/all/0/1"&gt;Danilo Dessi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Helaoui_R/0/1/0/all/0/1"&gt;Rim Helaoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vivek Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Recupero_D/0/1/0/all/0/1"&gt;Diego Reforgiato Recupero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riboni_D/0/1/0/all/0/1"&gt;Daniele Riboni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Exact Poly-Time Membership-Queries Algorithm for Extraction a three-Layer ReLU Network. (arXiv:2105.09673v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09673</id>
        <link href="http://arxiv.org/abs/2105.09673"/>
        <updated>2021-05-23T06:10:39.448Z</updated>
        <summary type="html"><![CDATA[As machine learning increasingly becomes more prevalent in our everyday life,
many organizations offer neural-networks based services as a black-box. The
reasons for hiding a learning model may vary: e.g., preventing copying of its
behavior or keeping back an adversarial from reverse-engineering its mechanism
and revealing sensitive information about its training data.

However, even as a black-box, some information can still be discovered by
specific queries. In this work, we show a polynomial-time algorithm that uses a
polynomial number of queries to mimic precisely the behavior of a three-layer
neural network that uses ReLU activation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Daniely_A/0/1/0/all/0/1"&gt;Amit Daniely&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Granot_E/0/1/0/all/0/1"&gt;Elad Granot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonlinear Hawkes Process with Gaussian Process Self Effects. (arXiv:2105.09618v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.09618</id>
        <link href="http://arxiv.org/abs/2105.09618"/>
        <updated>2021-05-23T06:10:39.438Z</updated>
        <summary type="html"><![CDATA[Traditionally, Hawkes processes are used to model time--continuous point
processes with history dependence. Here we propose an extended model where the
self--effects are of both excitatory and inhibitory type and follow a Gaussian
Process. Whereas previous work either relies on a less flexible
parameterization of the model, or requires a large amount of data, our
formulation allows for both a flexible model and learning when data are scarce.
We continue the line of work of Bayesian inference for Hawkes processes, and
our approach dispenses with the necessity of estimating a branching structure
for the posterior, as we perform inference on an aggregated sum of Gaussian
Processes. Efficient approximate Bayesian inference is achieved via data
augmentation, and we describe a mean--field variational inference approach to
learn the model parameters. To demonstrate the flexibility of the model we
apply our methodology on data from three different domains and compare it to
previously reported results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Malem_Shinitski_N/0/1/0/all/0/1"&gt;Noa Malem-Shinitski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ojeda_C/0/1/0/all/0/1"&gt;Cesar Ojeda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Opper_M/0/1/0/all/0/1"&gt;Manfred Opper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Navigation Turing Test (NTT): Learning to Evaluate Human-Like Navigation. (arXiv:2105.09637v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2105.09637</id>
        <link href="http://arxiv.org/abs/2105.09637"/>
        <updated>2021-05-23T06:10:39.418Z</updated>
        <summary type="html"><![CDATA[A key challenge on the path to developing agents that learn complex
human-like behavior is the need to quickly and accurately quantify
human-likeness. While human assessments of such behavior can be highly
accurate, speed and scalability are limited. We address these limitations
through a novel automated Navigation Turing Test (ANTT) that learns to predict
human judgments of human-likeness. We demonstrate the effectiveness of our
automated NTT on a navigation task in a complex 3D environment. We investigate
six classification models to shed light on the types of architectures best
suited to this task, and validate them against data collected through a human
NTT. Our best models achieve high accuracy when distinguishing true human and
agent behavior. At the same time, we show that predicting finer-grained human
assessment of agents' progress towards human-like behavior remains unsolved.
Our work takes an important step towards agents that more effectively learn
complex human-like behavior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Devlin_S/0/1/0/all/0/1"&gt;Sam Devlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Georgescu_R/0/1/0/all/0/1"&gt;Raluca Georgescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Momennejad_I/0/1/0/all/0/1"&gt;Ida Momennejad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rzepecki_J/0/1/0/all/0/1"&gt;Jaroslaw Rzepecki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuniga_E/0/1/0/all/0/1"&gt;Evelyn Zuniga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Costello_G/0/1/0/all/0/1"&gt;Gavin Costello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leroy_G/0/1/0/all/0/1"&gt;Guy Leroy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaw_A/0/1/0/all/0/1"&gt;Ali Shaw&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1"&gt;Katja Hofmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[See, Hear, Read: Leveraging Multimodality with Guided Attention for Abstractive Text Summarization. (arXiv:2105.09601v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09601</id>
        <link href="http://arxiv.org/abs/2105.09601"/>
        <updated>2021-05-23T06:10:39.393Z</updated>
        <summary type="html"><![CDATA[In recent years, abstractive text summarization with multimodal inputs has
started drawing attention due to its ability to accumulate information from
different source modalities and generate a fluent textual summary. However,
existing methods use short videos as the visual modality and short summary as
the ground-truth, therefore, perform poorly on lengthy videos and long
ground-truth summary. Additionally, there exists no benchmark dataset to
generalize this task on videos of varying lengths. In this paper, we introduce
AVIATE, the first large-scale dataset for abstractive text summarization with
videos of diverse duration, compiled from presentations in well-known academic
conferences like NDSS, ICML, NeurIPS, etc. We use the abstract of corresponding
research papers as the reference summaries, which ensure adequate quality and
uniformity of the ground-truth. We then propose {\name}, a factorized
multi-modal Transformer based decoder-only language model, which inherently
captures the intra-modal and inter-modal dynamics within various input
modalities for the text summarization task. {\name} utilizes an increasing
number of self-attentions to capture multimodality and performs significantly
better than traditional encoder-decoder based networks. Extensive experiments
illustrate that {\name} achieves significant improvement over the baselines in
both qualitative and quantitative evaluations on the existing How2 dataset for
short videos and newly introduced AVIATE dataset for videos with diverse
duration, beating the best baseline on the two datasets by $1.39$ and $2.74$
ROUGE-L points respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Atri_Y/0/1/0/all/0/1"&gt;Yash Kumar Atri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pramanick_S/0/1/0/all/0/1"&gt;Shraman Pramanick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_V/0/1/0/all/0/1"&gt;Vikram Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1"&gt;Tanmoy Chakraborty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distribution Agnostic Symbolic Representations for Time Series Dimensionality Reduction and Online Anomaly Detection. (arXiv:2105.09592v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.09592</id>
        <link href="http://arxiv.org/abs/2105.09592"/>
        <updated>2021-05-23T06:10:39.382Z</updated>
        <summary type="html"><![CDATA[Due to the importance of the lower bounding distances and the attractiveness
of symbolic representations, the family of symbolic aggregate approximations
(SAX) has been used extensively for encoding time series data. However, typical
SAX-based methods rely on two restrictive assumptions; the Gaussian
distribution and equiprobable symbols. This paper proposes two novel
data-driven SAX-based symbolic representations, distinguished by their
discretization steps. The first representation, oriented for general data
compaction and indexing scenarios, is based on the combination of kernel
density estimation and Lloyd-Max quantization to minimize the information loss
and mean squared error in the discretization step. The second method, oriented
for high-level mining tasks, employs the Mean-Shift clustering method and is
shown to enhance anomaly detection in the lower-dimensional space. Besides, we
verify on a theoretical basis a previously observed phenomenon of the intrinsic
process that results in a lower than the expected variance of the intermediate
piecewise aggregate approximation. This phenomenon causes an additional
information loss but can be avoided with a simple modification. The proposed
representations possess all the attractive properties of the conventional SAX
method. Furthermore, experimental evaluation on real-world datasets
demonstrates their superiority compared to the traditional SAX and an
alternative data-driven SAX variant.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bountrogiannis_K/0/1/0/all/0/1"&gt;Konstantinos Bountrogiannis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tzagkarakis_G/0/1/0/all/0/1"&gt;George Tzagkarakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsakalides_P/0/1/0/all/0/1"&gt;Panagiotis Tsakalides&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fed-EINI: An Efficient and Interpretable Inference Framework for Decision Tree Ensembles in Federated Learning. (arXiv:2105.09540v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09540</id>
        <link href="http://arxiv.org/abs/2105.09540"/>
        <updated>2021-05-23T06:10:39.363Z</updated>
        <summary type="html"><![CDATA[The increasing concerns about data privacy and security drives the emergence
of a new field of studying privacy-preserving machine learning from isolated
data sources, i.e., \textit{federated learning}. Vertical federated learning,
where different parties hold different features for common users, has a great
potential of driving a more variety of business cooperation among enterprises
in different fields. Decision tree models especially decision tree ensembles
are a class of widely applied powerful machine learning models with high
interpretability and modeling efficiency. However, the interpretability are
compromised in these works such as SecureBoost since the feature names are not
exposed to avoid possible data breaches due to the unprotected decision path.
In this paper, we shall propose Fed-EINI, an efficient and interpretable
inference framework for federated decision tree models with only one round of
multi-party communication. We shall compute the candidate sets of leaf nodes
based on the local data at each party in parallel, followed by securely
computing the weight of the only leaf node in the intersection of the candidate
sets. We propose to protect the decision path by the efficient additively
homomorphic encryption method, which allows the disclosure of feature names and
thus makes the federated decision trees interpretable. The advantages of
Fed-EINI will be demonstrated through theoretical analysis and extensive
numerical results. Experiments show that the inference efficiency is improved
by over $50\%$ in average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaolin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shuai Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kai Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1"&gt;Hao Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zejin Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yongji Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Negational Symmetry of Quantum Neural Networks for Binary Pattern Classification. (arXiv:2105.09580v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09580</id>
        <link href="http://arxiv.org/abs/2105.09580"/>
        <updated>2021-05-23T06:10:39.355Z</updated>
        <summary type="html"><![CDATA[Entanglement is a physical phenomenon, which has fueled recent successes of
quantum algorithms. Although quantum neural networks (QNNs) have shown
promising results in solving simple machine learning tasks recently, for the
time being, the effect of entanglement in QNNs and the behavior of QNNs in
binary pattern classification are still underexplored. In this work, we provide
some theoretical insight into the properties of QNNs by presenting and
analyzing a new form of invariance embedded in QNNs for both quantum binary
classification and quantum representation learning, which we term negational
symmetry. Given a quantum binary signal and its negational counterpart where a
bitwise NOT operation is applied to each quantum bit of the binary signal, a
QNN outputs the same logits. That is to say, QNNs cannot differentiate a
quantum binary signal and its negational counterpart in a binary classification
task. We further empirically evaluate the negational symmetry of QNNs in binary
pattern classification tasks using Google's quantum computing framework. The
theoretical and experimental results suggest that negational symmetry is a
fundamental property of QNNs, which is not shared by classical models. Our
findings also imply that negational symmetry is a double-edged sword in
practical quantum applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1"&gt;Nanqing Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kampffmeyer_M/0/1/0/all/0/1"&gt;Michael Kampffmeyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Voiculescu_I/0/1/0/all/0/1"&gt;Irina Voiculescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1"&gt;Eric Xing&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Logarithmic landscape and power-law escape rate of SGD. (arXiv:2105.09557v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09557</id>
        <link href="http://arxiv.org/abs/2105.09557"/>
        <updated>2021-05-23T06:10:39.348Z</updated>
        <summary type="html"><![CDATA[Stochastic gradient descent (SGD) undergoes complicated multiplicative noise
for the mean-square loss. We use this property of the SGD noise to derive a
stochastic differential equation (SDE) with simpler additive noise by
performing a non-uniform transformation of the time variable. In the SDE, the
gradient of the loss is replaced by that of the logarithmized loss.
Consequently, we show that, near a local or global minimum, the stationary
distribution $P_\mathrm{ss}(\theta)$ of the network parameters $\theta$ follows
a power-law with respect to the loss function $L(\theta)$, i.e.
$P_\mathrm{ss}(\theta)\propto L(\theta)^{-\phi}$ with the exponent $\phi$
specified by the mini-batch size, the learning rate, and the Hessian at the
minimum. We obtain the escape rate formula from a local minimum, which is
determined not by the loss barrier height $\Delta L=L(\theta^s)-L(\theta^*)$
between a minimum $\theta^*$ and a saddle $\theta^s$ but by the logarithmized
loss barrier height $\Delta\log L=\log[L(\theta^s)/L(\theta^*)]$. Our
escape-rate formula explains an empirical fact that SGD prefers flat minima
with low effective dimensions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mori_T/0/1/0/all/0/1"&gt;Takashi Mori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziyin_L/0/1/0/all/0/1"&gt;Liu Ziyin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kangqiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ueda_M/0/1/0/all/0/1"&gt;Masahito Ueda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Kronecker neural networks: A general framework for neural networks with adaptive activation functions. (arXiv:2105.09513v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09513</id>
        <link href="http://arxiv.org/abs/2105.09513"/>
        <updated>2021-05-23T06:10:39.334Z</updated>
        <summary type="html"><![CDATA[We propose a new type of neural networks, Kronecker neural networks (KNNs),
that form a general framework for neural networks with adaptive activation
functions. KNNs employ the Kronecker product, which provides an efficient way
of constructing a very wide network while keeping the number of parameters low.
Our theoretical analysis reveals that under suitable conditions, KNNs induce a
faster decay of the loss than that by the feed-forward networks. This is also
empirically verified through a set of computational examples. Furthermore,
under certain technical assumptions, we establish global convergence of
gradient descent for KNNs. As a specific case, we propose the Rowdy activation
function that is designed to get rid of any saturation region by injecting
sinusoidal fluctuations, which include trainable parameters. The proposed Rowdy
activation function can be employed in any neural network architecture like
feed-forward neural networks, Recurrent neural networks, Convolutional neural
networks etc. The effectiveness of KNNs with Rowdy activation is demonstrated
through various computational experiments including function approximation
using feed-forward neural networks, solution inference of partial differential
equations using the physics-informed neural networks, and standard deep
learning benchmark problems using convolutional and fully-connected neural
networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jagtap_A/0/1/0/all/0/1"&gt;Ameya D. Jagtap&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1"&gt;Yeonjong Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1"&gt;Kenji Kawaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1"&gt;George Em Karniadakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the $\alpha$-lazy version of Markov chains in estimation and testing problems. (arXiv:2105.09536v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2105.09536</id>
        <link href="http://arxiv.org/abs/2105.09536"/>
        <updated>2021-05-23T06:10:39.325Z</updated>
        <summary type="html"><![CDATA[We formulate extendibility of the minimax one-trajectory length of several
statistical Markov chains inference problems and give sufficient conditions for
both the possibility and impossibility of such extensions. We follow up and
apply this framework to recently published results on learning and identity
testing of ergodic Markov chains. In particular, we show that for some of the
aforementioned results, we can omit the aperiodicity requirement by simulating
an $\alpha$-lazy version of the original process, and quantify the incurred
cost of removing this assumption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Fried_S/0/1/0/all/0/1"&gt;Sela Fried&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wolfer_G/0/1/0/all/0/1"&gt;Geoffrey Wolfer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aggregate Learning for Mixed Frequency Data. (arXiv:2105.09579v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09579</id>
        <link href="http://arxiv.org/abs/2105.09579"/>
        <updated>2021-05-23T06:10:39.318Z</updated>
        <summary type="html"><![CDATA[Large and acute economic shocks such as the 2007-2009 financial crisis and
the current COVID-19 infections rapidly change the economic environment. In
such a situation, the importance of real-time economic analysis using
alternative datais emerging. Alternative data such as search query and location
data are closer to real-time and richer than official statistics that are
typically released once a month in an aggregated form. We take advantage of
spatio-temporal granularity of alternative data and propose a
mixed-FrequencyAggregate Learning (MF-AGL)model that predicts economic
indicators for the smaller areas in real-time. We apply the model for the
real-world problem; prediction of the number of job applicants which is closely
related to the unemployment rates. We find that the proposed model predicts (i)
the regional heterogeneity of the labor market condition and (ii) the rapidly
changing economic status. The model can be applied to various tasks, especially
economic analysis]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1"&gt;Takamichi Toda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moriwaki_D/0/1/0/all/0/1"&gt;Daisuke Moriwaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ota_K/0/1/0/all/0/1"&gt;Kazuhiro Ota&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-informed neural networks (PINNs) for fluid mechanics: A review. (arXiv:2105.09506v1 [physics.flu-dyn])]]></title>
        <id>http://arxiv.org/abs/2105.09506</id>
        <link href="http://arxiv.org/abs/2105.09506"/>
        <updated>2021-05-23T06:10:39.309Z</updated>
        <summary type="html"><![CDATA[Despite the significant progress over the last 50 years in simulating flow
problems using numerical discretization of the Navier-Stokes equations (NSE),
we still cannot incorporate seamlessly noisy data into existing algorithms,
mesh-generation is complex, and we cannot tackle high-dimensional problems
governed by parametrized NSE. Moreover, solving inverse flow problems is often
prohibitively expensive and requires complex and expensive formulations and new
computer codes. Here, we review flow physics-informed learning, integrating
seamlessly data and mathematical models, and implementing them using
physics-informed neural networks (PINNs). We demonstrate the effectiveness of
PINNs for inverse problems related to three-dimensional wake flows, supersonic
flows, and biomedical flows.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Cai_S/0/1/0/all/0/1"&gt;Shengze Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Mao_Z/0/1/0/all/0/1"&gt;Zhiping Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhicheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Yin_M/0/1/0/all/0/1"&gt;Minglang Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Karniadakis_G/0/1/0/all/0/1"&gt;George Em Karniadakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Preference Random Walk Algorithm for Link Prediction through Mutual Influence Nodes in Complex Networks. (arXiv:2105.09494v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2105.09494</id>
        <link href="http://arxiv.org/abs/2105.09494"/>
        <updated>2021-05-23T06:10:39.302Z</updated>
        <summary type="html"><![CDATA[Predicting links in complex networks has been one of the essential topics
within the realm of data mining and science discovery over the past few years.
This problem remains an attempt to identify future, deleted, and redundant
links using the existing links in a graph. Local random walk is considered to
be one of the most well-known algorithms in the category of quasi-local
methods. It traverses the network using the traditional random walk with a
limited number of steps, randomly selecting one adjacent node in each step
among the nodes which have equal importance. Then this method uses the
transition probability between node pairs to calculate the similarity between
them. However, in most datasets, this method is not able to perform accurately
in scoring remarkably similar nodes. In the present article, an efficient
method is proposed for improving local random walk by encouraging random walk
to move, in every step, towards the node which has a stronger influence.
Therefore, the next node is selected according to the influence of the source
node. To do so, using mutual information, the concept of the asymmetric mutual
influence of nodes is presented. A comparison between the proposed method and
other similarity-based methods (local, quasi-local, and global) has been
performed, and results have been reported for 11 real-world networks. It had a
higher prediction accuracy compared with other link prediction approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Berahmand_K/0/1/0/all/0/1"&gt;Kamal Berahmand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasiri_E/0/1/0/all/0/1"&gt;Elahe Nasiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forouzandeh_S/0/1/0/all/0/1"&gt;Saman Forouzandeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuefeng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning for Many-to-many Multilingual Neural Machine Translation. (arXiv:2105.09501v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09501</id>
        <link href="http://arxiv.org/abs/2105.09501"/>
        <updated>2021-05-23T06:10:39.262Z</updated>
        <summary type="html"><![CDATA[Existing multilingual machine translation approaches mainly focus on
English-centric directions, while the non-English directions still lag behind.
In this work, we aim to build a many-to-many translation system with an
emphasis on the quality of non-English language directions. Our intuition is
based on the hypothesis that a universal cross-language representation leads to
better multilingual translation performance. To this end, we propose \method, a
training method to obtain a single unified multilingual translation model.
mCOLT is empowered by two techniques: (i) a contrastive learning scheme to
close the gap among representations of different languages, and (ii) data
augmentation on both multiple parallel and monolingual data to further align
token representations. For English-centric directions, mCOLT achieves
competitive or even better performance than a strong pre-trained model mBART on
tens of WMT benchmarks. For non-English directions, mCOLT achieves an
improvement of average 10+ BLEU compared with the multilingual baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xiao Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mingxuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Liwei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decomposing reverse-mode automatic differentiation. (arXiv:2105.09469v1 [cs.PL])]]></title>
        <id>http://arxiv.org/abs/2105.09469</id>
        <link href="http://arxiv.org/abs/2105.09469"/>
        <updated>2021-05-23T06:10:39.240Z</updated>
        <summary type="html"><![CDATA[We decompose reverse-mode automatic differentiation into (forward-mode)
linearization followed by transposition. Doing so isolates the essential
difference between forward- and reverse-mode AD, and simplifies their joint
implementation. In particular, once forward-mode AD rules are defined for every
primitive operation in a source language, only linear primitives require an
additional transposition rule in order to arrive at a complete reverse-mode AD
implementation. This is how reverse-mode AD is written in JAX and Dex.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Frostig_R/0/1/0/all/0/1"&gt;Roy Frostig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1"&gt;Matthew J. Johnson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maclaurin_D/0/1/0/all/0/1"&gt;Dougal Maclaurin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paszke_A/0/1/0/all/0/1"&gt;Adam Paszke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radul_A/0/1/0/all/0/1"&gt;Alexey Radul&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning for solution and inversion of structural mechanics and vibrations. (arXiv:2105.09477v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09477</id>
        <link href="http://arxiv.org/abs/2105.09477"/>
        <updated>2021-05-23T06:10:39.180Z</updated>
        <summary type="html"><![CDATA[Deep learning has been the most popular machine learning method in the last
few years. In this chapter, we present the application of deep learning and
physics-informed neural networks concerning structural mechanics and vibration
problems. Demonstration problems involve de-noising data, solution to
time-dependent ordinary and partial differential equations, and characterizing
the system's response for a given data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haghighat_E/0/1/0/all/0/1"&gt;Ehsan Haghighat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bekar_A/0/1/0/all/0/1"&gt;Ali Can Bekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madenci_E/0/1/0/all/0/1"&gt;Erdogan Madenci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juanes_R/0/1/0/all/0/1"&gt;Ruben Juanes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI-Decision Support System Interface Using Cancer Related Data for Lung Cancer Prognosis. (arXiv:2105.09471v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09471</id>
        <link href="http://arxiv.org/abs/2105.09471"/>
        <updated>2021-05-23T06:10:39.153Z</updated>
        <summary type="html"><![CDATA[Until the beginning of 2021, lung cancer is known to be the most common
cancer in the world. The disease is common due to factors such as occupational
exposure, smoking and environmental pollution. The early diagnosis and
treatment of the disease is of great importance as well as the prevention of
the causes that cause the disease. The study was planned to create a web
interface that works with machine learning algorithms to predict prognosis
using lung cancer clinical and gene expression in the GDC data portal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leblebici_A/0/1/0/all/0/1"&gt;Asim Leblebici&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gesoglu_O/0/1/0/all/0/1"&gt;Omer Gesoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basbinar_Y/0/1/0/all/0/1"&gt;Yasemin Basbinar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying sources of uncertainty in drug discovery predictions with probabilistic models. (arXiv:2105.09474v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09474</id>
        <link href="http://arxiv.org/abs/2105.09474"/>
        <updated>2021-05-23T06:10:39.144Z</updated>
        <summary type="html"><![CDATA[Knowing the uncertainty in a prediction is critical when making expensive
investment decisions and when patient safety is paramount, but machine learning
(ML) models in drug discovery typically provide only a single best estimate and
ignore all sources of uncertainty. Predictions from these models may therefore
be over-confident, which can put patients at risk and waste resources when
compounds that are destined to fail are further developed. Probabilistic
predictive models (PPMs) can incorporate uncertainty in both the data and
model, and return a distribution of predicted values that represents the
uncertainty in the prediction. PPMs not only let users know when predictions
are uncertain, but the intuitive output from these models makes communicating
risk easier and decision making better. Many popular machine learning methods
have a PPM or Bayesian analogue, making PPMs easy to fit into current
workflows. We use toxicity prediction as a running example, but the same
principles apply for all prediction models used in drug discovery. The
consequences of ignoring uncertainty and how PPMs account for uncertainty are
also described. We aim to make the discussion accessible to a broad
non-mathematical audience. Equations are provided to make ideas concrete for
mathematical readers (but can be skipped without loss of understanding) and
code is available for computational researchers
(https://github.com/stanlazic/ML_uncertainty_quantification).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lazic_S/0/1/0/all/0/1"&gt;Stanley E. Lazic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williams_D/0/1/0/all/0/1"&gt;Dominic P. Williams&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Physics-Constrained Deep Learning Model for Simulating Multiphase Flow in 3D Heterogeneous Porous Media. (arXiv:2105.09467v1 [physics.geo-ph])]]></title>
        <id>http://arxiv.org/abs/2105.09467</id>
        <link href="http://arxiv.org/abs/2105.09467"/>
        <updated>2021-05-23T06:10:39.136Z</updated>
        <summary type="html"><![CDATA[In this work, an efficient physics-constrained deep learning model is
developed for solving multiphase flow in 3D heterogeneous porous media. The
model fully leverages the spatial topology predictive capability of
convolutional neural networks, and is coupled with an efficient
continuity-based smoother to predict flow responses that need spatial
continuity. Furthermore, the transient regions are penalized to steer the
training process such that the model can accurately capture flow in these
regions. The model takes inputs including properties of porous media, fluid
properties and well controls, and predicts the temporal-spatial evolution of
the state variables (pressure and saturation). While maintaining the continuity
of fluid flow, the 3D spatial domain is decomposed into 2D images for reducing
training cost, and the decomposition results in an increased number of training
data samples and better training efficiency. Additionally, a surrogate model is
separately constructed as a postprocessor to calculate well flow rate based on
the predictions of state variables from the deep learning model. We use the
example of CO2 injection into saline aquifers, and apply the
physics-constrained deep learning model that is trained from physics-based
simulation data and emulates the physics process. The model performs prediction
with a speedup of ~1400 times compared to physics-based simulations, and the
average temporal errors of predicted pressure and saturation plumes are 0.27%
and 0.099% respectively. Furthermore, water production rate is efficiently
predicted by a surrogate model for well flow rate, with a mean error less than
5%. Therefore, with its unique scheme to cope with the fidelity in fluid flow
in porous media, the physics-constrained deep learning model can become an
efficient predictive model for computationally demanding inverse problems or
other coupled processes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Yan_B/0/1/0/all/0/1"&gt;Bicheng Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Harp_D/0/1/0/all/0/1"&gt;Dylan Robert Harp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bailian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Pawar_R/0/1/0/all/0/1"&gt;Rajesh Pawar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Learning-Accelerated Data Assimilation and Forecasting Workflow for Commercial-Scale Geologic Carbon Storage. (arXiv:2105.09468v1 [physics.geo-ph])]]></title>
        <id>http://arxiv.org/abs/2105.09468</id>
        <link href="http://arxiv.org/abs/2105.09468"/>
        <updated>2021-05-23T06:10:39.125Z</updated>
        <summary type="html"><![CDATA[Fast assimilation of monitoring data to update forecasts of pressure buildup
and carbon dioxide (CO2) plume migration under geologic uncertainties is a
challenging problem in geologic carbon storage. The high computational cost of
data assimilation with a high-dimensional parameter space impedes fast
decision-making for commercial-scale reservoir management. We propose to
leverage physical understandings of porous medium flow behavior with deep
learning techniques to develop a fast history matching-reservoir response
forecasting workflow. Applying an Ensemble Smoother Multiple Data Assimilation
framework, the workflow updates geologic properties and predicts reservoir
performance with quantified uncertainty from pressure history and CO2 plumes
interpreted through seismic inversion. As the most computationally expensive
component in such a workflow is reservoir simulation, we developed surrogate
models to predict dynamic pressure and CO2 plume extents under multi-well
injection. The surrogate models employ deep convolutional neural networks,
specifically, a wide residual network and a residual U-Net. The workflow is
validated against a flat three-dimensional reservoir model representative of a
clastic shelf depositional environment. Intelligent treatments are applied to
bridge between quantities in a true-3D reservoir model and those in a
single-layer reservoir model. The workflow can complete history matching and
reservoir forecasting with uncertainty quantification in less than one hour on
a mainstream personal workstation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Tang_H/0/1/0/all/0/1"&gt;Hewei Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Fu_P/0/1/0/all/0/1"&gt;Pengcheng Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Sherman_C/0/1/0/all/0/1"&gt;Christopher S. Sherman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jize Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ju_X/0/1/0/all/0/1"&gt;Xin Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Hamon_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Hamon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Azzolina_N/0/1/0/all/0/1"&gt;Nicholas A. Azzolina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Burton_Kelly_M/0/1/0/all/0/1"&gt;Matthew Burton-Kelly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Morris_J/0/1/0/all/0/1"&gt;Joseph P. Morris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An IoT-Based Framework for Remote Fall Monitoring. (arXiv:2105.09461v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2105.09461</id>
        <link href="http://arxiv.org/abs/2105.09461"/>
        <updated>2021-05-23T06:10:39.102Z</updated>
        <summary type="html"><![CDATA[Fall detection is a serious healthcare issue that needs to be solved. Falling
without quick medical intervention would lower the chances of survival for the
elderly, especially if living alone. Hence, the need is there for developing
fall detection algorithms with high accuracy. This paper presents a novel
IoT-based system for fall detection that includes a sensing device transmitting
data to a mobile application through a cloud-connected gateway device. Then,
the focus is shifted to the algorithmic aspect where multiple features are
extracted from 3-axis accelerometer data taken from existing datasets. The
results emphasize on the significance of Continuous Wavelet Transform (CWT) as
an influential feature for determining falls. CWT, Signal Energy (SE), Signal
Magnitude Area (SMA), and Signal Vector Magnitude (SVM) features have shown
promising classification results using K-Nearest Neighbors (KNN) and E-Nearest
Neighbors (ENN). For all performance metrics (accuracy, recall, precision,
specificity, and F1 Score), the achieved results are higher than 95% for a
dataset of small size, while more than 98.47% score is achieved in the
aforementioned criteria over the UniMiB-SHAR dataset by the same algorithms,
where the classification time for a single test record is extremely efficient
and is real-time]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Al_Kababji_A/0/1/0/all/0/1"&gt;Ayman Al-Kababji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amira_A/0/1/0/all/0/1"&gt;Abbes Amira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bensaali_F/0/1/0/all/0/1"&gt;Faycal Bensaali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jarouf_A/0/1/0/all/0/1"&gt;Abdulah Jarouf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shidqi_L/0/1/0/all/0/1"&gt;Lisan Shidqi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Djelouat_H/0/1/0/all/0/1"&gt;Hamza Djelouat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepCAD: A Deep Generative Network for Computer-Aided Design Models. (arXiv:2105.09492v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09492</id>
        <link href="http://arxiv.org/abs/2105.09492"/>
        <updated>2021-05-23T06:10:39.091Z</updated>
        <summary type="html"><![CDATA[Deep generative models of 3D shapes have received a great deal of research
interest. Yet, almost all of them generate discrete shape representations, such
as voxels, point clouds, and polygon meshes. We present the first 3D generative
model for a drastically different shape representation -- describing a shape as
a sequence of computer-aided design (CAD) operations. Unlike meshes and point
clouds, CAD models encode the user creation process of 3D shapes, widely used
in numerous industrial and engineering design tasks. However, the sequential
and irregular structure of CAD operations poses significant challenges for
existing 3D generative models. Drawing an analogy between CAD operations and
natural language, we propose a CAD generative network based on the Transformer.
We demonstrate the performance of our model for both shape autoencoding and
random shape generation. To train our network, we create a new CAD dataset
consisting of 179,133 models and their CAD construction sequences. We have made
this dataset publicly available to promote future research on this topic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1"&gt;Rundi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Chang Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Changxi Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Superpixel-based Domain-Knowledge Infusion in Computer Vision. (arXiv:2105.09448v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09448</id>
        <link href="http://arxiv.org/abs/2105.09448"/>
        <updated>2021-05-23T06:10:39.076Z</updated>
        <summary type="html"><![CDATA[Superpixels are higher-order perceptual groups of pixels in an image, often
carrying much more information than raw pixels. There is an inherent relational
structure to the relationship among different superpixels of an image. This
relational information can convey some form of domain information about the
image, e.g. relationship between superpixels representing two eyes in a cat
image. Our interest in this paper is to construct computer vision models,
specifically those based on Deep Neural Networks (DNNs) to incorporate these
superpixels information. We propose a methodology to construct a hybrid model
that leverages (a) Convolutional Neural Network (CNN) to deal with spatial
information in an image, and (b) Graph Neural Network (GNN) to deal with
relational superpixel information in the image. The proposed deep model is
learned using a generic hybrid loss function that we call a `hybrid' loss. We
evaluate the predictive performance of our proposed hybrid vision model on four
popular image classification datasets: MNIST, FMNIST, CIFAR-10 and CIFAR-100.
Moreover, we evaluate our method on three real-world classification tasks:
COVID-19 X-Ray Detection, LFW Face Recognition, and SOCOFing Fingerprint
Identification. The results demonstrate that the relational superpixel
information provided via a GNN could improve the performance of standard
CNN-based vision systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1"&gt;Gunjan Chhablani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Abheesht Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1"&gt;Harshit Pandey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dash_T/0/1/0/all/0/1"&gt;Tirtharaj Dash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimum-Delay Adaptation in Non-Stationary Reinforcement Learning via Online High-Confidence Change-Point Detection. (arXiv:2105.09452v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09452</id>
        <link href="http://arxiv.org/abs/2105.09452"/>
        <updated>2021-05-23T06:10:39.068Z</updated>
        <summary type="html"><![CDATA[Non-stationary environments are challenging for reinforcement learning
algorithms. If the state transition and/or reward functions change based on
latent factors, the agent is effectively tasked with optimizing a behavior that
maximizes performance over a possibly infinite random sequence of Markov
Decision Processes (MDPs), each of which drawn from some unknown distribution.
We call each such MDP a context. Most related works make strong assumptions
such as knowledge about the distribution over contexts, the existence of
pre-training phases, or a priori knowledge about the number, sequence, or
boundaries between contexts. We introduce an algorithm that efficiently learns
policies in non-stationary environments. It analyzes a possibly infinite stream
of data and computes, in real-time, high-confidence change-point detection
statistics that reflect whether novel, specialized policies need to be created
and deployed to tackle novel contexts, or whether previously-optimized ones
might be reused. We show that (i) this algorithm minimizes the delay until
unforeseen changes to a context are detected, thereby allowing for rapid
responses; and (ii) it bounds the rate of false alarm, which is important in
order to minimize regret. Our method constructs a mixture model composed of a
(possibly infinite) ensemble of probabilistic dynamics predictors that model
the different modes of the distribution over underlying latent MDPs. We
evaluate our algorithm on high-dimensional continuous reinforcement learning
problems and show that it outperforms state-of-the-art (model-free and
model-based) RL algorithms, as well as state-of-the-art meta-learning methods
specially designed to deal with non-stationarity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alegre_L/0/1/0/all/0/1"&gt;Lucas N. Alegre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bazzan_A/0/1/0/all/0/1"&gt;Ana L. C. Bazzan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_B/0/1/0/all/0/1"&gt;Bruno C. da Silva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Sanitation with Application to Node Classification. (arXiv:2105.09384v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09384</id>
        <link href="http://arxiv.org/abs/2105.09384"/>
        <updated>2021-05-23T06:10:38.908Z</updated>
        <summary type="html"><![CDATA[The past decades have witnessed the prosperity of graph mining, with a
multitude of sophisticated models and algorithms designed for various mining
tasks, such as ranking, classification, clustering and anomaly detection.
Generally speaking, the vast majority of the existing works aim to answer the
following question, that is, given a graph, what is the best way to mine it? In
this paper, we introduce the graph sanitation problem, to answer an orthogonal
question. That is, given a mining task and an initial graph, what is the best
way to improve the initially provided graph? By learning a better graph as part
of the input of the mining model, it is expected to benefit graph mining in a
variety of settings, ranging from denoising, imputation to defense. We
formulate the graph sanitation problem as a bilevel optimization problem, and
further instantiate it by semi-supervised node classification, together with an
effective solver named GaSoliNe. Extensive experimental results demonstrate
that the proposed method is (1) broadly applicable with respect to different
graph neural network models and flexible graph modification strategies, (2)
effective in improving the node classification accuracy on both the original
and contaminated graphs in various perturbation scenarios. In particular, it
brings up to 25% performance improvement over the existing robust graph neural
network methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhe Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1"&gt;Hanghang Tong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring The Limits Of Data Augmentation For Retinal Vessel Segmentation. (arXiv:2105.09365v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.09365</id>
        <link href="http://arxiv.org/abs/2105.09365"/>
        <updated>2021-05-23T06:10:38.879Z</updated>
        <summary type="html"><![CDATA[Retinal Vessel Segmentation is important for diagnosis of various diseases.
The research on retinal vessel segmentation focuses mainly on improvement of
the segmentation model which is usually based on U-Net architecture. In our
study we use the U-Net architecture and we rely on heavy data augmentation in
order to achieve better performance. The success of the data augmentation
relies on successfully addressing the problem of input images. By analyzing
input images and performing the augmentation accordingly we show that the
performance of the U-Net model can be increased dramatically. Results are
reported using the most widely used retina dataset, DRIVE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Uysal_E/0/1/0/all/0/1"&gt;Enes Sadi Uysal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bilici_M/0/1/0/all/0/1"&gt;M.&amp;#x15e;afak Bilici&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zaza_B/0/1/0/all/0/1"&gt;B. Selin Zaza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ozgenc_M/0/1/0/all/0/1"&gt;M. Yi&amp;#x11f;it &amp;#xd6;zgen&amp;#xe7;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Boyar_O/0/1/0/all/0/1"&gt;Onur Boyar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Autonomous Road Vehicle Integrated Approaches to an Emergency Obstacle Avoidance Maneuver. (arXiv:2105.09446v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.09446</id>
        <link href="http://arxiv.org/abs/2105.09446"/>
        <updated>2021-05-23T06:10:38.865Z</updated>
        <summary type="html"><![CDATA[As passenger vehicle technologies have advanced, so have their capabilities
to avoid obstacles, especially with developments in tires, suspensions,
steering, as well as safety technologies like ABS, ESC, and more recently, ADAS
systems. However, environments around passenger vehicles have also become more
complex, and dangerous. There have previously been studies that outline driver
tendencies and performance capabilities when attempting to avoid obstacles
while driving passenger vehicles. Now that autonomous vehicles are being
developed with obstacle avoidance capabilities, it is important to target
performance that meets or exceeds that of human drivers. This manuscript
highlights systems that are crucial for an emergency obstacle avoidance
maneuver (EOAM) and identifies the state-of-the-art for each of the related
systems, while considering the nuances of traveling at highway speeds. Some of
the primary EOAM-related systems/areas that are discussed in this review are:
general path planning methods, system hierarchies, decision-making, trajectory
generation, and trajectory-tracking control methods. After concluding remarks,
suggestions for future work which could lead to an ideal EOAM development, are
discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lowe_E/0/1/0/all/0/1"&gt;Evan Lowe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guvenc_L/0/1/0/all/0/1"&gt;Levent Guven&amp;#xe7;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DistTune: Distributed Fine-Grained Adaptive Traffic Speed Prediction for Growing Transportation Networks. (arXiv:2105.09421v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09421</id>
        <link href="http://arxiv.org/abs/2105.09421"/>
        <updated>2021-05-23T06:10:38.850Z</updated>
        <summary type="html"><![CDATA[Over the past decade, many approaches have been introduced for traffic speed
prediction. However, providing fine-grained, accurate, time-efficient, and
adaptive traffic speed prediction for a growing transportation network where
the size of the network keeps increasing and new traffic detectors are
constantly deployed has not been well studied. To address this issue, this
paper presents DistTune based on Long Short-Term Memory (LSTM) and the
Nelder-Mead method. Whenever encountering an unprocessed detector, DistTune
decides if it should customize an LSTM model for this detector by comparing the
detector with other processed detectors in terms of the normalized traffic
speed patterns they have observed. If similarity is found, DistTune directly
shares an existing LSTM model with this detector to achieve time-efficient
processing. Otherwise, DistTune customizes an LSTM model for the detector to
achieve fine-grained prediction. To make DistTune even more time-efficient,
DistTune performs on a cluster of computing nodes in parallel. To achieve
adaptive traffic speed prediction, DistTune also provides LSTM re-customization
for detectors that suffer from unsatisfactory prediction accuracy due to for
instance traffic speed pattern change. Extensive experiments based on traffic
data collected from freeway I5-N in California are conducted to evaluate the
performance of DistTune. The results demonstrate that DistTune provides
fine-grained, accurate, time-efficient, and adaptive traffic speed prediction
for a growing transportation network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Ming-Chang Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jia-Chun Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gran_E/0/1/0/all/0/1"&gt;Ernst Gunnar Gran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Separation of Powers in Federated Learning. (arXiv:2105.09400v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2105.09400</id>
        <link href="http://arxiv.org/abs/2105.09400"/>
        <updated>2021-05-23T06:10:38.835Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) enables collaborative training among mutually
distrusting parties. Model updates, rather than training data, are concentrated
and fused in a central aggregation server. A key security challenge in FL is
that an untrustworthy or compromised aggregation process might lead to
unforeseeable information leakage. This challenge is especially acute due to
recently demonstrated attacks that have reconstructed large fractions of
training data from ostensibly "sanitized" model updates.

In this paper, we introduce TRUDA, a new cross-silo FL system, employing a
trustworthy and decentralized aggregation architecture to break down
information concentration with regard to a single aggregator. Based on the
unique computational properties of model-fusion algorithms, all exchanged model
updates in TRUDA are disassembled at the parameter-granularity and re-stitched
to random partitions designated for multiple TEE-protected aggregators. Thus,
each aggregator only has a fragmentary and shuffled view of model updates and
is oblivious to the model architecture. Our new security mechanisms can
fundamentally mitigate training reconstruction attacks, while still preserving
the final accuracy of trained models and keeping performance overheads low.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1"&gt;Pau-Chen Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eykholt_K/0/1/0/all/0/1"&gt;Kevin Eykholt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1"&gt;Zhongshu Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jamjoom_H/0/1/0/all/0/1"&gt;Hani Jamjoom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jayaram_K/0/1/0/all/0/1"&gt;K. R. Jayaram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valdez_E/0/1/0/all/0/1"&gt;Enriquillo Valdez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_A/0/1/0/all/0/1"&gt;Ashish Verma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speech & Song Emotion Recognition Using Multilayer Perceptron and Standard Vector Machine. (arXiv:2105.09406v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2105.09406</id>
        <link href="http://arxiv.org/abs/2105.09406"/>
        <updated>2021-05-23T06:10:38.820Z</updated>
        <summary type="html"><![CDATA[Herein, we have compared the performance of SVM and MLP in emotion
recognition using speech and song channels of the RAVDESS dataset. We have
undertaken a journey to extract various audio features, identify optimal
scaling strategy and hyperparameter for our models. To increase sample size, we
have performed audio data augmentation and addressed data imbalance using
SMOTE. Our data indicate that optimised SVM outperforms MLP with an accuracy of
82 compared to 75%. Following data augmentation, the performance of both
algorithms was identical at ~79%, however, overfitting was evident for the SVM.
Our final exploration indicated that the performance of both SVM and MLP were
similar in which both resulted in lower accuracy for the speech channel
compared to the song channel. Our findings suggest that both SVM and MLP are
powerful classifiers for emotion recognition in a vocal-dependent manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Javaheri_B/0/1/0/all/0/1"&gt;Behzad Javaheri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable Health Risk Predictor with Transformer-based Medicare Claim Encoder. (arXiv:2105.09428v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09428</id>
        <link href="http://arxiv.org/abs/2105.09428"/>
        <updated>2021-05-23T06:10:38.807Z</updated>
        <summary type="html"><![CDATA[In 2019, The Centers for Medicare and Medicaid Services (CMS) launched an
Artificial Intelligence (AI) Health Outcomes Challenge seeking solutions to
predict risk in value-based care for incorporation into CMS Innovation Center
payment and service delivery models. Recently, modern language models have
played key roles in a number of health related tasks. This paper presents, to
the best of our knowledge, the first application of these models to patient
readmission prediction. To facilitate this, we create a dataset of 1.2 million
medical history samples derived from the Limited Dataset (LDS) issued by CMS.
Moreover, we propose a comprehensive modeling solution centered on a deep
learning framework for this data. To demonstrate the framework, we train an
attention-based Transformer to learn Medicare semantics in support of
performing downstream prediction tasks thereby achieving 0.91 AUC and 0.91
recall on readmission classification. We also introduce a novel data
pre-processing pipeline and discuss pertinent deployment considerations
surrounding model explainability and bias.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lahlou_C/0/1/0/all/0/1"&gt;Chuhong Lahlou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crayton_A/0/1/0/all/0/1"&gt;Ancil Crayton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trier_C/0/1/0/all/0/1"&gt;Caroline Trier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Willett_E/0/1/0/all/0/1"&gt;Evan Willett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[L1 Regression with Lewis Weights Subsampling. (arXiv:2105.09433v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09433</id>
        <link href="http://arxiv.org/abs/2105.09433"/>
        <updated>2021-05-23T06:10:38.786Z</updated>
        <summary type="html"><![CDATA[We consider the problem of finding an approximate solution to $\ell_1$
regression while only observing a small number of labels. Given an $n \times d$
unlabeled data matrix $X$, we must choose a small set of $m \ll n$ rows to
observe the labels of, then output an estimate $\widehat{\beta}$ whose error on
the original problem is within a $1 + \varepsilon$ factor of optimal. We show
that sampling from $X$ according to its Lewis weights and outputting the
empirical minimizer succeeds with probability $1-\delta$ for $m >
O(\frac{1}{\varepsilon^2} d \log \frac{d}{\varepsilon \delta})$. This is
analogous to the performance of sampling according to leverage scores for
$\ell_2$ regression, but with exponentially better dependence on $\delta$. We
also give a corresponding lower bound of $\Omega(\frac{d}{\varepsilon^2} + (d +
\frac{1}{\varepsilon^2}) \log\frac{1}{\delta})$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parulekar_A/0/1/0/all/0/1"&gt;Aditya Parulekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parulekar_A/0/1/0/all/0/1"&gt;Advait Parulekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1"&gt;Eric Price&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heterogeneous Contrastive Learning. (arXiv:2105.09401v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09401</id>
        <link href="http://arxiv.org/abs/2105.09401"/>
        <updated>2021-05-23T06:10:38.751Z</updated>
        <summary type="html"><![CDATA[With the advent of big data across multiple high-impact applications, we are
often facing the challenge of complex heterogeneity. The newly collected data
usually consist of multiple modalities and characterized with multiple labels,
thus exhibiting the co-existence of multiple types of heterogeneity. Although
state-of-the-art techniques are good at modeling the complex heterogeneity with
sufficient label information, such label information can be quite expensive to
obtain in real applications, leading to sub-optimal performance using these
techniques. Inspired by the capability of contrastive learning to utilize rich
unlabeled data for improving performance, in this paper, we propose a unified
heterogeneous learning framework, which combines both weighted unsupervised
contrastive loss and weighted supervised contrastive loss to model multiple
types of heterogeneity. We also provide theoretical analyses showing that the
proposed weighted supervised contrastive loss is the lower bound of the mutual
information of two samples from the same class and the weighted unsupervised
contrastive loss is the lower bound of the mutual information between the
hidden representation of two views of the same sample. Experimental results on
real-world data sets demonstrate the effectiveness and the efficiency of the
proposed method modeling multiple types of heterogeneity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Lecheng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yada Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jingrui He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1"&gt;Jinjun Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Balancing Robustness and Sensitivity using Feature Contrastive Learning. (arXiv:2105.09394v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09394</id>
        <link href="http://arxiv.org/abs/2105.09394"/>
        <updated>2021-05-23T06:10:38.598Z</updated>
        <summary type="html"><![CDATA[It is generally believed that robust training of extremely large networks is
critical to their success in real-world applications. However, when taken to
the extreme, methods that promote robustness can hurt the model's sensitivity
to rare or underrepresented patterns. In this paper, we discuss this trade-off
between sensitivity and robustness to natural (non-adversarial) perturbations
by introducing two notions: contextual feature utility and contextual feature
sensitivity. We propose Feature Contrastive Learning (FCL) that encourages a
model to be more sensitive to the features that have higher contextual utility.
Empirical results demonstrate that models trained with FCL achieve a better
balance of robustness and sensitivity, leading to improved generalization in
the presence of noise on both vision and NLP datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seungyeon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glasner_D/0/1/0/all/0/1"&gt;Daniel Glasner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramalingam_S/0/1/0/all/0/1"&gt;Srikumar Ramalingam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papineni_K/0/1/0/all/0/1"&gt;Kishore Papineni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sanjiv Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Machine Learning Techniques to Identify Key Risk Factors for Diabetes and Undiagnosed Diabetes. (arXiv:2105.09379v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09379</id>
        <link href="http://arxiv.org/abs/2105.09379"/>
        <updated>2021-05-23T06:10:38.585Z</updated>
        <summary type="html"><![CDATA[This paper reviews a wide selection of machine learning models built to
predict both the presence of diabetes and the presence of undiagnosed diabetes
using eight years of National Health and Nutrition Examination Survey (NHANES)
data. Models are tuned and compared via their Brier Scores. The most relevant
variables of the best performing models are then compared. A Support Vector
Machine with a linear kernel performed best for predicting diabetes, returning
a Brier score of 0.0654 and an AUROC of 0.9235 on the test set. An elastic net
regression performed best for predicting undiagnosed diabetes with a Brier
score of 0.0294 and an AUROC of 0.9439 on the test set. Similar features appear
prominently in the models for both sets of models. Blood osmolality, family
history, the prevalance of various compounds, and hypertension are key
indicators for all diabetes risk. For undiagnosed diabetes in particular, there
are ethnicity or genetic components which arise as strong correlates as well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Adler_A/0/1/0/all/0/1"&gt;Avraham Adler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VOILA: Visual-Observation-Only Imitation Learning for Autonomous Navigation. (arXiv:2105.09371v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.09371</id>
        <link href="http://arxiv.org/abs/2105.09371"/>
        <updated>2021-05-23T06:10:38.567Z</updated>
        <summary type="html"><![CDATA[While imitation learning for vision based autonomous mobile robot navigation
has recently received a great deal of attention in the research community,
existing approaches typically require state action demonstrations that were
gathered using the deployment platform. However, what if one cannot easily
outfit their platform to record these demonstration signals or worse yet the
demonstrator does not have access to the platform at all? Is imitation learning
for vision based autonomous navigation even possible in such scenarios? In this
work, we hypothesize that the answer is yes and that recent ideas from the
Imitation from Observation (IfO) literature can be brought to bear such that a
robot can learn to navigate using only ego centric video collected by a
demonstrator, even in the presence of viewpoint mismatch. To this end, we
introduce a new algorithm, Visual Observation only Imitation Learning for
Autonomous navigation (VOILA), that can successfully learn navigation policies
from a single video demonstration collected from a physically different agent.
We evaluate VOILA in the photorealistic AirSim simulator and show that VOILA
not only successfully imitates the expert, but that it also learns navigation
policies that can generalize to novel environments. Further, we demonstrate the
effectiveness of VOILA in a real world setting by showing that it allows a
wheeled Jackal robot to successfully imitate a human walking in an environment
using a video recorded using a mobile phone camera.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karnan_H/0/1/0/all/0/1"&gt;Haresh Karnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Warnell_G/0/1/0/all/0/1"&gt;Garrett Warnell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1"&gt;Xuesu Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1"&gt;Peter Stone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Adversarial Neural Architecture Search. (arXiv:2105.09356v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09356</id>
        <link href="http://arxiv.org/abs/2105.09356"/>
        <updated>2021-05-23T06:10:38.549Z</updated>
        <summary type="html"><![CDATA[Despite the empirical success of neural architecture search (NAS) in deep
learning applications, the optimality, reproducibility and cost of NAS schemes
remain hard to assess. In this paper, we propose Generative Adversarial NAS
(GA-NAS) with theoretically provable convergence guarantees, promoting
stability and reproducibility in neural architecture search. Inspired by
importance sampling, GA-NAS iteratively fits a generator to previously
discovered top architectures, thus increasingly focusing on important parts of
a large search space. Furthermore, we propose an efficient adversarial learning
approach, where the generator is trained by reinforcement learning based on
rewards provided by a discriminator, thus being able to explore the search
space without evaluating a large number of architectures. Extensive experiments
show that GA-NAS beats the best published results under several cases on three
public NAS benchmarks. In the meantime, GA-NAS can handle ad-hoc search
constraints and search spaces. We show that GA-NAS can be used to improve
already optimized baselines found by other NAS methods, including EfficientNet
and ProxylessNAS, in terms of ImageNet accuracy or the number of parameters, in
their original search space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rezaei_S/0/1/0/all/0/1"&gt;Seyed Saeed Changiz Rezaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1"&gt;Fred X. Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1"&gt;Di Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salameh_M/0/1/0/all/0/1"&gt;Mohammad Salameh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1"&gt;Keith Mills&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1"&gt;Shuo Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wei Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1"&gt;Shangling Jui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[User Label Leakage from Gradients in Federated Learning. (arXiv:2105.09369v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2105.09369</id>
        <link href="http://arxiv.org/abs/2105.09369"/>
        <updated>2021-05-23T06:10:38.516Z</updated>
        <summary type="html"><![CDATA[Federated learning enables multiple users to build a joint model by sharing
their model updates (gradients), while their raw data remains local on their
devices. In contrast to the common belief that this provides privacy benefits,
we here add to the very recent results on privacy risks when sharing gradients.
Specifically, we propose Label Leakage from Gradients (LLG), a novel attack to
extract the labels of the users' training data from their shared gradients. The
attack exploits the direction and magnitude of gradients to determine the
presence or absence of any label. LLG is simple yet effective, capable of
leaking potential sensitive information represented by labels, and scales well
to arbitrary batch sizes and multiple classes. We empirically and
mathematically demonstrate the validity of our attack under different settings.
Moreover, empirical results show that LLG successfully extracts labels with
high accuracy at the early stages of model training. We also discuss different
defense mechanisms against such leakage. Our findings suggest that gradient
compression is a practical technique to prevent our attack.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wainakh_A/0/1/0/all/0/1"&gt;Aidmar Wainakh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ventola_F/0/1/0/all/0/1"&gt;Fabrizio Ventola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mussig_T/0/1/0/all/0/1"&gt;Till M&amp;#xfc;&amp;#xdf;ig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keim_J/0/1/0/all/0/1"&gt;Jens Keim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cordero_C/0/1/0/all/0/1"&gt;Carlos Garcia Cordero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zimmer_E/0/1/0/all/0/1"&gt;Ephraim Zimmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grube_T/0/1/0/all/0/1"&gt;Tim Grube&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1"&gt;Kristian Kersting&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muhlhauser_M/0/1/0/all/0/1"&gt;Max M&amp;#xfc;hlh&amp;#xe4;user&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepDebug: Fixing Python Bugs Using Stack Traces, Backtranslation, and Code Skeletons. (arXiv:2105.09352v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2105.09352</id>
        <link href="http://arxiv.org/abs/2105.09352"/>
        <updated>2021-05-23T06:10:38.347Z</updated>
        <summary type="html"><![CDATA[The joint task of bug localization and program repair is an integral part of
the software development process. In this work we present DeepDebug, an
approach to automated debugging using large, pretrained transformers. We begin
by training a bug-creation model on reversed commit data for the purpose of
generating synthetic bugs. We apply these synthetic bugs toward two ends.
First, we directly train a backtranslation model on all functions from 200K
repositories. Next, we focus on 10K repositories for which we can execute
tests, and create buggy versions of all functions in those repositories that
are covered by passing tests. This provides us with rich debugging information
such as stack traces and print statements, which we use to finetune our model
which was pretrained on raw source code. Finally, we strengthen all our models
by expanding the context window beyond the buggy function itself, and adding a
skeleton consisting of that function's parent class, imports, signatures,
docstrings, and method bodies, in order of priority. On the QuixBugs benchmark,
we increase the total number of fixes found by over 50%, while also decreasing
the false positive rate from 35% to 5% and decreasing the timeout from six
hours to one minute. On our own benchmark of executable tests, our model fixes
68% of all bugs on its first attempt without using traces, and after adding
traces it fixes 75% on first attempt. We will open-source our framework and
validation set for evaluating on executable tests.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1"&gt;Dawn Drain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clement_C/0/1/0/all/0/1"&gt;Colin B. Clement&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Serrato_G/0/1/0/all/0/1"&gt;Guillermo Serrato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1"&gt;Neel Sundaresan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Objects as Extreme Points. (arXiv:2104.14066v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14066</id>
        <link href="http://arxiv.org/abs/2104.14066"/>
        <updated>2021-05-23T06:08:18.236Z</updated>
        <summary type="html"><![CDATA[Object detection can be regarded as a pixel clustering task, and its boundary
is determined by four extreme points (leftmost, top, rightmost, and bottom).
However, most studies focus on the center or corner points of the object, which
are actually conditional results of the extreme points. In this paper, we
present an Extreme-Point-Prediction-Based object detector (EPP-Net), which
directly regresses the relative displacement vector between each pixel and the
four extreme points. We also propose a new metric to measure the similarity
between two groups of extreme points, namely, Extreme Intersection over Union
(EIoU), and incorporate this EIoU as a new regression loss. Moreover, we
propose a novel branch to predict the EIoU between the ground-truth and the
prediction results, and combine it with the classification confidence as the
ranking keyword in non-maximum suppression. On the MS-COCO dataset, our method
achieves an average precision (AP) of 44.0% with ResNet-50 and an AP of 48.3%
with ResNeXt-101-DCN. The proposed EPP-Net provides a new method to detect
objects and outperforms state-of-the-art anchor-free detectors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Min Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_B/0/1/0/all/0/1"&gt;Bo Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zihao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Junxing Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1"&gt;Degang Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physically-Consistent Generative Adversarial Networks for Coastal Flood Visualization. (arXiv:2104.04785v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04785</id>
        <link href="http://arxiv.org/abs/2104.04785"/>
        <updated>2021-05-23T06:08:18.215Z</updated>
        <summary type="html"><![CDATA[As climate change increases the intensity of natural disasters, society needs
better tools for adaptation. Floods, for example, are the most frequent natural
disaster, and better tools for flood risk communication could increase the
support for flood-resilient infrastructure development. Our work aims to enable
more visual communication of large-scale climate impacts via visualizing the
output of coastal flood models as satellite imagery. We propose the first deep
learning pipeline to ensure physical-consistency in synthetic visual satellite
imagery. We advanced a state-of-the-art GAN called pix2pixHD, such that it
produces imagery that is physically-consistent with the output of an
expert-validated storm surge model (NOAA SLOSH). By evaluating the imagery
relative to physics-based flood maps, we find that our proposed framework
outperforms baseline models in both physical-consistency and photorealism. We
envision our work to be the first step towards a global visualization of how
climate change shapes our landscape. Continuing on this path, we show that the
proposed pipeline generalizes to visualize arctic sea ice melt. We also publish
a dataset of over 25k labelled image-pairs to study image-to-image translation
in Earth observation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn L&amp;#xfc;tjens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leshchinskiy_B/0/1/0/all/0/1"&gt;Brandon Leshchinskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Requena_Mesa_C/0/1/0/all/0/1"&gt;Christian Requena-Mesa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chishtie_F/0/1/0/all/0/1"&gt;Farrukh Chishtie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_Rodriguez_N/0/1/0/all/0/1"&gt;Natalia D&amp;#xed;az-Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boulais_O/0/1/0/all/0/1"&gt;Oc&amp;#xe9;ane Boulais&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sankaranarayanan_A/0/1/0/all/0/1"&gt;Aruna Sankaranarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pina_A/0/1/0/all/0/1"&gt;Aaron Pi&amp;#xf1;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raissi_C/0/1/0/all/0/1"&gt;Chedy Ra&amp;#xef;ssi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lavin_A/0/1/0/all/0/1"&gt;Alexander Lavin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Newman_D/0/1/0/all/0/1"&gt;Dava Newman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Learning for Bone Mineral Density Estimation in Hip X-ray Images. (arXiv:2103.13482v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13482</id>
        <link href="http://arxiv.org/abs/2103.13482"/>
        <updated>2021-05-23T06:08:18.206Z</updated>
        <summary type="html"><![CDATA[Bone mineral density (BMD) is a clinically critical indicator of
osteoporosis, usually measured by dual-energy X-ray absorptiometry (DEXA). Due
to the limited accessibility of DEXA machines and examinations, osteoporosis is
often under-diagnosed and under-treated, leading to increased fragility
fracture risks. Thus it is highly desirable to obtain BMDs with alternative
cost-effective and more accessible medical imaging examinations such as X-ray
plain films. In this work, we formulate the BMD estimation from plain hip X-ray
images as a regression problem. Specifically, we propose a new semi-supervised
self-training algorithm to train the BMD regression model using images coupled
with DEXA measured BMDs and unlabeled images with pseudo BMDs. Pseudo BMDs are
generated and refined iteratively for unlabeled images during self-training. We
also present a novel adaptive triplet loss to improve the model's regression
accuracy. On an in-house dataset of 1,090 images (819 unique patients), our BMD
estimation method achieves a high Pearson correlation coefficient of 0.8805 to
ground-truth BMDs. It offers good feasibility to use the more accessible and
cheaper X-ray imaging for opportunistic osteoporosis screening.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zheng_K/0/1/0/all/0/1"&gt;Kang Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yirui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xiaoyun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fakai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_L/0/1/0/all/0/1"&gt;Le Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chihung Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_L/0/1/0/all/0/1"&gt;Lingyun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xie_G/0/1/0/all/0/1"&gt;Guotong Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1"&gt;Jing Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kuo_C/0/1/0/all/0/1"&gt;Chang-Fu Kuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Miao_S/0/1/0/all/0/1"&gt;Shun Miao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MoDL-QSM: Model-based Deep Learning for Quantitative Susceptibility Mapping. (arXiv:2101.08413v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08413</id>
        <link href="http://arxiv.org/abs/2101.08413"/>
        <updated>2021-05-23T06:08:18.199Z</updated>
        <summary type="html"><![CDATA[Quantitative susceptibility mapping (QSM) has demonstrated great potential in
quantifying tissue susceptibility in various brain diseases. However, the
intrinsic ill-posed inverse problem relating the tissue phase to the underlying
susceptibility distribution affects the accuracy for quantifying tissue
susceptibility. Recently, deep learning has shown promising results to improve
accuracy by reducing the streaking artifacts. However, there exists a mismatch
between the observed phase and the theoretical forward phase estimated by the
susceptibility label. In this study, we proposed a model-based deep learning
architecture that followed the STI (susceptibility tensor imaging) physical
model, referred to as MoDL-QSM. Specifically, MoDL-QSM accounts for the
relationship between STI-derived phase contrast induced by the susceptibility
tensor terms (ki13,ki23,ki33) and the acquired single-orientation phase. The
convolution neural networks are embedded into the physical model to learn a
regularization term containing prior information. ki33 and phase induced by
ki13 and ki23 terms were used as the labels for network training. Quantitative
evaluation metrics (RSME, SSIM, and HFEN) were compared with recently developed
deep learning QSM methods. The results showed that MoDL-QSM achieved superior
performance, demonstrating its potential for future applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1"&gt;Ruimin Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jiayi Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;He Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Baofeng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jie Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yuting Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Ming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chunlei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuyao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1"&gt;Jie Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1"&gt;Hongjiang Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Person Extreme Motion Prediction with Cross-Interaction Attention. (arXiv:2105.08825v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08825</id>
        <link href="http://arxiv.org/abs/2105.08825"/>
        <updated>2021-05-23T06:08:18.192Z</updated>
        <summary type="html"><![CDATA[Human motion prediction aims to forecast future human poses given a sequence
of past 3D skeletons. While this problem has recently received increasing
attention, it has mostly been tackled for single humans in isolation. In this
paper we explore this problem from a novel perspective, involving humans
performing collaborative tasks. We assume that the input of our system are two
sequences of past skeletons for two interacting persons, and we aim to predict
the future motion for each of them. For this purpose, we devise a novel cross
interaction attention mechanism that exploits historical information of both
persons and learns to predict cross dependencies between self poses and the
poses of the other person in spite of their spatial or temporal distance. Since
no dataset to train such interactive situations is available, we have captured
ExPI (Extreme Pose Interaction), a new lab-based person interaction dataset of
professional dancers performing acrobatics. ExPI contains 115 sequences with
30k frames and 60k instances with annotated 3D body poses and shapes. We
thoroughly evaluate our cross-interaction network on this dataset and show that
both in short-term and long-term predictions, it consistently outperforms
baselines that independently reason for each person. We plan to release our
code jointly with the dataset and the train/test splits to spur future research
on the topic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1"&gt;Wen Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bie_X/0/1/0/all/0/1"&gt;Xiaoyu Bie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1"&gt;Xavier Alameda-Pineda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1"&gt;Francesc Moreno-Noguer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Decade Survey of Content Based Image Retrieval using Deep Learning. (arXiv:2012.00641v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00641</id>
        <link href="http://arxiv.org/abs/2012.00641"/>
        <updated>2021-05-23T06:08:18.175Z</updated>
        <summary type="html"><![CDATA[The content based image retrieval aims to find the similar images from a
large scale dataset against a query image. Generally, the similarity between
the representative features of the query image and dataset images is used to
rank the images for retrieval. In early days, various hand designed feature
descriptors have been investigated based on the visual cues such as color,
texture, shape, etc. that represent the images. However, the deep learning has
emerged as a dominating alternative of hand-designed feature engineering from a
decade. It learns the features automatically from the data. This paper presents
a comprehensive survey of deep learning based developments in the past decade
for content based image retrieval. The categorization of existing
state-of-the-art methods from different perspectives is also performed for
greater understanding of the progress. The taxonomy used in this survey covers
different supervision, different networks, different descriptor type and
different retrieval type. A performance analysis is also performed using the
state-of-the-art methods. The insights are also presented for the benefit of
the researchers to observe the progress and to make the best choices. The
survey presented in this paper will help in further research progress in image
retrieval using deep learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1"&gt;Shiv Ram Dubey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-19 Lung Lesion Segmentation Using a Sparsely Supervised Mask R-CNN on Chest X-rays Automatically Computed from Volumetric CTs. (arXiv:2105.08147v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08147</id>
        <link href="http://arxiv.org/abs/2105.08147"/>
        <updated>2021-05-23T06:08:18.169Z</updated>
        <summary type="html"><![CDATA[Chest X-rays of coronavirus disease 2019 (COVID-19) patients are frequently
obtained to determine the extent of lung disease and are a valuable source of
data for creating artificial intelligence models. Most work to date assessing
disease severity on chest imaging has focused on segmenting computed tomography
(CT) images; however, given that CTs are performed much less frequently than
chest X-rays for COVID-19 patients, automated lung lesion segmentation on chest
X-rays could be clinically valuable. There currently exists a universal
shortage of chest X-rays with ground truth COVID-19 lung lesion annotations,
and manually contouring lung opacities is a tedious, labor-intensive task. To
accelerate severity detection and augment the amount of publicly available
chest X-ray training data for supervised deep learning (DL) models, we leverage
existing annotated CT images to generate frontal projection "chest X-ray"
images for training COVID-19 chest X-ray models. In this paper, we propose an
automated pipeline for segmentation of COVID-19 lung lesions on chest X-rays
comprised of a Mask R-CNN trained on a mixed dataset of open-source chest
X-rays and coronal X-ray projections computed from annotated volumetric CTs. On
a test set containing 40 chest X-rays of COVID-19 positive patients, our model
achieved IoU scores of 0.81 $\pm$ 0.03 and 0.79 $\pm$ 0.03 when trained on a
dataset of 60 chest X-rays and on a mixed dataset of 10 chest X-rays and 50
projections from CTs, respectively. Our model far outperforms current baselines
with limited supervised training and may assist in automated COVID-19 severity
quantification on chest X-rays.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ramesh_V/0/1/0/all/0/1"&gt;Vignav Ramesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rister_B/0/1/0/all/0/1"&gt;Blaine Rister&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rubin_D/0/1/0/all/0/1"&gt;Daniel L. Rubin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SAFIN: Arbitrary Style Transfer With Self-Attentive Factorized Instance Normalization. (arXiv:2105.06129v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06129</id>
        <link href="http://arxiv.org/abs/2105.06129"/>
        <updated>2021-05-23T06:08:18.162Z</updated>
        <summary type="html"><![CDATA[Artistic style transfer aims to transfer the style characteristics of one
image onto another image while retaining its content. Existing approaches
commonly leverage various normalization techniques, although these face
limitations in adequately transferring diverse textures to different spatial
locations. Self-Attention-based approaches have tackled this issue with partial
success but suffer from unwanted artifacts. Motivated by these observations,
this paper aims to combine the best of both worlds: self-attention and
normalization. That yields a new plug-and-play module that we name
Self-Attentive Factorized Instance Normalization (SAFIN). SAFIN is essentially
a spatially adaptive normalization module whose parameters are inferred through
attention on the content and style image. We demonstrate that plugging SAFIN
into the base network of another state-of-the-art method results in enhanced
stylization. We also develop a novel base network composed of Wavelet Transform
for multi-scale style transfer, which when combined with SAFIN, produces
visually appealing results with lesser unwanted textures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Aaditya Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hingane_S/0/1/0/all/0/1"&gt;Shreeshail Hingane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1"&gt;Xinyu Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Remote Pulse Estimation in the Presence of Face Masks. (arXiv:2101.04096v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.04096</id>
        <link href="http://arxiv.org/abs/2101.04096"/>
        <updated>2021-05-23T06:08:18.154Z</updated>
        <summary type="html"><![CDATA[Remote photoplethysmography (rPPG), a family of techniques for monitoring
blood volume changes, may be especially useful for widespread contactless
health monitoring using face video from consumer-grade visible-light cameras.
The COVID-19 pandemic has caused the widespread use of protective face masks.
We found that occlusions from cloth face masks increased the mean absolute
error of heart rate estimation by more than 80\% when deploying methods
designed on unmasked faces. We show that augmenting unmasked face videos by
adding patterned synthetic face masks forces the model to attend to the
periocular and forehead regions, improving performance and closing the gap
between masked and unmasked pulse estimation. To our knowledge, this paper is
the first to analyse the impact of face masks on the accuracy of pulse
estimation and offers several novel contributions: (a) 3D CNN-based method
designed for remote photoplethysmography in a presence of face masks, (b) two
publicly available pulse estimation datasets acquired from 86 unmasked and 61
masked subjects, (c) evaluations of handcrafted algorithms and a 3D CNN trained
on videos of unmasked faces and with masks synthetically added, and (d) data
augmentation method to add a synthetic mask to a face video.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Speth_J/0/1/0/all/0/1"&gt;Jeremy Speth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vance_N/0/1/0/all/0/1"&gt;Nathan Vance&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flynn_P/0/1/0/all/0/1"&gt;Patrick Flynn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bowyer_K/0/1/0/all/0/1"&gt;Kevin Bowyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Czajka_A/0/1/0/all/0/1"&gt;Adam Czajka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Audio-Driven Emotional Video Portraits. (arXiv:2104.07452v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07452</id>
        <link href="http://arxiv.org/abs/2104.07452"/>
        <updated>2021-05-23T06:08:18.147Z</updated>
        <summary type="html"><![CDATA[Despite previous success in generating audio-driven talking heads, most of
the previous studies focus on the correlation between speech content and the
mouth shape. Facial emotion, which is one of the most important features on
natural human faces, is always neglected in their methods. In this work, we
present Emotional Video Portraits (EVP), a system for synthesizing high-quality
video portraits with vivid emotional dynamics driven by audios. Specifically,
we propose the Cross-Reconstructed Emotion Disentanglement technique to
decompose speech into two decoupled spaces, i.e., a duration-independent
emotion space and a duration dependent content space. With the disentangled
features, dynamic 2D emotional facial landmarks can be deduced. Then we propose
the Target-Adaptive Face Synthesis technique to generate the final high-quality
video portraits, by bridging the gap between the deduced landmarks and the
natural head poses of target videos. Extensive experiments demonstrate the
effectiveness of our method both qualitatively and quantitatively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1"&gt;Xinya Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kaisiyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wayne Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1"&gt;Chen Change Loy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1"&gt;Xun Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1"&gt;Feng Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Object Recognition in Indoor Environments Using Topologically Persistent Features. (arXiv:2010.03196v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03196</id>
        <link href="http://arxiv.org/abs/2010.03196"/>
        <updated>2021-05-23T06:08:18.140Z</updated>
        <summary type="html"><![CDATA[Object recognition in unseen indoor environments remains a challenging
problem for visual perception of mobile robots. In this letter, we propose the
use of topologically persistent features, which rely on the objects' shape
information, to address this challenge. In particular, we extract two kinds of
features, namely, sparse persistence image (PI) and amplitude, by applying
persistent homology to multi-directional height function-based filtrations of
the cubical complexes representing the object segmentation maps. The features
are then used to train a fully connected network for recognition. For
performance evaluation, in addition to a widely used shape dataset and a
benchmark indoor scenes dataset, we collect a new dataset, comprising scene
images from two different environments, namely, a living room and a mock
warehouse. The scenes are captured using varying camera poses under different
illumination conditions and include up to five different objects from a given
set of fourteen objects. On the benchmark indoor scenes dataset, sparse PI
features show better recognition performance in unseen environments than the
features learned using the widely used ResNetV2-56 and EfficientNet-B4 models.
Further, they provide slightly higher recall and accuracy values than Faster
R-CNN, an end-to-end object detection method, and its state-of-the-art variant,
Domain Adaptive Faster R-CNN. The performance of our methods also remains
relatively unchanged from the training environment (living room) to the unseen
environment (mock warehouse) in the new dataset. In contrast, the performance
of the object detection methods drops substantially. We also implement the
proposed method on a real-world robot to demonstrate its usefulness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samani_E/0/1/0/all/0/1"&gt;Ekta U. Samani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xingjian Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1"&gt;Ashis G. Banerjee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-19 Detection in Computed Tomography Images with 2D and 3D Approaches. (arXiv:2105.08506v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08506</id>
        <link href="http://arxiv.org/abs/2105.08506"/>
        <updated>2021-05-23T06:08:18.131Z</updated>
        <summary type="html"><![CDATA[Detecting COVID-19 in computed tomography (CT) or radiography images has been
proposed as a supplement to the definitive RT-PCR test. We present a deep
learning ensemble for detecting COVID-19 infection, combining slice-based (2D)
and volume-based (3D) approaches. The 2D system detects the infection on each
CT slice independently, combining them to obtain the patient-level decision via
different methods (averaging and long-short term memory networks). The 3D
system takes the whole CT volume to arrive to the patient-level decision in one
step. A new high resolution chest CT scan dataset, called the IST-C dataset, is
also collected in this work. The proposed ensemble, called IST-CovNet, obtains
90.80% accuracy and 0.95 AUC score overall on the IST-C dataset in detecting
COVID-19 among normal controls and other types of lung pathologies; and 93.69%
accuracy and 0.99 AUC score on the publicly available MosMed dataset that
consists of COVID-19 scans and normal controls only. The system is deployed at
Istanbul University Cerrahpasa School of Medicine.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ahmed_S/0/1/0/all/0/1"&gt;Sara Atito Ali Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yavuz_M/0/1/0/all/0/1"&gt;Mehmet Can Yavuz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sen_M/0/1/0/all/0/1"&gt;Mehmet Umut Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gulsen_F/0/1/0/all/0/1"&gt;Fatih Gulsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tutar_O/0/1/0/all/0/1"&gt;Onur Tutar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Korkmazer_B/0/1/0/all/0/1"&gt;Bora Korkmazer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Samanci_C/0/1/0/all/0/1"&gt;Cesur Samanci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sirolu_S/0/1/0/all/0/1"&gt;Sabri Sirolu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hamid_R/0/1/0/all/0/1"&gt;Rauf Hamid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Eryurekli_A/0/1/0/all/0/1"&gt;Ali Ergun Eryurekli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mammadov_T/0/1/0/all/0/1"&gt;Toghrul Mammadov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yanikoglu_B/0/1/0/all/0/1"&gt;Berrin Yanikoglu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple Simultaneous Pseudo Image Classification with Random Fields and a Deep Belief Network for Disease Indication. (arXiv:2104.10762v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10762</id>
        <link href="http://arxiv.org/abs/2104.10762"/>
        <updated>2021-05-23T06:08:18.110Z</updated>
        <summary type="html"><![CDATA[We show how to use random field theory in a supervised, energy-based model
for multiple pseudo image classification of 2D integer matrices. In the model,
each row of a 2D integer matrix is a pseudo image where a local receptive field
focuses on multiple portions of individual rows for simultaneous learning. The
model is used for a classification task consisting of presence of patient
biomarkers indicative of a particular disease.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Murphy_R/0/1/0/all/0/1"&gt;Robert A. Murphy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Which Parts determine the Impression of the Font?. (arXiv:2103.14216v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14216</id>
        <link href="http://arxiv.org/abs/2103.14216"/>
        <updated>2021-05-23T06:08:18.103Z</updated>
        <summary type="html"><![CDATA[Various fonts give different impressions, such as legible, rough, and
comic-text.This paper aims to analyze the correlation between the local shapes,
or parts, and the impression of fonts. By focusing on local shapes instead of
the whole letter shape, we can realize letter-shape independent and more
general analysis. The analysis is performed by newly combining SIFT and
DeepSets, to extract an arbitrary number of essential parts from a particular
font and aggregate them to infer the font impressions by nonlinear regression.
Our qualitative and quantitative analyses prove that (1)fonts with similar
parts have similar impressions, (2)many impressions, such as legible and rough,
largely depend on specific parts, (3)several impressions are very irrelevant to
parts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ueda_M/0/1/0/all/0/1"&gt;Masaya Ueda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kimura_A/0/1/0/all/0/1"&gt;Akisato Kimura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1"&gt;Seiichi Uchida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pruning of Deep Spiking Neural Networks through Gradient Rewiring. (arXiv:2105.04916v2 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04916</id>
        <link href="http://arxiv.org/abs/2105.04916"/>
        <updated>2021-05-23T06:08:18.096Z</updated>
        <summary type="html"><![CDATA[Spiking Neural Networks (SNNs) have been attached great importance due to
their biological plausibility and high energy-efficiency on neuromorphic chips.
As these chips are usually resource-constrained, the compression of SNNs is
thus crucial along the road of practical use of SNNs. Most existing methods
directly apply pruning approaches in artificial neural networks (ANNs) to SNNs,
which ignore the difference between ANNs and SNNs, thus limiting the
performance of the pruned SNNs. Besides, these methods are only suitable for
shallow SNNs. In this paper, inspired by synaptogenesis and synapse elimination
in the neural system, we propose gradient rewiring (Grad R), a joint learning
algorithm of connectivity and weight for SNNs, that enables us to seamlessly
optimize network structure without retrain. Our key innovation is to redefine
the gradient to a new synaptic parameter, allowing better exploration of
network structures by taking full advantage of the competition between pruning
and regrowth of connections. The experimental results show that the proposed
method achieves minimal loss of SNNs' performance on MNIST and CIFAR-10 dataset
so far. Moreover, it reaches a $\sim$3.5% accuracy loss under unprecedented
0.73% connectivity, which reveals remarkable structure refining capability in
SNNs. Our work suggests that there exists extremely high redundancy in deep
SNNs. Our codes are available at
https://github.com/Yanqi-Chen/Gradient-Rewiring .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yanqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhaofei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1"&gt;Wei Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1"&gt;Tiejun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Temporally Consistent Image-based Sun Tracking Algorithm for Solar Energy Forecasting Applications. (arXiv:2012.01059v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01059</id>
        <link href="http://arxiv.org/abs/2012.01059"/>
        <updated>2021-05-23T06:08:18.079Z</updated>
        <summary type="html"><![CDATA[Improving irradiance forecasting is critical to further increase the share of
solar in the energy mix. On a short time scale, fish-eye cameras on the ground
are used to capture cloud displacements causing the local variability of the
electricity production. As most of the solar radiation comes directly from the
Sun, current forecasting approaches use its position in the image as a
reference to interpret the cloud cover dynamics. However, existing Sun tracking
methods rely on external data and a calibration of the camera, which requires
access to the device. To address these limitations, this study introduces an
image-based Sun tracking algorithm to localise the Sun in the image when it is
visible and interpolate its daily trajectory from past observations. We
validate the method on a set of sky images collected over a year at SIRTA's
lab. Experimental results show that the proposed method provides robust smooth
Sun trajectories with a mean absolute error below 1% of the image size.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paletta_Q/0/1/0/all/0/1"&gt;Quentin Paletta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1"&gt;Joan Lasenby&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auto-Tuned Sim-to-Real Transfer. (arXiv:2104.07662v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07662</id>
        <link href="http://arxiv.org/abs/2104.07662"/>
        <updated>2021-05-23T06:08:18.072Z</updated>
        <summary type="html"><![CDATA[Policies trained in simulation often fail when transferred to the real world
due to the `reality gap' where the simulator is unable to accurately capture
the dynamics and visual properties of the real world. Current approaches to
tackle this problem, such as domain randomization, require prior knowledge and
engineering to determine how much to randomize system parameters in order to
learn a policy that is robust to sim-to-real transfer while also not being too
conservative. We propose a method for automatically tuning simulator system
parameters to match the real world using only raw RGB images of the real world
without the need to define rewards or estimate state. Our key insight is to
reframe the auto-tuning of parameters as a search problem where we iteratively
shift the simulation system parameters to approach the real-world system
parameters. We propose a Search Param Model (SPM) that, given a sequence of
observations and actions and a set of system parameters, predicts whether the
given parameters are higher or lower than the true parameters used to generate
the observations. We evaluate our method on multiple robotic control tasks in
both sim-to-sim and sim-to-real transfer, demonstrating significant improvement
over naive domain randomization. Project videos and code at
https://yuqingd.github.io/autotuned-sim2real/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yuqing Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watkins_O/0/1/0/all/0/1"&gt;Olivia Watkins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1"&gt;Trevor Darrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1"&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1"&gt;Deepak Pathak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Empirical Analysis of Image Caption Generation using Deep Learning. (arXiv:2105.09906v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09906</id>
        <link href="http://arxiv.org/abs/2105.09906"/>
        <updated>2021-05-23T06:08:18.065Z</updated>
        <summary type="html"><![CDATA[Automated image captioning is one of the applications of Deep Learning which
involves fusion of work done in computer vision and natural language
processing, and it is typically performed using Encoder-Decoder architectures.
In this project, we have implemented and experimented with various flavors of
multi-modal image captioning networks where ResNet101, DenseNet121 and VGG19
based CNN Encoders and Attention based LSTM Decoders were explored. We have
studied the effect of beam size and the use of pretrained word embeddings and
compared them to baseline CNN encoder and RNN decoder architecture. The goal is
to analyze the performance of each approach using various evaluation metrics
including BLEU, CIDEr, ROUGE and METEOR. We have also explored model
explainability using Visual Attention Maps (VAM) to highlight parts of the
images which has maximum contribution for predicting each word of the generated
caption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1"&gt;Aditya Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Girishekar_E/0/1/0/all/0/1"&gt;Eshwar Shamanna Girishekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deshpande_P/0/1/0/all/0/1"&gt;Padmakar Anil Deshpande&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seismic Fault Segmentation via 3D-CNN Training by a Few 2D Slices Labels. (arXiv:2105.03857v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03857</id>
        <link href="http://arxiv.org/abs/2105.03857"/>
        <updated>2021-05-23T06:08:18.058Z</updated>
        <summary type="html"><![CDATA[Detection faults in seismic data is a crucial step for seismic structural
interpretation, reservoir characterization and well placement. Some recent
works regard it as an image segmentation task. The task of image segmentation
requires huge labels, especially 3D seismic data, which has a complex structure
and lots of noise. Therefore, its annotation requires expert experience and a
huge workload. In this study, we present {\lambda}-BCE and {\lambda}-smooth
L1loss to effectively train 3D-CNN by some slices from 3D seismic data, so that
the model can learn the segmentation of 3D seismic data from a few 2D slices.
In order to fully extract information from limited data and suppress seismic
noise, we propose an attention module that can be used for active supervision
training and embedded in the network. The attention heatmap target is generated
by the original label, and letting it supervise the attention module using the
{\lambda}-smooth L1loss. The experiment proves the effectiveness of our loss
function and attention module, it also shows that our method can extract 3D
seismic features from a few 2D slices labels, and the segmentation effect
achieves state-of-the-art. We only use 3.3% of the all labels, and we can
achieve similar performance as using all labels. This work has been submitted
to the IEEE for possible publication. Copyright may be transferred without
notice, after which this version may no longer be accessible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1"&gt;YiMin Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Kewen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jianbing Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1"&gt;Yingjie Xi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Camouflaged Instance Segmentation In-The-Wild: Dataset And Benchmark Suite. (arXiv:2103.17123v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.17123</id>
        <link href="http://arxiv.org/abs/2103.17123"/>
        <updated>2021-05-23T06:08:18.024Z</updated>
        <summary type="html"><![CDATA[This paper pushes the envelope on camouflaged regions to decompose them into
meaningful components, namely, camouflaged instances. To promote the new task
of camouflaged instance segmentation in-the-wild, we introduce a new dataset,
namely CAMO++, by extending our preliminary CAMO dataset (camouflaged object
segmentation) in terms of quantity and diversity. The new dataset substantially
increases the number of images with hierarchical pixel-wise ground-truths. We
also provide a benchmark suite for the task of camouflaged instance
segmentation. In particular, we conduct extensive evaluation of
state-of-the-art instance segmentation methods on our newly constructed CAMO++
dataset in various scenarios. We also propose Camouflage Fusion Learning (CFL)
framework for camouflaged instance segmentation to further improve the
state-of-the-art performance. The dataset, model, evaluation suite, and
benchmark will be publicly available at our project page.
\url{https://sites.google.com/view/ltnghia/research/camo\_plus\_plus}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1"&gt;Trung-Nghia Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yubo Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Tan-Cong Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_M/0/1/0/all/0/1"&gt;Minh-Quan Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Khanh-Duy Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1"&gt;Thanh-Toan Do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1"&gt;Minh-Triet Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Tam V. Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Discriminative Learning of Sounds for Audio Event Classification. (arXiv:2105.09279v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09279</id>
        <link href="http://arxiv.org/abs/2105.09279"/>
        <updated>2021-05-23T06:08:18.017Z</updated>
        <summary type="html"><![CDATA[Recent progress in network-based audio event classification has shown the
benefit of pre-training models on visual data such as ImageNet. While this
process allows knowledge transfer across different domains, training a model on
large-scale visual datasets is time consuming. On several audio event
classification benchmarks, we show a fast and effective alternative that
pre-trains the model unsupervised, only on audio data and yet delivers on-par
performance with ImageNet pre-training. Furthermore, we show that our
discriminative audio learning can be used to transfer knowledge across audio
datasets and optionally include ImageNet pre-training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hornauer_S/0/1/0/all/0/1"&gt;Sascha Hornauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Ke Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Stella X. Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghaffarzadegan_S/0/1/0/all/0/1"&gt;Shabnam Ghaffarzadegan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1"&gt;Liu Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MIN2Net: End-to-End Multi-Task Learning for Subject-Independent Motor Imagery EEG Classification. (arXiv:2102.03814v3 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03814</id>
        <link href="http://arxiv.org/abs/2102.03814"/>
        <updated>2021-05-23T06:08:18.005Z</updated>
        <summary type="html"><![CDATA[Advances in the motor imagery (MI)-based brain-computer interfaces (BCIs)
allow control of several applications by decoding neurophysiological phenomena,
which are usually recorded by electroencephalography (EEG) using a non-invasive
technique. Despite great advances in MI-based BCI, EEG rhythms are specific to
a subject and various changes over time. These issues point to significant
challenges to enhance the classification performance, especially in a
subject-independent manner. To overcome these challenges, we propose MIN2Net, a
novel end-to-end multi-task learning to tackle this task. We integrate deep
metric learning into a multi-task autoencoder to learn a compact and
discriminative latent representation from EEG and perform classification
simultaneously. This approach reduces the complexity in pre-processing, results
in significant performance improvement on EEG classification. Experimental
results in a subject-independent manner show that MIN2Net outperforms the
state-of-the-art techniques, achieving an F1-score improvement of 6.72%, and
2.23% on the SMR-BCI, and OpenBMI datasets, respectively. We demonstrate that
MIN2Net improves discriminative information in the latent representation. This
study indicates the possibility and practicality of using this model to develop
MI-based BCI applications for new users without the need for calibration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Autthasan_P/0/1/0/all/0/1"&gt;Phairot Autthasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chaisaen_R/0/1/0/all/0/1"&gt;Rattanaphon Chaisaen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sudhawiyangkul_T/0/1/0/all/0/1"&gt;Thapanun Sudhawiyangkul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rangpong_P/0/1/0/all/0/1"&gt;Phurin Rangpong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kiatthaveephong_S/0/1/0/all/0/1"&gt;Suktipol Kiatthaveephong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dilokthanakul_N/0/1/0/all/0/1"&gt;Nat Dilokthanakul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bhakdisongkhram_G/0/1/0/all/0/1"&gt;Gun Bhakdisongkhram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Phan_H/0/1/0/all/0/1"&gt;Huy Phan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Guan_C/0/1/0/all/0/1"&gt;Cuntai Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wilaiprasitporn_T/0/1/0/all/0/1"&gt;Theerawit Wilaiprasitporn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anchor-based Plain Net for Mobile Image Super-Resolution. (arXiv:2105.09750v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.09750</id>
        <link href="http://arxiv.org/abs/2105.09750"/>
        <updated>2021-05-23T06:08:17.998Z</updated>
        <summary type="html"><![CDATA[Along with the rapid development of real-world applications, higher
requirements on the accuracy and efficiency of image super-resolution (SR) are
brought forward. Though existing methods have achieved remarkable success, the
majority of them demand plenty of computational resources and large amount of
RAM, and thus they can not be well applied to mobile device. In this paper, we
aim at designing efficient architecture for 8-bit quantization and deploy it on
mobile device. First, we conduct an experiment about meta-node latency by
decomposing lightweight SR architectures, which determines the portable
operations we can utilize. Then, we dig deeper into what kind of architecture
is beneficial to 8-bit quantization and propose anchor-based plain net (ABPN).
Finally, we adopt quantization-aware training strategy to further boost the
performance. Our model can outperform 8-bit quantized FSRCNN by nearly 2dB in
terms of PSNR, while satisfying realistic needs at the same time. Code is
avaliable at https://github.com/NJU- Jet/SR_Mobile_Quantization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Du_Z/0/1/0/all/0/1"&gt;Zongcai Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jie Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_G/0/1/0/all/0/1"&gt;Gangshan Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient and Robust LiDAR-Based End-to-End Navigation. (arXiv:2105.09932v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.09932</id>
        <link href="http://arxiv.org/abs/2105.09932"/>
        <updated>2021-05-23T06:08:17.961Z</updated>
        <summary type="html"><![CDATA[Deep learning has been used to demonstrate end-to-end neural network learning
for autonomous vehicle control from raw sensory input. While LiDAR sensors
provide reliably accurate information, existing end-to-end driving solutions
are mainly based on cameras since processing 3D data requires a large memory
footprint and computation cost. On the other hand, increasing the robustness of
these systems is also critical; however, even estimating the model's
uncertainty is very challenging due to the cost of sampling-based methods. In
this paper, we present an efficient and robust LiDAR-based end-to-end
navigation framework. We first introduce Fast-LiDARNet that is based on sparse
convolution kernel optimization and hardware-aware model design. We then
propose Hybrid Evidential Fusion that directly estimates the uncertainty of the
prediction from only a single forward pass and then fuses the control
predictions intelligently. We evaluate our system on a full-scale vehicle and
demonstrate lane-stable as well as navigation capabilities. In the presence of
out-of-distribution events (e.g., sensor failures), our system significantly
improves robustness and reduces the number of takeovers in the real world.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhijian Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1"&gt;Alexander Amini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Sibo Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karaman_S/0/1/0/all/0/1"&gt;Sertac Karaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Song Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1"&gt;Daniela Rus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine-learning based methodologies for 3d x-ray measurement, characterization and optimization for buried structures in advanced ic packages. (arXiv:2103.04838v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04838</id>
        <link href="http://arxiv.org/abs/2103.04838"/>
        <updated>2021-05-23T06:08:17.947Z</updated>
        <summary type="html"><![CDATA[For over 40 years lithographic silicon scaling has driven circuit integration
and performance improvement in the semiconductor industry. As silicon scaling
slows down, the industry is increasingly dependent on IC package technologies
to contribute to further circuit integration and performance improvements. This
is a paradigm shift and requires the IC package industry to reduce the size and
increase the density of internal interconnects on a scale which has never been
done before. Traditional package characterization and process optimization
relies on destructive techniques such as physical cross-sections and delayering
to extract data from internal package features. These destructive techniques
are not practical with today's advanced packages. In this paper we will
demonstrate how data acquired non-destructively with a 3D X-ray microscope can
be enhanced and optimized using machine learning, and can then be used to
measure, characterize and optimize the design and production of buried
interconnects in advanced IC packages. Test vehicles replicating 2.5D and HBM
construction were designed and fabricated, and digital data was extracted from
these test vehicles using 3D X-ray and machine learning techniques. The
extracted digital data was used to characterize and optimize the design and
production of the interconnects and demonstrates a superior alternative to
destructive physical analysis. We report an mAP of 0.96 for 3D object
detection, a dice score of 0.92 for 3D segmentation, and an average of 2.1um
error for 3D metrology on the test dataset. This paper is the first part of a
multi-part report.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pahwa_R/0/1/0/all/0/1"&gt;Ramanpreet S Pahwa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_S/0/1/0/all/0/1"&gt;Soon Wee Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1"&gt;Ren Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_R/0/1/0/all/0/1"&gt;Richard Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Min_O/0/1/0/all/0/1"&gt;Oo Zaw Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jie_W/0/1/0/all/0/1"&gt;Wang Jie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_V/0/1/0/all/0/1"&gt;Vempati Srinivasa Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nwe_T/0/1/0/all/0/1"&gt;Tin Lay Nwe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yanjing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neumann_J/0/1/0/all/0/1"&gt;Jens Timo Neumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pichumani_R/0/1/0/all/0/1"&gt;Ramani Pichumani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gregorich_T/0/1/0/all/0/1"&gt;Thomas Gregorich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pedestrian Intention Prediction: A Multi-task Perspective. (arXiv:2010.10270v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10270</id>
        <link href="http://arxiv.org/abs/2010.10270"/>
        <updated>2021-05-23T06:08:17.928Z</updated>
        <summary type="html"><![CDATA[In order to be globally deployed, autonomous cars must guarantee the safety
of pedestrians. This is the reason why forecasting pedestrians' intentions
sufficiently in advance is one of the most critical and challenging tasks for
autonomous vehicles. This work tries to solve this problem by jointly
predicting the intention and visual states of pedestrians. In terms of visual
states, whereas previous work focused on x-y coordinates, we will also predict
the size and indeed the whole bounding box of the pedestrian. The method is a
recurrent neural network in a multi-task learning approach. It has one head
that predicts the intention of the pedestrian for each one of its future
position and another one predicting the visual states of the pedestrian.
Experiments on the JAAD dataset show the superiority of the performance of our
method compared to previous works for intention prediction. Also, although its
simple architecture (more than 2 times faster), the performance of the bounding
box prediction is comparable to the ones yielded by much more complex
architectures. Our code is available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bouhsain_S/0/1/0/all/0/1"&gt;Smail Ait Bouhsain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saadatnejad_S/0/1/0/all/0/1"&gt;Saeed Saadatnejad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alahi_A/0/1/0/all/0/1"&gt;Alexandre Alahi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flexible Compositional Learning of Structured Visual Concepts. (arXiv:2105.09848v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09848</id>
        <link href="http://arxiv.org/abs/2105.09848"/>
        <updated>2021-05-23T06:08:17.922Z</updated>
        <summary type="html"><![CDATA[Humans are highly efficient learners, with the ability to grasp the meaning
of a new concept from just a few examples. Unlike popular computer vision
systems, humans can flexibly leverage the compositional structure of the visual
world, understanding new concepts as combinations of existing concepts. In the
current paper, we study how people learn different types of visual
compositions, using abstract visual forms with rich relational structure. We
find that people can make meaningful compositional generalizations from just a
few examples in a variety of scenarios, and we develop a Bayesian program
induction model that provides a close fit to the behavioral data. Unlike past
work examining special cases of compositionality, our work shows how a single
computational approach can account for many distinct types of compositional
generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yanli Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lake_B/0/1/0/all/0/1"&gt;Brenden M. Lake&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Face, Body, Voice: Video Person-Clustering with Multiple Modalities. (arXiv:2105.09939v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09939</id>
        <link href="http://arxiv.org/abs/2105.09939"/>
        <updated>2021-05-23T06:08:17.901Z</updated>
        <summary type="html"><![CDATA[The objective of this work is person-clustering in videos -- grouping
characters according to their identity. Previous methods focus on the narrower
task of face-clustering, and for the most part ignore other cues such as the
person's voice, their overall appearance (hair, clothes, posture), and the
editing structure of the videos. Similarly, most current datasets evaluate only
the task of face-clustering, rather than person-clustering. This limits their
applicability to downstream applications such as story understanding which
require person-level, rather than only face-level, reasoning. In this paper we
make contributions to address both these deficiencies: first, we introduce a
Multi-Modal High-Precision Clustering algorithm for person-clustering in videos
using cues from several modalities (face, body, and voice). Second, we
introduce a Video Person-Clustering dataset, for evaluating multi-modal
person-clustering. It contains body-tracks for each annotated character,
face-tracks when visible, and voice-tracks when speaking, with their associated
features. The dataset is by far the largest of its kind, and covers films and
TV-shows representing a wide range of demographics. Finally, we show the
effectiveness of using multiple modalities for person-clustering, explore the
use of this new broad task for story understanding through character
co-occurrences, and achieve a new state of the art on all available datasets
for face and person-clustering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brown_A/0/1/0/all/0/1"&gt;Andrew Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalogeiton_V/0/1/0/all/0/1"&gt;Vicky Kalogeiton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1"&gt;Andrew Zisserman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Biologically Inspired Semantic Lateral Connectivity for Convolutional Neural Networks. (arXiv:2105.09830v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09830</id>
        <link href="http://arxiv.org/abs/2105.09830"/>
        <updated>2021-05-23T06:08:17.895Z</updated>
        <summary type="html"><![CDATA[Lateral connections play an important role for sensory processing in visual
cortex by supporting discriminable neuronal responses even to highly similar
features. In the present work, we show that establishing a biologically
inspired Mexican hat lateral connectivity profile along the filter domain can
significantly improve the classification accuracy of a variety of lightweight
convolutional neural networks without the addition of trainable network
parameters. Moreover, we demonstrate that it is possible to analytically
determine the stationary distribution of modulated filter activations and
thereby avoid using recurrence for modeling temporal dynamics. We furthermore
reveal that the Mexican hat connectivity profile has the effect of ordering
filters in a sequence resembling the topographic organization of feature
selectivity in early visual cortex. In an ordered filter sequence, this profile
then sharpens the filters' tuning curves.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weidler_T/0/1/0/all/0/1"&gt;Tonio Weidler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehnen_J/0/1/0/all/0/1"&gt;Julian Lehnen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Denman_Q/0/1/0/all/0/1"&gt;Quinton Denman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebok_D/0/1/0/all/0/1"&gt;D&amp;#xe1;vid Seb&amp;#x151;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weiss_G/0/1/0/all/0/1"&gt;Gerhard Weiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Driessens_K/0/1/0/all/0/1"&gt;Kurt Driessens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Senden_M/0/1/0/all/0/1"&gt;Mario Senden&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PLSM: A Parallelized Liquid State Machine for Unintentional Action Detection. (arXiv:2105.09909v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09909</id>
        <link href="http://arxiv.org/abs/2105.09909"/>
        <updated>2021-05-23T06:08:17.884Z</updated>
        <summary type="html"><![CDATA[Reservoir Computing (RC) offers a viable option to deploy AI algorithms on
low-end embedded system platforms. Liquid State Machine (LSM) is a bio-inspired
RC model that mimics the cortical microcircuits and uses spiking neural
networks (SNN) that can be directly realized on neuromorphic hardware. In this
paper, we present a novel Parallelized LSM (PLSM) architecture that
incorporates spatio-temporal read-out layer and semantic constraints on model
output. To the best of our knowledge, such a formulation has been done for the
first time in literature, and it offers a computationally lighter alternative
to traditional deep-learning models. Additionally, we also present a
comprehensive algorithm for the implementation of parallelizable SNNs and LSMs
that are GPU-compatible. We implement the PLSM model to classify
unintentional/accidental video clips, using the Oops dataset. From the
experimental results on detecting unintentional action in video, it can be
observed that our proposed model outperforms a self-supervised model and a
fully supervised traditional deep learning model. All the implemented codes can
be found at our repository
https://github.com/anonymoussentience2020/Parallelized_LSM_for_Unintentional_Action_Recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1"&gt;Dipayan Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1"&gt;Saumik Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1"&gt;Umapada Pal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chanda_S/0/1/0/all/0/1"&gt;Sukalpa Chanda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AnaXNet: Anatomy Aware Multi-label Finding Classification in Chest X-ray. (arXiv:2105.09937v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09937</id>
        <link href="http://arxiv.org/abs/2105.09937"/>
        <updated>2021-05-23T06:08:17.877Z</updated>
        <summary type="html"><![CDATA[Radiologists usually observe anatomical regions of chest X-ray images as well
as the overall image before making a decision. However, most existing deep
learning models only look at the entire X-ray image for classification, failing
to utilize important anatomical information. In this paper, we propose a novel
multi-label chest X-ray classification model that accurately classifies the
image finding and also localizes the findings to their correct anatomical
regions. Specifically, our model consists of two modules, the detection module
and the anatomical dependency module. The latter utilizes graph convolutional
networks, which enable our model to learn not only the label dependency but
also the relationship between the anatomical regions in the chest X-ray. We
further utilize a method to efficiently create an adjacency matrix for the
anatomical regions using the correlation of the label across the different
regions. Detailed experiments and analysis of our results show the
effectiveness of our method when compared to the current state-of-the-art
multi-label chest X-ray image classification methods while also providing
accurate location information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agu_N/0/1/0/all/0/1"&gt;Nkechinyere N. Agu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Joy T. Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_H/0/1/0/all/0/1"&gt;Hanqing Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lourentzou_I/0/1/0/all/0/1"&gt;Ismini Lourentzou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Arjun Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1"&gt;Mehdi Moradi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_P/0/1/0/all/0/1"&gt;Pingkun Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hendler_J/0/1/0/all/0/1"&gt;James Hendler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classification of Urban Morphology with Deep Learning: Application on Urban Vitality. (arXiv:2105.09908v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09908</id>
        <link href="http://arxiv.org/abs/2105.09908"/>
        <updated>2021-05-23T06:08:17.864Z</updated>
        <summary type="html"><![CDATA[There is a prevailing trend to study urban morphology quantitatively thanks
to the growing accessibility to various forms of spatial big data, increasing
computing power, and use cases benefiting from such information. The methods
developed up to now measure urban morphology with numerical indices describing
density, proportion, and mixture, but they do not directly represent
morphological features from human's visual and intuitive perspective. We take
the first step to bridge the gap by proposing a deep learning-based technique
to automatically classify road networks into four classes on a visual basis.
The method is implemented by generating an image of the street network (Colored
Road Hierarchy Diagram), which we introduce in this paper, and classifying it
using a deep convolutional neural network (ResNet-34). The model achieves an
overall classification accuracy of 0.875. Nine cities around the world are
selected as the study areas and their road networks are acquired from
OpenStreetMap. Latent subgroups among the cities are uncovered through a
clustering on the percentage of each road network category. In the subsequent
part of the paper, we focus on the usability of such classification: the
effectiveness of our human perception augmentation is examined by a case study
of urban vitality prediction. An advanced tree-based regression model is for
the first time designated to establish the relationship between morphological
indices and vitality indicators. A positive effect of human perception
augmentation is detected in the comparative experiment of baseline model and
augmented model. This work expands the toolkit of quantitative urban morphology
study with new techniques, supporting further studies in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wangyang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1"&gt;Abraham Noah Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biljecki_F/0/1/0/all/0/1"&gt;Filip Biljecki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Where Do Deep Fakes Look? Synthetic Face Detection via Gaze Tracking. (arXiv:2101.01165v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.01165</id>
        <link href="http://arxiv.org/abs/2101.01165"/>
        <updated>2021-05-23T06:08:17.845Z</updated>
        <summary type="html"><![CDATA[Following the recent initiatives for the democratization of AI, deep fake
generators have become increasingly popular and accessible, causing dystopian
scenarios towards social erosion of trust. A particular domain, such as
biological signals, attracted attention towards detection methods that are
capable of exploiting authenticity signatures in real videos that are not yet
faked by generative approaches. In this paper, we first propose several
prominent eye and gaze features that deep fakes exhibit differently. Second, we
compile those features into signatures and analyze and compare those of real
and fake videos, formulating geometric, visual, metric, temporal, and spectral
variations. Third, we generalize this formulation to the deep fake detection
problem by a deep neural network, to classify any video in the wild as fake or
real. We evaluate our approach on several deep fake datasets, achieving 92.48%
accuracy on FaceForensics++, 80.0% on Deep Fakes (in the wild), 88.35% on
CelebDF, and 99.27% on DeeperForensics datasets. Our approach outperforms most
deep and biological fake detectors with complex network architectures without
the proposed gaze signatures. We conduct ablation studies involving different
features, architectures, sequence durations, and post-processing artifacts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Demir_I/0/1/0/all/0/1"&gt;Ilke Demir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciftci_U/0/1/0/all/0/1"&gt;Umur A. Ciftci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepAVO: Efficient Pose Refining with Feature Distilling for Deep Visual Odometry. (arXiv:2105.09899v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09899</id>
        <link href="http://arxiv.org/abs/2105.09899"/>
        <updated>2021-05-23T06:08:17.838Z</updated>
        <summary type="html"><![CDATA[The technology for Visual Odometry (VO) that estimates the position and
orientation of the moving object through analyzing the image sequences captured
by on-board cameras, has been well investigated with the rising interest in
autonomous driving. This paper studies monocular VO from the perspective of
Deep Learning (DL). Unlike most current learning-based methods, our approach,
called DeepAVO, is established on the intuition that features contribute
discriminately to different motion patterns. Specifically, we present a novel
four-branch network to learn the rotation and translation by leveraging
Convolutional Neural Networks (CNNs) to focus on different quadrants of optical
flow input. To enhance the ability of feature selection, we further introduce
an effective channel-spatial attention mechanism to force each branch to
explicitly distill related information for specific Frame to Frame (F2F) motion
estimation. Experiments on various datasets involving outdoor driving and
indoor walking scenarios show that the proposed DeepAVO outperforms the
state-of-the-art monocular methods by a large margin, demonstrating competitive
performance to the stereo VO algorithm and verifying promising potential for
generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1"&gt;Ran Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Mingkun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1"&gt;Rujun Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1"&gt;Bo Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1"&gt;Zhuoling Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying the Complexity of Standard Benchmarking Datasets for Long-Term Human Trajectory Prediction. (arXiv:2005.13934v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.13934</id>
        <link href="http://arxiv.org/abs/2005.13934"/>
        <updated>2021-05-23T06:08:17.832Z</updated>
        <summary type="html"><![CDATA[Methods to quantify the complexity of trajectory datasets are still a missing
piece in benchmarking human trajectory prediction models. In order to gain a
better understanding of the complexity of trajectory prediction tasks and
following the intuition, that more complex datasets contain more information,
an approach for quantifying the amount of information contained in a dataset
from a prototype-based dataset representation is proposed. The dataset
representation is obtained by first employing a non-trivial spatial sequence
alignment, which enables a subsequent learning vector quantization (LVQ) stage.
A large-scale complexity analysis is conducted on several human trajectory
prediction benchmarking datasets, followed by a brief discussion on indications
for human trajectory prediction and benchmarking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hug_R/0/1/0/all/0/1"&gt;Ronny Hug&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Becker_S/0/1/0/all/0/1"&gt;Stefan Becker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hubner_W/0/1/0/all/0/1"&gt;Wolfgang H&amp;#xfc;bner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arens_M/0/1/0/all/0/1"&gt;Michael Arens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fully Convolutional Networks for Automatically Generating Image Masks to Train Mask R-CNN. (arXiv:2003.01383v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.01383</id>
        <link href="http://arxiv.org/abs/2003.01383"/>
        <updated>2021-05-23T06:08:17.823Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel automatically generating image masks method for
the state-of-the-art Mask R-CNN deep learning method. The Mask R-CNN method
achieves the best results in object detection until now, however, it is very
time-consuming and laborious to get the object Masks for training, the proposed
method is composed by a two-stage design, to automatically generating image
masks, the first stage implements a fully convolutional networks (FCN) based
segmentation network, the second stage network, a Mask R-CNN based object
detection network, which is trained on the object image masks from FCN output,
the original input image, and additional label information. Through
experimentation, our proposed method can obtain the image masks automatically
to train Mask R-CNN, and it can achieve very high classification accuracy with
an over 90% mean of average precision (mAP) for segmentation]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siebert_J/0/1/0/all/0/1"&gt;Jan Paul Siebert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiangrong Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trained Trajectory based Automated Parking System using Visual SLAM on Surround View Cameras. (arXiv:2001.02161v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.02161</id>
        <link href="http://arxiv.org/abs/2001.02161"/>
        <updated>2021-05-23T06:08:17.817Z</updated>
        <summary type="html"><![CDATA[Automated Parking is becoming a standard feature in modern vehicles. Existing
parking systems build a local map to be able to plan for maneuvering towards a
detected slot. Next generation parking systems have an use case where they
build a persistent map of the environment where the car is frequently parked,
say for example, home parking or office parking. The pre-built map helps in
re-localizing the vehicle better when its trying to park the next time. This is
achieved by augmenting the parking system with a Visual SLAM pipeline and the
feature is called trained trajectory parking in the automotive industry. In
this paper, we discuss the use cases, design and implementation of a trained
trajectory automated parking system. The proposed system is deployed on
commercial vehicles and the consumer application is illustrated in
\url{https://youtu.be/nRWF5KhyJZU}. The focus of this paper is on the
application and the details of vision algorithms are kept at high level.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tripathi_N/0/1/0/all/0/1"&gt;Nivedita Tripathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1"&gt;Senthil Yogamani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The MAMe Dataset: On the relevance of High Resolution and Variable Shape image properties. (arXiv:2007.13693v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.13693</id>
        <link href="http://arxiv.org/abs/2007.13693"/>
        <updated>2021-05-23T06:08:17.797Z</updated>
        <summary type="html"><![CDATA[In the image classification task, the most common approach is to resize all
images in a dataset to a unique shape, while reducing their precision to a size
which facilitates experimentation at scale. This practice has benefits from a
computational perspective, but it entails negative side-effects on performance
due to loss of information and image deformation. In this work we introduce the
MAMe dataset, an image classification dataset with remarkable high resolution
and variable shape properties. The goal of MAMe is to provide a tool for
studying the impact of such properties in image classification, while
motivating research in the field. The MAMe dataset contains thousands of
artworks from three different museums, and proposes a classification task
consisting on differentiating between 29 mediums (i.e. materials and
techniques) supervised by art experts. After reviewing the singularity of MAMe
in the context of current image classification tasks, a thorough description of
the task is provided, together with dataset statistics. Experiments are
conducted to evaluate the impact of using high resolution images, variable
shape inputs and both properties at the same time. Results illustrate the
positive impact in performance when using high resolution images, while
highlighting the lack of solutions to exploit variable shapes. An additional
experiment exposes the distinctiveness between the MAMe dataset and the
prototypical ImageNet dataset. Finally, the baselines are inspected using
explainability methods and expert knowledge, to gain insights on the challenges
that remain ahead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pares_F/0/1/0/all/0/1"&gt;Ferran Par&amp;#xe9;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arias_Duart_A/0/1/0/all/0/1"&gt;Anna Arias-Duart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_Gasulla_D/0/1/0/all/0/1"&gt;Dario Garcia-Gasulla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Campo_Frances_G/0/1/0/all/0/1"&gt;Gema Campo-Franc&amp;#xe9;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viladrich_N/0/1/0/all/0/1"&gt;Nina Viladrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayguade_E/0/1/0/all/0/1"&gt;Eduard Ayguad&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labarta_J/0/1/0/all/0/1"&gt;Jes&amp;#xfa;s Labarta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Spatio-temporal Attention-based Model for Infant Movement Assessment from Videos. (arXiv:2105.09783v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09783</id>
        <link href="http://arxiv.org/abs/2105.09783"/>
        <updated>2021-05-23T06:08:17.790Z</updated>
        <summary type="html"><![CDATA[The absence or abnormality of fidgety movements of joints or limbs is
strongly indicative of cerebral palsy in infants. Developing computer-based
methods for assessing infant movements in videos is pivotal for improved
cerebral palsy screening. Most existing methods use appearance-based features
and are thus sensitive to strong but irrelevant signals caused by background
clutter or a moving camera. Moreover, these features are computed over the
whole frame, thus they measure gross whole body movements rather than specific
joint/limb motion.

Addressing these challenges, we develop and validate a new method for fidgety
movement assessment from consumer-grade videos using human poses extracted from
short clips. Human poses capture only relevant motion profiles of joints and
limbs and are thus free from irrelevant appearance artifacts. The dynamics and
coordination between joints are modeled using spatio-temporal graph
convolutional networks. Frames and body parts that contain discriminative
information about fidgety movements are selected through a spatio-temporal
attention mechanism. We validate the proposed model on the cerebral palsy
screening task using a real-life consumer-grade video dataset collected at an
Australian hospital through the Cerebral Palsy Alliance, Australia. Our
experiments show that the proposed method achieves the ROC-AUC score of 81.87%,
significantly outperforming existing competing methods with better
interpretability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_Thai_B/0/1/0/all/0/1"&gt;Binh Nguyen-Thai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1"&gt;Vuong Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morgan_C/0/1/0/all/0/1"&gt;Catherine Morgan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badawi_N/0/1/0/all/0/1"&gt;Nadia Badawi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1"&gt;Truyen Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1"&gt;Svetha Venkatesh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Face Image Restoration and Frontalization for Recognition. (arXiv:2105.09907v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09907</id>
        <link href="http://arxiv.org/abs/2105.09907"/>
        <updated>2021-05-23T06:08:17.782Z</updated>
        <summary type="html"><![CDATA[In real-world scenarios, many factors may harm face recognition performance,
e.g., large pose, bad illumination,low resolution, blur and noise. To address
these challenges, previous efforts usually first restore the low-quality faces
to high-quality ones and then perform face recognition. However, most of these
methods are stage-wise, which is sub-optimal and deviates from the reality. In
this paper, we address all these challenges jointly for unconstrained face
recognition. We propose an Multi-Degradation Face Restoration (MDFR) model to
restore frontalized high-quality faces from the given low-quality ones under
arbitrary facial poses, with three distinct novelties. First, MDFR is a
well-designed encoder-decoder architecture which extracts feature
representation from an input face image with arbitrary low-quality factors and
restores it to a high-quality counterpart. Second, MDFR introduces a pose
residual learning strategy along with a 3D-based Pose Normalization Module
(PNM), which can perceive the pose gap between the input initial pose and its
real-frontal pose to guide the face frontalization. Finally, MDFR can generate
frontalized high-quality face images by a single unified network, showing a
strong capability of preserving face identity. Qualitative and quantitative
experiments on both controlled and in-the-wild benchmarks demonstrate the
superiority of MDFR over state-of-the-art methods on both face frontalization
and face restoration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tu_X/0/1/0/all/0/1"&gt;Xiaoguang Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jian Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qiankun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ai_W/0/1/0/all/0/1"&gt;Wenjie Ai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1"&gt;Guodong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhifeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiashi Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BodyPressure -- Inferring Body Pose and Contact Pressure from a Depth Image. (arXiv:2105.09936v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09936</id>
        <link href="http://arxiv.org/abs/2105.09936"/>
        <updated>2021-05-23T06:08:17.774Z</updated>
        <summary type="html"><![CDATA[Contact pressure between the human body and its surroundings has important
implications. For example, it plays a role in comfort, safety, posture, and
health. We present a method that infers contact pressure between a human body
and a mattress from a depth image. Specifically, we focus on using a depth
image from a downward facing camera to infer pressure on a body at rest in bed
occluded by bedding, which is directly applicable to the prevention of pressure
injuries in healthcare. Our approach involves augmenting a real dataset with
synthetic data generated via a soft-body physics simulation of a human body, a
mattress, a pressure sensing mat, and a blanket. We introduce a novel deep
network that we trained on an augmented dataset and evaluated with real data.
The network contains an embedded human body mesh model and uses a white-box
model of depth and pressure image generation. Our network successfully infers
body pose, outperforming prior work. It also infers contact pressure across a
3D mesh model of the human body, which is a novel capability, and does so in
the presence of occlusion from blankets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Clever_H/0/1/0/all/0/1"&gt;Henry M. Clever&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grady_P/0/1/0/all/0/1"&gt;Patrick Grady&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turk_G/0/1/0/all/0/1"&gt;Greg Turk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kemp_C/0/1/0/all/0/1"&gt;Charles C. Kemp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Pruning at Initialization. (arXiv:2002.08797v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.08797</id>
        <link href="http://arxiv.org/abs/2002.08797"/>
        <updated>2021-05-23T06:08:17.767Z</updated>
        <summary type="html"><![CDATA[Overparameterized Neural Networks (NN) display state-of-the-art performance.
However, there is a growing need for smaller, energy-efficient, neural networks
tobe able to use machine learning applications on devices with limited
computational resources. A popular approach consists of using pruning
techniques. While these techniques have traditionally focused on pruning
pre-trained NN (LeCun et al.,1990; Hassibi et al., 1993), recent work by Lee et
al. (2018) has shown promising results when pruning at initialization. However,
for Deep NNs, such procedures remain unsatisfactory as the resulting pruned
networks can be difficult to train and, for instance, they do not prevent one
layer from being fully pruned. In this paper, we provide a comprehensive
theoretical analysis of Magnitude and Gradient based pruning at initialization
and training of sparse architectures. This allows us to propose novel
principled approaches which we validate experimentally on a variety of NN
architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hayou_S/0/1/0/all/0/1"&gt;Soufiane Hayou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ton_J/0/1/0/all/0/1"&gt;Jean-Francois Ton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1"&gt;Arnaud Doucet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Teh_Y/0/1/0/all/0/1"&gt;Yee Whye Teh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probing the Effect of Selection Bias on NN Generalization with a Thought Experiment. (arXiv:2105.09934v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09934</id>
        <link href="http://arxiv.org/abs/2105.09934"/>
        <updated>2021-05-23T06:08:17.749Z</updated>
        <summary type="html"><![CDATA[Learned networks in the domain of visual recognition and cognition impress in
part because even though they are trained with datasets many orders of
magnitude smaller than the full population of possible images, they exhibit
sufficient generalization to be applicable to new and previously unseen data.
Although many have examined issues regarding generalization from several
perspectives, we wondered If a network is trained with a biased dataset that
misses particular samples corresponding to some defining domain attribute, can
it generalize to the full domain from which that training dataset was
extracted? It is certainly true that in vision, no current training set fully
captures all visual information and this may lead to Selection Bias. Here, we
try a novel approach in the tradition of the Thought Experiment. We run this
thought experiment on a real domain of visual objects that we can fully
characterize and look at specific gaps in training data and their impact on
performance requirements. Our thought experiment points to three conclusions:
first, that generalization behavior is dependent on how sufficiently the
particular dimensions of the domain are represented during training; second,
that the utility of any generalization is completely dependent on the
acceptable system error; and third, that specific visual features of objects,
such as pose orientations out of the imaging plane or colours, may not be
recoverable if not represented sufficiently in a training set. Any currently
observed generalization in modern deep learning networks may be more the result
of coincidental alignments and whose utility needs to be confirmed with respect
to a system's performance specification. Our Thought Experiment Probe approach,
coupled with the resulting Bias Breakdown can be very informative towards
understanding the impact of biases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsotsos_J/0/1/0/all/0/1"&gt;John K. Tsotsos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jun Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-supervised, Topology-Aware Segmentation of Tubular Structures from Live Imaging 3D Microscopy. (arXiv:2105.09737v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09737</id>
        <link href="http://arxiv.org/abs/2105.09737"/>
        <updated>2021-05-23T06:08:17.742Z</updated>
        <summary type="html"><![CDATA[Motivated by a challenging tubular network segmentation task, this paper
tackles two commonly encountered problems in biomedical imaging: Topological
consistency of the segmentation, and limited annotations. We propose a
topological score which measures both topological and geometric consistency
between the predicted and ground truth segmentations, applied for model
selection and validation. We apply our topological score in three scenarios: i.
a U-net ii. a U-net pretrained on an autoencoder, and iii. a semisupervised
U-net architecture, which offers a straightforward approach to jointly training
the network both as an autoencoder and a segmentation algorithm. This allows us
to utilize un-annotated data for training a representation that generalizes
across test data variability, in spite of our annotated training data having
very limited variation. Our contributions are validated on a challenging
segmentation task, locating tubular structures in the fetal pancreas from noisy
live imaging confocal microscopy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arnavaz_K/0/1/0/all/0/1"&gt;Kasra Arnavaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_O/0/1/0/all/0/1"&gt;Oswin Krause&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krivokapic_J/0/1/0/all/0/1"&gt;Jelena M. Krivokapic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heilmann_S/0/1/0/all/0/1"&gt;Silja Heilmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baerentzen_J/0/1/0/all/0/1"&gt;Jakob Andreas B&amp;#xe6;rentzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nyeng_P/0/1/0/all/0/1"&gt;Pia Nyeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feragen_A/0/1/0/all/0/1"&gt;Aasa Feragen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly-Supervised Physically Unconstrained Gaze Estimation. (arXiv:2105.09803v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09803</id>
        <link href="http://arxiv.org/abs/2105.09803"/>
        <updated>2021-05-23T06:08:17.735Z</updated>
        <summary type="html"><![CDATA[A major challenge for physically unconstrained gaze estimation is acquiring
training data with 3D gaze annotations for in-the-wild and outdoor scenarios.
In contrast, videos of human interactions in unconstrained environments are
abundantly available and can be much more easily annotated with frame-level
activity labels. In this work, we tackle the previously unexplored problem of
weakly-supervised gaze estimation from videos of human interactions. We
leverage the insight that strong gaze-related geometric constraints exist when
people perform the activity of "looking at each other" (LAEO). To acquire
viable 3D gaze supervision from LAEO labels, we propose a training algorithm
along with several novel loss functions especially designed for the task. With
weak supervision from two large scale CMU-Panoptic and AVA-LAEO activity
datasets, we show significant improvements in (a) the accuracy of
semi-supervised gaze estimation and (b) cross-domain generalization on the
state-of-the-art physically unconstrained in-the-wild Gaze360 gaze estimation
benchmark. We open source our code at
https://github.com/NVlabs/weakly-supervised-gaze.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kothari_R/0/1/0/all/0/1"&gt;Rakshit Kothari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mello_S/0/1/0/all/0/1"&gt;Shalini De Mello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iqbal_U/0/1/0/all/0/1"&gt;Umar Iqbal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Byeon_W/0/1/0/all/0/1"&gt;Wonmin Byeon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Seonwook Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1"&gt;Jan Kautz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Perspective Anomaly Detection. (arXiv:2105.09903v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09903</id>
        <link href="http://arxiv.org/abs/2105.09903"/>
        <updated>2021-05-23T06:08:17.679Z</updated>
        <summary type="html"><![CDATA[Multi-view classification is inspired by the behavior of humans, especially
when fine-grained features or in our case rarely occurring anomalies are to be
detected. Current contributions point to the problem of how high-dimensional
data can be fused. In this work, we build upon the deep support vector data
description algorithm and address multi-perspective anomaly detection using
three different fusion techniques i.e. early fusion, late fusion, and late
fusion with multiple decoders. We employ different augmentation techniques with
a denoising process to deal with scarce one-class data, which further improves
the performance (ROC AUC = 80\%). Furthermore, we introduce the dices dataset
that consists of over 2000 grayscale images of falling dices from multiple
perspectives, with 5\% of the images containing rare anomalies (e.g. drill
holes, sawing, or scratches). We evaluate our approach on the new dices dataset
using images from two different perspectives and also benchmark on the standard
MNIST dataset. Extensive experiments demonstrate that our proposed approach
exceeds the state-of-the-art on both the MNIST and dices datasets. To the best
of our knowledge, this is the first work that focuses on addressing
multi-perspective anomaly detection in images by jointly using different
perspectives together with one single objective function for anomaly detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Madan_M/0/1/0/all/0/1"&gt;Manav Madan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jakob_P/0/1/0/all/0/1"&gt;Peter Jakob&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmid_Schirling_T/0/1/0/all/0/1"&gt;Tobias Schmid-Schirling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1"&gt;Abhinav Valada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[M4Depth: A motion-based approach for monocular depth estimation on video sequences. (arXiv:2105.09847v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09847</id>
        <link href="http://arxiv.org/abs/2105.09847"/>
        <updated>2021-05-23T06:08:17.646Z</updated>
        <summary type="html"><![CDATA[Getting the distance to objects is crucial for autonomous vehicles. In
instances where depth sensors cannot be used, this distance has to be estimated
from RGB cameras. As opposed to cars, the task of estimating depth from
on-board mounted cameras is made complex on drones because of the lack of
constrains on motion during flights. %In the case of drones, this task is even
more complex than for car-mounted cameras since the camera motion is
unconstrained. In this paper, we present a method to estimate the distance of
objects seen by an on-board mounted camera by using its RGB video stream and
drone motion information. Our method is built upon a pyramidal convolutional
neural network architecture and uses time recurrence in pair with geometric
constraints imposed by motion to produce pixel-wise depth maps. %from a RGB
video stream of a camera attached to the drone In our architecture, each level
of the pyramid is designed to produce its own depth estimate based on past
observations and information provided by the previous level in the pyramid. We
introduce a spatial reprojection layer to maintain the spatio-temporal
consistency of the data between the levels. We analyse the performance of our
approach on Mid-Air, a public drone dataset featuring synthetic drone
trajectories recorded in a wide variety of unstructured outdoor environments.
Our experiments show that our network outperforms state-of-the-art depth
estimation methods and that the use of motion information is the main
contributing factor for this improvement. The code of our method is publicly
available on GitHub; see
$\href{https://github.com/michael-fonder/M4Depth}{\text{https://github.com/michael-fonder/M4Depth}}$]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fonder_M/0/1/0/all/0/1"&gt;Micha&amp;#xeb;l Fonder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ernst_D/0/1/0/all/0/1"&gt;Damien Ernst&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Droogenbroeck_M/0/1/0/all/0/1"&gt;Marc Van Droogenbroeck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[POCFormer: A Lightweight Transformer Architecture for Detection of COVID-19 Using Point of Care Ultrasound. (arXiv:2105.09913v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.09913</id>
        <link href="http://arxiv.org/abs/2105.09913"/>
        <updated>2021-05-23T06:08:17.622Z</updated>
        <summary type="html"><![CDATA[The rapid and seemingly endless expansion of COVID-19 can be traced back to
the inefficiency and shortage of testing kits that offer accurate results in a
timely manner. An emerging popular technique, which adopts improvements made in
mobile ultrasound technology, allows for healthcare professionals to conduct
rapid screenings on a large scale. We present an image-based solution that aims
at automating the testing process which allows for rapid mass testing to be
conducted with or without a trained medical professional that can be applied to
rural environments and third world countries. Our contributions towards rapid
large-scale testing include a novel deep learning architecture capable of
analyzing ultrasound data that can run in real-time and significantly improve
the current state-of-the-art detection accuracies using image-based COVID-19
detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Perera_S/0/1/0/all/0/1"&gt;Shehan Perera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Adhikari_S/0/1/0/all/0/1"&gt;Srikar Adhikari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yilmaz_A/0/1/0/all/0/1"&gt;Alper Yilmaz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepDarts: Modeling Keypoints as Objects for Automatic Scorekeeping in Darts using a Single Camera. (arXiv:2105.09880v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09880</id>
        <link href="http://arxiv.org/abs/2105.09880"/>
        <updated>2021-05-23T06:08:17.580Z</updated>
        <summary type="html"><![CDATA[Existing multi-camera solutions for automatic scorekeeping in steel-tip darts
are very expensive and thus inaccessible to most players. Motivated to develop
a more accessible low-cost solution, we present a new approach to keypoint
detection and apply it to predict dart scores from a single image taken from
any camera angle. This problem involves detecting multiple keypoints that may
be of the same class and positioned in close proximity to one another. The
widely adopted framework for regressing keypoints using heatmaps is not
well-suited for this task. To address this issue, we instead propose to model
keypoints as objects. We develop a deep convolutional neural network around
this idea and use it to predict dart locations and dartboard calibration points
within an overall pipeline for automatic dart scoring, which we call DeepDarts.
Additionally, we propose several task-specific data augmentation strategies to
improve the generalization of our method. As a proof of concept, two datasets
comprising 16k images originating from two different dartboard setups were
manually collected and annotated to evaluate the system. In the primary dataset
containing 15k images captured from a face-on view of the dartboard using a
smartphone, DeepDarts predicted the total score correctly in 94.7% of the test
images. In a second more challenging dataset containing limited training data
(830 images) and various camera angles, we utilize transfer learning and
extensive data augmentation to achieve a test accuracy of 84.0%. Because
DeepDarts relies only on single images, it has the potential to be deployed on
edge devices, giving anyone with a smartphone access to an automatic dart
scoring system for steel-tip darts. The code and datasets are available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McNally_W/0/1/0/all/0/1"&gt;William McNally&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walters_P/0/1/0/all/0/1"&gt;Pascale Walters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vats_K/0/1/0/all/0/1"&gt;Kanav Vats&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alexander Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McPhee_J/0/1/0/all/0/1"&gt;John McPhee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simple Transparent Adversarial Examples. (arXiv:2105.09685v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09685</id>
        <link href="http://arxiv.org/abs/2105.09685"/>
        <updated>2021-05-23T06:08:17.573Z</updated>
        <summary type="html"><![CDATA[There has been a rise in the use of Machine Learning as a Service (MLaaS)
Vision APIs as they offer multiple services including pre-built models and
algorithms, which otherwise take a huge amount of resources if built from
scratch. As these APIs get deployed for high-stakes applications, it's very
important that they are robust to different manipulations. Recent works have
only focused on typical adversarial attacks when evaluating the robustness of
vision APIs. We propose two new aspects of adversarial image generation methods
and evaluate them on the robustness of Google Cloud Vision API's optical
character recognition service and object detection APIs deployed in real-world
settings such as sightengine.com, picpurify.com, Google Cloud Vision API, and
Microsoft Azure's Computer Vision API. Specifically, we go beyond the
conventional small-noise adversarial attacks and introduce secret embedding and
transparent adversarial examples as a simpler way to evaluate robustness. These
methods are so straightforward that even non-specialists can craft such
attacks. As a result, they pose a serious threat where APIs are used for
high-stakes applications. Our transparent adversarial examples successfully
evade state-of-the art object detections APIs such as Azure Cloud Vision
(attack success rate 52%) and Google Cloud Vision (attack success rate 36%).
90% of the images have a secret embedded text that successfully fools the
vision of time-limited humans but is detected by Google Cloud Vision API's
optical character recognition. Complementing to current research, our results
provide simple but unconventional methods on robustness evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Borkar_J/0/1/0/all/0/1"&gt;Jaydeep Borkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Pin-Yu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical Study of Vehicle Re-Identification on the AI City Challenge. (arXiv:2105.09701v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09701</id>
        <link href="http://arxiv.org/abs/2105.09701"/>
        <updated>2021-05-23T06:08:17.562Z</updated>
        <summary type="html"><![CDATA[This paper introduces our solution for the Track2 in AI City Challenge 2021
(AICITY21). The Track2 is a vehicle re-identification (ReID) task with both the
real-world data and synthetic data. We mainly focus on four points, i.e.
training data, unsupervised domain-adaptive (UDA) training, post-processing,
model ensembling in this challenge. (1) Both cropping training data and using
synthetic data can help the model learn more discriminative features. (2) Since
there is a new scenario in the test set that dose not appear in the training
set, UDA methods perform well in the challenge. (3) Post-processing techniques
including re-ranking, image-to-track retrieval, inter-camera fusion, etc,
significantly improve final performance. (4) We ensemble CNN-based models and
transformer-based models which provide different representation diversity. With
aforementioned techniques, our method finally achieves 0.7445 mAP score,
yielding the first place in the competition. Codes are available at
https://github.com/michuanhaohao/AICITY2021_Track2_DMT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Hao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weihua Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xianzhe Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jianyang Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yiqi Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shuting He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Content-adaptive Representation Learning for Fast Image Super-resolution. (arXiv:2105.09645v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09645</id>
        <link href="http://arxiv.org/abs/2105.09645"/>
        <updated>2021-05-23T06:08:17.554Z</updated>
        <summary type="html"><![CDATA[Deep convolutional networks have attracted great attention in image
restoration and enhancement. Generally, restoration quality has been improved
by building more and more convolutional block. However, these methods mostly
learn a specific model to handle all images and ignore difficulty diversity. In
other words, an area in the image with high frequency tend to lose more
information during compressing while an area with low frequency tends to lose
less. In this article, we adrress the efficiency issue in image SR by
incorporating a patch-wise rolling network(PRN) to content-adaptively recover
images according to difficulty levels. In contrast to existing studies that
ignore difficulty diversity, we adopt different stage of a neural network to
perform image restoration. In addition, we propose a rolling strategy that
utilizes the parameters of each stage more flexible. Extensive experiments
demonstrate that our model not only shows a significant acceleration but also
maintain state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yukai Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1"&gt;Jinghui Qin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Medical Image Segmentation using Squeeze-and-Expansion Transformers. (arXiv:2105.09511v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.09511</id>
        <link href="http://arxiv.org/abs/2105.09511"/>
        <updated>2021-05-23T06:08:17.533Z</updated>
        <summary type="html"><![CDATA[Medical image segmentation is important for computer-aided diagnosis. Good
segmentation demands the model to see the big picture and fine details
simultaneously, i.e., to learn image features that incorporate large context
while keep high spatial resolutions. To approach this goal, the most widely
used methods -- U-Net and variants, extract and fuse multi-scale features.
However, the fused features still have small "effective receptive fields" with
a focus on local image cues, limiting their performance. In this work, we
propose Segtran, an alternative segmentation framework based on transformers,
which have unlimited "effective receptive fields" even at high feature
resolutions. The core of Segtran is a novel Squeeze-and-Expansion transformer:
a squeezed attention block regularizes the self attention of transformers, and
an expansion block learns diversified representations. Additionally, we propose
a new positional encoding scheme for transformers, imposing a continuity
inductive bias for images. Experiments were performed on 2D and 3D medical
image segmentation tasks: optic disc/cup segmentation in fundus images
(REFUGE'20 challenge), polyp segmentation in colonoscopy images, and brain
tumor segmentation in MRI scans (BraTS'19 challenge). Compared with
representative existing methods, Segtran consistently achieved the highest
segmentation accuracy, and exhibited good cross-domain generalization
capabilities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1"&gt;Shaohua Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sui_X/0/1/0/all/0/1"&gt;Xiuchao Sui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiangde Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xinxing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Goh_R/0/1/0/all/0/1"&gt;Rick Siow Mong Goh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Covid-19 Detection from Chest X-ray and Patient Metadata using Graph Convolutional Neural Networks. (arXiv:2105.09720v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.09720</id>
        <link href="http://arxiv.org/abs/2105.09720"/>
        <updated>2021-05-23T06:08:17.526Z</updated>
        <summary type="html"><![CDATA[The novel corona virus (Covid-19) has introduced significant challenges due
to its rapid spreading nature through respiratory transmission. As a result,
there is a huge demand for Artificial Intelligence (AI) based quick disease
diagnosis methods as an alternative to high demand tests such as Polymerase
Chain Reaction (PCR). Chest X-ray (CXR) Image analysis is such cost-effective
radiography technique due to resource availability and quick screening. But, a
sufficient and systematic data collection that is required by complex deep
leaning (DL) models is more difficult and hence there are recent efforts that
utilize transfer learning to address this issue. Still these transfer learnt
models suffer from lack of generalization and increased bias to the training
dataset resulting poor performance for unseen data. Limited correlation of the
transferred features from the pre-trained model to a specific medical imaging
domain like X-ray and overfitting on fewer data can be reasons for this
circumstance. In this work, we propose a novel Graph Convolution Neural Network
(GCN) that is capable of identifying bio-markers of Covid-19 pneumonia from CXR
images and meta information about patients. The proposed method exploits
important relational knowledge between data instances and their features using
graph representation and applies convolution to learn the graph data which is
not possible with conventional convolution on Euclidean domain. The results of
extensive experiments of proposed model on binary (Covid vs normal) and three
class (Covid, normal, other pneumonia) classification problems outperform
different benchmark transfer learnt models, hence overcoming the aforementioned
drawbacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mudiyanselage_T/0/1/0/all/0/1"&gt;Thosini Bamunu Mudiyanselage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Senanayake_N/0/1/0/all/0/1"&gt;Nipuna Senanayake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ji_C/0/1/0/all/0/1"&gt;Chunyan Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1"&gt;Yi Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yanqing Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DPN-SENet:A self-attention mechanism neural network for detection and diagnosis of COVID-19 from chest x-ray images. (arXiv:2105.09683v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.09683</id>
        <link href="http://arxiv.org/abs/2105.09683"/>
        <updated>2021-05-23T06:08:17.519Z</updated>
        <summary type="html"><![CDATA[Background and Objective: The new type of coronavirus is also called
COVID-19. It began to spread at the end of 2019 and has now spread across the
world. Until October 2020, It has infected around 37 million people and claimed
about 1 million lives. We propose a deep learning model that can help
radiologists and clinicians use chest X-rays to diagnose COVID-19 cases and
show the diagnostic features of pneumonia. Methods: The approach in this study
is: 1) we propose a data enhancement method to increase the diversity of the
data set, thereby improving the generalization performance of the model. 2) Our
deep convolution neural network model DPN-SE adds a self-attention mechanism to
the DPN network. The addition of a self-attention mechanism has greatly
improved the performance of the network. 3) Use the Lime interpretable library
to mark the feature regions on the X-ray medical image that helps doctors more
quickly diagnose COVID-19 in people. Results: Under the same network model, the
data with and without data enhancement is put into the model for training
respectively. At last, comparing two experimental results: among the 10 network
models with different structures, 7 network models have improved their effects
after using data enhancement, with an average improvement of 1% in recognition
accuracy. We propose that the accuracy and recall rates of the DPN-SE network
are 93% and 98% of cases (COVID vs. pneumonia bacteria vs. viral pneumonia vs.
normal). Compared with the original DPN, the respective accuracy is improved by
2%. Conclusion: The data augmentation method we used has achieved effective
results on a small amount of data set, showing that a reasonable data
augmentation method can improve the recognition accuracy without changing the
sample size and model structure. Overall, the proposed method and model can
effectively become a very useful tool for clinical radiologists.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_B/0/1/0/all/0/1"&gt;Bo Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xue_R/0/1/0/all/0/1"&gt;Ruhui Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Laili Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xiang_W/0/1/0/all/0/1"&gt;Wei Xiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FVC: A New Framework towards Deep Video Compression in Feature Space. (arXiv:2105.09600v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.09600</id>
        <link href="http://arxiv.org/abs/2105.09600"/>
        <updated>2021-05-23T06:08:17.511Z</updated>
        <summary type="html"><![CDATA[Learning based video compression attracts increasing attention in the past
few years. The previous hybrid coding approaches rely on pixel space operations
to reduce spatial and temporal redundancy, which may suffer from inaccurate
motion estimation or less effective motion compensation. In this work, we
propose a feature-space video coding network (FVC) by performing all major
operations (i.e., motion estimation, motion compression, motion compensation
and residual compression) in the feature space. Specifically, in the proposed
deformable compensation module, we first apply motion estimation in the feature
space to produce motion information (i.e., the offset maps), which will be
compressed by using the auto-encoder style network. Then we perform motion
compensation by using deformable convolution and generate the predicted
feature. After that, we compress the residual feature between the feature from
the current frame and the predicted feature from our deformable compensation
module. For better frame reconstruction, the reference features from multiple
previous reconstructed frames are also fused by using the non-local attention
mechanism in the multi-frame feature fusion module. Comprehensive experimental
results demonstrate that the proposed framework achieves the state-of-the-art
performance on four benchmark datasets including HEVC, UVG, VTL and MCL-JCV.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhihao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_G/0/1/0/all/0/1"&gt;Guo Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1"&gt;Dong Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Connected Component Labelling algorithm for multi-pixel per clock cycle video strea. (arXiv:2105.09658v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09658</id>
        <link href="http://arxiv.org/abs/2105.09658"/>
        <updated>2021-05-23T06:08:17.504Z</updated>
        <summary type="html"><![CDATA[This work describes the hardware implementation of a connected component
labelling (CCL) module in reprogammable logic. The main novelty of the design
is the "full", i.e. without any simplifications, support of a 4 pixel per clock
format (4 ppc) and real-time processing of a 4K/UltraHD video stream (3840 x
2160 pixels) at 60 frames per second. To achieve this, a special labelling
method was designed and a functionality that stops the input data stream in
order to process pixel groups which require writing more than one merger into
the equivalence table. The proposed module was verified in simulation and in
hardware on the Xilinx Zynq Ultrascale+ MPSoC chip on the ZCU104 evaluation
board.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kowalczyk_M/0/1/0/all/0/1"&gt;Marcin Kowalczyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kryjak_T/0/1/0/all/0/1"&gt;Tomasz Kryjak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic segmentation of multispectral photoacoustic images using deep learning. (arXiv:2105.09624v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.09624</id>
        <link href="http://arxiv.org/abs/2105.09624"/>
        <updated>2021-05-23T06:08:17.485Z</updated>
        <summary type="html"><![CDATA[Photoacoustic imaging has the potential to revolutionise healthcare due to
the valuable information on tissue physiology that is contained in
multispectral photoacoustic measurements. Clinical translation of the
technology requires conversion of the high-dimensional acquired data into
clinically relevant and interpretable information. In this work, we present a
deep learning-based approach to semantic segmentation of multispectral
photoacoustic images to facilitate the interpretability of recorded images.
Manually annotated multispectral photoacoustic imaging data are used as gold
standard reference annotations and enable the training of a deep learning-based
segmentation algorithm in a supervised manner. Based on a validation study with
experimentally acquired data of healthy human volunteers, we show that
automatic tissue segmentation can be used to create powerful analyses and
visualisations of multispectral photoacoustic images. Due to the intuitive
representation of high-dimensional information, such a processing algorithm
could be a valuable means to facilitate the clinical translation of
photoacoustic imaging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Grohl_J/0/1/0/all/0/1"&gt;Janek Gr&amp;#xf6;hl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schellenberg_M/0/1/0/all/0/1"&gt;Melanie Schellenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dreher_K/0/1/0/all/0/1"&gt;Kris Dreher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Holzwarth_N/0/1/0/all/0/1"&gt;Niklas Holzwarth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tizabi_M/0/1/0/all/0/1"&gt;Minu D. Tizabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Seitel_A/0/1/0/all/0/1"&gt;Alexander Seitel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Maier_Hein_L/0/1/0/all/0/1"&gt;Lena Maier-Hein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised learning of text line segmentationby differentiating coarse patterns. (arXiv:2105.09405v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09405</id>
        <link href="http://arxiv.org/abs/2105.09405"/>
        <updated>2021-05-23T06:08:17.479Z</updated>
        <summary type="html"><![CDATA[Despite recent advances in the field of supervised deep learning for text
line segmentation, unsupervised deep learning solutions are beginning to gain
popularity. In this paper, we present an unsupervised deep learning method that
embeds document image patches to a compact Euclidean space where distances
correspond to a coarse text line pattern similarity. Once this space has been
produced, text line segmentation can be easily implemented using standard
techniques with the embedded feature vectors. To train the model, we extract
random pairs of document image patches with the assumption that neighbour
patches contain a similar coarse trend of text lines, whereas if one of them is
rotated, they contain different coarse trends of text lines. Doing well on this
task requires the model to learn to recognize the text lines and their salient
parts. The benefit of our approach is zero manual labelling effort. We evaluate
the method qualitatively and quantitatively on several variants of text line
segmentation datasets to demonstrate its effectivity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barakat_B/0/1/0/all/0/1"&gt;Berat Kurar Barakat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Droby_A/0/1/0/all/0/1"&gt;Ahmad Droby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saabni_R/0/1/0/all/0/1"&gt;Raid Saabni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Sana_J/0/1/0/all/0/1"&gt;Jihad El-Sana&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intra-Model Collaborative Learning of Neural Networks. (arXiv:2105.09590v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09590</id>
        <link href="http://arxiv.org/abs/2105.09590"/>
        <updated>2021-05-23T06:08:17.472Z</updated>
        <summary type="html"><![CDATA[Recently, collaborative learning proposed by Song and Chai has achieved
remarkable improvements in image classification tasks by simultaneously
training multiple classifier heads. However, huge memory footprints required by
such multi-head structures may hinder the training of large-capacity baseline
models. The natural question is how to achieve collaborative learning within a
single network without duplicating any modules. In this paper, we propose four
ways of collaborative learning among different parts of a single network with
negligible engineering efforts. To improve the robustness of the network, we
leverage the consistency of the output layer and intermediate layers for
training under the collaborative learning framework. Besides, the similarity of
intermediate representation and convolution kernel is also introduced to reduce
the reduce redundant in a neural network. Compared to the method of Song and
Chai, our framework further considers the collaboration inside a single model
and takes smaller overhead. Extensive experiments on Cifar-10, Cifar-100,
ImageNet32 and STL-10 corroborate the effectiveness of these four ways
separately while combining them leads to further improvements. In particular,
test errors on the STL-10 dataset are decreased by $9.28\%$ and $5.45\%$ for
ResNet-18 and VGG-16 respectively. Moreover, our method is proven to be robust
to label noise with experiments on Cifar-10 dataset. For example, our method
has $3.53\%$ higher performance under $50\%$ noise ratio setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1"&gt;Shijie Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tong Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized Few-Shot Object Detection without Forgetting. (arXiv:2105.09491v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09491</id>
        <link href="http://arxiv.org/abs/2105.09491"/>
        <updated>2021-05-23T06:08:17.466Z</updated>
        <summary type="html"><![CDATA[Recently few-shot object detection is widely adopted to deal with
data-limited situations. While most previous works merely focus on the
performance on few-shot categories, we claim that detecting all classes is
crucial as test samples may contain any instances in realistic applications,
which requires the few-shot detector to learn new concepts without forgetting.
Through analysis on transfer learning based methods, some neglected but
beneficial properties are utilized to design a simple yet effective few-shot
detector, Retentive R-CNN. It consists of Bias-Balanced RPN to debias the
pretrained RPN and Re-detector to find few-shot class objects without
forgetting previous knowledge. Extensive experiments on few-shot detection
benchmarks show that Retentive R-CNN significantly outperforms state-of-the-art
methods on overall performance among all settings as it can achieve competitive
results on few-shot classes and does not degrade the base class performance at
all. Our approach has demonstrated that the long desired never-forgetting
learner is available in object detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1"&gt;Zhibo Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yuchen Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zeming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jian Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VTNet: Visual Transformer Network for Object Goal Navigation. (arXiv:2105.09447v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09447</id>
        <link href="http://arxiv.org/abs/2105.09447"/>
        <updated>2021-05-23T06:08:17.458Z</updated>
        <summary type="html"><![CDATA[Object goal navigation aims to steer an agent towards a target object based
on observations of the agent. It is of pivotal importance to design effective
visual representations of the observed scene in determining navigation actions.
In this paper, we introduce a Visual Transformer Network (VTNet) for learning
informative visual representation in navigation. VTNet is a highly effective
structure that embodies two key properties for visual representations: First,
the relationships among all the object instances in a scene are exploited;
Second, the spatial locations of objects and image regions are emphasized so
that directional navigation signals can be learned. Furthermore, we also
develop a pre-training scheme to associate the visual representations with
navigation signals, and thus facilitate navigation policy learning. In a
nutshell, VTNet embeds object and region features with their location cues as
spatial-aware descriptors and then incorporates all the encoded descriptors
through attention operations to achieve informative representation for
navigation. Given such visual representations, agents are able to explore the
correlations between visual observations and navigation actions. For example,
an agent would prioritize "turning right" over "turning left" when the visual
representation emphasizes on the right side of activation map. Experiments in
the artificial environment AI2-Thor demonstrate that VTNet significantly
outperforms state-of-the-art methods in unseen testing environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1"&gt;Heming Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Liang Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Crowd Counting by Self-supervised Transfer Colorization Learning and Global Prior Classification. (arXiv:2105.09684v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09684</id>
        <link href="http://arxiv.org/abs/2105.09684"/>
        <updated>2021-05-23T06:08:17.452Z</updated>
        <summary type="html"><![CDATA[Labeled crowd scene images are expensive and scarce. To significantly reduce
the requirement of the labeled images, we propose ColorCount, a novel CNN-based
approach by combining self-supervised transfer colorization learning and global
prior classification to leverage the abundantly available unlabeled data. The
self-supervised colorization branch learns the semantics and surface texture of
the image by using its color components as pseudo labels. The classification
branch extracts global group priors by learning correlations among image
clusters. Their fused resultant discriminative features (global priors,
semantics and textures) provide ample priors for counting, hence significantly
reducing the requirement of labeled images. We conduct extensive experiments on
four challenging benchmarks. ColorCount achieves much better performance as
compared with other unsupervised approaches. Its performance is close to the
supervised baseline with substantially less labeled data (10\% of the original
one).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1"&gt;Haoyue Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1"&gt;Song Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1"&gt;S.-H. Gary Chan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classifying concepts via visual properties. (arXiv:2105.09422v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2105.09422</id>
        <link href="http://arxiv.org/abs/2105.09422"/>
        <updated>2021-05-23T06:08:17.426Z</updated>
        <summary type="html"><![CDATA[We assume that substances in the world are represented by two types of
concepts, namely substance concepts and classification concepts, the former
instrumental to (visual) perception, the latter to (language based)
classification. Based on this distinction, we introduce a general methodology
for building lexico-semantic hierarchies of substance concepts, where nodes are
annotated with the media, e.g.,videos or photos, from which substance concepts
are extracted, and are associated with the corresponding classification
concepts. The methodology is based on Ranganathan's original faceted approach,
contextualized to the problem of classifying substance concepts. The key
novelty is that the hierarchy is built exploiting the visual properties of
substance concepts, while the linguistically defined properties of
classification concepts are only used to describe substance concepts. The
validity of the approach is exemplified by providing some highlights of an
ongoing project whose goal is to build a large scale multimedia multilingual
concept hierarchy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Giunchiglia_F/0/1/0/all/0/1"&gt;Fausto Giunchiglia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bagchi_M/0/1/0/all/0/1"&gt;Mayukh Bagchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Attractor-Guided Neural Networks for Skeleton-Based Human Motion Prediction. (arXiv:2105.09711v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09711</id>
        <link href="http://arxiv.org/abs/2105.09711"/>
        <updated>2021-05-23T06:08:17.418Z</updated>
        <summary type="html"><![CDATA[Joint relation modeling is a curial component in human motion prediction.
Most existing methods tend to design skeletal-based graphs to build the
relations among joints, where local interactions between joint pairs are well
learned. However, the global coordination of all joints, which reflects human
motion's balance property, is usually weakened because it is learned from part
to whole progressively and asynchronously. Thus, the final predicted motions
are sometimes unnatural. To tackle this issue, we learn a medium, called
balance attractor (BA), from the spatiotemporal features of motion to
characterize the global motion features, which is subsequently used to build
new joint relations. Through the BA, all joints are related synchronously, and
thus the global coordination of all joints can be better learned. Based on the
BA, we propose our framework, referred to Attractor-Guided Neural Network,
mainly including Attractor-Based Joint Relation Extractor (AJRE) and
Multi-timescale Dynamics Extractor (MTDE). The AJRE mainly includes Global
Coordination Extractor (GCE) and Local Interaction Extractor (LIE). The former
presents the global coordination of all joints, and the latter encodes local
interactions between joint pairs. The MTDE is designed to extract dynamic
information from raw position information for effective prediction. Extensive
experiments show that the proposed framework outperforms state-of-the-art
methods in both short and long-term predictions in H3.6M, CMU-Mocap, and 3DPW.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_P/0/1/0/all/0/1"&gt;Pengxiang Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1"&gt;Jianqin Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anabranch Network for Camouflaged Object Segmentation. (arXiv:2105.09451v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09451</id>
        <link href="http://arxiv.org/abs/2105.09451"/>
        <updated>2021-05-23T06:08:17.411Z</updated>
        <summary type="html"><![CDATA[Camouflaged objects attempt to conceal their texture into the background and
discriminating them from the background is hard even for human beings. The main
objective of this paper is to explore the camouflaged object segmentation
problem, namely, segmenting the camouflaged object(s) for a given image. This
problem has not been well studied in spite of a wide range of potential
applications including the preservation of wild animals and the discovery of
new species, surveillance systems, search-and-rescue missions in the event of
natural disasters such as earthquakes, floods or hurricanes. This paper
addresses a new challenging problem of camouflaged object segmentation. To
address this problem, we provide a new image dataset of camouflaged objects for
benchmarking purposes. In addition, we propose a general end-to-end network,
called the Anabranch Network, that leverages both classification and
segmentation tasks. Different from existing networks for segmentation, our
proposed network possesses the second branch for classification to predict the
probability of containing camouflaged object(s) in an image, which is then
fused into the main branch for segmentation to boost up the segmentation
accuracy. Extensive experiments conducted on the newly built dataset
demonstrate the effectiveness of our network using various fully convolutional
networks. \url{https://sites.google.com/view/ltnghia/research/camo}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1"&gt;Trung-Nghia Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Tam V. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_Z/0/1/0/all/0/1"&gt;Zhongliang Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1"&gt;Minh-Triet Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugimoto_A/0/1/0/all/0/1"&gt;Akihiro Sugimoto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepCAD: A Deep Generative Network for Computer-Aided Design Models. (arXiv:2105.09492v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09492</id>
        <link href="http://arxiv.org/abs/2105.09492"/>
        <updated>2021-05-23T06:08:17.404Z</updated>
        <summary type="html"><![CDATA[Deep generative models of 3D shapes have received a great deal of research
interest. Yet, almost all of them generate discrete shape representations, such
as voxels, point clouds, and polygon meshes. We present the first 3D generative
model for a drastically different shape representation -- describing a shape as
a sequence of computer-aided design (CAD) operations. Unlike meshes and point
clouds, CAD models encode the user creation process of 3D shapes, widely used
in numerous industrial and engineering design tasks. However, the sequential
and irregular structure of CAD operations poses significant challenges for
existing 3D generative models. Drawing an analogy between CAD operations and
natural language, we propose a CAD generative network based on the Transformer.
We demonstrate the performance of our model for both shape autoencoding and
random shape generation. To train our network, we create a new CAD dataset
consisting of 179,133 models and their CAD construction sequences. We have made
this dataset publicly available to promote future research on this topic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1"&gt;Rundi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Chang Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Changxi Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A low-rank representation for unsupervised registration of medical images. (arXiv:2105.09548v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09548</id>
        <link href="http://arxiv.org/abs/2105.09548"/>
        <updated>2021-05-23T06:08:17.397Z</updated>
        <summary type="html"><![CDATA[Registration networks have shown great application potentials in medical
image analysis. However, supervised training methods have a great demand for
large and high-quality labeled datasets, which is time-consuming and sometimes
impractical due to data sharing issues. Unsupervised image registration
algorithms commonly employ intensity-based similarity measures as loss
functions without any manual annotations. These methods estimate the
parameterized transformations between pairs of moving and fixed images through
the optimization of the network parameters during training. However, these
methods become less effective when the image quality varies, e.g., some images
are corrupted by substantial noise or artifacts. In this work, we propose a
novel approach based on a low-rank representation, i.e., Regnet-LRR, to tackle
the problem. We project noisy images into a noise-free low-rank space, and then
compute the similarity between the images. Based on the low-rank similarity
measure, we train the registration network to predict the dense deformation
fields of noisy image pairs. We highlight that the low-rank projection is
reformulated in a way that the registration network can successfully update
gradients. With two tasks, i.e., cardiac and abdominal intra-modality
registration, we demonstrate that the low-rank representation can boost the
generalization ability and robustness of models as well as bring significant
improvements in noisy data registration scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_D/0/1/0/all/0/1"&gt;Dengqiang Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1"&gt;Shangqi Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qunlong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xinzhe Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1"&gt;Xiahai Zhuang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[More Than Just Attention: Learning Cross-Modal Attentions with Contrastive Constraints. (arXiv:2105.09597v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09597</id>
        <link href="http://arxiv.org/abs/2105.09597"/>
        <updated>2021-05-23T06:08:17.378Z</updated>
        <summary type="html"><![CDATA[Attention mechanisms have been widely applied to cross-modal tasks such as
image captioning and information retrieval, and have achieved remarkable
improvements due to its capability to learn fine-grained relevance across
different modalities. However, existing attention models could be sub-optimal
and lack preciseness because there is no direct supervision involved during
training. In this work, we propose Contrastive Content Re-sourcing (CCR) and
Contrastive Content Swapping (CCS) constraints to address such limitation.
These constraints supervise the training of attention models in a contrastive
learning manner without requiring explicit attention annotations. Additionally,
we introduce three metrics, namely Attention Precision, Recall and F1-Score, to
quantitatively evaluate the attention quality. We evaluate the proposed
constraints with cross-modal retrieval (image-text matching) task. The
experiments on both Flickr30k and MS-COCO datasets demonstrate that integrating
these attention constraints into two state-of-the-art attention-based models
improves the model performance in terms of both retrieval accuracy and
attention metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuxiao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Jianbo Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Long Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1"&gt;Rui Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1"&gt;Larry Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1"&gt;Dimitris N. Metaxas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Superpixel-based Domain-Knowledge Infusion in Computer Vision. (arXiv:2105.09448v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09448</id>
        <link href="http://arxiv.org/abs/2105.09448"/>
        <updated>2021-05-23T06:08:17.371Z</updated>
        <summary type="html"><![CDATA[Superpixels are higher-order perceptual groups of pixels in an image, often
carrying much more information than raw pixels. There is an inherent relational
structure to the relationship among different superpixels of an image. This
relational information can convey some form of domain information about the
image, e.g. relationship between superpixels representing two eyes in a cat
image. Our interest in this paper is to construct computer vision models,
specifically those based on Deep Neural Networks (DNNs) to incorporate these
superpixels information. We propose a methodology to construct a hybrid model
that leverages (a) Convolutional Neural Network (CNN) to deal with spatial
information in an image, and (b) Graph Neural Network (GNN) to deal with
relational superpixel information in the image. The proposed deep model is
learned using a generic hybrid loss function that we call a `hybrid' loss. We
evaluate the predictive performance of our proposed hybrid vision model on four
popular image classification datasets: MNIST, FMNIST, CIFAR-10 and CIFAR-100.
Moreover, we evaluate our method on three real-world classification tasks:
COVID-19 X-Ray Detection, LFW Face Recognition, and SOCOFing Fingerprint
Identification. The results demonstrate that the relational superpixel
information provided via a GNN could improve the performance of standard
CNN-based vision systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1"&gt;Gunjan Chhablani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Abheesht Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1"&gt;Harshit Pandey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dash_T/0/1/0/all/0/1"&gt;Tirtharaj Dash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Content-Augmented Feature Pyramid Network with Light Linear Transformers. (arXiv:2105.09464v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09464</id>
        <link href="http://arxiv.org/abs/2105.09464"/>
        <updated>2021-05-23T06:08:17.363Z</updated>
        <summary type="html"><![CDATA[Recently, plenty of work has tried to introduce transformers into computer
vision tasks, with good results. Unlike classic convolution networks, which
extract features within a local receptive field, transformers can adaptively
aggregate similar features from a global view using self-attention mechanism.
For object detection, Feature Pyramid Network (FPN) proposes feature
interaction across layers and proves its extremely importance. However, its
interaction is still in a local manner, which leaves a lot of room for
improvement. Since transformer was originally designed for NLP tasks, adapting
processing subject directly from text to image will cause unaffordable
computation and space overhead. In this paper, we utilize a linearized
attention function to overcome above problems and build a novel architecture,
named Content-Augmented Feature Pyramid Network (CA-FPN), which proposes a
global content extraction module and deeply combines with FPN through light
linear transformers. What's more, light transformers can further make the
application of multi-head attention mechanism easier. Most importantly, our
CA-FPN can be readily plugged into existing FPN-based models. Extensive
experiments on the challenging COCO object detection dataset demonstrated that
our CA-FPN significantly outperforms competitive baselines without bells and
whistles. Code will be made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yongxiang Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1"&gt;Xiaolin Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1"&gt;Yuncong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lu Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AGSFCOS: Based on attention mechanism and Scale-Equalizing pyramid network of object detection. (arXiv:2105.09596v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09596</id>
        <link href="http://arxiv.org/abs/2105.09596"/>
        <updated>2021-05-23T06:08:17.355Z</updated>
        <summary type="html"><![CDATA[Recently, the anchor-free object detection model has shown great potential
for accuracy and speed to exceed anchor-based object detection. Therefore, two
issues are mainly studied in this article: (1) How to let the backbone network
in the anchor-free object detection model learn feature extraction? (2) How to
make better use of the feature pyramid network? In order to solve the above
problems, Experiments show that our model has a certain improvement in accuracy
compared with the current popular detection models on the COCO dataset, the
designed attention mechanism module can capture contextual information well,
improve detection accuracy, and use sepc network to help balance abstract and
detailed information, and reduce the problem of semantic gap in the feature
pyramid network. Whether it is anchor-based network model YOLOv3, Faster RCNN,
or anchor-free network model Foveabox, FSAF, FCOS. Our optimal model can get
39.5% COCO AP under the background of ResNet50.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Li Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1"&gt;Wei Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_R/0/1/0/all/0/1"&gt;Ruhui Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_K/0/1/0/all/0/1"&gt;Kaida Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Laili Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Egocentric Activity Recognition and Localization on a 3D Map. (arXiv:2105.09544v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09544</id>
        <link href="http://arxiv.org/abs/2105.09544"/>
        <updated>2021-05-23T06:08:17.347Z</updated>
        <summary type="html"><![CDATA[Given a video captured from a first person perspective and recorded in a
familiar environment, can we recognize what the person is doing and identify
where the action occurs in the 3D space? We address this challenging problem of
jointly recognizing and localizing actions of a mobile user on a known 3D map
from egocentric videos. To this end, we propose a novel deep probabilistic
model. Our model takes the inputs of a Hierarchical Volumetric Representation
(HVR) of the environment and an egocentric video, infers the 3D action location
as a latent variable, and recognizes the action based on the video and
contextual cues surrounding its potential locations. To evaluate our model, we
conduct extensive experiments on a newly collected egocentric video dataset, in
which both human naturalistic actions and photo-realistic 3D environment
reconstructions are captured. Our method demonstrates strong results on both
action recognition and 3D action localization across seen and unseen
environments. We believe our work points to an exciting research direction in
the intersection of egocentric vision, and 3D scene understanding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Miao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lingni Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Somasundaram_K/0/1/0/all/0/1"&gt;Kiran Somasundaram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1"&gt;Kristen Grauman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1"&gt;James M. Rehg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Unsupervised Document Image Blind Denoising. (arXiv:2105.09437v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09437</id>
        <link href="http://arxiv.org/abs/2105.09437"/>
        <updated>2021-05-23T06:08:17.320Z</updated>
        <summary type="html"><![CDATA[Removing noise from scanned pages is a vital step before their submission to
optical character recognition (OCR) system. Most available image denoising
methods are supervised where the pairs of noisy/clean pages are required.
However, this assumption is rarely met in real settings. Besides, there is no
single model that can remove various noise types from documents. Here, we
propose a unified end-to-end unsupervised deep learning model, for the first
time, that can effectively remove multiple types of noise, including salt \&
pepper noise, blurred and/or faded text, as well as watermarks from documents
at various levels of intensity. We demonstrate that the proposed model
significantly improves the quality of scanned images and the OCR of the pages
on several test datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gangeh_M/0/1/0/all/0/1"&gt;Mehrdad J Gangeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plata_M/0/1/0/all/0/1"&gt;Marcin Plata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Motahari_H/0/1/0/all/0/1"&gt;Hamid Motahari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duffy_N/0/1/0/all/0/1"&gt;Nigel P Duffy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heterogeneous Contrastive Learning. (arXiv:2105.09401v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09401</id>
        <link href="http://arxiv.org/abs/2105.09401"/>
        <updated>2021-05-23T06:08:17.301Z</updated>
        <summary type="html"><![CDATA[With the advent of big data across multiple high-impact applications, we are
often facing the challenge of complex heterogeneity. The newly collected data
usually consist of multiple modalities and characterized with multiple labels,
thus exhibiting the co-existence of multiple types of heterogeneity. Although
state-of-the-art techniques are good at modeling the complex heterogeneity with
sufficient label information, such label information can be quite expensive to
obtain in real applications, leading to sub-optimal performance using these
techniques. Inspired by the capability of contrastive learning to utilize rich
unlabeled data for improving performance, in this paper, we propose a unified
heterogeneous learning framework, which combines both weighted unsupervised
contrastive loss and weighted supervised contrastive loss to model multiple
types of heterogeneity. We also provide theoretical analyses showing that the
proposed weighted supervised contrastive loss is the lower bound of the mutual
information of two samples from the same class and the weighted unsupervised
contrastive loss is the lower bound of the mutual information between the
hidden representation of two views of the same sample. Experimental results on
real-world data sets demonstrate the effectiveness and the efficiency of the
proposed method modeling multiple types of heterogeneity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Lecheng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yada Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jingrui He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1"&gt;Jinjun Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Birds of a Feather: Capturing Avian Shape Models from Images. (arXiv:2105.09396v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09396</id>
        <link href="http://arxiv.org/abs/2105.09396"/>
        <updated>2021-05-23T06:08:17.274Z</updated>
        <summary type="html"><![CDATA[Animals are diverse in shape, but building a deformable shape model for a new
species is not always possible due to the lack of 3D data. We present a method
to capture new species using an articulated template and images of that
species. In this work, we focus mainly on birds. Although birds represent
almost twice the number of species as mammals, no accurate shape model is
available. To capture a novel species, we first fit the articulated template to
each training sample. By disentangling pose and shape, we learn a shape space
that captures variation both among species and within each species from image
evidence. We learn models of multiple species from the CUB dataset, and
contribute new species-specific and multi-species shape models that are useful
for downstream reconstruction tasks. Using a low-dimensional embedding, we show
that our learned 3D shape space better reflects the phylogenetic relationships
among birds than learned perceptual features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yufu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolotouros_N/0/1/0/all/0/1"&gt;Nikos Kolotouros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1"&gt;Kostas Daniilidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badger_M/0/1/0/all/0/1"&gt;Marc Badger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust partial Fourier reconstruction for diffusion-weighted imaging using a recurrent convolutional neural network. (arXiv:2105.09378v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.09378</id>
        <link href="http://arxiv.org/abs/2105.09378"/>
        <updated>2021-05-23T06:08:17.262Z</updated>
        <summary type="html"><![CDATA[Purpose: To develop an algorithm for robust partial Fourier (PF)
reconstruction applicable to diffusion-weighted (DW) images with non-smooth
phase variations.

Methods: Based on an unrolled proximal splitting algorithm, a neural network
architecture is derived which alternates between data consistency operations
and regularization implemented by recurrent convolutions. In order to exploit
correlations, multiple repetitions of the same slice are jointly reconstructed
under consideration of permutation-equivariance. The proposed method is trained
on DW liver data of 60 volunteers and evaluated on retrospectively and
prospectively sub-sampled data of different anatomies and resolutions. In
addition, the benefits of using a recurrent network over other unrolling
strategies is investigated.

Results: Conventional PF techniques can be significantly outperformed in
terms of quantitative measures as well as perceptual image quality. The
proposed method is able to generalize well to brain data with contrasts and
resolution not present in the training set. The reduction in echo time (TE)
associated with prospective PF-sampling enables DW imaging with higher signal.
Also, the TE increase in acquisitions with higher resolution can be compensated
for. It can be shown that unrolling by means of a recurrent network produced
better results than using a weight-shared network or a cascade of networks.

Conclusion: This work demonstrates that robust PF reconstruction of DW data
is feasible even at strong PF factors in applications with severe phase
variations. Since the proposed method does not rely on smoothness priors of the
phase but uses learned recurrent convolutions instead, artifacts of
conventional PF methods can be avoided.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gadjimuradov_F/0/1/0/all/0/1"&gt;Fasil Gadjimuradov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Benkert_T/0/1/0/all/0/1"&gt;Thomas Benkert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nickel_M/0/1/0/all/0/1"&gt;Marcel Dominik Nickel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1"&gt;Andreas Maier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Endless Loops: Detecting and Animating Periodic Patterns in Still Images. (arXiv:2105.09374v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2105.09374</id>
        <link href="http://arxiv.org/abs/2105.09374"/>
        <updated>2021-05-23T06:08:17.231Z</updated>
        <summary type="html"><![CDATA[We present an algorithm for producing a seamless animated loop from a single
image. The algorithm detects periodic structures, such as the windows of a
building or the steps of a staircase, and generates a non-trivial displacement
vector field that maps each segment of the structure onto a neighboring segment
along a user- or auto-selected main direction of motion. This displacement
field is used, together with suitable temporal and spatial smoothing, to warp
the image and produce the frames of a continuous animation loop. Our
cinemagraphs are created in under a second on a mobile device. Over 140,000
users downloaded our app and exported over 350,000 cinemagraphs. Moreover, we
conducted two user studies that show that users prefer our method for creating
surreal and structured cinemagraphs compared to more manual approaches and
compared to previous methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Halperin_T/0/1/0/all/0/1"&gt;Tavi Halperin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hakim_H/0/1/0/all/0/1"&gt;Hanit Hakim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vantzos_O/0/1/0/all/0/1"&gt;Orestis Vantzos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hochman_G/0/1/0/all/0/1"&gt;Gershon Hochman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benaim_N/0/1/0/all/0/1"&gt;Netai Benaim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sassy_L/0/1/0/all/0/1"&gt;Lior Sassy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kupchik_M/0/1/0/all/0/1"&gt;Michael Kupchik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bibi_O/0/1/0/all/0/1"&gt;Ofir Bibi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fried_O/0/1/0/all/0/1"&gt;Ohad Fried&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring The Limits Of Data Augmentation For Retinal Vessel Segmentation. (arXiv:2105.09365v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2105.09365</id>
        <link href="http://arxiv.org/abs/2105.09365"/>
        <updated>2021-05-23T06:08:17.203Z</updated>
        <summary type="html"><![CDATA[Retinal Vessel Segmentation is important for diagnosis of various diseases.
The research on retinal vessel segmentation focuses mainly on improvement of
the segmentation model which is usually based on U-Net architecture. In our
study we use the U-Net architecture and we rely on heavy data augmentation in
order to achieve better performance. The success of the data augmentation
relies on successfully addressing the problem of input images. By analyzing
input images and performing the augmentation accordingly we show that the
performance of the U-Net model can be increased dramatically. Results are
reported using the most widely used retina dataset, DRIVE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Uysal_E/0/1/0/all/0/1"&gt;Enes Sadi Uysal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bilici_M/0/1/0/all/0/1"&gt;M.&amp;#x15e;afak Bilici&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zaza_B/0/1/0/all/0/1"&gt;B. Selin Zaza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ozgenc_M/0/1/0/all/0/1"&gt;M. Yi&amp;#x11f;it &amp;#xd6;zgen&amp;#xe7;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Boyar_O/0/1/0/all/0/1"&gt;Onur Boyar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VOILA: Visual-Observation-Only Imitation Learning for Autonomous Navigation. (arXiv:2105.09371v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2105.09371</id>
        <link href="http://arxiv.org/abs/2105.09371"/>
        <updated>2021-05-23T06:08:17.176Z</updated>
        <summary type="html"><![CDATA[While imitation learning for vision based autonomous mobile robot navigation
has recently received a great deal of attention in the research community,
existing approaches typically require state action demonstrations that were
gathered using the deployment platform. However, what if one cannot easily
outfit their platform to record these demonstration signals or worse yet the
demonstrator does not have access to the platform at all? Is imitation learning
for vision based autonomous navigation even possible in such scenarios? In this
work, we hypothesize that the answer is yes and that recent ideas from the
Imitation from Observation (IfO) literature can be brought to bear such that a
robot can learn to navigate using only ego centric video collected by a
demonstrator, even in the presence of viewpoint mismatch. To this end, we
introduce a new algorithm, Visual Observation only Imitation Learning for
Autonomous navigation (VOILA), that can successfully learn navigation policies
from a single video demonstration collected from a physically different agent.
We evaluate VOILA in the photorealistic AirSim simulator and show that VOILA
not only successfully imitates the expert, but that it also learns navigation
policies that can generalize to novel environments. Further, we demonstrate the
effectiveness of VOILA in a real world setting by showing that it allows a
wheeled Jackal robot to successfully imitate a human walking in an environment
using a video recorded using a mobile phone camera.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karnan_H/0/1/0/all/0/1"&gt;Haresh Karnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Warnell_G/0/1/0/all/0/1"&gt;Garrett Warnell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1"&gt;Xuesu Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1"&gt;Peter Stone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimate The Efficiency Of Multiprocessor's Cash Memory Work Algorithms. (arXiv:2102.03848v2 [cs.NI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03848</id>
        <link href="http://arxiv.org/abs/2102.03848"/>
        <updated>2021-05-23T06:08:17.168Z</updated>
        <summary type="html"><![CDATA[Many computer systems for calculating the proper organization of memory are
among the most critical issues. Using a tier cache memory (along with branching
prediction) is an effective means of increasing modern multi-core processors'
performance. Designing high-performance processors is a complex task and
requires preliminary verification and analysis of the model level, usually used
in analytical and simulation modeling. The refinement of extreme programming is
an unfortunate challenge. Few experts disagree with the synthesis of access
points. This article demonstrates that Internet QoS and 16-bit architectures
are always incompatible, but it's the same situation for write-back caches. The
solution to this problem can be implemented by analyzing simulation models of
different complexity in combination with the analytical evaluation of
individual algorithms. This work is devoted to designing a multi-parameter
simulation model of a multi-process for evaluating the performance of cache
memory algorithms and the optimality of the structure. Optimization of the
structures and algorithms of the cache memory allows you to accelerate the
interaction of the memory process and improve the performance of the entire
system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hamada_M/0/1/0/all/0/1"&gt;Mohamed A. Hamada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdallah_A/0/1/0/all/0/1"&gt;Abdelrahman Abdallah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Adversarial Neural Architecture Search. (arXiv:2105.09356v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09356</id>
        <link href="http://arxiv.org/abs/2105.09356"/>
        <updated>2021-05-23T06:08:17.161Z</updated>
        <summary type="html"><![CDATA[Despite the empirical success of neural architecture search (NAS) in deep
learning applications, the optimality, reproducibility and cost of NAS schemes
remain hard to assess. In this paper, we propose Generative Adversarial NAS
(GA-NAS) with theoretically provable convergence guarantees, promoting
stability and reproducibility in neural architecture search. Inspired by
importance sampling, GA-NAS iteratively fits a generator to previously
discovered top architectures, thus increasingly focusing on important parts of
a large search space. Furthermore, we propose an efficient adversarial learning
approach, where the generator is trained by reinforcement learning based on
rewards provided by a discriminator, thus being able to explore the search
space without evaluating a large number of architectures. Extensive experiments
show that GA-NAS beats the best published results under several cases on three
public NAS benchmarks. In the meantime, GA-NAS can handle ad-hoc search
constraints and search spaces. We show that GA-NAS can be used to improve
already optimized baselines found by other NAS methods, including EfficientNet
and ProxylessNAS, in terms of ImageNet accuracy or the number of parameters, in
their original search space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rezaei_S/0/1/0/all/0/1"&gt;Seyed Saeed Changiz Rezaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1"&gt;Fred X. Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1"&gt;Di Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salameh_M/0/1/0/all/0/1"&gt;Mohammad Salameh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1"&gt;Keith Mills&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1"&gt;Shuo Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wei Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1"&gt;Shangling Jui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Graph-Based Behavior-Aware Recommendation for Interactive News. (arXiv:1812.00002v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1812.00002</id>
        <link href="http://arxiv.org/abs/1812.00002"/>
        <updated>2021-05-23T06:08:17.152Z</updated>
        <summary type="html"><![CDATA[Interactive news recommendation has been launched and attracted much
attention recently. In this scenario, user's behavior evolves from single click
behavior to multiple behaviors including like, comment, share etc. However,
most of the existing methods still use single click behavior as the unique
criterion of judging user's preferences. Further, although heterogeneous graphs
have been applied in different areas, a proper way to construct a heterogeneous
graph for interactive news data with an appropriate learning mechanism on it is
still desired. To address the above concerns, we propose a graph-based
behavior-aware network, which simultaneously considers six different types of
behaviors as well as user's demand on the news diversity. We have three main
steps. First, we build an interaction behavior graph for multi-level and
multi-category data. Second, we apply DeepWalk on the behavior graph to obtain
entity semantics, then build a graph-based convolutional neural network called
G-CNN to learn news representations, and an attention-based LSTM to learn
behavior sequence representations. Third, we introduce core and coritivity
features for the behavior graph, which measure the concentration degree of
user's interests. These features affect the trade-off between accuracy and
diversity of our personalized recommendation system. Taking these features into
account, our system finally achieves recommending news to different users at
their different levels of concentration degrees.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1"&gt;Mingyuan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Na_S/0/1/0/all/0/1"&gt;Sen Na&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Congzhou Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unified Conversational Recommendation Policy Learning via Graph-based Reinforcement Learning. (arXiv:2105.09710v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.09710</id>
        <link href="http://arxiv.org/abs/2105.09710"/>
        <updated>2021-05-23T06:08:17.142Z</updated>
        <summary type="html"><![CDATA[Conversational recommender systems (CRS) enable the traditional recommender
systems to explicitly acquire user preferences towards items and attributes
through interactive conversations. Reinforcement learning (RL) is widely
adopted to learn conversational recommendation policies to decide what
attributes to ask, which items to recommend, and when to ask or recommend, at
each conversation turn. However, existing methods mainly target at solving one
or two of these three decision-making problems in CRS with separated
conversation and recommendation components, which restrict the scalability and
generality of CRS and fall short of preserving a stable training procedure. In
the light of these challenges, we propose to formulate these three
decision-making problems in CRS as a unified policy learning task. In order to
systematically integrate conversation and recommendation components, we develop
a dynamic weighted graph based RL method to learn a policy to select the action
at each conversation turn, either asking an attribute or recommending items.
Further, to deal with the sample efficiency issue, we propose two action
selection strategies for reducing the candidate action space according to the
preference and entropy information. Experimental results on two benchmark CRS
datasets and a real-world E-Commerce application show that the proposed method
not only significantly outperforms state-of-the-art methods but also enhances
the scalability and stability of CRS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yang Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yaliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1"&gt;Fei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1"&gt;Bolin Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1"&gt;Wai Lam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interactive Query Formulation using Query By Navigation. (arXiv:2105.09562v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.09562</id>
        <link href="http://arxiv.org/abs/2105.09562"/>
        <updated>2021-05-23T06:08:17.118Z</updated>
        <summary type="html"><![CDATA[Effective information disclosure in the context of databases with a large
conceptual schema is known to be a non-trivial problem. In particular the
formulation of ad-hoc queries is a major problem in such contexts. Existing
approaches for tackling this problem include graphical query interfaces, query
by navigation, query by construction, and point to point queries. In this
report we propose an adoption of the query by navigation mechanism that is
especially geared towards the InfoAssistant product. Query by navigation is
based on ideas from the information retrieval world, in particular on the
stratified hypermedia architecture. When using our approach to the formulations
of queries, a user will first formulate a number of simple queries
corresponding to linear paths through the information structure. The
formulation of the linear paths is the result of the {\em explorative phase} of
the query formulation. Once users have specified a number of these linear
paths, they may combine them to form more complex queries. Examples of such
combinations are: concatenation, union, intersection and selection. This last
process is referred to as {\em query by construction}, and is the {\em
constructive phase} of the query formulation process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Proper_H/0/1/0/all/0/1"&gt;H. A. Proper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distribution Agnostic Symbolic Representations for Time Series Dimensionality Reduction and Online Anomaly Detection. (arXiv:2105.09592v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.09592</id>
        <link href="http://arxiv.org/abs/2105.09592"/>
        <updated>2021-05-23T06:08:17.108Z</updated>
        <summary type="html"><![CDATA[Due to the importance of the lower bounding distances and the attractiveness
of symbolic representations, the family of symbolic aggregate approximations
(SAX) has been used extensively for encoding time series data. However, typical
SAX-based methods rely on two restrictive assumptions; the Gaussian
distribution and equiprobable symbols. This paper proposes two novel
data-driven SAX-based symbolic representations, distinguished by their
discretization steps. The first representation, oriented for general data
compaction and indexing scenarios, is based on the combination of kernel
density estimation and Lloyd-Max quantization to minimize the information loss
and mean squared error in the discretization step. The second method, oriented
for high-level mining tasks, employs the Mean-Shift clustering method and is
shown to enhance anomaly detection in the lower-dimensional space. Besides, we
verify on a theoretical basis a previously observed phenomenon of the intrinsic
process that results in a lower than the expected variance of the intermediate
piecewise aggregate approximation. This phenomenon causes an additional
information loss but can be avoided with a simple modification. The proposed
representations possess all the attractive properties of the conventional SAX
method. Furthermore, experimental evaluation on real-world datasets
demonstrates their superiority compared to the traditional SAX and an
alternative data-driven SAX variant.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bountrogiannis_K/0/1/0/all/0/1"&gt;Konstantinos Bountrogiannis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tzagkarakis_G/0/1/0/all/0/1"&gt;George Tzagkarakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsakalides_P/0/1/0/all/0/1"&gt;Panagiotis Tsakalides&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Personalized Fairness based on Causal Notion. (arXiv:2105.09829v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.09829</id>
        <link href="http://arxiv.org/abs/2105.09829"/>
        <updated>2021-05-23T06:08:17.079Z</updated>
        <summary type="html"><![CDATA[Recommender systems are gaining increasing and critical impacts on human and
society since a growing number of users use them for information seeking and
decision making. Therefore, it is crucial to address the potential unfairness
problems in recommendations. Just like users have personalized preferences on
items, users' demands for fairness are also personalized in many scenarios.
Therefore, it is important to provide personalized fair recommendations for
users to satisfy their personalized fairness demands. Besides, previous works
on fair recommendation mainly focus on association-based fairness. However, it
is important to advance from associative fairness notions to causal fairness
notions for assessing fairness more properly in recommender systems. Based on
the above considerations, this paper focuses on achieving personalized
counterfactual fairness for users in recommender systems. To this end, we
introduce a framework for achieving counterfactually fair recommendations
through adversary learning by generating feature-independent user embeddings
for recommendation. The framework allows recommender systems to achieve
personalized fairness for users while also covering non-personalized
situations. Experiments on two real-world datasets with shallow and deep
recommendation algorithms show that our method can generate fairer
recommendations for users with a desirable recommendation performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yunqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hanxiong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shuyuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1"&gt;Yingqiang Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongfeng Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lexicon Enhanced Chinese Sequence Labeling Using BERT Adapter. (arXiv:2105.07148v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07148</id>
        <link href="http://arxiv.org/abs/2105.07148"/>
        <updated>2021-05-23T06:08:17.061Z</updated>
        <summary type="html"><![CDATA[Lexicon information and pre-trained models, such as BERT, have been combined
to explore Chinese sequence labelling tasks due to their respective strengths.
However, existing methods solely fuse lexicon features via a shallow and random
initialized sequence layer and do not integrate them into the bottom layers of
BERT. In this paper, we propose Lexicon Enhanced BERT (LEBERT) for Chinese
sequence labelling, which integrates external lexicon knowledge into BERT
layers directly by a Lexicon Adapter layer. Compared with the existing methods,
our model facilitates deep lexicon knowledge fusion at the lower layers of
BERT. Experiments on ten Chinese datasets of three tasks including Named Entity
Recognition, Word Segmentation, and Part-of-Speech tagging, show that LEBERT
achieves the state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1"&gt;Xiyan Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yue Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1"&gt;Wenming Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FreshDiskANN: A Fast and Accurate Graph-Based ANN Index for Streaming Similarity Search. (arXiv:2105.09613v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.09613</id>
        <link href="http://arxiv.org/abs/2105.09613"/>
        <updated>2021-05-23T06:08:17.044Z</updated>
        <summary type="html"><![CDATA[Approximate nearest neighbor search (ANNS) is a fundamental building block in
information retrieval with graph-based indices being the current
state-of-the-art and widely used in the industry. Recent advances in
graph-based indices have made it possible to index and search billion-point
datasets with high recall and millisecond-level latency on a single commodity
machine with an SSD.

However, existing graph algorithms for ANNS support only static indices that
cannot reflect real-time changes to the corpus required by many key real-world
scenarios (e.g. index of sentences in documents, email, or a news index). To
overcome this drawback, the current industry practice for manifesting updates
into such indices is to periodically re-build these indices, which can be
prohibitively expensive.

In this paper, we present the first graph-based ANNS index that reflects
corpus updates into the index in real-time without compromising on search
performance. Using update rules for this index, we design FreshDiskANN, a
system that can index over a billion points on a workstation with an SSD and
limited memory, and support thousands of concurrent real-time inserts, deletes
and searches per second each, while retaining $>95\%$ 5-recall@5. This
represents a 5-10x reduction in the cost of maintaining freshness in indices
when compared to existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Aditi Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Subramanya_S/0/1/0/all/0/1"&gt;Suhas Jayaram Subramanya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnaswamy_R/0/1/0/all/0/1"&gt;Ravishankar Krishnaswamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simhadri_H/0/1/0/all/0/1"&gt;Harsha Vardhan Simhadri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilingual Offensive Language Identification for Low-resource Languages. (arXiv:2105.05996v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05996</id>
        <link href="http://arxiv.org/abs/2105.05996"/>
        <updated>2021-05-23T06:08:17.026Z</updated>
        <summary type="html"><![CDATA[Offensive content is pervasive in social media and a reason for concern to
companies and government organizations. Several studies have been recently
published investigating methods to detect the various forms of such content
(e.g. hate speech, cyberbullying, and cyberaggression). The clear majority of
these studies deal with English partially because most annotated datasets
available contain English data. In this paper, we take advantage of available
English datasets by applying cross-lingual contextual word embeddings and
transfer learning to make predictions in low-resource languages. We project
predictions on comparable data in Arabic, Bengali, Danish, Greek, Hindi,
Spanish, and Turkish. We report results of 0.8415 F1 macro for Bengali in
TRAC-2 shared task, 0.8532 F1 macro for Danish and 0.8701 F1 macro for Greek in
OffensEval 2020, 0.8568 F1 macro for Hindi in HASOC 2019 shared task and 0.7513
F1 macro for Spanish in in SemEval-2019 Task 5 (HatEval) showing that our
approach compares favourably to the best systems submitted to recent shared
tasks on these three languages. Additionally, we report competitive performance
on Arabic, and Turkish using the training and development sets of OffensEval
2020 shared task. The results for all languages confirm the robustness of
cross-lingual contextual embeddings and transfer learning for this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1"&gt;Tharindu Ranasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1"&gt;Marcos Zampieri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intra-Document Cascading: Learning to Select Passages for Neural Document Ranking. (arXiv:2105.09816v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.09816</id>
        <link href="http://arxiv.org/abs/2105.09816"/>
        <updated>2021-05-23T06:08:17.014Z</updated>
        <summary type="html"><![CDATA[An emerging recipe for achieving state-of-the-art effectiveness in neural
document re-ranking involves utilizing large pre-trained language models -
e.g., BERT - to evaluate all individual passages in the document and then
aggregating the outputs by pooling or additional Transformer layers. A major
drawback of this approach is high query latency due to the cost of evaluating
every passage in the document with BERT. To make matters worse, this high
inference cost and latency varies based on the length of the document, with
longer documents requiring more time and computation. To address this
challenge, we adopt an intra-document cascading strategy, which prunes passages
of a candidate document using a less expensive model, called ESM, before
running a scoring model that is more expensive and effective, called ETM. We
found it best to train ESM (short for Efficient Student Model) via knowledge
distillation from the ETM (short for Effective Teacher Model) e.g., BERT. This
pruning allows us to only run the ETM model on a smaller set of passages whose
size does not vary by document length. Our experiments on the MS MARCO and TREC
Deep Learning Track benchmarks suggest that the proposed Intra-Document
Cascaded Ranking Model (IDCM) leads to over 400% lower query latency by
providing essentially the same effectiveness as the state-of-the-art BERT-based
document ranking models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hofstatter_S/0/1/0/all/0/1"&gt;Sebastian Hofst&amp;#xe4;tter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_B/0/1/0/all/0/1"&gt;Bhaskar Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1"&gt;Hamed Zamani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Craswell_N/0/1/0/all/0/1"&gt;Nick Craswell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1"&gt;Allan Hanbury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic and Variational Recommendation Denoising. (arXiv:2105.09605v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.09605</id>
        <link href="http://arxiv.org/abs/2105.09605"/>
        <updated>2021-05-23T06:08:16.990Z</updated>
        <summary type="html"><![CDATA[Learning from implicit feedback is one of the most common cases in the
application of recommender systems. Generally speaking, interacted examples are
considered as positive while negative examples are sampled from uninteracted
ones. However, noisy examples are prevalent in real-world implicit feedback. A
noisy positive example could be interacted but it actually leads to negative
user preference. A noisy negative example which is uninteracted because of
unawareness of the user could also denote potential positive user preference.
Conventional training methods overlook these noisy examples, leading to
sub-optimal recommendation. In this work, we propose probabilistic and
variational recommendation denoising for implicit feedback. Through an
empirical study, we find that different models make relatively similar
predictions on clean examples which denote the real user preference, while the
predictions on noisy examples vary much more across different models. Motivated
by this observation, we propose denoising with probabilistic inference (DPI)
which aims to minimize the KL-divergence between the real user preference
distributions parameterized by two recommendation models while maximize the
likelihood of data observation. We then show that DPI recovers the evidence
lower bound of an variational auto-encoder when the real user preference is
considered as the latent variables. This leads to our second learning framework
denoising with variational autoencoder (DVAE). We employ the proposed DPI and
DVAE on four state-of-the-art recommendation models and conduct experiments on
three datasets. Experimental results demonstrate that DPI and DVAE
significantly improve recommendation performance compared with normal training
and other denoising methods. Codes will be open-sourced.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1"&gt;Xin Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1"&gt;Zaiqiao Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangnan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jose_J/0/1/0/all/0/1"&gt;Joemon Jose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1"&gt;Fuli Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counterfactual Interventions Reveal the Causal Effect of Relative Clause Representations on Agreement Prediction. (arXiv:2105.06965v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06965</id>
        <link href="http://arxiv.org/abs/2105.06965"/>
        <updated>2021-05-23T06:08:16.967Z</updated>
        <summary type="html"><![CDATA[When language models process syntactically complex sentences, do they use
abstract syntactic information present in these sentences in a manner that is
consistent with the grammar of English, or do they rely solely on a set of
heuristics? We propose a method to tackle this question, AlterRep. For any
linguistic feature in the sentence, AlterRep allows us to generate
counterfactual representations by altering how this feature is encoded, while
leaving all other aspects of the original representation intact. Then, by
measuring the change in a models' word prediction with these counterfactual
representations in different sentences, we can draw causal conclusions about
the contexts in which the model uses the linguistic feature (if any). Applying
this method to study how BERT uses relative clause (RC) span information, we
found that BERT uses information about RC spans during agreement prediction
using the linguistically correct strategy. We also found that counterfactual
representations generated for a specific RC subtype influenced the number
prediction in sentences with other RC subtypes, suggesting that information
about RC boundaries was encoded abstractly in BERT's representation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1"&gt;Shauli Ravfogel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prasad_G/0/1/0/all/0/1"&gt;Grusha Prasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1"&gt;Tal Linzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1"&gt;Yoav Goldberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Encoding Explanatory Knowledge for Zero-shot Science Question Answering. (arXiv:2105.05737v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05737</id>
        <link href="http://arxiv.org/abs/2105.05737"/>
        <updated>2021-05-23T06:08:16.959Z</updated>
        <summary type="html"><![CDATA[This paper describes N-XKT (Neural encoding based on eXplanatory Knowledge
Transfer), a novel method for the automatic transfer of explanatory knowledge
through neural encoding mechanisms. We demonstrate that N-XKT is able to
improve accuracy and generalization on science Question Answering (QA).
Specifically, by leveraging facts from background explanatory knowledge
corpora, the N-XKT model shows a clear improvement on zero-shot QA.
Furthermore, we show that N-XKT can be fine-tuned on a target QA dataset,
enabling faster convergence and more accurate results. A systematic analysis is
conducted to quantitatively analyze the performance of the N-XKT model and the
impact of different categories of knowledge on the zero-shot generalization
task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zili Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1"&gt;Marco Valentino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Landers_D/0/1/0/all/0/1"&gt;Donal Landers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1"&gt;Andre Freitas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FastCorrect: Fast Error Correction with Edit Alignment for Automatic Speech Recognition. (arXiv:2105.03842v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03842</id>
        <link href="http://arxiv.org/abs/2105.03842"/>
        <updated>2021-05-23T06:08:16.945Z</updated>
        <summary type="html"><![CDATA[Error correction techniques have been used to refine the output sentences
from automatic speech recognition (ASR) models and achieve a lower word error
rate (WER) than original ASR outputs. Previous works usually use a
sequence-to-sequence model to correct an ASR output sentence autoregressively,
which causes large latency and cannot be deployed in online ASR services. A
straightforward solution to reduce latency, inspired by non-autoregressive
(NAR) neural machine translation, is to use an NAR sequence generation model
for ASR error correction, which, however, comes at the cost of significantly
increased ASR error rate. In this paper, observing distinctive error patterns
and correction operations (i.e., insertion, deletion, and substitution) in ASR,
we propose FastCorrect, a novel NAR error correction model based on edit
alignment. In training, FastCorrect aligns each source token from an ASR output
sentence to the target tokens from the corresponding ground-truth sentence
based on the edit distance between the source and target sentences, and
extracts the number of target tokens corresponding to each source token during
edition/correction, which is then used to train a length predictor and to
adjust the source tokens to match the length of the target sentence for
parallel generation. In inference, the token number predicted by the length
predictor is used to adjust the source tokens for target sequence generation.
Experiments on the public AISHELL-1 dataset and an internal industrial-scale
ASR dataset show the effectiveness of FastCorrect for ASR error correction: 1)
it speeds up the inference by 6-9 times and maintains the accuracy (8-14% WER
reduction) compared with the autoregressive correction model; and 2) it
outperforms the accuracy of popular NAR models adopted in neural machine
translation by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1"&gt;Yichong Leng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xu Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Linchen Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1"&gt;Renqian Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Linquan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang-Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1"&gt;Ed Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Cross-Domain Prerequisite Chain Learning using Variational Graph Autoencoders. (arXiv:2105.03505v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03505</id>
        <link href="http://arxiv.org/abs/2105.03505"/>
        <updated>2021-05-23T06:08:16.936Z</updated>
        <summary type="html"><![CDATA[Learning prerequisite chains is an essential task for efficiently acquiring
knowledge in both known and unknown domains. For example, one may be an expert
in the natural language processing (NLP) domain but want to determine the best
order to learn new concepts in an unfamiliar Computer Vision domain (CV). Both
domains share some common concepts, such as machine learning basics and deep
learning models. In this paper, we propose unsupervised cross-domain concept
prerequisite chain learning using an optimized variational graph autoencoder.
Our model learns to transfer concept prerequisite relations from an
information-rich domain (source domain) to an information-poor domain (target
domain), substantially surpassing other baseline models. Also, we expand an
existing dataset by introducing two new domains: CV and Bioinformatics (BIO).
The annotated data and resources, as well as the code, will be made publicly
available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1"&gt;Irene Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_V/0/1/0/all/0/1"&gt;Vanessa Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianxiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_R/0/1/0/all/0/1"&gt;Rihao Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1"&gt;Dragomir Radev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering. (arXiv:2012.00955v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00955</id>
        <link href="http://arxiv.org/abs/2012.00955"/>
        <updated>2021-05-23T06:08:16.916Z</updated>
        <summary type="html"><![CDATA[Recent works have shown that language models (LM) capture different types of
knowledge regarding facts or common sense. However, because no model is
perfect, they still fail to provide appropriate answers in many cases. In this
paper, we ask the question "how can we know when language models know, with
confidence, the answer to a particular query?" We examine this question from
the point of view of calibration, the property of a probabilistic model's
predicted probabilities actually being well correlated with the probabilities
of correctness. We examine three strong generative models -- T5, BART, and
GPT-2 -- and study whether their probabilities on QA tasks are well calibrated,
finding the answer is a relatively emphatic no. We then examine methods to
calibrate such models to make their confidence scores correlate better with the
likelihood of correctness through fine-tuning, post-hoc probability
modification, or adjustment of the predicted outputs or inputs. Experiments on
a diverse range of datasets demonstrate the effectiveness of our methods. We
also perform analysis to study the strengths and limitations of these methods,
shedding light on further improvements that may be made in methods for
calibrating LMs. We have released the code at
https://github.com/jzbjyb/lm-calibration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhengbao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Araki_J/0/1/0/all/0/1"&gt;Jun Araki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1"&gt;Haibo Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1"&gt;Graham Neubig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning. (arXiv:2012.15409v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15409</id>
        <link href="http://arxiv.org/abs/2012.15409"/>
        <updated>2021-05-23T06:08:16.907Z</updated>
        <summary type="html"><![CDATA[Existed pre-training methods either focus on single-modal tasks or
multi-modal tasks, and cannot effectively adapt to each other. They can only
utilize single-modal data (i.e. text or image) or limited multi-modal data
(i.e. image-text pairs). In this work, we propose a unified-modal pre-training
architecture, namely UNIMO, which can effectively adapt to both single-modal
and multi-modal understanding and generation tasks. Large scale of free text
corpus and image collections can be utilized to improve the capability of
visual and textual understanding, and cross-modal contrastive learning (CMCL)
is leveraged to align the textual and visual information into a unified
semantic space over a corpus of image-text pairs. As the non-paired
single-modal data is very rich, our model can utilize much larger scale of data
to learn more generalizable representations. Moreover, the textual knowledge
and visual knowledge can enhance each other in the unified semantic space. The
experimental results show that UNIMO significantly improves the performance of
several single-modal and multi-modal downstream tasks. Our code and pre-trained
models are public at
https://github.com/PaddlePaddle/Research/tree/master/NLP/UNIMO]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1"&gt;Can Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Guocheng Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1"&gt;Xinyan Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiachen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haifeng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Subverting the Jewtocracy": Online Antisemitism Detection Using Multimodal Deep Learning. (arXiv:2104.05947v2 [cs.MM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05947</id>
        <link href="http://arxiv.org/abs/2104.05947"/>
        <updated>2021-05-23T06:08:16.899Z</updated>
        <summary type="html"><![CDATA[The exponential rise of online social media has enabled the creation,
distribution, and consumption of information at an unprecedented rate. However,
it has also led to the burgeoning of various forms of online abuse. Increasing
cases of online antisemitism have become one of the major concerns because of
its socio-political consequences. Unlike other major forms of online abuse like
racism, sexism, etc., online antisemitism has not been studied much from a
machine learning perspective. To the best of our knowledge, we present the
first work in the direction of automated multimodal detection of online
antisemitism. The task poses multiple challenges that include extracting
signals across multiple modalities, contextual references, and handling
multiple aspects of antisemitism. Unfortunately, there does not exist any
publicly available benchmark corpus for this critical task. Hence, we collect
and label two datasets with 3,102 and 3,509 social media posts from Twitter and
Gab respectively. Further, we present a multimodal deep learning system that
detects the presence of antisemitic content and its specific antisemitism
category using text and images from posts. We perform an extensive set of
experiments on the two datasets to evaluate the efficacy of the proposed
system. Finally, we also present a qualitative analysis of our study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_M/0/1/0/all/0/1"&gt;Mohit Chandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pailla_D/0/1/0/all/0/1"&gt;Dheeraj Pailla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhatia_H/0/1/0/all/0/1"&gt;Himanshu Bhatia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchawala_A/0/1/0/all/0/1"&gt;Aadilmehdi Sanchawala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1"&gt;Manish Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1"&gt;Manish Shrivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1"&gt;Ponnurangam Kumaraguru&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Infinite use of finite means: Zero-Shot Generalization using Compositional Emergent Protocols. (arXiv:2012.05011v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05011</id>
        <link href="http://arxiv.org/abs/2012.05011"/>
        <updated>2021-05-23T06:08:16.830Z</updated>
        <summary type="html"><![CDATA[Human language has been described as a system that makes \textit{use of
finite means to express an unlimited array of thoughts}. Of particular interest
is the aspect of compositionality, whereby, the meaning of a compound language
expression can be deduced from the meaning of its constituent parts. If
artificial agents can develop compositional communication protocols akin to
human language, they can be made to seamlessly generalize to unseen
combinations. However, the real question is, how do we induce compositionality
in emergent communication? Studies have recognized the role of curiosity in
enabling linguistic development in children. It is this same intrinsic urge
that drives us to master complex tasks with decreasing amounts of explicit
reward. In this paper, we seek to use this intrinsic feedback in inducing a
systematic and unambiguous protolanguage in artificial agents. We show how
these rewards can be leveraged in training agents to induce compositionality in
absence of any external feedback. Additionally, we introduce gComm, an
environment for investigating grounded language acquisition in 2D-grid
environments. Using this, we demonstrate how compositionality can enable agents
to not only interact with unseen objects but also transfer skills from one task
to another in a zero-shot setting: \textit{Can an agent, trained to `pull' and
`push twice', `pull twice'?}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hazra_R/0/1/0/all/0/1"&gt;Rishi Hazra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dixit_S/0/1/0/all/0/1"&gt;Sonu Dixit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sen_S/0/1/0/all/0/1"&gt;Sayambhu Sen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting News Article Structure for Automatic Corpus Generation. (arXiv:2010.11574v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11574</id>
        <link href="http://arxiv.org/abs/2010.11574"/>
        <updated>2021-05-23T06:08:16.822Z</updated>
        <summary type="html"><![CDATA[Transformers represent the state-of-the-art in Natural Language Processing
(NLP) in recent years, proving effective even in tasks done in low-resource
languages. While pretrained transformers for these languages can be made, it is
challenging to measure their true performance and capacity due to the lack of
hard benchmark datasets, as well as the difficulty and cost of producing them.
In this paper, we present three contributions: First, we propose a methodology
for automatically producing Natural Language Inference (NLI) benchmark datasets
for low-resource languages using published news articles. Through this, we
create and release NewsPH-NLI, the first sentence entailment benchmark dataset
in the low-resource Filipino language. Second, we produce new pretrained
transformers based on the ELECTRA technique to further alleviate the resource
scarcity in Filipino, benchmarking them on our dataset against other
commonly-used transfer learning techniques. Lastly, we perform analyses on
transfer learning techniques to shed light on their true performance when
operating in low-data domains through the use of degradation tests.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cruz_J/0/1/0/all/0/1"&gt;Jan Christian Blaise Cruz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Resabal_J/0/1/0/all/0/1"&gt;Jose Kristian Resabal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;James Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Velasco_D/0/1/0/all/0/1"&gt;Dan John Velasco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1"&gt;Charibeth Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring Coding Challenge Competence With APPS. (arXiv:2105.09938v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2105.09938</id>
        <link href="http://arxiv.org/abs/2105.09938"/>
        <updated>2021-05-23T06:08:16.792Z</updated>
        <summary type="html"><![CDATA[While programming is one of the most broadly applicable skills in modern
society, modern machine learning models still cannot code solutions to basic
problems. It can be difficult to accurately assess code generation performance,
and there has been surprisingly little work on evaluating code generation in a
way that is both flexible and rigorous. To meet this challenge, we introduce
APPS, a benchmark for code generation. Unlike prior work in more restricted
settings, our benchmark measures the ability of models to take an arbitrary
natural language specification and generate Python code fulfilling this
specification. Similar to how companies assess candidate software developers,
we then evaluate models by checking their generated code on test cases. Our
benchmark includes 10,000 problems, which range from having simple one-line
solutions to being substantial algorithmic challenges. We fine-tune large
language models on both GitHub and our training set, and we find that the
prevalence of syntax errors is decreasing exponentially. Recent models such as
GPT-Neo can pass approximately 15% of the test cases of introductory problems,
so we find that machine learning models are beginning to learn how to code. As
the social significance of automatic code generation increases over the coming
years, our benchmark can provide an important measure for tracking
advancements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1"&gt;Dan Hendrycks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1"&gt;Steven Basart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadavath_S/0/1/0/all/0/1"&gt;Saurav Kadavath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazeika_M/0/1/0/all/0/1"&gt;Mantas Mazeika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1"&gt;Akul Arora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_E/0/1/0/all/0/1"&gt;Ethan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burns_C/0/1/0/all/0/1"&gt;Collin Burns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Puranik_S/0/1/0/all/0/1"&gt;Samir Puranik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Horace He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1"&gt;Dawn Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1"&gt;Jacob Steinhardt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Head Attention: Collaborate Instead of Concatenate. (arXiv:2006.16362v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.16362</id>
        <link href="http://arxiv.org/abs/2006.16362"/>
        <updated>2021-05-23T06:08:16.782Z</updated>
        <summary type="html"><![CDATA[Attention layers are widely used in natural language processing (NLP) and are
beginning to influence computer vision architectures. Training very large
transformer models allowed significant improvement in both fields, but once
trained, these networks show symptoms of over-parameterization. For instance,
it is known that many attention heads can be pruned without impacting accuracy.
This work aims to enhance current understanding on how multiple heads interact.
Motivated by the observation that attention heads learn redundant key/query
projections, we propose a collaborative multi-head attention layer that enables
heads to learn shared projections. Our scheme decreases the number of
parameters in an attention layer and can be used as a drop-in replacement in
any transformer architecture. Our experiments confirm that sharing key/query
dimensions can be exploited in language understanding, machine translation and
vision. We also show that it is possible to re-parametrize a pre-trained
multi-head attention layer into our collaborative attention layer.
Collaborative multi-head attention reduces the size of the key and query
projections by 4 for same accuracy and speed. Our code is public.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cordonnier_J/0/1/0/all/0/1"&gt;Jean-Baptiste Cordonnier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loukas_A/0/1/0/all/0/1"&gt;Andreas Loukas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UmlsBERT: Clinical Domain Knowledge Augmentation of Contextual Embeddings Using the Unified Medical Language System Metathesaurus. (arXiv:2010.10391v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10391</id>
        <link href="http://arxiv.org/abs/2010.10391"/>
        <updated>2021-05-23T06:08:16.766Z</updated>
        <summary type="html"><![CDATA[Contextual word embedding models, such as BioBERT and Bio_ClinicalBERT, have
achieved state-of-the-art results in biomedical natural language processing
tasks by focusing their pre-training process on domain-specific corpora.
However, such models do not take into consideration expert domain knowledge.

In this work, we introduced UmlsBERT, a contextual embedding model that
integrates domain knowledge during the pre-training process via a novel
knowledge augmentation strategy. More specifically, the augmentation on
UmlsBERT with the Unified Medical Language System (UMLS) Metathesaurus was
performed in two ways: i) connecting words that have the same underlying
`concept' in UMLS, and ii) leveraging semantic group knowledge in UMLS to
create clinically meaningful input embeddings. By applying these two
strategies, UmlsBERT can encode clinical domain knowledge into word embeddings
and outperform existing domain-specific models on common named-entity
recognition (NER) and clinical natural language inference clinical NLP tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Michalopoulos_G/0/1/0/all/0/1"&gt;George Michalopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuanxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaka_H/0/1/0/all/0/1"&gt;Hussam Kaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Helen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alexander Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Cross-Dataset Generalization in Automatic Detection of Online Abuse. (arXiv:2010.07414v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07414</id>
        <link href="http://arxiv.org/abs/2010.07414"/>
        <updated>2021-05-23T06:08:16.758Z</updated>
        <summary type="html"><![CDATA[NLP research has attained high performances in abusive language detection as
a supervised classification task. While in research settings, training and test
datasets are usually obtained from similar data samples, in practice systems
are often applied on data that are different from the training set in topic and
class distributions. Also, the ambiguity in class definitions inherited in this
task aggravates the discrepancies between source and target datasets. We
explore the topic bias and the task formulation bias in cross-dataset
generalization. We show that the benign examples in the Wikipedia Detox dataset
are biased towards platform-specific topics. We identify these examples using
unsupervised topic modeling and manual inspection of topics' keywords. Removing
these topics increases cross-dataset generalization, without reducing in-domain
classification performance. For a robust dataset design, we suggest applying
inexpensive unsupervised methods to inspect the collected data and downsize the
non-generalizable content before manually annotating for class labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nejadgholi_I/0/1/0/all/0/1"&gt;Isar Nejadgholi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiritchenko_S/0/1/0/all/0/1"&gt;Svetlana Kiritchenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BRUMS at SemEval-2020 Task 3: Contextualised Embeddings for Predicting the (Graded) Effect of Context in Word Similarity. (arXiv:2010.06269v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.06269</id>
        <link href="http://arxiv.org/abs/2010.06269"/>
        <updated>2021-05-23T06:08:16.745Z</updated>
        <summary type="html"><![CDATA[This paper presents the team BRUMS submission to SemEval-2020 Task 3: Graded
Word Similarity in Context. The system utilises state-of-the-art contextualised
word embeddings, which have some task-specific adaptations, including stacked
embeddings and average embeddings. Overall, the approach achieves good
evaluation scores across all the languages, while maintaining simplicity.
Following the final rankings, our approach is ranked within the top 5 solutions
of each language while preserving the 1st position of Finnish subtask 2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hettiarachchi_H/0/1/0/all/0/1"&gt;Hansi Hettiarachchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1"&gt;Tharindu Ranasinghe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mondegreen: A Post-Processing Solution to Speech Recognition Error Correction for Voice Search Queries. (arXiv:2105.09930v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2105.09930</id>
        <link href="http://arxiv.org/abs/2105.09930"/>
        <updated>2021-05-23T06:08:16.721Z</updated>
        <summary type="html"><![CDATA[As more and more online search queries come from voice, automatic speech
recognition becomes a key component to deliver relevant search results. Errors
introduced by automatic speech recognition (ASR) lead to irrelevant search
results returned to the user, thus causing user dissatisfaction. In this paper,
we introduce an approach, Mondegreen, to correct voice queries in text space
without depending on audio signals, which may not always be available due to
system constraints or privacy or bandwidth (for example, some ASR systems run
on-device) considerations. We focus on voice queries transcribed via several
proprietary commercial ASR systems. These queries come from users making
internet, or online service search queries. We first present an analysis
showing how different the language distribution coming from user voice queries
is from that in traditional text corpora used to train off-the-shelf ASR
systems. We then demonstrate that Mondegreen can achieve significant
improvements in increased user interaction by correcting user voice queries in
one of the largest search systems in Google. Finally, we see Mondegreen as
complementing existing highly-optimized production ASR systems, which may not
be frequently retrained and thus lag behind due to vocabulary drifts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sodhi_S/0/1/0/all/0/1"&gt;Sukhdeep S. Sodhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chio_E/0/1/0/all/0/1"&gt;Ellie Ka-In Chio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jash_A/0/1/0/all/0/1"&gt;Ambarish Jash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1"&gt;Santiago Onta&amp;#xf1;&amp;#xf3;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Apte_A/0/1/0/all/0/1"&gt;Ajit Apte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Ankit Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeje_A/0/1/0/all/0/1"&gt;Ayooluwakunmi Jeje&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuzmin_D/0/1/0/all/0/1"&gt;Dima Kuzmin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fung_H/0/1/0/all/0/1"&gt;Harry Fung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Heng-Tze Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Effrat_J/0/1/0/all/0/1"&gt;Jon Effrat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bali_T/0/1/0/all/0/1"&gt;Tarush Bali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jindal_N/0/1/0/all/0/1"&gt;Nitin Jindal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_P/0/1/0/all/0/1"&gt;Pei Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sarvjeet Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Senqiang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_T/0/1/0/all/0/1"&gt;Tameen Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wankhede_A/0/1/0/all/0/1"&gt;Amol Wankhede&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alzantot_M/0/1/0/all/0/1"&gt;Moustafa Alzantot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1"&gt;Allen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_T/0/1/0/all/0/1"&gt;Tushar Chandra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simplifying Paragraph-level Question Generation via Transformer Language Models. (arXiv:2005.01107v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.01107</id>
        <link href="http://arxiv.org/abs/2005.01107"/>
        <updated>2021-05-23T06:08:16.708Z</updated>
        <summary type="html"><![CDATA[Question generation (QG) is a natural language generation task where a model
is trained to ask questions corresponding to some input text. Most recent
approaches frame QG as a sequence-to-sequence problem and rely on additional
features and mechanisms to increase performance; however, these often increase
model complexity, and can rely on auxiliary data unavailable in practical use.
A single Transformer-based unidirectional language model leveraging transfer
learning can be used to produce high quality questions while disposing of
additional task-specific complexity. Our QG model, finetuned from GPT-2 Small,
outperforms several paragraph-level QG baselines on the SQuAD dataset by 0.95
METEOR points. Human evaluators rated questions as easy to answer, relevant to
their context paragraph, and corresponding well to natural human speech. Also
introduced is a new set of baseline scores on the RACE dataset, which has not
previously been used for QG tasks. Further experimentation with varying model
capacities and datasets with non-identification type questions is recommended
in order to further verify the robustness of pretrained Transformer-based LMs
as question generators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lopez_L/0/1/0/all/0/1"&gt;Luis Enrico Lopez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cruz_D/0/1/0/all/0/1"&gt;Diane Kathryn Cruz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cruz_J/0/1/0/all/0/1"&gt;Jan Christian Blaise Cruz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1"&gt;Charibeth Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A practical introduction to the Rational Speech Act modeling framework. (arXiv:2105.09867v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09867</id>
        <link href="http://arxiv.org/abs/2105.09867"/>
        <updated>2021-05-23T06:08:16.699Z</updated>
        <summary type="html"><![CDATA[Recent advances in computational cognitive science (i.e., simulation-based
probabilistic programs) have paved the way for significant progress in formal,
implementable models of pragmatics. Rather than describing a pragmatic
reasoning process in prose, these models formalize and implement one, deriving
both qualitative and quantitative predictions of human behavior -- predictions
that consistently prove correct, demonstrating the viability and value of the
framework. The current paper provides a practical introduction to and critical
assessment of the Bayesian Rational Speech Act modeling framework, unpacking
theoretical foundations, exploring technological innovations, and drawing
connections to issues beyond current applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Scontras_G/0/1/0/all/0/1"&gt;Gregory Scontras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tessler_M/0/1/0/all/0/1"&gt;Michael Henry Tessler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franke_M/0/1/0/all/0/1"&gt;Michael Franke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Latency Real-Time Non-Parallel Voice Conversion based on Cyclic Variational Autoencoder and Multiband WaveRNN with Data-Driven Linear Prediction. (arXiv:2105.09858v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2105.09858</id>
        <link href="http://arxiv.org/abs/2105.09858"/>
        <updated>2021-05-23T06:08:16.671Z</updated>
        <summary type="html"><![CDATA[This paper presents a low-latency real-time (LLRT) non-parallel voice
conversion (VC) framework based on cyclic variational autoencoder (CycleVAE)
and multiband WaveRNN with data-driven linear prediction (MWDLP). CycleVAE is a
robust non-parallel multispeaker spectral model, which utilizes a
speaker-independent latent space and a speaker-dependent code to generate
reconstructed/converted spectral features given the spectral features of an
input speaker. On the other hand, MWDLP is an efficient and a high-quality
neural vocoder that can handle multispeaker data and generate speech waveform
for LLRT applications with CPU. To accommodate LLRT constraint with CPU, we
propose a novel CycleVAE framework that utilizes mel-spectrogram as spectral
features and is built with a sparse network architecture. Further, to improve
the modeling performance, we also propose a novel fine-tuning procedure that
refines the frame-rate CycleVAE network by utilizing the waveform loss from the
MWDLP network. The experimental results demonstrate that the proposed framework
achieves high-performance VC, while allowing for LLRT usage with a single-core
of $2.1$--$2.7$~GHz CPU on a real-time factor of $0.87$--$0.95$, including
input/output, feature extraction, on a frame shift of $10$ ms, a window length
of $27.5$ ms, and $2$ lookup frames.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tobing_P/0/1/0/all/0/1"&gt;Patrick Lumban Tobing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1"&gt;Tomoki Toda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Fidelity and Low-Latency Universal Neural Vocoder based on Multiband WaveRNN with Data-Driven Linear Prediction for Discrete Waveform Modeling. (arXiv:2105.09856v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2105.09856</id>
        <link href="http://arxiv.org/abs/2105.09856"/>
        <updated>2021-05-23T06:08:16.662Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel high-fidelity and low-latency universal neural
vocoder framework based on multiband WaveRNN with data-driven linear prediction
for discrete waveform modeling (MWDLP). MWDLP employs a coarse-fine bit WaveRNN
architecture for 10-bit mu-law waveform modeling. A sparse gated recurrent unit
with a relatively large size of hidden units is utilized, while the multiband
modeling is deployed to achieve real-time low-latency usage. A novel technique
for data-driven linear prediction (LP) with discrete waveform modeling is
proposed, where the LP coefficients are estimated in a data-driven manner.
Moreover, a novel loss function using short-time Fourier transform (STFT) for
discrete waveform modeling with Gumbel approximation is also proposed. The
experimental results demonstrate that the proposed MWDLP framework generates
high-fidelity synthetic speech for seen and unseen speakers and/or language on
300 speakers training data including clean and noisy/reverberant conditions,
where the number of training utterances is limited to 60 per speaker, while
allowing for real-time low-latency processing using a single core of $\sim\!$
2.1--2.7~GHz CPU with $\sim\!$ 0.57--0.64 real-time factor including
input/output and feature extraction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tobing_P/0/1/0/all/0/1"&gt;Patrick Lumban Tobing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1"&gt;Tomoki Toda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intra-Document Cascading: Learning to Select Passages for Neural Document Ranking. (arXiv:2105.09816v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2105.09816</id>
        <link href="http://arxiv.org/abs/2105.09816"/>
        <updated>2021-05-23T06:08:16.636Z</updated>
        <summary type="html"><![CDATA[An emerging recipe for achieving state-of-the-art effectiveness in neural
document re-ranking involves utilizing large pre-trained language models -
e.g., BERT - to evaluate all individual passages in the document and then
aggregating the outputs by pooling or additional Transformer layers. A major
drawback of this approach is high query latency due to the cost of evaluating
every passage in the document with BERT. To make matters worse, this high
inference cost and latency varies based on the length of the document, with
longer documents requiring more time and computation. To address this
challenge, we adopt an intra-document cascading strategy, which prunes passages
of a candidate document using a less expensive model, called ESM, before
running a scoring model that is more expensive and effective, called ETM. We
found it best to train ESM (short for Efficient Student Model) via knowledge
distillation from the ETM (short for Effective Teacher Model) e.g., BERT. This
pruning allows us to only run the ETM model on a smaller set of passages whose
size does not vary by document length. Our experiments on the MS MARCO and TREC
Deep Learning Track benchmarks suggest that the proposed Intra-Document
Cascaded Ranking Model (IDCM) leads to over 400% lower query latency by
providing essentially the same effectiveness as the state-of-the-art BERT-based
document ranking models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hofstatter_S/0/1/0/all/0/1"&gt;Sebastian Hofst&amp;#xe4;tter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_B/0/1/0/all/0/1"&gt;Bhaskar Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1"&gt;Hamed Zamani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Craswell_N/0/1/0/all/0/1"&gt;Nick Craswell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1"&gt;Allan Hanbury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Head-driven Phrase Structure Parsing in O($n^3$) Time Complexity. (arXiv:2105.09835v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09835</id>
        <link href="http://arxiv.org/abs/2105.09835"/>
        <updated>2021-05-23T06:08:16.627Z</updated>
        <summary type="html"><![CDATA[Constituent and dependency parsing, the two classic forms of syntactic
parsing, have been found to benefit from joint training and decoding under a
uniform formalism, Head-driven Phrase Structure Grammar (HPSG). However,
decoding this unified grammar has a higher time complexity ($O(n^5)$) than
decoding either form individually ($O(n^3)$) since more factors have to be
considered during decoding. We thus propose an improved head scorer that helps
achieve a novel performance-preserved parser in $O$($n^3$) time complexity.
Furthermore, on the basis of this proposed practical HPSG parser, we
investigated the strengths of HPSG-based parsing and explored the general
method of training an HPSG-based parser from only a constituent or dependency
annotations in a multilingual scenario. We thus present a more effective, more
in-depth, and general work on HPSG parsing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zuchao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Junru Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hai Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parnow_K/0/1/0/all/0/1"&gt;Kevin Parnow&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KLUE: Korean Language Understanding Evaluation. (arXiv:2105.09680v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09680</id>
        <link href="http://arxiv.org/abs/2105.09680"/>
        <updated>2021-05-23T06:08:16.503Z</updated>
        <summary type="html"><![CDATA[We introduce Korean Language Understanding Evaluation (KLUE) benchmark. KLUE
is a collection of 8 Korean natural language understanding (NLU) tasks,
including Topic Classification, Semantic Textual Similarity, Natural Language
Inference, Named Entity Recognition, Relation Extraction, Dependency Parsing,
Machine Reading Comprehension, and Dialogue State Tracking. We build all of the
tasks from scratch from diverse source corpora while respecting copyrights, to
ensure accessibility for anyone without any restrictions. With ethical
considerations in mind, we carefully design annotation protocols. Along with
the benchmark tasks and data, we provide suitable evaluation metrics and
fine-tuning recipes for pretrained language models for each task. We
furthermore release the pretrained language models (PLM), KLUE-BERT and
KLUE-RoBERTa, to help reproduce baseline models on KLUE and thereby facilitate
future research. We make a few interesting observations from the preliminary
experiments using the proposed KLUE benchmark suite, already demonstrating the
usefulness of this new benchmark suite. First, we find KLUE-RoBERTa-large
outperforms other baselines, including multilingual PLMs and existing
open-source Korean PLMs. Second, we see minimal degradation in performance even
when we replace personally identifiable information from the pretraining
corpus, suggesting that privacy and NLU capability are not at odds with each
other. Lastly, we find that using BPE tokenization in combination with
morpheme-level pre-tokenization is effective in tasks involving morpheme-level
tagging, detection and generation. In addition to accelerating Korean NLP
research, our comprehensive documentation on creating KLUE will facilitate
creating similar resources for other languages in the future. KLUE is available
at this https URL (https://klue-benchmark.com/).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Sungjoon Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1"&gt;Jihyung Moon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sungdong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_W/0/1/0/all/0/1"&gt;Won Ik Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiyoon Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jangwon Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1"&gt;Chisung Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Junseong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yongsook Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1"&gt;Taehwan Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Joohong Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1"&gt;Juhyun Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1"&gt;Sungwon Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1"&gt;Younghoon Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1"&gt;Inkwon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1"&gt;Sangwoo Seo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dongjun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyunwoo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Myeonghwa Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jang_S/0/1/0/all/0/1"&gt;Seongbo Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Do_S/0/1/0/all/0/1"&gt;Seungwon Do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sunkyoung Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1"&gt;Kyungtae Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jongwon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1"&gt;Kyumin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1"&gt;Jamin Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seonghyun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_L/0/1/0/all/0/1"&gt;Lucy Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1"&gt;Alice Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1"&gt;Jungwoo Ha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1"&gt;Kyunghyun Cho Alice Oh Jungwoo Ha Kyunghyun Cho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manual Evaluation Matters: Reviewing Test Protocols of Distantly Supervised Relation Extraction. (arXiv:2105.09543v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09543</id>
        <link href="http://arxiv.org/abs/2105.09543"/>
        <updated>2021-05-23T06:08:16.493Z</updated>
        <summary type="html"><![CDATA[Distantly supervised (DS) relation extraction (RE) has attracted much
attention in the past few years as it can utilize large-scale auto-labeled
data. However, its evaluation has long been a problem: previous works either
took costly and inconsistent methods to manually examine a small sample of
model predictions, or directly test models on auto-labeled data -- which, by
our check, produce as much as 53% wrong labels at the entity pair level in the
popular NYT10 dataset. This problem has not only led to inaccurate evaluation,
but also made it hard to understand where we are and what's left to improve in
the research of DS-RE. To evaluate DS-RE models in a more credible way, we
build manually-annotated test sets for two DS-RE datasets, NYT10 and Wiki20,
and thoroughly evaluate several competitive models, especially the latest
pre-trained ones. The experimental results show that the manual evaluation can
indicate very different conclusions from automatic ones, especially some
unexpected observations, e.g., pre-trained models can achieve dominating
performance while being more susceptible to false-positives compared to
previous methods. We hope that both our manual test sets and novel observations
can help advance future DS-RE research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1"&gt;Tianyu Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_K/0/1/0/all/0/1"&gt;Keyue Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yuzhuo Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zhiyu Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yankai Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Target-dependent Sentiment Classification in News Articles. (arXiv:2105.09660v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09660</id>
        <link href="http://arxiv.org/abs/2105.09660"/>
        <updated>2021-05-23T06:08:16.482Z</updated>
        <summary type="html"><![CDATA[Extensive research on target-dependent sentiment classification (TSC) has led
to strong classification performances in domains where authors tend to
explicitly express sentiment about specific entities or topics, such as in
reviews or on social media. We investigate TSC in news articles, a much less
researched domain, despite the importance of news as an essential information
source in individual and societal decision making. This article introduces
NewsTSC, a manually annotated dataset to explore TSC on news articles.
Investigating characteristics of sentiment in news and contrasting them to
popular TSC domains, we find that sentiment in the news is expressed less
explicitly, is more dependent on context and readership, and requires a greater
degree of interpretation. In an extensive evaluation, we find that the state of
the art in TSC performs worse on news articles than on other domains (average
recall AvgRec = 69.8 on NewsTSC compared to AvgRev = [75.6, 82.2] on
established TSC datasets). Reasons include incorrectly resolved relation of
target and sentiment-bearing phrases and off-context dependence. As a major
improvement over previous news TSC, we find that BERT's natural language
understanding capabilities capture the less explicit sentiment used in news
articles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hamborg_F/0/1/0/all/0/1"&gt;Felix Hamborg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donnay_K/0/1/0/all/0/1"&gt;Karsten Donnay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1"&gt;Bela Gipp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Case Study on Pros and Cons of Regular Expression Detection and Dependency Parsing for Negation Extraction from German Medical Documents. Technical Report. (arXiv:2105.09702v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09702</id>
        <link href="http://arxiv.org/abs/2105.09702"/>
        <updated>2021-05-23T06:08:16.455Z</updated>
        <summary type="html"><![CDATA[We describe our work on information extraction in medical documents written
in German, especially detecting negations using an architecture based on the
UIMA pipeline. Based on our previous work on software modules to cover medical
concepts like diagnoses, examinations, etc. we employ a version of the NegEx
regular expression algorithm with a large set of triggers as a baseline. We
show how a significantly smaller trigger set is sufficient to achieve similar
results, in order to reduce adaptation times to new text types. We elaborate on
the question whether dependency parsing (based on the Stanford CoreNLP model)
is a good alternative and describe the potentials and shortcomings of both
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Profitlich_H/0/1/0/all/0/1"&gt;Hans-J&amp;#xfc;rgen Profitlich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sonntag_D/0/1/0/all/0/1"&gt;Daniel Sonntag&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustness of end-to-end Automatic Speech Recognition Models -- A Case Study using Mozilla DeepSpeech. (arXiv:2105.09742v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09742</id>
        <link href="http://arxiv.org/abs/2105.09742"/>
        <updated>2021-05-23T06:08:16.444Z</updated>
        <summary type="html"><![CDATA[When evaluating the performance of automatic speech recognition models,
usually word error rate within a certain dataset is used. Special care must be
taken in understanding the dataset in order to report realistic performance
numbers. We argue that many performance numbers reported probably underestimate
the expected error rate. We conduct experiments controlling for selection bias,
gender as well as overlap (between training and test data) in content, voices,
and recording conditions. We find that content overlap has the biggest impact,
but other factors like gender also play a role.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1"&gt;Aashish Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zesch_T/0/1/0/all/0/1"&gt;Torsten Zesch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The impact of virtual mirroring on customer satisfaction. (arXiv:2105.09571v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2105.09571</id>
        <link href="http://arxiv.org/abs/2105.09571"/>
        <updated>2021-05-23T06:08:16.435Z</updated>
        <summary type="html"><![CDATA[We investigate the impact of a novel method called "virtual mirroring" to
promote employee self-reflection and impact customer satisfaction. The method
is based on measuring communication patterns, through social network and
semantic analysis, and mirroring them back to the individual. Our goal is to
demonstrate that self-reflection can trigger a change in communication
behaviors, which lead to increased customer satisfaction. We illustrate and
test our approach analyzing e-mails of a large global services company by
comparing changes in customer satisfaction associated with team leaders exposed
to virtual mirroring (the experimental group). We find an increase in customer
satisfaction in the experimental group and a decrease in the control group
(team leaders not involved in the virtual mirroring process). With regard to
the individual communication indicators, we find that customer satisfaction is
higher when employees are more responsive, use a simpler language, are embedded
in less centralized communication networks, and show more stable leadership
patterns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gloor_P/0/1/0/all/0/1"&gt;P. Gloor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1"&gt;A. Fronzetti Colladon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giacomelli_G/0/1/0/all/0/1"&gt;G. Giacomelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saran_T/0/1/0/all/0/1"&gt;T. Saran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grippa_F/0/1/0/all/0/1"&gt;F. Grippa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A comprehensive comparative evaluation and analysis of Distributional Semantic Models. (arXiv:2105.09825v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09825</id>
        <link href="http://arxiv.org/abs/2105.09825"/>
        <updated>2021-05-23T06:08:16.424Z</updated>
        <summary type="html"><![CDATA[Distributional semantics has deeply changed in the last decades. First,
predict models stole the thunder from traditional count ones, and more recently
both of them were replaced in many NLP applications by contextualized vectors
produced by Transformer neural language models. Although an extensive body of
research has been devoted to Distributional Semantic Model (DSM) evaluation, we
still lack a thorough comparison with respect to tested models, semantic tasks,
and benchmark datasets. Moreover, previous work has mostly focused on
task-driven evaluation, instead of exploring the differences between the way
models represent the lexical semantic space. In this paper, we perform a
comprehensive evaluation of type distributional vectors, either produced by
static DSMs or obtained by averaging the contextualized vectors generated by
BERT. First of all, we investigate the performance of embeddings in several
semantic tasks, carrying out an in-depth statistical analysis to identify the
major factors influencing the behavior of DSMs. The results show that i.) the
alleged superiority of predict based models is more apparent than real, and
surely not ubiquitous and ii.) static DSMs surpass contextualized
representations in most out-of-context semantic tasks and datasets.
Furthermore, we borrow from cognitive neuroscience the methodology of
Representational Similarity Analysis (RSA) to inspect the semantic spaces
generated by distributional models. RSA reveals important differences related
to the frequency and part-of-speech of lexical items.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lenci_A/0/1/0/all/0/1"&gt;Alessandro Lenci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahlgren_M/0/1/0/all/0/1"&gt;Magnus Sahlgren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeuniaux_P/0/1/0/all/0/1"&gt;Patrick Jeuniaux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gyllensten_A/0/1/0/all/0/1"&gt;Amaru Cuba Gyllensten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miliani_M/0/1/0/all/0/1"&gt;Martina Miliani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TF-IDF vs Word Embeddings for Morbidity Identification in Clinical Notes: An Initial Study. (arXiv:2105.09632v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09632</id>
        <link href="http://arxiv.org/abs/2105.09632"/>
        <updated>2021-05-23T06:08:16.414Z</updated>
        <summary type="html"><![CDATA[Today, we are seeing an ever-increasing number of clinical notes that contain
clinical results, images, and textual descriptions of patient's health state.
All these data can be analyzed and employed to cater novel services that can
help people and domain experts with their common healthcare tasks. However,
many technologies such as Deep Learning and tools like Word Embeddings have
started to be investigated only recently, and many challenges remain open when
it comes to healthcare domain applications. To address these challenges, we
propose the use of Deep Learning and Word Embeddings for identifying sixteen
morbidity types within textual descriptions of clinical records. For this
purpose, we have used a Deep Learning model based on Bidirectional Long-Short
Term Memory (LSTM) layers which can exploit state-of-the-art vector
representations of data such as Word Embeddings. We have employed pre-trained
Word Embeddings namely GloVe and Word2Vec, and our own Word Embeddings trained
on the target domain. Furthermore, we have compared the performances of the
deep learning approaches against the traditional tf-idf using Support Vector
Machine and Multilayer perceptron (our baselines). From the obtained results it
seems that the latter outperforms the combination of Deep Learning approaches
using any word embeddings. Our preliminary results indicate that there are
specific features that make the dataset biased in favour of traditional machine
learning approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dessi_D/0/1/0/all/0/1"&gt;Danilo Dessi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Helaoui_R/0/1/0/all/0/1"&gt;Rim Helaoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vivek Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Recupero_D/0/1/0/all/0/1"&gt;Diego Reforgiato Recupero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riboni_D/0/1/0/all/0/1"&gt;Daniele Riboni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Detecting Need for Empathetic Response in Motivational Interviewing. (arXiv:2105.09649v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09649</id>
        <link href="http://arxiv.org/abs/2105.09649"/>
        <updated>2021-05-23T06:08:16.391Z</updated>
        <summary type="html"><![CDATA[Empathetic response from the therapist is key to the success of clinical
psychotherapy, especially motivational interviewing. Previous work on
computational modelling of empathy in motivational interviewing has focused on
offline, session-level assessment of therapist empathy, where empathy captures
all efforts that the therapist makes to understand the client's perspective and
convey that understanding to the client. In this position paper, we propose a
novel task of turn-level detection of client need for empathy. Concretely, we
propose to leverage pre-trained language models and empathy-related general
conversation corpora in a unique labeller-detector framework, where the
labeller automatically annotates a motivational interviewing conversation
corpus with empathy labels to train the detector that determines the need for
therapist empathy. We also lay out our strategies of extending the detector
with additional-input and multi-task setups to improve its detection and
explainability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zixiu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Helaoui_R/0/1/0/all/0/1"&gt;Rim Helaoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vivek Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Recupero_D/0/1/0/all/0/1"&gt;Diego Reforgiato Recupero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riboni_D/0/1/0/all/0/1"&gt;Daniele Riboni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MLBiNet: A Cross-Sentence Collective Event Detection Network. (arXiv:2105.09458v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09458</id>
        <link href="http://arxiv.org/abs/2105.09458"/>
        <updated>2021-05-23T06:08:16.366Z</updated>
        <summary type="html"><![CDATA[We consider the problem of collectively detecting multiple events,
particularly in cross-sentence settings. The key to dealing with the problem is
to encode semantic information and model event inter-dependency at a
document-level. In this paper, we reformulate it as a Seq2Seq task and propose
a Multi-Layer Bidirectional Network (MLBiNet) to capture the document-level
association of events and semantic information simultaneously. Specifically, a
bidirectional decoder is firstly devised to model event inter-dependency within
a sentence when decoding the event tag vector sequence. Secondly, an
information aggregation module is employed to aggregate sentence-level semantic
and event tag information. Finally, we stack multiple bidirectional decoders
and feed cross-sentence information, forming a multi-layer bidirectional
tagging architecture to iteratively propagate information across sentences. We
show that our approach provides significant improvement in performance compared
to the current state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lou_D/0/1/0/all/0/1"&gt;Dongfang Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1"&gt;Zhilin Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1"&gt;Shumin Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Huajun Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LAST at SemEval-2021 Task 1: Improving Multi-Word Complexity Prediction Using Bigram Association Measures. (arXiv:2105.09653v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09653</id>
        <link href="http://arxiv.org/abs/2105.09653"/>
        <updated>2021-05-23T06:08:16.352Z</updated>
        <summary type="html"><![CDATA[This paper describes the system developed by the Laboratoire d'analyse
statistique des textes (LAST) for the Lexical Complexity Prediction shared task
at SemEval-2021. The proposed system is made up of a LightGBM model fed with
features obtained from many word frequency lists, published lexical norms and
psychometric data. For tackling the specificity of the multi-word task, it uses
bigram association measures. Despite that the only contextual feature used was
sentence length, the system achieved an honorable performance in the multi-word
task, but poorer in the single word task. The bigram association measures were
found useful, but to a limited extent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bestgen_Y/0/1/0/all/0/1"&gt;Yves Bestgen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning for Many-to-many Multilingual Neural Machine Translation. (arXiv:2105.09501v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09501</id>
        <link href="http://arxiv.org/abs/2105.09501"/>
        <updated>2021-05-23T06:08:16.338Z</updated>
        <summary type="html"><![CDATA[Existing multilingual machine translation approaches mainly focus on
English-centric directions, while the non-English directions still lag behind.
In this work, we aim to build a many-to-many translation system with an
emphasis on the quality of non-English language directions. Our intuition is
based on the hypothesis that a universal cross-language representation leads to
better multilingual translation performance. To this end, we propose \method, a
training method to obtain a single unified multilingual translation model.
mCOLT is empowered by two techniques: (i) a contrastive learning scheme to
close the gap among representations of different languages, and (ii) data
augmentation on both multiple parallel and monolingual data to further align
token representations. For English-centric directions, mCOLT achieves
competitive or even better performance than a strong pre-trained model mBART on
tens of WMT benchmarks. For non-English directions, mCOLT achieves an
improvement of average 10+ BLEU compared with the multilingual baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xiao Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mingxuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Liwei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable Health Risk Predictor with Transformer-based Medicare Claim Encoder. (arXiv:2105.09428v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09428</id>
        <link href="http://arxiv.org/abs/2105.09428"/>
        <updated>2021-05-23T06:08:16.214Z</updated>
        <summary type="html"><![CDATA[In 2019, The Centers for Medicare and Medicaid Services (CMS) launched an
Artificial Intelligence (AI) Health Outcomes Challenge seeking solutions to
predict risk in value-based care for incorporation into CMS Innovation Center
payment and service delivery models. Recently, modern language models have
played key roles in a number of health related tasks. This paper presents, to
the best of our knowledge, the first application of these models to patient
readmission prediction. To facilitate this, we create a dataset of 1.2 million
medical history samples derived from the Limited Dataset (LDS) issued by CMS.
Moreover, we propose a comprehensive modeling solution centered on a deep
learning framework for this data. To demonstrate the framework, we train an
attention-based Transformer to learn Medicare semantics in support of
performing downstream prediction tasks thereby achieving 0.91 AUC and 0.91
recall on readmission classification. We also introduce a novel data
pre-processing pipeline and discuss pertinent deployment considerations
surrounding model explainability and bias.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lahlou_C/0/1/0/all/0/1"&gt;Chuhong Lahlou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crayton_A/0/1/0/all/0/1"&gt;Ancil Crayton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trier_C/0/1/0/all/0/1"&gt;Caroline Trier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Willett_E/0/1/0/all/0/1"&gt;Evan Willett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dependency Parsing with Bottom-up Hierarchical Pointer Networks. (arXiv:2105.09611v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09611</id>
        <link href="http://arxiv.org/abs/2105.09611"/>
        <updated>2021-05-23T06:08:16.202Z</updated>
        <summary type="html"><![CDATA[Dependency parsing is a crucial step towards deep language understanding and,
therefore, widely demanded by numerous Natural Language Processing
applications. In particular, left-to-right and top-down transition-based
algorithms that rely on Pointer Networks are among the most accurate approaches
for performing dependency parsing. Additionally, it has been observed for the
top-down algorithm that Pointer Networks' sequential decoding can be improved
by implementing a hierarchical variant, more adequate to model dependency
structures. Considering all this, we develop a bottom-up-oriented Hierarchical
Pointer Network for the left-to-right parser and propose two novel
transition-based alternatives: an approach that parses a sentence in
right-to-left order and a variant that does it from the outside in. We
empirically test the proposed neural architecture with the different algorithms
on a wide variety of languages, outperforming the original approach in
practically all of them and setting new state-of-the-art results on the English
and Chinese Penn Treebanks for non-contextualized and BERT-based embeddings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fernandez_Gonzalez_D/0/1/0/all/0/1"&gt;Daniel Fern&amp;#xe1;ndez-Gonz&amp;#xe1;lez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1"&gt;Carlos G&amp;#xf3;mez-Rodr&amp;#xed;guez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computational Morphology with Neural Network Approaches. (arXiv:2105.09404v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09404</id>
        <link href="http://arxiv.org/abs/2105.09404"/>
        <updated>2021-05-23T06:08:16.168Z</updated>
        <summary type="html"><![CDATA[Neural network approaches have been applied to computational morphology with
great success, improving the performance of most tasks by a large margin and
providing new perspectives for modeling. This paper starts with a brief
introduction to computational morphology, followed by a review of recent work
on computational morphology with neural network approaches, to provide an
overview of the area. In the end, we will analyze the advantages and problems
of neural network approaches to computational morphology, and point out some
directions to be explored by future research and study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Ling Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[See, Hear, Read: Leveraging Multimodality with Guided Attention for Abstractive Text Summarization. (arXiv:2105.09601v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2105.09601</id>
        <link href="http://arxiv.org/abs/2105.09601"/>
        <updated>2021-05-23T06:08:16.148Z</updated>
        <summary type="html"><![CDATA[In recent years, abstractive text summarization with multimodal inputs has
started drawing attention due to its ability to accumulate information from
different source modalities and generate a fluent textual summary. However,
existing methods use short videos as the visual modality and short summary as
the ground-truth, therefore, perform poorly on lengthy videos and long
ground-truth summary. Additionally, there exists no benchmark dataset to
generalize this task on videos of varying lengths. In this paper, we introduce
AVIATE, the first large-scale dataset for abstractive text summarization with
videos of diverse duration, compiled from presentations in well-known academic
conferences like NDSS, ICML, NeurIPS, etc. We use the abstract of corresponding
research papers as the reference summaries, which ensure adequate quality and
uniformity of the ground-truth. We then propose {\name}, a factorized
multi-modal Transformer based decoder-only language model, which inherently
captures the intra-modal and inter-modal dynamics within various input
modalities for the text summarization task. {\name} utilizes an increasing
number of self-attentions to capture multimodality and performs significantly
better than traditional encoder-decoder based networks. Extensive experiments
illustrate that {\name} achieves significant improvement over the baselines in
both qualitative and quantitative evaluations on the existing How2 dataset for
short videos and newly introduced AVIATE dataset for videos with diverse
duration, beating the best baseline on the two datasets by $1.39$ and $2.74$
ROUGE-L points respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Atri_Y/0/1/0/all/0/1"&gt;Yash Kumar Atri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pramanick_S/0/1/0/all/0/1"&gt;Shraman Pramanick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_V/0/1/0/all/0/1"&gt;Vikram Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1"&gt;Tanmoy Chakraborty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geographic Question Answering: Challenges, Uniqueness, Classification, and Future Directions. (arXiv:2105.09392v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09392</id>
        <link href="http://arxiv.org/abs/2105.09392"/>
        <updated>2021-05-23T06:08:16.136Z</updated>
        <summary type="html"><![CDATA[As an important part of Artificial Intelligence (AI), Question Answering (QA)
aims at generating answers to questions phrased in natural language. While
there has been substantial progress in open-domain question answering, QA
systems are still struggling to answer questions which involve geographic
entities or concepts and that require spatial operations. In this paper, we
discuss the problem of geographic question answering (GeoQA). We first
investigate the reasons why geographic questions are difficult to answer by
analyzing challenges of geographic questions. We discuss the uniqueness of
geographic questions compared to general QA. Then we review existing work on
GeoQA and classify them by the types of questions they can address. Based on
this survey, we provide a generic classification framework for geographic
questions. Finally, we conclude our work by pointing out unique future research
directions for GeoQA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mai_G/0/1/0/all/0/1"&gt;Gengchen Mai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Janowicz_K/0/1/0/all/0/1"&gt;Krzysztof Janowicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1"&gt;Rui Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1"&gt;Ling Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lao_N/0/1/0/all/0/1"&gt;Ni Lao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Knowledge-Enhanced Bayesian Meta-Learning for Few-shot Event Detection. (arXiv:2105.09509v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09509</id>
        <link href="http://arxiv.org/abs/2105.09509"/>
        <updated>2021-05-23T06:08:16.101Z</updated>
        <summary type="html"><![CDATA[Event detection (ED) aims at detecting event trigger words in sentences and
classifying them into specific event types. In real-world applications, ED
typically does not have sufficient labelled data, thus can be formulated as a
few-shot learning problem. To tackle the issue of low sample diversity in
few-shot ED, we propose a novel knowledge-based few-shot event detection method
which uses a definition-based encoder to introduce external event knowledge as
the knowledge prior of event types. Furthermore, as external knowledge
typically provides limited and imperfect coverage of event types, we introduce
an adaptive knowledge-enhanced Bayesian meta-learning method to dynamically
adjust the knowledge prior of event types. Experiments show our method
consistently and substantially outperforms a number of baselines by at least 15
absolute F1 points under the same few-shot settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1"&gt;Shirong Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tongtong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1"&gt;Guilin Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuan-Fang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1"&gt;Gholamreza Haffari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1"&gt;Sheng Bi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unified Dual-view Cognitive Model for Interpretable Claim Verification. (arXiv:2105.09567v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2105.09567</id>
        <link href="http://arxiv.org/abs/2105.09567"/>
        <updated>2021-05-23T06:08:16.080Z</updated>
        <summary type="html"><![CDATA[Recent studies constructing direct interactions between the claim and each
single user response (a comment or a relevant article) to capture evidence have
shown remarkable success in interpretable claim verification. Owing to
different single responses convey different cognition of individual users
(i.e., audiences), the captured evidence belongs to the perspective of
individual cognition. However, individuals' cognition of social things is not
always able to truly reflect the objective. There may be one-sided or biased
semantics in their opinions on a claim. The captured evidence correspondingly
contains some unobjective and biased evidence fragments, deteriorating task
performance. In this paper, we propose a Dual-view model based on the views of
Collective and Individual Cognition (CICD) for interpretable claim
verification. From the view of the collective cognition, we not only capture
the word-level semantics based on individual users, but also focus on
sentence-level semantics (i.e., the overall responses) among all users and
adjust the proportion between them to generate global evidence. From the view
of individual cognition, we select the top-$k$ articles with high degree of
difference and interact with the claim to explore the local key evidence
fragments. To weaken the bias of individual cognition-view evidence, we devise
inconsistent loss to suppress the divergence between global and local evidence
for strengthening the consistent shared evidence between the both. Experiments
on three benchmark datasets confirm that CICD achieves state-of-the-art
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lianwei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1"&gt;Yuan Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1"&gt;Yuqian Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Ling Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1"&gt;Zhaoyin Qi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Decade Survey of Content Based Image Retrieval using Deep Learning. (arXiv:2012.00641v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00641</id>
        <link href="http://arxiv.org/abs/2012.00641"/>
        <updated>2021-05-23T06:08:15.920Z</updated>
        <summary type="html"><![CDATA[The content based image retrieval aims to find the similar images from a
large scale dataset against a query image. Generally, the similarity between
the representative features of the query image and dataset images is used to
rank the images for retrieval. In early days, various hand designed feature
descriptors have been investigated based on the visual cues such as color,
texture, shape, etc. that represent the images. However, the deep learning has
emerged as a dominating alternative of hand-designed feature engineering from a
decade. It learns the features automatically from the data. This paper presents
a comprehensive survey of deep learning based developments in the past decade
for content based image retrieval. The categorization of existing
state-of-the-art methods from different perspectives is also performed for
greater understanding of the progress. The taxonomy used in this survey covers
different supervision, different networks, different descriptor type and
different retrieval type. A performance analysis is also performed using the
state-of-the-art methods. The insights are also presented for the benefit of
the researchers to observe the progress and to make the best choices. The
survey presented in this paper will help in further research progress in image
retrieval using deep learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1"&gt;Shiv Ram Dubey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Subverting the Jewtocracy": Online Antisemitism Detection Using Multimodal Deep Learning. (arXiv:2104.05947v2 [cs.MM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05947</id>
        <link href="http://arxiv.org/abs/2104.05947"/>
        <updated>2021-05-23T06:08:15.873Z</updated>
        <summary type="html"><![CDATA[The exponential rise of online social media has enabled the creation,
distribution, and consumption of information at an unprecedented rate. However,
it has also led to the burgeoning of various forms of online abuse. Increasing
cases of online antisemitism have become one of the major concerns because of
its socio-political consequences. Unlike other major forms of online abuse like
racism, sexism, etc., online antisemitism has not been studied much from a
machine learning perspective. To the best of our knowledge, we present the
first work in the direction of automated multimodal detection of online
antisemitism. The task poses multiple challenges that include extracting
signals across multiple modalities, contextual references, and handling
multiple aspects of antisemitism. Unfortunately, there does not exist any
publicly available benchmark corpus for this critical task. Hence, we collect
and label two datasets with 3,102 and 3,509 social media posts from Twitter and
Gab respectively. Further, we present a multimodal deep learning system that
detects the presence of antisemitic content and its specific antisemitism
category using text and images from posts. We perform an extensive set of
experiments on the two datasets to evaluate the efficacy of the proposed
system. Finally, we also present a qualitative analysis of our study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_M/0/1/0/all/0/1"&gt;Mohit Chandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pailla_D/0/1/0/all/0/1"&gt;Dheeraj Pailla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhatia_H/0/1/0/all/0/1"&gt;Himanshu Bhatia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchawala_A/0/1/0/all/0/1"&gt;Aadilmehdi Sanchawala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1"&gt;Manish Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1"&gt;Manish Shrivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1"&gt;Ponnurangam Kumaraguru&lt;/a&gt;</name>
        </author>
    </entry>
</feed>